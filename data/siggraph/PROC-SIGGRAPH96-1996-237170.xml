<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date></start_date>
		<end_date></end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[]]></city>
		<state></state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>237170</proc_id>
	<acronym>SIGGRAPH '96</acronym>
	<proc_desc>Proceedings of the 23rd annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>0-89791-746-4</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1996</copyright_year>
	<publication_date>08-01-1996</publication_date>
	<pages>528</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source>ACM member price $50</other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>I.3.0</cat_node>
			<descriptor/>
			<type/>
		</primary_category>
		<other_category>
			<cat_node>I.3.3</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.3.5</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.3.7</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010371.10010396</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010352</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
			<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010372</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10003752.10010061.10010063</concept_id>
			<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10003752.10010061</concept_id>
			<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010382</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Algorithms</gt>
		<gt>Design</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>PP39097619</person_id>
			<author_profile_id><![CDATA[81339499981]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[John]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Fujii]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Hewlett-Packard]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1996</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>237191</article_id>
		<sort_key>11</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Modeling and rendering architecture from photographs]]></title>
		<subtitle><![CDATA[a hybrid geometry- and image-based approach]]></subtitle>
		<page_from>11</page_from>
		<page_to>20</page_to>
		<doi_number>10.1145/237170.237191</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237191</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P221187</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California at Berkeley, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39046626</person_id>
				<author_profile_id><![CDATA[81100518594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Camillo]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Taylor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California at Berkeley, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15029665</person_id>
				<author_profile_id><![CDATA[81100342430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jitendra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California at Berkeley, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>628686</ref_obj_id>
				<ref_obj_pid>628319</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ali Azarbayejani and Alex Pentland. Recursive estimation of motion, structure, and focal length. IEEE Trans. Pattern Anal. Machine Intell., 17 (6):562-575, June 1995.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H.H. Baker and T. O. Binford. Depth from edge and intensity based stereo. In Proceedings of the Seventh IJCAI, Vancouvel; BC, pages 631-636,1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>894876</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. Technical Report UCB//CSD-96-893, U.C. Berkeley, CS Division, January 1996.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108646</ref_obj_id>
				<ref_obj_pid>108641</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D.J.Fleet, A.D.Jepson, and M.R.M. Jenkin. Phase-based disparity measurement. CVGIP: Image Understanding, 53 (2): 198-210,1991.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Oliver Faugeras and Giorgio Toscani. The calibration problem for stereo. In P1vceedings lEEE CVPR 86, pages 15-20,1986.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Olivier Faugeras. Three-Dimensional Computer Vision. MIT Press, 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Olivier Faugeras, Stephane Laveau, Luc Robert, Gabriella Csurka, and Cyril Zeller. 3-d reconstruction of urban scenes from sequences of images. Technical Report 2572, INRIA, June 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[W.E.L. Grimson. Firm Images to Sulface. MIT Press, 1981.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>146442</ref_obj_id>
				<ref_obj_pid>146435</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[D. Jones and J. Malik. Computational framework for determining stereo correspondence from a set of linear spatial filters. Image and Vision Computing, 10(10) :699-708, December 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[E. Kruppa. Zur ermittlung eines objectes aus zwei perspektiven mit innerer orientierung. Sitz.-Be~: Akad. Wiss., Wien, Math. Naturw. Kl., Abt. Ila., 122:1939- 1948, 1913.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[H.C. Longuet-Higgins. A computer algorithm for reconstructing a scene from two projections. Nature, 293:133-135, September 1981.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. Marr and T. Poggio. A computationaltheory of human stereo vision. P~vceedings of the Royal Society of London, 204:301-328,1979.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan and Gary Bishop. Plenoptic modeling: An image-based rendering system. In SIGGRAPH' 95, 1995.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218442</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Eric N. Mortensen and William A. Barrett. Intelligent scissors for image composition. In SIGGRAPH' 95, 1995.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[S.B. Pollard, J. E. W. Mayhew, and J. R Frisby. A stereo correspondence algorithm using a disparity gradient limit. Perception, 14:449-470,1985.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. Szeliski. Image mosaicing for tele-reality applications. In IEEE Computer Graphics and Applications, 1996.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>628737</ref_obj_id>
				<ref_obj_pid>628324</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Camillo J. Taylor and David J. Kriegman. Structure and motion from line segments in multiple images. IEEE Trans. Pattern Anal. Machine Intell., 17(11), November 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192279</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[S.J. Teller, Celeste Fowler, Thomas Funkhouser, and Pat Hanrahan. Partitioning and ordering large radiosity computations. In SIGGRAPH '94, pages 443-450, 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>144403</ref_obj_id>
				<ref_obj_pid>144398</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Carlo Tomasi and Takeo Kanade. Shape and motion from image streams under orthography: a factorization method. International Journal of Computer Vision, 9(2):137-154, November 1992.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Roger Tsai. A versatile camera calibration technique for high accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. IEEE Journal of Robotics and Automation, 3 (4):323-344, August 1987.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[S. Ullman. The Interpretation of Visual Motion. The MIT Press, Cambridge, MA, 1979.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[L Williams. Casting curved shadows on curved surfaces. In SIGGRAPH'78, pages 270-274,1978.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Lance Williams and Eric Chen. View interpolation for image synthesis. In SIG- GRAPH '93, 1993.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>189412</ref_obj_id>
				<ref_obj_pid>189359</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Mourad Zerroug and Ramakant Nevatia. Segmentation and recovery of shgcs from a real intensity image. In Eu~vpean Conference on Computer Vision, pages 319-330,1994.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Modeling and Rendering Architecture from Photographs: A hybrid geometry-and image-based approach Paul 
E. Debevec Camillo J. Taylor Jitendra Malik University of California at Berkeley1 ABSTRACT We present 
a new approach for modeling and rendering existing ar­chitectural scenes from a sparse set of still photographs. 
Our mod­eling approach, which combines both geometry-based and image­based techniques, has two components. 
The .rst component is a photogrammetricmodeling method which facilitates the recovery of the basic geometry 
of the photographed scene. Our photogrammet­ric modeling approach is effective, convenient, and robust 
because it exploits the constraints that are characteristic of architectural scenes. The second component 
is a model-based stereo algorithm, which recovers how the real scene deviates from the basic model. By 
making use of the model, our stereo technique robustly recovers accurate depth from widely-spaced image 
pairs. Consequently, our approach can model large architectural environments with far fewer photographs 
than current image-based modeling approaches. For producing renderings, we present view-dependent texture 
mapping, a method of compositing multiple views of a scene that better sim­ulates geometric detail on 
basic models. Our approach can be used to recover models for use in either geometry-based or image-based 
rendering systems. We present results that demonstrate our ap­proach s ability to create realistic renderings 
of architectural scenes from viewpoints far from the original photographs. CR Descriptors: I.2.10 [Arti.cial 
Intelligence]: Vision and Scene Understanding -Modeling and recovery of physical at­tributes; I.3.7 [Computer 
Graphics]: Three-Dimensional Graph­ics and Realism -Color, shading, shadowing, and texture I.4.8 [Im­age 
Processing]: Scene Analysis -Stereo;J.6 [Computer-Aided Engineering]: Computer-aided design (CAD). 1 
INTRODUCTION Efforts to model the appearance and dynamics of the real world have produced some of the 
most compelling imagery in computer graphics. In particular, efforts to model architectural scenes, from 
the Amiens Cathedral to the Giza Pyramids to Berkeley s Soda Hall, have produced impressive walk-throughs 
and inspiring .y­bys. Clearly, it is an attractive application to be able to explore the world s architecture 
unencumbered by fences, gravity, customs, or jetlag. 1Computer Science Division, University of California 
at Berkeley, Berkeley, CA 94720-1776. fdebevec,camillo,malikg@cs.berkeley.edu. See also http://www.cs.berkeley.edu/ 
debevec/Research Permission to make digital or hard copies of part or all of this work or personal or 
classroom use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 Unfortunately, current geometry-based methods 
(Fig. 1a) of modeling existing architecture, in which a modeling program is used to manually position 
the elements of the scene, have several drawbacks. First, the process is extremely labor-intensive, typically 
involving surveying the site, locating and digitizing architectural plans (if available), or converting 
existing CAD data (again, if avail­able). Second, it is dif.cult to verify whether the resulting model 
is accurate. Most disappointing, though, is that the renderings of the resulting models are noticeably 
computer-generated; even those that employ liberal texture-mapping generally fail to resemble real pho­tographs. 
(b) Hybrid Approach images user input (a) Geometry-Based (c) Image-Based renderings Image Warping renderings 
renderings Figure 1: Schematic of how our hybrid approach combines geometry-based and image-based approaches 
to modeling and ren­dering architecture from photographs. Recently, creating models directly from photographs 
has re­ceived increased interest in computer graphics. Since real images are used as input, such an image-based 
system (Fig. 1c) has an ad­vantage in producing photorealistic renderings as output. Some of the most 
promising of these systems [16, 13] rely on the computer vision technique of computational stereopsis 
to automatically deter­mine the structure of the scene from the multiple photographs avail­able. As a 
consequence, however, these systems are only as strong as the underlying stereo algorithms. This has 
caused problems be­cause state-of-the-art stereo algorithms have a number of signi.­cant weaknesses; 
in particular, the photographs need to appear very similar for reliable results to be obtained. Because 
of this, current image-based techniques must use many closely spaced images, and in some cases employ 
signi.cant amounts of user input for each im­age pair to supervise the stereo algorithm. In this framework, 
cap­turing the data for a realistically renderable model would require an impractical number of closely 
spaced photographs, and deriving the depth from the photographs could require an impractical amount of 
user input. These concessions to the weakness of stereo algorithms bode poorly for creating large-scale, 
freely navigable virtual envi­ronments from photographs. Our research aims to make the process of modeling 
architectural scenes more convenient, more accurate, and more photorealistic than the methods currently 
available. To do this, we have developed a new approach that draws on the strengths of both geometry-based 
andimage-basedmethods,asillustratedinFig. 1b.Theresultisthat our approach to modeling and rendering architecture 
requires only a sparse set of photographs and can produce realistic renderings from arbitrary viewpoints. 
In our approach, a basic geometric model of the architecture is recovered interactively with an easy-to-use 
pho­togrammetric modeling system, novel views are created using view­dependent texture mapping, and additional 
geometric detail can be recovered automatically through stereo correspondence. The .nal images can be 
rendered with current image-based rendering tech­niques. Because only photographs are required, our approach 
to modeling architecture is neither invasive nor does it require archi­tectural plans, CAD models, or 
specialized instrumentation such as surveying equipment, GPS sensors or range scanners. 1.1 Background 
and Related Work The process of recovering 3D structure from 2D images has been a central endeavor within 
computer vision, and the process of ren­dering such recovered structures is a subject receiving increased 
interest in computer graphics. Although no general technique ex­ists to derive models from images, four 
particular areas of research have provided results that are applicable to the problem of modeling and 
rendering architectural scenes. They are: Camera Calibration, Structure from Motion, Stereo Correspondence, 
and Image-Based Rendering. 1.1.1 Camera Calibration Recovering 3D structure from images becomes a simpler 
problem when the cameras used are calibrated, that is, the mapping between image coordinates and directions 
relative to each camera is known. This mapping is determined by, among other parameters, the cam­era 
s focal length and its pattern of radial distortion. Camera cali­bration is a well-studied problem both 
in photogrammetry and com­puter vision; some successful methods include [20] and [5]. While there has 
been recent progress in the use of uncalibrated views for 3D reconstruction [7], we have found camera 
calibration to be a straightforward process that considerably simpli.es the problem. 1.1.2 Structure 
from Motion Given the 2D projection of a point in the world, its position in 3D space could be anywhere 
on a ray extending out in a particular di­rection from the camera s optical center. However, when the 
pro­jections of a suf.cient number of points in the world are observed in multiple images from different 
positions, it is theoretically possi­ble to deduce the 3D locations of the points as well as the positions 
of the original cameras, up to an unknown factor of scale. This problem has been studied in the area 
of photogrammetry for the principal purpose of producing topographic maps. In 1913, Kruppa [10] proved 
the fundamental result that given two views of .ve distinct points, one could recover the rotation and 
translation between the two camera positions as well as the 3D locations of the points (up to a scale 
factor). Since then, the problem s mathematical and algorithmic aspects have been explored starting from 
the funda­mental work of Ullman [21] and Longuet-Higgins [11], in the early 1980s. Faugeras s book [6] 
overviews the state of the art as of 1992. So far, a key realization has been that the recovery of structure 
is very sensitive to noise in image measurements when the translation between the available camera positions 
is small. Attention has turned to using more than two views with image stream methods such as [19] or 
recursive approaches (e.g. [1]). [19] shows excellent results for the case of orthographic cameras, but 
di­rect solutions for the perspective case remain elusive. In general, linear algorithms for the problem 
fail to make use of all available information while nonlinear minimization methods are prone to dif­.culties 
arising from local minima in the parameter space. An alter­native formulation of the problem [17] uses 
lines rather than points as image measurements, but the previously stated concerns were shown to remain 
largely valid. For purposes of computer graph­ics, there is yet another problem: the models recovered 
by these al­gorithms consist of sparse point .elds or individual line segments, which are not directly 
renderable as solid 3D models. In our approach, we exploit the fact that we are trying to re­cover geometric 
models of architectural scenes, not arbitrary three­dimensional point sets. This enables us to include 
additional con­straints not typically available to structure from motion algorithms and to overcome the 
problems of numerical instability that plague such approaches. Our approach is demonstrated in a useful 
interac­tive system for building architectural models from photographs. 1.1.3 Stereo Correspondence 
The geometrical theory of structure from motion assumes that one is able to solve the correspondence 
problem, which is to identify the points in two or more images that are projections of the same point 
in the world. In humans, corresponding points in the two slightly differing images on the retinas are 
determined by the visual cortex in the process called binocular stereopsis. Years of research (e.g. [2, 
4, 8, 9, 12, 15]) have shown that de­termining stereo correspondences by computer is dif.cult problem. 
In general, current methods are successful only when the images are similar in appearance, as in the 
case of human vision, which is usu­ally obtained by using cameras that are closely spaced relative to 
the objects in the scene. When the distance between the cameras (often called the baseline) becomes large, 
surfaces in the images exhibit different degrees of foreshortening, different patterns of occlusion, 
and large disparities in their locations in the two images, all of which makes it much more dif.cult 
for the computer to determine correct stereo correspondences. Unfortunately, the alternative of improving 
stereo correspondence by using images taken from nearby locations has the disadvantage that computing 
depth becomes very sensitive to noise in image measurements. In this paper, we show that having an approximate 
model of the photographed scene makes it possible to robustly determine stereo correspondences from images 
taken from widely varying view­points. Speci.cally, the model enables us to warp the images to eliminate 
unequal foreshortening and to predict major instances of occlusion before trying to .nd correspondences. 
 1.1.4 Image-Based Rendering In an image-based rendering system, the model consists of a set of images 
of a scene and their corresponding depth maps. When the depth of every point in an image is known, the 
image can be re­rendered from any nearby point of view by projecting the pixels of the image to their 
proper 3D locations and reprojecting them onto a new image plane. Thus, a new image of the scene is created 
by warping the images according to their depth maps. A principal at­traction of image-based rendering 
is that it offers a method of ren­dering arbitrarily complex scenes with a constant amount of com­putation 
required per pixel. Using this property, [23] demonstrated how regularly spaced synthetic images (with 
their computed depth maps) could be warped and composited in real time to produce a vir­tual environment. 
More recently, [13] presented a real-time image-based rendering system that used panoramic photographs 
with depth computed, in part, from stereo correspondence. One .nding of the paper was that extracting 
reliable depth estimates from stereo is very dif.cult . The method was nonetheless able to obtain acceptable 
results for nearby views using user input to aid the stereo depth recovery: the correspondencemapforeachimagepairwasseededwith 
100to 500 user-supplied point correspondences and also post-processed. Even with user assistance, the 
images used still had to be closely spaced; the largest baseline described in the paper was .ve feet. 
 The requirement that samples be close together is a serious lim­itation to generating a freely navigable 
virtual environment. Cov­ering the size of just one city block would require thousands of panoramic images 
spaced .ve feet apart. Clearly, acquiring so many photographs is impractical. Moreover, even a dense 
lattice of ground-basedphotographswouldonlyallow renderingsto begener­ated from within a few feet of 
the original camera level, precluding any virtual .y-bys of the scene. Extending the dense lattice of 
pho­tographs into three dimensions would clearly make the acquisition process even more dif.cult. The 
approach described in this paper takes advantage of the structure in architectural scenes so that it 
re­quires only a sparse set of photographs. For example, our approach has yielded a virtual .y-around 
of a building from just twelve stan­dard photographs.  1.2 Overview In this paper we present three 
new modeling and rendering tech­niques: photogrammetric modeling, view-dependent texture map­ping, and 
model-based stereo. We show how these techniques can be used in conjunction to yield a convenient, accurate, 
and photo­realistic method of modeling and rendering architecture from pho­tographs. In our approach, 
the photogrammetric modeling program is used to create a basic volumetric model of the scene, which is 
then used to constrain stereo matching. Our rendering method compos­ites information frommultiple imageswith 
view-dependenttexture­mapping. Our approach is successful because it splits the task of modeling from 
images into tasks which are easily accomplished by a person (but not a computer algorithm), and tasks 
which are easily performed by a computer algorithm (but not a person). In Section 2, we present our photogrammetric 
modeling method. In essence, we have recast the structure from motion prob­lem not as the recovery of 
individual point coordinates, but as the recovery of the parameters of a constrained hierarchy of parametric 
primitives. The result is that accurate architectural models can be recovered robustly from just a few 
photographs and with a minimal number of user-supplied correspondences. In Section 3, we present view-dependent 
texture mapping,and show how it can be used to realistically render the recovered model. Unlike traditional 
texture-mapping, in which a single static image is used to color in each face of the model, view-dependent 
tex­ture mapping interpolates between the available photographs of the scene depending on the user s 
point of view. This results in more lifelike animations that better capture surface specularities and 
un­modeled geometric detail. Lastly, in Section 4, we present model-based stereo,which is used to automatically 
re.ne a basic model of a photographed scene. This technique can be used to recover the structure of architectural 
ornamentation that would be dif.cult to recover with photogram­metric modeling. In particular, we show 
that projecting pairs of im­ages onto an initial approximate model allows conventional stereo techniques 
to robustly recover very accurate depth measurements from images with widely varying viewpoints.  2 
Photogrammetric Modeling In this section we present our method for photogrammetric model­ing, in which 
the computer determines the parameters of a hierar­chical model of parametric polyhedral primitives to 
reconstruct the architectural scene. We have implemented this method in Fac¸ade, an easy-to-use interactive 
modeling program that allows the user to construct a geometric model of a scene from digitized photographs. 
We .rst overview Fac¸ade from the point of view of the user, then we describe our model representation, 
and then we explain our recon­struction algorithm. Lastly, we present results from using Fac¸ade to reconstruct 
several architectural scenes. 2.1 The User s View Constructing a geometric model of an architectural 
scene using Fac¸ade is an incremental and straightforward process. Typically, the user selects a small 
number of photographs to begin with, and mod­els the scene one piece at a time. The user may re.ne the 
model and include more images in the project until the model meets the desired level of detail. Fig. 
2(a) and (b) shows the two types of windows used in Fac¸ade: image viewers and model viewers. The user 
instantiates the com­ponents of the model, marks edges in the images, and corresponds the edges in the 
images to the edges in the model. When instructed, Fac¸ade computes the sizes and relative positions 
of the model com­ponents that best .t the edges marked in the photographs. Components of the model, called 
blocks, are parameterized ge­ometric primitives such as boxes, prisms, and surfaces of revolu­tion. A 
box, for example, is parameterized by its length, width, and height. The user models the scene as a collection 
of such blocks, creating new block classes as desired. Of course, the user does not need to specify numerical 
values for the blocks parameters, since these are recovered by the program. The user may choose to constrain 
the sizes and positions of any of the blocks. In Fig. 2(b), most of the blocks have been constrained 
to have equal length and width. Additionally, the four pinnacles have been constrained to have the same 
shape. Blocks may also be placed in constrained relations to one other. For example, many of the blocks 
in Fig. 2(b) have been constrained to sit centered and on top of the block below. Such constraints are 
speci.ed using a graph­ical 3D interface. When such constraints are provided, they are used to simplify 
the reconstruction problem. The user marks edge features in the images using a point-and­click interface; 
a gradient-based technique as in [14] can be used to align the edges with sub-pixel accuracy. We use 
edge rather than point features since they are easier to localize and less likely to be completely obscured. 
Only a section of each edge needs to be marked, making it possible to use partially visible edges. For 
each marked edge, the user also indicates the corresponding edge in the model. Generally, accurate reconstructions 
are obtained if there are as many correspondences in the images as there are free camera and model parameters. 
Thus, Fac¸ade reconstructs scenes accurately even when just a portion of the visible edges and marked 
in the im­ages, and when just a portion of the model edges are given corre­spondences. At any time, the 
user may instruct the computer to reconstruct the scene. The computer then solves for the parameters 
of the model that cause it to align with the marked features in the images. Dur­ing the reconstruction, 
the computer computes and displays the lo­cations from which the photographs were taken. For simple models 
consisting of just a few blocks, a full reconstruction takes only a few seconds; for more complex models, 
it can take a few minutes. For this reason, the user can instruct the computer to employ faster but less 
precise reconstruction algorithms (see Sec. 2.4) during the in­termediate stages of modeling. To verify 
the the accuracy of the recovered model and camera po­sitions, Fac¸ade can project the model into the 
original photographs. Typically, the projected model deviates from the photographs by less than a pixel. 
Fig. 2(c) shows the results of projecting the edges of the model in Fig. 2(b) into the original photograph. 
Lastly, the user may generate novel views of the model by posi­tioning a virtual camera at any desired 
location. Fac¸ade will then use the view-dependent texture-mapping method of Section 3 to render a novel 
view of the scene from the desired location. Fig. 2(d) shows an aerial rendering of the tower model. 
 2.2 Model Representation Thepurposeofourchoiceofmodelrepresentationisto representthe scene as a surface 
model with as few parameters as possible: when  (a) (b) (c) (d) Figure 2: (a) A photograph of the Campanile, 
Berkeley s clock tower, with marked edges shown in green. (b) The model recovered by our photogrammetricmodelingmethod. 
Althoughonlytheleftpinnaclewasmarked,theremainingthree(includingonenotvisible)wererecovered from symmetrical 
constraints in the model. Our method allows any number of images to be used, but in this case constraints 
of symmetry made it possible to recover an accurate 3D model from a single photograph. (c) The accuracy 
of the model is veri.ed by reprojecting it into the original photograph through the recovered camera 
position. (d) A synthetic view of the Campanile generated using the view-dependent texture-mapping method 
described in Section 3. A real photograph from this position would be dif.cult to take since the camera 
position is 250 feet above the ground. the model has fewer parameters, the user needs to specify fewer 
cor­respondences, and the computer can reconstruct the model more ef­.ciently. In Fac¸ade, the scene 
is represented as a constrained hier­archical model of parametric polyhedral primitives, called blocks. 
Each block has a small set of parameters which serve to de.ne its size and shape. Each coordinate of 
each vertex of the block is then expressed as linear combination of the block s parameters, rel­ative 
to an internal coordinate frame. For example, for the wedge block in Fig. 3, the coordinates of the vertex 
Poare written in terms of the block parameters width, height,and lengthas Po= (-width;-height;length)T. 
Each block is also given an associ­ated bounding box.  Figure 3: A wedge block with its parameters 
and bounding box. ground_plane g (X)1 first_storey g (X)2 roof entrance (a) (b) Figure 4: (a) A geometric 
model of a simple building. (b) The model s hierarchical representation. The nodes in the tree repre­sent 
parametric primitives (called blocks) while the links contain the spatial relationships between the blocks. 
The blocks in Fac¸ade are organized in a hierarchical tree structure as shown in Fig. 4(b). Each node 
of the tree represents an individual block, while the links in the tree contain the spatial relationships 
be­tween blocks, called relations. Such hierarchical structures are also used in traditional modeling 
systems. The relation between a block and its parent is most generally rep­resented as a rotation matrix 
Rand a translation vector t.This rep­resentation requires six parameters: three each for Rand t.In archi­tectural 
scenes, however, the relationship between two blocks usu­ally has a simple form that can be represented 
with fewer parame­ters, and Fac¸ade allows the user to build such constraints onRand tinto the model. 
The rotation Rbetween a block and its parent can be speci.ed in one of three ways: .rst, as an unconstrained 
rotation, requiring three parameters; second, as a rotation about a particular coordinate axis, requiring 
just one parameter; or third, as a .xed or null rotation, requiring no parameters. Likewise, Fac¸ade 
allows for constraints to be placed on each component of the translation vector t. Speci.cally, the user 
can constrain the bounding boxes of two blocks to align themselves in some manner along each dimension. 
For example, in order to en­sure that the roof block in Fig. 4 lies on top of the .rst story block, the 
user can require that the maximum yextent of the .rst story block be equal to the minimum yextent of 
the roof block. With this constraint, the translation along the yaxis is computed (ty= MAX MIN (firststory-roof)) 
rather than represented as a pa­ yy rameter of the model. Each parameter of each instantiated block is 
actually a reference to a named symbolic variable, as illustrated in Fig. 5. As a result, two parameters 
of different blocks (or of the same block) can be equated by having each parameter reference the same 
symbol. This facility allows the user to equate two or more of the dimensions in a model, which makes 
modeling symmetrical blocks and repeated structure more convenient. Importantly, these constraints reduce 
the number of degrees of freedom in the model, which, as we will show, simpli.es the structure recovery 
problem. Once the blocks and their relations have been parameterized, it is straightforward to derive 
expressions for the world coordinates of the block vertices. Consider the set of edges which link a spe­ci.c 
block in the model to the ground plane as shown in Fig. 4. BLOCKS VARIABLES Block1 type: wedge length 
width height Block2 type: box length width height Figure 5: Representation of block parameters as symbol 
references. A single variable can be referenced by the model in multiple places, allowing constraints 
of symmetry to be embedded in the model. Let g1(X);:::;gn(X)represent the rigid transformations associated 
with each of these links, where Xrepresents the vector of all the model parameters. The world coordinates 
Pw(X)of a particular block vertex P(X)is then: Pw(X)=g(X):::gn(X)P(X) (1) 1 Similarly, the world orientation 
vw(X)of a particular line seg­ment v(X)is: vw(X)=g(X):::gn(X)v(X) (2) 1 In these equations, the point 
vectors Pand Pwand the orientation vectors vand vware represented in homogeneous coordinates. Modeling 
the scene with polyhedral blocks, as opposed to points, line segments, surface patches, or polygons, 
is advantageous for a number of reasons: Most architectural scenes are well modeled by an arrangement 
of geometric primitives.  Blocks implicitly contain common architectural elements such as parallel lines 
and right angles.  Manipulating block primitives is convenient since they are at a suitably high level 
of abstraction; individual features such as points and lines are less manageable.  A surface model of 
the scene is readily obtained from the blocks, so there is no need to infer surfaces from discrete fea­tures. 
 Modeling in terms of blocks and relationships greatly reduces the number of parameters that the reconstruction 
algorithm needs to recover.  The last point is crucial to the robustness of our reconstruction al­gorithm 
and the viability of our modeling system, and is illustrated best with an example. The model in Fig. 
2 is parameterized by just 33 variables (the unknown camera position adds six more). If each block in 
the scene were unconstrained (in its dimensions and posi­tion), the model would have 240 parameters; 
if each line segment in the scene were treated independently, the model would have 2,896 parameters. 
This reduction in the number of parameters greatly en­hances the robustness and ef.ciency of the method 
as compared to traditional structure from motion algorithms. Lastly, since the num­ber of correspondences 
needed to suitably overconstrain the mini­mization is roughly proportional to the number of parameters 
in the model, this reduction means that the number of correspondences re­quired of the user is manageable. 
 2.3 Reconstruction Algorithm Our reconstruction algorithm works by minimizing an objective function 
Othat sums the disparity between the projected edges of P the model and the edges marked in the images, 
i.e. O=Erri where Errirepresents the disparity computed for edge feature i. Thus, the unknown model parameters 
and camera positions are computed by minimizing Owith respect to these variables. Our sys­tem uses the 
the error function Errifrom [17], described below. (x1, y1) h1 P(s)h(s) Observed edge segment <R, t> 
predicted line: mxx + myy + mzf = 0 (x2, y2) (a) (b) Figure 6: (a) Projection of a straight line onto 
a camera s image plane. (b) The error function used in the reconstruction algorithm. Theheavyline representstheobservededgesegment(markedbythe 
user)and the lighter line representsthe modeledgepredictedby the current camera and model parameters. 
Fig. 6(a) shows how a straight line in the model projects onto the image plane of a camera. The straight 
line can be de.ned by a pair of vectors hv;diwhere vrepresents the direction of the line and drepresents 
a point on the line. These vectors can be computed from equations 2 and 1 respectively. The position 
of the camera with respect to world coordinates is given in terms of a rotation matrix Rj and a translation 
vector tj. The normal vector denoted by min the .gure is computed from the following expression: m=Rj(v 
(d-tj)) (3) The projection of the line onto the image plane is simply the in­tersection of the plane 
de.ned by mwith the image plane, located at z=-fwhere fis the focal length of the camera. Thus, the image 
edge is de.ned by the equation mxx+myy-mzf=0. Fig. 6(b) shows how the error between the observed image 
edge f(x1;y1 );(x2 ;y2 )gand the predicted image line is calculated for each correspondence. Points on 
the observed edge segment can be parameterized by a single scalar variable s2[0;l]where lis the length 
of the edge. We let h(s)be the function that returns the short­est distance from a point on the segment, 
p(s), to the predicted edge. With these de.nitions, the total error between the observed edge segment 
and the predicted edge is calculated as: Z l 2 22 TT l Erri= h(s)ds=(h1+h1h2+h2)=m(ABA)m 3 0 (4) where: 
T m=(mx;my;mz) .. x 1 y1 1 A= x 2 y2 1 .. l 10:5 B= 22 3(mx+my)0:51 The .nal objective function Ois the 
sum of the error terms result­ing from each correspondence. We minimize Ousing a variant of the Newton-Raphson 
method, which involves calculating the gradi­ent and Hessian of Owith respect to the parameters of the 
camera and the model. As we have shown, it is simple to construct sym­bolic expressions for min terms 
of the unknown model parameters. The minimization algorithm differentiates these expressions sym­bolically 
to evaluate the gradient and Hessian after each iteration. The procedure is inexpensive since the expressions 
for dand vin Equations 2 and 1 have a particularly simple form.  2.4 Computing an Initial Estimate The 
objective function described in Section 2.3 section is non-linear with respect to the model and camera 
parameters and consequently can have local minima. If the algorithm begins at a random loca­tion in the 
parameter space, it stands little chance of converging to the correct solution. To overcome this problem 
we have developed a method to directly compute a good initial estimate for the model parameters and camera 
positions that is near the correct solution. In practice, our initial estimate method consistently enables 
the non­linear minimization algorithm to converge to the correct solution. Our initial estimate method 
consists of two procedures performed in sequence. The .rst procedure estimates the camera rotations while 
the second estimates the camera translations and the parame­ters of the model. Both initial estimate 
procedures are based upon an examination of Equation 3. From this equation the following con­straints 
can be deduced: T mRjv=0 (5) T mRj(d-tj)=0 (6) Given an observed edge uijthe measured normal m0 to the 
plane passing through the camera center is:  .!.! x 1 x2 m0 =y1 y2 (7) -f -f From these equations, 
we see that any model edges of known ori­entation constrain the possible values for Rj. Since most architec­tural 
models contain many such edges (e.g. horizontal and vertical lines), each camera rotation can be usually 
be estimated from the model independent of the model parameters and independent of the camera s location 
in space. Our method does this by minimizing the following objective function O1that sums the extents 
to which the rotations Rjviolate the constraints arising from Equation 5: X T2 O1=(mRjvi);vi2fx;y;z^ 
^^g (8) i Once initial estimates for the camera rotations are computed, Equation 6 is used to obtain 
initial estimates of the model param­eters and camera locations. Equation 6 re.ects the constraint that 
all of the points on the line de.ned by the tuple hv;dishould lie on the plane with normal vector mpassing 
through the camera center. This constraint is expressed in the following objective function O2 where 
Pi(X)and Qi(X)are expressionsfor the verticesof an edge of the model. X T2 T2 O2=(mRj(Pi(X)-tj))+(mRj(Qi(X)-tj))(9) 
In the special case where all of the block relations in the model have a known rotation, this objective 
function becomes a simple quadratic form which is easily minimized by solving a set of linear equations. 
Once the initial estimate is obtained, the non-linear minimization over the entire parameter space is 
applied to produce the best possi­ble reconstruction. Typically, the minimization requires fewer than 
ten iterations and adjusts the parameters of the model by at most a few percent from the initial estimates. 
The edges of the recovered models typically conform to the original photographs to within a pixel. Figure 
7: Threeoftwelve photographsusedto reconstructthe entire exterior of University High School in Urbana, 
Illinois. The super­imposed lines indicate the edges the user has marked.  (a) (b)  Figure 9: A synthetic 
view of University High School. This is a frame from an animation of .ying around the entire building. 
6 (a) (b) (c) Figure 10: Reconstructionof Hoover Tower, Stanford, CA (a) Origi­nal photograph, with marked 
edges indicated. (b) Model recovered from the single photograph shown in (a). (c) Texture-mappedaerial 
view from the virtual camera position indicated in (b). Regions not seen in (a) are indicated in blue. 
2.5 Results Fig. 2 showed the results of using Fac¸ade to reconstruct a clock tower from a single image. 
Figs. 7 and 8 show the results of us­ing Fac¸ade to reconstruct a high school building from twelve pho­tographs. 
(The model was originally constructed from just .ve im­ages; the remaining images were added to the project 
for purposesof generating renderings using the techniques of Section 3.) The pho­tographs were taken 
with a calibrated 35mm still camera with a stan­dard 50mm lens and digitized with the PhotoCD process. 
Images at the 15361024pixel resolution were processed to correct for lens distortion, then .ltered down 
to 768512pixels for use in the mod­eling system. Fig. 8 shows some views of the recovered model and camera 
positions, and Fig. 9 shows a synthetic view of the building generated by the technique in Sec. 3. Fig. 
10 shows the reconstruction of another tower from a sin­gle photograph. The dome was modeled specially 
since the recon­struction algorithm does not recover curved surfaces. The user con­strained a two-parameter 
hemisphere block to sit centered on top of the tower, and manually adjusted its height and width to align 
with the photograph. Each of the models presented took approximately four hours to create.  3 View-Dependent 
Texture-Mapping In this section we present view-dependent texture-mapping, an ef­fective method of rendering 
the scene that involves projecting the original photographs onto the model. This form of texture-mapping 
is most effective when the model conforms closely to the actual structure of the scene, and when the 
original photographs show the scene in similar lighting conditions. In Section 4 we will show how view-dependent 
texture-mapping can be used in conjunction with model-based stereo to produce realistic renderings when 
the recov­ered model only approximately models the structure of the scene. Since the camera positions 
of the original photographs are re­covered during the modeling phase, projecting the images onto the 
model is straightforward. In this section we .rst describe how we project a single image onto the model, 
and then how we merge sev­eral image projections to render the entire model. Unlike tradi­tional texture-mapping, 
our method projects different images onto the model depending on the user s viewpoint. As a result, our 
view­dependent texture mapping can give a better illusion of additional geometric detail in the model. 
3.1 Projecting a Single Image The process of texture-mapping a single image onto the model can be thought 
of as replacing each camera with a slide projector that projects the original image onto the model. When 
the model is not convex, it is possible that some parts of the model will shadow oth­ers with respect 
to the camera. While such shadowed regions could be determined using an object-space visible surface 
algorithm, or an image-space ray casting algorithm, we use an image-space shadow map algorithm based 
on [22] since it is ef.ciently implemented us­ing z-buffer hardware. Fig. 11, upper left, shows the results 
of mapping a single image onto the high school building model. The recovered camera posi­tion for the 
projected image is indicated in the lower left corner of the image. Because of self-shadowing, not every 
point on the model within the camera s viewing frustum is mapped.  3.2 Compositing Multiple Images In 
general, each photograph will view only a piece of the model. Thus, it is usually necessary to use multiple 
images in order to ren­der the entire model from a novel point of view. The top images of Fig. 11 show 
two different images mapped onto the model and ren­dered from a novel viewpoint. Some pixels are colored 
in just one of the renderings, while some are colored in both. These two render­ings can be merged into 
a composite rendering by considering the corresponding pixels in the rendered views. If a pixel is mapped 
in only one rendering, its value from that rendering is used in the com­posite. If it is mapped in more 
than one rendering, the renderer has to decide which image (or combination of images) to use. It would 
be convenient, of course, if the projected images would agree perfectly where they overlap. However, 
the images will not necessarily agree if there is unmodeled geometric detail in the build­ing, or if 
the surfaces of the building exhibit non-Lambertian re.ec­tion. In this case, the best image to use is 
clearly the one with the viewing angle closest to that of the rendered view. However, using the image 
closest in angle at every pixel means that neighboring ren­dered pixels may be sampled from different 
original images. When this happens, specularity and unmodeled geometric detail can cause visible seams 
in the rendering. To avoid this problem, we smooth these transitions through weighted averaging as in 
Fig. 12. Figure 11: The process of assembling projected images to form a composite rendering. The top 
two pictures show two images pro­jected onto the model from their respective recovered camera posi­tions. 
The lower left picture shows the results of compositing these two renderings using our view-dependent 
weighting function. The lower right picture shows the results of compositing renderings of all twelve 
original images. Some pixels near the front edge of the roof not seen in any image have been .lled in 
with the hole-.lling algorithm from [23]. Even with this weighting, neighboring pixels can still be sam­pled 
from different views at the boundary of a projected image, since the contribution of an image must be 
zero outside its boundary. To view 1 Figure 12: The weighting function used in view-dependent texture 
mapping. The pixel in the virtual view corresponding to the point on the model is assigned a weighted 
average of the corresponding pixels in actual views 1 and 2. The weights w1and w2are inversely inversely 
proportional to the magnitude of angles a1and a2.Al­ternately, more sophisticated weighting functions 
based on expected foreshortening and image resampling can be used. address this, the pixel weights are 
ramped down near the boundary of the projected images. Although this method does not guarantee smooth 
transitions in all cases, we have found that it eliminates most artifacts in renderings and animations 
arising from such seams. If an original photograph features an unwanted car, tourist, or other object 
in front of the architecture of interest, the unwanted ob­ject will be projected onto the surface of 
the model. To prevent this from happening, the user may mask out the object by painting over the obstruction 
with a reserved color. The rendering algorithm will then set the weights for any pixels corresponding 
to the masked re­gions to zero, and decrease the weights of the pixels near the bound­ary as before to 
minimize seams. Any regions in the composite im­age which are occluded in every projected image are .lled 
in using the hole-.lling method from [23]. In the discussion so far, projected image weights are computed 
at every pixel of every projected rendering. Since the weighting func­tion is smooth (though not constant) 
across .at surfaces, it is not generally not necessary to compute it for every pixel of every face of 
the model. For example, using a single weight for each face of the model, computed at the face s center, 
produces acceptable re­sults. By coarsely subdividing large faces, the results are visually indistinguishable 
from the case where a unique weight is computed for every pixel. Importantly, this technique suggests 
a real-time im­plementation of view-dependent texture mapping using a texture­mapping graphics pipeline 
to render the projected views, and .­channel blending to composite them. For complex models where most 
images are entirely occluded for the typical view, it can be very inef.cient to project every original 
photograph to the novel viewpoint. Some ef.cient techniques to de­termine such visibility apriori in 
architectural scenes through spa­tial partitioning are presented in [18].  4 Model-Based Stereopsis 
The modeling system described in Section 2 allows the user to cre­ate a basic model of a scene, but in 
general the scene will have ad­ditional geometric detail (such as friezes and cornices) not captured 
in the model. In this section we present a new method of recov­ering such additional geometric detail 
automatically through stereo correspondence, which we call model-based stereo. Model-based stereo differs 
from traditional stereo in that it measures how the ac­tual scene deviates from the approximate model, 
rather than trying to measure the structure of the scene without any prior information. The model serves 
to place the images into a common frame of ref­erence that makes the stereo correspondence possible even 
for im­ (a) (b) (c) (d)  Figure 13: View-dependent texture mapping. (a) A detail view of the high 
school model. (b) A renderingofthemodelfromthesameposi­tion using view-dependent texture mapping. Note 
that although the model does not capture the slightly recessed windows, the windows appear properly recessed 
because the texture map is sampled pri­marily from a photograph which viewed the windows from approx­imately 
the same direction. (c) The same piece of the model viewed from a different angle, using the same texture 
map as in (b). Since the texture is not selected from an image that viewed the model from approximately 
the same angle, the recessed windows appear unnat­ural. (d) A more natural result obtained by using view-dependent 
texture mapping. Since the angle of view in (d) is different than in (b), adifferentcompositionoforiginalimagesis 
usedto texture-map the model. ages taken from relatively far apart. The stereo correspondence in­formation 
can then be used to render novel views of the scene using image-based rendering techniques. As in traditional 
stereo, given two images (which we call the key and offset), model-based stereo computes the associated 
depth map for the key image by determining corresponding points in the key and offset images. Like many 
stereo algorithms, our method is correlation-based, in that it attempts to determine the corresponding 
point in the offset image by comparing small pixel neighborhoods around the points. As such, correlation-based 
stereo algorithms gen­erally require the neighborhood of each point in the key image to resemble the 
neighborhood of its corresponding point in the offset image. The problem we face is that when the key 
and offset images are taken from relatively far apart, as is the case for our modeling method, corresponding 
pixel neighborhoods can be foreshortened very differently. In Figs. 14(a) and (c), pixel neighborhoods 
toward the right of the key image are foreshortened horizontally by nearly a factor of four in the offset 
image. The key observation in model-based stereo is that even though two images of the same scene may 
appear very different, they ap­pear similar after being projected onto an approximate model of the scene. 
In particular, projecting the offset image onto the model and viewing it from the position of the key 
image produces what we call the warped offset image, which appears very similar to the key im­age. The 
geometrically detailed scene in Fig. 14 was modeled as two .at surfaces with our modeling program, which 
also determined the relative camera positions. As expected, the warped offset image (Fig. 14(b)) exhibits 
the same pattern of foreshortening as the key image. In model-based stereo, pixel neighborhoods are compared 
be­tween the key and warped offset images rather than the key and off­  (a) Key Image (b) Warped Offset 
Image (c) Offset Image (d) Computed Disparity Map set images. When a correspondence is found, it is simple 
to convert its disparity to the corresponding disparity between the key and off­set images, from which 
the point s depth is easily calculated. Fig. 14(d) shows a disparity map computed for the key image in 
(a). The reduction of differences in foreshortening is just one of sev­eral ways that the warped offset 
image simpli.es stereo correspon­dence. Some other desirable properties of the warped offset image are: 
 Any point in the scene which lies on the approximate model will have zero disparity between the key 
image and the warped offset image.  Disparities between the key and warped offset images are eas­ily 
converted to a depth map for the key image.  Depth estimates are far less sensitive to noise in image 
mea­surements since images taken from relatively far apart can be compared.  Places where the model 
occludes itself relative to the key im­age can be detected and indicated in the warped offset image. 
 A linear epipolar geometry (Sec. 4.1) exists between the key and warped offset images, despite the 
warping. In fact, the epipolar lines of the warped offset image coincide with the epipolar lines of the 
key image.  4.1 Model-Based Epipolar Geometry In traditional stereo, the epipolar constraint (see [6]) 
is often used to constrain the search for corresponding points in the offset im­age to searching along 
an epipolar line. This constraint simpli.es stereo not only by reducing the search for each correspondence 
to one dimension, but also by reducing the chance of selecting a false matches. In this section we show 
that taking advantage of the epipo­larconstraintis nomore dif.cultin model-basedstereocase,despite the 
fact that the offset image is non-uniformly warped. Fig. 15 shows the epipolar geometry for model-based 
stereo. If we consider a point Pin the scene, there is a unique epipolar plane which passes through Pand 
the centers of the key and offset cam­eras. This epipolar plane intersects the key and offset image planes 
in epipolar lines ekand eo. If we consider the projection pkof P onto the key image plane, the epipolar 
constraint states that the cor­responding point in the offset image must lie somewhere along the offset 
image s epipolar line. In model-based stereo, neighborhoods in the key image are com­pared to the warped 
offset image rather than the offset image. Thus, to make use of the epipolar constraint, it is necessary 
to determine where the pixels on the offset image s epipolar line project to in the warped offset image. 
The warped offset image is formed by project­ing the offset image onto the model, and then reprojecting 
the model onto the image plane of the key camera. Thus, the projection poof Pin the offset image projects 
onto the model at Q, and then repro­jects to qkin the warped offset image. Since each of these projec­tions 
occurs within the epipolar plane, any possible correspondence Key Offset Camera Camera for pkin the 
key image must lie on the key image s epipolar line in the warped offset image. In the case where the 
actual structure and the model coincide at P, pois projected to Pand then reprojected to pk, yielding 
a correspondence with zero disparity. The fact that the epipolar geometry remains linear after the warp­ing 
step also facilitates the use of the ordering constraint [2, 6] through a dynamic programming technique. 
 4.2 Stereo Results and Rerendering While the warping step makes it dramatically easier to determine 
stereo correspondences, a stereo algorithm is still necessary to ac­tually determine them. The algorithm 
we developed to produce the images in this paper is described in [3]. Once a depth map has been computed 
for a particular image, we can rerender the scene from novel viewpoints using the methods described in 
[23, 16, 13]. Furthermore, when several images and their corresponding depth maps are available, we can 
use the view­dependent texture-mapping method of Section 3 to composite the multiple renderings. The 
novel views of the chapel fac¸ade in Fig. 16 were produced through such compositing of four images. 
 5 Conclusion and Future Work To conclude, we have presented a new, photograph-based approach to modeling 
and rendering architectural scenes. Our modeling approach, which combines both geometry-based and image-based 
modeling techniques, is built from two components that we have developed. The .rst component is an easy-to-use 
photogrammet­ric modeling system which facilitates the recovery of a basic geo­metric model of the photographed 
scene. The second component is a model-based stereo algorithm, which recovers precisely how the real 
scene differs from the basic model. For rendering, we have pre­sented view-dependenttexture-mapping, 
which produces images by warping and compositing multiple views of the scene. Through ju­dicious use 
of images, models, and human assistance, our approach is more convenient, more accurate, and more photorealistic 
than current geometry-based or image-based approaches for modeling and rendering real-world architectural 
scenes. Figure 16: Novel views of the scene generated from four original photographs. These are frames 
from an animated movie in which the fac¸ade rotates continuously. The depth is computed from model-based 
stereo and the frames are made by compositing image-based renderings with view-dependent texture-mapping. 
There are several improvements and extensions that can be made to our approach. First, surfaces of revolution 
represent an important component of architecture (e.g. domes, columns, and minarets) that are not recovered 
in our photogrammetric modeling approach. (As noted, the dome in Fig. 10 was manually sized by the user.) 
Fortu­nately, there has been much work (e.g. [24]) that presents methods of recovering such structures 
from image contours. Curved model geometry is also entirely consistent with our approach to recovering 
additional detail with model-based stereo. Second, our techniques should be extended to recognize and 
model the photometric properties of the materials in the scene. The system should be able to make better 
use of photographs taken in varying lighting conditions, and it should be able to render images of the 
scene as it would appear at any time of day, in any weather, and with any con.guration of arti.cial light. 
Already, the recovered model can be used to predict shadowing in the scene with respect to an arbitrary 
light source. However, a full treatment of the problem will require estimating the photometric properties 
(i.e. the bidirec­tional re.ectance distribution functions) of the surfaces in the scene. Third, it is 
clear that further investigation should be made into the problem of selecting which original images to 
use when rendering a novel view of the scene. This problem is especially dif.cult when the available 
images are taken at arbitrary locations. Our current so­lution to this problem, the weighting function 
presented in Section 3, still allows seams to appear in renderings and does not consider issues arising 
from image resampling. Another form of view selec­tion is required to choose which pairs of images should 
be matched to recover depth in the model-based stereo algorithm. Lastly, it will clearly be an attractive 
application to integrate the models created with the techniques presented in this paper into forthcoming 
real-time image-based rendering systems.  Acknowledgments This research was supported by a National 
Science Foundation Graduate Research Fellowship and grants from Interval Research Corporation, the California 
MICRO program, and JSEP contract F49620-93-C-0014. The authors also wish to thank Tim Hawkins, Carlo 
Sequin, David Forsyth, and Jianbo Shi for their valuable help in revising this paper.  References [1] 
Ali Azarbayejani and Alex Pentland. Recursive estimation of motion, structure, and focal length. IEEE 
Trans. Pattern Anal. Machine Intell., 17(6):562 575,June 1995. [2] H. H. Baker and T. O. Binford. Depth 
from edge and intensity based stereo. In Proceedings of the Seventh IJCAI, Vancouver, BC, pages 631 636, 
1981. [3] Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. Modeling and rendering architecture 
from photographs: A hybrid geometry-and image-based approach. Technical Report UCB//CSD-96-893, U.C. 
Berkeley, CS Division, January 1996. [4] D.J.Fleet, A.D.Jepson, and M.R.M. Jenkin. Phase-based disparity 
measurement. CVGIP: Image Understanding, 53(2):198 210,1991. [5] Oliver Faugeras and Giorgio Toscani. 
The calibration problem for stereo. In Proceedings IEEE CVPR 86, pages 15 20, 1986. [6] Olivier Faugeras. 
Three-Dimensional Computer Vision. MIT Press, 1993. [7] Olivier Faugeras, Stephane Laveau, Luc Robert, 
Gabriella Csurka, and Cyril Zeller. 3-d reconstruction of urban scenes from sequences of images. Techni­cal 
Report 2572, INRIA, June 1995. [8] W.E.L. Grimson. From Images to Surface. MIT Press, 1981. [9] D. Jones 
and J. Malik. Computational framework for determining stereo cor­respondence from a set of linear spatial 
.lters. Image and Vision Computing, 10(10):699 708, December 1992. [10] E. Kruppa. Zur ermittlung eines 
objectes aus zwei perspektiven mit innerer ori­entierung. Sitz.-Ber. Akad. Wiss., Wien, Math. Naturw. 
Kl., Abt. Ila., 122:1939 1948, 1913. [11] H.C. Longuet-Higgins. A computer algorithm for reconstructing 
a scene from two projections. Nature, 293:133 135, September 1981. [12] D.MarrandT.Poggio.Acomputationaltheoryofhumanstereovision. 
Proceed­ings of the Royal Society of London, 204:301 328, 1979. [13] Leonard McMillan and Gary Bishop. 
Plenoptic modeling: An image-based ren­dering system. In SIGGRAPH 95, 1995. [14] Eric N. Mortensen and 
William A. Barrett. Intelligent scissors for image compo­sition. In SIGGRAPH 95, 1995. [15] S. B. Pollard, 
J. E. W. Mayhew, and J. P. Frisby. A stereo correspondence algo­rithm using a disparity gradient limit. 
Perception, 14:449 470, 1985. [16] R. Szeliski. Image mosaicing for tele-reality applications. In IEEE 
Computer Graphics and Applications, 1996. [17] Camillo J. Taylor and David J. Kriegman. Structure and 
motion from line seg­ments in multiple images. IEEE Trans. Pattern Anal. Machine Intell., 17(11), November 
1995. [18] S. J. Teller, Celeste Fowler, Thomas Funkhouser, and Pat Hanrahan. Partitioning and ordering 
large radiosity computations. In SIGGRAPH 94, pages 443 450, 1994. [19] Carlo Tomasi and Takeo Kanade. 
Shape and motion from image streams under orthography: a factorization method. International Journal 
of Computer Vision, 9(2):137 154, November 1992. [20] Roger Tsai. A versatile camera calibration technique 
for high accuracy 3d ma­chine vision metrology using off-the-shelf tv cameras and lenses. IEEE Journal 
of Robotics and Automation, 3(4):323 344, August 1987. [21] S. Ullman. The Interpretation of Visual Motion. 
The MIT Press, Cambridge, MA, 1979. [22] L Williams. Casting curved shadows on curved surfaces. In SIGGRAPH 
78, pages 270 274, 1978. [23] Lance Williams and Eric Chen. View interpolation for image synthesis. In 
SIG-GRAPH 93, 1993. [24] Mourad Zerroug and Ramakant Nevatia. Segmentation and recovery of shgcs from 
a real intensity image. In European Conference on Computer Vision, pages 319 330,1994.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237196</article_id>
		<sort_key>21</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[View morphing]]></title>
		<page_from>21</page_from>
		<page_to>30</page_to>
		<doi_number>10.1145/237170.237196</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237196</url>
		<keywords>
			<kw><![CDATA[image metamorphosis]]></kw>
			<kw><![CDATA[image warping]]></kw>
			<kw><![CDATA[morphing]]></kw>
			<kw><![CDATA[view interpolation]]></kw>
			<kw><![CDATA[view synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Interpolation formulas</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Geometric correction</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Registration</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP77027133</person_id>
				<author_profile_id><![CDATA[81407593498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Seitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Sciences, University of Wisconsin--Madison, 1210 W. Dayton St., Madison WI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39071353</person_id>
				<author_profile_id><![CDATA[81100123835]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Dyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Sciences, University of Wisconsin--Madison, 1210 W. Dayton St., Madison WI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BEIER, T., AND NEELY, S. Feature-based image metamorphosis. Proc. SIGGRAPH 92. In Computer Graphics (1992), pp. 35-42.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CHEN, S.E. Quicktime VR An image-based approach to virtual environment navigation. Proc. SIGGRAPH 95. In Computer Graphics (1995), pp. 29-38.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CHEN, S. E., AND WILLIAMS, L. View interpolation for image synthesis. Proc. SIGGRAPH 93. In Computer Graphics (1993), pp. 279-288.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[FAUGERAS, O. Three-Dimensional Computer Vision, A Geometric Viewpoint. MIT Press, Cambridge, MA, 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>840009</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[HARTLEY, R. I. In defence of the 8-point algorithm. In Proc. Fifth Intl. Conference on Computer Vision (1995), pp. 1064- 1070.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836232</ref_obj_id>
				<ref_obj_pid>832295</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[KUMAR, R., ANANDAN, P., IRANI, M., BERGEN, J., AND HANNA, K. Representation of scenes from collections of images. In Proc. IEEE Workshop on Representations of Visual Scenes (1995), pp. 10-17.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[LAVEAU, S., AND FAUGERAS, O. 3-D scene representation as a collection of images. In Proc. International Conference on Pattern Recognition (1994), pp. 689-691.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218501</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[LEE, S.-Y., CHWA, K.-Y., SHIN, S. Y., AND WOLBERG, G. Image metamorphosis using snakes and free-form deformations. Proc. SIGGRAPH 92. In Computer Graphics (1992), pp. 439-448.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[LUONG, Q.-T., AND FAUGERAS, O. The fundamental matrix: Theory, algorithms, and stability analysis. Intl. Journal of Computer Vision 17, 1 (1996), 43-75.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[MCMILLAN, L., AND BISHOP, G. Plenoptic modeling. Proc. SIGGRAPH 95. In Computer Graphics (1995), pp. 39-46.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[POGGIO, T., AND BEYMER, D. Learning networks for face analysis and synthesis. In Proc. Intl. Workshop on Automatic Face- and Gesture-Recognition (Zurich, 1995), pp. 160-165.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836230</ref_obj_id>
				<ref_obj_pid>832295</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[SEITZ, S. M., AND DYER, C. R. Physically-valid view synthesis by image interpolation. In Proc. IEEE Workshop on Representations of Visual Scenes (1995), pp. 18-25.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325242</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[SHOEMAKE, K. Animating rotation with quaternion curves. Proc. SIGGRAPH 85. In Computer Graphics (1985), pp. 245- 254.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1437353</ref_obj_id>
				<ref_obj_pid>1435699</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[SZELISKI, R. Video mosaics for virtual environments. IEEE Computer Graphics and Applications 16, 2 (1996), 22-30.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[WOLBERG, G. Digital Image Warping. IEEE Computer Society Press, Los Alamitos, CA, 1990.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 View Morphing Steven M. Seitz Charles R. Dyer Department of Computer Sciences 1 University of Wisconsin 
Madison ABSTRACT Image morphing techniques can generate compelling 2D transitions between images. However, 
differences in object pose or viewpoint often cause unnatural distortions in image morphs that are dif.cult 
to correct manually. Using basic principles of projective geometry, this paper introduces a simple extension 
to image morphing that cor­rectly handles 3D projective camera and scene transformations. The technique, 
called view morphing, works by prewarping two images prior to computing a morph and then postwarping 
the interpolated images. Because no knowledge of 3D shape is required, the tech­nique may be applied 
to photographs and drawings, as well as ren­dered scenes. The ability to synthesize changes both in viewpoint 
and image structure affords a wide variety of interesting 3D effects via simple image transformations. 
CR Categories and Subject Descriptors: I.3.3 [Computer Graph­ics]: Picture/Image Generation viewing algorithms; 
I.3.7 [Com­puter Graphics]: Three-Dimensional Graphics and Realism ani­mation; I.4.3 [Image Processing]: 
Enhancement geometric correc­tion, registration. Additional Keywords: Morphing, image metamorphosis, 
view in­terpolation, view synthesis, image warping. 1 INTRODUCTION Recently there has been a great deal 
of interest in morphing tech­niques for producing smooth transitions between images. These techniques 
combine 2D interpolations of shape and color to create dramatic special effects. Part of the appeal of 
morphing is that the images produced can appear strikingly lifelike and visually convinc­ing. Despite 
being computed by 2D image transformations, effec­tive morphs can suggest a natural transformation between 
objects in the 3D world. The fact that realistic 3D shape transformations can arise from 2D image morphs 
is rather surprising, but extremely useful, in that 3D shape modeling can be avoided. Although current 
techniques enable the creation of effective im­age transitions, they do not ensure that the resulting 
transitions ap­pear natural. It is entirely up to the user to evaluate a morph transi­ 11210 W. Dayton 
St., Madison WI 53706 Email: fseitz jdyerg@cs.wisc.edu Web: http://www.cs.wisc.edu/.dyer/vision.html 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use 
is granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 Virtual Cameras Morphed View Figure 1: View morphing between two 
images of an object taken from two different viewpoints produces the illusion of physically moving a 
virtual camera. tion and to design the interpolation to achieve the best results. Part of the problem 
is that existing image morphing methods do not ac­count for changes in viewpoint or object pose. As a 
result, sim­ple 3D transformations (e.g., translations, rotations) become sur­prisingly dif.cult to convey 
convincingly using existing methods. In this paper, we describe a simple extension called view morph­ing 
that allows current image morphing methods to easily synthe­size changes in viewpoint and other 3D effects. 
When morphing be­tween different views of an object or scene, the technique produces new views of the 
same scene, ensuring a realistic image transition. The effect can be described by what you would see 
if you physically moved the object (or the camera) between its con.gurations in the two images and .lmed 
the transition, as shown in Fig. 1. More gen­erally, the approach can synthesize 3D projective transformations 
of objects, a class including 3D rotations, translations, shears, and ta­pering deformations, by operating 
entirely on images (no 3D shape information is required). Because view morphing employs existing image 
morphing techniques as an intermediate step, it may also be used to interpolate between different views 
of different 3D objects, combining image morphing s capacity for dramatic shape transfor­mations with 
view morphing s ability to achieve changes in view­point. The result is a simultaneous interpolation 
of shape, color, and pose, giving rise to image transitions that appear strikingly 3D. View morphing 
works by prewarping two images, computing a morph (image warp and cross-dissolve) between the prewarped 
im­ages, and then postwarping each in-between image produced by the morph. The prewarping step is performed 
automatically, while the postwarping procedure may be interactively controlled by means of a small number 
of user-speci.ed control points. Any of several im­age morphing techniques, for instance [15, 1, 8], 
may be used to compute the intermediate image interpolation. View morphing does Figure 2: A Shape-Distorting 
Morph. Linearly interpolating two perspective views of a clock (far left and far right) causes a geometric 
bending effect in the in-between images. The dashed line shows the linear path of one feature during 
the course of the transformation. This example is indicative of the types of distortions that can arise 
with image morphing techniques.  not require knowledge of 3D shape, thereby allowing virtual ma­nipulations 
of unknown objects or scenes given only as drawings or photographs. In terms of its ability to achieve 
changes in viewpoint, view mor­phing is related to previous view-based techniques such as view syn­thesis 
[3, 7, 11, 12] and mosaics [10, 2, 14, 6]. However, this paper focuses on creating natural transitions 
between images rather than on synthesizing arbitrary views of an object or scene. This distinc­tion has 
a number of important consequences. First, in computing the transition between two perspective views, 
we are free to choose a natural camera path. By choosing this path along the line con­necting the two 
optical centers, we show that the formulation and implementation is greatly simpli.ed. Second, our approach 
is gen­eral in that it can be used to compute transitions between any two images, thereby encompassing 
both rigid and nonrigid transforma­tions. In contrast, previous view-based techniques have focused on 
rigid scenes. Finally, view morphing takes advantage of existing image morphing techniques, already in 
widespread use, for part of the computation. Existing image morphing tools may be easily ex­tended to 
produce view morphs by adding the image prewarping and postwarping steps described in this paper. The 
remainder of this paper is structured as follows: In Section 2 we review image morphing and argue that 
existing techniques may produce unnatural results when morphing between images of the same or similar 
shapes. Section 3 describes how to convert image morphing techniques into view morphing techniques by 
adding pre­warping and postwarping steps. Section 4 extends the method to en­able interpolations between 
views of arbitrary projective transfor­mations of the same 3D object. In addition, interactive techniques 
for controlling the image transformations are introduced. We con­clude with some examples in Section 
5.  2 IMAGE MORPHING Image morphing, or metamorphosis, is a popular class of techniques for producing 
transitions between images. There are a variety of morphing methods in the literature, all based on interpolating 
the po­sitions and colors of pixels in two images. At present, there appears to be no universal criterion 
for evaluating the quality or realism of a morph, let alone of a morphing method. A natural question 
to ask, however, is does the method preserve 3D shape. That is, does a morph between two different views 
of an object produce new views of the same object? Our investigation indicates that unless special care 
is taken, morphing between images of similar 3D shapes of­ten results in shapes that are mathematically 
quite different, leading to surprisingly complex and unnatural image transitions. These ob­servations 
motivate view morphing, introduced in the next section, which preserves 3D shape under interpolation. 
We write vectors and matrices in bold face and scalars in roman. Scene and image quantities are written 
in capitals and lowercase re­spectively. When possible, we also write corresponding image and scene quantities 
using the same letter. Images, I, and 3D shapes or scenes, S, are expressed as point sets. For example, 
an image point (x;y)p2Iis the projection of a scene point (X;Y;Z)P2 S. A morph is determined from two 
images I0and I1and maps C0:I0Iand C1:I1Ispecifying a complete correspon­dence between points in the two 
images. Two maps are required be­cause the correspondence may not be one-to-one. In practice, C0 and 
C1are partially speci.ed by having the user provide a sparse set of matching features or regions in the 
two images. The remain­ing correspondences are determined automatically by interpolation [15, 1, 8]. 
A warp function for each image is computed from the correspondence maps, usually based on linear interpolation: 
10 W0(p0;s) (1-s)p0+sC0(p0)(1) W1(p1;s) (1-s)C1(p1)+sp(2) 1 W0and W1give the displacement of each point 
p02I0and p12I1as a function of s2[0;1]. The in-between images Is are computed by warping the two original 
images and averaging the pixel colors of the warped images. Existing morphing methods vary principally 
in how the correspondence maps are computed. In ad­dition, some techniques allow .ner control over interpolation 
rates and methods. For instance, Beier et al. [1] suggested two different methods of interpolating line 
features, using linear interpolation of endpoints, per Eqs. (1) and (2), or of position and angle. In 
this pa­per, the term image morphing refers speci.cally to methods that use linear interpolation to compute 
feature positions in in-between im­ages, including [15, 1, 8]. To illustrate the potentially severe 3D 
distortions incurred by im­age morphing, it is useful to consider interpolating between two dif­ferent 
views of a planar shape. Any two such images are related by a 2D projective mapping of the form: ax+by+cdx+ey+f 
H(x;y)( ;) gx+hy+igx+hy+i Projective mappings are not preserved under 2D linear interpola­tion since 
the sum of two such expressions is in general a ratio of quadratics and therefore not a projective mapping. 
Consequently, morphing is a shape-distorting transformation, as in-between im­ages may not correspond 
to new views of the same shape. A partic­ularly disturbing effect of image morphing is its tendency to 
bend straight lines, yielding quite unintuitive image transitions. Fig. 2 shows a Dali-esque morph between 
two views of a clock in which it appears to bend in half and then straighten out again during the course 
of the transition. The in-between shapes were computed by linearly interpolating points in the two views 
that correspond to the same point on the clock. 3 VIEW MORPHING In the previous section we argued that 
unless special care is taken, image interpolations do not convey 3D rigid shape transformations. We say 
that an image transformation is shape-preserving if from two images of a particular object, it produces 
a new image representing a view of the same object. In this section we describe an interpolation­based 
image morphing procedure that is shape-preserving. Morphs generated by this technique create the illusion 
that the object moves rigidly (rotating and translating in 3D) between its positions in the two images. 
Computing the morph requires the following: (1) two images I0and I1, representing views of the same 3D 
object or scene, (2) their respective projection matrices I0and I1, and (3) a corre­spondence between 
pixels in the two images. Note that no a pri­ori knowledge of 3D shape information is needed. The requirement 
that projection matrices be known differentiates this technique from previous morphing methods. However, 
there exist a variety of tech­niques for obtaining the projection matrices from the images them­selves 
and knowledge of either the internal camera parameters or the 3D positions of a small number of image 
points. For an overview of both types of techniques, consult [4]. In Section 4 we introduce a variant 
that does not require knowledge of the projection matrices and also allows interpolations between views 
of different 3D objects or scenes. The pixel correspondences are derived by a combination of user­interaction 
and automatic interpolation provided by existing mor­phing techniques. When the correspondence is correct, 
the meth­ods described in this section guarantee shape-preserving morphs. In practice, we have found 
that an approximate correspondence is often suf.cient to produce transitions that are visually convincing. 
Major errors in correspondence may result in visible artifacts such as ghosting and shape distortions. 
Some examples of these ef­fects are shown in Section 5. Other errors may occur as a result of changes 
in visibility. In order to completely infer the appearance of a surface from a new viewpoint, that surface 
must be visible in both I0and I1. Changes in visibility may result in folds or holes,as dis­cussed in 
Section 3.4. Following convention, we represent image and scene quanti­ties using homogeneous coordinates: 
a scene point with Euclidean coordinates (X;Y;Z)is expressed by the column vector P [XYZ1]Tand a Euclidean 
image point (x;y)by p[xy1]T . We reserve the notation Pand pfor points expressed in Euclidean coordinates, 
i.e., whose last coordinate is 1. Scalar multiples of ~ these points will be written with a tilde, as 
Pand p~. A camera is represented by a 3x4homogeneous projection matrix of the form I[Hj-HC]. The vector 
Cgives the Euclidean position of the camera s optical center and the 3x3matrix Hspeci.es the po­sition 
and orientation of its image plane with respect to the world coordinate system. The perspective projection 
equation is p~IP (3) The term view will henceforth refer to the tuple hI;Iicomprised of an image and 
its associated projection matrix. 3.1 Parallel Views We begin by considering situations in which linear 
interpolation of images is shape-preserving. Suppose we take a photograph I0of an object, move the object 
in a direction parallel to the image plane of the camera, zoom out, and take a second picture I1, as 
shown P Figure 3: Morphing Parallel Views. Linear interpolation of cor­responding pixels in parallel 
views with image planes I0and I1 creates image I0:5, representing another parallel view of the same scene. 
in Fig. 3. Alternatively, we could produce the same two images by moving the camera instead of the object. 
Chen and Williams [3] pre­viously considered this special case, arguing that linear image in­terpolation 
should produce new perspective views when the camera moves parallel to the image plane. Indeed, suppose 
that the camera is moved from the world origin to position (CX;CY;0)and the fo­cal length changes from 
f0to f1. We write the respective projection matrices, I0and I1,as: "# f 0 000 . 0 0f0 00 0010 "# f 1 
00-f1CX . 1 0f1 0-f1CY 0010 We refer to cameras or views with projection matrices in this form as parallel 
cameras or parallel views, respectively. Let p02Iand 0 p12Ibe projections of a scene point P[XYZ1]T. 
Linear 1 interpolation of p0and p1yields 11 (1-s)p0+sp1 (1-s)I0P+sI1P ZZ 1 IsP (4) Z where Is(1-s)I0+sI(5) 
1 Image interpolation therefore produces a new view whose projec­tion matrix, Is, is a linear interpolation 
of I0and I1, representing a camera with center Csand focal length fsgiven by: Cs(sCX;sCY;0) (6) fs(1-s)f0+sf1 
(7) Consequently, interpolating images produced from parallel cameras produces the illusion of simultaneously 
moving the camera on the line C0C1between the two optical centers and zooming continu­ously. Because 
the image interpolation produces new views of the same object, it is shape-preserving. In fact, the above 
derivation relies only on the equality of the third rows of I0and I1. Views satisfying this more general 
cri­terion represent a broader class of parallel views for which linear image interpolation is shape 
preserving. An interesting special case is the class of orthographic projections, i.e., projections I0and 
I1 whose last row is [0001]. Linear interpolation of any two or­thographic views of a scene therefore 
produces a new orthographic view of the same scene. 3.2 Non-Parallel Views In this section we describe 
how to generate a sequence of in-between views from two non-parallel perspective images of the same 3D 
ob­ject or scene. For convenience, we choose to model the transforma­tion as a change in viewpoint, as 
opposed to a rotation and transla­tion of the object or scene. The only tools used are image reprojec­tion 
and linear interpolation, both of which may be performed using ef.cient scanline methods. 3.2.1 Image 
Reprojection Any two views that share the same optical center are related by a planar projective transformation. 
Let Iand I^be two images with ^^^ projection matrices I[Hj-HC]and I[Hj-HC]. ~^ The projections p~2Iand 
p^2Iof any scene point Pare related by the following transformation: ^^ HH .1 p~HH .1IP ^ IP ~ p^ ^ The 
3x3matrix HH .1is a projective transformation that repro­jects the image plane of Ionto that of I^ . 
More generally, any in­vertible 3x3matrix represents a planar projective transformation, a one-to-one 
map of the plane that transforms points to points and lines to lines. The operation of reprojection is 
very powerful be­cause it allows the gaze direction to be modi.ed after a photograph is taken, or a scene 
rendered. Our use of projective transforms to compute reprojections takes advantage of an ef.cient scanline 
al­gorithm [15]. Reprojection can also be performed through texture­mapping and can therefore exploit 
current graphics hardware. Image reprojection has been used previously in a number of ap­plications [15]. 
Our use of reprojection is most closely related to the techniques used for rectifying stereo views to 
simplify 3D shape re­construction [4]. Image mosaic techniques [10, 2, 14, 6] also rely heavily on reprojection 
methods to project images onto a planar, cylindrical, or spherical manifold. In the next section we describe 
how reprojection may be used to improve image morphs. 3.2.2 A Three Step Algorithm Using reprojection, 
the problem of computing a shape-preserving morph from two non-parallel perspective views can be reduced 
to the case treated in Section 3.1. To this end, let I0and I1be two perspective views with projection 
matrices I0 [Hj-HC] 000 and I1 [Hj-HC]. It is convenient to choose the world 111 coordinate system so 
that both C0and C1lie on the world X-axis, i.e., C0 [X000]Tand C1 [X100]T . The two remaining axes should 
be chosen in a way that reduces the distortion incurred by image reprojection. A simple choice that works 
well in practice is to choose the Yaxis in the direction of the cross product of the two image plane 
normals. P ^ .0 .1 .1 Figure 4: View Morphing in Three Steps. (1) Original images I0 and I1are prewarped 
to form parallel views I^ 0and I1.(2) I^ sis ^ produced by morphing (interpolating) the prewarped images. 
(3) I^ s is postwarped to form Is. In between perspective views on the line C0C1may be synthe­sized by 
a combination of image reprojections and interpolations, depicted in Fig. 4. Given a projection matrix 
Is[Hsj-HsCs], with Cs.xed by Eq. (6), the following sequence of operations pro­duces an image Iscorresponding 
to a view with projection matrix Is: 1. Prewarp: apply projective transforms H .01to I01and H .1to I1, 
producing prewarped images I^ 0and I^ 1 2. Morph: form I^ sby linearly interpolating positions and col­ 
 ors of corresponding points in I^ 0and I^ 1, using Eq. (4) or any image morphing technique that approximates 
it 3. Postwarp: apply Hsto I^ s, yielding image Is  Prewarping brings the image planes into alignment 
without chang­ing the optical centers of the two cameras. Morphing the prewarped images moves the optical 
center to Cs. Postwarping transforms the image plane of the new view to its desired position and orientation. 
Notice that the prewarped images I^ 0and I^ 1represent views with ^^ projection matrices I0 [Ij-C0]and 
I1 [Ij-C1], where Iis the 3x3identity matrix. Due to the special form of these projection matrices, I^ 
0and I^ 1have the property that corresponding points in the two images appear in the same scanline. Therefore, 
the interpolation I^ smay be computed one scanline at a time using only 1D warping and resampling operations. 
The prewarping and postwarping operations, combined with the intermediate morph, require multiple image 
resampling operations that may contribute to a noticeable blurring in the in-between im­ages. Resampling 
effects can be reduced by supersampling the input images [15] or by composing the image transformations 
into one ag­gregate warp for each image. The latter approach is especially com­patible with image morphing 
techniques that employ inverse map­ping, such as the Beier and Neely method [1], since the inverse post­warp, 
morph, and prewarp can be directly concatenated into a sin­gle inverse map. Composing the warps has disadvantages 
however, Figure 5: Singular Views. In the parallel con.guration (top), each camera s optical center is 
out of the .eld of view of the other. A sin­gular con.guration (bottom) arises when the optical center 
of cam­era Bis in the .eld of view of camera A. Because prewarping does not change the .eld of view, 
singular views cannot be reprojected to form parallel views. including loss of both the scanline property 
and the ability to use off-the-shelf image morphing tools to compute the intermediate in­terpolation. 
 3.3 Singular View Con.gurations Certain con.gurations of views cannot be made parallel through re­projection 
operations. For parallel cameras, (Fig. 5, top) the optical center of neither camera is within the .eld 
of view of the other. Note that reprojection does not change a camera s .eld of view, only its viewing 
direction. Therefore any pair of views for which the optical center of one camera is within the .eld 
of view of the other cannot be made parallel through prewarping1. Fig. 5 (bottom) depicts such apairof 
singular views, for which the prewarping procedure fails. Singular con.gurations arise when the camera 
motion is roughly parallel to the viewing direction, a condition detectable from the im­ages themselves 
(see the Appendix). Singular views are not a prob­lem when the prewarp, morph, and postwarp are composed 
into a single aggregate warp, since prewarped images are never explicitly constructed. With aggregate 
warps, view morphing may be applied to arbitrary pairs of views, including singular views. 3.4 Changes 
in Visibility So far, we have described how to correct for distortions in image morphs by manipulating 
the projection equations. Eq. (3), however, does not model the effects that changes in visibility have 
on image content. From the standpoint of morphing, changes in visibility re­sult in two types of conditions: 
folds and holes. A fold occurs in an in-between image Iswhen a visible surface in I0(or I1) becomes 1Prewarping 
is possible if the images are .rst cropped to exclude the epipoles (see the Appendix). occluded in Is. 
In this situation, multiple pixels of I0maptothe same point in Is, causing an ambiguity. The opposite 
case, of an occluded surface suddenly becoming visible, gives rise to a hole; a region of Ishaving no 
correspondence in I0. Folds can be resolved using Z-buffer techniques [3], provided depth information 
is available. In the absence of 3D shape informa­tion, we use point disparity instead. The disparity 
of corresponding points p0and p1in two parallel views is de.ned to be the differ­ence of their x-coordinates. 
For parallel views, point disparity is in­versely proportional to depth so that Z-buffer techniques may 
be di­rectly applied, with inverse disparity substituted for depth. Because our technique makes images 
parallel prior to interpolation, this sim­ple strategy suf.ces in general. Furthermore, since the interpolation 
is computed one scanline at a time, Z-buffering may be performed at the scanline level, thereby avoiding 
the large memory requirements commonly associated with Z-buffering algorithms. An alternative method 
using a Painter s method instead of Z-buffering is presented in [10]. Unlike folds, holes cannot always 
be eliminated using image in­formation alone. Chen and Williams [3] suggested different meth­ods for 
.lling holes, using a designated background color, interpola­tion with neighboring pixels, or additional 
images for better surface coverage. The neighborhood interpolation approach is prevalent in existing 
image morphing methods and was used implicitly in our ex­periments. 3.4.1 Producing the Morph Producing 
a shape-preserving morph between two images requires choosing a sequence of projection matrices Is[Hsj-HsCs], 
beginning with I0and ending with I1.Since Csis determined by Eq. (6), this task reduces to choosing Hsfor 
each value of s2 (0;1), specifying a continuous transformation of the image plane from the .rst view 
to the second. There are many ways to specify this transformation. A natural one is to interpolate the 
orientations of the image planes by a single axis rotation. If the image plane normals are denoted by 
3D unit vectors N0and N1,the axis Dand angle of rotation (are given by DN0xN1 (cos .1(N0·N1) Alternatively, 
if the orientations are expressed using quaternions, the interpolation is computed by spherical linear 
interpolation [13]. In either case, camera parameters such as focal length and aspect ra­tio should be 
interpolated separately.  4 PROJECTIVE TRANSFORMATIONS By generalizing what we mean by a view , the 
technique described in the previous section can be extended to accommodate a range of 3D shape deformations. 
In particular, view morphing can be used to interpolate between images of different 3D projective transforma­tions 
of the same object, generating new images of the same object, projectively transformed. The advantage 
of using view morphing in this context is that salient features such as lines and conics are preserved 
during the course of the transformation from the .rst im­age to the second. In contrast, straightforward 
image morphing can cause severe geometric distortions, as seen in Fig. 2. As described in Section 3.1, 
a 2D projective transformation may be expressed as a 3x3homogeneous matrix transformation. Sim­ilarly, 
a 3D projective transformation is given by a 4x4matrix T. This class of transformations encompasses 3D 
rotations, trans­lations, scales, shears, and tapering deformations. Applying Tto a ~ homogeneous scene 
point produces the point QTP.The cor­responding point Qin 3D Euclidean coordinates is obtained by di­ 
~ viding Qby its fourth component. 3D projective transformations are notable in that they may be absorbed 
by the camera transfor­mation. Speci.cally, consider rendering an image of a scene that has been transformed 
by a 3D projective transformation T.If the projection matrix is given by I, a point Pin the scene appears 
at position pin the image, where p~I(TP).If we de.ne the 3x4 ~ matrix IIT, the combined transformation 
may be expressed ~ as a single projection, representing a view with projection matrix I. By allowing 
arbitrary 3x4projections, we can model the changes in shape induced by projective transformations by 
changes in view­point. In doing so, the problem of interpolating images of projective transformations 
of an unknown shape is reduced to a form to which the three-step algorithm of Section 3.2.2 may be applied. 
However, recall that the three-step algorithm requires that the camera view­points be known. In order 
to morph between two different faces, this would require apriori knowledge of the 3D projective transforma­tion 
that best relates them. Since this knowledge may be dif.cult to obtain, we describe here a modi.cation 
that doesn t require know­ing the projection matrices. Suppose we wish to smoothly interpolate two images 
I0and I1of objects related by a 3D projective transformation. Suppose further that only the images themselves 
and pixel correspondences are provided. In order to ensure that in-between images depict the same 3D 
shape (projectively transformed), I0and I1must .rst be transformed so as to represent parallel views. 
As explained in Sec­ tion 3.2.2, the transformed images, I^ 0and I^ 1, have the property that corresponding 
points appear in the same scanline of each image, i.e., ^^ two points p02I0and p12I1are projections of 
the same scene point only if their y-coordinates are equal. In fact, this condition is suf.cient to ensure 
that two views are parallel. Consequently I0and I1may be made parallel by .nding any pair of 2D projective 
trans­formations H0and H1that send corresponding points to the same scanline. One approach for determining 
H0and H1using 8 or more image point correspondences is given in the Appendix. 4.1 Controlling the Morph 
To fully determine a view morph, Hsmust be provided for each in­between image. Rather than specifying 
the 3x3matrix explicitly, it is convenient to provide Hsindirectly by establishing constraints on the 
in-between images. A simple yet powerful way of doing this is to interactively specify the paths of four 
image points through the entire morph transition. These control points can represent the posi­tions of 
four point features, the endpoints of two lines, or the bound­ing quadrilateral of an arbitrary image 
region2. Fig. 6 illustrates the process: .rst, four control points bounding a quadrilateral region of 
I^ 0:5are selected, determining corresponding quadrilaterals in I0 and I1. Second, the control points 
are interactively moved to their desired positions in I0:5, implicitly specifying the postwarp trans­formation 
and thus determining the entire image I0:5. The post­warps of other in-between images are then determined 
by interpo­lating the control points. The positions of the control points in Is ^ and Isspecify a linear 
system of equations whose solution yields Hs[15]. The four curves traced out by the control points may 
also be manually edited for .ner control of the interpolation parameters. The use of image control points 
bears resemblance to the view synthesis work of Laveau and Faugeras [7], who used .ve pairs of corresponding 
image points to specify projection parameters. How­ever, in their case, the points represented the projection 
of a new im­age plane and optical center and were speci.ed only in the original 2Care should be taken 
to ensure that no three of the control points are colinear in any image. images. In our approach, the 
control points are speci.ed in the in­between image(s), providing more direct control over image appear­ance. 
 4.2 View Morphing Without Prewarping Prewarping is less effective for morphs between different objects 
not closely related by a 3D projective transform. With objects that are considerably different, it is 
advisable to leave out the prewarp entirely, since its automatic computation becomes less stable [9]. 
The postwarp step should not be omitted, however, since it can be used to reduce image plane distortions 
for more natural morphs. For instance, a large change in orientation results in a noticeable 2D im­age 
contraction, as seen in Fig. 10. Prewarping is not strictly necessary for images that are approx­imately 
orthographic, as noted in Section 3.1. Images taken with a telephoto lens often fall into this category, 
as do images of objects whose variation in depth is small relative to their distance from the camera. 
In either case, the images may be morphed directly, yield­ing new orthographic views. However, the prewarping 
step does in­.uence the camera motion which, in the orthographic case, cannot be controlled solely by 
postwarping. The camera transformation de­termined by Eq. (5) may introduce unnatural skews and twists 
of the image plane due to the fact that linear matrix interpolation does not preserve row orthogonality. 
Prewarping the images ensures that the view plane undergoes a single axis rotation. More details on the 
or­thographic case are given in [12].  5 RESULTS Fig. 6 illustrates the view morphing procedure applied 
to two images of a bus. We manually selected a set of about 50 corresponding line features in the two 
images. These features were used to automat­ically prewarp the images to achieve parallelism using the 
method described in the Appendix. Inspection of the prewarped images con­.rms that corresponding features 
do in fact occupy the same scan­lines. An implementation of the Beier-Neely .eld-morphing algo­rithm 
[1] was used to compute the intermediate images, based on the same set of features used to prewarp the 
images. The resulting im­ages were postwarped by selecting a quadrilateral region delimited by four control 
points in ^and moving the control points to their I0:5desired positions in I0:5. The .nal positions of 
the control points for the image in the center of Fig. 6 were computed automatically by roughly calibrating 
the two images based on their known focal lengths and interpolating the changes in orientation [4]. Different 
images obtained by other settings of the control points are shown in Fig. 8. As these images indicate, 
a broad range of 3D projective effects may be achieved through the postwarping procedure. For in­stance, 
the rectangular shape of the bus can be skewed in different directions and tapered to depict different 
3D shapes. Fig. 7 shows some results on interpolating human faces in vary­ing poses. The .rst example 
shows selected frames from a morph computed by interpolating views of the same person facing in two different 
directions. The resulting animation depicts the subject continuously turning his head from right to left. 
Because the sub­ject s right ear is visible in only one of the original images, it appears ghosted in 
intermediate frames due to the interpolation of inten­sity values. In addition, the subject s nose appears 
slightly distorted as a result of similar changes in visibility. The second sequence shows a morph between 
different views of two different faces.In­terpolating different faces is one of the most popular applications 
of image morphing. Here, we combine image morphing s capac­ity for dramatic facial interpolations with 
view morphing s ability to achieve changes in viewpoint. The result is a simultaneous inter­polation 
of facial structure, color, and pose, giving rise to an image transition conveying a metamorphosis that 
appears strikingly 3D. When an object has bilateral symmetry, view morphs can be com­puted from a single 
image. Fig. 9 depicts a view morph between an image of Leonardo da Vinci s Mona Lisa and its mirror re.ection. 
Although the two sides of the face and torso are not perfectly sym­metric, the morph conveys a convincing 
facial rotation. Fig. 10 compares image morphing with view morphing using two ray-traced images of a 
helicopter toy. The image morph was com­puted by linearly interpolating corresponding pixels in the two 
orig­inal images. The change in orientation between the original images caused the in-between images 
to contract. In addition, the bending effects seen in Fig. 2 are also present. Image morphing techniques 
such as [1] that preserve lines can reduce bending effects, but only when line features are present. 
An interesting side-effect is that a large hole appears in the image morph, between the stick and pro­peller, 
but not in the view morph, since the eye-level is constant throughout the transition. To be sure, view 
morphs may also pro­duce holes, but only as a result of a change in visibility. 6 CONCLUSIONS Achieving 
realistic image transitions is possible but often dif.cult with existing image morphing techniques due 
to the lack of avail­able 3D information. In this paper, we demonstrated how to ac­curately convey a 
range of 3D transformations based on a simple yet powerful extension to the image morphing paradigm called 
view morphing. In addition to changes in viewpoint, view morphing ac­commodates changes in projective 
shape. By integrating these ca­pabilities with those already afforded by existing image morphing methods, 
view morphing enables transitions between images of dif­ferent objects that give a strong sense of metamorphosis 
in 3D. Be­cause no knowledge of 3D shape is required, the technique may be applied to photographs and 
drawings, as well as to arti.cially ren­dered scenes. Two different methods for controlling the image 
tran­sition were described, using either automatic interpolation of cam­era parameters or interactive 
user-manipulation of image control points, based on whether or not the camera viewpoints are known. Because 
view morphing relies exclusively on image information, it is sensitive to changes in visibility. In our 
experiments, the best morphs resulted when visibility was nearly constant, i.e., most sur­faces were 
visible in both images. The visible effects of occlu­sions may often be minimized by experimenting with 
different fea­ture correspondences. Additional user input could be used to re­duce ghosting effects by 
specifying the paths of image regions visi­ble in only one of the original images. A topic of future 
work will be to investigate ways of extending view morphing to handle extreme changes in visibility, 
enabling 180 or 360 degree rotations in depth.  ACKNOWLEDGEMENTS The authors would like to thank Paul 
Heckbert for providing the im­age morphing code used to interpolate prewarped images in this pa­per. 
The original helicopter images in Fig. 10 were rendered using Rayshade with a Z-buffer output extension 
written by Mark Mai­mone. The support of the National Science Foundation under Grant Nos. IRI 9220782 
and CDA 9222948 is gratefully acknowledged. REFERENCES [1] BEIER,T., AND NEELY, S. Feature-based image 
metamor­phosis. Proc. SIGGRAPH 92. In Computer Graphics (1992), pp. 35 42. [2] CHEN, S. E. Quicktime 
VR An image-based approach to virtual environment navigation. Proc. SIGGRAPH 95. In Computer Graphics 
(1995), pp. 29 38. [3] CHEN,S. E., AND WILLIAMS, L. View interpolation for im­age synthesis. Proc. SIGGRAPH 
93. In Computer Graphics (1993), pp. 279 288. [4] FAUGERAS,O. Three-Dimensional Computer Vision, A Geo­metric 
Viewpoint. MIT Press, Cambridge, MA, 1993. [5] HARTLEY, R. I. In defence of the 8-point algorithm. In 
Proc. Fifth Intl. Conference on Computer Vision (1995), pp. 1064 1070. [6] KUMAR,R., ANANDAN,P., IRANI,M., 
BERGEN,J., AND HANNA, K. Representation of scenes from collections of im­ages. In Proc. IEEE Workshop 
on Representations of Visual Scenes (1995), pp. 10 17. [7] LAVEAU, S., AND FAUGERAS, O. 3-D scene representation 
as a collection of images. In Proc. International Conference on Pattern Recognition (1994), pp. 689 691. 
[8] LEE, S.-Y., CHWA,K.-Y., SHIN,S.Y., AND WOLBERG, G. Image metamorphosis using snakes and free-form 
defor­mations. Proc. SIGGRAPH 92. In Computer Graphics (1992), pp. 439 448. [9] LUONG,Q.-T., AND FAUGERAS, 
O. The fundamental ma­trix: Theory, algorithms, and stability analysis. Intl. Journal of Computer Vision 
17, 1 (1996), 43 75. [10] MCMILLAN,L., AND BISHOP, G. Plenoptic modeling. Proc. SIGGRAPH 95. In Computer 
Graphics (1995), pp. 39 46. [11] POGGIO,T., AND BEYMER, D. Learning networks for face analysis and synthesis. 
In Proc. Intl. Workshop on Automatic Face-and Gesture-Recognition (Zurich, 1995), pp. 160 165. [12] SEITZ,S. 
M., AND DYER, C. R. Physically-valid view syn­thesis by image interpolation. In Proc. IEEE Workshop on 
Representations of Visual Scenes (1995), pp. 18 25. [13] SHOEMAKE, K. Animating rotation with quaternion 
curves. Proc. SIGGRAPH 85. In Computer Graphics (1985), pp. 245 254. [14] SZELISKI, R. Video mosaics 
for virtual environments. IEEE Computer Graphics and Applications 16, 2 (1996), 22 30. [15] WOLBERG,G. 
Digital Image Warping. IEEE Computer So­ciety Press, Los Alamitos, CA, 1990.  APPENDIX This appendix 
describes how to automatically compute the image prewarping transforms H0and H1from the images themselves. 
We assume that the 2D positions of 8 or more corresponding points are given in each image. The fundamental 
matrix of two views is de­.ned to be the 3x3, rank-two matrix Fsuch that for every pair of corresponding 
image points p02Iand p12I, 01 T p1Fp0 0 Fis de.ned up to a scale factor and can be computed from 8 or 
more such points using linear [5] or non-linear [9] methods. A suf.cient condition for two views to be 
parallel is that their fun­ damental matrix have the form: " # 0 0 0 ^F 0 0 -1 (8) 0 1 0   ^ ^ ^ 
I0 I0.5 I1  II I0 0.5 1 Figure 6: View Morphing Procedure: A set of features (yellow lines) is selected 
in original images I0and I1. Using these features, the images ^ I0 I1. The prewarped images are morphed 
to create a sequence of in-between images, the middle is interactively postwarped by selecting a quadrilateral 
region (marked red) and specifying its ^ are automatically prewarped to produceand I0:5, is shown at 
top-center.desired con.guration, Q0:5,in I0:5. The postwarps for other in-between images are determined 
by interpolating the quadrilaterals (bottom). ^ ^ of which, I0:5  I 0 I0:25 I0:5 I0:75 I1 Figure 7: 
Facial View Morphs. Top: morph between two views of the same person. Bottom: morph between views of two 
different people. In each case, view morphing captures the change in facial pose between original images 
I0and I1, conveying a natural 3D rotation. Figure 8: Postwarping deformations obtained by different 
settings of the control quadrilateral. Figure 9: Mona Lisa View Morph. Morphed view (center) is halfway 
between original image (left) and it s re.ection (right).  Consequently, any two images with fundamental 
matrix Fmay be prewarped (i.e., made parallel) by choosing any two projective ^ transforms H0and H1such 
that (H .1) T FH .1 F.Here we 10 describe one method that applies a rotation in depth to make the im­ages 
planes parallel, followed by an af.ne transformation to align corresponding scanlines. The procedure 
is determined by choos­ [dxyT ing an (arbitrary) axis of rotation d00d00]2I0.Given [xyz]TFd0, the corresponding 
axis in I1is determined ac­cording to d1 [-yx0]T . To compute the angles of depth rota­ tion we need 
the epipoles, also known as vanishing points, e02I 0 xyzTxyzT and e12I1. e0 [e0e0e0]and e1 [e1e1e1]are 
the unit eigenvectors of Fand FTrespectively, corresponding to eigenval­ues of 0. A view s epipole represents 
the projection of the optical center of the other view. The following procedure will work pro­vided the 
views are not singular, i.e., the epipoles are outside the image borders and therefore not within the 
.eld of view. The an­gles of rotation in depth about diare given by yxy dx (i--tan(iizi)2 ei 7.1de-ie 
We denote as Rdeii the 3x3matrix corresponding to a rotation of angle (iabout axis di. Applying Rde00to 
I0and Rde11to I1makes the two image planes parallel. Although this is technically suf.cient for prewarping, 
it is useful to add an additional af.ne warp to align the scanlines. This simpli.es the morph step to 
a scanline interpo­lation and also avoids bottleneck problems that arise as a result of image plane rotations 
[15]. The next step is to rotate the images so that epipolar lines are hor­ xy izontal. The new epipoles 
are [e~e~0]TRdeiiei. The angles of ii.1yx rotation <0and <1are given by <i-tan(e~ i e~ i). After ap­plying 
these image plane rotations, the fundamental matrix has the form "# 0 0 0 ~F R,1Rd1 e1 FRd0 .e0 R.,0 
0 0 a 0 b c The 3x3matrix Redenotes an image plane (zaxis) rotation of angle (. Finally, to get Finto 
the form of Eq. (8), the second image is translated and vertically scaled by the matrix "# 100 T0-a-c 
00b In summary, the prewarping transforms H0and H1are H 0 R,0Rde00 H 1 TR,1Rde11 The entire procedure 
is determined by selecting d0. A suitable yxT choice is to select d0orthogonal to e0, i.e., d0 [-e0e00]. 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237199</article_id>
		<sort_key>31</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Light field rendering]]></title>
		<page_from>31</page_from>
		<page_to>42</page_to>
		<doi_number>10.1145/237170.237199</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237199</url>
		<keywords>
			<kw><![CDATA[epipolar analysis]]></kw>
			<kw><![CDATA[holographic stereogram]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[light field]]></kw>
			<kw><![CDATA[vector quantization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor>Approximate methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University, Gates Computer Science Building 3B, Stanford University Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University, Gates Computer Science Building 3B, Stanford University Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adelson, E.H., Bergen, J.R., "The Plenoptic Function and the Elements of Early Vision," In Computation Models of Visual Processing, M. Landy and J.A. Movshon, eds., MIT Press, Cambridge, 1991.]]></ref_text>
				<ref_id>Adelson91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ashdown, I., "Near-Field Photometry: A New Approach," Journal of the Illuminating Engineering Society, Vol. 22, No. 1, Winter, 1993, pp. 163-180.]]></ref_text>
				<ref_id>Ashdown93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237276</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Beers, A., Agrawala, M., Chaddha, N., "Rendering from Compressed Textures." In these proceedings.]]></ref_text>
				<ref_id>Beers96</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Benton, S., "Survey of Holographic Stereograms," Processing and Display of Three-Dimensional Data, Proc. SPIE, Vol. 367, 1983.]]></ref_text>
				<ref_id>Benton83</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Blinn, J.F., Newell, M.E., "Texture and Reflection in Computer Generated Images," CACM, Vol. 19, No. 10, October, 1976, pp. 542-547.]]></ref_text>
				<ref_id>Blinn76</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bolles, R., Baker, H., Marimont, D., "Epipolar-Plane Image Analysis: An Approach to Determining Structure from Motion," International Journal of Computer Vision, Vol. 1, No. 1, 1987, pp. 7-55.]]></ref_text>
				<ref_id>Bolles87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chen, S.E., Williams, L., "View Interpolation for Image Synthesis," Proc. SIGGRAPH '93 (Anaheim, California, August 1-6, 1993). In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 279-288.]]></ref_text>
				<ref_id>Chen93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Chen, S.E., "QuickTime VR- An Image-Based Approach to Virtual Environment Navigation," Proc. SIGGRAPH '95 (Los Angeles, CA, August 6-11, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH, pp. 29-38.]]></ref_text>
				<ref_id>Chen95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Bishop, G., Arthur, K., McMillan, L., Bajcsy, R., Lee, S.W., Farid, H., Kanade, T., "Virtual Space Teleconferencing Using a Sea of Cameras," Proc. First International Conference on Medical Robotics and Computer Assisted Surgery, 1994, pp. 161-167.]]></ref_text>
				<ref_id>Fuchs94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>128857</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gersho, A., Gray, R.M., Vector Quantization and Signal Compression, Kluwer Academic Publishers, 1992.]]></ref_text>
				<ref_id>Gersho92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Gershun, A., "The Light Field," Moscow, 1936. Translated by E Moon and G. Timoshenko in Journal of Mathematics and Physics, Vol. XVIII, MIT, 1939, pp. 51-151.]]></ref_text>
				<ref_id>Gershun36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gortler, S.J., Grzeszczuk, R., Szeliski, R., Cohen, M., "The Lumigraph." In these proceedings.]]></ref_text>
				<ref_id>Gortler96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13023</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Greene, N., "Environment Mapping and Other Applications of World Projections," IEEE Computer Graphics and Applications, Vol. 6, No. 11, November, 1986, pp. 21-29.]]></ref_text>
				<ref_id>Greene86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Greene, N. and Kass, M., "Approximating Visibility with Environment Maps," Apple Technical Report No. 41, November, 1994.]]></ref_text>
				<ref_id>Greene94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Halle, M., "Holographic Stereograms as Discrete Imaging Systems." Practical Holography, Proc. SPIE, Vol. 2176, February, 1994.]]></ref_text>
				<ref_id>Halle94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Katayama, A., Tanaka, K., Oshino, T., Tamura, H., "Viewpoint-Dependent Stereoscopic Display Using Interpolation of Multiviewpoint Images," Stereoscopic Displays and Virtual Reality Systems H, Proc. SPIE, Vol. 2409, S. Fisher, J. Merritt, B. Bolas eds. 1995, pp. 11-20.]]></ref_text>
				<ref_id>Katayama95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Laveau, S., Faugeras, O.D., "3-D Scene Representation as a Collection of Images and Fundamental Matrices," INRIA Technical Report No. 2205, 1994.]]></ref_text>
				<ref_id>Laveau94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Levin, R., "Photometric Characteristics of Light Controlling Apparatus," Illuminating Engineering, Vol. 66, No. 4, 1971, pp. 205-215.]]></ref_text>
				<ref_id>Levin71</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[McMillan, L., Bishop, G., "Head-Tracked Stereoscopic Display Using Image Warping," Stereoscopic Displays and Virtual Reality Systems II, Proc. SPIE, Vol. 2409, S. Fisher, J. Merritt, B. Bolas eds. 1995, pp. 21-30.]]></ref_text>
				<ref_id>McMillan95a</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[McMillan, L., Bishop, G., Plenoptic Modeling: An Image-Based Rendering System, Proc. SIGGRAPH '95 (Los Angeles, CA, August 6-11, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH, pp. 39-46.]]></ref_text>
				<ref_id>McMillan95b</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Miller, G., "Volumetric Hyper-Reality: A Computer Graphics Holy Grail for the 21 st Century?," Proc. Graphics Interface '95, W. Davis and E Prusinkiewicz eds., Canadian Information Processing Society, 1995, pp. 56-64.]]></ref_text>
				<ref_id>Miller95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Moon, E, Spencer, D.E., The Photic Field, MIT Press, 1981.]]></ref_text>
				<ref_id>Moon81</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836234</ref_obj_id>
				<ref_obj_pid>832295</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Narayanan, EJ., "Virtualized Reality: Concepts and Early Results," Proc. IEEE Workshop on the Representation of Visual Scenes, IEEE, 1995.]]></ref_text>
				<ref_id>Narayanan95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Nimeroff, J., Simoncelli, E., Dorsey, J., "Efficient Rerendering of Naturally Illuminated Scenes," Proc. Fifth Eurographics Rendering Workshop, 1994, pp. 359-373.]]></ref_text>
				<ref_id>Nimeroff94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Sbert, A.M., "An Integral Geometry Based Method for Form- Factor Computation," Computer Graphics Forum, Vol. 13, No. 3, 1993, pp. 409-420.]]></ref_text>
				<ref_id>Sbert93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836230</ref_obj_id>
				<ref_obj_pid>832295</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Seitz, S., Dyer, C., "Physically-Valid View Synthesis by Image Interpolation," Proc. IEEE Workshop on the Representation of Visual Scenes, IEEE, 1995.]]></ref_text>
				<ref_id>Seitz95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Williams, L., "Pyramidal Parametrics," Computer Graphics (Proc. Siggraph '83), Vol. 17, No. 3, July, 1983, pp. 1-11.]]></ref_text>
				<ref_id>Williams83</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Ziv, J., Lempel, A., "A universal algorithm for sequential data compression," IEEE Transactions on Information Theory, IT-23:337-343, 1977.]]></ref_text>
				<ref_id>Ziv77</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Light Field Rendering Marc Levoy and Pat Hanrahan Computer Science Department Stanford University Abstract 
A number of techniques have been proposed for .ying through scenes by redisplaying previously rendered 
or digitized views. Techniques have also been proposed for interpolating between views by warping input 
images, using depth information or correspondences between multiple images. In this paper, we describe 
a simple and robust method for generating new views from arbitrary camera positions without depth information 
or fea­ture matching, simply by combining and resampling the available images. The key to this technique 
lies in interpreting the input images as 2D slices of a 4D function - the light .eld. This func­tion 
completely characterizes the .ow of light through unob­structed space in a static scene with .xed illumination. 
We describe a sampled representation for light .elds that allows for both ef.cient creation and display 
of inward and out­ward looking views. We have created light .elds from large arrays of both rendered 
and digitized images. The latter are acquired using a video camera mounted on a computer-controlled gantry. 
Once a light .eld has been created, new views may be constructed in real time by extracting slices in 
appropriate direc­tions. Since the success of the method depends on having a high sample rate, we describe 
a compression system that is able to compress the light .elds we have generated by more than a factor 
of 100:1 with very little loss of .delity. We also address the issues of antialiasing during creation, 
and resampling during slice extrac­tion. CR Categories: I.3.2 [Computer Graphics]: Picture/Image Gener­ation 
 Digitizing and scanning, Viewing algorithms; I.4.2 [Com­puter Graphics]: Compression Approximate methods 
Additional keywords: image-based rendering, light .eld, holo­graphic stereogram, vector quantization, 
epipolar analysis 1. Introduction Traditionally the input to a 3D graphics system is a scene consisting 
of geometric primitives composed of different materials and a set of lights. Based on this input speci.cation, 
the rendering system computes and outputs an image. Recently a new approach to rendering has emerged: 
image-based rendering. Image-based rendering systems generate different views of an environment from 
a set of pre-acquired imagery. There are several advantages to this approach: Address: Gates Computer 
Science Building 3B levoy@cs.stanford.edu Stanford University hanrahan@cs.stanford.edu Stanford, CA 94305 
http://www-graphics.stanford.edu Permission to make digital or hard copies of part or all of this work 
or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 The display algorithms 
for image-based rendering require modest computational resources and are thus suitable for real­time 
implementation on workstations and personal computers.  The cost of interactively viewing the scene 
is independent of scene complexity.  The source of the pre-acquired images can be from a real or virtual 
environment, i.e. from digitized photographs or from rendered models. In fact, the two can be mixed together. 
  The forerunner to these techniques is the use of environ­ment maps to capture the incoming light in 
a texture map [Blinn76, Greene86]. An environment map records the incident light arriving from all directions 
at a point. The original use of environment maps was to ef.ciently approximate re.ections of the environment 
on a surface. However, environment maps also may be used to quickly display any outward looking view 
of the environment from a .xed location but at a variable orientation. This is the basis of the Apple 
QuickTimeVR system [Chen95]. In this system environment maps are created at key locations in the scene. 
The user is able to navigate discretely from location to location, and while at each location continuously 
change the view­ing direction. The major limitation of rendering systems based on envi­ronment maps is 
that the viewpoint is .xed. One way to relax this .xed position constraint is to use view interpolation 
[Chen93, Greene94, Fuchs94, McMillan95a, McMillan95b, Narayanan95]. Most of these methods require a depth 
value for each pixel in the environment map, which is easily provided if the environment maps are synthetic 
images. Given the depth value it is possible to reproject points in the environment map from different 
vantage points to warp between multiple images. The key challenge in this warping approach is to ".ll 
in the gaps" when previously occluded areas become visible. Another approach to interpolating between 
acquired images is to .nd corresponding points in the two [Laveau94, McMillan95b, Seitz95]. If the positions 
of the cameras are known, this is equivalent to .nding the depth values of the corre­sponding points. 
Automatically .nding correspondences between pairs of images is the classic problem of stereo vision, 
and unfor­tunately although many algorithms exist, these algorithms are fairly fragile and may not always 
.nd the correct correspon­dences. In this paper we propose a new technique that is robust and allows 
much more freedom in the range of possible views. The major idea behind the technique is a representation 
of the light .eld, the radiance as a function of position and direction, in regions of space free of 
occluders (free space). In free space, the light .eld is a 4D, not a 5D function. An image is a two dimen­sional 
slice of the 4D light .eld. Creating a light .eld from a set of images corresponds to inserting each 
2D slice into the 4D light .eld representation. Similarly, generating new views corresponds to extracting 
and resampling a slice. Generating a new image from a light .eld is quite different than previous view 
interpolation approaches. First, the new image is generally formed from many different pieces of the 
original input images, and need not look like any of them. Second, no model information, such as depth 
values or image correspon­dences, is needed to extract the image values. Third, image gener­ation involves 
only resampling, a simple linear process. This representation of the light .eld is similar to the epipo­lar 
volumes used in computer vision [Bolles87] and to horizontal­parallax-only holographic stereograms [Benton83]. 
An epipolar volume is formed from an array of images created by translating a camera in equal increments 
in a single direction. Such a represen­tation has recently been used to perform view interpolation [Katayama95]. 
A holographic stereogram is formed by exposing a piece of .lm to an array of images captured by a camera 
moving sideways. Halle has discussed how to set the camera aperture to properly acquire images for holographic 
stereograms [Halle94], and that theory is applicable to this work. Gavin Miller has also recognized the 
potential synergy between true 3D display tech­nologies and computer graphics algorithms [Miller95]. 
There are several major challenges to using the light .eld approach to view 3D scenes on a graphics workstation. 
First, there is the choice of parameterization and representation of the light .eld. Related to this 
is the choice of sampling pattern for the .eld. Second, there is the issue of how to generate or acquire 
the light .eld. Third, there is the problem of fast generation of differ­ent views. This requires that 
the slice representing rays through a point be easily extracted, and that the slice be properly resampled 
to avoid artifacts in the .nal image. Fourth, the obvious disadvan­tage of this approach is the large 
amount of data that may be required. Intuitively one suspects that the light .eld is coherent and that 
it may be compressed greatly. In the remaining sections we discuss these issues and our proposed solutions. 
 2. Representation We de.ne the light .eld as the radiance at a point in a given direction. Note that 
our de.nition is equivalent to the plenoptic function introduced by Adelson and Bergen [Adel­son91]. 
The phrase light .eld was coined by A. Gershun in his classic paper describing the radiometric properties 
of light in a space [Gershun36]. 1 McMillan and Bishop [McMillan95b] dis­cuss the representation of 5D 
light .elds as a set of panoramic images at different 3D locations. However, the 5D representation may 
be reduced to 4D in free space (regions free of occluders). This is a consequence of the fact that the 
radiance does not change along a line unless blocked. 4D light .elds may be interpreted as functions 
on the space of oriented lines. The redundancy of the 5D representation is undesirable for two reasons: 
.rst, redundancy increases the size of the total dataset, and second, redundancy complicates the reconstruction 
of the radiance function from its samples. This reduction in dimension has been used to simplify the 
representa­tion of radiance emitted by luminaires [Levin71, Ashdown93]. For the remainder of this paper 
we will be only concerned with 4D light .elds. 1 For those familiar with Gershun s paper, he actually 
uses the term light .eld to mean the irradiance vector as a function of position. For this reason P. 
Moon in a lat­er book [Moon81] uses the term photic .eld to denote what we call the light .eld. Although 
restricting the validity of the representation to free space may seem like a limitation, there are two 
common situ­ations where this assumption is useful. First, most geometric models are bounded. In this 
case free space is the region outside the convex hull of the object, and hence all views of an object 
from outside its convex hull may be generated from a 4D light .eld. Second, if we are moving through 
an architectural model or an outdoor scene we are usually moving through a region of free space; therefore, 
any view from inside this region, of objects out­side the region, may be generated. The major issue in 
choosing a representation of the 4D light .eld is how to parameterize the space of oriented lines. There 
are several issues in choosing the parameterization: Ef.cient calculation. The computation of the position 
of a line from its parameters should be fast. More importantly, for the purposes of calculating new views, 
it should be easy to compute the line parameters given the viewing transformation and a pixel location. 
Control over the set of lines. The space of all lines is in.nite, but only a .nite subset of line space 
is ever needed. For exam­ple, in the case of viewing an object we need only lines inter­secting the convex 
hull of the object. Thus, there should be an intuitive connection between the actual lines in 3-space 
and line parameters. Uniform sampling. Given equally spaced samples in line parameter space, the pattern 
of lines in 3-space should also be uniform. In this sense, a uniform sampling pattern is one where the 
number of lines in intervals between samples is constant everywhere. Note that the correct measure for 
number of lines is related to the form factor kernel [Sbert93]. The solution we propose is to parameterize 
lines by their intersections with two planes in arbitrary position (see .gure 1). By convention, the 
coordinate system on the .rst plane is (u, v) and on the second plane is (s, t). An oriented line is 
de.ned by connecting a point on the uv plane to a point on the st plane. In practice we restrict u, v, 
s, and t to lie between 0 and 1, and thus points on each plane are restricted to lie within a convex 
quadrilat­eral. We call this representation a light slab. Intuitively, a light slab represents the beam 
of light entering one quadrilateral and exiting another quadrilateral. A nice feature of this representation 
is that one of the planes may be placed at in.nity. This is convenient since then lines may be parameterized 
by a point and a direction. The latter will prove useful for constructing light .elds either from ortho­graphic 
images or images with a .xed .eld of view. Furthermore, if all calculations are performed using homogeneous 
coordinates, the two cases may be handled at no additional cost. t v L(u,v,s,t)  us Figure 1: The 
light slab representation. y . . x r Figure 2: De.nition of the line space we use to visualize sets 
of light rays. Each oriented line in Cartesian space (at left) is represented in line space (at right) 
by a point. To simplify the visualizations, we show only lines in 2D; the extension to 3D is straightforward. 
 Figure 3: Using line space to visualize ray coverage. (a) shows a single light slab. Light rays (drawn 
in gray) connect points on two de.ning lines (drawn in red and green). (c) shows an arrangement of four 
rotated copies of (a). (b) and (d) show the corresponding line space visualizations. For any set of lines 
in Cartesian space, the envelope formed by the correspond­ing points in line space indicates our coverage 
of position and direction; ideally the coverage should be complete in . and as wide as possible in r. 
As these .gures show, the single slab in (a) does not provide full coverage in . , but the four-slab 
arrangement in (c) does. (c) is, however, narrow in r. Such an arrangement is suitable for inward-looking 
views of a small object placed at the origin. It was used to generate the lion light .eld in .gure 14d. 
A big advantage of this representation is the ef.ciency of geometric calculations. Mapping from (u, v) 
to points on the plane is a projective map and involves only linear algebra (multiplying by a 3x3 matrix). 
More importantly, as will be discussed in sec­tion 5, the inverse mapping from an image pixel (x, y)to 
(u, v, s, t) is also a projective map. Methods using spherical or cylindrical coordinates require substantially 
more computation.  Figure 4: Using line space to visualize sampling uniformity. (a) shows a light slab 
de.ned by two lines at right angles. (c) shows a light slab where one de.ning line is at in.nity. This 
arrangement generates rays passing through the other de.ning line with an angle between -45° and +45°. 
(b) and (d) show the corresponding line space visualizations. Our use of (r, . ) to parameterize line 
space has the property that equal areas in line space correspond to equally dense sampling of position 
and orientation in Carte­sian space; ideally the density of points in line space should be uniform. As 
these .gures show, the singularity at the corner in (a) leads to a highly nonuniform and therefore inef.cient 
sampling pattern, indicated by dark areas in (b) at angles of 0 and -p /2. (c) generates a more uniform 
set of lines. Although (c) does not provide full coverage of . , four rotated copies do. Such an arrangement 
is suitable for outward-looking views by an observer standing near the origin. It was used to generate 
the hallway light .eld in .gure 14c. Many properties of light .elds are easier to understand in line 
space (.gures 2 through 4). In line space, each oriented line is represented by a point and each set 
of lines by a region. In par­ticular, the set of lines represented by a light slab and the set of lines 
intersecting the convex hull of an object are both regions in line space. All views of an object could 
be generated from one light slab if its set of lines include all lines intersecting the convex hull of 
the object. Unfortunately, this is not possible. Therefore, it takes multiple light slabs to represent 
all possible views of an object. We therefore tile line space with a collection of light slabs, as shown 
in .gure 3. An important issue related to the parameterization is the sampling pattern. Assuming that 
all views are equally likely to be generated, then any line is equally likely to be needed. Thus all 
regions of line space should have an equal density of samples. Figure 4 shows the density of samples 
in line space for different arrangements of slabs. Note that no slab arrangement is perfect: arrangements 
with a singularity such as two polygons joined at a corner (4a) are bad and should be avoided, whereas 
slabs formed from parallel planes (3a) generate fairly uniform patterns. In addi­tion, arrangements where 
one plane is at in.nity (4c) are better than those with two .nite planes (3a). Finally, because of symme­try 
the spacing of samples in uv should in general be the same as st. However, if the observer is likely 
to stand near the uv plane, then it may be acceptable to sample uv less frequently than st. 3. Creation 
of light .elds In this section we discuss the creation of both virtual light .elds (from rendered images) 
and real light .elds (from digitized images). One method to create a light .eld would be to choose a 
4D sampling pattern, and for each line sample, .nd the radiance. This is easily done directly for virtual 
environments by a ray tracer. This could also be done in a real environment with a spot radiometer, but 
it would be very tedious. A more practical way to generate light .elds is to assemble a collection of 
images. 3.1. From rendered images For a virtual environment, a light slab is easily generated simply 
by rendering a 2D array of images. Each image represents a slice of the 4D light slab at a .xed uv value 
and is formed by placing the center of projection of the virtual camera at the sample Field of view Focal 
plane (st) Camera plane (uv)  Figure 5: The viewing geometry used to create a light slab from an array 
of perspective images. location on the uv plane. The only issue is that the xy samples of each image 
must correspond exactly with the st samples. This is easily done by performing a sheared perspective 
projection (.gure 5) similar to that used to generate a stereo pair of images. Figure 6 shows the resulting 
4D light .eld, which can be visualized either as a uv array of st images or as an st array of uv images. 
 Figure 6: Two visualizations of a light .eld. (a) Each image in the array represents the rays arriving 
at one point on the uv plane from all points on the st plane, as shown at left. (b) Each image represents 
the rays leaving one point on the st plane bound for all points on the uv plane. The images in (a) are 
off­axis (i.e. sheared) perspective views of the scene, while the images in (b) look like re.ectance 
maps. The latter occurs because the object has been placed astride the focal plane, making sets of rays 
leaving points on the focal plane similar in character to sets of rays leaving points on the object. 
Two other viewing geometries are useful. A light slab may be formed from a 2D array of orthographic views. 
This can be modeled by placing the uv plane at in.nity, as shown in .gure 4c. In this case, each uv sample 
corresponds to the direction of a par­allel projection. Again, the only issue is to align the xy and 
st samples of the image with the st quadrilateral. The other useful geometry consists of a 2D array of 
outward looking (non-sheared) perspective views with .xed .eld of view. In this case, each image is a 
slice of the light slab with the st plane at in.nity. The fact that all these cases are equally easy 
to handle with light slabs attests to the elegance of projective geometry. Light .elds using each arrangement 
are presented in section 6 and illustrated in .g­ure 14. As with any sampling process, sampling a light 
.eld may lead to aliasing since typical light .elds contain high frequencies. Fortunately, the effects 
of aliasing may be alleviated by .ltering before sampling. In the case of a light .eld, a 4D .lter in 
the space of lines must be employed (see .gure 7). Assuming a box .lter, a weighted average of the radiances 
on all lines connecting sample squares in the uv and st planes must be computed. If a camera is placed 
on the uv plane and focussed on the st plane, then the .ltering process corresponds to integrating both 
over a pixel corresponding to an st sample, and an aperture equal in size to a uv sample, as shown in 
.gure 8. The theory behind this .lter­ing process has been discussed in the context of holographic stere­ograms 
by Halle [Halle94]. Note that although pre.ltering has the desired effect of antialiasing the light .eld, 
it has what at .rst seems like an unde­sirable side effect introducing blurriness due to depth of .eld. 
However, this blurriness is precisely correct for the situation. Recall what happens when creating a 
pair of images from two adjacent camera locations on the uv plane: a given object point will project 
to different locations, potentially several pixels apart, in these two images. The distance between the 
two projected locations is called the stereo disparity. Extending this idea to mul­tiple camera locations 
produces a sequence of images in which the object appears to jump by a distance equal to the disparity. 
This jumping is aliasing. Recall now that taking an image with a .nite aperture causes points out of 
focus to be blurred on the .lm plane by a circle of confusion. Setting the diameter of the aperture to 
the spacing between camera locations causes the circle of confu­sion for each object point to be equal 
in size to its stereo disparity. This replaces the jumping with a sequence of blurred images. Thus, we 
are removing aliasing by employing .nite depth of .eld. st uv Pixel filter + Aperture filter = Ray filter 
Figure 7: Pre.ltering a light .eld. To avoid aliasing, a 4D low pass .lter must be applied to the radiance 
function. Camera planeFocal plane (uv) (st) Figure 8: Pre.ltering using an aperture. This .gure shows 
a cam­era focused on the st plane with an aperture on the uv plane whose size is equal to the uv sample 
spacing. A hypothetical .lm plane is drawn behind the aperture. Ignore the aperture for a moment (con­sider 
a pinhole camera that precisely images the st plane onto the .lm plane). Then integrating over a pixel 
on the .lm plane is equivalent to integrating over an st region bounded by the pixel. Now consider .xing 
a point on the .lm plane while using a .nite sized aperture (recall that all rays from a point on the 
.lm through the aperture are focussed on a single point on the focal plane). Then integrating over the 
aperture corresponds to integrating all rays through the uv region bounded by the aperture. Therefore, 
by simultaneously integrating over both the pixel and the aperture, the proper 4D integral is computed. 
 The necessity for pre.ltering can also be understood in line space. Recall from our earlier discussion 
that samples of the light .eld correspond to points in line space. Having a .nite depth of .eld with 
an aperture equal in size to the uv sample spacing insures that each sample adequately covers the interval 
between these line space points. Too small or too large an aperture yields gaps or overlaps in line space 
coverage, resulting in views that are either aliased or excessively blurry, respectively. 3.2. From 
digitized images Digitizing the imagery required to build a light .eld of a physical scene is a formidable 
engineering problem. The number of images required is large (hundreds or thousands), so the process must 
be automated or at least computer-assisted. Moreover, the lighting must be controlled to insure a static 
light .eld, yet .exible enough to properly illuminate the scene, all the while staying clear of the camera 
to avoid unwanted shadows. Finally, real optical systems impose constraints on angle of view, focal distance, 
depth of .eld, and aperture, all of which must be managed. Similar issues have been faced in the construction 
of devices for perform­ing near-.eld photometric measurements of luminaires [Ash­down93]. In the following 
paragraphs, we enumerate the major design decisions we faced in this endeavor and the solutions we adopted. 
Inward versus outward looking. The .rst decision to be made was between a .yaround of a small object 
and a .ythrough of a large-scale scene. We judged .yarounds to be the simpler case, so we attacked them 
.rst.  Figure 9: Our prototype camera gantry. A modi.ed Cyberware MS motion platform with additional 
stepping motors from Lin-Tech and Parker provide four degrees of freedom: horizontal and vertical translation, 
pan, and tilt. The camera is a Panasonic WV-F300 3-CCD video camera with a Canon f/1.7 10-120mm zoom 
lens. We keep it locked off at its widest setting (10mm) and mounted so that the pitch and yaw axes pass 
through the center of projection. While digitizing, the camera is kept point­ed at the center of the 
focal plane. Calibrations and alignments are veri.ed with the aid of a Faro digitizing arm, which is 
accu­rate to 0.3 mm. Human versus computer-controlled. An inexpensive approach to digitizing light .elds 
is to move a handheld camera through the scene, populating the .eld from the resulting images [Gortler96]. 
This approach necessitates estimating camera pose at each frame and interpolating the light .eld from 
scattered data - two challenging problems. To simplify the situ­ation, we chose instead to build a computer-controlled 
camera gantry and to digitize images on a regular grid. Spherical versus planar camera motion. For .yarounds 
of small objects, an obvious gantry design consists of two concen­tric hemicycles, similar to a gyroscope 
mounting. The camera in such a gantry moves along a spherical surface, always point­ing at the center 
of the sphere. Apple Computer has con­structed such a gantry to acquire imagery for Quick-Time VR .yarounds 
[Chen95]. Unfortunately, the lighting in their sys­tem is attached to the moving camera, so it is unsuitable 
for acquiring static light .elds. In general, a spherical gantry has three advantages over a planar gantry: 
(a) it is easier to cover the entire range of viewing directions, (b) the sampling rate in direction 
space is more uniform, and (c) the distance between the camera and the object is .xed, providing sharper 
focus throughout the range of camera motion. A planar gantry has two advantages over a spherical gantry: 
(a) it is easier to build; the entire structure can be assembled from linear motion stages, and (b) it 
is closer to our light slab representation. For our .rst prototype gantry, we chose to build a planar 
gantry, as shown in .gure 9. Field of view. Our goal was to build a light .eld that allowed 360 degrees 
of azimuthal viewing. To accomplish this using a planar gantry meant acquiring four slabs each providing 
90 Figure 10: Object and lighting support. Objects are mounted on a Bogen .uid-head tripod, which we 
manually rotate to four orientations spaced 90 degrees apart. Illumination is provided by two 600W Lowell 
Omni spotlights attached to a ceiling­mounted rotating hub that is aligned with the rotation axis of 
the tripod. A stationary 6 x 6 diffuser panel is hung between the spotlights and the gantry, and the 
entire apparatus is enclosed in black velvet to eliminate stray light. degrees. This can be achieved 
with a camera that translates but does not pan or tilt by employing a wide-angle lens. This solu­tion 
has two disadvantages: (a) wide-angle lenses exhibit signif­icant distortion, which must be corrected 
after acquisition, and (b) this solution trades off angle of view against sensor resolu­tion. Another 
solution is to employ a view camera in which the sensor and optical system translate in parallel planes, 
the former moving faster than the latter. Horizontal parallax holographic stereograms are constructed 
using such a camera [Halle94]. Incorporating this solution into a gantry that moves both hori­zontally 
and vertically is dif.cult. We instead chose to equip our camera with pan and tilt motors, enabling us 
to use a nar­row-angle lens. The use of a rotating camera means that, in order to transfer the acquired 
image to the light slab representa­tion, it must be reprojected to lie on a common plane. This reprojection 
is equivalent to keystone correction in architectural photography. Standoff distance. A disadvantage 
of planar gantries is that the distance from the camera to the object changes as the camera translates 
across the plane, making it dif.cult to keep the object in focus. The view camera described above does 
not suffer from this problem, because the ratio of object distance to image distance stays constant as 
the camera translates. For a rotating camera, servo-controlled focusing is an option, but changing the 
focus of a camera shifts its center of projection and changes the image magni.cation, complicating acquisition. 
We instead mit­igate this problem by using strong lighting and a small aperture to maximize depth of 
.eld. Sensor rotation. Each sample in a light slab should ideally rep­resent the integral over a pixel, 
and these pixels should lie on a common focal plane. A view camera satis.es this constraint because its 
sensor translates in a plane. Our use of a rotating camera means that the focal plane also rotates. Assuming 
that we resample the images carefully during reprojection, the pres­ence of a rotated focal plane will 
introduce no additional error into the light .eld. In practice, we have not seen artifacts due to this 
resampling process. Aperture size. Each sample in a light slab should also represent the integral over 
an aperture equal in size to a uv sample. Our use of a small aperture produces a light .eld with little 
or no uv antialiasing. Even fully open, the apertures of commercial video cameras are small. We can approximate 
the required antialiasing by averaging together some number of adjacent views, thereby creating a synthetic 
aperture. Howev er, this technique requires a very dense spacing of views, which in turn requires rapid 
acquisition. We do not currently do this. Object support. In order to acquire a 360-degree light .eld 
in four 90-degree segments using a planar gantry, either the gantry or the object must be rotated to 
each of four orientations spaced 90 degrees apart. Given the massiveness of our gantry, the lat­ter was 
clearly easier. For these experiments, we mounted our objects on a tripod, which we manually rotate to 
the four posi­tions as shown in .gure 10. Lighting. Given our decision to rotate the object, satisfying 
the requirement for .xed illumination means that either the lighting must exhibit fourfold symmetry or 
it must rotate with the object. We chose the latter solution, attaching a lighting system to a rotating 
hub as shown in .gure 10. Designing a lighting system that stays clear of the gantry, yet provides enough 
light to evenly illuminate an object, is a challenging problem. Using this gantry, our procedure for 
acquiring a light .eld is as follows. For each of the four orientations, the camera is translated through 
a regular grid of camera positions. At each position, the camera is panned and tilted to point at the 
center of the object, which lies along the axis of rotation of the tripod. We then acquire an image, 
and, using standard texture mapping algo­rithms, reproject it to lie on a common plane as described earlier. 
Table II gives a typical set of acquisition parameters. Note that the distance between camera positions 
(3.125 cm) exceeds the diameter of the aperture (1.25 mm), underscoring the need for denser spacing and 
a synthetic aperture.  4. Compression Light .eld arrays are large the largest example in this paper 
is 1.6 GB. To make creation, transmission, and display of light .elds practical, they must be compressed. 
In choosing from among many available compression techniques, we were guided by several unique characteristics 
of light .elds: Data redundancy. A good compression technique removes redundancy from a signal without 
affecting its content. Light .elds exhibit redundancy in all four dimensions. For example, the smooth 
regions in .gure 6a tell us that this light .eld con­tains redundancy in s and t, and the smooth regions 
in .gure 6b tell us that the light .eld contains redundancy in u and v. The former corresponds to our 
usual notion of interpixel coherence in a perspective view. The latter can be interpreted either as the 
interframe coherence one expects in a motion sequence or as the smoothness one expects in the bidirectional 
re.ectance dis­tribution function (BRDF) for a diffuse or moderately specular surface. Occlusions introduce 
discontinuities in both cases, of course. Random access. Most compression techniques place some con­straint 
on random access to data. For example, variable-bitrate coders may require scanlines, tiles, or frames 
to be decoded at once. Examples in this class are variable-bitrate vector quanti­zation and the Huffman 
or arithmetic coders used in JPEG or MPEG. Predictive coding schemes further complicate random­access 
because pixels depend on previously decoded pixels, scanlines, or frames. This poses a problem for light 
.elds since the set of samples referenced when extracting an image from a light .eld are dispersed in 
memory. As the observer moves, the access patterns change in complex ways. We therefore seek a compression 
technique that supports low-cost random access to individual samples. Asymmetry. Applications of compression 
can be classi.ed as symmetric or asymmetric depending on the relative time spent encoding versus decoding. 
We assume that light .elds are assembled and compressed ahead of time, making this an asym­metric application. 
Computational expense. We seek a compression scheme that can be decoded without hardware assistance. 
Although soft­ware decoders have been demonstrated for standards like JPEG and MPEG, these implementations 
consume the full power of a modern microprocessor. In addition to decompression, the dis­play algorithm 
has additional work to perform, as will be described in section 5. We therefore seek a compression scheme 
that can be decoded quickly. The compression scheme we chose was a two-stage pipeline consisting of 
.xed-rate vector quantization followed by entropy coding (Lempel-Ziv), as shown in .gure 11. Following 
similar motivations, Beers et al. use vector quantization to com­press textures for use in rendering 
pipelines [Beers96]. 4.1. Vector quantization The .rst stage of our compression pipeline is vector quanti­zation 
(VQ) [Gersho92], a lossy compression technique wherein a vector of samples is quantized to one of a number 
of predeter­mined reproduction vectors. A reproduction vector is called a codeword, and the set of codewords 
available to encode a source is called the codebook, Codebooks are constructed during a train­ing phase 
in which the quantizer is asked to .nd a set of code­words that best approximates a set of sample vectors, 
called the training set. The quality of a codeword is typically characterized codebook LZ (0.8 MB) light 
field VQ  bitstream (402 MB) (3.4 MB) indices LZ (16.7 MB) Figure 11 Two-stage compression pipeline. 
The light .eld is parti­tioned into tiles, which are encoded using vector quantization to form an array 
of codebook indices. The codebook and the array of indices are further compressed using Lempel-Ziv coding. 
Decom­pression also occurs in two stages: entropy decoding as the .le is loaded into memory, and dequantization 
on demand during interac­tive viewing. Typical .le sizes are shown beside each stage. using mean-squared 
error (MSE), i.e. the sum over all samples in the vector of the squared difference between the source 
sample and the codeword sample. Once a codebook has been constructed, encoding consists of partitioning 
the source into vectors and .nd­ing for each vector the closest approximating codeword from the codebook. 
Decoding consists of looking up indices in the code­book and outputting the codewords found there a 
very fast operation. Indeed, decoding speed is one of the primary advan­tages of vector quantization. 
In our application, we typically use 2D or 4D tiles of the light .eld, yielding 12-dimensional or 48-dimensional 
vectors, respectively. The former takes advantage of coherence in s and t only, while the latter takes 
advantage of coherence in all four dimensions. To maximize image quality, we train on a representa­tive 
subset of each light .eld to be compressed, then transmit the resulting codebook along with the codeword 
index array. Since light .elds are large, even after compression, the additional over­head of transmitting 
a codebook is small, typically less than 20%. We train on a subset rather than the entire light .eld 
to reduce the expense of training. The output of vector quantization is a sequence of .xed­rate codebook 
indices. Each index is log N bits where N is the number of codewords in the codebook, so the compression 
rate of the quantizer is (kl)/(log N) where k is the number of elements per vector (i.e. the dimension), 
and l is the number of bits per ele­ment, usually 8. In our application, we typically use 16384-word 
codebooks, leading to a compression rate for this stage of the pipeline of (48 x 8) / (log 16384) = 384 
bits / 14 bits = 27:1. To simplify decoding, we represent each index using an integral num­ber of bytes, 
2 in our case, which reduces our compression slightly, to 24:1. 4.2. Entropy coding The second stage 
of our compression pipeline is an entropy coder designed to decrease the cost of representing high­probability 
code indices. Since our objects are typically rendered or photographed against a constant-color background, 
the array contains many tiles that occur with high probability. For the examples in this paper, we employed 
gzip, an implementation of Lempel-Ziv coding [Ziv77]. In this algorithm, the input stream is partitioned 
into nonoverlapping blocks while constructing a dic­tionary of blocks seen thus far. Applying gzip to 
our array of code indices typically gives us an additional 5:1 compression. Huffman coding would probably 
yield slightly higher compression, but encoding and decoding would be more expensive. Our total com­pression 
is therefore 24 x 5 = 120:1. See section 6 and table III for more detail on our compression results. 
 4.3. Decompression Decompression occurs in two stages. The .rst stage gzip decoding is performed as 
the .le is loaded into memory. The output of this stage is a codebook and an array of code indices packed 
in 16-bit words. Although some ef.ciency has been lost by this decoding, the light .eld is still compressed 
24:1, and it is now represented in a way that supports random access. The second stage dequantization 
 proceeds as follows. As the observer moves through the scene, the display engine requests samples of 
the light .eld. Each request consists of a (u, v, s, t) coordinate tuple. For each request, a subscripting 
calcu­lation is performed to determine which sample tile is being addressed. Each tile corresponds to 
one quantization vector and is thus represented in the index array by a single entry. Looking this index 
up in the codebook, we .nd a vector of sample values. A second subscripting calculation is then performed, 
giving us the offset of the requested sample within the vector. With the aid of precomputed subscripting 
tables, dequantization can be imple­mented very ef.ciently. In our tests, decompression consumes about 
25% of the CPU cycles.  5. Display The .nal part of the system is a real time viewer that con­structs 
and displays an image from the light slab given the imag­ing geometry. The viewer must resample a 2D 
slice of lines from the 4D light .eld; each line represents a ray through the eye point and a pixel center 
as shown in .gure 12. There are two steps to this process: step 1 consists of computing the (u, v, s, 
t) line parameters for each image ray, and step 2 consists of resampling the radiance at those line parameters. 
As mentioned previously, a big advantage of the light slab representation is the ef.ciency of the inverse 
calculation of the line parameters. Conceptually the (u, v) and (s, t) parameters may be calculated by 
determining the point of intersection of an image ray with each plane. Thus, any ray tracer could easily 
be adapted to use light slabs. However, a polygonal rendering system also may be used to view a light 
slab. The transformation from image coordinates (x, y) to both the (u, v) and the (s, t) coordinates 
is a projective map. Therefore, computing the line coordinates can be done using texture mapping. The 
uv quadrilateral is drawn using the current viewing transformation, and during scan conversion the (uw, 
vw, w) coordinates at the corners of the quadrilateral are interpolated. The resulting u = uw/w and v 
= vw/w coordinates at each pixel represent the ray intersection with the uv quadrilateral. A similar 
procedure can be used to generate the (s, t) coordinates by drawing the st quadrilateral. Thus, the inverse 
transformation from (x, y)to(u, v, s, t) reduces essentially to two texture coordi­nate calculations 
per ray. This is cheap and can be done in real time, and is supported in many rendering systems, both 
hardware and software. Only lines with (u, v) and (s, t) coordinates inside both quadrilaterals are represented 
in the light slab. Thus, if the texture coordinates for each plane are computed by drawing each quadri­laterial 
one after the other, then only those pixels that have both valid uv and st coordinates should be looked 
up in the light slab array. Alternatively, the two quadrilaterals may be simultaneously scan converted 
in their region of overlap to cut down on unneces­sary calculations; this is the technique that we use 
in our software implementation.  us Figure 12: The process of resampling a light slab during display. 
 To draw an image of a collection of light slabs, we draw them sequentially. If the sets of lines in 
the collection of light slabs do not overlap, then each pixel is drawn only once and so this is quite 
ef.cient. To further increase ef.ciency, "back-facing" light slabs may be culled. The second step involves 
resampling the radiance. The ideal resampling process .rst reconstructs the function from the original 
samples, and then applies a bandpass .lter to the recon­structed function to remove high frequencies 
that may cause alias­ing. In our system, we approximate the resampling process by simply interpolating 
the 4D function from the nearest samples. This is correct only if the new sampling rate is greater than 
the original sampling rate, which is usually the case when displaying light .elds. However, if the image 
of the light .eld is very small, then some form of pre.ltering should be applied. This could eas­ily 
be done with a 4D variation of the standard mipmapping algo­rithm [Williams83]. Figure 13 shows the effect 
of nearest neighbor versus bilin­ear interpolation on the uv plane versus quadrilinear interpolation 
of the full 4D function. Quadralinear interpolation coupled with the proper pre.ltering generates images 
with few aliasing arti­facts. The improvement is particularly dramatic when the object or camera is moving. 
However, quadralinear .ltering is more expensive and can sometimes be avoided. For example, if the sampling 
rates in the uv and st planes are different, and then the bene.ts of .ltering one plane may be greater 
than the other plane. 6. Results buddha kidney hallway lion Number of slabs Images per slab Total images 
Pixels per image Raw size (MB) Pre.ltering 1 16x16 256 2562 50 uvst 1 64x64 4096 1282 201 st only 4 64x32 
8192 2562 1608 uvst 4 32x16 2048 2562 402 st only Table I: Statistics of the light .elds shown in .gure 
14. antialiasing. The light .eld con.guration was a single slab similar to that shown in .gure 3a. Our 
second light .eld is a human abdomen constructed from volume renderings. The two tan-colored organs on 
either side of the spine are the kidneys. In this case, the input images were orthographic views, so 
we employed a slab with one plane at in.nity as shown in .gure 4c. Because an orthographic image contains 
rays of constant direction, we generated more input images than in the .rst example in order to provide 
the angular range needed for creating perspective views. The images include pixel antialiasing but no 
aperture antialiasing. However, the dense spacing of input images reduces aperture aliasing artifacts 
to a minimum. Our third example is an outward-looking light .eld depict­ing a hallway in Berkeley s Soda 
Hall, rendered using a radiosity program. To allow a full range of observer motion while optimiz­ing 
sampling uniformity, we used four slabs with one plane at in.nity, a four-slab version of .gure 4c. The 
input images were rendered on an SGI RealityEngine, using the accumulation buffer to provide both pixel 
and aperture antialiasing. Our last example is a light .eld constructed from digitized images. The scene 
is of a toy lion, and the light .eld consists of four slabs as shown in .gure 3c, allowing the observer 
to walk completely around the object. The sensor and optical system pro­vide pixel antialiasing, but 
the aperture diameter was too small to provide correct aperture antialiasing. As a result, the light 
.eld exhibits some aliasing, which appears as double images. These artifacts are worst near the head 
and tail of the lion because of their greater distance from the axis around which the camera rotated. 
Table I summarizes the statistics of each light .eld. Table II gives additional information on the lion 
dataset. Table III gives the performance of our compression pipeline on two representa­tive datasets. 
The buddha was compressed using a 2D tiling of the Figure 14 shows images extracted from four light .elds. 
The .rst is a buddha constructed from rendered images. The model is an irregular polygon mesh constructed 
from range data. The input images were generated using RenderMan, which also provided the machinery for 
computing pixel and aperture Camera motion translation per slab 100 cm x 50 cm pan and tilt per slab 
90° x45° number of slabs 4 slabs 90° apart total pan and tilt 360° x45° Sampling density distance to 
object 50 cm camera pan per sample 3.6° camera translation per sample 3.125 cm Aperture focal distance 
of lens 10mm F-number f/8 aperture diameter 1.25 mm Acquisition time time per image 3 seconds total acquisition 
time 4 hours Table II: Acquisition parameters for the lion light .eld. Distance to object and camera 
pan per sample are given at the center of the plane of camera motion. Total acquisition time includes 
longer gantry movements at the end of each row and manual setup time for each of the four orientations. 
The aperture diameter is the focal length divided by the F-number. buddha lion Vector quantization raw 
size (MB) 50.3 402.7 fraction in training set 5% 3% samples per tile 2x2x1x1 2x2x2x2 bytes per sample 
3 3 vector dimension 12 48 number of codewords 8192 16384 codebook size (MB) 0.1 0.8 bytes per codeword 
index 2 2 index array size (MB) 8.4 16.8 total size (MB) 8.5 17.6 compression rate Entropy coding 6:1 
23:1 gzipped codebook (MB) 0.1 0.6 gzipped index array (MB) 1.0 2.8 total size (MB) 1.1 3.4 compression 
due to gzip 8:1 5:1 total compression Compression performance 45:1 118:1 training time 15 mins 4 hrs 
encoding time 1 mins 8 mins original entropy (bits/pixel) 4.2 2.9 image quality (PSNR) 36 27 Table III: 
Compression statistics for two light .elds. The buddha was compressed using 2D tiles of RGB pixels, forming 
12-dimen­sional vectors, and the lion was compressed using 4D tiles (2D tiles of RGB pixels from each 
of 2 x 2 adjacent camera positions), forming 48-dimensional vectors. Bytes per codeword index in­clude 
padding as described in section 4. Peak signal-to-noise ratio (PSNR) is computed as 10 log10(2552/MSE). 
 light .eld, yielding a total compression rate of 45:1. The lion was compressed using a 4D tiling, yielding 
a higher compression rate of 118:1. During interactive viewing, the compressed buddha is indistinguishable 
from the original; the compressed lion exhibits some artifacts, but only at high magni.cations. Representative 
images are shown in .gure 15. We have also experimented with higher rates. As a general rule, the artifacts 
become objectionable only above 200:1. Finally, table IV summarizes the performance of our inter­active 
viewer operating on the lion light .eld. As the table shows, we achieve interactive playback rates for 
reasonable image sizes. Note that the size of the light .eld has no effect on playback rate; only the 
image size matters. Memory size is not an issue because the compressed .elds are small. 7. Discussion 
and future work We have described a new light .eld representation, the light slab, for storing all the 
radiance values in free space. Both inserting images into the .eld and extracting new views from the 
.eld involve resampling, a simple and robust procedure. The resulting system is easily implemented on 
workstations and per­sonal computers, requiring modest amounts of memory and cycles. Thus, this technique 
is useful for many applications requir­ing interaction with 3D scenes. Display times (ms) no bilerp uv 
lerp uvst lerp coordinate calculation 13 13 13 sample extraction 14 59 214 overhead 3 3 3 total 30 75 
230 Table IV: Display performance for the lion light .eld. Displayed images are 192 x 192 pixels. Sample 
extraction includes VQ de­coding and sample interpolation. Display overhead includes read­ing the mouse, 
computing the observer position, and copying the image to the frame buffer. Timings are for a software-only 
imple­mentation on a 250 MHz MIPS 4400 processor. There are three major limitation of our method. First, 
the sampling density must be high to avoid excessive blurriness. This requires rendering or acquiring 
a large number of images, which may take a long time and consume a lot of memory. However, denser sample 
spacing leads to greater inter-sample coherence, so the size of the light .eld is usually manageable 
after compression. Second, the observer is restricted to regions of space free of occluders. This limitation 
can be addressed by stitching together multiple light .elds based on a partition of the scene geometry 
into convex regions. If we augment light .elds to include Z­depth, the regions need not even be convex. 
Third, the illumina­tion must be .xed. If we ignore interre.ections, this limitation can be addressed 
by augmenting light .elds to include surface normals and optical properties. To handle interre.ections, 
we might try representing illumination as a superposition of basis functions [Nimeroff94]. This would 
correspond in our case to computing a sum of light .elds each lit with a different illumina­tion function. 
It is useful to compare this approach with depth-based or correspondence-based view interpolation. In 
these systems, a 3D model is created to improve quality of the interpolation and hence decrease the number 
of pre-acquired images. In our approach, a much larger number of images is acquired, and at .rst this 
seems like a disadvantage. However, because of the 3D structure of the light .eld, simple compression 
schemes are able to .nd and exploit this same 3D structure. In our case, simple 4D block cod­ing leads 
to compression rates of over 100:1. Given the success of the compression, a high density compressed light 
.eld has an advantage over other approaches because the resampling process is simpler, and no explicit 
3D structure must be found or stored. There are many representations for light used in computer graphics 
and computer vision, for example, images, shadow and environment maps, light sources, radiosity and radiance 
basis functions, and ray tracing procedures. However, abstract light rep­resentations have not been systematically 
studied in the same way as modeling and display primitives. A fruitful line of future research would 
be to reexamine these representations from .rst principles. Such reexaminations may in turn lead to new 
methods for the central problems in these .elds. Another area of future research is the design of instrumen­tation 
for acquisition. A large parallel array of cameras connected to a parallel computer could be built to 
acquire and compress a light .eld in real time. In the short term, there are many interest­ing engineering 
issues in designing and building gantries to move a small number of cameras and lights to sequentially 
acquire both inward- and outward-looking light .elds. This same instrumenta­tion could lead to breakthroughs 
in both 3D shape acquisition and re.ection measurements. In fact, the interaction of light with any object 
can be represented as a higher-dimensional interaction matrix; acquiring, compressing, and manipulating 
such represen­tations are a fruitful area for investigation. 8. Acknowledgements We would like to thank 
David Addleman and George Dabrowski of Cyberware for helping us design and build the cam­era gantry, 
Craig Kolb and James Davis for helping us calibrate it, Brian Curless for scanning the buddha, Julie 
Dorsey for shading it and allowing us to use it, Carlo Sequin for the Soda Hall model, Seth Teller, Celeste 
Fowler, and Thomas Funkhauser for its radiosity solution, Lucas Pereira for rendering it, Benjamin Zhu 
for reimplementing our hardware-accelerated viewer in software, and Navin Chaddha for his vector quantization 
code. We also wish to thank Eric Chen and Michael Chen for allowing us to examine the Apple ObjectMaker, 
and Alain Fournier and Bob Lewis for showing us their wavelet light .eld work. Finally, we wish to thank 
Nina Amenta for sparking our interest in two-plane parameterizations of lines, Michael Cohen for reinforcing 
our interest in image-based representations, and Gavin Miller for inspiring us with his grail of volumetric 
hyperreality. This work was supported by the NSF under contracts CCR-9157767 and CCR-9508579. 9. References 
[Adelson91] Adelson, E.H., Bergen, J.R., The Plenoptic Function and the Elements of Early Vision, In 
Computation Models of Visual Processing, M. Landy and J.A. Movshon, eds., MIT Press, Cam­bridge, 1991. 
[Ashdown93] Ashdown, I., Near-Field Photometry: A New Approach, Journal of the Illuminating Engineering 
Society, Vol. 22, No. 1, Win­ter, 1993, pp. 163-180. [Beers96] Beers, A., Agrawala, M., Chaddha, N., 
 Rendering from Com­pressed Textures. In these proceedings. [Benton83] Benton, S., Survey of Holographic 
Stereograms, Process­ing and Display of Three-Dimensional Data, Proc. SPIE, Vol. 367, 1983. [Blinn76] 
Blinn, J.F., Newell, M.E., Texture and Re.ection in Computer Generated Images, CACM, Vol. 19, No. 10, 
October, 1976, pp. 542-547. [Bolles87] Bolles, R., Baker, H., Marimont, D., Epipolar-Plane Image Analysis: 
An Approach to Determining Structure from Motion, International Journal of Computer Vision, Vol. 1, 
No. 1, 1987, pp. 7-55. [Chen93] Chen, S.E., Williams, L., View Interpolation for Image Syn­thesis, 
Proc. SIGGRAPH 93 (Anaheim, California, August 1-6, 1993). In Computer Graphics Proceedings, Annual Conference 
Series, 1993, ACM SIGGRAPH, pp. 279-288. [Chen95] Chen, S.E., QuickTime VR An Image-Based Approach 
to Virtual Environment Navigation, Proc. SIGGRAPH 95 (Los Ange­les, CA, August 6-11, 1995). In Computer 
Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH, pp. 29-38. [Fuchs94] Fuchs, H., Bishop, 
G., Arthur, K., McMillan, L., Bajcsy, R., Lee, S.W., Farid, H., Kanade, T., Virtual Space Teleconferencing 
Using a Sea of Cameras, Proc. First International Conference on Medical Robotics and Computer Assisted 
Surgery, 1994, pp. 161-167. [Gersho92] Gersho, A., Gray, R.M., Vector Quantization and Signal Com­pression, 
Kluwer Academic Publishers, 1992. [Gershun36] Gershun, A., The Light Field, Moscow, 1936. Translated 
by P. Moon and G. Timoshenko in Journal of Mathematics and Physics, Vol. XVIII, MIT, 1939, pp. 51-151. 
[Gortler96] Gortler, S.J., Grzeszczuk, R., Szeliski, R., Cohen, M., The Lumigraph. In these proceedings. 
[Greene86] Greene, N., Environment Mapping and Other Applications of World Projections, IEEE Computer 
Graphics and Applications, Vol. 6, No. 11, November, 1986, pp. 21-29. [Greene94] Greene, N. and Kass, 
M., Approximating Visibility with Environment Maps, Apple Technical Report No. 41, November, 1994. 
[Halle94] Halle, M., Holographic Stereograms as Discrete Imaging Sys­tems. Practical Holography, Proc. 
SPIE, Vol. 2176, February, 1994. [Katayama95] Katayama, A., Tanaka, K., Oshino, T., Tamura, H., View­point-Dependent 
Stereoscopic Display Using Interpolation of Multi­viewpoint Images, Stereoscopic Displays and Virtual 
Reality Sys­tems II, Proc. SPIE, Vol. 2409, S. Fisher, J. Merritt, B. Bolas eds. 1995, pp. 11-20. [Laveau94] 
Laveau, S., Faugeras, O.D., 3-D Scene Representation as a Collection of Images and Fundamental Matrices, 
 INRIA Technical Report No. 2205, 1994. [Levin71] Levin, R., Photometric Characteristics of Light Controlling 
Apparatus, Illuminating Engineering, Vol. 66, No. 4, 1971, pp. 205-215. [McMillan95a] McMillan, L., 
Bishop, G., Head-Tracked Stereoscopic Display Using Image Warping, Stereoscopic Displays and Virtual 
Reality Systems II, Proc. SPIE, Vol. 2409, S. Fisher, J. Merritt, B. Bolas eds. 1995, pp. 21-30. [McMillan95b] 
McMillan, L., Bishop, G., Plenoptic Modeling: An Image-Based Rendering System, Proc. SIGGRAPH 95 (Los 
Ange­les, CA, August 6-11, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM 
SIGGRAPH, pp. 39-46. [Miller95] Miller, G., Volumetric Hyper-Reality: A Computer Graphics Holy Grail 
for the 21st Century?, Proc. Graphics Interface 95,W. Davis and P. Prusinkiewicz eds., Canadian Information 
Processing Society, 1995, pp. 56-64. [Moon81] Moon, P., Spencer, D.E., The Photic Field, MIT Press, 1981. 
[Narayanan95] Narayanan, P.J., Virtualized Reality: Concepts and Early Results, Proc. IEEE Workshop 
on the Representation of Visual Scenes, IEEE, 1995. [Nimeroff94] Nimeroff, J., Simoncelli, E., Dorsey, 
J., Ef.cient Re­rendering of Naturally Illuminated Scenes, Proc. Fifth Eurograph­ics Rendering Workshop, 
1994, pp. 359-373. [Sbert93] Sbert, A.M., An Integral Geometry Based Method for Form-Factor Computation, 
 Computer Graphics Forum, Vol. 13, No. 3, 1993, pp. 409-420. [Seitz95] Seitz, S., Dyer, C., Physically-Valid 
View Synthesis by Image Interpolation, Proc. IEEE Workshop on the Representation of Visual Scenes, IEEE, 
1995. [Williams83] Williams, L., Pyramidal Parametrics, Computer Graphics (Proc. Siggraph 83), Vol. 
17, No. 3, July, 1983, pp. 1-11. [Ziv77] Ziv, J., Lempel, A., A universal algorithm for sequential data 
compression, IEEE Transactions on Information Theory, IT-23:337-343, 1977.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237200</article_id>
		<sort_key>43</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[The lumigraph]]></title>
		<page_from>43</page_from>
		<page_to>54</page_to>
		<doi_number>10.1145/237170.237200</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237200</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Verification</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P269895</person_id>
				<author_profile_id><![CDATA[81100259454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Gortler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P237369</person_id>
				<author_profile_id><![CDATA[81100560226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Radek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grzeszczuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15023234</person_id>
				<author_profile_id><![CDATA[81100122769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szeliski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15024219</person_id>
				<author_profile_id><![CDATA[81406592138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADELSON, E. H., AND BERGEN, J. R. The plenoptic function and the elements of early vision. In ComputationalModels of Visual Processing, Landy and Movshon, Eds. MIT Press, Cambridge, Massachusetts, 1991, ch. 1.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ASHDOWN, I. Near-field photometry: A new approach. Journal of the Illumination Engineering Society 22, 1 (1993), 163-180.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BENTON, S. A. Survey of holographic stereograms. Proceedings of the SPIE 391 (1982), 15-22.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BOLLES, R. C., BAKER, H. H., AND MARIMONT, D. H. Epipolar-plane image analysis: An approach to determining structure from motion. International Journal of Computer Vision 1 (1987), 7-55.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BURT, P. J. Moment images, polynomial fit flters, and the problem of surface interpolation. In Proceedings of Computer Vision and Pattern Recognition (June 1988), IEEE Computer Society Press, pp. 144-152.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CHEN, S. E. Quicktime VR - an image-based approach to virtual environment navigation. In Computer Graphics, Annual Conference Series, 1995, pp. 29-38.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CHEN, S. E., AND WILLIAMS, L. View interpolation for image synthesis. In Computer Graphics, Annual Conference Series, 1993, pp. 279-288.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>163196</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CHUI, C. K. An Introduction to Wavelets. Academic Press Inc., 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HALLE, M. W. Holographic stereograms as discrete imaging systems. Practical Holography VIII (SPIE) 2176 (1994), 73-84.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. Progressive meshes. In Computer Graphics, Annual Conference Series, 1996.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[KATAYAMA, A., TANAKA, K., OSHINO, T., AND TAMURA, H. A viewpoint independent stereoscopic display using interpolation of multi-viewpoint images. Steroscopic displays and virtal reality sytems H (SPIE) 2409 (1995), 11-20.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>154060</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[KLINKER, G. J. A PhysicaI Applvach to Color Image Understanding. A K Peters, Wellesley, Massachusetts, 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>628563</ref_obj_id>
				<ref_obj_pid>628309</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[LAURENTINI, A. The visual hull concept for silhouette-based image understanding. IEEE Transactions on Pattern Analysis and Machine lntelligence 16, 2 (February 1994), 150-162.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[LAVEAU, S., AND FAUGERAS, O. 3-D scene representation as a collection of images and fundamental matrices. Tech. Rep. 2205, INRIA-Sophia Antipolis, February 1994.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LEVIN, R. E. Photometric characteristics of light-controlling apparatus. Illuminating Engineering 66, 4 (1971), 205-215.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., AND HANRAHAN, P. Light-fieldrendering. In Computer Graphics, Annual Conference Series, 1996.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>336250</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LEWIS, R. R., AND FOURNIER, A. Light-driven global illumination with a wavelet representation of light transport. UBC CS Technical Reports 95-28, University of British Columbia, 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192270</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LITWINOWICZ, P., AND WILLIAMS, L. Animating images with drawings. In Computer Graphics, Annual Conference Series, 1994, pp. 409-412.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MCMILLAN, L., AND BISHOP, G. Plenoptic modeling: An image-based rendering system. In Computer Graphics, Annual Conference Series, 1995, pp. 39-46.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37410</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MITCHELL, D. P. Generating antialiased images at low sampling densities. Computer Graphics 21, 4 (1987), 65-72.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>31124</ref_obj_id>
				<ref_obj_pid>31123</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[POTMESIL, M. Generating octree models of 3D objects from their silhouettes in a sequence of images. Computer Vision, Graphics, and Image P~vcessing 40 (1987), 1-29.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[POWELL, M. J. D., AND SWANN, J. Weighted uniform sampling - a monte carlo technique for reducing variance. J. Inst. Maths Applics 2 (1966), 228-236.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[ROSENFELD, A., AND KAK, A. C. DigitalPicture P~vcessing. Academic Press, New York, New York, 1976.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[SIMONCELLI, E. P., FREEMAN, W. T., ADELSON, E. H., AND HEEGER, D. J. Shiftable multiscale transforms. IEEE Transactions on Information Theory 38 (1992), 587-607.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134094</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SNYDER, J. M., AND KAJIYA, J. Y. Generative modeling: A symbolic system for geometric modeling. Computer Graphics 26, 2 (1992), 369-379.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>167383</ref_obj_id>
				<ref_obj_pid>171245</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SZELISKI, R. Rapid octree construction from image sequences. CVGIP: Image Understanding58, 1 (July 1993), 23-32.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[TAUBIN, G. A signal processing approach to fair surface design. In Computer Graphics, Annual Conference Series, 1995, pp. 351-358.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5994</ref_obj_id>
				<ref_obj_pid>5979</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D. Regularization of inverse visual problems involving discontinuities. IEEE PAMI 8, 4 (July 1986), 413-424.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[TSAI, R. Y. A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelfTV cameras and lenses. IEEE Journal of Robotics and Automation RA-3 , 4 (August 1987), 323-344.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>840038</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WERNER, Y., HERSCH, R. D., AND HLAVAC, g. Rendering real-world objects using view interpolation. In Fifth International Conference on Computer Vision (ICCV' 95) (Cambridge, Massachusetts, June 1995), pp. 957-962.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192962</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[WILLSON, R. G. Modeling and Calibration of Automated Zoom Lenses. PhD thesis, Carnegie Mellon University, 1994.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237204</article_id>
		<sort_key>55</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[The feudal priority algorithm on hidden-surface removal]]></title>
		<page_from>55</page_from>
		<page_to>64</page_to>
		<doi_number>10.1145/237170.237204</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237204</url>
		<keywords>
			<kw><![CDATA[the binary space partitioning tree algorithm]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Trees</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10003634</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP311392000</person_id>
				<author_profile_id><![CDATA[81540493556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Han-Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mechanical Engineering, National Taiwan University, Taipei, Taiwan 107, R.O.C.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P297684</person_id>
				<author_profile_id><![CDATA[81100215702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wen-Teng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mechanical Engineering, National Taiwan University, Taipei, Taiwan 107, R.O.C.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>144204</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CHEN, HAN-MING. A Real-Time Rendering System for 3-D Objects in Computer-Aided Design and Manufacturing, Ph.D. Dissertation, University of California at Berkeley, May 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CHEN, HAN-MING, and ADIGA, S. An Ingenious Algorithm for Hidden-Surface Removal. The Proceedings of The Second International Conference on CAD &amp; CG. September 1991, Hangzhou, China, pp. 159-164.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CHEN, HAN-MING, and JIANG, L. S. Partitioning A Concave Polygon into Simply-Connected Polygons. The Proceedings of The Twenty-third Midwestern Mechanics Conference, October 1993, Lincoln, U.S.A., pp.75-77.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[FOLEY, J. D., VAN DAM, A., FEINER, S. K., and HUGHES, J. F. Computer Graphics: Principles and Practice, second ed. Addison-Wesley, Reading, MA, 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FUCHS, H., KEDEM, Z. M., and NAYLOR, B. F. On Visible Surface Generation by A Priori Tree Structures. Computer Graphics, Vo1.14(3), July 1980, pp.124-133.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801134</ref_obj_id>
				<ref_obj_pid>964967</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FUCHS, H., ABRAM, G. D., and GRANT, E. D. Near Real-Time Shaded Display of Rigid Objects. Computer Graphics, Vo1.17(3), July 1983, pp.65-72.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909951</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[NAYLOR, B. F. A Priori Based Techniques for Determining Visibility Priority for 3-D Scenes, Ph.D. Dissertation, University of Texas at Dallas, May 1981.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>569954</ref_obj_id>
				<ref_obj_pid>800193</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[NEwEI~I~, M. E., NEwEI~I~, R. G., and S ANCHA, T. L. A Solution to the Hidden Surface Problem. The Proceedings of the ACM National Conference, 1972, pp.443-450.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>4333</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[PREPARATA, F. P., and SHAMOS, M. I. Computational Geometry: An Introduction, Springer-Verlag, New York, 1985.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[SCHUMACKER, e. A., BRAND, B., GILLILAND, M. G., and SHARP, W. H. Study for Applying Computer-Generated Images to Visual Simulation. Technical Report AFHRL-TR- 69-14, NTIS AD700375fR, U.S. Air Force Human Resources Lab., Air Force Systems Command, Brooks AFB, TX, September 1969.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[SUTHERLAND, I. E., SPROULL, R. F., and SCHUMACKER R. A. Sorting and the Hidden-Surface Problem. The Proceedings of the National Computer Conference, 1973, pp.685-693.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[SUTHEaLAND, I. E., SPaOUI~L, R. F., and SCHUMACKER, U. A. A Characterization of Ten Hidden-Surface Algorithms. ACM Computing Surveys, Vol.6(1), March 1974, pp.l-55.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360802</ref_obj_id>
				<ref_obj_pid>360767</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[SUTHEaLAND, I. E., and HODGMAN, a. W. Reentrant Polygon Clipping. Communications of the ACM, Vol.17(1), January 1974, pp.32-42.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37421</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[THIBAULT, W. C., and NAYLOR, B. F. Set Operations on Polyhedra Using Binary Space Partitioning Trees. Computer Graphics, Vol.21(4), July 1987, pp.153-162.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[YAO, F. F. On the Priority Approach to Hidden-Surface Algorithms. The Proceedings of the IEEE Symposium on the Foundations of Computer Science, 21st Annual, 1980, pp.301-307.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237207</article_id>
		<sort_key>65</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Hierarchical polygon tiling with coverage masks]]></title>
		<page_from>65</page_from>
		<page_to>74</page_to>
		<doi_number>10.1145/237170.237207</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237207</url>
		<keywords>
			<kw><![CDATA[BSP tree]]></kw>
			<kw><![CDATA[antialiasing]]></kw>
			<kw><![CDATA[coverage mask]]></kw>
			<kw><![CDATA[octree]]></kw>
			<kw><![CDATA[recursive subdivision]]></kw>
			<kw><![CDATA[tiling]]></kw>
			<kw><![CDATA[visibility]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.1.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003777.10003778</concept_id>
				<concept_desc>CCS->Theory of computation->Computational complexity and cryptography->Complexity classes</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003777</concept_id>
				<concept_desc>CCS->Theory of computation->Computational complexity and cryptography</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP43125610</person_id>
				<author_profile_id><![CDATA[81339502048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ned]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Greene]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apple Computer, 1 Infinite Loop, Cupertino, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>325177</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[G. Abram, L. Westover, and T. Whitted, "Efficient Alias-Free Rendering using Bit-Masks and Look-Up Tables," Proceedings of SIGGFtAPH '85, July 1985, 53-59.]]></ref_text>
				<ref_id>Abram-et-al85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808585</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[L. Carpenter, "The A-buffer, an Antialiased Hidden Surface Method," Proceedings of SIGGFtAPH '8g, July 1984, 103-108.]]></ref_text>
				<ref_id>Carpenter84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[E. Catmull, "A Subdivision Algorithm for Computer Display of Curved Surfaces," Phi) Thesis, Report UTEC-CSc- 74-133, Computer Science Dept., University of Utah, Salt Lake City, Utah, Dec. 1974.]]></ref_text>
				<ref_id>Catmull74</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[E. Catmull, "A Hidden-Surface Algorithm with Anti- Aliasing," Proceedings of SIGGRAPH '78, Aug. 1978, 6-11.]]></ref_text>
				<ref_id>Catmull78</ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R. Cook, "Stochastic Sampling in Computer Graphics," ACM Transactions on Graphics, Jan. 1986, 51-72.]]></ref_text>
				<ref_id>Cook86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325182</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. A. Z. I)ipp6 and E. H. Wold, "Antialiasing through Stochastic Sampling," Proceedings of SIGGRAPH '85, July 1985, 69-78.]]></ref_text>
				<ref_id>Dipp&#233;-Wold85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801143</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[E. Fiume, A. Fournier, and L. Rudolph, "A Parallel Scan Conversion Algorithm with Anti-Aliasing for a General- Purpose Ultracomputer," Proceedings of SIGGRAPH '83, July 1983, 141-150.]]></ref_text>
				<ref_id>Fiume-et-al83</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108685</ref_obj_id>
				<ref_obj_pid>108681</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[E. Fiume, "Coverage Masks and Convolution Tables for Fast Area Sampling," Graphical Models and Image Processing, 53(1), Jan. 1991, 25-30.]]></ref_text>
				<ref_id>Fiume91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. Foley, A. van Dam, S. Feiner, and J. Hughes, Computer Graphics, Principles and Practice, 2nd edition, Addison- Wesley, Reading, MA, 1990.]]></ref_text>
				<ref_id>Foley-et-al90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[H. Fuchs, J. Kedem, and B. Naylor, "On Visible Surface Generation by a Priori Tree Structures," Proceedings of SIGGRAPH '80, June 1980, 124-133.]]></ref_text>
				<ref_id>Fuchs-Kedem-Naylor80</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[N. Greene, M. Kass, and G. Miller, "Hierarchical Z-Buffer Visibility," Proceedings of SIGGRAPH '93, July 1993, 231-238.]]></ref_text>
				<ref_id>Greene-Kass-Miller93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192173</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[N. Greene and M. Kass, "Error-Bounded Antialiased Rendering of Complex Environments," Proceedings of SIGGRAPH '94, July 1994, 59-66.]]></ref_text>
				<ref_id>Greene-Kass94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>922141</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[N. Greene, "Hierarchical Rendering of Complex Environments," Phi) Thesis, Univ. of California at Santa Cruz, Report No. UCSC-CRL-95-27, June 1995.]]></ref_text>
				<ref_id>Greene95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>254026</ref_obj_id>
				<ref_obj_pid>253607</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[N. Greene, "Naked Empire," ACM Siggraph Video Review Issue 115: The Siggraph '96 Electronic Theater, August 1996.]]></ref_text>
				<ref_id>Greene96</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[P. Haeberli and K. Akeley, "The Accumulation Buffer: Hardware Support for High-Quality Rendering," Proceedings of SIGGRAPH '90, Aug. 1990, 309-318.]]></ref_text>
				<ref_id>Haeberli-Akeley90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319126</ref_obj_id>
				<ref_obj_pid>319120</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. Kaufman, "3i) Scan Conversion Algorithms for Voxel-Based Graphics," Proceedings of the 1986 Workshop on Interactive 3D Graphics, Oct. 1986, 45-75.]]></ref_text>
				<ref_id>Kaufman86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[I). Meagher, "The Octree Encoding Method for Efficient Solid Modeling," Phi) Thesis, Electrical Engineering Dept., Rensselaer Polytechnic Institute, Troy, New York, Aug. 1982.]]></ref_text>
				<ref_id>Meagher82</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155296</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[B. Naylor, "Interactive Solid Geometry Via Partitioning Trees," Proceedings of Graphics Interface '92, May 1992, 11- 18.]]></ref_text>
				<ref_id>Naylor92a</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155318</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[B. Naylor, "Partitioning Tree Image Representation and Generation from 3i) Geometric Models," Proceedings of Graphics Interface '92, May 1992, 201-212.]]></ref_text>
				<ref_id>Naylor92b</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2213</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[I). Rogers, Procedural Elements for Computer Graphics, McGraw-Hill, New York, 1985.]]></ref_text>
				<ref_id>Rogers85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[P. Sabella and M. Wozny, "Toward Fast Color- Shaded Images of CAD/CAM Geometry," IEEE Computer Graphics and Applications, 3(8), Nov. 1983, 60-71.]]></ref_text>
				<ref_id>Sabella-Wozny83</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[S. Teller, "Visibility Computations in Densely Occluded Polyhedral Environments," Phi) Thesis, Univ. of California at Berkeley, Report No. UCB/CSI) 92/708, Oct. 1992.]]></ref_text>
				<ref_id>Teller92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>905316</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J. Warnock, "A Hidden Surface Algorithm for Computer Generated Halftone Pictures," Phi) Thesis, Computer Science Dept., University of Utah, TR 4-15, June 1969.]]></ref_text>
				<ref_id>Warnock69</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hierarchical Polygon Tiling with Coverage Masks Ned Greene * Apple Computer Abstract We present a novel 
polygon tiling algorithm in which recur­sive subdivision of image space is driven by coverage masks that 
classify a convex polygon as inside, outside, or inter­secting cells in an image hierarchy. This approach 
permits Warnock-style subdivision with its logarithmic search prop­erties to be driven very e.ciently 
by bit-mask operations. The resulting hierarchical polygon tiling algorithm performs subdivision and 
visibility computations very rapidly while only visiting cells in the image hierarchy that are crossed 
by visible edges in the output image. Visible samples are never overwritten. At 512 ×512 resolution, 
the algorithm tiles as rapidly as traditional incremental scan conversion, and at high resolution (e.g. 
4096×4096) it is much faster, making it well suited to antialiasing by oversampling and .ltering. For 
densely occluded scenes, we combine hierarchi­cal tiling with the hierarchical visibility algorithm to 
enable hierarchical object-space culling. When we tested this com­bination on a densely occluded model, 
it computed visibility on a 4096 ×4096 grid as rapidly as hierarchical z-bu.ering [Greene-Kass-Miller93] 
tiled a 512 ×512 grid, and it e.ec­tively antialiased scenes containing hundreds of thousands of visible 
polygons. The algorithm requires strict front-to­back traversal of polygons, so we represent a scene 
as a BSP tree or as an octree of BSP trees. When maintaining depth order of polygons is not convenient, 
we combine hierarchical tiling with hierarchical z-bu.ering, resorting to z-bu.ering only in regions 
of the screen where the closest object is not encountered .rst. CR Categories: I.3.7 [Computer Graphics]: 
Three-Dimensional Graphics and Realism -Hidden line/surface removal; I.3.3 [Computer Graphics]: Picture/Image 
Gener­ation. Keywords: tiling, coverage mask, antialiasing, visibility, BSP tree, octree, recursive subdivision. 
1 INTRODUCTION Polygon tiling algorithms have been an important topic in computer image synthesis since 
the advent of raster graph­ics some two decades ago. Their purpose is to determine * Contact author at 
greene@apple.com, Apple Computer, 1 In.nite Loop, Cupertino, CA 95014 Permission to make digital or hard 
copies of part or all of this work or personal or classroom use is granted without fee provided that 
copies are not made or distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
which point samples on an image raster are covered by the visible portion of each of the polygons composing 
a scene. Currently, polygon tiling software running on inexpensive computers can render point-sampled 
images of simple scenes at interactive rates. The fastest tiling algorithms have been carefully tuned 
to exploit image-space coherence by using incremental methods wherever possible. However, they fail to 
exploit opportunities for precomputation and they waste time tiling hidden geometry. There is a need 
for more e.­cient tiling algorithms that e.ectively exploit coherence and precomputation to enable e.cient 
culling of hidden geome­try and e.cient tiling of visible geometry. The dominant polygon tiling algorithm 
in use today is in­cremental scan conversion. Typically, raster samples on a polygon s perimeter are 
traversed with an incremental line­tiling algorithm. Edge samples on each intersected scan­line de.ne 
spans within a polygon, which are then traversed pixel-by-pixel, permitting incremental update of shading 
pa­rameters and, in the case of z-bu.ering, depth values. Vis­ibility of samples can be determined by 
a) maintaining a z-bu.er and performing depth comparisons [Catmull74], b) traversing primitives back 
to front and writing every pixel tiled [Foley-et-al90], or c) traversing primitives front to back and 
overwriting only vacant pixels [Foley-et-al90]. With in­cremental scan conversion, the cost per pixel 
tiled is very low because incremental edge and span traversal e.ectively exploits image-space coherence. 
One problem with traditional incremental scan conver­sion is that it must tile every sample on every 
primitive, whether or not it is visible, and so it wastes time tiling hid­den geometry. This is not a 
big problem for simple scenes, but for densely occluded scenes it severely impairs e.ciency. Ideally, 
a tiling algorithm should cull hidden geometry e.­ciently so that running time is proportional to the 
visible complexity of the scene and independent of the complexity of hidden geometry. The Warnock subdivision 
algorithm [Warnock69] ap­proaches this goal, performing logarithmic search for visible tiles in the quadtree 
subdivision of a polygon. If scene prim­itives are processed front to back, only visible tiles and their 
children in the quadtree are visited. Although Warnock sub­division satis.es our desire to work only 
on visible regions of primitives, the traditional subdivision procedure is relatively slow and consequently, 
this approach is slower than incre­mental scan conversion, except for densely occluded scenes. Neither 
traditional incremental scan conversion nor Warnock subdivision is well suited to tiling scenes of moderate 
depth complexity. A second shortcoming of incremental scan conversion is that it spends most of its time 
tiling edges and spans, travers­ing these features pixel by pixel, even though all possi­ble tiling patterns 
for an edge crossing a block of samples can be precomputed and stored as bit masks called cover­age masks 
. Then the samples that a convex polygon covers within a block can be quickly found by compositing the 
cov­erage masks of its edges. Previously, this technique has been used to estimate coverage of polygonal 
fragments within a pixel to accelerate .ltering [Carpenter84, Sabella-Wozny83, Fiume-et-al83, Fiume91]. 
Here we present a polygon tiling algorithm that combines the best features of traditional algorithms. 
The key innova­tion that makes this integration possible is the generaliza­tion of coverage masks to 
permit their application to image hierarchies. The generalized masks, which we call triage coverage masks 
, classify cells in the image hierarchy as in­side outside intersecting an edge. This enables them to 
, ,or drive Warnock-style subdivision of image space. The result is a hierarchical tiling algorithm that 
.nds visible geometry by logarithmic search, as with the Warnock algorithm, that exploits precomputation 
of tiling patterns, as with .ltering with coverage masks, and that also uses incremental meth­ods to 
exploit image-space coherence, as with incremental scan conversion. The algorithm e.ciently performs 
high­resolution tiling (e.g. 4096 ×4096), so it naturally supports high-quality antialiasing by oversampling 
and .ltering. A­bu.er-style antialiasing with coverage masks [Carpenter84] is particularly convenient. 
For densely occluded scenes we combine hierarchi­cal tiling with the hierarchical visibility algorithm 
[Greene-Kass-Miller93, Greene-Kass94, Greene95] to per­mit hierarchical culling of hidden regions of 
object space. This combination of algorithms enables very rapid rendering of complex polygonal scenes 
with high-quality antialiasing. The method has been tested and shown to work e.ectively on densely occluded 
scenes. On a test scene containing up­wards of 167 million replicated polygons, the algorithm com­puted 
visibility on a 4096 ×4096 grid as rapidly as hierarchi­cal z-bu.ering [Greene-Kass-Miller93] tiled a 
512 ×512 grid. In §2, we survey previous work on e.cient polygon tiling. In §3, we introduce triage coverage 
masks, and in §4 we present the rendering algorithm in which they are applied. In §5, we discuss how 
rendering of densely occluded scenes can be accelerated with object-space culling methods. In §6, we 
discuss strategies for e.ciently processing dynamic scenes. In §7, we compare the hierarchical tiling 
algorithm to hierarchical z-bu.ering. In §8, we describe hierarchical tiling of polyhedra. In §9, we 
describe our implementation and show results for both simple and densely occluded scenes. Finally, we 
present our conclusions in §10. 2 PREVIOUS WORK 2.1 Warnock Subdivision Our tiling algorithm is loosely 
based on the Warnock algo­rithm [Warnock69], a recursive subdivision procedure that .nds the quadtree 
subdivision of visible edges in a scene by logarithmic search. Scene primitives are inserted into a quadtree 
data structure beginning at the root cell, which rep­resents the whole screen. At each level of subdivision, 
the algorithm classi.es the quadrants of the current quadtree cell as inside, outside, or intersecting 
the primitive being processed, and only intersected quadrants are subdivided. Quadrants which are entirely 
covered by one or more primi­tives are identi.ed, permitting hidden geometry within them to be culled. 
The Warnock algorithm is actually a family of algorithms based on a common subdivision procedure, and 
the control structure varies from implementation to imple­mentation [Rogers85]. A typical implementation 
processes primitives in no particular order, maintains lists of poten­tially visible primitives at quadtree 
cells, and expends con­siderable work performing depth comparisons in order to cull hidden geometry. 
When circumstances permit convenient front-to-back traversal of primitives, as with a presorted static 
polygonal scene, a simpler and more e.cient variation of the Warnock algorithm can be employed. In this 
case, we insert primi­tives into the quadtree one at a time in front-to-back order. As subdivision proceeds, 
we mark cells that primitives com­pletely cover as occupied and ignore cells that are already oc­cupied, 
since any geometry that projects to them is known to be hidden. We complete subdivision of one primitive 
down to the .nest level of the quadtree before processing the next. This version of the Warnock algorithm 
is simpler because it need not maintain lists of primitives or perform depth com­parisons. It is more 
e.cient because, unlike the traditional algorithm, it only subdivides cells crossed by edges that are 
visible in the output image. Our tiling algorithm is based on this variation of the Warnock algorithm, 
which we will refer to as the depth-priority Warnock algorithm . Although Meagher s volume rendering 
algorithm uses this procedure to tile faces of octree cubes [Meagher82], to the best of our knowledge 
this variation of the Warnock algorithm has not been applied previously to rendering geometric models. 
In­cidentally, front-to-back traversal of primitives would accel­erate Warnock-style subdivision in the 
error-bounded ren­dering algorithm described in [Greene-Kass94].  2.2 Coverage Masks We turn now to 
reviewing how .ltering algorithms ex­ploit precomputation with coverage masks [Carpenter84, Sabella-Wozny83, 
Fiume-et-al83, Fiume91]. The underly­ing idea is that all possible tiling patterns for a single edge 
crossing a grid of raster samples within a pixel can be precomputed and later retrieved, indexed by the 
points where the edge intersects the pixel s border [Fiume-et-al83, Sabella-Wozny83]. These tiling patterns 
can be stored as bit masks, permitting samples inside a convex polygon to be determined by ANDING together 
the coverage masks for its edges. Moreover, if polygons are processed front to back or back to front, 
visible-surface determination within a pixel can also be performed with bit-mask operations. For exam­ple, 
Carpenter s A-bu.er algorithm [Carpenter84] clips poly­gons to pixel borders, sorts the polygonal fragments 
front to back, and determines the visible samples on each frag­menton a 4 ×8 grid by compositing coverage 
masks. The A-bu.er algorithm also uses coverage masks to accelerate .ltering. For each visible fragment, 
a single shading value is computed, weighted by the bit count of its mask, and added to pixel color. 
This shading method e.ciently ap­proximates area sampling [Catmull78] and it e.ectively an­tialiases 
edges. Abram, Westover, and Whitted advance sim­ilar methods that permit jitter, convolution with arbitrary 
.lter kernels, and evaluation of simple shading functions to be performed by table lookup [Abram-et-al85]. 
3 TRIAGE COVERAGE MASKS To accelerate polygon tiling, the hierarchical tiling algorithm generalizes coverage 
masks to operate on image hierarchies, thereby enabling Warnock-style subdivision of image space to be 
driven by bit-mask operations. A conventional cov­erage mask for an edge classi.es each grid point within 
a square region of the screen as inside or outside the edge, as shown in .gure 1a. In the context of 
Warnock subdivision, the analogous operation is classifying subcells of an image hi­erarchy as inside, 
outside, or intersecting an edge, as shown in .gure 1b for an edge crossing a square containing a 4×4 
grid of subcells. We call such masks triage coverage masks because the three states that they distinguish 
correspond to trivial rejection, trivial acceptance, and do further work. We represent each triage mask 
as a pair of bit masks, one indicating inside subcells, the other indicating outside sub­cells, as shown 
in .gures 1c and 1d. We will refer to the bit mask for inside subcells as the C mask (for covered) and 
the bit mask for outside subcells as the V mask (for vacant). We call the intersected subcells the active 
region of the mask, because the corresponding regions of the screen require further work and will later 
be subdivided. The bit mask for the active region is A = (C | V ), as shown in .gure 1e. 1 In practice, 
we use 8×8 masks rather than the illustrated 4×4 masks. conventional coverage mask a represented as 
two bit masks triage C bit mask V bit mask A = ~(C|V) The basic tiling and visibility operations performed 
by coverage mask (covered region) (vacant region) (active region) conventional coverage masks are (1) 
.nding the mask of a convex polygon from the masks of its edges, and (2) .nding the visible samples on 
a polygon within a pixel by com­ positing the polygon s mask with the pixel s mask, which represents 
previously tiled samples. In the context of the hierarchical tiling algorithm, tiling and visibility 
operations  performed by triage masks are entirely analogous, except inside outside intersected subcells 
subcells subcells that compositing is performed recursively on an image hier­ archy rather than a single 
square region of the screen. The bcd e image hierarchy is a coverage pyramid constructed from both conventional 
and triage coverage masks, as schemat­ically illustrated in .gure 2 (see caption). Operations (1) and 
(2) for triage masks are easily understood by analogy with conventional coverage masks, as outlined below. 
See [Greene95] for derivations of the formulas for triage masks and examples illustrating compositing 
of triage masks. Tiling a convex polygon into a square region of the screen using coverage masks. The 
existing coverage mask for a screen cell represents previously tiled polygons, which are in front of 
the polygon being tiled. Conventional Coverage Masks Existing pixel mask: C (1) Find intercepts of edges 
with pixel border and look up edge masks (call them E1, E2, ... , EN). Find mask P of convex polygon 
from edge masks: P = E1&#38; E2 &#38; ... &#38; EN (2) Find mask W of visible samples on polygon within 
C: W=P &#38;~C Update C: C =C |P.  Triage Coverage Masks Existing triage mask for cell in the coverage 
pyramid: (Cc,Cv) -covered and vacant bit masks (1) Find intercepts of edges with cell border and look 
 up edge masks ((E1c,E1v), ... ,(ENc,ENv)). Find mask (Pc,Pv) of polygon from edge masks: Pc = E1c &#38; 
E2c &#38; ... &#38; ENc Pv = E1v | E2v | ... | ENv (2) Find mask W of entirely visible cells on polygon 
 within (Cc,Cv): W=Cv &#38; Pc Find mask A of active cells on polygon in (Cc,Cv): A = ~(W | Pv | Cc) 
Update (Cc,Cv): Cc = Cc | W Cv = Cv &#38; ~W (Note: (Cc,Cv) may also be modified by propagation from 
finer levels.) 1We use standard notation for bit-mask operations: &#38; for bit­wise AND, | for bitwise 
OR, and for bitwise complement. Figure 1: A conventional coverage mask classi.es grid points as inside 
or outside an edge (panel a). A triage coverage mask classi.es subcells as inside, outside, or intersecting 
an edge (panel b). We refer to these regions as covered (panel c), vacant (panel d), and active (panel 
e), respectively. We represent triage masks as the pair of bit masks (CV, ) indicating the covered and 
vacant regions. In practice, we use 8×8 masks rather than 4×4 masks. WHOLE SCREEN SINGLE PIXEL triage 
mask triage mask triage mask one-bit mask    NL-1xNL-1 N2xN2 block NxN block NxN grid of block of 
pixels of pixels of pixels raster samples Figure 2: Schematic diagram of a pyramid of × NNmasks with 
Llevels for an image with × NNoversampling at each pixel. This coverage pyramid is built from triage 
masks, except at the .nest level where a conventional one-bit coverage mask is associated with each pixel. 
In this hierarchical representation of the screen, the Cand Vbits for each subcell in triage masks indicate 
whether a square region of the screen is covered, vacant, or active. At the coarsest level, a single 
triage mask represents the whole screen (left), and at the .nest level, a single one-bit mask represents 
the raster samples within a pixel (right). A four-level pyramid of 8×8 masks corresponds to a 512×512 
image with 8×8 oversampling at each pixel. The corresponding diagram for a point-sampled image is the 
same, except that the masks represent an NN× block of 22 pixels, an NN× block of pixels, and so forth. 
 3.1 Tiling by Recursive Subdivision Now that the primitive tiling and visibility operations have been 
described, we are ready to outline the recursive proce­dure for tiling a convex polygon into the coverage 
pyramid. Initially, the masks in the coverage pyramid are a hierar­chical representation of regions of 
the image raster that are already occupied by previously tiled polygons. To make the discussion more 
concrete, the following outline assumes 8 ×8 oversampling and .ltering. To tile polygon P, we begin by 
.nding the triage mask for each of its edges that crosses the screen by .nding its intercepts on the 
screen border and looking up the corre­sponding mask in a precomputed table. Then we composite the edge 
masks according to operation (1) above in order to construct P s triage mask. Next, beginning at the 
root cell of the coverage pyramid, we composite P s mask with cells in the pyramid, using triage mask 
operations to distinguish three classes of cells: where P is entirely hidden, where P is entirely visible, 
and where P s visibility is uncertain, i.e. active cells (operation (2)). We ignore cells where P is 
entirely hidden, we display (or tag) cells where P is entirely visible (mask W), and we recursively subdivide 
active cells (mask A). During subdivision, edge intercepts used to look up edge masks are computed incrementally. 
In regions of the screen where P s edges cross vacant or active cells, sub­division continues, ultimately 
down to all vacant and active pixels crossed by P s edges. At the pixel level, coverage masks in the 
pyramid are conventional one-bit masks. If we are box .ltering, operations may follow the traditional 
A­bu.er algorithm: we .nd P s visible samples, compute their contribution to pixel value and add it to 
the accumulation bu.er, and then update the pixel s coverage mask. If the status of a pixel changes from 
vacant or active to active or covered, the status of masks in coarser levels of the pyra­mid may also 
change, so whenever this occurs, we propagate coverage information to coarser levels by performing simple 
bit-mask operations during recursive traversal of the pyra­mid. When this recursive tiling procedure 
.nishes, all visible samples on P have been tiled and the coverage pyramid has been updated. This procedure 
is outlined in LISTING 1 . 4 RENDERING A SCENE Now that the procedure for tiling a polygon has been de­scribed, 
we are ready to place it in the context of rendering a frame. But .rst we describe the underlying data 
structures: the coverage pyramid, the image array, and the model tree. 4.1 Data Structures To permit 
Warnock subdivision to be driven by bit-mask op­erations, we maintain visibility information about previously 
tiled polygons in an image-space pyramid of coverage masks. As schematically illustrated in .gure 2, 
a single triage mask represents the whole screen, triage masks at the next level of the pyramid correspond 
to subcells in the root mask, and so forth. Thus, this coverage pyramid is a hierarchical rep­resentation 
of the screen with the C and V bits for each subcell in the triage masks indicating whether a square 
re­gion of the screen is covered , vacant , or active. Within a covered region, all corresponding samples 
in the underlying image raster are covered, within a vacant region, all corre­sponding raster samples 
are vacant, and within an active region, at least one but not all corresponding raster samples are covered. 
At the .nest level of the pyramid only, we use LISTING 1 (pseudocode) /* Recursive subdivision procedure 
for tiling a convex polygon P. After clipping P to the near clipping plane in object space, if necessary, 
and projecting P s vertices into the image plane, we call tile_poly with the root mask of the mask pyramid, 
P s edge list, and "level" set to 1. arguments: (Cc,Cv): pyramid mask (input and output) edge_list: 
P s edges that intersect pyramid mask level: pyramid level: 1 is root, 2 is next coarsest, etc. */ tile_poly((Cc,Cv), 
edge_list, level) { set active_edge_list to nil /* build P s mask (Pc,Pv) */ Pc = all_ones Pv = all_zeros 
for each edge on edge_list { find intercepts on square perimeter of mask if square is outside edge then 
return /* polygon doesn t intersect mask */ if edge intersects square, then {  append edge to active_edge_list 
/* Note: at the pixel level, Ec is a conventional coverage mask and Ev = ~Ec */ look up edge mask (Ec,Ev) 
Pc = Pc&#38; Ec Pv = Pv| Ev } } /* make "write" bit mask and update pyramid mask */ W=Cv &#38; Pc Cc 
= Cc | W Cv = Cv &#38; ~W if level is the pixel level, then { /* filter pixel using coverage mask W 
*/ /* to perform A-buffer box filtering: add bitcount*color to accumulation buffer */ evaluate shading 
and update accumulation buffer return } for each TRUE bit in W { for each pixel in this square region 
of screen { /* to perform A-buffer box filtering: add 64*(polygon color) to accumulation buffer */ evaluate 
shading and update accumulation buffer } } /* Recursive Subdivision */ /* make "active" bit mask */ A 
= ~(W | Pv | Cc) /* subdivide active subcells */ for each TRUE bit in A { /* call corresponding subcell 
S call its pyramid mask (Sc,Sv) */ copy all edges on active_edge_list that intersect S to S_edge_list 
tile_poly((Sc,Sv), S_edge_list, level+1) /* propagate coverage status to coarser levels of mask pyramid 
*/ if Sc is all_ones then Cc = Cc | active_bit /* set covered status */ if Sv is not all_ones then Cv 
= Cv &#38; ~active_bit /* clear vacant status */ } } conventional one-bit coverage masks to indicate 
whether or not point samples in the image raster have been covered. If we are oversampling and .ltering, 
each of these one-bit masks corresponds to the 8 ×8 grid of raster samples within a pixel. The appropriate 
pyramid for a 512 ×512 image with 8×8 oversampling at each pixel has four levels, three arrays of triage 
masks with dimensions 1 ×1, 8 ×8, and 64 ×64, and one 512 ×512 array of one-bit masks. Alternatively, 
if we are point sampling rather than .ltering, each one-bit mask cor­responds to an 8 ×8 block of pixels. 
In this case, the pyramid for a 512 ×512 image would have two arrays of triage masks with dimensions 
1 ×1 and 8 ×8, and one 64 ×64 array of one­bit masks. Memory requirements for the coverage pyramid are 
very modest. Since the .nest level requires only one bit per raster sample and the vast majority of cells 
in the pyramid are in the .nest level, total memory requirements are only slightly more than one bit 
per raster sample. The actual number of bits per raster sample required for an n-level pyramid lies in 
the range [1 1/32 1 2/63) for n >1. Note that a z-bu.er requires a great deal more memory because it 
stores a depth value for each raster sample. The other image-space data structure that our algorithm 
requires is an image array with an element for each color component at each pixel. If we perform A-bu.er-style 
.lter­ing [Carpenter84], shading contributions from 64 subpixel samples accumulate in each array element. 
Thus, elements in this accumulation bu.er require considerable depth. We use 16 bits per pixel per color 
channel. When .ltering with a convolution kernel that overlaps multiple pixels, we store color components 
as .oating-point values in the accumula­tion bu.er. If no .ltering is performed, pixel values do not 
accumulate, so a conventional image array is employed. Now for representing the model. Our algorithm 
requires front-to-back traversal of polygons in the scene, so we rep­resent the scene as a binary space 
partitioning tree (BSP tree) [Fuchs-Kedem-Naylor80], which permits very e.cient traversal in depth order. 
Strategies for handling dynamic scenes are discussed in §6.  4.2 Precomputation Step In a precomputation 
step, we build a BSP tree for the model. We also build lookup tables for both conventional and triage 
coverage masks. In building mask tables, we divide the perimeter of a canonical square into some number 
of equal intervals (e.g. 64) and create an entry in a two-dimensional table for each pair of intervals 
not lying on a common edge. Once this table has been constructed, to obtain the mask for an arbitrary 
edge we determine which two intervals it crosses and look up the corresponding table entry. To conserve 
stor­age, we can use the same table entry for edges with opposite directions, because complementing the 
( CV, ) bit masks in a triage mask corresponds to reversing an edge. Hierarchical tiling depends on accurate 
classi.cation of vacant and cov­ered regions in triage masks, so we construct them with the following 
conservative procedure. The endpoints of the pair of intervals used to index a coverage mask de.ne a 
quadri­lateral. Any subcells intersected by the quadrilateral are classi.ed active, guaranteeing that 
cells classi.ed covered are completely covered and cells classi.ed vacant are completely vacant. 4.3 
Generating a Frame We begin a frame by clearing the accumulation bu.er and the coverage pyramid. We traverse 
polygons in the model s BSP tree in front-to-back order. We clip each polygon to the front clipping plane, 
if necessary, and project its vertices into the image plane. There is no need to preserve depth infor­mation. 
Before tiling a polygon, we .rst determine whether its bounding box is visible. If this procedure fails 
to prove that the polygon is hidden, we then tile it into the smallest enclosing cell in the coverage 
pyramid using the procedure outlined in LISTING 1 . In regions of the screen where the polygon is visible, 
this procedure updates pixel values in the image bu.er and updates coverage status in the coverage pyramid. 
After all polygons have been processed, the scene is complete and we display the image bu.er. 4.4 Other 
Filtering Methods We have already discussed A-bu.er-style .ltering by area sampling, a term used to describe 
convolution of visible sam­ples with a pixel-sized box .lter [Catmull78]. Abram, West­over, and Whitted 
extended coverage-mask techniques to include jitter, table-driven convolution with arbitrary .lter kernels, 
and evaluation of simple shading functions by table lookup [Abram-et-al85]. All of these methods are 
compati­ble with hierarchical tiling. To perform table-driven convo­lution, the contribution of each 
subpixel sample to neighbor­ing pixels is precomputed and stored in a table of .ltering coe.cients. For 
some simple shading functions, the con­tribution of arbitrary collections of samples can be stored as 
precomputed coe.cients which enables, for example, ef­.cient byte-by-byte processing of coverage masks. 
We use this method when .ltering 3 ×3 pixel neighborhoods with a one-pixel radius cosine-hump kernel. 
 4.5 Point Sampling Modifying the algorithm to produce point-sampled rather than .ltered images is straightforward. 
In this case, each mask at the .nest level of the pyramid corresponds to an 8×8 block of pixels. So for 
each TRUE subcell in the W mask (see pseudocode), we evaluate the shading function at the corresponding 
pixel and write the result to the image bu.er. Since pixel values correspond to point samples, color 
values do not accumulate, so we use a conventional image array rather than an accumulation bu.er. Note 
that it is not necessary to clear the image array at the beginning of a frame. Instead, after tiling 
all scene polygons, we composite a screen-sized polygon of the desired background color (or texture) 
with the root mask, thereby tiling all remaining vacant pixels in the image. 5 HIERARCHICAL OBJECT-SPACE 
CULLING Because of its ability to cull hierarchically in image space, the hierarchical tiling algorithm 
processes densely occluded scenes much more e.ciently than conventional tiling meth­ods, which must traverse 
all hidden geometry pixel by pixel. Nonetheless, it must still consider every polygon in a scene, doing 
some work even on those that are entirely hidden. To avoid this behavior, we integrate our algorithm 
with the hierarchical visibility algorithm [Greene-Kass-Miller93, Greene-Kass94, Greene95] to enable 
hierarchical object­space culling of hidden regions of the model. This can be done by substituting hierarchical 
tiling for z-bu.ering in the hierarchical z-bu.er algorithm of [Greene-Kass-Miller93], al­though this 
requires some changes in both the object-space and image-space hierarchies. In image space, instead of 
us­ing a z-pyramid of depth samples to maintain visibility in­formation, we use a coverage pyramid. In 
object space, we modify the octree to permit strict front-to-back traversal of polygons. Note that the 
z-bu.er algorithm traverses oc­tree cubes in front-to-back order, but not the primitives con­tained within 
them. And since octree cubes are nested, it is not su.cient to simply organize the primitives inside 
each cube into a BSP tree. Instead we use the following algo­rithm for building an octree of BSP trees 
that permits strict front-to-back traversal. 5.1 Building an Octree of BSP Trees Starting with a root 
cube which bounds model space, we insert polygons one at a time into the cube. If the polygon count in 
the cube reaches a speci.ed threshold (e.g. 30), we subdivide the cube into eight octants and insert 
each of its polygons into each octant that it intersects, clipping to the cube s three median planes. 
When all polygons in the scene have been inserted into the root cube and propagated through the tree, 
we have an octree where all polygons are associated only with leaf nodes, thereby circumventing the ordering 
problem caused by nesting. The last step is to orga­nize the polygons in each leaf node of the octree 
into a BSP tree [Foley-et-al90]. Now scene polygons can be traversed in strict front-to-back order by 
traversing octree cubes front to back and traversing their BSP trees front to back.  5.2 Combining Hierarchical 
Tiling with Hierarchical Visibility Now that we have established how to traverse scene polygons in front-to-back 
order, combining hierarchical tiling with the basic hierarchical visibility algorithm is straightforward. 
As with hierarchical z-bu.ering, we traverse octree cubes in front-to-back order, testing them for visibility 
and culling those that are hidden. As with hierarchical z-bu.ering, we determine whether a cube is visible 
by tiling it, stopping if a visible sample is found. Note that it is only necessary to tile a cube s 
polygonal silhouette (unless it intersects the front clipping plane), rather than tiling its front faces. 
By comparison, z-bu.ering often needs to tile three faces of a cube to establish its visibility. To test 
cube silhouettes for visibility, we modify the tiling procedure of LISTING 1 to re­port visibility status, 
returning TRUE whenever a polygon s mask indicates that it covers a vacant subcell or a vacant grid point 
in the image raster. Once we have established that an octree cube is visible, we traverse the polygons 
in its BSP tree in front-to-back order, tiling them into the cov­erage pyramid. When we .nish traversing 
the octree, all visible polygons have been tiled and the image is complete. This version of the hierarchical 
visibility algorithm has very e.cient traversal properties in both object-space and image-space. Like 
the hierarchical z-bu.er algorithm, in ob­ject space the algorithm only visits visible octree nodes and 
their children, and it only renders polygons that are in visible octree nodes. In image space, when tiling 
polygons into the coverage pyramid, hierarchical tiling only visits cells that are crossed by visible 
edges in the output image. Visible sam­ples are never overwritten. As a result of these properties, this 
variation of the hierarchical visibility algorithm is very e.cient at both culling hidden geometry and 
tiling visible geometry. If a hardware graphics accelerator is available to perform shading operations 
such as texture mapping, we can per­form visibility operations with software and shading with hardware. 
We would use the usual hierarchical tiling algo­rithm to maintain the coverage pyramid and perform object­space 
culling, and we would render visible polygons with the graphics accelerator, using an accumulation bu.er, 
if available, to perform antialiasing [Haeberli-Akeley90]. This would be a fast way to produce texture-mapped 
images of densely occluded scenes. 6 HANDLING DYNAMIC SCENES One weakness of the hierarchical tiling 
algorithm is that it requires strict front-to-back traversal of polygons. This does not present a problem 
for a static model, since it may be represented as a BSP tree [Fuchs-Kedem-Naylor80], and if only a relatively 
small number of polygons are moving, the tree can be e.ciently maintained [Naylor92a]. However, in scenes 
with numerous moving polygons, maintaining depth order can impose a severe computational burden. Here 
we consider two di.erent methods that address this problem. 6.1 Lazy Z-Bu.ering The following lazy z-bu.ering 
algorithm is an attractive alternative whenever at least part of the model can be con­veniently traversed 
in approximate front-to-back order. For convenience, the following discussion assumes that we are oversampling 
and box-.ltering. With this variation of hier­archical tiling, we make the following changes to the basic 
algorithm. For every cell in the coverage pyramid, we main­tain znear and zfar depth values for all potentially 
visible polygons thus far encountered that intersect the cell. In­stead of automatically culling a portion 
of a polygon that intersects a covered cell, it is culled only if it lies behind the cell s zfar value. 
At a pixel, we assume that fragments arrive in an order that permits tiling with coverage masks, i.e., 
one or more non-overlapping fragments cover all of the pixel s samples before any other fragments arrive. 
These conditions are easily monitored using the pixel s coverage mask and znear/zfar values. Unless and 
until a fragment violating the conditions arrives, we perform .ltering like the usual algorithm, adding 
shading contributions to the accu­mulation bu.er and updating the pixel s coverage mask. We also cache 
information about each fragment in case we need it later. If and when the conditions are violated, we 
discard the current accumulated color value for the pixel and revert to ordinary z-bu.ering, allocating 
the memory required for storing color and depth at each subpixel sample, and then tiling the cached fragments. 
This produces the same image samples as if we had been maintaining an oversampled z­bu.er all along. 
The last step after all polygons in the scene have been tiled is to .lter the z-bu.ered pixels. This 
pro­cedure produces the same image as hierarchical tiling would have produced if polygons had been traversed 
in depth order. This simple strategy exploits whatever depth coherence is in the scene being processed. 
If polygons are mostly in front­to-back order, lazy z-bu.ering will not do much more work than the usual 
hierarchical tiling algorithm. This would oc­cur, for example, if a few small dynamic objects were posi­tioned 
in front of a static background model that was tra­versed in depth order. In the worst case, when frontmost 
objects are never processed .rst, lazy z-bu.ering does only slightly more work than hierarchical z-bu.ering. 
 Thing Being Compared Hierarchical Tiling Hierarchical Z-Bu.ering object-space hierarchy BSP tree / octree 
of BSP trees octree image-space hierarchy pyramid of coverage masks z-pyramid front-to-back polygon traversal 
required? yes no visibility information per raster sample < 2/631 coverage-mask bits (usu. 24-32 bits) 
Z color information per raster sample none (usu. 24-36 bits) RGB type of output-image bu.er accumulation 
(deep) standard need to store coverage-mask LUTs? yes no pixel overwrite? no yes mask support for .ltering 
built-in? yes no covered identi.es image-pyramid cells? yes no Table 1: Some points of comparison between 
hierarchical polygon tiling and hierarchical z-bu.ering. 6.2 Merging Octrees For polygonal scenes consisting 
of independently moving rigid bodies, another strategy can be employed that guar­antees front-to-back 
traversal of polygons, permitting us to render polygons with the standard hierarchical tiling proce­dure. 
According to this method, each rigid body is repre­sented as an octree of BSP trees. To render a frame, 
we simultaneously traverse all octrees front to back, culling any octree cubes which are hidden by the 
coverage pyramid, and using the following strategy to synchronize traversal of oc­trees. For each octree, 
we determine the current frontmost leaf cube and then determine the frontmost leaf cube of all octrees. 
If this single frontmost cube does not intersect a leaf cube in any other octree, we can safely render 
its BSP tree. If this cube does intersect other leaf cubes, we clip their poly­gons to the frontmost 
cube, insert the clipped fragments into the frontmost cube s BSP tree, and then render that BSP tree. 
This procedure ultimately will cull or render all oc­tree leaf nodes, whereupon rendering of the scene 
is .nished. This procedure for rendering dynamic scenes is nearly as fast as the standard hierarchical 
tiling algorithm, except for the time spent merging octree leaf nodes. Although merging operations can 
require considerable computation, for many scenes merging will only rarely be required, and in such cases 
this algorithm will run e.ciently. 7 HIERARCHICAL TILING VERSUS HIERARCHI-CAL Z-BUFFERING Table 1 summarizes 
some points of comparison between hi­erarchical polygon tiling and hierarchical z-bu.ering. As the table 
points out, hierarchical tiling requires strict front-to­back traversal of polygons, which complicates 
the object­space hierarchy, assuming that we are maintaining an octree of BSP trees to enable object-space 
culling. Another point in favor of hierarchical z-bu.ering is that it does not need to build or store 
lookup tables for coverage masks. The other points of comparison strongly favor hierarchical tiling. 
One big advantage is that its memory requirements are much less. Whereas hierarchical z-bu.ering needs 
to store depth and color information for each raster sample, hierarchical tiling only needs to store 
slightly more than one bit of coverage information for each raster sample. The resulting memory savings 
can be very substantial. In fact, if we are rendering a 512×512 image with 8 ×8 oversampling at each 
pixel, hierar­chical tiling requires only about 3.7% of the image memory required for z-bu.ering. Other 
points in favor of hierar­chical tiling are that it never overwrites visible samples, it has built-in 
support for .ltering with coverage masks, and it facilitates exploiting image-space coherence by identify­ing 
regions of the image-space pyramid that are completely covered by individual polygons. 8 TILING POLYHEDRA 
Hierarchical tiling with coverage masks can also be applied to Warnock subdivision in three dimensions 
to tile convex polyhedra into a voxel grid. In this case, 64-bit triage masks would classify cells within 
a 4 ××4 4 subdivision of a cube as inside, outside, or intersecting a plane. The triage mask for a convex 
polyhedron within a cube would be obtained by compositing the triage masks of its face planes. The re­cursive 
subdivision procedure for tiling a polyhedron into a 3D pyramid of coverage masks would be analogous 
to hier­archical polygon tiling, and it would only visit cells in the pyramid that are intersected by 
the polyhedron s faces. The speed and modest memory requirements of this volume tiling algorithm make 
it an attractive alternative to traditional methods [Kaufman86]. 9 IMPLEMENTATION AND RESULTS Our implementation 
of hierarchical polygon tiling is pro­grammed in C and renders either point-sampled or .ltered images 
of scenes composed of .at-shaded convex polygons. Our polygon tiling program follows the pseudocode outline, 
except that we tile a polygon into the smallest enclosing cell in the coverage pyramid after .rst testing 
its bounding box for visibility, as described in §4.3. As described in §3 and §4, .ltering is performed 
by box .ltering according to the A-bu.er method, or by table-driven convolution with a one­pixel radius 
cosine-hump kernel. In the latter case, kernel coe.cients are precomputed for all byte patterns and ac­cessed 
by table lookup for each non-zero byte within a poly­gon s coverage mask at a pixel. Color components 
in the accumulation bu.er are represented as 16-bit integer val­ues when box .ltering, and as 32-bit 
.oating-point values when .ltering with a cosine-hump kernel. Tables of cover­age masks are constructed 
with 64 intervals along each edge of the bounding square. One-bit coverage masks for .ltering pixels 
are constructed with jitter, using random placement of raster samples within the corresponding sub-pixel 
square [Dipp´e-Wold85, Cook86]. All of the following tests were per­formed on a SGI Indigo2 with a 75 
megahertz R8000 pro­cessor, which performs atomic 64-bit mask operations. To compare the e.ciency of 
hierarchical tiling to tradi­tional incremental scan conversion for tiling simple polyg­onal scenes, 
we employed the color-cube model of .gure 5, composed of 192 presorted front-facing squares. We ren­dered 
this model with hierarchical tiling and with a back­to-front painter s algorithm [Foley-et-al90]. The 
painter s algorithm maintained a color triplet for each point in the image raster and performed tiling 
by incremental scan con­version, overwriting the image at every pixel encountered. On a 512 ×512 grid, 
hierarchical tiling tiled the color-cube model approximately ten percent faster than the painter s algorithm 
(.087 seconds versus .097 seconds). At higher res­olution, the speed advantage of hierarchical tiling 
was much more pronounced. For example, hierarchical tiling took .357 seconds to tile the model on a 4096 
×4096 grid and produce the 512 ×512 box-.ltered image of .gure 5. By comparison, the painter s algorithm 
took 5.3 times longer (1.91 seconds) to tile this scene on a 2048 ×2048 grid without .ltering (our Indigo 
didn t have enough memory to render a 4096 ×4096 RGB image). By timing the painter s algorithm at various 
resolutions, we found that it was only able to tile a 910 ×910 grid in the .357 seconds it took hierarchical 
tiling to tile and .lter the image of .gure 5. This example illustrates that for software tiling at su.cient 
resolution to enable high-quality antialiasing by oversampling and .ltering, hierarchical tiling is much 
more e.cient than traditional incremental scan con­version, even for simple scenes. To test the e.ectiveness 
of hierarchical tiling on densely occluded scenes we integrated hierarchical tiling with hi­erarchical 
visibility as described in §5, performing tiling of both model polygons and octree-cube silhouettes with 
the hierarchical tiling method. For a test model, we used a version of the modular o.ce building described 
in [Greene-Kass-Miller93]. We built an octree of BSP trees for the repeating module using the method 
described in §5.1, each BSP tree containing approximately 16,000 quadrilat­erals. We replicated this 
octree within the shell of a 408­story building resembling the Empire State Building to cre­ate a model 
consisting of approximately 167 million repli­cated quadrilaterals. Figures 4, 6, and 7 show various 
views of this model. To compare the relative speed of hierarchical tiling and hi­erarchical z-bu.ering, 
we rendered animation of a building walk-through. We found that hierarchical tiling was able to perform 
tiling on a 4096 ×4096 grid and produce box-.ltered 512×512 frames as fast as hierarchical z-bu.ering 
produced 512×512 point-sampled frames. On viewing the animation produced with the z-bu.er algorithm, 
we observed consider­able aliasing as expected. By comparison, we observed high­quality antialiasing 
with the box-.ltered animation gener­ated with hierarchical tiling. Next, we compared the speed of various 
rendering options. Hierarchical tiling took 3.21 seconds to tile the scene of .gure 4 on a 4096 ×4096 
grid and produce the pictured box-.ltered image. When we rendered this same box-.ltered image with­out 
using the bounding-box culling method of §4.3 and in­stead tiled all polygons into the root cell of the 
coverage pyramid, rendering took 1.28 seconds longer, indicating that the bounding-box culling strategy 
provides signi.cant accel­eration. Next we rendered the scene of .gure 4 with higher­quality antialiasing 
using cosine-hump .ltering within a 3 ×3­pixel neighborhood. With this .ltering method, it took 4.42 
seconds to tile the scene on a 4096 ×4096 grid and produce the .ltered image. Finally, we used the point-sampling 
vari­ation of hierarchical tiling to render a 512 ×512 image of the scene, which took 1.71 seconds. To 
compare the algorithmic e.ciency of hierarchical tiling to hierarchical z-bu.ering, we constructed work 
images that show the number of times during frame generation that each cell in the coverage pyramid is 
visited (not count­ing subpixel samples), with an access to a coarser-than­pixel cell being amortized 
over the corresponding window of the screen. 2 Work images show the depth complex­ity of the visibility 
computation and indicate where the algorithm is working hardest [Greene95, Greene-Kass94, Greene-Kass-Miller93]. 
An average intensity of one in a work image means that, an average, only a single pyramid cell is accessed 
in the coverage pyramid during visibility operations for each pixel in the output image. With hierarchical 
tiling, except for very complex scenes or .nely tessellated models, average intensity is usually less 
than one because visibility at most pixels is established at a coarser level in the hierarchy. For example, 
for the simple model of .gure 5, an average of only .123 cells in the coverage pyramid are traversed 
per pixel in the output image. Figure 3 shows log-scale work images corresponding to .gure 4. Left to 
right, the images show work tiling cube sil­houettes during visibility tests (.09 cells visited per pixel, 
on average), work tiling model polygons into the coverage pyra­mid (1.01 cells visited per pixel, on 
average), and the sum of these two images, showing total work performed on tiling (1.10 cells visited 
per pixel, on average). In other words, hierarchical tiling visited an average of only 1.10 cells in 
the coverage pyramid for each pixel in the 512 ×512 output image, even though tiling and .ltering were 
performed on a 4096 ×4096 grid. Far fewer cells in the image pyramid are visited with hierarchical tiling 
than with hierarchical z­bu.ering [Greene95] because it only visits cells in the image pyramid that are 
crossed by visible edges in the output im­age. When we performed a motion test on the scene of .gure 
4, we found that the number of pyramid cells visited was ap­proximately one for frames rendered with 
hierarchical tiling and approximately three for frames rendered with hierar­chical z-bu.ering. The lower 
.gure for hierarchical tiling is particularly impressive considering that it resolves visibility at 64 
times as many raster samples and many more poly­gons are visible. Of course the depth complexity of visibility 
computations for the scene of .gure 4 is far lower for both hierarchical tiling and hierarchical z-bu.ering 
than for naive z-bu.ering, which visits each pixel dozens of times on aver­age [Greene95]. To explore 
the limits of hierarchical tiling to e.ectively .lter images of very complex scenes, we rendered a motion 
sequence in which the camera .ies around and through the 408-story model of the Empire State Building 
[Greene96]. Figures 6 and 7 are 512 ×512 frames from this animation, which was produced by tiling on 
a jittered 4096 ×4096 grid and .ltering with a cosine-hump kernel as previously de­scribed. From the 
viewpoint of .gure 6, this scene poses a formidable challenge to e.ective .ltering, since approx­imately 
765,000 polygons are visible, and dozens of poly­gons are visible within some pixels. Nonetheless, we 
ob­served high-quality antialiasing in the motion sequence. Jit­tering of sub-pixel samples e.ectively 
converted aliasing to noise [Dipp´e-Wold85, Cook86], which was noticeable only in frames having hundreds 
of thousands of visible polygons. For this motion sequence, we observed subtle patterned aliasing artifacts 
when the same jitter pattern was employed at all pixels, a problem that was overcome by using several 
dif­ferent jitter patterns. To reduce temporal aliasing we ren­dered each video .eld separately and to 
reduce .icker we 2Our accounting of work done on visibility does not include clearing of the coverage 
pyramid. Clearing the pyramid at the beginning of a frame visits each pyramid cell once, but this is 
not necessary if a lazy clearing strategy is employed. applied a 1-4-6-4-1 .lter to every other scanline 
(not applied to .gure 6 or 7). With cosine-hump .ltering and multiple jitter patterns, rendering times 
were 5.15 minutes for .g­ure 6 and 34 seconds for .gure 7, which has approximately 81,300 visible polygons. 
We also recorded the motion se­quence with box .ltering, but observed noticeably worse im­age quality, 
particularly the characteristic ropyness of area sampling. When box .ltering with a single jitter pattern, 
rendering time for the scene of .gure 6 was 4.28 minutes. We also rendered the version of this model 
shown in .gure 1 of [Greene-Kass94], which took the error-bounded render­ing algorithm described in that 
article one hour to produce on a 50-megahertz workstation. By comparison, this same scene took hierarchical 
tiling 34 seconds to render. These examples illustrate that hierarchical tiling with object-space culling 
can produce high-quality animation of very complex scenes in reasonable frame times. Adding a level to 
the cov­erage pyramid would permit the algorithm to accurately .l­ter even more complex scenes. 10 CONCLUSION 
Warnock subdivision with its elegant simplicity and logarith­mic search properties endures as one of 
the great computer­graphics algorithms. Although polygon tiling by Warnock subdivision is well known, 
it has rarely been used in practice due to the ine.ciency of the traditional subdivision proce­dure. 
Here we have shown that Warnock-style subdivision can be driven very e.ciently with triage coverage masks 
. The resulting hierarchical polygon tiling algorithm is very e.cient, visiting only cells in the image 
hierarchy that are crossed by visible edges in the output image and never over­writing a visible image 
sample. At high resolution, hier­archical tiling is much faster than traditional incremental scan conversion, 
so it is well suited to antialiasing by over­sampling and .ltering. Moreover, hierarchical tiling with 
object-space culling can process densely occluded scenes ex­tremely e.ciently, considerably faster than 
hierarchical z­bu.ering, while facilitating high-quality .ltering. Although the practicality of the basic 
algorithm for dynamic scenes is constrained by the requirement that polygons be traversed front to back, 
whenever at least part of the model can be tra­versed in approximate front-to-back order, lazy z-bu.ering 
helps to overcome this shortcoming. The algorithm is com­pact, straightforward to implement, and has 
very modest memory requirements. In short, hierarchical tiling o.ers the prospect of generating high-quality 
animation at reasonable frame rates with modest computing resources. 11 Acknowledgments Gavin Miller 
suggested the method of combining hardware and software tiling discussed in §5.2. Gavin also contributed 
to mak­ing the test model of .gure 4, as did Eric Chen and Steve Rubin. A discussion with Piter van Zee 
was my impetus for working out the algorithm for merging octrees presented in §6.2. Finally, I gratefully 
acknowledge Siggraph reviewer #4 for a thoughtful cri­tique of this article. References [Abram-et-al85] 
G. Abram, L. Westover, and T. Whitted, E.cient Alias-Free Rendering using Bit-Masks and Look-Up Tables, 
Proceedings of SIGGRAPH 85 , July 1985, 53 59. [Carpenter84] L. Carpenter, The A-bu.er, an Antialiased 
Hidden Surface Method, Proceedings of SIGGRAPH 84 , July 1984, 103 108. [Catmull74] E. Catmull, A Subdivision 
Algorithm for Computer Display of Curved Surfaces, PhD Thesis, Report UTEC-CSc­74-133, Computer Science 
Dept., University of Utah, Salt Lake City, Utah, Dec. 1974. [Catmull78] E. Catmull, A Hidden-Surface 
Algorithm with Anti-Aliasing, Proceedings of SIGGRAPH 78 , Aug. 1978, 6 11. [Cook86] R. Cook, Stochastic 
Sampling in Computer Graphics, ACM Transactions on Graphics , Jan. 1986, 51 72. [Dipp´e-Wold85] M. A. 
Z. Dipp´e and E. H. Wold, Antialiasing through Stochastic Sampling, Proceedings of SIGGRAPH 85, July 
1985, 69 78. [Fiume-et-al83] E. Fiume, A. Fournier, and L. Rudolph, A Parallel Scan Conversion Algorithm 
with Anti-Aliasing for a General-Purpose Ultracomputer, Proceedings of SIGGRAPH 83 , July 1983, 141 150. 
[Fiume91] E. Fiume, Coverage Masks and Convolution Tables for Fast Area Sampling, Graphical Models and 
Image Processing , 53(1), Jan. 1991, 25 30. [Foley-et-al90] J. Foley, A. van Dam, S. Feiner, and J. Hughes, 
Com­puter Graphics, Principles and Practice , 2nd edition, Addison-Wesley, Reading, MA, 1990. [Fuchs-Kedem-Naylor80] 
H. Fuchs, J. Kedem, and B. Naylor, On Visible Surface Generation by a Priori Tree Structures, Pro­ceedings 
of SIGGRAPH 80 , June 1980, 124 133. [Greene-Kass-Miller93] N. Greene, M. Kass, and G. Miller, Hier­archical 
Z-Bu.er Visibility, Proceedings of SIGGRAPH 93 , July 1993, 231 238. [Greene-Kass94] N. Greene and M. 
Kass, Error-Bounded An­tialiased Rendering of Complex Environments, Proceedings of SIGGRAPH 94 , July 
1994, 59 66. [Greene95] N. Greene, Hierarchical Rendering of Complex Environ­ments, PhD Thesis, Univ. 
of California at Santa Cruz, Report No. UCSC-CRL-95-27, June 1995. [Greene96] N. Greene, Naked Empire, 
ACM Siggraph Video Re­view Issue 115: The Siggraph 96 Electronic Theater, August 1996. [Haeberli-Akeley90] 
P. Haeberli and K. Akeley, The Accumulation Bu.er: Hardware Support for High-Quality Rendering, Pro­ceedings 
of SIGGRAPH 90 , Aug. 1990, 309 318. [Kaufman86] A. Kaufman, 3D Scan Conversion Algorithms for Voxel-Based 
Graphics, Proceedings of the 1986 Workshop on Interactive 3D Graphics , Oct. 1986, 45 75. [Meagher82] 
D. Meagher, The Octree Encoding Method for E.cient Solid Modeling, PhD Thesis, Electrical Engineering 
Dept., Rensselaer Polytechnic Institute, Troy, New York, Aug. 1982. [Naylor92a] B. Naylor, Interactive 
Solid Geometry Via Partitioning Trees, Proceedings of Graphics Interface 92 , May 1992, 11 18. [Naylor92b] 
B. Naylor, Partitioning Tree Image Representation and Generation from 3D Geometric Models, Proceedings 
of Graph­ics Interface 92 , May 1992, 201 212. [Rogers85] D. Rogers, Procedural Elements for Computer 
Graphics, McGraw-Hill, New York, 1985. [Sabella-Wozny83] P. Sabella and M. Wozny, Toward Fast Color-Shaded 
Images of CAD/CAM Geometry, IEEE Computer Graphics and Applications , 3(8), Nov. 1983, 60 71. [Teller92] 
S. Teller, Visibility Computations in Densely Occluded Polyhedral Environments, PhD Thesis, Univ. of 
California at Berkeley, Report No. UCB/CSD 92/708, Oct. 1992. [Warnock69] J. Warnock, A Hidden Surface 
Algorithm for Com­puter Generated Halftone Pictures, PhD Thesis, Computer Science Dept., University of 
Utah, TR 4-15, June 1969.  Figure 3: Log-scale work images showing the number of times that cells in 
the coverage pyramid were visited while tiling the frame of .gure 4. These images depict the depth complexity 
of the visibility computation, showing where the algorithm is working hardest. Left: work tiling cubes: 
.09 cells visited per pixel (avg) Middle: work tiling polygons: 1.01 cells visited per pixel (avg) Right: 
total work on tiling: 1.10 cells visited per pixel (avg)  Figure 4: Interior view of the Empire State 
Building model. Hierarchical tiling took 3.21 seconds to tile this scene on a 4096×4096 grid and produce 
this 512 ×512 box-.ltered image (75 Mhz processor).  Figure 5: Hierarchical tiling took .36 seconds 
to tile this simple model on a 4096 ×4096 grid and produce this 512 ×512 box-.ltered image (75 Mhz processor). 
Figure 6: A frame from Naked Empire, animation produced for the Siggraph 96 Electronic Theater [Greene96]. 
The model of this 408-story building consists of approximately 167 million quadrilaterals, 765,000 of 
which are visible in this frame. This 512×512 frame was produced by tiling and .ltering on a jittered 
4096 ×4096 grid. Jitter converted aliasing to noise, which is evident in complex regions of the image. 
Rendering took 5.15 minutes on a 75 Mhz processor. Figure 7: Another frame from Naked Empire. Note that 
the building model has no outer shell, making it possible to see deep inside. Rendering time for this 
frame was 34 seconds (75 Mhz processor).    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237209</article_id>
		<sort_key>75</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Hierarchical image caching for accelerated walkthroughs of complex environments]]></title>
		<page_from>75</page_from>
		<page_to>82</page_to>
		<doi_number>10.1145/237170.237209</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237209</url>
		<keywords>
			<kw><![CDATA[BSP-tree]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[level-of-detail]]></kw>
			<kw><![CDATA[path coherence]]></kw>
			<kw><![CDATA[spatial hierarchy]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Trees</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10003634</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P147447</person_id>
				<author_profile_id><![CDATA[81100530281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shade]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14153841</person_id>
				<author_profile_id><![CDATA[81311486606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dani]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lischinski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045514</person_id>
				<author_profile_id><![CDATA[81100493833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeRose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39079390</person_id>
				<author_profile_id><![CDATA[81100167784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snyder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[John M. Airey, John H. Rohlf, and Frederick E Brooks, Jr. Towards image realism with interactive update rates in complex virtual building environments. Computer Graphics (1990 Symposium on Interactive 3D Graphics), 24(2):41-50, March 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>241060</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bradford Chamberlain, Tony DeRose, Dani Lischinski, David Salesin, and John Snyder. Fast rendering of complex environments using a spatial hierarchy. In Proceedings of Graphics Interface '96, May 1996.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. In Computer Graphics Proceedings, Annual Conference Series, pp. 279-288, August 1993.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[James H. Clark. Hierarchical geometric models for visible surface algorithms. Communications of the ACM, 19(10):547-554, October 1976.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Matthias Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. Multiresolution analysis for arbitrary meshes. In Computer Graphics Proceedings, Annual Conference Series, pp. 173-182, August 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Henry Fuchs, Zvi M. Kedem, and Bruce Naylor. On visible surface generation by a priori tree structures. Computer Graphics, 14(3): 175- 181, June 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166149</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Thomas A. Funkhouser and Carlo H. S6quin. Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. In Computer Graphics Proceedings, Annual Conference Series, pp. 247-254, August 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B. Garlick, D. Baum, and J. Winget. Interactive viewing of large geometric databases using multiprocessor graphics workstations. SIG- GRAPH '90 Course Notes: Parallel Algorithms and Architectures for 3D Image Generation, 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ned Greene, Michael Kass, and Gavin Miller. Hierarchical z-buffer visibility. In Computer Graphics Proceedings, Annual Conference Series, pp. 231-238, August 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15916</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Timothy L. Kay and James T. Kajiya. Ray tracing complex scenes. Computer Graphics, 20(4):269-278, August 1986.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199422</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Daivid Luebke and Chris Georges. Portals and mirrors: Simple, fast evaluation of potentially visible sets. In 1995 Symposium on Interactive 3D Graphics, pp. 105-106, April 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199420</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Paulo W. C. Maciel and Peter Shirley. Visual navigation of large environments using textured clusters. In 1995 Symposium on Interactive 3D Graphics, pp. 95-102, April 1995.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan and Gary Bishop. Plenoptic modeling: An imagebased rendering system. In Computer Graphics Proceedings, Annual Conference Series, pp. 39-46, August 1995.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Jackie Neider, Tom Davis, and Mason Woo. OpengGL Programming Guide. Addison Wesley, 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192192</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Matthew Regan and Ronald Pose. Priority rendering with a virtual reality address recalculation pipeline. In Computer Graphics Proceedings, Annual Conference Series, pp. 155-162, July 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Jarek Rossignac and Paul Borrel. Multi-resolution 3D approximations for rendering complex scenes. Research Report RC 17697 (#77951), IBM, Yorktown Heights, New York 10598, 1992. Also appeared in the IFIP TC 5.WG 5.10.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836035</ref_obj_id>
				<ref_obj_pid>832290</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Gernot Schaufler. Exploiting frame to frame coherence in a virtual reality system. In Proceedings of VRAIS '96, pp. 95-102, April 1996.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Gernot Schaufler and Wolfgang Stiirzlinger. A three dimensional image cache for virtual reality. In Proceedings of Eurographics '96, 1996. To appear.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Seth J. Teller. Visibility Computations in Densely Occluded Polyhedral Environments. PhD thesis, Computer Science Division (EECS), UC Berkeley, Berkeley, California 94720, October 1992. Available as Report No. UCB/CSD-92-708.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237274</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Jay Torborg and Jim Kajiya. Talisman: Commodity realtime 3D graphics for the PC. In Computer Graphics Proceedings, Annual Conference Series, August 1996.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hierarchical Image Caching for Accelerated Walkthroughs of Complex Environments Jonathan Shade Dani 
Lischinski David H. Salesin Tony DeRose John Snydery University of Washington yMicrosoft Research Abstract 
We present a new method that utilizes path coherence to accelerate walkthroughs of geometrically complex 
static scenes. As a prepro­cessing step, our method constructs a BSP-tree that hierarchically partitions 
the geometric primitives in the scene. In the course of a walkthrough, images of nodes at various levels 
of the hierarchy are cached for reuse in subsequent frames. A cached image is reused by texture-mapping 
it onto a single quadrilateral that is drawn in­stead of the geometry contained in the corresponding 
node. Visual artifacts are kept under control by using an error metric that quan­ti.es the discrepancy 
between the appearance of the geometry con­tained in a node and the cached image. The new method is shown 
to achieve speedups of an order of magnitude for walkthroughs of a complex outdoor scene, with little 
or no loss in rendering quality. CR Categories and Subject Descriptors: I.3.3 [Computer Graph­ics]: Picture/Image 
Generation Display algorithms; I.3.7 [Com­puter Graphics]: Three-Dimensional Graphics and Realism. Additional 
Key Words: BSP-tree, image-based rendering, level­of-detail (LOD), path coherence, spatial hierarchy, 
texture map­ping. Introduction Interactive visualization of extremely complex geometric environ­ments 
is becoming an increasingly important application of com­puter graphics. Though the throughput of graphics 
hardware over the past decade has improved dramatically, the demand for perfor­mance continues to outpace 
the supply, as virtual scenes containing many millions of polygons become increasingly common. In order 
to rapidly visualize truly complex scenes, rendering algorithms must intelligently limit the number of 
geometric primitives rendered in each frame. This paper presents a new method for accelerating walkthroughs 
of geometrically complex and largely unoccluded static scenes by hier­archically caching images of scene 
portions. As a viewer navigates through a virtual environment, the appearance of distant parts of the 
scene changes little from frame to frame. We exploit this path co­herence by caching images created in 
one frame for possible reuse in many subsequent frames. fshade jdanix jsalesing@cs.washington.edu derose@pixar.com, 
johnsny@microsoft.com Permission to make digital or hard copies of part or all of this work or personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage and that copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 Our method starts with a preprocessing 
stage. Given an unstructured set of objects comprising a scene, we construct a BSP-tree [6] by placing 
splitting planes inside gaps between objects. This construc­tion produces a hierarchical spatial partitioning 
of the scene with geometry stored only at the leaves of the hierarchy. During a walk­through of the scene, 
our method traverses the hierarchy and caches images of nodes at various levels to be reused in subsequent 
frames. An error metric that quanti.es the discrepancy between the appear­ance of the actual geometry 
contained in a node and its cached im­age is used to estimate the number of frames for which the cached 
image is likely to provide an adequate approximation of the node s contents. A simple cost-bene.t analysis 
is performed at each node in order to decide whether or not an image should be cached. The main contribution 
of our approach is the successful combina­tion of two powerful paradigms: hierarchical methods and image­based 
rendering. Image-based rendering is capable of drawing arbi­trarily complex objects in constant time, 
once the image is created. Using a hierarchy of images leverages the power of image-based rendering by 
signi.cantly reducing the number of images that must be drawn. Another contribution is the introduction 
of a new simple error metric that provides automatic quality control. 1.1 Previous work Previous work 
on accelerating the rendering of complex environ­ments can be classi.ed into three major categories: 
visibility culling, level-of-detail modeling, and image-based rendering. Visibility culling Visibility 
culling algorithms attempt to avoid drawing objects that are not visible in the image. This approach 
was .rst investigated by Clark [4], who used an object hierarchy to rapidly cull surfaces that lie outside 
the viewing frustum. Garlick et al. [8] applied this idea to spatial subdivisions of scenes. View-frustum 
culling techniques are most effective when only a small part of the scene s geometry is inside the view 
frustum at any single frame. In a complex environ­ment enough geometry remains inside the view frustum 
to overload the graphics pipeline, and additional acceleration techniques are re­quired. Airey et al. 
[1] and Teller [19] described methods for interactive walkthroughs of complex buildings that compute 
the potentially vis­ible set of polygons for each room in a building. Only the potentially visible set 
of polygons for the room currently containing the viewer needs to be rendered at each frame. Both of 
these methods require a lengthy preprocessing step for large models. More recently, Lue­bke and Georges 
[11] developed a dynamic version of this algorithm that eliminates the preprocessing. Such methods can 
be very effec­tive for densely occluded polyhedral environments, such as build­ing interiors, but they 
are not suited for mostly unoccluded outdoor scenes. The hierarchical Z-buffer [9] is another approach 
to fast visibility culling that allows a region of the scene to be culled whenever its closest depth 
value is greater than those of the pixels that have al­ready been drawn at its projected screen location. 
Like previous ap­proaches, this method can achieve dramatic speed-ups for environ­ments with signi.cant 
occlusion but is less effective for largely un­occluded environments with high visible complexity, such 
as a land­scape containing thousands of trees. Level-of-detail modeling Another approach for accelerating 
rendering is the use of multireso­lution or level-of-detail (LOD) modeling. The idea is to render pro­gressively 
coarser representations of a model as it moves further from the viewer. Such an approach has been used 
since the early days of .ight simulators, and has more recently been incorporated in walkthrough systems 
for complex environments by Funkhouser and S´equin [7], Maciel and Shirley [12], and Chamberlain et al. 
[2]. One of the chief dif.culties with the LOD approach is the problem of generating the various coarse-level 
representations of a model. Funkhouser and S´equin [7] created the different LOD models man­ually. Eck 
et al. [5] described methods based on wavelet analysis that can be used to automatically create reasonably 
accurate low­detail models of surfaces. Maciel and Shirley [12] used a number of LOD representations, 
including geometric simpli.cations cre­ated by hand, texture maps, and colored bounding boxes. Chamber­lain 
et al. [2] partitioned the scene into a spatial hierarchy of cells and associated with each cell a colored 
box representing its con­tents. Another approach to creating LOD models is described by Rossignac and 
Borrel [16], in which objects of arbitrary topology are simpli.ed by collapsing groups of nearby vertices 
into a single representative vertex, regardless of whether they belong to the same logical part. Another 
problem with geometric LOD approaches is that the shad­ing function becomes undersampled, as geometry 
is decimated. This undersampling causes shading artifacts, especially with Gouraud shading hardware, 
which evaluates the shading function only a the (decreasing number of) polygon vertices. Our approach 
can be thought of as a technique for automatically and dynamically creating view-dependent image-based 
LOD mod­els. Among the above LOD approaches, ours is closest to that of Ma­ciel and Shirley. However, 
there are several important differences. First, our approach computes LOD models on demand in a view­dependent 
fashion, rather than precomputing a .xed set of LOD models and using them throughout the walkthrough. 
Thus, we incur neither the preprocessing nor the storage costs associated with pre­computed LOD models. 
Second, we use a spatial hierarchy rather than an object hierarchy, and our LOD models represent regions 
of the scene rather than individual objects. Spatial partitioning allows us to correctly depth-sort the 
LOD models chosen for rendering at each frame, whereas an object hierarchy can suffer from occlusion 
artifacts where objects overlap. Image-based rendering A different approach for interactive scene display 
is based on the idea of view interpolation, in which different views of a scene are rendered as a pre-processing 
step, and intermediate views are generated by morphing between the precomputed images in real time. Chen 
and Williams [3] and McMillan and Bishop [13] have demonstrated two variants of this approach for restricted 
movement in three-dimensional environments. Although not general purpose, these algorithms provide a 
viable method of rendering complex en­vironments on machines that do not have fast graphics hardware. 
Images provide a method of rendering arbitrarily complex scenes in a constant amount of time. This idea 
is central to both of these papers and to the method we present here. Another image-based approach, described 
by Regan and Pose [15], renders the scene onto the faces of a cube centered around the viewer location. 
Their method allows the display to be updated very rapidly when the viewer is standing in place and looking 
about. They also use multiple display memories and image compositing with depth to allow different parts 
of an environment to be updated at differ­ent rates. Only parts of the environment that change or move 
signif­icantly are re-rendered from one frame to the next, resulting in the majority of objects being 
rendered infrequently. Our method can be thought of as a hierarchical extension to the method of Regan 
and Pose, but with more .exibility: instead of us­ing a .xed number of possible update rates, our method 
updates each object at its own rate. Another important difference is that instead of simply reusing an 
object s image over several consecutive frames, we use texture-mapping hardware to compensate for motion 
paral­lax. Schau.er and St¨urzlinger [17, 18] have concurrently and indepen­dently investigated ideas 
similar to our own. Our approach differs from theirs mostly in the formulation of the error metric and 
in the cost-bene.t analysis that we perform in order to decide whether or not to cache an image. 1.2 
Overview The remainder of the paper is organized as follows. In the next sec­tion, we describe our algorithm 
in detail. In Section 3, we present the error metric used to control the updating of cached images. In 
Section 4, we describe the preprocessing stage that constructs a hi­erarchical spatial partitioning of 
the environment. In Section 5, we report on the performance of our algorithm for a walkthrough of a complex 
outdoor scene. Section 6 closes with conclusions and fu­ture work.  2 Algorithm As a viewer follows 
a continuous path through a virtual environ­ment, there is typically considerable coherence between successive 
frames. The basic idea behind our algorithm is to exploit this coher­ence by caching images of objects 
rendered in one frame for possi­ble reuse in many subsequent frames. However, instead of simply reusing 
the same image, we apply the image as a texture map to a .xed quadrilateral placed at the center of the 
object. This textured quadrilateral is then rendered instead of the original object during several successive 
frames, using the current viewing transformation at each frame. In this way, at each frame, the image 
of the object is slightly warped, approximately correcting for the slight changes in the perspective 
projection of the original object as the viewer moves through the scene. Compensating for motion parallax 
in this manner results in fewer snapping artifacts when the cached image is up­dated and increases the 
number of frames for which the cache yields an acceptable approximation to the object s appearance. To 
gain the most from image caching, it is not enough to cache im­ages for individual objects. If too many 
objects are visible, the sheer number of textured polygons that must be rendered at each frame may overwhelm 
the hardware. However, distant objects that require infrequent updates can be grouped into clusters, 
and a single im­age can be cached and rendered in place of the entire cluster. Thus, our algorithm operates 
on a hierarchical representation of the entire scene, rather than on a collection of individual objects. 
An image can be computed and cached for any node in the hierarchy; hence the name hierarchical image 
caching . We construct the hierarchy as a preprocessing step by computing a BSP-tree [6] partitioning 
of the environment, as described in Sec­tion 4. We chose to use a BSP-tree since it allows us to traverse 
the scene in back-to-front order, which is necessary to ensure that the partially-transparent textured 
quadrilaterals are composited cor­rectly in the frame-buffer. In addition, BSP-trees are more .exible 
than other spatial partitioning data structures, making it is easier to avoid splitting objects. The 
leaf nodes of the BSP-tree correspond to convex regions of space and have associated with them a set 
of geometric primitives. This set consists of all the geometric primitives contained inside the node. 
In addition, it also contains nearby primitives from the neigh­boring nodes, as will be explained in 
Section 4. Any node in the tree may also contain a cached image. At each frame we traverse the BSP-tree 
twice. The .rst traversal culls nodes that are outside the view frustum and updates the image caches 
of the visible nodes: UpdateCaches(node, viewpoint) if node is outside the view frustum then node.status 
+CULL else if node.cache is valid for viewpoint then node.status +DRAWCACHE else if node is a leaf then 
UpdateNode(node, viewpoint) else UpdateCaches(node.back, viewpoint) UpdateCaches(node.front, viewpoint) 
UpdateNode(node, viewpoint) For a leaf node, the routine UpdateNode decides whether, for the current 
viewpoint, it is more cost-effective to draw the geometry stored with the node, or to compute and cache 
an image: UpdateNode(node, viewpoint) if viewpoint 2node then if node is a leaf then node.status +DRAWGEOM 
 else node.status +RECURSE return k +EstimateCacheLifeSpan(node, viewpoint) amortizedCost +hcost to create 
cachei/k + hcost to draw cachei if amortizedCost <hcost to draw contentsithen CreateCache(node, viewpoint) 
node.status +DRAWCACHE node.drawingCost +hcost to draw cachei else if node is a leaf then node.status 
+DRAWGEOM node.drawingCost +hcost to draw geometryi else node.status +RECURSE node.drawingCost + node.back.drawingCost 
+ node.front.drawingCost Geometry is always drawn if the viewpoint is inside the node. Other­wise, the 
routine EstimateCacheLifeSpan computes an estimate of the number of frames k for which we expect the 
cached image to re­main valid, as will be described in Section 3. This estimate is used to compute an 
amortized cost-per-frame for this node for each of the next k frames. We compute and cache an image only 
if the amortized cost is smaller than the cost of simply drawing the node s contents. For a leaf node, 
this cost is simply the cost of drawing the contained geometry, while for an interior node, this cost 
is the cost of drawing the node s children. The costs to draw geometric primitives and to create a cached 
image are established experimentally on each plat­form and are given as input to our system. The routine 
CreateCache starts by computing an axis-aligned rect­angle that is guaranteed to contain the image of 
the node s contents on the screen. This rectangle is obtained by transforming the cor­ners of the node 
s bounding box from world coordinates to screen coordinates and taking the minima and maxima along each 
axis. If the dimensions of the rectangle exceed those of the viewport, no im­age is cached. Otherwise, 
we rede.ne the viewing frustum so that it contains the entire node without changing the viewpoint or 
the view direction, and render the node. For a leaf node we draw all of its geometry, while for an interior 
node we draw its children. In many cases, the children are drawn using their cached images, if any ex­ist. 
Thus, caching an image typically does not involve drawing all the geometry contained in the corresponding 
subtree. After draw­ing the contents of the node, we copy the corresponding rectangular block of pixels 
into the node s image cache. As mentioned earlier, we use the cached image as a texture map that is applied 
to a quadri­lateral representing the entire node. In order to de.ne an appropri­ate quadrilateral in 
world space, we project the corners of the image rectangle onto a plane of constant depth with respect 
to the view­point that goes through the center of the node s bounding box. Once the cached images have 
been updated, we can proceed to ren­der the scene into the frame-buffer, during a second traversal of 
the BSP-tree from back to front: Render(node, viewpoint) if node.status == CULL then return else if 
node.status 2fDRAWCACHE, DRAWGEOMgthen Draw(node) else if viewpoint is in front of node.splittingPlane 
then Render(node.back, viewpoint) Render(node.front, viewpoint) else Render(node.front, viewpoint) Render(node.back, 
viewpoint) To complete the description of our algorithm, the next section de­scribes the error metric 
we use to determine whether a cached im­age is valid with respect to a given viewpoint and to estimate 
the life-span of a cached image. Section 4 describes in more detail our BSP-tree construction algorithm. 
 3Error metric The algorithm described in the previous section requires answers to the following two 
closely-related questions: 1. Given a node with a cached image computed for some previous view, is the 
cached image valid for the current view? 2. Given a node in the hierarchy and the current view, if we 
were to compute and cache an image of this node, for how many frames is the cached image likely to remain 
valid?  In order to answer these questions ef.ciently we need to de.ne an error metric, which, given 
a node in the hierarchy, its cache, and the current viewpoint, quanti.es the difference between the appearance 
of the cached image and that of the actual geometry. If this differ­ence is smaller than some user-speci.ed 
threshold c, the approxi­mation is deemed acceptable, and the cache is considered valid. An important 
requirement for an acceptable error metric is that it must be fast to compute. For example, we cannot 
afford to analyze the ge­ometric contents of the node, as the number of primitives contained in a node 
can be very large. Our algorithm employs an error metric that measures the maximum angular discrepancy 
between a point inside a node and the point that represents it in the cached image. We shall use the 
2D diagram shown in Figure 1 to de.ne our error metric more precisely. The rectangle in this diagram 
represents the bounding box of a node in the hierarchy. The thick line segment crossing the bounding 
box rep­resents the quadrilateral onto which the cached image is texture­mapped, as described in Section 
2. The viewpoint for which the ~ a v 1 v0 Figure 1 Angular discrepancy. cache was computed is v0.Let 
a be a point inside the node. The point that corresponds to a on the quadrilateral is a. By construction, 
a and a coincide when viewed from v0; however, for most other views, the two points subtend some angle 
(>0, as illustrated by view­point v1 in the diagram. Our error metric measures the maximum angular discrepancy 
over all points a inside the node: Error (v, v0)= max ((a, v, a) (1) a For a given view direction and 
.eld of view, the smaller the max­imum angular discrepancy is allowed to be, the closer the projec­tions 
of points a and a are in the image. Thus, using a smaller error threshold results in fewer visual artifacts 
caused by using the cached images instead of rendering the geometry. The right-hand side of equation 
(1) may be approximated by com­puting the angular discrepancy for each of the eight corners of a node 
s bounding box. This is not a conservative estimate, but it is fast to compute, and has been found to 
work well in practice. In order to predict the life span of a cached image before creating it for some 
view v0, we must estimate how far from v0 we can travel while keeping the error under c. If the view 
trajectory is known to us in advance, we can simply search along the trajectory for the far­thest point 
for which the error is within tolerance. This is probably the best course of action for recording a walkthrough 
or .y-by off­line. For an interactive walkthrough, the path of the viewer is not known in advance; however, 
the current velocity and acceleration are known at any frame, and an upper bound on the acceleration 
is typically available. In this situation, for each node in the hierar­chy we can attempt to .nd a safety 
zone around v0, that is, a set of viewpoints v such that for each viewpoint in this set the error is 
less than c: SafetyZone(v0)fv jError (v, v0) :cg, (2) Given the safety zone and using bounds on velocity 
and accelera­tion, we can compute a lower bound on the number of frames for which the cache will remain 
valid. Alternatively, we can obtain a non-conservative estimate by extrapolating the viewer s path and 
intersecting it with the safety zone. Our implementation uses non­conservative estimates. Next we describe 
how safety zones are com­puted in our algorithm. Consider the 2D diagram in Figure 2. Let v0 be the current 
view­point, a a point inside a node, and a its projection onto the textured quadrilateral, as in Figure 
1. Note that all viewpoints v from which the angle subtended by a and a is equal to cmust lie on one 
of the two circles of radius ka -a k r = (3) 2sin c passing through a and a. Thus, we can conservatively 
de.ne a cir­cular safety zone around v0 (a sphere in 3D), whose radius d is given by the shortest distance 
between these circles and v0: p d = h2+ r2+2hr sin c-r (4) Figure 2 The shaded region contains all the 
viewpoints v from which a and a subtend an angle greater than or equal to E. The lower circle is a conservative 
safety zone. where h is the distance between v0 and a. In order to approximate the safety zone for a 
leaf node in the hi­erarchy, we evaluate d for each corner of the node s bounding vol­ume and take the 
smallest of these distances. We then set the safety zone to be the axis-aligned cube inscribed inside 
a sphere of radius d around v0. The safety zone of an interior node is computed by .rst computing the 
safety zone using the bounding box of the node, and then taking the intersection of this safety zone 
with the safety zones of the children. In our implementation, the user speci.es the error threshold in 
pix­els. This threshold is converted to an angular error threshold using the current resolution and .eld-of-view 
angle. If either the resolu­tion or the .eld-of-view change in the course of a walkthrough, the angular 
error threshold must be adjusted accordingly.  4 Partitioning As a preprocessing step, we construct 
a BSP-tree [6] partitioning of the scene. The goals of the partitioning algorithm are as follows: 1. 
split as few objects as possible; 2. make the hierarchy as balanced as possible (in terms of the num­ber 
of geometric primitives contained in each subtree); 3. make the aspect ratio of each node s bounding 
volume as close to 1 as possible.  The .rst goal aims to reduce visual artifacts. The second and third 
goals help improve performance: balanced trees facilitate hierarchi­cal view-frustum culling, and cached 
images of nodes with good as­pect ratios tend to remain valid longer. Computing the optimal BSP­tree 
that satis.es these potentially contradictory goals appears dif.­cult. Therefore, our partitioning algorithm 
employs a simple greedy approach that is not optimal, but seems to work well in practice. Given a list 
of objects to partition, we look for gaps between objects, place a splitting plane in the best gap we 
can .nd, and then re­curse on the lists of objects on each side of that plane. To facilitate .nding the 
gaps between objects, we compute their extents with a method similar to the parallelepiped bounding volumes 
of Kay and Kajiya [10]. For each object, we compute its extent along each of N different directions on 
the unit sphere. Each splitting plane in the BSP-tree is constrained to be perpendicular to one of the 
N vectors. For example, if we chose the three coordinate axes as our direction vectors, our partitioning 
algorithm would yield a binary tree of axis­aligned boxes.  (a) (b) (c) Figure 3 (a) A bird s eye view 
of the island scene. (b) The partitioning of the scene. (c) A viewpoint on the walkthrough path. For 
each of the N directions, we create two sorted lists of objects: one, according to the lower bound of 
each object s extent; the other, according to the upper bound. We then scan these lists, while keep­ing 
track of the number of active objects (i.e., objects whose ex­tents we are currently in). Intervals where 
the number of active ob­jects is a local minimum are the gaps that we are looking for. Ideally, we are 
looking for a gap with zero active objects, such that the num­ber of geometric primitives on each side 
of the gap is roughly equal. Such a gap does not always exist, so we compute a cost for each gap that 
is a function of the number of its active objects and the ratio of the number of primitives on either 
side of the gap. For each of the N directions, we choose the gap with the smallest cost. To create good 
aspect ratios, we tend to choose the best gap from the direction along which the combined extent of all 
the objects on the list is greatest. The current implementation of our system is geared towards visual­ization 
of complex landscapes. Such scenes have a special structure: they essentially consist of a height-.eld 
representing land and wa­ter, and of objects such as trees and houses scattered on that height­.eld. 
Thus, assuming that the positive Y axis points up, all of the objects are spread above the XZ plane. 
Our partitioning algorithm takes advantage of this structure by using N direction vectors that evenly 
divide the unit circle perpendicular to the Y axis. As a result, all of the splitting planes of the BSP-tree 
are perpendicular to the XZ plane. In all of the experiments reported in Section 5, two direction vectors 
were used, resulting in axis-aligned boundaries between re­gions. When objects are split between two 
or more leaf nodes, visual arti­facts that look like gaps or cracks sometimes appear in the split sur­faces. 
This problem results from approximating a single object by multiple images, with no constraint that the 
images match along the split boundary. Such artifacts can occur even with small error thresh­olds because 
of the discrete sampling involved in creating the caches and rendering the textured quadrilaterals. For 
small error thresholds, it is possible to overcome these artifacts by ensuring a small amount of overlap 
in the geometry contained in neighboring leaf nodes. To achieve this overlap, we construct a slightly 
in.ated version of each leaf region, and associate with each leaf node the extra geome­try that is contained 
in its in.ated region, in addition to the geometry contained in the original region. In our current implementation, 
the amount by which regions are in.ated is a user-speci.ed parameter (typically 10 to 20 percent).  
5Results This section demonstrates the performance of our method using a walkthrough of a complex outdoor 
scene. All tests were performed on a Silicon Graphics Indigo2 workstation with a 250MHz R4400 processor, 
320 megabytes of RAM, and a Maximum Impact graph­ics board with 4 megabytes of texture memory. The outdoor 
scene used in these tests is a terrain of an island pop­ulated with 1117 willow trees. The terrain consists 
of 131,072 tri­angles, and each tree consists of 36,230 triangles. The total number of triangles in the 
database is 40,599,982. To keep the storage re­quirements down the trees were instanced, and the total 
amount of storage for the database before any processing by our method is 20 megabytes. The amount of 
storage required for this scene without instancing is 3.5 gigabytes. Figure 3(a) shows a bird s eye view 
of the island. Constructing the BSP-tree for this database took 46 seconds. The resulting partitioning 
(shown in Figure 3(b)) has 13 levels, 1072 leaf nodes, and is fairly balanced in terms of the geometric 
primitives contained in each subtree. Most leaf nodes contain a single tree and a portion of the terrain. 
The partitioning algorithm managed to avoid splitting any of the trees, and the only object split was 
the terrain. Partitioning the database causes an increase in the required storage. This increase is primarily 
due to the need to in.ate the leaf re­gions, as described in Section 4. For this database, we used an 
in.a­tion factor of 17 percent, increasing the storage to 150 megabytes. Note that the increase is only 
4 percent relative to the storage the original database would have required if we did not use instancing 
on the trees. We recorded timings for several walkthroughs of the island. Each of the walkthroughs was 
along the same path, de.ned by a B-spline space curve shown in white in Figure 3(c). This path was designed 
to help us study the relative performance of image caching over a range of visible scene complexities: 
the camera .rst tracks along the edge of the model, then .ies in toward the center of the island at tree­top 
level. Although the path was known in advance, we did not take advantage of this information, in order 
to get a better sense of how the algorithm would behave under interactive control. Figure 3(c) provides 
a snapshot illustrating our algorithm for a par­ticular viewpoint on the path. The view frustum for that 
viewpoint is indicated by green lines. Nodes outlined in purple are culled, as they lie outside the view 
frustum. Nodes outlined in yellow are ren­dered using their geometry. Nodes outlined in red are rendered 
using their cached images. The quadrilaterals onto which these images are mapped are shown in black. 
To assess the relative performance of our algorithm, we .rst com­puted two 1200-frame walkthroughs. Each 
frame was rendered at a resolution of 640 x480. The .rst walkthrough was performed Figure 4 Frames from 
walkthroughs of the island. The top row shows two frames rendered using the original geometry. The second 
row shows the same frames rendered with image caching using an error threshold of two pixels. The third 
row illustrates the visual artifacts resulting from a larger error threshold (eight pixels).  Figure 
5 Image caching versus rendering geometry. using an algorithm that employs hierarchical view frustum 
culling (using the same BSP-tree), but renders all of the original geometry contained in leaf nodes that 
are inside the view frustum. The second walkthrough was performed using our method with an error thresh­old 
of two pixels. The top row of images in Figure 4 shows two different frames from the walkthrough rendered 
using the original geometry. The second row shows the same frames rendered by our method. The images 
are not identical to those in the top row, but it is very hard to tell them apart, except for the distant 
trees that appear slightly softer and less blocky when rendered with our method, because of the linear 
.ltering used when rendering texture-mapped primitives. The plot in Figure 5 shows the rendering times 
for the two walk­throughs. For each frame, we plot the rendering time spent by each of the two methods. 
It takes our method 134 seconds to compute the very .rst frame of the walkthrough, which is two times 
longer than the time required when rendering the geometry. However, once the initial image caches have 
been computed, subsequent frames can be rendered 4.1 to 25.2 times faster with our method, with an overall 
speedup factor of 11.9 for the entire sequence. In the experiment above, our method used a fairly small 
error thresh­old: an angle subtended by roughly two pixels on the image plane. As a result, there are 
almost no perceptible visual artifacts in the walkthrough, as compared to rendering the geometry. If 
the error threshold is relaxed, more visual artifacts start to appear, but the rendering becomes faster, 
as cached images have longer life spans. For instance, with the error threshold set to eight pixels, 
the overall speedup increases to 14.1. Frames that were rendered with this error threshold are shown 
in the bottom row of Figure 4. Comparing these images with the ones rendered using geometry (in the top 
row) re­veals increased ruggedness along the silhouette of the mountains, as well as some cracks in the 
terrain, through which the blue back­ground shows through. Since our method utilizes path coherence, 
it is interesting to examine how different frame rates along the same path affect performance. Therefore, 
we rendered the same walkthrough using different num­bers of frames, equally spaced along the path. For 
example, when using two frames, the .rst frame is computed at the beginning of the path and the second 
in the middle of the path. Thus, for very small numbers of frames there is not much frame-to-frame coher­ence 
at all. For each walkthrough the overall speedup factor was computed, and the results are plotted in 
Figure 6. As expected, the speedup factor becomes larger, as more frames are rendered along the same 
path. Note that our method is faster than geometry with as Figure 6 Speedup as a function of frame rate. 
18 16 14 12 10 8 6 4 2 0 5 1015202530354045 Database Size (millions of triangles) Figure 7 Speedup as 
a function of scene complexity. few as 30 frames along the path. Another interesting statistic is the 
behavior of our method as a func­tion of overall scene complexity. The same walkthrough path was computed 
for several versions of the scene, each containing a dif­ferent number of trees. Except for the number 
of trees, all of the scenes were identical. The overall speedup factors for these scenes (for a 1200-frame 
walkthrough with a two-pixel error threshold) are plotted in Figure 7. The speedup factor introduced 
by our algorithm .rst rapidly increases with the geometrical complexity of the scene, but there is a 
drop in the speedup when the number of triangles in­creases from 20 million (574 trees) to 40 million 
(1117 trees). The reason for this behavior is that increasing the tree density on the is­land causes 
signi.cantly more extra geometry to be added to each leaf node when its region is in.ated. This extra 
geometry makes the overhead of creating a cached image for the node substantially larger. An important 
limiting factor on the performance of image caching is the constraint imposed by OpenGL [14] that texture 
maps have dimensions in powers of 2. Because of these limitations on texture size, almost half of the 
pixels in the textures de.ned by our method go unused. The handling of so many unused pixels results 
in a per­formance penalty for our image caching method. Speedup  Conclusions References [1] John M. 
Airey, John H. Rohlf, and Frederick P. Brooks, Jr. Towards There are many ways to extend the work presented 
in this paper: image realism with interactive update rates in complex virtual building Animation. Although 
our method is currently applicable only to static scenes, it should be easy to extend it to handle a 
few small moving objects or animated sprites. A more challenging problem for further research is to allow 
scenes where many objects are ca­pable of moving and/or deforming their geometry.  Pre-caching. Our 
algorithm should be extended to caching im­ages not only for nodes already in the view frustum, but also 
for nodes that should come into view in the next few frames. This extension would help alleviate temporary 
degradations in render­ing performance that occur as a user travels into an area of the scene that is 
more complex. Pre-caching could be particularly ef­fective if the caching computations are done in parallel 
by a sep­arate thread.  Geometric LOD modeling. Many of the objects drawn while cre­ating cached images 
occupy only a small number of pixels in the image. Thus, instead of drawing such objects in full detail, 
we could draw a coarser model of the same object, using a multi­resolution representation such as the 
one by Eck et al. [5] or Chamberlain et al. [2]. Using a multi-resolution representation could also accelerate 
rendering of objects for which no cached images were created.  Persistent caches. As regions of the 
scene pass out of the view frustum, the images cached for the newly culled nodes are inval­idated, and 
the memory is released. In the case that the viewer is simply looking around, these culled caches are 
still valid rep­resentations of their regions. Suspending invalidation of image caches in this case could 
potentially save a great deal of computa­tion, in much the same way as the method of Regan and Pose [15]. 
 Talisman. Image caching should prove even more effective in an architecture that optimizes the reuse 
of rendered images as tex­ture maps or sprites, such as the Tasliman architecture [20]. To make the best 
use of Talisman s capabilities, an af.ne warp of the cached image should be computed rather than the 
more gen­eral perspective warp resulting from texture-map the cached im­age onto a quadrilateral in 3D. 
 In summary, we have presented a new method for accelerating walk­throughs of complex environments by 
utilizing path coherence. We have demonstrated speedups of an order of magnitude on a current graphics 
architecture, the Indigo2 Maximum Impact. The speedups increase with the frame rate. While these speedups 
are signi.cant, we believe they could be made still more dramatic through fur­ther optimizations in the 
underlying graphics hardware and libraries, such as improving the pixel transfer rate from the frame 
buffer to texture memory, relaxing the existing restrictions on texture map sizes, and providing applications 
with better control over texture memory management. Acknowledgments We would like to thank Eric Brechner, 
Ka Chai, Brad Chamberlain, Michael Cohen, Hugues Hoppe, and Jack Tumblin for many useful discussions 
during the early stages of this project. This work was supported by an Alfred P. Sloan Research Fellow­ship 
(BR-3495), an NSF Postdoctoral Research Associates in Ex­perimental Sciences award (CDA-9404959), an 
NSF Presidential Faculty Fellow award (CCR-9553199), an ONR Young Investigator award (N00014-95-1-0728), 
a grant from the Washington Technol­ogy Center, and industrial gifts from Interval, Microsoft, and Xerox. 
environments. Computer Graphics (1990 Symposium on Interactive 3D Graphics), 24(2):41 50, March 1990. 
[2] Bradford Chamberlain, Tony DeRose, Dani Lischinski, David Salesin, and John Snyder. Fast rendering 
of complex environments using a spa­tial hierarchy. In Proceedings of Graphics Interface 96, May 1996. 
[3] Shenchang Eric Chen and Lance Williams. View interpolation for im­age synthesis. In Computer Graphics 
Proceedings, Annual Conference Series, pp. 279 288, August 1993. [4] James H. Clark. Hierarchical geometric 
models for visible surface algorithms. Communications of the ACM, 19(10):547 554, October 1976. [5] Matthias 
Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. Multiresolution 
analysis for arbitrary meshes. In Computer Graphics Proceedings, Annual Conference Se­ries, pp. 173 182, 
August 1995. [6] Henry Fuchs, Zvi M. Kedem, and Bruce Naylor. On visible surface generation by a priori 
tree structures. Computer Graphics, 14(3):175 181, June 1980. [7] Thomas A. Funkhouser and Carlo H. S´equin. 
Adaptive display algo­rithm for interactive frame rates during visualization of complex vir­tual environments. 
In Computer Graphics Proceedings, Annual Con­ference Series, pp. 247 254, August 1993. [8] B. Garlick, 
D. Baum, and J. Winget. Interactive viewing of large ge­ometric databases using multiprocessor graphics 
workstations. SIG-GRAPH 90 Course Nores: Parallel Algorithms and Architectures for 3D Image Generation, 
1990. [9] Ned Greene, Michael Kass, and Gavin Miller. Hierarchical z-buffer visibility. In Computer Graphics 
Proceedings, Annual Conference Se­ries, pp. 231 238, August 1993. [10] Timothy L. Kay and James T. Kajiya. 
Ray tracing complex scenes. Computer Graphics, 20(4):269 278, August 1986. [11] Daivid Luebke and Chris 
Georges. Portals and mirrors: Simple, fast evaluation of potentially visible sets. In 1995 Symposium 
on Interac­tive 3D Graphics, pp. 105 106, April 1995. [12] Paulo W. C. Maciel and Peter Shirley. Visual 
navigation of large en­vironments using textured clusters. In 1995 Symposium on Interactive 3D Graphics, 
pp. 95 102, April 1995. [13] Leonard McMillan and Gary Bishop. Plenoptic modeling: An image­based rendering 
system. In Computer Graphics Proceedings, Annual Conference Series, pp. 39 46, August 1995. [14] Jackie 
Neider, Tom Davis, and Mason Woo. OpengGL Programming Guide. Addison Wesley, 1993. [15] Matthew Regan 
and Ronald Pose. Priority rendering with a virtual re­ality address recalculation pipeline. In Computer 
Graphics Proceed­ings, Annual Conference Series, pp. 155 162, July 1994. [16] Jarek Rossignac and Paul 
Borrel. Multi-resolution 3D approximations for rendering complex scenes. Research Report RC 17697 (#77951), 
IBM, Yorktown Heights, New York 10598, 1992. Also appeared in the IFIP TC 5.WG 5.10. [17] Gernot Schau.er. 
Exploiting frame to frame coherence in a virtual re­ality system. In Proceedings of VRAIS 96, pp. 95 
102, April 1996. [18] Gernot Schau.er and Wolfgang St¨urzlinger. A three dimensional im­age cache for 
virtual reality. In Proceedings of Eurographics 96, 1996. To appear. [19] Seth J. Teller. Visibility 
Computations in Densely Occluded Polyhe­dral Environments. PhD thesis, Computer Science Division (EECS), 
UC Berkeley, Berkeley, California 94720, October 1992. Available as Report No. UCB/CSD-92-708. [20] Jay 
Torborg and Jim Kajiya. Talisman: Commodity realtime 3D graph­ics for the PC. In Computer Graphics Proceedings, 
Annual Conference Series, August 1996.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237211</article_id>
		<sort_key>83</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Hierarchical view-dependent structures for interactive scene manipulation]]></title>
		<page_from>83</page_from>
		<page_to>90</page_to>
		<doi_number>10.1145/237170.237211</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237211</url>
		<keywords>
			<kw><![CDATA[color tree]]></kw>
			<kw><![CDATA[image quadtree]]></kw>
			<kw><![CDATA[interactive system]]></kw>
			<kw><![CDATA[ray tree]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[scene editing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Trees</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10003634</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P210377</person_id>
				<author_profile_id><![CDATA[81392613902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Normand]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bri&#232;re]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[D&#233;partement d'Informatique et de Recherche Op&#233;rationnelle, Universit&#233; de Montr&#233;al, C.P. 6128, succ. Centre-Ville, Montr&#233;al (Qc) Canada H3C 3J7]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39077437</person_id>
				<author_profile_id><![CDATA[81100349049]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poulin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[D&#233;partement d'Informatique et de Recherche Op&#233;rationnelle, Universit&#233; de Montr&#233;al, C.P. 6128, succ. Centre-Ville, Montr&#233;al (Qc) Canada H3C 3J7]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[James Arvo. Backward ray tracing. SIGGRAPH 86 Tutorial notes on Developments in Ray Tracing, August 1986.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook. Shade trees. Proceedings of SIGGRAPH 84. In Computer Graphics, 18, 3 (July 1984), pp. 223-231.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Franklin C. Crow. Shadow algorithms for computer graphics. Proceedings of SIGGRAPH 77. In Computer Graphics, 11, 2 (July 1977), pp. 242-248.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166149</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Thomas A. Funkhouser and Carlo H. S6quin. Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. Proceedings of SIGGRAPH 93. In Computer Graphics Proceedings, Annual Conterence Series, August 1993, pp. 247-254.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>16584</ref_obj_id>
				<ref_obj_pid>16564</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ned Greene. Applications of world projections. Proceedings of Graphics Interface 86, (May 1986), pp. 108-114.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>180902</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ned Greene. Detecting intersection of a rectangular solid and a convex polyhedron. In Paul Heckbert, editor, Graphics Gems IV, pages 74-82. Academic Press, Boston, 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ned Greene and M. Kass. Hierarchical Z-buffer visibility. Proceedings of SIG- GRAPH 93. In Computer Graphics Proceedings, Annual Conference Series, August 1993, pp. 231-240.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218470</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Brian Guenter, Todd B. Knoblock, and Erik Ruf. Specializing shaders. Proceedings of SIGGRAPH 95. In Computer Graphics Proceedings, Annual Conference Series, August 1995, pp. 343-350.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Eric Haines and John Wallace. Shaft culling for efficient ray-traced radiosity. In Eurographics Workshop on Rendering, 1991.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Paul E. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. Proceedings of SIGGRAPH 90. In Computer Graphics, 24, 4 (August 1990), pp. 215-223.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155315</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[David A. Jevans. Object space temporal coherence for ray tracing. Proceedings of Graphics Interface 92, (May 1992), pp. 176-183.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192187</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Peter Litwinowicz and Gavin Miller. Efficient techniques for interactive texture placement. Proceedings of SIGGRAPH 94. In Computer Graphics Proceedings, Annual Conference Series, July 1994, pp. 119-122.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[K. Murakami and K. Hirota. Incremental ray tracing. In Eurographics Workshop on Photosimulation, Realism and Physics in Computer Graphics, June 1989, pp. 17-32.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Mark Segal, Carl Korobkin, Rolf van Widenfelt, Jim Foran, and Paul E. Haeberli. Fast shadows and lighting effects using texture mapping. Proceedings of SIG- GRAPH 92. In Computer Graphics, 26, 2 (July 1992), pp. 249-252.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74365</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Carlo H. S6quin and Eliot K. Smyrl. Parameterized ray tracing. Proceedings of SIGGRAPH 89. In Computer Graphics, 23, 3 (July 1989), pp. 307-314.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>107217</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Alan Watt and Mark Watt. Advanced Animation and Rendering Techniques: Theory and Practice. Addison-Wesley Publishing Company, 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Turner Whitted. An improved illumination model for shaded display. Communications of the ACM, 23(6):343-349, June 1980.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Casting curved shadows on curved surfaces. Proceedings of SIGGRAPH 78. In Computer Graphics, 12, (August 1978), pp. 270-274.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hierarchical View-dependent Structures for Interactive Scene Manipulation Normand Bri`ere and Pierre 
Poulin D´epartement d Informatique et de Recherche Op´erationnelle . Universit´edeMontr´eal Abstract 
The result of a scene manipulation is usually displayed by re­rendering the entire image even if the 
change has affected only a small portion of it. This paper presents a system that ef.ciently de­tects 
and recomputes the exact portion of the image that has changed after an arbitrary manipulation of a scene 
viewed from a .xed cam­era. The incremental rendering allows for all visual effects produced by ray tracing, 
including shadows, re.ections, refractions, textures, and bump maps. Two structures are maintained to 
achieve this. A ray tree is as­sociated with each pixel and is used to detect and rebuild only the modi.ed 
rays after an optical or geometrical change. A color tree represents the complete color expression of 
a pixel. All changes af­fecting the color of a pixel without changing the corresponding ray tree require 
only re-evaluation of the affected portions of the color tree. Optimizations are presented to ef.ciently 
detect the modi.ed structures by the use of strategies such as grouping similar informa­tion and building 
hierarchies. Pruning and weighted re-evaluation of information are also considered to manage the memory 
require­ments. The incremental rendering is done ef.ciently and accurately and is suitable in an interactive 
context. CR Categories and Subject Descriptors: I.3.7 [Computer Graph­ics]: Three-Dimensional Graphics 
and Realism. Additional Key Words and Phrases: scene editing, interactive system, rendering, image quadtree, 
color tree, ray tree 1 Introduction Computing the image of a 3D synthetic scene is a complex process, 
especially when shadows, textures, bump maps, re.ections, and re­fractions are desired. A simple yet 
powerful algorithm producing such effects is ray tracing [17]. Unfortunately, its computational re­quirements 
are generally too high to be considered suitable to cal­culate intermediate images resulting from an 
interactive scene ma­nipulation. In fact, ray tracing is mostly used for high quality .nal rendering, 
ranging from minutes to hours of computing. .C.P. 6128, succ. Centre-Ville, Montr´eal (Qc) Canada H3C 
3J7 f briere jpoulin g@iro.umontreal.ca Permission to make digital or hard copies of part or all of this 
work or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 One important reason 
why ray tracing is rarely considered within an interactive tool is that most interactive systems recompute 
the entire image of the scene without considering any incremental ren­dering from the previous image. 
The fastest rendering technique is usually the projection of wireframe models without any line re­moval. 
Unfortunately, it can only convey information about shape, and complex objects are dif.cult to interpret. 
The Z-buffer algo­rithm is quite fast and treats visible-surface determination. It be­comes even more 
competitive by integrating the technique with hi­erarchical models and treating the image hierarchically 
[7]. Shading and shadowing for directional and point light sources, as well as tex­tures and .ltering 
can be simulated [3] [18] [14], but with a signi.­cant impact on performance and memory usage. However, 
other im­portant visual phenomena such as re.ection and refraction can only be approximated by textures, 
and this with great effort and potential artifacts [5]. For interaction, one can also bene.t from re-ordering 
the ren­dering with respect to the phenomena being manipulated, or from choosing between different levels 
of object complexity [4]. In spe­ci.c contexts, some manipulations have been optimized by prepro­cessing. 
Hanrahan and Haeberli [10] edit material properties on a preprocessed sphere. Interactivity is obtained 
but the manipula­tion is done from a different visual context than the scene itself. Litwinowicz and 
Miller [12] interactively distort a texture directly on a preprocessed projection of the uvcoordinates 
of the texture parametrization. Instead of rendering the entire scene, other researchers have con­sidered 
updating only the elements affected by a change. For each surface, Cook [2] conserves in a tree structure 
(shade tree) the sym­bolic evaluation of the illumination model. If a manipulation does not modify the 
shape of the tree, then the local illumination may be updated by simply evaluating the shade tree. The 
evaluation of a tree may not be faster than the calculation itself, but if the tree is replicated at 
each pixel and compressed according to the parameter currently adjusted, then the evaluation of the smaller 
tree is faster. By expressing a RenderMan shader by source code instead of a sym­bolic tree, Guenter 
et al. [8] have de.ned specialized shaders. In these techniques, the preprocessing is usually applied 
on the .rst visible surface. S´equin and Smyrl [15] preserve in a tree the color expressions of all intersections 
obtained by ray tracing. The image is updated by traversing these color trees with modi.ed pa­rameters. 
They only consider changes that do not alter the shape of the trees, which leads to signi.cant time savings 
because this avoids recomputing visibility. They also propose several compres­sion techniques to reduce 
the memory usage and improve on the display time. Systems such as Atlantis from Abvent and IPR from Wavefront 
extend these color trees by adding a .xed number of ex­tra ray generations to allow a user, for instance, 
to make re.ective a previously non-re.ective object. Murakami and Hirota [13] extend these previous techniques 
to handle also changes in visibility for a scene rendered from a static viewpoint. A ray is indexed by 
the list of regular voxels it tra­verses. Any change to the scene is associated with its affected vox­els, 
which in turn determine the potentially affected rays. They also Color Parameters surface color ambient, 
diffuse, specular coef.cients surface roughness proportion of re.ection, refraction, transparency proportion 
plastic-metallic light color and intensity texture parameter bump map surface parametrization Optical 
Manipulation add/delete a re.ection, refraction and transparency attribute change a refraction index 
add/delete/change a bump map of a re.ective or refractive object Geometrical Manipulation add/delete/transform 
an object add/delete/transform a light source add/delete/transform a displacement map Table 1: Color 
and ray tree dependent manipulations use a clever hashing scheme to identify quickly the rays affected 
by a given voxel. However the visibility determination is performed with respect to the affected voxels 
rather than the transformed ob­jects. Therefore, all intersections between a ray and the objects in the 
affected voxels must be precomputed and saved, or recomputed each time a voxel is affected. Increasing 
the number of voxels re­duces this visibility determination, but at the cost of storing many voxels, 
and also of handling more entries in the hashing table. Je­vans [11] removed the previous dependency 
upon image resolution by storing instead in each voxel the identi.cation of limited regions potentially 
affected by this voxel. However more unaffected rays can thus be wrongly identi.ed as affected, and the 
visibility is done with respect to the entire scene. In this paper, we present a system for the manipulation 
of a scene viewed from a .xed camera. The visual effects in the images of the scene can include the richness 
of all those produced by ray trac­ing, including textures, shadows, re.ections, refractions, and bump 
maps. The central concept behind this system is the ability to ef.­ciently detect and recompute only 
the modi.ed image portion. Al­lowed changes are of any kind, whether modifying a surface shad­ing parameter, 
a texture, making a surface re.ective or refractive, or transforming the geometry of any object. To achieve 
this, some concepts from the systems described above are uni.ed into two tree-like structures: the color 
tree and the ray tree. In the next section, we describe these two structures and how they are used. In 
section 3, ef.ciency issues are addressed for the detection and updating of these structures. In section 
4, we consider trade-offs between memory requirements and computing ef.ciency. Finally, we give some 
typical results from using our system and con­clude by summing up our work and by listing potential extensions 
and applications.  2 Scene Manipulation Changing some portion of a synthetic scene can affect different 
properties treated at various stages of the rendering algorithm. To ef.ciently update the image affected 
by a speci.c change may thus require more than one data structure. All possible changes can be divided 
into two categories. A color change may modify the color of at least one pixel without affecting the 
scene visibility, while a ray change (optical or geometrical) may change this visibility. In an interactive 
context, the user selects a group of objects (se­lection) and applies a given change to it. In the next 
subsections, we explain the particularities of each manipulation and describe the two structures used 
to incrementally render the current image. Please refer to .gure 1 for a graphical representation of 
the concepts de­scribed in this section. 2.1 The Color-tree Structure In ray tracing, the color of a 
pixel is computed by following a ray through the scene. At each intersection, the color returned depends 
upon the surface re.ection/refraction model which speci.es how much of the light reaching this point 
is sent back along the ray di­rection. We store the color of a pixel as an expression tree (.gure 1 color 
tree) in which each leaf is a constant or a pointer to a param­eter, and each node is an n-ary function. 
If a color subtree does not contain any parameter, it is replaced by a constant leaf. One can view our 
color tree as a union of concepts such as shade trees [2], texture trees [16], and parametrized rays 
[15]. Indeed, a color tree corresponds to the entire symbolic evaluation of a pixel color without preserving 
information about the visibility. A color change is the most simple and ef.cient change to han­dle. It 
corresponds to a change that does not alter the visibility in the image. Re-rendering the modi.ed scene 
consists of simply re-evaluating the color expression for each pixel. The refresh rate therefore depends 
upon the number of nodes in each color tree, which is a function of the number of light sources and interre.ec­tion/interrefraction 
combinations. In most cases, the display time is uniform and quite fast. All the shading parameters listed 
in the top section of table 1 are associated with color changes. Any shading parameter may be a variable 
or more generally, a texture function controlling this parameter. At a given intersection point, such 
a texture function may depend upon that point or upon the surface parametrization at that point. Since 
a texture function can be expressed as a tree structure, it is integrated as a subtree of our color tree. 
A change of any texture parameter is thus also resolved by evaluating the color tree. Finally, if the 
user manipulates a higher-level property such as changing a procedural texture to another, or changing 
the illumina­tion model itself, the associated color subtrees are appropriately re­placed.  2.2 The 
Ray-tree Structure In order to compute visibility changes ef.ciently, an extended struc­ture is used, 
namely, the ray tree. It is similar to Murakami and Hi­rota s ray set [13], but without information related 
to voxels. The ray path of a pixel is the ordered list of objects encountered by a ray originating from 
the eye position through this pixel. The ray tree of a pixel (.gure 1 ray tree) is the geometrically-speci.c 
informa­tion of the ray path. Each node of the ray tree contains intersection point dependent information 
(PDI): the intersection point, its nor­mal, and surface parametrization. Nodes for the eye and for each 
light are considered global and are not duplicated. A ray segment joins two consecutive points of a ray 
tree. A node of a ray path is only a pointer to an object from the list of objects de.ning the scene. 
Rebuilding a portion of a ray tree causes an immediate update of the corresponding color tree. An optical 
change occurs when a ray tree is modi.ed without changing the scene geometry. The middle section of table 
1 lists some optical changes. An optical change is effective at a node when its object pointed to has 
been modi.ed by an optical change. The in­tersection point at this node is guaranteed to be valid. However 
new re.ected or refracted rays may have to be re-shot from this point and intersected with respect to 
the entire scene. A geometrical change occurs when the geometry of the scene has changed. Some of these 
changes appear in the bottom section of ta­ble 1. A general geometrical change is handled in three steps. 
First the selection is removed from all ray trees. The resulting ray trees Parameters: Change type: 
none Object floor object list F3. diffuse coefficient F2. ambient coefficient F1. surface color F4. specular 
coefficient Transformations Object sphere Point light L1. light color Position ... F5. surface roughness 
F6. proportion of reflection *point light *point light eye node *floor ray tree surface parametrization 
primary segment (pixel) shadow segment ... PDI Intersection Point-dependent Info (PDI) surface normal 
shadow segment intersection point reflection segment *sphere color tree *L1 *F2 *F3 dot H *F6 exp dot 
N *F1 *F1 1/(r*r) L *L1 1/(r*r) *F5 *F4 N non-reflective P2 point light PL opaque Ei = Pi-E ||Pi-E|| 
eye sphere reflective floor P1 E ri = ||PL-Pi|| ||Li+Ei|| Li = PL-Pi ||PL-Pi|| Hi = Li+Ei Figure 1: 
Color and ray trees will not contain any reference to the selection. Then only the selec­tion is rendered 
while it is manipulated by the user. Finally when the user releases the selection, the ray trees are 
updated. A ray test is a visibility determination with respect to the selection only. A ray update is 
a visibility determination with respect to the entire scene (except for primary rays which need only 
a ray test), but only for the affected pixels. This is typically a small fraction of the image. To remove 
the selection from the ray trees, all of their nodes are visited. If the corresponding object at a node 
is contained in the se­lection, then the entire subtree is no longer valid. It is replaced by a new ray 
subtree traced from the intersection point of the previous node in the same direction as the incoming 
ray segment. The inter­sections are computed with respect to the entire scene, but without the selection 
itself. The intermediate rendering does not modify the ray trees in any way since at that time they do 
not contain any reference to the se­lection. All ray trees are simply traversed and only intersection 
tests between the segments and the selection are performed. If a ray seg­ment intersects the selection 
then the intersection point is computed and all emanating rays (shadow, re.ection, refraction) must be 
re­shot from that point with respect to the entire scene, including the selection. Adding the selection 
to the ray trees corresponds to rendering it only once while updating the modi.ed ray trees.  3 Hierarchies 
and Groups The color and ray trees allow incremental ray tracing of the scene during manipulation. In 
most cases, this is already an appreciable gain because a structure test is much faster than its update. 
Also, the visibility determination is typically the most expensive opera­tion and none are performed 
for a color change. Unfortunately, traversing all structures to determine what may be affected quickly 
becomes a bottleneck. In this section, we propose some optimizations to cull many unnecessary tests. 
Section 5 will demonstrate the ef.ciency of these optimizations. 3.1 Selection-dependent Preprocessing 
The image represented by its color and ray trees is built as a quadtree. Typically, the user adjusts 
the scene by applying sev­eral consecutive color or optical changes to the same selection. The quadtree 
hierarchical structure can greatly speed up such subsequent changes. Indeed, because the selection does 
not change, the portion of the image dependent upon the selection remains the same. A .ag is kept in 
each region (quadtree element) that indicates if the region is dependent or not upon the current selection. 
The preprocessing is performed at the .rst change and the .ags are not reset as long as the user applies 
color or optical changes to the same selection. Because of the hierarchical nature of the quadtree, the 
rejection of independent regions is very fast. Figure 2 b) shows with white contours the regions dependent 
upon the central sphere: the primary ray segments intersecting the central sphere, the interre.ections 
on the .oor and the left sphere, and the refraction in the right sphere. Here, only shadows from opaque 
objects are handled. Note that this preprocessing is also ap­plicable for removing the selection from 
the ray trees.  3.2 Groups and Atoms It is also possible to speed up the selection-dependent preprocessing 
by using the concept of groups. A group is a region for which each ray path is identical. As such, their 
corresponding ray trees only differ by their PDIs, and the subregions of a group are also groups themselves. 
A .ag in each region indicates if the region is a group by a pointer to its corresponding ray path. If 
the region is a group, only the ray path needs to be tested. After optical and geometrical changes, ray 
paths may change and therefore some image groups must be up­dated. However, because these changes are 
propagated from the bottom of the quadtree to its top, the number of regions to be up­dated is minimal. 
Figure 2 c) shows with white contours the groups in the image. In a scene composed of many visible objects, 
the number of groups can be rather large and their ef.ciency thus much reduced. If the scene is constructed 
as a hierarchy of objects, it is possible to consider an atom to be at a higher level than the leaves 
in the model hierarchy. By raising the atom in the hierarchy, the num­ber of groups and ray paths should 
globally decrease, but if the user tries to modify a subatomic object, the system will have to consider 
the entire atom as modi.ed. For instance in .gure 4, the bust of Beethoven, boat, individual trees, and 
cows could each be consid­ered an atom. Many regions contain typically a limited number of ray paths, 
but because of the distribution of these ray paths within a region, the quadtree subdivision might produce 
a large number of smaller groups, each in the worst case consisting of a single ray path. To re­duce 
these situations and decrease further the number of groups, the groups are generalized to path groups, 
p-groups, where pindicates the number of different paths in a region. A group is then a 1-group. Determining 
if a p-group is dependent upon the selection should be faster as there are no subregions to test, but 
it may require testing up to ppaths. The user can control a maximum value mpfor pso the gains in culling 
of p-groups are not outweighed by maintaining and testing the list of ray paths.  a) Original scene 
b) Selection-dependent regions c) Image groups ............................................................................................................................................................................................................................................................................................................................................... 
................................................................................................................................................................................................................................................. 
.................................................................................................................................................... 
 d) Geometrically-dependent e) Scaling down the selection f) Using up to two tunnels regions per region 
Figure 2: A simple scene  3.3 Color-tree Evaluation For a color update, the user typically adjusts the 
same parameter fre­quently. In order to speed up the color update, color expression trees can be compressed 
according to the currently-selected parameter by maintaining at each node the evaluation of its subtree. 
In particu­lar, the entire color expression of a pixel independent of the current parameter is temporarily 
replaced by a constant color value. This occurs when the pixel depends upon the selection but not on 
the se­lected shading or texture parameter. This optimization is present in S´equin and Smyrl [15] in 
which a subtree is simply replaced by its value. The color trees could be also compressed physically, 
at the cost of rebuilding them each time another parameter is selected. This is also similar to compressing 
the shaders in Guenter et al. [8].  3.4 Bounding Ray Trees Testing each ray tree for intersection with 
the selection becomes pro­hibitive if our goal is interactive manipulation. In order to greatly speed 
up ray tests, we use a hierarchy of bounding volumes for the ray trees. If the selection does not intersect 
such a bounding volume, then no ray test has to be performed for the enclosed ray trees. 3.4.1 Tunnels 
For each region of the image, a volume that encloses every point of all its ray trees is kept. This region 
s main volume, called a tunnel, is built as a union of convex volumes, each called a section. Figure 
3 shows a 2D representation of a tunnel and its 3D counterpart for a particular region (a group) and 
viewpoint from the scene depicted in .gure 2. The tunnel encloses the eye (right apex), points on the 
.oor, re.ection points on the sphere, and two branching sections go­ing towards the light above.  Figure 
3: 2D and 3D representations of a tunnel The .rst section of a tunnel encloses the eye point and all 
points directly visible from the eye through this region (.rst-generation ray segments). For each point 
light source, there is another section en­closing the same .rst-generation intersection points and the 
point light source. All the re.ected ray segments of the second genera­tion are enclosed by another section, 
and similarly for the refracted ray segments of this second generation. The two sets of endpoints of 
these secondary segments each generate another section respec­tively with each point light source. This 
process continues recur­sively for the next-generation segments. A special situation occurs when the 
region is a group. In this case, all nray trees have the same path and each section always encloses exactly 
nray segments. These tunnels are usually thinner as the rays tend to remain closer to each other. However, 
if the region is not a group, some sections enclose ray segments whose endpoints do not belong to the 
same objects. Such a section may be much larger than necessary as some ray segments are not correlated. 
However, if a region forming a p-group consists of ptunnels associated with each pdifferent ray paths, 
then many of those undesirable sections are eliminated. If the region has more ray paths than the maximum 
number mp,the mthtunnel contains all ray trees belonging to the p p.mp+1remaining ray paths.  3.4.2 
Sections A section bounds a set of ray segments. We adopted the shafts intro­duced by Haines and Wallace 
[9]. A shaft is built using two aligned boxes with each bounding plane passing through a face of a box 
or through a pair of relevant edges, each of them belonging to a dif­ferent box. This construction has 
the property that if a shaft is built with two given boxes, then it encloses any subshaft built using 
two subboxes. A pyramid shaft is a particular shaft where all points at one end are the same. It is used 
for sections whose apex is the eye or a point light (.gure 3). Rather than testing for an intersection 
between the shaft and the selection itself, we use a bounding box around the selection. The test is based 
only on trivial rejections between the vertices of these two bounding volumes and their supporting planes. 
This is simpler and faster to compute, although if no trivial rejection has occurred, the test proceeds 
as if there were an intersection. For an exact test between a box and an arbitrary convex polyhedron, 
see Greene [6]. Figure 2 d) shows with white contours the regions updated when a geometrical change is 
applied to the selection (the central sphere). The updated portion of the image is larger than the minimal 
one in .gure 2 b) because the bounding volume of the selection is used for the intersection test. This 
appears .rst in the projection of the selection in the image, and also in larger shadows and re.ections 
of the selection. Moreover, this test is limited to trivial rejections only. The selection is scaled 
by 1in .gure 2 e). The corresponding 4 smaller updated regions are displayed with white contours. One 
can see that some irrelevant regions are located at the silhouette of the re.ective sphere because of 
some incoherently large sections. Al­lowing up to two tunnels per region eliminates some of these sec­tions 
(.gure 2 f). 3.4.3 Updating the Tunnels Adding an object to the ray trees may change some tunnels. This 
is also the case after an optical change. The tunnel of a region with at least one modi.ed ray tree must 
be recalculated. However, the modi.cation is propagated from bottom to top and no tunnel is re­built 
needlessly. A region is formed by four subregions (quadtree structure), so a tunnel of this region is 
formed by its four subtunnels. At the lowest level, the ray segments form the shaft which is built using 
the two bounding boxes of their endpoints. At a higher level, the bounding boxes used for the shaft bound 
the respective subboxes of the four subshafts. Each level is thus updated in constant time.   4 Spacetime 
Considerations In previous sections, we described the structures and optimizations allowing us to manipulate 
a scene interactively. However, we did not discuss memory requirements. Memory usage is the major drawback 
of our scheme as each pixel contains a lot of information. In order to manipulate a scene at various 
image resolutions given limited memory, we present various trade-offs between the space re­quired by 
the full structures and the computing time necessary to re­build them. In this section, we discuss strategies 
to lower the memory require­ments, based upon controlling the number of different information pieces. 
This leads to a more general approach based upon the rela­tive importance of an information. Note that 
none of these memory strategies affect the resulting images; they only in.uence what infor­mation will 
need to be recomputed. This makes the system .exible according to the space and time constraints. 4.1 
Eliminating Information All color trees, ray trees, and tunnels represent the largest portion of the 
memory usage in our system (table 2). Fortunately, we can exploit the locality involved with most changes. 
By limiting to mcthe number of color trees, and if a color change does not update more than mcpixels, 
then the refresh rate is not af­fected. However, if the number of updated pixels is larger than mc, the 
excess pixels can be updated using the (slower) ray tree evalua­tion. Similar to the color and ray tree 
reduction, a maximum number of tunnels can be speci.ed by the user in order to control their memory usage. 
The time for reconstructing a tunnel becomes a function of the number of its immediate subtunnels already 
constructed, and the number of subtunnels to reconstruct. So the worst case of rebuilding a tunnel depends 
upon its level in the hierarchy. 4.1.1 Color Trees Another solution speci.c to color trees prunes color 
subtrees by re­placing them by their corresponding functions rather than their ex­pansion in a tree. 
A procedural texture is an example of such a func­tion. However when changing one parameter of this texture, 
the smaller trees thus obtained may be slower to evaluate as the function calculation is usually slower 
than its subtree evaluation. The user could also select a subset of parameters which are subject to change, 
and thus replace any subtrees in which no such parameter appears by a constant. Changing another parameter 
will involve however a ray tree evaluation. 4.1.2 Ray Trees The ray trees represent another important 
portion of the storage used. All of this information can be removed in order to reduce the memory usage, 
requiring any intersection point, normal vector, and surface parametrization to be recalculated on demand. 
However, these quantities are not usually expensive to recompute, because the object to intersect with 
is already known. So the visibility calcula­tion for the intersection point is done with respect to that 
object only. Although it is faster to preserve all this information, another space­time trade-off is 
possible by keeping only a certain number of ray trees. A deleted ray tree may be recalculated from a 
pointer to its ray path, which is stored in a global list. 4.1.3 Tunnels Because of their hierarchical 
nature, many lower level subtunnels are rarely accessed. Also the lower we get in the structure, the 
more such subtunnels there are. By simply eliminating the tunnels at the two lowest levels, one obtains 
a reduction of a factor of about 16 in the number of tunnels. Moreover, this contributes to improving 
the global performance, as testing a lowest-level tunnel is more expen­sive than testing the few enclosed 
ray trees themselves.  4.2 Shadow Counters For shadows cast by opaque objects, it is not necessary to 
know in which order objects are blocking the light, but only if the light is visible or not. Therefore, 
instead of constructing the list of ray seg­ments starting from a given intersection point up to a point 
light, we use only a counter to indicate the number of blocking objects. When an object is removed from 
the ray trees, the counter for a light at a ray node is decremented if the object was a blocker. If the 
Scene Color trees Ray paths 1-groups Ray trees Tunnels Figure 2 (simple) 66,000 32.5 MB 125 4,500 66,000 
5.3 MB 10,900 5.4 MB Figure 4 (complex) 66,000 20.5 MB 5,300 12,100 66,000 3.7 MB 10,900 4.3 MB Table 
2: Statistics on memory requirements counter reaches zero, the illumination must be recomputed; other­wise, 
nothing has to be recomputed. When an object is added to the ray trees, some counters may be incremented. 
If it is the case for a counter previously at zero, the illumination from that light (now in shadow) 
must be recalculated. Still, lsuch counters must be associated with each intersection point for a scene 
with llights. To control the space used by the coun­ters, weset amaximum of b+1bits for each counter, 
hence handling up to 2b .1blockers. The extra bit represents over.ow and it is set only when more than 
2b .1blockers lie between the light and the in­tersection point. When removing a blocker, if any shadow 
counter is decremented to zero and its extra bit is set, we know that the point is still in shadow, but 
not the number of blockers. It is therefore nec­essary to re-shoot a ray towards the light with respect 
to the entire scene to update the counter value. If the value of bis zero, the extra bit becomes a simple 
.ag indicating if the light is visible or not. Be­cause the shadow counters are stored in the ray paths, 
a small value of breduces the number of ray paths, but the increased cost of hav­ing to re-evaluate more 
often shadow rays. If there are semi-transparent blockers among opaque blockers, a counter between two 
consecutive transparent blockers (along a shadow segment) is used. 4.3 Information Weight A color subtree, 
a ray tree and a tunnel do not have the same space and time requirements. The speci.cation of a weight 
with each piece of information, which indicates its relative importance among the others, is a more general 
approach. The weight can be a function taking into account (1) the memory size needed by the information, 
(2) the time needed to recalculate it, and (3) its latest access time. The system will thus give preference 
to remove information that is more space intensive, that is faster to rebuild, and that has been inactive 
for a long time. This weight func­tion provides a way to remove the less important information and to 
preserve the rest. The memory size of each type of information depends upon the implementation but is 
simple to estimate. The time to recalculate a piece of information can be measured empirically, or estimated 
by various means. For instance, due to its hierarchical nature, the time needed to compute a tunnel corresponds 
to the computing time of its subtunnels plus its own computing time. For tunnels, we can also consider 
its surface area as a weight factor since the probability of intersecting an object is proportional to 
this area. An information s inactive time is not measured in absolute time. Indeed, the wait between 
two successive manipulations should not in.uence the inactivity time. We suggest considering instead 
the number of changes since the latest access to the information.  5 Results This section provides some 
statistics on our current implementation. The scene on which the manipulations were applied appears in 
.g­ure 4, with the original image (a) before any manipulation and the resulting image (b) after all modi.cations. 
All times are in seconds and were gathered on a Silicon Graphics Indigo2 Extreme R4400, running at 150 
MHz, with 128 MB of RAM. The original scene consists of about 16,500 objects (13,500 poly­gons). It takes 
1,100 seconds to preprocess the original image at a 256.256resolution, Simply ray tracing the same scene 
requires 640 seconds. Statistics about the number of different pieces of in­formation as well as their 
respective memory requirements are pro­vided in table 2. To compare, we also give the same statistics 
for the simple scene of .gure 2. One can notice that although simpler in terms of geometry, the visibility 
complexity of .gure 2 is higher than the one of .gure 4, which is illustrated by larger memory needs 
for its color trees, ray trees and tunnels. The top section of table 3 shows statistics on color changes. 
The modi.able parameters that are integrated in the color tree are all tex­ture parameters de.ning a 
constant color, except for the color of the light source. Traversing the image quadtree to identify the 
pixels de­pendent upon the selection takes from 1 to 2 seconds. Using groups for this selection-dependent 
preprocessing (SDP) leads to 20-40% savings of the .rst color updates. Computing all colors from the 
ray paths takes less than 10 seconds. If all ray trees are used instead of recomputed from the ray paths, 
this time goes down by 40-70%. If the color trees themselves are used, this time is reduced by 90-95%. 
The four color changes in table 3 are all updated under half a second. The savings due to compression 
of the color trees are less signi.cant (1-20%). The optical change in the middle section of table 3 displays 
a similar behavior than .rst color changes with respect to the use of groups for the selection preprocessing 
(25%). Some statistics on geometrical changes are given in the bottom section of table 3. Changes are 
dependent upon the number of rays that must be shot and how expensive they are to intersect with re­spect 
to the entire scene. The use of tunnels greatly reduces all geo­metrical changes, especially when only 
a fraction of all ray trees are allowed to reside in memory. This is due to the fact that many ray trees 
do not need to be recomputed from ray paths because simply culled by the tunnels. Indeed, tunnels culled 
between 65-90% of the ray tests. The hierarchical nature of the image quadtree and tunnels shows that 
almost no performance is lost, even when using 10% of all ray trees.  6 Summary and Conclusion In this 
paper, we presented two tree structures allowing an incre­mental recomputation of the image after any 
modi.cation of a scene viewed from a .xed camera. The color tree preserves the entire expression leading 
to the .nal color of a pixel. Any changes affecting the value of parameters in these trees, such as shading 
and texture parameters, are quickly dis­played by re-evaluating only the subtrees dependent upon the 
mod­i.ed parameters. By storing the image in a quadtree of regions, a preprocessing step identi.es by 
a .ag each region within which at least one color tree is affected by a given color change. This preprocessing 
is, in addition, sped up by the notion of groups, where regions are formed by pixels with identical ray 
paths. So, an entire group can be elimi­nated by testing a single ray path. The ray tree preserves only 
the visibility speci.c information of the rays generated from a pixel. Any changes affecting a ray tree 
are handled by re-shooting rays from the previous valid intersection point. Any modi.cation in a ray 
tree is directly updated in its cor­responding color tree. All optical and geometrical changes can be 
handled with this structure. After a geometrical change, it is possible to avoid testing each individual 
ray segment with the current selection. To do so, the ray trees are combined into tunnels formed by a 
union of shafts. The ray segments modi.ed by a geometrical change are therefore quickly  a) Before changes 
b) After changes c) Higher resolution Figure 4: A more complex scene Table 3: Statistics on changes 
(times in seconds) detected by intersecting the selection with the hierarchical structure. Any change 
in the ray trees is updated in a bottom up fashion, from the modi.ed ray trees up only to its bounding 
tunnels. As a result, the images are usually updated in less than a second for most color changes. Optical 
and geometrical update times de­pend upon the number of rays shot, and upon the complexity of the object 
(selection or scene) to intersect with these rays. However the number of new rays is usually a small 
fraction of all rays necessary to render the image. The question of high memory requirements is addressed 
by prun­ing color subtrees and eliminating tunnels and ray trees. This in­formation can be ef.ciently 
recomputed on demand by keeping a pointer to its corresponding ray path. Also, an adjustable weight function 
based on memory size, recomputation time, and age of in­formation, helps to determine the best information 
to keep within the available memory space. The main conclusion we can draw from our scheme and its cur­rent 
results is that the entire visibility could be handled ef.ciently using only ray paths. This information 
does not require so much memory for the important gains it provides. All remaining mem­ory can be used 
to speed up speci.c changes, by building various structures such as color trees, ray trees, and tunnels. 
Their respec­tive memory spaces can be managed adaptively according to local changes. We expect the bene.ts 
of ef.cient incremental re-rendering to lead to more advanced interactive systems, since processing time 
can be concentrated on the phenomena the user is interested in, rather than on redundant rendering. 
 7 Future Work The current system suggests some interesting avenues to investi­gate. It could be easily 
extended to render ef.ciently animated se­quences from a .xed camera when a limited number of objects 
are moving. It should be possible to exploit time coherency from our knowledge of all motions. The hierarchical 
structures, ray tracing rendering and weight functions provide essential information about what is changing 
with respect to all previously computed information. All this knowledge makes the system a potential 
candidate for well balanced workload distribution in parallel processing, and memory management. For 
instance, the use of a large storage device provides an alternate so­lution to the memory usage for higher 
image resolution. Such a virtual storage has typically slower access time that can however be factored 
in the information weight when it is removed from the prime memory. So faster access memory acts then 
as cache which we could manage accordingly. The structures can also be used for other purposes. The similar­ity 
between image formation and light propagation [1] suggests to use the ray paths only for light preprocessing 
in order to handle the high memory requirements to reduce the aliasing effects. The incre­mental updating 
for changing scene geometry should avoid much unnecessary lighting recomputations. Furthermore, the beam-like 
shape of tunnels suggests a way to estimate the contribution of par­ticipating media, coherent ray tracing, 
and image .ltering.  Acknowledgments We would like to thank Chris Romanzin and Neil Stewart for their 
help. We ac­knowledge .nancial support from NSERC, FCAR, the Universit´e de Montr´eal, and Taarna Studios. 
 References [1] James Arvo. Backward ray tracing. SIGGRAPH 86 Tutorial notes on Develop­ments in Ray 
Tracing, August 1986. [2] Robert L. Cook. Shade trees. Proceedings of SIGGRAPH 84. In Computer Graphics, 
18, 3 (July 1984), pp. 223 231. [3] Franklin C. Crow. Shadow algorithms for computer graphics. Proceedings 
of SIGGRAPH 77. In Computer Graphics, 11, 2 (July 1977), pp. 242 248. [4] Thomas A. Funkhouser and Carlo 
H. S´equin. Adaptive display algorithm for in­teractive frame rates during visualization of complex virtual 
environments. Pro­ceedings of SIGGRAPH 93. In Computer Graphics Proceedings, Annual Con­ference Series, 
August 1993, pp. 247 254. [5] Ned Greene. Applications of world projections. Proceedings of Graphics 
Inter­face 86, (May 1986), pp. 108 114. [6] Ned Greene. Detecting intersection of a rectangular solid 
and a convex polyhe­dron. In Paul Heckbert, editor, Graphics Gems IV, pages 74 82. Academic Press, Boston, 
1994. [7] Ned Greene and M. Kass. Hierarchical Z-buffer visibility. Proceedings of SIG-GRAPH 93. In Computer 
Graphics Proceedings, Annual Conference Series, Au­gust 1993, pp. 231 240. [8] Brian Guenter, Todd B. 
Knoblock, and Erik Ruf. Specializing shaders. Proceed­ings of SIGGRAPH 95. In Computer Graphics Proceedings, 
Annual Conference Series, August 1995, pp. 343 350. [9] Eric Haines and John Wallace. Shaft culling for 
ef.cient ray-traced radiosity. In Eurographics Workshop on Rendering, 1991. [10] Pat Hanrahan and Paul 
E. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. Proceedings of SIGGRAPH 90. In Computer 
Graphics, 24, 4 (August 1990), pp. 215 223. [11] David A. Jevans. Object space temporal coherence for 
ray tracing. Proceedings of Graphics Interface 92, (May 1992), pp. 176 183. [12] Peter Litwinowicz and 
Gavin Miller. Ef.cient techniques for interactive texture placement. Proceedings of SIGGRAPH 94. In Computer 
Graphics Proceedings, Annual Conference Series, July 1994, pp. 119 122. [13] K. Murakami and K. Hirota. 
Incremental ray tracing. In Eurographics Workshop on Photosimulation, Realism and Physics in Computer 
Graphics, June 1989, pp. 17 32. [14] Mark Segal, Carl Korobkin, Rolf van Widenfelt, Jim Foran, and Paul 
E. Haeberli. Fast shadows and lighting effects using texture mapping. Proceedings of SIG-GRAPH 92. In 
Computer Graphics, 26, 2 (July 1992), pp. 249 252. [15] Carlo H. S´equin and Eliot K. Smyrl. Parameterized 
ray tracing. Proceedings of SIGGRAPH 89. In Computer Graphics, 23, 3 (July 1989), pp. 307 314. [16] Alan 
Watt and Mark Watt. Advanced Animation and Rendering Techniques: The­ory and Practice. Addison-Wesley 
Publishing Company, 1992. [17] Turner Whitted. An improved illumination model for shaded display. Communi­cations 
of the ACM, 23(6):343 349, June 1980. [18] Lance Williams. Casting curved shadows on curved surfaces. 
Proceedings of SIGGRAPH 78. In Computer Graphics, 12, (August 1978), pp. 270 274.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237213</article_id>
		<sort_key>91</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Interactive multiresolution surface viewing]]></title>
		<page_from>91</page_from>
		<page_to>98</page_to>
		<doi_number>10.1145/237170.237213</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237213</url>
		<keywords>
			<kw><![CDATA[geometric modeling]]></kw>
			<kw><![CDATA[multiresolution analysis]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
			<kw><![CDATA[viewer]]></kw>
			<kw><![CDATA[wavelets]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>E.1</cat_node>
				<descriptor>Trees</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152.10003161.10003163.10003415</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems->Record storage systems->Directory structures->B-trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10003634</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P17786</person_id>
				<author_profile_id><![CDATA[81100485570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Certain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P149786</person_id>
				<author_profile_id><![CDATA[81100620337]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jovan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popovic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045514</person_id>
				<author_profile_id><![CDATA[81100493833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeRose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17009957</person_id>
				<author_profile_id><![CDATA[81100301736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duchamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematics, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63624</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P298076</person_id>
				<author_profile_id><![CDATA[81100357122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Werner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stuetzle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Statistics, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Matthias Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. Multiresolution analysis of arbitrary meshes. In Robert Cook, editor, SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 173-182. ACM SIGGRAPH, Addison Wesley, August 1995. held in Los Angeles, California, 06-11 August 1995.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192223</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Adam Finkelstein and David Salesin. Multiresolution curves. Computer Graphics (SIGGRAPH '94 Proceedings), 28(3):261-268, July 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe. Progressive meshes. In SIGGRAPH 96 Conference Proceedings, Annual Conference Series. ACM SIGGRAPH, Addison Wesley, August 1996. held in New Orleans, Louisiana, 04-09 August 1996.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. In J.T. Kajiya, editor, SIGGRAPH 93 Conference Proceedings, Annual Conference Series, pages 19-26. ACM SIG- GRAPH, Addison Wesley, August 1993. held in Anaheim, California, 01-06 August 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222932</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Michael Lounsbery. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. PhD thesis, Department of Computer Science and Engineering, University of Washington, September 1994. Available as ftp://cs.washington.edu/pub/graphics/LounsPhd.ps.Z.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Michael Lounsbery, Tony DeRose, and Joe Warren. Multiresolution analysis for surfaces of arbitrary topological type. Submitted for publication, 1994. Preliminary version available as Technical Report 93-10-05b, Department of Computer Science and Engineering, University of Washington, January, 1994. Also available as ftp://cs.washington.edu/pub/graphics/TR931005b.ps.Z.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>67254</ref_obj_id>
				<ref_obj_pid>67253</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Stephane Mallat. A theory for multiresolution signal decomposition: The wavelet representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11(7):674-693, July 1989.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Yves Meyer. Ondelettes et fonctions splines. Technical report, S~minaire EDP, l~cole Polytechnique, Paris, 1986.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac and E Borrel. Multi-resolution 3D approximations for rendering. In B. Falcidieno and T.L. Kunii, editors,Modeling in Computer Graphics, pages 455-465. Springer-Verlag, June-July 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Peter Schr6der and Wim Sweldens. Spherical wavelets: Efficiently representing functions on the sphere. In Robert Cook, editor, SIG- GRAPH 95 Conference Proceedings, Annual Conference Series, pages 161-172. ACM SIGGRAPH, Addison Wesley, August 1995. held in Los Angeles, California, 06-11 August 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>286071</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Eric Stollnitz, Tony DeRose, and David Salesin. Wavelets for Computer Graphics: Theory and Applications~ Morgan-Kaufmann, 1996.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Multiresolution Surface Viewing z vy y. Andrew CertainJovan Popovic´Tony DeRose Tom Duchamp 
David Salesin Werner Stuetzle Department of Computer Science and Engineering Department of Mathematics 
Department of Statistics University of Washington Abstract A more sophisticated way of coping with both 
the transmission Multiresolution analysis has been proposed as a basic tool supporting com­pression, 
progressive transmission, and level-of-detail control of complex meshes in a uni.ed and theoretically 
sound way. We extend previous work on multiresolution analysis of meshes in two ways. First, we show 
how to perform multiresolution analysis of colored meshes by separately analyzing shape and color. Second, 
we describe ef.cient al­gorithms and data structures that allow us to incrementally construct lower resolution 
approximations to colored meshes from the geometry and color wavelet coef.cients at interactive rates. 
We have integrated these algorithms in a prototype mesh viewer that supports progressive transmission, 
dynamic display at a constant frame rate independent of machine characteristics and load, and interactive 
choice of tradeoff between the amount of detail in ge­ometry and color. The viewer operates as a helper 
application to Netscape, and can therefore be used to rapidly browse and display complex geometric models 
stored on the World Wide Web. CR Categories and Subject Descriptors: I.3.5 [Computer Graphics]: Computational 
Geometry and Object Modeling surfaces and object rep­resentations; J.6 [Computer-Aided Engineering]: 
Computer-Aided Design (CAD). Additional Keywords: Geometric modeling, wavelets, multiresolution analysis, 
texture mapping, viewer. 1 Introduction Three-dimensional meshes of large complexity are rapidly becom­ing 
commonplace. Laser scanning systems, for example, routinely produce geometric models with hundreds of 
thousands of vertices, each of which may contain additional information, such as color. Working with 
such complex meshes poses a number of problems. They require a large amount of storage and consequently 
are slow to transmit. Additionally, they contain more faces than can be in­teractively displayed on any 
current hardware. Existing viewers either do not deal with these problems at all, or do so only in crude 
ways, for example by showing wireframes or by displaying only a fraction of the faces during dynamic 
viewing, and then switching back to surfaces once the motion has stopped. z Department of Computer Science 
and Engineering, University of Washington, Box 352350, Seattle, WA 98195-2350 Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
and dynamic display problems is to use a precomputed sequence of lower detail approximations to the mesh. 
Such approximations can be computed, for example, using the method of Rossignac and Bor­rel [9]. During 
transmission, a cruder approximation is displayed while the next more detailed approximation is received. 
For dy­namic display, one chooses the highest detail approximation com­patible with a desired frame rate. 
A major disadvantage of this approach is that the total amount of data that has to be transmitted and 
stored is larger than the descrip­tion of the full resolution mesh. In fact, there is a tradeoff between 
granularity (the difference in resolution between successive models) on the one hand and transmission 
time and storage requirements on the other hand. Previous work [1, 5, 6, 10] has demonstrated that, at 
least in princi­ple, multiresolution analysis offers a uni.ed and theoretically sound way of dealing 
with these problems. A multiresolution representa­tion of a mesh consists of a simple approximation called 
the base mesh, together with a sequence of correction terms called wavelet coef.cients which supply the 
missing detail. The key point is that truncated sequences of wavelet coef.cients de.ne approximations 
to the mesh with fewer faces. Although promising, previous work is lacking in at least two ways. First, 
either color or geometry were represented in multiresolution form, but not both. Second, algorithms for 
reconstructing and dis­playing multiresolution meshes were much too slow for interactive use. In this 
paper we address both of these de.ciencies. We deal with complex colored meshes using separate multiresolution 
representa­tions for geometry and color that are combined only at display time. We also describe ef.cient 
algorithms and data structures that allow us to incrementally construct and render lower resolution approx­imations 
to the mesh from the color and geometry wavelet coef.­cients at interactive rates. The separation of 
color and geometry, together with our incremen­tal algorithms, allows the ef.cient implementation of 
the following features: Progressive transmission: We .rst transmit and display the base mesh and then 
transmit the wavelet coef.cients in decreasing or­der of magnitude. As wavelets are received, they are 
incorpo­rated into the approximation, and the approximation is periodi­cally re-rendered. In the examples 
we have tried the approxima­tion rapidly converges to the original mesh (see Color Plates 1(a d)). Only 
a small penalty is incurred for progressive transmission (see Section 5.1). Performance tuning: By truncating 
the color and geometry ex­display, we truncate the expansions at a level of detail that can be rendered 
with the desired frame rate. We monitor the frame rate and dynamically modify the level of detail in 
response to chang­ing machine load. Automatic texture map generation: The separation between color and 
geometry and the way in which they are represented allows us to take advantage of texture-mapping hardware, 
as de­scribed in Section 3.3. Color Plates 1(g) and 1(h) illustrate the gains obtained by exploiting 
texture mapping. For a given num­ber of polygons, texture mapping allows display of a far better approximation 
(Color Plate 1(h)), as all the polygons can be ded­icated to capturing geometric detail. Color can always 
be dis­played at full resolution because adding color detail does not in­crease the polygon count. Adapting 
to user preferences: Color and geometry expansions can be truncated independently. In the absence of 
texture map­ping, the number of faces of the resulting mesh will depend on the truncation thresholds. 
There will in general be many combi­nations of color threshold and geometry threshold that result in 
approximately the same number of faces (see Color Plates 1(e g)). Automatically .nding the combination 
giving the best look­ing approximation seems to be a hard problem, as it will cer­tainly depend on the 
model itself. Instead, we allow the user to interactively choose the tradeoff. To demonstrate our ideas, 
we have built a prototype viewer running as a helper application for Netscape. As demonstrated in the 
accom­panying videotape, our viewer can be used to rapidly browse and display complex geometric models 
stored on the World Wide Web. The rest of the paper is organized as follows. In Section 2 we present 
a brief summary of multiresolution analysis of colored meshes. In Section 3 we describe the basic data 
structures and algorithms for ef­.ciently constructing and rendering truncated models. In Section 4 we 
sketch the architecture of our viewer. In Section 5 we present the results of several numerical experiments. 
Finally, Section 6 contains a discussion and ideas for future work. 2 Background In this section we 
.rst present a synopsis of multiresolution analy­sis for piecewise linear functions on triangular meshes. 
For a more complete exposition, see Stollnitz et al. [11]. We then describe how to convert an arbitrary 
colored mesh to a parametric form amenable to multiresolution analysis. 2.1 Multiresolution analysis 
The central idea of multiresolution analysis is to decompose a func­tion into a low resolution ( coarse 
) part and a sequence of correc­tion ( detail ) terms at increasing resolutions. Multiresolution anal­ysis 
for functions on Rn was formalized by Meyer [8] and Mallat [7]. Lounsbery [5] and Lounsbery et al. [6] 
extended multiresolution analysis to a class of functions including functions de.ned on trian­gular meshes, 
which we call level J piecewise linear. A function f de.ned on a triangular mesh M0 is called level J 
piecewise linear if it is piecewise linear on the mesh MJ obtained by performing J recursive 4-to-1 subdivisions 
of the faces of M0 (see Figure 1). Let V j denote the vector space of level j piecewise linear functions 
on M0.Let ji denote the unique level j piecewise linear function as­ e suming value 1 at vertex i and 
value 0 at all other vertices of Mj. These level j hat functions form a basis of V j. In the context 
of mul­tiresolution analysis they are often referred to as scaling functions. The spaces V 0, V 1, form 
a nested sequence, as required by mul­ eee tiresolution analysis.  Figure 1 Recursive 4-to-1 splitting 
of a tetrahedron: (a)M0, (b) M1, (c) M2. Besides a nested sequence of spaces, the other basic ingredient 
of multiresolution analysis is an inner product. We use the inner prod­uct   hji XZ 2 fg= f (x) g(x) 
dx, xT T where the sum is taken over all faces of M0 and dx is the area ele­ment, normalized so that 
all faces of M0 have unit area. Given a nested sequence of function spaces and an inner product, we can 
now de.ne wavelets. The orthogonal complements W j of V j in V j+1, for 0 jJ, are called orthogonal wavelet 
spaces.A wavelet s i basis for V J consists of the level 0 scaling functions, together with bases for 
the wavelet spaces W 0,..., W J1. Given such a wavelet basis, we can express any level J piecewise linear 
function f on M0 as a linear combination of scaling functions and wavelets at various levels. Ideally 
we would like the wavelets, together with the level 0 scal­ing functions, to form an orthonormal basis 
for V J. We could then calculate the best k term L2 approximation to a function fV J by 2 keeping the 
k terms of the expansion with the largest coef.cients. On the other hand, we want wavelets to have small 
support so that the contribution to the approximation from each wavelet term can be rapidly incorporated 
into the model. Unfortunately, orthogonality of wavelet spaces and small spatial support of wavelets 
are con.icting goals. As small spatial support is essential for applications, we relax the orthogonality 
requirement. Lounsbery et al. [6] stipulate a priori the size k of the support and n then construct biorthogonal 
wavelets ij that span W j and are as or­thogonal as possible to V j. The wavelets obtained in this way 
are called k-disk wavelets [11]. More precisely, consider a vertex i of Mj+1 that is located at the mid­point 
of an edge e of Mj. The k-disk wavelet centered at vertex i is a function of the form ne X 2 e ij = 
ji +1 + sjv v , (1) vNk where Nk denotes a set of level j vertices in a neighborhood of vertex i. The 
neighborhoods Nk are de.ned recursively. The neighborhood N0 for the 0-disk wavelet consists of the endpoints 
of e; the neigh­borhood Nk contains the vertices of all triangles incident on Nk1 (see Figure 2). The 
wavelet consisting of only the levelj + 1 scaling function is called the lazy wavelet. The coef.cients 
sjiv are chosen to minimize the norm of the orthog­onal projection of ij onto V j. They are determined 
by solving the following system of linear equations:   X 2 h e j ne i =ah ee ji j 2 ju jvsjiv = ju 
ji +1 , forall u Nk. vNk Note that the system is local to vertex i. The size of the system for 0-disk 
wavelets is only 2 2. For larger values of k the size of the == system depends on the valence of the 
parent vertices; in regular re­gions of the mesh where all vertices have valence 6, the system has size 
10 10 for k = 1, and size 24 24 for k = 2. The process of expressing a level J piecewise linear function 
in terms of level 0 scaling functions and wavelets is called .lterbank analysis. For a description see 
Stollnitz et al. [11].  Figure 2 (a) The support of the 1-disk wavelet j. Dark shaded area:  i N0-neighborhood 
of center edge; light shaded area: faces added to form N1-neighborhood. (b) The triangles required to 
introduce j i during reconstruction. (c) The graph of ij .  2.2 Conversion of colored meshes to multiresolution 
form Multiresolution analysis of a colored mesh M is based on the premise that M is de.ned parametrically 
by two vector valued level J piecewise linear functions, a geometry function fgeom and a color function 
fcolor, each mapping a triangular base mesh M0 into R3 . Typically, M will not be given in this form, 
but instead in the form of vertices, edges, and faces, vertex positions, and vertex colors. In or­der 
to apply multiresolution analysis, M must be converted to para­metric form. We do this by .rst applying 
the remeshing algorithm of Eck et al. [1]. The output of the remeshing algorithm is a base mesh M0 with 
a relatively small number of faces, a parameteriza­tion : M0 M, and an approximation of by a level J 
piecewise n gg n P e linear embedding fgeom : M0 R3 of the form fgeom = i fi iJ , where fi are vectors 
in R3 representing the geometric positions of the vertices of MJ . We next apply the .lter bank analysis 
algorithm of Lounsberyet al. [6] to obtain a wavelet expansion offgeom. Note that this analysis will 
generate a vector of three coef.cients for each wavelet, one for each of the three coordinate functions. 
We sort these coef.cient vectors in order of decreasing length and then store them together with iden­ti.ers 
for the wavelets (center vertex and level) in a .le called the geometry-wavelet .le. We now turn to multiresolution 
analysis of color. Color is originally given at the vertices of M, and can be extended to all of M by 
linear interpolation. The parametrization: M0 M obtained during nxx g x remeshing induces a color function 
on M0. To construct a level J piecewise linear approximation fcolor to , we sampleat the ver­tices of 
MJ . As in the case of geometry, we then compute the wavelet expansion of fcolor by .lterbank analysis 
and store the wavelet coef­.cient vectors in order of decreasing length in a color-wavelet .le. The base 
mesh M0, its vertex positions (the coef.cients of the level 0 scaling functions in the expansion of fgeom) 
and its vertex colors are stored in a base .le. The geometry-wavelet .le, the color-wavelet .le, and 
the base .le constitute the input to our multiresolution viewer. 3 Algorithms and data structures In 
this section we describe the algorithms and data structures that form the basis of our multiresolution 
viewer. We assume that the colored mesh is represented in multiresolution form, i.e., by a base mesh 
and wavelet expansions of the color and geometry functions. At the full resolution, the number of faces 
ofMJ is 4J times the num­ber of faces of M0. The faces of MJ can be naturally organized into a tree Q. 
The root of Q has as many children as there are faces in the base mesh, while every other internal node 
has four children. Each leaf of Q corresponds to a face of MJ . This tree organization was also used 
by Schroeder and Sweldens [10]. The mesh is rendered by traversing the tree Q, evaluating fgeom and fcolor 
at the vertices, and generating a colored triangle for each leaf. In the absence of texture-mapping hardware, 
color and geometry are handled identically, so we will couch the discussion in terms of ge­ometry alone. 
The use of texture mapping is the topic of Section 3.3. First some terminology: let fr denote the approximation 
to fgeom obtained by summing the scaling functions and the largest r wavelets, and let Qr denote the 
smallest subtree of Q we for which fr geom geom is linear on each leaf. For progressive transmission 
we .rst transmit the base meshM0 and the coef.cients of the level 0 scaling functions. The associated 
tree Q0 consists only of the root node and as many leaves as there are faces in the base mesh. As wavelets 
arrive, we incrementally grow Qr and update fr , and periodically render the mesh. geom Use of the wavelet 
representation for performance tuned view­ing and level-of-detail control is based on the observation 
that for small r, the tree Qr will also be small, and therefore rendering Qr will result in many fewer 
triangles than rendering Q. In principle we could generate approximations with almost any desired number 
of faces by growing from the base mesh. For ef.ciency reasons we cache trees and vertex positions for 
a sequence of approximations, and then grow the desired tree from the closest approximation with fewer 
than the desired number of faces. 3.1 Data structures As previously stated, the primary data structure 
used to represent the mesh is a tree Q, which has as many descendents from the root as there are faces 
in the base mesh and is a quadtree for all other levels. We represent all nodes of Q, except for the 
root, with the following data structure: type Face = record level: Integer children[4]: array of pointer 
to Face cornerVertex[3]: array of pointer to Vertex edgeVertex[3]: array of pointer to Vertex end record 
A face is said to be of level j if it is a face of Mj. The array corner-Vertex has pointers to three 
vertices of the face, and the arrayedge-Vertex has pointers to three vertices that subdivide the edges 
of this face. We represent vertices with the following data structure: type Vertex = record parentV[2]: 
array of pointer to Vertex parentF[2]: array of pointer to Face fGeom: XYZposition fColor: RGBcolor g: 
XYZvector hGeom[ ]: array of HatFunctionCoef.cients hColor[ ]: array of HatFunctionCoef.cients end record 
The array parentV contains pointers to the two vertices on either end of the edge that the vertex subdivides 
 these are called parent ver­tices of the vertex. The array parentF contains pointers to the two faces 
on either side of the edge that the vertex subdivides these are called the parent faces of the vertex. 
A vertex is said to be of level j if it was created at the j-th level of subdivision, i.e., if its par­ent 
faces are of level j 1. The .elds fGeom and fColor contain the a values of fgeomr and fr color at the 
vertex. The role of hGeom and hColor is explained in Section 3.2. Vertices of level j 0 are indexed by 
the base face they lie in, to­ . gether with their barycentric coordinates within the face. As it is 
of­ten necessary to .nd the node representing a vertex from its index, we maintain an auxiliary hash 
table that maps vertex indices to ver­tex nodes. Whenever a vertex is created, it is added to the table. 
 3.2 Algorithms Suppose we have already constructed the face tree Qr and evalu­ated fr at all its vertices. 
Adding a wavelet requires growing Qr geom into Qr+1 and evaluating fr+1 . For ef.ciency reasons we do 
not re­ n geom evaluate fr+1 for every new wavelet. Instead we gather a sequence geom of s wavelets, 
then evaluate fr+s when the new mesh is rendered. n geom We now describe the gather and evaluate stages. 
3.2.1 The gather stage Gathering a wavelet ij with wavelet coef.cient aij involves three steps: 1. Decompose 
the term ij into a sum of hat functions at level j and j + 1 according to Equation (1). 2. For each 
hat function in the decomposition, grow the current face tree to accommodate it. A face tree is said 
to accommodate a function if the function is linear over each face. This process is described more fully 
below. 3. For each hat function in the decomposition vj, j= j, j + 1, centered at vertex v, update 
the hGeom .eld of v:  0 00 ee 0 0 e 0 jjj j v.hGeom[j] += aisv , where sv is the coef.cient of v in 
the decomposition of step 1. The most complicated part of gathering is growing the current face tree 
Qr to accommodate a level j hat function jv centered at a ver­tex v. We call a level j vertex complete 
if its parent faces have been subdivided. (By de.nition, all level 0 vertices are complete.) As each 
vertex has pointers to its two parent faces (nil if a parent face does not exist), it is easy to test 
a vertex for completeness. Clearly, Qr can accommodate a hat function vj if the level j neigh­ e bors 
of vertex v are complete. Thus, there is a simple recursive pro­cedure to make a vertex complete: Make 
its two parent vertices complete; Subdivide the two parent faces of the vertex. Whenever a new vertex 
w is created in the completion process, fr geom is evaluated at the vertex, and the value is recorded 
in w.fGeom. Since fr is linear on the edges of Qr, this evaluation is accom­ geom plished by averaging 
the fGeom values of w s parent vertices. While this growing process is simple, it can generate more than 
the minimum number of triangles needed to accommodate a hat func­tion (see Figure 3). 3.2.2 The evaluate 
stage Recall that wavelets are added in two stages. In the gather stage the face tree is grown so that 
it contains all the faces necessary to ac­commodate the new wavelets. At this stage we also compute the 
val­ues of the current approximation fr at the newly introduced ver­ geom Figure 3 Making a vertex complete: 
(a) A vertex to be made com­plete. (The dashed faces are the minimal number that must be added to make 
the vertex complete.) (b) The parent vertices are created and made complete by subdividing their parent 
faces. (c) Subdividing the parent faces of the vertex makes it complete. tex positions. The wavelets 
are decomposed into hat functions, and the coef.cient arrays for their center vertices are updated. The 
new geometry function fr+s is not evaluated until the tree is rendered, at geom which time the contributions 
from all the hat functions are summed in a single tree traversal. We will now describe this evaluation 
stage. Let g denote the sum of all the hat functions gathered since the last evaluation stage, and let 
gk denote the partial sum obtained by adding all the contributions from hat functions of levelk or smaller. 
By construction g = gL, where L is the maximum level of any leaf of Qr+s. Note that since gk is linear 
over the faces of level k and above, it is completely determined by its values at the vertices of Qr+s 
of level k and less. We now present an inductive procedure to compute the values ofgL at all of the vertices 
of Qr+s . It is easy to compute the values of g0 at the level 0 vertices they are the coef.cients of 
the gathered level 0 hat functions. Next we describe how to compute the values ofgk+1 at all vertices 
of Qr+s of level k + 1 and smaller from the values of gk at all vertices of level k and smaller. Let 
hkv +1 denote the coef.cient of the level k +1 hat function centered at v. If v is a vertex of level 
k or less, then gk+1(v)= gk (v)+ hkv +1. If v is a level k + 1 vertex, then it splits an edge connecting 
its two level k parent vertices, Therefore, gk(v) is the average of the values of gk at its parent vertices, 
and gk+1(v)= gk(v)+ hkv +1. The calculation of gL can be performed ef.ciently during a breadth .rst traversal 
of Qr+s, as summarized in the pseudocode given in Fig­ure 4.  3.3 Treatment of color As mentioned earlier, 
in the absence of texture-mapping hardware color and geometry are handled identically: both color and 
geome­try wavelets are gathered and evaluated as described in the previous section. Representing colored 
meshes in multiresolution form makes it easy to exploit texture mapping hardware. The basic idea is to 
associate a region of texture memory with each face of the base mesh. If the full resolution model is 
subdivided to level J,a2J 2J texture map = is allocated, but only the lower diagonal is actually used. 
(To reduce the wasted texture memory, we pair adjacent base mesh faces when­ever possible. We then allocate 
a square region of texture memory to the pair.) Since geometry is represented parametrically by a piecewise 
linear function over MJ , there is a straightforward solution for the nor­mally dif.cult problem of generating 
texture coordinates for arbi­trary meshes. The texture coordinates for any vertex are simply the pre-image 
of the vertex under the parametrization. Therefore, the corner vertices of a base mesh face have texture 
coordinates (0,0), (1,0), and (0,1), and the texture coordinates for every other vertex are the average 
of its parents coordinates. The image displayed in procedure Evaluate() queue Level 0 faces . do while 
queue != empty currentFace GetFirstFace(queue) currentLevel currentFace.level if IsSubdivided(currentFace) 
then for each cornerVertex v of currentFace do v.g += v.hGeom[currentLevel] v.hGeom[currentLevel] 0 . 
end for for each edgeVertex e of currentFace do if e has two parent faces then   f. g e will be visited 
twice, so add 1/2 per visit e.g += 0.25 (e.parentV[1].g+e.parentV[2].g) else e.g += 0.5 (e.parentV[1].g+e.parentV[2].g) 
end if end for for each i 0, 1, 2, 3 do 2 Append currentFaces.children[i] to queue else for each cornerVertex 
v of currentFace v.fGeom += v.g + v.hGeom[currentLevel] v.g v.hGeom[currentLevel] 0 ..  end for AddToDisplayList(currentFace) 
end if end while end procedure  Figure 4 Figure 5 illustrates texture mapping. The base mesh has been 
ren­dered with only the scaling functions of fgeom, but with all of the terms of fcolor. The texture 
map associated with a face of the base mesh is initial­ized by linearly interpolating between the colors 
at the vertices of the face (i.e., the level 0 color scaling function coef.cients). The texture map is 
updated as soon as color wavelets are received, es­sentially by painting the wavelet into the texture 
map. Since the addition of color wavelets does not increase the triangle count, sys­tems with texture-mapping 
hardware color can always display color at its highest resolution. 4 Viewer Architecture Our viewer, 
written in OpenGL and Motif for Silicon Graphics Iris workstations, is con.gured as a helper application 
for Netscape. When a multiresolution-surface link is followed, the viewer appli­cation opens an HTTP 
connection for the base mesh .le. After re­ceiving the base mesh, the viewer displays it in a graphics 
window (see Figure 5) and opens two parallel HTTP connections, one for the color wavelets .le and one 
for the geometry wavelets .le. As wavelet coef.cients are received they are incorporated as described 
in Section 3, and the model is periodically redisplayed. Color Plates 1(a d) illustrate a model at various 
stages of transmission. Assum­ing a 64Kbs link (ISDN speeds), the images shown represent, from top to 
bottom, the model after 3 seconds, 17 seconds, 59 seconds, and 180 seconds (the full model). In standard 
operation, the quality of the model displayed in the viewer is controlled by the slider labeledFrame 
Time. When the user is rotating or translating the model, the viewer attempts to maintain that frame 
rate by measuring the polygon performance for the previ­ous frames and predicting the desired model size 
for the upcoming frame. When there is no interaction, a more re.ned model is ren­dered, allowing the 
user to see more detail. If the re.ned model takes a signi.cant time to render, the rendering is performed 
in stages, so that the viewer can check for user events during the rendering. If the user decides to 
interact with the model while the viewer is drawing 1 Plate wavelet # geom # color # polys L2 Ltype 
wavelets wavelets error error (a) 0-disk 770 830 4701 .0961 .3217 (b) 0-disk 4166 4445 22725 .0375 .0949 
 (c) 0-disk 14350 14605 56418 .0076 .0136 (d) 0-disk 49530 49530 98304 2.3e-6 1.92e-6 (e) 0-disk 114 
811 3006 .2555 .5246 (f) 0-disk 371 567 3033 .1607 .3461 (g) 0-disk 743 324 3015 .1225 .2777 (h) 0-disk 
774 49530 2994 .1203 .2777 (i) Lazy 16380 16380 32760 6.7e-8 4.7e-7 (j) Lazy 1254 1350 5561 .0099 .0503 
 (k) 0-disk 1129 1084 5510 .0075 .0459 (l) 2-disk 735 883 5573 .0092 .0676  Table 1 Statistics for 
Color Plate 1 a re.ned model, rendering is aborted, and the system returns to in­teractivity. The quality 
of the model can also be controlled in two other ways: the user can explicitly set either the number 
of geometry and color wavelets to be added to the base mesh, or the number of polygons to be used in 
creating the approximation. If either the frame time or the number of polygons is speci.ed, the tradeoff 
between color and geometry is controlled with the slider la­beled Color to Geom. Moving the slider to 
the left indicates a pref­erence for geometry detail, whereas moving it to the right indicates a preference 
for color detail. The tradeoff is shown in Color Plates 1(e g), where (e) corresponds to a strong preference 
for color, (g) corresponds to a strong preference for geometry, and (f) corresponds to a balance between 
the two. Each of these model consists of the same number of Gouraud shaded polygons. The color/geometry 
slider is only active on machines without texture-mapping hardware. If the machine has texture-mapping 
hardware, color wavelets do not increase the polygon count, so they are always included. Color Plate 
1(h) shows the model that can be displayed for the same polygon budget used in Plates 1(e g). 5 Results 
In this section we present various statistics for the color plates, and we describe a number of numerical 
experiments we have performed. Statistics for the color plates are summarized in Table 1. Three dif­ferent 
types of wavelets were used as indicated by the second col­umn. All examples were computed using the 
same type of wavelet for both color and geometry, although in principle different types of wavelets could 
be used. The other columns should be self­explanatory. The errors reported in the last two columns are 
normal­ized so that the crudest model has error 1. In addition to using the viewer to create the color 
plates, we con­ducted a set of numerical experiments to compare the performance of four types of wavelets: 
lazy, 0-, 1-, and 2-disk wavelets. The ex­periments focused on the following factors: Convergence as 
a function of number of wavelet coef.cients: For .xed network bandwidth, the rate at which the transmitted 
model approaches the original depends on how quickly the error decreases as a function of the number 
of wavelet coef.cients. Figure 6 is a plot of L2 error in geometry vs. number of coef.­cients for the 
various types of wavelets for the head model shown 1 in Color Plates 1(e h). The plot of Lerror is qualitatively 
sim­ Figure 5 The multiresolution viewer. k-disk wavelets, but there seems to be no signi.cant difference 
between various values of k. Convergence as a function of number of polygons: For .xed polygon display 
rate and update frequency, the visual appearance of the model depends on how quickly the error decreases 
as a function of the number of polygons in the model. Figure 7 is a plot of the L2 error vs. number of 
polygons for the 1 same head model. Again, the corresponding plot for theLerror is qualitatively similar. 
Color Plates 1(i l) illustrate the visual .delity for the earth model when different types of wavelets 
are used to produce a model with a .xed polygon count. Table 5 indicates that the error for this number 
of polygons is actually less for lazy wavelets than for 2-disk wavelets, due to the large number of polygons 
that a 2­disk wavelet may introduce. Color plate 1(i) is the full-resolution earth model, subdivided 
to level-6. The next three color plates, 1(j l), depict the earth reconstructed to approximately 5500 
poly­gons using lazy wavelets, 1(j), 0-disk wavelets, 1(k), and 2-disk wavelets, 1(l). Although there 
are visual differences between the images, it is not clear which is preferable. Our conclusion is again 
that lazy wavelets perform slightly worse than k-disk wavelets numerically, but there apparently is no 
sig­ni.cant difference between various values of k. Visually, there is no clear preference. Numerical 
stability: In the conversions to and from multiresolu­tion form some numerical error is inevitable. While 
the numer­ical stability properties of orthogonal wavelet constructions are 100 10 1 0.1 0.01  
Error (log) Error (log) # of coeffs (log) Figure 6 L2 error vs. number of wavelet coef.cients. 100 
 10 1 0.1 0.01  10 100 1000 10000 100000 # of polys (log) Figure 7 L2 error vs. number of polygons. 
relatively well understood, stability of biorthogonal schemes like ours is less clear. Lacking theory 
to guide us, we ran the following experiment on the earth model. For each of the four types of wavelets 
we per­formed wavelet analysis followed by wavelet synthesis on a level J = 6 version of the model. For 
lazy wavelets, the relative error in the vertex positions was on the order of the machine precision. 
For 0-, 1-, and 2-disk wavelets, the relative errors were on the or­der of 0.00005, 0.001, and 0.002. 
When we reran the experiment using a level 3 version of the earth, the relative error for 2-disk wavelets 
was reduced to 1 in 106. Our conclusion is that wavelets with smaller supports are likely to be more 
stable numerically than those with larger ones, and that stability becomes increasingly important as 
the number of levels increases. Speed: Wavelets with larger support clearly take longer to add. There 
are potentially more new faces to introduce, and there are always a greater number of vertices whose 
hat function coef.­cients need to be updated. We ran a series of timing experiments and found that on 
average each type of wavelet could be added (the gather stage) at the following rate: lazy, 2700 coef.cients 
per second; 0-disk, 2300; 1-disk, 1200; 2-disk, 600. The time for the evaluate stage was unchanged relative 
to wavelet size, which was expected.  Overall, we conclude that 0-disk wavelets combine good visual 
.­delity for a given number of coef.cients and for a given number of polygons, with good numerical stability 
and computation time. These .ndings, however, are preliminary, and require further con­.rmation. 5.1 
Data encoding As mentioned in the introduction, there is a small penalty for rep­resenting a mesh in 
multiresolution form. Since the wavelet coef­.cients are sorted in magnitude order for progressive transmission, 
we need to transmit with each coef.cient the vertex identi.er for the center of the wavelet. This information 
could be made implicit if the complete model was transmitted before any processing or display took place. 
We use a simple encoding which represents the coef.cient for a color or geometry wavelet with three .oating 
point numbers, to­gether with a word of information for the vertex identi.er. This rep­resents a 33% 
penalty for the bene.t of progressive transmission. A suggestion for reducing this penalty is described 
below.  6 Discussion and future work We have extended previous work on multiresolution analysis of meshes 
in two ways. First, we have shown how to perform multires­olution analysis of colored meshes by separately 
analyzing shape and color. Second, we have developed ef.cient algorithms and data structures that allow 
us to incrementally construct lower resolution approximations to colored meshes at interactive rates. 
We have integrated these algorithms in a prototype mesh viewer that supports progressive transmission, 
dynamic display at a con­stant frame rate independent of machine performance and load, and the ability 
to interactively trade off the amount of detail in geome­try and color. The separation of geometry and 
color also allows us to make ef.cient use of texture-mapping hardware. In future work we intend to investigate: 
Multiresolution editing: In analogy to Finkelstein and Salesin s work on multiresolution curves [2] we 
plan to extend our mul­tiresolution viewer to allow editing of meshes at different levels of detail. 
Other wavelets: We currently use piecewise linear wavelets to represent geometry and color. When modeling 
smooth objects or objects without sharp color transitions, use of smooth wavelets may result in better 
compression. Automatic tradeoff between color and geometry: If there is no texture-mapping hardware, 
adding wavelets for either color or geometry will increase the number of polygons that have to be rendered. 
When there is an upper bound on the number of poly­gons, for example during dynamic viewing, one has 
to choose be­tween color detail and geometry detail. Currently the tradeoff is left to the user. Heuristics 
for automatically choosing a tradeoff that results in a visually close approximation would be useful. 
Comparison to progressive meshes: In simultaneous work Hoppe [3] has introduced the notion ofprogressive 
meshes to ad­dress the dif.culties of storage, transmission, and display of com­plex meshes. The basic 
idea is to record the changes a mesh opti­mizer [4] makes as it simpli.es a mesh. Since the original 
mesh can be recovered by running the record of changes in reverse, the progressive mesh representation 
is the simplest mesh together with the record of changes in reverse order. The relative advan­tages and 
disadvantages of such an approach need further study. Better encoding: The wavelet coef.cients for a 
particular model typically span a large dynamic range, making .oating point an obvious choice for encoding 
their values. Better use of bandwidth and storage could be made, however, by taking advantage of the 
wavelets being sorted in magnitude order. Fixed point numbers could be transmitted for each coef.cient, 
with the scale infor­mation being transmitted only as it changes. This improvement could potentially 
eliminate the overhead incurred for progressive transmission.  Acknowledgments This work was supported 
in part by the National Science Foundation under grants CCR-8957323, DMS-9103002, and DMS-9402734, by 
an Alfred P. Sloan Research Fellowship (BR-3495), an NSF Presi­dential Faculty Fellow award (CCR-9553199), 
an ONR Young In­vestigator award (N00014-95-1-0728), and industrial gifts from In­terval, Microsoft, 
and Xerox. Head models courtesy of Cyberware. References [1] Matthias Eck, Tony DeRose, Tom Duchamp, 
Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. Multiresolution analysis of arbitrary meshes. In 
Robert Cook, editor, SIGGRAPH 95 Conference Proceed­ings, Annual Conference Series, pages 173 182. ACM 
SIGGRAPH, Addison Wesley, August 1995. held in Los Angeles, California, 06-11 August 1995. [2] Adam Finkelstein 
and David Salesin. Multiresolution curves. Com­puter Graphics (SIGGRAPH 94 Proceedings), 28(3):261 268, 
July 1994. [3] H. Hoppe. Progressive meshes. In SIGGRAPH 96 Conference Proceedings, Annual Conference 
Series. ACM SIGGRAPH, Addison Wesley, August 1996. held in New Orleans, Louisiana, 04-09 August 1996. 
[4] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. In J.T. Kajiya, 
editor, SIGGRAPH 93 Conference Proceedings, Annual Conference Series, pages 19 26. ACM SIG-GRAPH, Addison 
Wesley, August 1993. held in Anaheim, California, 01-06 August 1993. [5] J. Michael Lounsbery. Multiresolution 
Analysis for Surfaces of Arbi­trary Topological Type. PhD thesis, Department of Computer Science and 
Engineering, University of Washington, September 1994. Avail­able as ftp://cs.washington.edu/pub/graphics/LounsPhd.ps.Z. 
[6] Michael Lounsbery, Tony DeRose, and Joe Warren. Multiresolu­tion analysis for surfaces of arbitrary 
topological type. Submit­ted for publication, 1994. Preliminary version available as Techni­cal Report 
93-10-05b, Department of Computer Science and Engi­neering, University of Washington, January, 1994. 
Also available as ftp://cs.washington.edu/pub/graphics/TR931005b.ps.Z. [7] Stephane Mallat. A theory 
for multiresolution signal decomposition: The wavelet representation. IEEE Transactions on Pattern Analysis 
and Machine Intelligence, 11(7):674 693, July 1989. [8] Yves Meyer. Ondelettes et fonctions splines. 
Technical report, S´eminaire EDP, Ecole Polytechnique, Paris, 1986. ´ [9] J. Rossignac and P. Borrel. 
Multi-resolution 3D approximations for rendering. In B. Falcidieno and T.L. Kunii, editors,Modeling in 
Com­puter Graphics, pages 455 465. Springer-Verlag, June-July 1993. [10] Peter Schr¨oder and Wim Sweldens. 
Spherical wavelets: Ef.ciently representing functions on the sphere. In Robert Cook, editor, SIG-GRAPH 
95 Conference Proceedings, Annual Conference Series, pages 161 172. ACM SIGGRAPH, Addison Wesley, August 
1995. held in Los Angeles, California, 06-11 August 1995. [11] Eric Stollnitz, Tony DeRose, and David 
Salesin. Wavelets for Com­puter Graphics: Theory and Applications. Morgan Kaufmann, 1996. a) 3 seconds... 
e) high color detail i) full resolution b) 17 seconds... f) equal color and geometry detail j) lazy 
wavelets c) 59 seconds... g) high geometry detail k) 0-disk wavelets d) full resolution h) texture 
mapping l) 2-disk wavelets Color Plate 1 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237216</article_id>
		<sort_key>99</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Progressive meshes]]></title>
		<page_from>99</page_from>
		<page_to>108</page_to>
		<doi_number>10.1145/237170.237216</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237216</url>
		<keywords>
			<kw><![CDATA[geometry compression]]></kw>
			<kw><![CDATA[level of detail]]></kw>
			<kw><![CDATA[mesh simplification]]></kw>
			<kw><![CDATA[progressive transmission]]></kw>
			<kw><![CDATA[shape interpolation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor>Approximate methods</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P112851</person_id>
				<author_profile_id><![CDATA[81100397561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hugues]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoppe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>546208</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[APPLE COMPUTER, INC. 3D graphics programming with QuickDraw 3D. Addison Wesley, 1995.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237213</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CERTAIN, A., POPOVIC, J., DUCHAMP, T., SALESIN, D., STUETZLE, W., AND DEROSE, T. Interactive multiresolution surface viewing. Computer Graphics (SIGGRAPH '96 Proceedings) (1996).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CLARK, J. Hierarchical geometric models for visible surface algorithms. Communications of the ACM 19, 10 (Oct. 1976), 547-554.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237220</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COHEN, J., VARSHNEY, A., MANOCHA, D., TURK, G., WEBER, H., AGARWAL, P., BROOKS, F., AND WRIGHT, W. Simplification envelopes. Computer Graphics (SIGGRAPH '96 Proceedings) (1996).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B., AND LEVOY, M. A volumetric method for building complex models from range images. Computer Graphics (SIGGRAPH '96 Proceedings) (1996).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DEERING, M. Geometry compression. Computer Graphics (SIGGRAPH '95 Proceedings) (1995), 13-20.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, ~/{., AND STUETZLE, W. Multiresolution analysis of arbitrary meshes. Computer Graphics (SIGGRAPH '95 Proceedings) (1995), 173-182.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166149</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FUNKHOUSER, T., AND S~,QUIN, C. Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. Computer Graphics (SIGGRAPH '93 Proceedings) (1995), 247-254.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, J., AND STUETZLE, W. Mesh optimization. Computer Graphics (SIGGRAPH '93 Proceedings) (1993), 19-26.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222932</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, J. M. Multiresolution analysis for swfaces of arbitrary topological type. PhD thesis, Dept. of Computer Science and Engineering, U. of Washington, 1994.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, ~/{., DEROSE, T., AND WARREN, J. Multiresolution analysis for surfaces of arbitrary topological type. Submitted for publication. (TR 93-10-05b, Dept. of Computer Science and Engineering, U. of Washington, January 1994.).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[ROSSIGNAC, J., AND BORREL, P. Multi-resolution 3D approximations for rendering complex scenes. In Modeling in Computer Graphics, B. Falcidieno and T. L. Kunii, Eds. Springer-Verlag, 1993, pp. 455-465.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[SCHRODER, P., AND SWELDENS, W. Spherical wavelets: Efficiently representing functions on the sphere. Computer Graphics (SIGGRAPH '95 Proceedings) (1995), 161-172.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[SCHROEDER, W., ZARGE, J., AND LORENSEN, W. Decimation of triangle meshes. Computer Graphics (SIGGRAPH '92 Proceedings) 26, 2 (1992), 65-70.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[TAUBIN, G., AND ROSSIGNAC, J. Geometry compression through topological surgery. Research Report RC-20340, IBM, January 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[TURAN, G. Succinct representations of graphs. Discrete Applied Mathematics 8 (1984), 289-294.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Re-tiling polygonal surfaces. Computer Graphics (SIGGRAPH '92 Proceedings) 26, 2 (1992), 55-64.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[UPSTILL, S. The RenderMan Companion. Addison-Wesley, 1990.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>214771</ref_obj_id>
				<ref_obj_pid>214762</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WITTEN, I., NEAL, R., AND CLEARY, J. Arithmetic coding for data compression. Communications of the ACM 30, 6 (June 1987), 520-540.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237217</article_id>
		<sort_key>109</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Real-time, continuous level of detail rendering of height fields]]></title>
		<page_from>109</page_from>
		<page_to>118</page_to>
		<doi_number>10.1145/237170.237217</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237217</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P224643</person_id>
				<author_profile_id><![CDATA[81100040340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lindstrom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Center for GIS and Spatial Analysis Technologies, Georgia Tech Research Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP55041439</person_id>
				<author_profile_id><![CDATA[81350587620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Center for GIS and Spatial Analysis Technologies, Georgia Tech Research Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39023907</person_id>
				<author_profile_id><![CDATA[81100028256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ribarsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Center for GIS and Spatial Analysis Technologies, Georgia Tech Research Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14021263</person_id>
				<author_profile_id><![CDATA[81100023789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hodges]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Center for GIS and Spatial Analysis Technologies, Georgia Tech Research Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P208625</person_id>
				<author_profile_id><![CDATA[81100423338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faust]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology, Center for GIS and Spatial Analysis Technologies, Georgia Tech Research Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15026695</person_id>
				<author_profile_id><![CDATA[81351596443]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Turner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SAIC, Simulation Technology Division]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166131</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AKELEY, K. RealityEngine Graphics. Proceedings of SIGGRAPH 93. In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIG- GRAPH, pp. 109-116.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[COSMAN, M. A., MATHISEN, A. E., and ROBINSON, J. A. A New Visual System to Support Advanced Requirements. In Proceedings, IMAGE V Conference, June 1990, pp. 370-380.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>220334</ref_obj_id>
				<ref_obj_pid>220279</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DE BERG, M. and DOBRINDT, K. T. G. On Levels of Detail in Terrains. In 11 th ACM Symposium on Computational Geometry, June 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>225297</ref_obj_id>
				<ref_obj_pid>225294</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DE FLORIANI, L. and PuPPO, E. Hierarchical Triangulation for Multiresolution Surface Description. ACM Transactions on Graphics 14(4), October 1995, pp. 363-411.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DOUGLAS, D. H. Experiments to Locate Ridges and Channels to Create a New Type of Digital Elevation Model. Cartographica 23(4), 1986, pp. 29-61.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, M., and STUETZLE, W. Multiresolution Analysis of Arbitrary Meshes. Proceedings of SIGGRAPH 95. In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH, pp. 173-182.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FALBY, J. S., ZYDA, M. J., PRATT, D. R., and MACKEY, R. L. NPSNET: Hierarchical Data Structures for Real-Time Three-Dimensional Visual Simulation. Computers &amp; Graphics 17(1), 1993, pp. 65-69.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FERGUSON, R. L., ECONOMY, R., KELLY, W. A., and RAMOS, P. P. Continuous Terrain Level of Detail for Visual Simulation. In P~vceedings, IMAGE V Conference, June 1990, pp. 144-151.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807444</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FOWLER, R. J. and LITTLE, J. J. Automatic Extraction of Irregular Network Digital Terrain Models. Proceedings of SIGGRAPH 79. In Computer Graphics 13(2) (August 1979),pp. 199-207.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M. and HECKBERT, P. S. Fast Polygonal Approximation of Terrains and Height Fields. Technical Report CMU-CS-95-181, CS Dept., Carnegie Mellon U., 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Graphics Library P1vgramming Guide. Silicon Graphics Computer Systems, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>833866</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GROSS, M. H., GATTI, e., and STAADT, O. Fast Multiresolution Surface Meshing. In P1vceedings of Visualization '95, October 1995, pp. 135-142.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P. S. and GARLAND, M. Multiresolution Modeling for Fast Rendering. In P1vceedings of Graphics Intelface '94, 1994, pp. 1-8.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, J., and STUETZLE, W. Mesh Optimization. Proceedings of SIGGRAPH 93. In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 19-26.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[NEIDER, J., DAVIS, T., and WOO, M. OpenGL P~vgramming Guide. Addison- Wesley, 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[ROHLF, J. and HELMAN, J. IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D Graphics. Proceedings of SIGGRAPH 94. In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIG- GRAPH, pp. 381-394.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356930</ref_obj_id>
				<ref_obj_pid>356924</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S AMET, H. The Quadtree and Related Hierarchical Data Structures. ACM Computing Surveys 16(2), June 1984, pp. 187-260.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[SCARLATOS, L. L. A Refined Triangulation Hierarchy for Multiple Levels of Terrain Detail. In P~vceedings, IMAGE V Conference, June 1990, pp. 114-122.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SCHRODER, F. and ROSSBACH, P. Managing the Complexity of Digital Terrain Models. Computers &amp; Graphics 18(6), 1994, pp. 775-783.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[SCHRoEDER, W. J., ZARGE, J. A., and LORENSON, W. E. Decimation of Triangle Meshes. Proceedings of SIGGRAPH 92. In Computer Graphics 26(2) (July 1992), pp. 65-70.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>139926</ref_obj_id>
				<ref_obj_pid>139834</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[SOUTHARD, D. A. Piecewise Planar Surface Models from Sampled Data. Scientific Visualization of Physical Phenomena, June 1991, pp. 667-680.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[TAYLOR, D. C. and BARRET, W. A. An Algorithm for Continuous Resolution Polygonalizations of a Discrete Surface. In P~vceedings of Graphics Interface '94, 1994,pp. 33-42.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-Time, Continuous Level of Detail Rendering of Height Fields Peter Lindstrom*David Koller*William 
Ribarsky* Larry F. Hodges*Nick Faust Gregory A. Turner * Georgia Institute of Technology SAIC Abstract 
We present an algorithm for real-time level of detail reduction and display of high-complexity polygonal 
surface data. The algorithm uses a compact and ef.cient regular grid representation, and em­ploys a variable 
screen-space threshold to bound the maximum er­ror of the projected image. A coarse level of simpli.cation 
is per­formed to select discrete levels of detail for blocks of the surface mesh, followed by further 
simpli.cation through repolygonaliza­tion in which individual mesh vertices are considered for removal. 
These steps compute and generate the appropriate level of detail dynamically in real-time, minimizing 
the number of rendered poly­gons and allowing for smooth changes in resolution across areas of the surface. 
The algorithm has been implemented for approxi­mating and rendering digital terrain models and other 
height .elds, and consistently performs at interactive frame rates with high image quality. 1 INTRODUCTION 
Modern graphics workstations allow the display of thousands of shaded or textured polygons at interactive 
rates. However, many applications contain graphical models with geometric complexity still greatly exceeding 
the capabilities of typical graphics hardware. This problem is particularly prevalent in applications 
dealing with large polygonal surface models, such as digital terrain modeling and visual simulation. 
In order to accommodate complex surface models while still maintaining real-time display rates, methods 
for approximating the polygonal surfaces and using multiresolution models have been proposed [13]. Simpli.cation 
algorithms can be used to generate multiple surface models at varying levels of detail, and techniques 
.Graphics, Visualization, &#38; Usability Center, College of Computing, Geor­gia Institute of Technology, 
Atlanta, GA 30332 0280. flindstro, koller, ribarsky, hodgesg@cc.gatech.edu. Center for GIS and Spatial 
Analysis Technologies, Georgia Tech Research Insti­tute. nick.faust@gtri.gatech.edu. Simulation Technology 
Division. gturner@std.saic.com. Permission to make digital or hard copies of part or all of this work 
or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50  Figure 1: Terrain 
surface tessellations corresponding to projected geometric error thresholds of one (left) and four (right) 
pixels. are employed by the display system to select and render the appro­priate level of detail model. 
In this paper we present a new level of detail display algorithm that is applicable to surfaces that 
are represented as uniformly­gridded polygonal height .elds. By extending the regular grid rep­resentation 
to allow polygons to be recursively combined where appropriate, a mesh with fewer polygons can be used 
to represent the height .eld (Figure 1). Such small, incremental changes to the mesh polygonalization 
provide for continuous levels of detail and a near optimal tessellation for any given viewpoint. The 
algorithm is characterized by the following set of features: Large reduction in the number of polygons 
to be rendered. Typically, the surface grid is decimated by several orders of magnitude with no or little 
loss in image quality, accommo­dating interactive frames rates for smooth animation. Smooth, continuous 
changes between different surface levels of detail. The number and distribution of rendered polygons 
change smoothly between successive frames, af­fording maintenance of consistent frame rates.  Dynamic 
generation of levels of detail in real-time. The need for expensive generation of multiresolution models 
ahead of time is eliminated, allowing dynamic changes to the surface geometry to be made with little 
computational cost.  Support for a user-speci.ed image quality metric. The al­gorithm is easily controlled 
to meet an image accuracy level within a speci.ed number of pixels. This parameterization al­lows for 
easy variation of the balance between rendering time and rendered image quality.  Related approaches 
to polygonal surface approximation and multiresolution rendering are discussed in the next section. The 
following sections of the paper describe the theory and procedures necessary for implementing the real-time 
continuous rendering al­gorithm. We conclude the paper by empirically evaluating the al­gorithm with 
results from its use in a typical application. 2 RELATED WORK A large number of researchers have developed 
algorithms for approximating terrains and other height .elds using polygonal meshes. These algorithms 
attempt to represent surfaces with a given number of vertices, or within a given geometric error metric, 
or in a manner that preserves application speci.c critical features of the surface. Uniform grid methods 
or irregular triangulations are employed to represent the surfaces, and techniques including hierarchical 
subdivisions and decimations of the mesh are used for simpli.cation and creation of multiresolution representations. 
Much of the previous work on polygonalization of terrain­like surfaces has concentrated on triangulated 
irregular networks (TINs). A number of different approaches have been developed to create TINs from height 
.elds using Delaunay and other triangula­tions [9, 10, 19], and hierarchical triangulation representations 
have been proposed that lend themselves to usage in level of detail algo­rithms [3, 4, 18]. TINs allow 
variable spacing between vertices of the triangular mesh, approximating a surface at any desired level 
of accuracy with fewer polygons than other representations. However, the algorithms required to create 
TIN models are generally compu­tationally expensive, prohibiting use of dynamically created TINs at interactive 
rates. Regular grid surface polygonalizations have also been imple­mented as terrain and general surface 
approximations [2, 7]. Such uniform polygonalizations generally produce many more polygons than TINs 
for a given level of approximation, but grid representa­tions are typically more compact. Regular grid 
representations also have the advantage of allowing for easier construction of a multiple level of detail 
hierarchy. Simply subsampling grid elevation values produces a coarser level of detail model, whereas 
TIN models gen­erally require complete retriangulation in order to generate multiple levels of detail. 
Other surface approximation representations include hybrids of these techniques, and methods that meet 
application speci.c crite­ria. Fowler and Little [9] construct TINs characterized by certain surface 
speci.c points and critical lines, allowing the TIN rep­resentation to closely match important terrain 
features. Douglas [5] locates speci.c terrain features such as ridges and channels in a terrain model 
database, and represents the surface with line seg­ments from these information rich features. This method 
gener­ates only a single surface approximation, however, and is not easily adapted to produce multiresolution 
models. Gross et al. [12] use a wavelet transform to produce adaptive surface meshing from uni­form grid 
data, allowing for local control of the surface level of detail. This technique, however, has not yet 
proven to yield inter­active frame rates. The general problem of surface simpli.cation has been addressed 
with methods for mesh decimation and opti­mization [14, 20], although these techniques are not suitable 
for on-the-.y generation of multiple levels of detail. Theissueof continuous levelof detailrepresentationsfor 
mod­els has been addressed both for surfaces and more general model­ing. Taylor and Barret [22] give 
an algorithm for surface polygonal­ization at multiple levels of detail, and use TIN morphing to pro­vide 
for visually continuous change from one resolution to another. Many visual simulation systems handle 
transitions between multi­ple levels of detail by alpha blending two models during the tran­sition period. 
Ferguson [8] claims that such blending techniques between levels of detail may be visually distracting, 
and discusses a method of Delaunay triangulation and triangle subdivision which smoothly matches edges 
across areas of different resolution. 3 MOTIVATION The algorithm presented in this paper has been designed 
to meet a number of criteria desirable for a real-time level of detail (LOD) algorithm for height .elds. 
These characteristics include: (i) At any instant, the mesh geometry and the components that describe 
it should be directly and ef.ciently queryable, al­lowing for surface following and fast spatial indexing 
of both polygons and vertices. (ii) Dynamic changes to the geometry of the mesh, leading to re­computation 
of surface parameters or geometry, should not signi.cantly impact the performance of the system.  (iii) 
High frequency data such as localized convexities and concav­ities, and/or local changes to the geometry, 
should not have a widespread global effect on the complexity of the model. (iv) Small changes to the 
view parameters (e.g. viewpoint, view direction, .eld of view) should lead only to small changes in complexity 
in order to minimize uncertainties in prediction and allow maintenance of (near) constant frame rates. 
 (v) The algorithm should provide a means of bounding the loss in image quality incurred by the approximated 
geometry of the mesh. That is, there should exist a consistent and direct rela­tionship between the input 
parameters to the LOD algorithm and the resulting image quality.  Note that some applications do not 
require the satisfaction of all of these criteria. However, a polygon-based level of detail algo­rithm 
that supports all of these features is clearly of great impor­tance in areas such as terrain rendering, 
which often requires both high frame rates and high visual .delity, as well as fast and frequent queries 
of a possibly deformable terrain surface. Our algorithm suc­cessfully achieves all of the goals listed 
above. Most contemporary approaches to level of detail management fail to meet at least one of these 
.ve criteria. TIN models, for ex­ample, do not in general meet the .rst two criteria. Generation of even 
modest size TINs requires extensive computational effort. Be­cause TINs are non-uniform in nature, surface 
following (e.g. for animation of objects on the surface) and intersection (e.g. for colli­sion detection, 
selection, and queries) are hard to handle ef.ciently due to the lack of a spatial organization of the 
mesh polygons. The importance of (ii) is relevant in many applications, such as games and military applications, 
where dynamic deformations of the mesh occur, e.g. in the form of explosions. The most common drawback 
of regular grid representations is that the polygonalization is seldom optimal, or even near optimal. 
Large, .at surfaces may require the same polygon density as small, rough areas do. This is due to the 
sensitivity to localized, high fre­quency data within large, uniform resolution areas of lower com­plexity. 
(Most level of detail algorithms require that the mesh is subdivided into rectangular blocks of polygons 
to allow for fast view culling and coarse level of detail selection.) Hence, (iii) is violated as a small 
bump in the mesh may force higher resolution data than is needed to describe the remaining area of a 
block. This problem may be alleviated by reducing the overall complexity and applying temporal blending, 
or morphing, between different levels of detail to avoid popping in the mesh [16, 22]. Common to typical 
TIN and regular grid LOD algorithms is the discreteness of the levels of detail. Often, only a relatively 
small number of models for a given area are de.ned, and the difference in the number of polygons in successive 
levels of detail may be quite large. When switching between two levels of detail, the net change in the 
number of rendered polygons may amount to a substantial fraction of the given rendering capacity, and 
may cause rapid .uc­tuations in the frame rate. Many LOD algorithms fail to recognize the need for an 
error bound in the rendered image. While many simpli.cation meth­ods are mathematically viable, the level 
of detail generation and selection are often not directly coupled with the screen-space error resulting 
from the simpli.cation. Rather, these algorithms char­acterize the data with a small set of parameters 
that are used in conjunction with viewpoint distance and view angle to select what could be considered 
appropriate levels of detail. Examples of such algorithms include TIN simpli.cation [9], feature (e.g. 
peaks, ridges, and valleys) identi.cation and preservation [5, 21], and fre­quency analysis/transforms 
such as wavelet simpli.cation [6, 12]. These algorithms often do not provide enough information to de­rive 
a tight bound on the maximum error in the projected image. If image quality is important and popping 
effects need to be min­imized in animations, the level of detail selection should be based on a user-speci.ed 
error tolerance measured in screen-space, and should preferably be done on a per polygon/vertex basis. 
The algorithm presented in this paper satis.es all of the above criteria. Some key features of the algorithm 
include: .exibility and ef.ciency afforded by a regular grid representation; localized poly­gon densities 
due to variable resolution within each block; screen­space error-driven LOD selection determined by a 
single threshold; and continuous level of detail, which will be discussed in the fol­lowing section. 
3.1 Continuous Level of Detail Continuous level of detail has recently been used to describe a va­riety 
of properties [8, 18, 22], some of which are discussed below. As mentioned in (iii) and (iv) above, it 
is important that the com­plexity of the surface geometry changes smoothly between consec­utive frames, 
and that the simpli.ed geometry doesn't lead to gaps or popping in the mesh. In a more precise description 
of the term continuity in the context of multiresolution height .elds, the con­tinuous function, its 
domain, and its range must be clearly de.ned. This function may be one of the following: (i) The elevation 
function z(x;y;t),where x;y;t 2R. The parame­ter t may denote time, distance, or some other scalar quantity. 
This function morphs (blends) the geometries of two discrete levels of detail de.ned on the same area, 
resulting in a vir­tually continuous change in level of detail over time, or over distance from the viewpoint 
to the mesh. (ii) The elevation function z(x;y)with domain R2. The function z is de.ned piecewise on 
a per block basis. When discrete levels of detail are used to represent the mesh, two adjacent blocks 
of different resolution may not align properly, and gaps along the boundaries of the blocks may be seen. 
The elevation z on these borders will not be continuous unless precautions are taken to ensure that such 
gaps are smoothed out.  (iii) The polygon distribution function n(v;A). For any given area A .R2, the 
number of polygons used to describe the area is continuous with respect to the viewpoint v.1 Note that 
A does not necessarily have to be a connected set. Since the image of n is discrete, we de.ne continuity 
in terms of the modulus of continuity .(d;n). We say that n is continuous iff .(d;n)!e,for some e :1, 
as d !0. That is, for suf.ciently small changes in the viewpoint, the change in the number of polygons 
over A is at most one. As a consequenceof a contin­uous polygon distribution, the number of rendered 
polygons (after clipping), n(v), is continuous with respect to the view­point. Note that a continuous 
level of detail algorithm may possess one or more of these independent properties (e.g. (i) does not 
in general 1This vector may be generalized to describe other view dependent pa­rameters, such as view 
direction and .eld of view. imply (iii), and vice versa). Depending on the constraints inherent in the 
tessellation method, criterion (iii) may or may not be satis.­able, but a small upper bound emax on e 
may exist. Our algorithm, as presented here, primarily addresses de.nition (iii), but has been designed 
to be easily extensible to cover the other two de.nitions (the color plates included in this paper re.ect 
an implementation satisfying (ii)).  4 SIMPLIFICATION CRITERIA The surface simpli.cation process presented 
here is best described as a sequence of two steps: a coarse-grained simpli.cation of the height .eld 
mesh geometry that is done to determine which dis­crete level of detail models are needed, followed by 
a .ne-grained retriangulation of each LOD model in which individual vertices are considered for removal. 
The algorithm ensures that no errors are introduced in the coarse simpli.cation beyond those that would 
be introduced if the .ne-grained simpli.cation were applied to the en­tire mesh. Both steps are executed 
for each rendered frame, and all evaluations involved in the simpli.cation are done dynamically in real-time, 
based on the location of the viewpoint and the geometry of the height .eld. The height .eld is described 
by a rectilinear grid of points ele­vated above the x-y plane, with discrete sampling intervals of xres 
and yres. The surface corresponding to the height .eld (before sim­pli.cation) is represented as a symmetric 
triangle mesh. The small­est mesh representable using this triangulation, the primitive mesh, has dimensions 
3 x3 vertices, and successively larger meshes are formed by grouping smaller meshes in a 2 x2 array con.guration 
(see Figure 2). For any level l in this recursive construction of the mesh, the vertex dimensions xdim 
and ydim are 2l +1. For a certain level n, the resulting mesh is said to form a block, or a discrete 
level of detail model. A set of such blocks of .xed dimensions 2n +1ver­tices squared, describes the 
height .eld dataset, where the boundary rows and columns between adjacent blocks are shared. While the 
dimensions of all blocks are .xed, the spatial extent of the blocks may vary by multiples of powers of 
two of the height .eld sampling resolution, i.e. the area of a block is 2m+nxres x2m+nyres where m is 
some non-negative integer. Thus, lower resolution blocks can be obtained by discarding every other row 
and column of four higher resolution blocks. We term these decimated vertices the lowest level vertices 
of a block (see Figure 2c). A quadtree data structure [17] naturally lends itself to the block partitioning 
of the height .eld dataset described above. a.  dl cr cl br dr al ar bl  b. lrml kr ll irjl hr il 
mr nl kl nr gl jr gr hl orpl br ol cl fr er fl al pr ar bl crdl dr el c. d.  Figure 2: (a, b) Triangulation 
of uniform height .elds of dimen­sions 3x3and 5x5 vertices, respectively. (c) Lowest level vertices (un.lled). 
(d) Block quadrants. In the following sections, we describe the different simpli.cation steps. We begin 
by deriving a criterion for the .ne-grained (vertex­based) simpli.cation. The coarse-grained (block-based) 
level of detail selection is then described in terms of the former. 4.1 Vertex-Based Simpli.cation In 
the .ne-grained simpli.cation step, many smaller triangles are removed and replaced with fewer larger 
triangles. Conceptually, at the beginning of each rendered frame, the entire height .eld dataset at its 
highest resolution is considered. Wherever certain conditions are met, a triangle/co-triangle pair (4al 
;4ar )is reduced to one single triangle 4al4, and the resulting triangle and its co­ ar triangle (if 
one exists) are considered for further simpli.cation in arecursivemanner. In the x-y plane with xres 
=yres, a triangle/co­triangle pair is de.ned by the two congruent right triangles obtained by bisecting 
a larger isosceles right triangle. Recursive bisection of the resulting two triangles yields lower level 
triangle/co-triangle pairs. Triangle/co-triangle pairs within a block are descended from the four triangular 
quadrants of the block, de.ned by the block boundary and its diagonals (see Figure 2d). For arbitrary 
height .eld resolutions, the square mesh is simply stretched in either di­mension while retaining the 
vertex connections. Figure 2a and 2b illustrate the lowest level pairs, where each pair has been assigned 
a unique letter. The conditions under which a triangle pair can be combined into a single triangle are 
primarily described by the amount of change in slope between the two triangles. For triangles 4ABE and 
4BCE, with A, B,and C in a plane perpendicular to the x-y plane, the slope change is measured by the 
vertical (z axis) distance Az+Cz dB =jBz -2 j, i.e. the maximum vertical distance between 4ACE =4ABE4BCE 
and the triangles 4ABE and 4BCE (see Figure 3). This distance is referred to as vertex B's delta value.As 
the delta value increases, the chance of triangle fusion decreases. By projecting the delta segment,de.ned 
by B and the midpoint of AC, onto the projection plane, one can determine the maximum per­ceived geometric 
(linear) error between the merged triangle and its corresponding sub-triangles. If this error is smaller 
than a given threshold, t, the triangles may be fused. If the resulting triangle has a co-triangle with 
error smaller than the threshold, this pair is considered for further simpli.cation. This process is 
applied recur­sively until no further simpli.cation of the mesh can be made. Note that this scheme typically 
involves a reduction of an already sim­pli.ed mesh, and the resulting errors (i.e. the projected delta 
seg­ments) are not de.ned with respect to the highest resolution mesh, but rather relative to the result 
of the previous iteration in the simpli­.cation process. However, empirical data indicates that the effects 
of this approximation are negligible (see Section 7). We now derive a formula for the length of the projected 
delta segment. Let v be the midpoint of the delta segment,2 and de.ne [ [ v+dd =v +00 2, v-=v -00 2.Let 
e be the viewpoint and x , y , z be the orthonormal eye coordinate axes ex­pressed in world coordinates. 
Furthermore, let d be the distance from e to the projection plane, and de.ne . to be the number of pixels 
per world coordinate unit in the screen x-y coordinate sys­tem. (We assume that the pixel aspect ratio 
is 1:1.) The subscripts eye and screen are used to denote vectors represented in eye coordi­nates (after 
the view transformation) and screen coordinates (after the perspective projection), respectively. Using 
these de.nitions, the following approximations are made: When projecting the vectors v+and v-, their 
midpoint v is al­ways assumed to be in the center of view, i.e. along -z .This 2One may safely substitute 
the vertex associated with the delta segment for its midpoint. G Figure 3: Geometric representation 
of delta values. dB =4, dD =2:5, dF =1:5, dH =0. approximation is reasonable as long as the .eld of view 
is rel­atively small, and its effect is that the projected delta segments that represent the errors in 
the triangle simpli.cation become relatively smaller at the periphery of the screen, where less de­tail 
is then used an artifact that is often acceptable as human visual perception degrades toward the periphery. 
We assume v+'v-'in the perspective divi­ eyez eyez veyez sion 1. This is a fair assumption because, in 
general, - veyez djje -vjj=-veyez . According to the .rst approximation, the viewing matrix is then: 
23 ex-vx x xy x 0 jje-vjj 6 ey-vy 7 6x yy y 0 7 jje-vjj 67 M = 6 ez-vz 7 x zy z 0 45 jje-vjj e-v -e ·x 
-e ·y -e ·1 jje-vjj with x and y perpendicular to e -v at all times. This de.nition of M leads to the 
following equalities: v+-v-=v+M -v-M eye eye hi ez-vz =d x zy z 0 jje-vjj 2 22 ez -vz x +y =1 ­ zz jje 
-vjj The length of the projected delta segment is then described by the following set of equations: + 
­ dscreen =jjvscreen -vscreenjj q + -+ ­ d.(veyex -veyex )2 +(veyey -veyey )2 = -veyez q d.(dx z)2 +(dy 
z)2 = jje -vjj r ()2 ez-vz d.d 1 ­ jje-vjj = jje -vjj q 2 d.d(ex -vx)2 +(ey -vy)= (1) 2 22 (ex -vx)+(ey 
-vy)+(ez -vz) For performance reasons, d2 is compared to t2 so that the screen square root can be avoided: 
 d2.2d2 (2 (ex -vx)2 +(ey -vy)( :t2 22 (ex -vx)2 +(ey -vy)2 +(ez -vz) An equivalent inequality that de.nes 
the simpli.cation condition reduces to a few additions and multiplications: () d22 2 (ex -vx)+(ey -vy): 
() 22 22 .2 (ex -vx)+(ey -vy)+(ez -vz)(2) where . = dt. is a constant. Whenever ex =vx and ey =vy,i.e. 
when the viewpoint is directly above or below the delta segment, the projection is zero, and the triangles 
are coalesced. The prob­ability of satisfying the inequality decreases as ez approaches vz, or when the 
delta segment is viewed from the side. This makes sense, intuitively, as less detail is required for 
a top-down view of the mesh (assuming a monoscopic view), while more detail is nec­essary to accurately 
retain contours and silhouettes in side views. The geometric interpretation of the complement of Equation 
2 is a bialy a solid circular torus with no center hole centered at v, d.d with radius r =(see Figure 
4). The triangles associated with v 2t can be combined provided that the viewpoint is not contained in 
the bialy. Figure 4: Geometric representation (and its cross-section) of the boundary of Equation 2. 
 4.2 Block-Based Simpli.cation Complex datasets may consist of millions of polygons, and it is clearly 
too computationally expensive to run the simpli.cation pro­cess described in the previous section on 
all polygon vertices for each individual frame. By obtaining a conservative estimate of whether certain 
groups of vertices can be eliminated in a block, the mesh can often be decimated by several factors with 
little computa­tional cost. If it is known that the maximum delta projection of all lowest level vertices 
in a block falls within t, those vertices can im­mediately be discarded, and the block can be replaced 
with a lower resolution block, which in turn is considered for further simpli.ca­tion. Accordingly, a 
large fraction of the delta projections can be avoided. The discrete level of detail selection is done 
by computing the maximum delta value, dmax, of the lowest level vertices for each block. Given the axis-aligned 
bounding box of a block and dmax, one can determine, for a given viewpoint, whether any of these vertices 
have delta values large enough to exceed the threshold t. If none of them do, a lower resolution model 
may be used. We can expand on this idea to obtain a more ef.cient simpli.cation algorithm. By using t, 
the view parameters, and the constraints provided by the bounding box, one can compute the smallest delta 
value dl that, when projected, can exceed t, aswellasthe largest delta value dh that may project smaller 
than t. Delta values between these extremes fall in an uncertainty interval, which we denote by Iu =(dl 
;dh], for which Equation 2 has to be evaluated. Vertices with delta values less than dl can readily be 
discarded without fur­ther evaluation, and conversely, vertices with delta values larger than dh cannot 
be removed. It would obviously be very costly to compute Iu by reversing the projection to get the delta 
value whose projection equals t for every single vertex within the block, but one can approximate Iu 
by assuming that the vertices are dense in the bounding box of the block, and thus obtain a slightly 
larger superset of Iu. From this point on, we will use Iu to denote this superset. To .nd the lower bound 
dl of Iu, the point in the bounding box that maximizes the delta projection must be found. From Equa­ 
q tion 1, de.ne r =(ex -vx)2 +(ey -vy)2 and h =ez -vz. We seek r to maximize the function f (r;h)=subject 
to the constraints r2+h2 r2 +h2 :d2 and v 2B,where d is the distance from the viewpoint to the projection 
plane and B is the set of points contained in the bounding box, described by the two vectors [ bmin =bminx 
bminy bminz [ bmax =bmaxx bmaxy bmaxz We solve this optimization problem by constraining r, such that 
2 22 d2 -h2 :rr:r(r and h are otherwise independent). min :max Clearly, then, h2 has to be minimized 
which is accomplished by setting h =hmin =jez -clamp(bminz ;ez;bmaxz )j,where 8 {xmin if x <xmin clamp(xmin;x;xmax)=xmax 
if x >xmax : x otherwise In the x-y plane, de.ne rmin to be the smallest distance from [ ex eyto the 
rectangular slice (including the interior) of the [[ bounding box de.ned by bminx bminyand bmaxx bmaxy, 
and de.ne rmax to be the largest such distance. Via partial differ­entiation with respect to r, the maximum 
fmax of f (r;h)is found at r =h.If no v exists under the given constraints that satis.es r =h, r is increased/decreased 
until v 2B,i.e. r =clamp(rmin;h;rmax). The upper bound, dh, is similarly found by minimizing f (r;h). 
This is done by setting h =hmax =maxfjez -bminz j;jez -bmaxz jg. fmin is then found when either r =rmin 
or r =rmax, whichever yields a smaller f (r;h). The bounds on Iu can now be found using the following 
equa­tions: t dl = (3) d. fmax 8 {0if t =0 dh = t if t >0and fmin >0 (4) d. fmin : 8 otherwise After 
computation of Iu, dmax is compared to dl , and if smaller, a lower resolution level of detail block 
is substituted, and the pro­cess is repeated for this block. If dmax >dl , it may be that a higher resolution 
block is needed. By maintaining d:=maxifdmaxi g, max the largest dmax of all higher resolution blocks 
(or block descen­dants) for the given area, d:is compared to dl for the current max block, and if greater, 
four higher resolution blocks replace the cur­rent block. As mentioned earlier, this implicit hierarchical 
organi­zation of blocks is best represented by a quadtree, where each block corresponds to a quadnode. 
 4.3 Vertex Dependencies As pointed out in Section 4.1, triangle fusion can occur only when the triangles 
in the triangle pair appear on the same level in the triangle subdivision. For example, in Figure 2b, 
4el 4and er 4fl 4fr cannot be coalesced unless the triangles in both pairs (4el ;4er )and (4fl ;4fr )have 
been fused. The triangles can be represented by nodes in a binary expression tree, where the small­est 
triangles correspond to terminal nodes, and coalesced triangles correspond to higher level, nonterminal 
nodes formed by recursive application of the operator (hence the subscripts l and r for left and right 
). Conceptually, this tree spans the entire height .eld dataset, but can be limited to each block. Another 
way of looking at triangle fusion is as vertex removal, i.e. when two triangles are fused, one vertex 
is removed. We call this vertex the base vertex of the triangle pair. Each triangle pair has a co-pair 
associated with it,3 and the pair/co-pair share the same base vertex. The mapping of vertices to triangle 
pairs, or the nodes associated with the operators that act on the triangle pairs, results in a vertex 
tree, wherein each vertex occurs exactly twice; once for each triangle pair (Figures 5g and 5h). Hence, 
each vertex has two distinct parents (or dependents) one in each of two binary subtrees T0 and T1 as 
well as four distinct children. If any of the descendants of a vertex v are included in the rendered 
mesh, so is v, and we say that v is enabled. If the projected delta segment associated with v exceeds 
the threshold t, v is said to be activated, which also implies that v is enabled. Thus, the enabled attribute 
of v is determined by activated(v)_ enabled(le ftT0(v))_ enabled(rightT0(v))_ enabled(le ftT1(v))_ enabled(rightT1(v))=enabled(v) 
An additional vertex attribute, locked, allows the enabled .agtobe hardwired to either true or false, 
overriding the relationship above. This may be necessary, for example, when eliminating gaps be­tween 
adjacent blocks if compatible levels of detail do not exist, i.e. some vertices on the boundaries of 
the higher resolution block may have to be permanently disabled. Figures 5a e show the de­pendency relations 
between vertices level by level. Figure 5f shows the in.uence of an enabled vertex over other vertices 
that directly or indirectly depend on it. Figures 5g and 5h depict the two possible vertex tree structures 
within a block, where intersections have been separated for clarity. To satisfy continuity condition 
(ii) (see Section 3.1), the al­gorithm must consider dependencies that cross block boundaries. Since 
the vertices on block boundaries are shared between adja­cent blocks, these vertices must be referenced 
uniquely, so that the dependencies may propagate across the boundaries. In most imple­mentations, such 
shared vertices are simply duplicated, and these redundancies must be resolved before or during the simpli.cation 
stage. One way of approaching this is to access each vertex via a pointer, and discard the redundant 
copies of the vertex before the block is .rst accessed. Another approach is to ensure that the at­tributes 
of all copies of a vertex are kept consistent when updates (e.g. enabled and activated transitions) occur. 
This can be achieved by maintaining a circular linked list of copies for each vertex.  5 ALGORITHM OUTLINE 
The algorithm presented here describes the steps necessary to se­lect which vertices should be included 
for rendering of the mesh. In Section 5.1, we describe how the mesh is rendered once the ver­tex selection 
is done. A discussion of appropriate data structures is presented in Section 6. Using the equations presented 
in previous 3Triangle pairs with base vertices on the edges of the .nite dataset are an exception. a. 
b.c. d.e. f. g. h. i.  Figure 5: (a e) Vertex dependencies by descending levels (left to right, top 
to bottom). An arc from A to B indicates that B depends on A. (f) Chain of dependencies originating from 
the solid vertex. (g, h) Symmetric binary vertex trees (the arcs in (g) correspond to (f)). (i) Triangulation 
corresponding to (f). sections, the algorithm is summarized by the pseudocode below. Unless quali.ed 
with superscripts, all variables are assumed to be­long to the current frame and block. MAIN() 1 for 
each frame n 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 for each active block b compute Iu 
(Equations 3 and 4) if dmax :dl replace b with lower resolution block else if d: dl max >replace b with 
higher resolution blocks for each active block b determine if b intersects the view frustum for each 
visible block b I0 +(dn-1 ;dn ll ] I1 +(dnh;dn-1] h for each vertex v with d(v)2I0 activated(v)+false 
UPDATE-VERTEX(v) for each vertex v with d(v)2I1 activated(v)+true UPDATE-VERTEX(v) for each vertex v 
with d(v)2Iu EVALUATE-VERTEX(v) for each visible block b RENDER-BLOCK(b) UPDATE-VERTEX(v) 1 if :locked(v) 
2 if :dependencyi(v)8i 3 if enabled(v)6=activated(v) 4 enabled(v)+:enabled(v) 5 NOTIFY(parentT0(v);branchT0(v);enabled(v)) 
6 NOTIFY(parentT1(v);branchT1(v);enabled(v)) EVALUATE-VERTEX(v) 1 if :locked(v) 2 if :dependencyi(v)8i 
3 activated(v)+:Equation 2 4 if enabled(v)6=activated(v) 5 enabled(v)+:enabled(v) 6 NOTIFY(parentT0(v);branchT0(v);enabled(v)) 
7 NOTIFY(parentT1(v);branchT1(v);enabled(v)) NOTIFY(v;child;state) 1 if v is a valid vertex 2 dependencychild(v)+state 
3 if :locked(v) 4 if :dependencyi(v)8i 5 if :activated(v) 6 enabled(v)+false 7 NOTIFY(parentT0(v);branchT0(v), 
false) 8 NOTIFY(parentT1(v);branchT1(v), false) 9 else 10 if :enabled(v) 11 enabled(v)+true 12 NOTIFY(parentT0(v);branchT0(v), 
true) 13 NOTIFY(parentT1(v);branchT1(v), true) The term active block refers to whether the block is currently 
the chosen level of detail for the area it covers. All blocks initially have Iu set to .0;8), and so 
do blocks that previously were inactive. When deactivating vertices with delta values smaller than dl 
,the in­terval I0 .0;dl ]is traversed. By inductive reasoning, vertices with deltas smaller than the 
lower bound of I0 must have been deacti­vated in previous frames. Similarly, I1 is used for vertex activation. 
In quadtree implementations, the condition on line 4 in MAIN may have to be supplemented; the condition 
dmax :dl should also hold for the three neighboring siblings of b before b can be replaced. If a vertex's 
enabled attribute changes, all dependent ver­tices must be noti.ed of this change so that their corresponding 
dependency .ags are kept consistent with this change. The proce­dure UPDATE-VERTEX checksif enabled(v)has 
changed, and if so, noti.es v's dependents by calling NOTIFY.If the enabled .ag of a dependent in turn 
is modi.ed, NOTIFY is called recursively. Since line2in NOTIFY necessarilyinvolvesachangeofa dependency 
bit, there may be a transition in enabled(v)from true to false on line 6 provided activated(v)is false 
as the vertex is no longer dependent. The evaluation of Equation 2 on line 3 in EVALUATE-VERTEX can be 
deferred if any of the vertex's dependency .ags are set, which is of signi.cant importance as this evaluation 
is one of the most com­putationally expensive parts of the algorithm. Note that there may be a one-frame 
delay before the activated attribute is corrected due to this deferral if the child vertices are evaluated 
after the dependent vertex (line 2 of EVALUATE-VERTEX and lines 4 5 of NOTIFY). The function branchT(v)refers 
to the .eld of the parent in tree T that re.ects the enabled .eld of vertex v. Note that a check has 
to be made (line 1 in NOTIFY) whether a vertex is valid as some vertices have fewer than two dependents 
(e.g. boundary vertices). 5.1 Mesh Rendering Once the vertex selection is made, a triangle mesh must 
be formed that connects the selected vertices. This mesh is de.ned by speci­fying the vertices encountered 
in a pre-order descent of the binary vertex trees. The recursive stopping condition is a false enabled 
attribute. To ef.ciently render the mesh, a triangle mesh graph­ics primitive, such as the one supported 
by IRIS GL and OpenGL [11, 15], may be used. For each speci.ed vertex v, the previous two vertices and 
v form the next triangle in the mesh. At certain points, the previous two vertices must be swapped via 
a swaptmesh() call (IRIS GL), or a glVertex() call (OpenGL). A copy of the two-entry graphics vertex 
buffer, my-buf fer, is maintained explic­itly to allow the decision as to when to swap the entries to 
be made. The most recent vertex in this buffer is indexed by ptr. The following pseudocode describes 
the mesh rendering algo­rithm. Each of the four triangular quadrants qi are rendered in coun­terclockwise 
order, with the .rst vertex in each quadrant coincident with the last vertex in the previous quadrant 
(see Figure 2d). Hence, a single triangle mesh can be used to render the entire block. The indices qil, 
qit,and qir correspond to the left, top, and right vertex indices of quadrant qi, respectively, with 
the top index being the center of the block. The block dimensions are 2n +1 squared. RENDER-BLOCK(b) 
1 enter triangle mesh mode 2 render vertex vq0l 3 my-buf ferptr +q0l 4 previous-level +0 5 for each quadrant 
qi in block b 6 if previous-level is even 7 toggle ptr 8 else 9 swap vertices in graphics buffer 10 render 
vertex vqil 11 my-buf ferptr +qil 12 previous-level +2n +1 13 RENDER-QUADRANT(qil;qit ;qir;2n) 14 render 
vertex vq0l 15 exit triangle mesh mode RENDER-QUADRANT(il;it ;ir ;level) 1 if level >0 2 if enabled(vit 
) il +ir 3 RENDER-QUADRANT(il;2 ;it ;level -1) 4 if it 62my-buf fer 5 if level +previous-level is odd 
6 toggle ptr 7 else 8 swap vertices in graphics buffer 9 render vertex vit 10 my-buf ferptr +it 11 previous-level 
+level il +ir 12 RENDER-QUADRANT(it;2 ;ir;level -1) The index il +ir corresponds to the (base) vertex 
that in the x-y plane 2 is the midpoint of the edge vil vir .Since my-buf fer re.ects what vertices are 
currently in the graphics buffer, line 9 in RENDER-BLOCK and line 8 in RENDER-QUADRANT could be implemented 
with a glVertex() call, passing the second most recent vertex in my-buf fer.   6 DATA STRUCTURES Many 
of the issues related to the data structures used with this al­gorithm have purposely been left open, 
as different needs may de­mand totally different approaches to their representations. In one implementation 
the one presented here as few as six bytes per vertex were used, and as many as 28 bytes per vertex were 
needed in another. In this section, we describe data structures that will be useful in many implementations. 
For a compact representation, the vertex elevation is discretized and stored as a 16-bit integer. A minimum 
of six additional bits per vertex are required for the various .ags, including the enabled, activated, 
and four dependency attributes. Optionally, the locked attribute can be added to these .ags. The theoreti­cal 
range of delta values becomes .0;216 -1]in steps of 21.We elect to store each d in compressed form as 
an 8-bit integer d in order to conserve space by encapsulating the vertex structure in a 32-bit aligned 
word. We de.ne the decompression function as 1 d 2/(28 -1)24 d =2 b(1 +d)1+-1c. This exponential mapping 
pre­serves the accuracy needed for the more frequent small deltas, while allowing large delta values 
to be represented, albeit with less accu­racy. The compression function is de.ned as the inverse of the 
de­compression function. Both functions are implemented as lookup tables. To accommodate tasks such as 
rendering and surface following, the vertices must be organized spatially for fast indexing. In Sec­tion 
4.2, however, we implied that vertices within ranges of delta values could be immediately accessed. This 
is accomplished by cre­ating an auxiliary array of indices, in which the entries are sorted on the corresponding 
vertices' delta values. Each entry uniquely references the corresponding vertex (i;j)via an index into 
the ar­ray of vertex structures. For each possible compressed delta value within a block, there is a 
pointer (index) pd to a bin that contains the vertex indices corresponding to that delta value. The 28 
bins are stored in ascending order in a contiguous, one-dimensional ar­ray. The entries in bin i are 
then indexed by pi;pi +1;:::;pi+1 -1 (pi =pi+1 implies that bin i is empty). For block dimensions up 
to 27 +1, the indices can be represented with 16 bits to save space, which in addition to the 32-bit 
structure described above, results in a total of six bytes storage per vertex. 7 RESULTS To show the 
effectiveness of the polygon reduction and display al­gorithm, we here present the results of a quantitative 
analysis of the number of polygons and delta projections, frame rates, computa­tion and rendering time, 
and errors in the approximated geometry. A set of color plates show the resulting wireframe triangulations 
and textured terrain surfaces at different stages of the simpli.ca­tion and for different choices of 
t. Two height .eld datasets were used in generating images and collecting data: a 64 km2 area digital 
elevation model of the Hunter-Liggett military base in California, sampled at 2 x2 meter resolution, 
and 1 meter height (z) resolution (Color Plates 1a c and 2a c); and a 1 x1 meter resolution, 14 km2 area 
of 29 Palms, California, with a z resolution of one tenth of a meter (Color Plates 3a d). The vertical 
.eld of view is 60°in all images, which were generated on a two-processor, 150 MHz SGI Onyx RealityEngine2 
[1], and have dimensions 1024 x768 pixels unless otherwise speci.ed. We .rst examine the amount of polygon 
reduction as a function of the threshold t. A typical view of the Hunter-Liggett terrain was chosen for 
this purpose, which includes a variety of features such as ridges, valleys, bumps, and relatively .at 
areas. Figure 6 shows four curves drawn on a logarithmic scale (vertical axis). The top horizontal line, 
n0(t)=13 ·106, shows the total number of poly­gons in the view frustum before any reduction method is 
applied. The curve second from the top, n1(t), represents the number of polygons remaining after the 
block-based level of detail selection is done. The number of polygons rendered, n2(t),i.e. the remain­ing 
polygons after the vertex-based simpli.cation, is shown by the lowest solid curve. As expected, these 
two curves .atten out as t is increased. The ratio n0(t)jn2(t)ranges from about 2 (t =0) to over 6,000 
(t =8). Of course, at t =0, only coplanar triangles are fused. The ratio n1(t)jn2(t)varies between 1.85 
and 160 over the same interval, which clearly demonstrates the advantage of re.ning each uniform level 
of detail block. We pay special attention to the data obtained at t =1, as this threshold is small enough 
that virtually no popping can be seen in .1 4This results in an upper bound 216 for the delta values. 
2 t displacement mean median max std. dev. >t (%) 0.000 0.125 0.250 0.500 1.000 2.000 4.000 8.000 0.00 
0.03 0.06 0.11 0.21 0.42 0.88 1.38 0.00 0.00 0.00 0.04 0.07 0.13 0.23 0.19 0.00 0.52 0.85 1.56 2.88 5.37 
10.41 16.69 0.00 0.05 0.09 0.15 0.29 0.59 1.24 2.08 0.00 6.41 4.52 3.14 2.61 2.84 3.27 1.38 Table 1: 
Screen-space error in simpli.ed geometry. animated sequences, and the resulting surfaces, when textured, 
are seemingly identical to the ones obtained with no mesh simpli.ca­tion. Color Plates 1a c illustrate 
the three stages of simpli.cation at t =1. In Color Plate 1c, note how many polygons are required for 
the high frequency data, while only a few, large polygons are used for the .atter areas. For this particular 
threshold, n0(1)jn2(1) is slightly above 200, while n1(1)jn2(1)is 18. The bottommost, dashed curve in 
Figure 6 represents the total number of delta values that fall in the uncertainty interval per frame 
(Section 4.2). Note that this quantity is generally an order of magnitude smaller than the number of 
rendered polygons. This is signi.cant as the evalu­ations associated with these delta values constitute 
the bulk of the computation in terms of CPU time. This also shows the advantage of computing the uncertainty 
interval, as out of the eight million vertices contained in the view frustum, only 14,000 evaluations 
of Equation 2 need to be made when t =1. 100000000 100000 1000000 10000000  Polygons before simplification 
Polygons after block­based simplification 10000 Polygons after vertex­based simplification 1000  Delta 
projections 100 0 2 4 6 8 Threshold (pixels) Figure 6: The number of polygons (n0, n1, n2, from top 
to bottom) as a function of t. The bottom curve shows the number of times Equation 2 was evaluated per 
frame. In order to evaluate the errors due to the simpli.cation, the points on the polygonal surface 
of the simpli.ed mesh that have been dis­placed vertically, as well as the remaining triangle vertices, 
are per­spective projected to screen-space and compared to the projections of the original set of vertices. 
Optimally, each such screen coordi­nate displacement should fall within the threshold distance t.How­ever, 
this constraint may in certain cases be violated due to the ap­proximations discussed in Section 4.1. 
Table 1 was compiled for each mesh after vertex-based simpli.cation was applied, and the surface points 
were correlated with the original eight million ver­tices shown in Color Plate 1a. The table summarizes 
the mean, median, maximum, and standard deviation of the displacements in number of pixels, as well as 
the fraction of displacements that ex­ceed t. In all cases, the average pixel error is well below t. 
It can be seen that the approximations presented in Section 4.1 do not sig­ni.cantly impact the accuracy, 
as the fraction of displacements that exceed t is typically less than .ve percent. Color Plates 2a c 
illustrate a checkerboard pattern draped over the polygonal meshes from Color Plates 1a c. Qualitatively, 
these images suggest little or no perceivable loss in image quality for a Time (ms) threshold of one 
pixel, even when the surface complexity is reduced by a factor of 200. Figure 7 demonstrates the ef.ciency 
of the algorithm. The com­putation time associated with the delta projections (lines 10 20 in MAIN, Section 
5) is typically only a small fraction of the render­ing time. This data was gathered for the views shown 
in Color Plates 3a d. Delta projection time Rendering time Total time 10000 1000 100 10 1 012345678 Threshold 
(pixels) Figure 7: Rendering and evaluation times and their sum as functions of t. Figure 8 shows how 
the quantities in Figure 6, as well as the frame rate vary with time. The data collection for 3,230 frames 
was done over a time period of 120 seconds, with the viewpoint follow­ing a circular path of radius 1 
km over the Hunter-Liggett dataset. The terrain was rendered as a wireframe mesh in a 640 x480 win­dow, 
with t =2 pixels. It can be seen that the number of rendered polygons does not depend on the total number 
of polygons in the view frustum, but rather on the complexity of the terrain intersected by the view 
frustum. As evidenced by the graph, a frame rate of at least 20 frames per second was sustained throughout 
the two min­utes of .y-through. 10000000 a 1000000 b 100000 10000 c d1000 100 e 10 0 500 1000 1500 2000 
2500 3000 Frame Figure 8: Time graph of (a) total number of polygons in view frustum, (b) number of polygons 
after block-based simpli.cation, (c) number of polygons after vertex-based simpli.cation, (d) num­ber 
of delta projections, and (e) frames per second. 8 CONCLUSION We have presented a height-.eld display 
algorithm based on real­time, per vertex level of detail evaluation, that achieves interac­tive and consistent 
frame rates exceeding twenty frames per sec­ond, with only a minor loss in image quality. Attractive 
features attributed to regular grid surface representations, such as fast geo­metric queries, compact 
representation, and fast mesh rendering are retained. The concept of continuous level of detail allows 
a polygon distribution that is near optimal for any given viewpoint and frame, and also yields smooth 
changes in the number of rendered poly­gons. A single parameter that can easily be changed interactively, 
with no incurred cost, determines the resulting image quality, and a direct relationship between this 
parameter and the number of ren­dered polygons exists, providing capabilities for maintaining con­sistent 
frame rates. The algorithm can easily be extended to handle the problem of gaps between blocks of different 
levels of detail, as well as temporal geometry morphing to further minimize popping effects.  Acknowledgement 
This work was performed in part under contract DAKF11 91 D 0004 0034 from the U.S. Army Research Laboratory. 
 References [1] AKELEY,K.RealityEngineGraphics.ProceedingsofSIGGRAPH93.In Com­puter Graphics Proceedings, 
Annual Conference Series, 1993, ACM SIG-GRAPH, pp. 109 116. [2] COSMAN,M. A., MATHISEN,A. E., and ROBINSON,J. 
A.A New Visual System to Support Advanced Requirements. In Proceedings, IMAGE V Confer­ence, June 1990, 
pp. 370 380. [3] DE BERG,M. and DOBRINDT, K. T. G. On Levels of Detail in Terrains. In 11th ACM Symposium 
on Computational Geometry, June 1995. [4] DE FLORIANI,L. and PUPPO, E. Hierarchical Triangulation for 
Multiresolu­tion Surface Description. ACM Transactions on Graphics 14(4), October 1995, pp. 363 411. 
[5] DOUGLAS,D.H.ExperimentstoLocateRidgesandChannelstoCreateaNew Type of Digital Elevation Model. Cartographica 
23(4), 1986, pp. 29 61. [6] ECK,M., DEROSE,T., DUCHAMP,T., HOPPE,H., LOUNSBERY,M., and STUETZLE, W. Multiresolution 
Analysis of Arbitrary Meshes. Proceedings of SIGGRAPH 95. In Computer Graphics Proceedings, Annual Conference 
Se­ries, 1995, ACM SIGGRAPH, pp. 173 182. [7] FALBY,J. S., ZYDA,M. J., PRATT,D. R., and MACKEY,R. L.NPSNET: 
Hierarchical Data Structures for Real-Time Three-Dimensional Visual Simula­tion. Computers &#38; Graphics 
17(1), 1993, pp. 65 69. [8] FERGUSON,R.L.,ECONOMY,R.,KELLY,W.A.,andRAMOS,P.P.Contin­uous Terrain Level 
of Detail for Visual Simulation. In Proceedings, IMAGE V Conference, June 1990, pp. 144 151. [9] FOWLER,R. 
J. and LITTLE, J. J. Automatic Extraction of Irregular Network Digital Terrain Models. Proceedings of 
SIGGRAPH 79. In Computer Graphics 13(2) (August 1979), pp. 199 207. [10] GARLAND,M.and HECKBERT, P. S. 
Fast Polygonal Approximation of Terrains and Height Fields. Technical Report CMU-CS-95-181, CS Dept., 
Carnegie Mellon U., 1995. [11] Graphics Library Programming Guide. Silicon Graphics Computer Systems, 
1991. [12] GROSS,M. H., GATTI,R., and STAADT, O. Fast Multiresolution Surface Meshing. In Proceedings 
of Visualization ' 95, October 1995, pp. 135 142. [13] HECKBERT,P. S. and GARLAND, M. Multiresolution 
Modeling for Fast Ren­dering. In Proceedings of Graphics Interface ' 94, 1994, pp. 1 8. [14] HOPPE,H., 
DEROSE,T., DUCHAMP,T., MCDONALD,J., and STUETZLE, W. Mesh Optimization. Proceedings of SIGGRAPH 93. In 
Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 19 26. [15] NEIDER,J., 
DAVIS,T.,and WOO,M. OpenGL ProgrammingGuide. Addison-Wesley, 1993. [16] ROHLF,J. and HELMAN, J. IRIS 
Performer: A High Performance Multipro­cessing Toolkit for Real-Time 3D Graphics. Proceedings of SIGGRAPH 
94. In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIG-GRAPH, pp. 381 394. [17] 
SAMET,H.TheQuadtreeandRelatedHierarchicalDataStructures. ACM Com­puting Surveys 16(2), June 1984, pp. 
187 260. [18] SCARLATOS, L. L. A Re.ned Triangulation Hierarchy for Multiple Levels of Terrain Detail. 
In Proceedings, IMAGE V Conference, June 1990, pp. 114 122. [19] SCHRODER,F.andROSSBACH,P.ManagingtheComplexityofDigitalTerrain 
Models. Computers &#38; Graphics 18(6), 1994, pp. 775 783. [20] SCHROEDER,W. J., ZARGE,J. A., and LORENSON,W. 
E.Decimation of Triangle Meshes. Proceedings of SIGGRAPH 92. In Computer Graphics 26(2) (July 1992), 
pp. 65 70. [21] SOUTHARD,D.A.PiecewisePlanarSurfaceModelsfromSampledData. Sci­enti.c Visualization of 
Physical Phenomena, June 1991, pp. 667 680. [22] TAYLOR,D.C.andBARRET,W.A.AnAlgorithmforContinuousResolution 
Polygonalizations of a Discrete Surface. In Proceedings of Graphics Interface '94, 1994, pp. 33 42. 
 1a. 1b. 1c.  2a. Before simplification2b. After block-based LOD2c. After vertex-based LOD 13,304,214 
polygons 1,179,690 polygons 64,065 polygons 3a. t = 0.5, 62,497 polygons 3b. t = 1.0, 23,287 polygons 
 3c. t = 2.0, 8,612 polygons 3d. t = 4.0, 3,385 polygons 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237220</article_id>
		<sort_key>119</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Simplification envelopes]]></title>
		<page_from>119</page_from>
		<page_to>128</page_to>
		<doi_number>10.1145/237170.237220</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237220</url>
		<keywords>
			<kw><![CDATA[geometric modeling]]></kw>
			<kw><![CDATA[hierarchical approximation]]></kw>
			<kw><![CDATA[levels-of-detail generation]]></kw>
			<kw><![CDATA[model simplification]]></kw>
			<kw><![CDATA[offsets]]></kw>
			<kw><![CDATA[shape approximation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Verification</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39067297</person_id>
				<author_profile_id><![CDATA[81452612087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15036256</person_id>
				<author_profile_id><![CDATA[81100561920]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Amitabh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Varshney]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, State University of New York, Stony Brook, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40029106</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084716</person_id>
				<author_profile_id><![CDATA[81100457973]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Turk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31099040</person_id>
				<author_profile_id><![CDATA[81341498173]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15038115</person_id>
				<author_profile_id><![CDATA[81100634961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Pankaj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agarwal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Duke University, Durham, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023440</person_id>
				<author_profile_id><![CDATA[81100077256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Frederick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brooks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39085589</person_id>
				<author_profile_id><![CDATA[81332536188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>314475</ref_obj_id>
				<ref_obj_pid>314464</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E Agarwal and S. Suri. Surface approximation and geometric partitions. In Proceedings Fifth Symposium on Discrete Algorithms, pages 24-33, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>178029</ref_obj_id>
				<ref_obj_pid>177424</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H. Br6nnimann and M. Goodrich. Almost optimal set covers in finite VC-dimension. In Proceedings Tenth ACM Symposium on Computational Geometry, pages 293-302,1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>672705</ref_obj_id>
				<ref_obj_pid>645929</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[K. L. Clarkson. Algorithms for polytope covering and approximation. In Proc. 3rd Workshop Algorithms Data Struct., Lecture Notes in Computer Science, 1993.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Cosman and R. Schumacker. System strategies to optimize CIG image content. In Proceedings of the Image II Conference, Scottsdale, Arizona, June 10-12 1981.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[G. Das and D. Joseph. The complexity of minimum convex nested polyhedra. In Proc. 2nd Canad. Conf. Comput. Geom., pages 296- 301, 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. J. DeHaemer, Jr. and M. J. Zyda. Simplification of objects rendered by polygonal approximations. Computers &amp; Graphics, 15(2):175-184,1991.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[T.D. DeRose, M. Lounsbery, and J. Warren. Multiresolution analysis for surface of arbitrary topological type. Report 93-10-05, Department of Computer Science, University of Washington, Seattle, WA, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. Multiresolution analysis of arbitrary meshes. Computer Graphics: Proceedings of SIGGRAPH' 95, pages 173-182,1995.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166149</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[T.A. Funkhouser and C. H. S6quin. Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments. In Computer Graphics (SIGGRAPH '93 Proceedings), volume 27, pages 247-254, August 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[N. Greene, M. Kass, and G. Miller. Hierarchical z-buffer visibility. In Computer Graphics: Proceedings of SIGGRAPH 1993, pages 231-238. ACM SIGGRAPH, 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>833850</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[T. He, L. Hong, A. Kaufman, A. Varshney, and S. Wang. Voxelbased object simplification. In G. M. Nielson and D. Silver, editors, IEEE Visualization '95 Proceedings, pages 296-303,1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[E Heckbert and M. Garland. Multiresolution modeling for fast rendering. Proceedings of Graphics Interface, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949882</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[E Hinker and C. Hansen. Geometric optimization. In Gregory M. Nielson and Dan Bergeron, editors, Proceedings Visualization '93, pages 189-195, October 1993.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH '93 Proceedings), volume 27, pages 19-26, August 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[A. D. Kalvin and R. H. Taylor. Superfaces: Polyhedral approximation with bounded error. Technical Report RC 19135 (#82286), IBM Research Division, T. J. Watson Research Center, Yorktown Heights, NY 10958, 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>139467</ref_obj_id>
				<ref_obj_pid>139404</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Mitchell and S. Suri. Separation and approximation of polyhedral surfaces. In Proceedings of 3rd ACM-SIAM Symposium on Discrete Algorithms, pages 296-306,1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836034</ref_obj_id>
				<ref_obj_pid>832290</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Kevin J. Renze and J. H. Oliver. Generalized surface and volume decimation for unstructured tessellated domains. In Proceedings of SIVE' 95, 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac and E Borrel. Multi-resolution 3D approximations for rendering. In Modeling in Computer Graphics, pages 455-465. Springer-Verlag, June-July 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[H.E. Rushmeier, C. Patterson, and A. Veerasamy. Geometric simplification for indirect illumination calculations. In Proceedings Graphics Interface '93, pages 227-236,1993.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15906</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[E J. Schmitt, B. A. Barsky, and W. Du. An adaptive subdivision method for surface-fitting from sampled data. Computer Graphics (SIGGRAPH '86 Proceedings), 20(4): 179-188,1986.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[W. J. Schroeder, J. A. Zarge, and W. E. Lorensen. Decimation of triangle meshes. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 65-70, July 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[G. Taubin. A signal processing approach to fair surface design. In Proc. of ACM Siggraph, pages 351-358,1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. Turk. Re-filing polygonal surfaces. In Computer Graphics (SIG- GRAPH '92 Proceedings), volume 26, pages 55-64, July 1992.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222231</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[A. Varshney. Hierarchical geometric approximations. Ph.D. Thesis TR-050-1994, Department of Computer Science, University of North Carolina, Chapel Hill, NC 27599-3175,1994.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simpli.cation Envelopes Jonathan Cohen* Amitabh Varshneyy Dinesh Manocha* Greg Turk* Hans Weber* Pankaj 
Agarwalz Frederick Brooks* William Wright* http://www.cs.unc.edu/ geom/envelope.html Abstract We propose 
the idea of simpli.cation envelopes for gen­erating a hierarchy of level-of-detail approximations for 
a given polygonal model. Our approach guarantees that all points of an approximation are within a user-speci.able 
distance ffrom the original model and that all points of the original model are within a distance ffrom 
the approxima­tion. Simpli.cationenvelopesprovideageneralframework within which a large collection of 
existing simpli.cation algorithms can run. We demonstrate this technique in con­junction with two algorithms, 
one local, the other global. The local algorithm provides a fast method for generating approximations 
to large input meshes (at least hundreds of thousands of triangles). The global algorithm provides the 
opportunity to avoid local minima and possibly achieve better simpli.cations as a result. Each approximation 
attempts to minimize the total num­ber of polygons required to satisfy the above fconstraint. The key 
advantages of our approach are: General technique providing guaranteed error bounds for genus-preserving 
simpli.cation  Automation of both the simpli.cation process and the selection of appropriate viewing 
distances  Prevention of self-intersection  Preservation of sharp features  Allows variation of approximation 
distance across dif­ferent portions of a model  CR Categories and Subject Descriptors: I.3.3 [Com­puter 
Graphics]: Picture/Image Generation Display algorithms; I.3.5 [Computer Graphics]: Computational Geometry 
and Object Modeling Curve, surface, solid, and object representations. Additional Key Words and Phrases: 
hierarchical approx­imation, model simpli.cation, levels-of-detail generation, shape approximation, geometric 
modeling, offsets. 'Department of Computer Science, University of North Carolina, Chapel Hill, NC 27599-3175. 
fcohenj,weberh,manocha,turk,brooks,wrightg@cs.unc.edu yDepartment of Computer Science, State University 
of New York, Stony Brook, NY 11794-4400. varshney@cs.sunysb.edu zDepartment of Computer Science, Duke 
University, Durham, NC 27708-0129. pankaj@cs.duke.edu Permission to make digital or hard copies of part 
or all of this work or personal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
 1 Introduction We present the framework of simpli.cation envelopes for computing various levels of detail 
of a given polygonal model. These hierarchical representations of an object can be used in several ways 
in computer graphics. Some of these are: Use in a level-of-detail-based rendering algorithm for providing 
desired frame update rates [4, 9].  Simplifyingtraditionallyover-sampled modelssuchas those generated 
from volume datasets, laser scanners, and satellites for storage and reducing CPU cycles during processing, 
with relatively few or no disadvan­tages [10, 11, 13, 15, 21, 23].  Using low-detail approximations 
of objects for illumi­nation algorithms, especially radiosity [19].  Simpli.cation envelopes are a generalization 
of offset surfaces. Given a polygonal representation of an object, they allow the generation of minimal 
approximations that are guaranteed not to deviate from the original by more than a user-speci.able amount 
while preserving global topol­ogy. We surround the original polygonal surface with two envelopes, then 
perform simpli.cation within this volume. A sample application of the algorithms we describe can be seen 
in Figure 1. Such an approach has several bene.ts in computer graph­ics. First, one can very precisely 
quantify the amount of approximation that is tolerable under given circumstances. Given a user-speci.ed 
error in number of pixels of devia­tion of an object s silhouette, it is possible to choose which level 
of detail to view from a particular distance to maintain that pixel error bound. Second, this approach 
allows one a .ne control over which regions of an object should be ap­proximated more and which ones 
less. This could be used for selectively preserving those features of an object that are perceptually 
important. Third, the user-speci.able tol­erance for approximation is the only parameter required to 
obtain the approximations; .ne tweaking of several param­eters depending upon the object to be approximated 
is not required. Thus, this approach is quite useful for automat­ing the process of topology-preserving 
simpli.cations of a large number of objects. This problem of scalability is im­portant for any simpli.cation 
algorithm. One of our main goals is to create a method for simpli.cation which is not only automatic 
for large datasets, but tends to preserve the shapes of the original models. The rest of the paper is 
organized in the following man­ner: we survey the related work in Section 2, explain our assumptions 
and terminology in Section 3, describe the en­velope and approximation computations in Sections 4 and 
5, present some useful extentions to and properties of the approximation algorithms in Section 6, and 
explain our im­plementation and results in Section 7. Background Approximation algorithms for polygonal 
models can be classi.ed into two broad categories: Min-# Approximations: For this version of the ap­proximation 
problem, given some error bound f,the objective is to minimize the number of vertices such that no point 
of the approximation Ais farther than f distance away from the input model I.  Min-fApproximations: 
Here we are given the num­ber of vertices of the approximation Aand the objec­tive is to minimize the 
error, or the difference, between Aand I.  Previous work in the area of min-# approximations has been 
done by [6, 20] where they adaptively subdivide a series of bicubic patches and polygons over a surface 
until they .t the data within the tolerance levels. In the second category, work has been done by several 
groups. Turk [23] .rst distributes a given number of vertices over the surface depending on the curvature 
and then re­triangulates them to obtain the .nal mesh. Schroeder et al. [21] and Hinker and Hansen [13] 
operate on a set of local rules such as deleting edges or vertices from almost coplanar adjacent faces, 
followed by local re-triangulation. These rules are applied iteratively till they are no longer applicable. 
A somewhat different local approach is taken in [18] where vertices that are close to each other are 
clustered and a new vertex is generated to represent them. The mesh is suitably updated to re.ect this. 
Hoppe et al. [14] proceed by iteratively optimizing an energy function over a mesh to minimize both the 
distance of the approximating mesh from the original, as well as the number of approximating vertices. 
An interesting and ele­gant solution to the problem of polygonal simpli.cation by using wavelets has 
been presented in [7, 8] where arbitrary polygonal meshes are .rst subdivided into patches with subdivision 
connectivity and then multiresolution wavelet analysis is used over each patch. This wavelet approach 
preserves global topology. In computational geometry, it has been shown that com­puting the minimal-facet 
f-approximation is NP-hard for both convex polytopes [5] and polyhedral terrains [1]. Thus, algorithms 
to solve these problems have evolved around .nding polynomial-time approximations that are close to the 
optimal. Let kobe the size of a min-# approximation. An algorithm has been given in [16] for computing 
an f­approximation of size O(kolog n)for convex polytopes. This has recently been improved by Clarkson 
in [3]; he proposes a randomized algorithm for computing an approx­ imation of size O(kolog ko)in expected 
time O(kon 1+O) for any J0 (the constant of proportionality depends on J, and tends to +1as Jtendsto0). 
In[2]Br¨onnimannand Goodrich observed that a variant of Clarkson s algorithm yields a polynomial-time 
deterministic algorithm that com­putes an approximation of size O(k0). Working with poly­hedral terrains, 
[1] present a polynomial-time algorithm that computes an f-approximation of size O(kolog ko)to a polyhedral 
terrain. Our work is different from the above in that it allows adaptive, genus-preserving, f-approximation 
of arbitrary polygonal objects. Additionally, we can simplify bordered meshes and meshes with holes. 
In terms of direct compari­son with the global topologypreserving approach presented in [7, 8], for a 
given four algorithm has been empirically able to obtain reduced" simpli.cations, which are much smaller 
in output size (as demonstrated in Section 7). The algorithm in [18] also guarantees a bound in terms 
of the Hausdorff metric. However, it is not guaranteed to preserve the genus of the original model. 3 
Terminology and Assumptions Let us assume that Iis a three-dimensional compact and ori­entable object 
whose polygonal representation Phas been given to us. Our objective is to compute a piecewise-linear 
approximation Aof P. Given two piecewise linear objects Pand Q, we say that Pand Qare f-approximationsof 
each other iff every point on Pis within a distance fof some point of Qand every point on Qis within 
a distance fof some point of P. Our goal is to outline a method to generate two envelope surfaces surrounding 
Pand demonstrate how the envelopes can be used to solve the following polygonal approximation problem. 
Given a polygonal representation Pof an object and an approximation parameter f, generate a genus-preserving 
f-approximation Awith minimal num­ber of polygons such that the vertices of Aare a subset of vertices 
of P. We assume that all polygons in Pare triangles and that Pis a well-behaved polygonal model, i.e., 
every edge has either one or two adjacent triangles, no two triangles inter­penetrate, there are no unintentional 
cracks" in the model, no T-junctions, etc. We also assume that each vertex of Phas a single normal vector, 
which must lie within 90oof the normal of each of its surrounding triangles. For the purpose of rendering, 
each vertex may have either a single normal or multiple normals. For the purpose of generating envelope 
surfaces, we shall compute a single vertex normal as a combination of the normals of the surrounding 
triangles. The three-dimensional f-offset surface for a parametric surface f(s;t)(f1(s;t);f2(s;t);f3(s;t)); 
 whose unit normal to fis n(s;t)(n1(s;t);n2(s;t);n3(s;t)); HHH is de.ned as fH(s;t)(f(s;t);f(s;t);f(s;t)),where 
123 fiH(s;t)fi(s;t)+fni(s;t): Note that offset surfaces for a polygonal object can self­intersect and 
may contain non-linear elements. We de.ne a simpli.cation envelope P(+f)(respectively P(-f))for an object 
Ito bea polygonal surface that lies within adis­tance of ffrom every point pon Iin the same (respectively 
opposite) direction as the normal to Iat p. Thus, the simpli­.cation envelopes can be thought of as an 
approximation to offset surfaces. Henceforth we shall refer to simpli.cation envelope by simply envelope. 
Let us refer to the triangles of the given polygonal repre­sentation Pas the fundamental triangles.Let 
e(v1;v2) be an edge of P. If the normals n1;n2 to Iat both v1 and v2, respectively, are identical, then 
we can construct a plane 7ethat passes through the edge eand has a normal that is perpendicular to that 
of v1. Thus v1, v2 and their normals all lie along 7e. Such a plane de.nes two half-spaces for edge e,say 
7e +and 7e.(see Fig 2(a)). However, in general the normals n1 and n2 at the vertices v1 and v2 need not 
be identical, in which case it is not clear how to de.ne the two half-spaces for an edge. One choice 
is to use a bilinear patch that passes through v1 and v2 and has a tangent n1 at v1 and n2 at v2. Let 
us call such a bilinear patch for eas the edge half-space fe. Let the two half-spaces for the edge e 
in this case be fe+and fe.. Thisisshown in Fig 2(b). - + e n2 e e (b)(a) Figure 2: Edge Half-spaces 
Let the vertices of a fundamental triangle be v1, v2,and v3. Let the coordinates and the normal of each 
vertex vbe represented as c(v)and n(v), respectively. The coordinates and the normal of a (+f)-offset 
vertex vi+for a vertex vi ++ are: c(v)c(v)+fn(v),and n(v)n(v).The iii ii (-f)-offset vertex can be similarly 
de.ned in the opposite direction. These offset vertices for a fundamental triangle are shown in Figure 
3. Now consider the closed object de.ned by vi+and vi. , i1;2;3. It is de.ned by two triangles, at the 
top and bottom, and three edge half-spaces. This object contains the fundamental triangle (shown shaded 
in Figure 3) and we will henceforth refer to it as the fundamental prism. 4 Envelope Computation In 
order to preserve the input topology of P,we desire that the simpli.cation envelopes do not self-intersect. 
To meet this criterion we reduce our level of approximation at certain places. In other words, to guarantee 
that no intersections amongst the envelopes occur, we have to be + v - v 3 -v1 Figure 3: The Fundamental 
Prism content at certain places with the distance between Pand the envelope being smaller than f. This 
is how simpli.cation envelopes differ from offset surfaces. We construct our envelope such that each 
of its trian­gles corresponds to a fundamental triangle. We offset each vertex of the original surface 
in the direction of its normal vector to transform the fundamental triangles into those of the envelope. 
If we offset each vertex viby the same amount f,to get the offset vertices vi+and vi., the resulting 
envelopes, P(+f)and P(-f), may contain self-intersections because one or more offset vertices are closer 
to some non-adjacent fundamental triangle. In other words, if we de.ne a Voronoi diagram over the fundamental 
triangles of the model, the condition for the envelopes to intersect is that there be at least one offset 
vertex lying in the Voronoi region of some non-adjacent fundamental triangle. This is shown in Fig­ure4bymeansofatwo-dimensionalexample. 
Inthe.gure, the offset vertices b+and c +are in the Voronoi regions of edges other than their own, thus 
causing self-intersection of the envelope. Figure 4: Offset Surfaces Once we make this observation, 
the solution to avoid self­intersections becomes quite simple just do not allow an offset vertex to 
go beyond the Voronoi regions of its adjacent fundamental triangles. In other words, determine the positive 
and negative ffor each vertex visuch that the vertices vi+and vi.determined with this new fdo not lie 
in the Voronoi regions of the non-adjacent fundamental triangles. While this works in theory, ef.cient 
and robust com­putation of the three-dimensional Voronoi diagram of the fundamental triangles is non-trivial. 
We now present two methods for computing the reduced ffor each vertex, the .rst method analytical, and 
the second numerical. 4.1 Analytical .Computation We adopt a conservative approach for recomputing the 
fat each vertex. This approach underestimates the values for the positive and negative f. In other words, 
it guarantees the envelope surfaces not to intersect, but it does not guar­antee that the fat each vertex 
is the largest permissible f. We next discuss this approach for the case of computing the positive ffor 
each vertex. Computation of negative f follows similarly. Consider a fundamental triangle t. Wede.ne 
aprism tpfor t, which is conceptually the same as its fundamental prism, but uses a value of 2finstead 
of ffor de.ning the envelope vertices. Next, consider all triangles .ithat do not share a vertex with 
t.If .iintersects tpabove t(the directions above and below tare determined by the direction of the normal 
to t, above is in the same direction as the normal to t), we .nd the point on .ithat lies within tpand 
is closest to t. This point would be either a vertex of .i, or the intersection point of one of its edges 
with the three sides of the prism tp. Once we .nd the point of closest approach, we compute the distance 
Jiof this point from t. This is shown in Figure 5. v3 v1 t p Figure 5: Computation of .i Once we have 
done this for all .i, we compute the new value of the positive ffor the triangle tas fnew 1 miniJi. 2 
If the vertices for this triangle thave this value of positive f, their positive envelope surface will 
not self-intersect. Once the fnew(t)values for all the triangles thave been computed, the fnew(v)for 
each vertex vis set to be the minimum of the fnew(t)values for all its adjacent triangles. We use an 
octree in our implementation to speed up the identi.cation of triangles .ithat intersect a given prism. 
 4.2 Numerical .Computation To compute an envelope surface numerically, we take an it­erative approach. 
Our envelope surface is initially identical to the input model surface. In each iteration, we sequen­tially 
attempt to move each envelope vertex a fraction of the fdistance in the direction of its normal vector 
(or the opposite direction, for the inner envelope). This effectively stretches or contracts all the 
triangles adjacent to the vertex. We test each of these adjacent triangles for intersection with each 
other and the rest of the model. If no such intersections are found, we accept the step, leaving the 
vertex in this new position. Otherwise we reject it, moving the vertex back to its previous position. 
The iteration terminates when all vertices have either moved for can no longer move. In an attempt to 
guarantee that each vertex gets to move a reasonable amount of its potential distance, we use an adaptive 
step size. We encourage a vertex to move at least K(an arbitrary constant which is scaled with respect 
to f and the size of the object) steps by allowing it to reduce its step size. If a vertex has moved 
less than Ksteps and its move is been rejected, it divides its step size in half and tries again (with 
some maximum number of divides allowed on any particular step). Notice that if a vertex moves isteps 
and is rejected on the (i+1)st step, we know it has moved at least i/(i+1)% of its potential distance, 
so K/(K+1) which is a lower bound of sorts. It is possible, though rare, for a vertex to move less than 
Ksteps, if its current position is already quite close to another triangle. Each vertex also has its 
own initial step size. We .rst choose a global, maximum step size based on a global prop­erty: either 
some small percentage of the object s bounding box diagonal length or f/K, whichever is smaller. Now 
for each vertex, we calculate a local step size. This local step size is some percentage of the vertex 
s shortest incident edge (only those edges within 90oof the offset direction are considered). We set 
the vertex s step size to the minimum of the global step size and its local step size. This makes it 
likely that each vertex s step size is appropriate for a step given the initial mesh con.guration. This 
approach to computing an envelope surface is ro­bust, simple to implement (if dif.cult to explain), and 
fair to all the vertices. It tends to maximize the minimum off­set distance amongst the envelope vertices. 
It works fairly well in practice, though there may still be some room for improvement in generating maximal 
offsets for thin objects. Figure 6 shows internal and external envelopes computed for three values of 
fusing this approach. As in the analytical approach, a simple octree data struc­ture makes these intersection 
tests reasonably ef.cient, es­pecially for models with evenly sized triangles.  5 Generation of Approximation 
Generating a surface approximation typically involves start­ing with the input surface and iteratively 
making modi.ca­tions to ultimately reduce its complexity. This process may be broken into two main stages: 
hole creation,and hole .lling. We .rst create a hole by removing some connected set of triangles from 
the surface mesh. Then we .ll the hole with a smaller set of triangles, resulting in some reduction of 
the mesh complexity. We demonstrate the generality of the simpli.cation en­velope approach by designing 
two algorithms. The hole .lling stages of these algorithms are quite similar, but their hole creation 
stages are quite different. The .rst algorithm makes only local choices, creating relatively small holes, 
while the second algorithm uses global information about the surface to create maximally-sized holes. 
These design choices produce algorithms with very different properties. We begin by describing the envelope 
validity test used to determine whether a candidate triangle is valid for inclusion in the approximation 
surface. We then proceed to the two example simpli.cation algorithms and a description of their relative 
merits. 5.1 Validity Test A candidate triangle is one which we are considering for inclusion in an approximation 
to the input surface. Valid candidate triangles must lie between the two envelopes. Because we construct 
candidate triangles from the vertices of the original model, we know its vertices lie between the two 
envelopes. Therefore, it is suf.cient to test the candidate triangle for intersections with the two envelope 
 Inner Envelopes fOuter Envelopes Figure 6: Simpli.cation envelopes for various . surfaces. We can perform 
such tests ef.ciently using a space-partitioning data structure such as an octree. A valid candidate 
triangle must also not cause a self­intersection in our surface, Therefore, it must not intersect any 
triangle of the current approximation surface. 5.2 Local Algorithm To handle large models ef.ciently 
within the framework of simpli.cation envelopes we construct a vertex-removal­based local algorithm. 
This algorithm draws heavily on the work of [21], [23], and [14]. Its main contributions are the use 
of envelopes to provide global error bounds as well as topology preservation and non-self-intersection. 
We have also explored the use of a more exhaustive hole-.lling approach than any previous work we have 
seen. The local algorithm begins by placing all vertices in a queue for removal processing. For each 
vertex in the queue, we attempt to remove it by creating a hole (remov­ing the vertex s adjacent triangles) 
and attempting to .ll it. If we can successfully .ll the hole, the mesh modi.cation is accepted, the 
vertex is removed from the queue, and its neighbors are placed back in the queue. If not, the vertex 
is removed from the queue and the mesh remains unchanged. This process terminates when the global error 
bounds even­tually prevent the removal of any more vertices. Once the vertex queue is empty we have our 
simpli.ed mesh. For a given vertex, we .rst create a hole by removing all adjacent triangles. We begin 
the hole-.lling process by generating all possible triangles formed by combinations of the vertices on 
the hole boundary. This is not strictly necessary, but it allows us to use a greedy strategy to favor 
triangles with nice aspect ratios. We .ll the hole by choos­ing a triangle, testing its validity, and 
recursively .lling the three (or fewer) smaller holes created by adding that trian­gle into the hole 
(see .gure 7). If a hole cannot be .lled at any level of the recursion, the entire hole .lling attempt 
is considered a failure. Note that this is a single-pass hole .lling strategy; we do not backtrack or 
undo selection of a triangle chosen for .lling a hole. Thus, this approach does not guarantee that if 
a triangulation of a hole exists we will .nd it. However, it is quite fast and works very well in practice. 
 Figure 7: Hole .lling: adding a triangle into a hole creates up to three smaller holes We have compared 
the above approach with an exhaus­tive approach in which we tried all possible hole-.lling tri­angulations. 
For simpli.cations resulting in the removal of hundreds of vertices (like highly oversampled laser-scanned 
models), the exhaustive pass yielded only a small improve­ment over the single-pass heuristic. This sort 
of con.rma­tion reassures us that the single-pass heuristic works well in practice. 5.3 Global Algorithm 
This algorithm extends the algorithm presented in [3] to non-convex surfaces. Our major contribution 
is the use of simpli.cation envelopes to bound the error on a non-convex polygonal surface and the use 
of fundamental prisms to provide a generalized projection mechanism for testing for regions of multiple 
covering (overlaps). We present only a sketch of the algorithm here ; see [24] for the full details. 
We begin by generating all possible candidate triangles for our approximation surface. These triangles 
are all 3­tuples of the input vertices which do not intersect either of the offset surfaces. Next we 
determine how many vertices each triangle covers. We rank the candidate triangles in order of decreasing 
covering. We then choose from this list of candidate triangles in a greedy fashion. For each triangle 
we choose, we create a large hole in the current approximation surface, removing all triangles which 
overlap this candidate triangle. Now we begin the recursive hole-.lling process by placing this candidate 
triangle into the hole and .lling all the subholes with other triangles, if possible. One further restriction 
in this process is that the candidate triangle we are testing should not overlap any of the candidate 
triangles we have previously accepted. 5.4 Algorithm Comparison The local simpli.cation algorithm is 
fast and robust enough to be applied to large models. The global strategy is the­oretically elegant. 
While the global algorithm works well for small models, its complexity rises at least quadratically, 
envelope curve original curve envelope curve approximating curve Figure 8: Curve at local minimum of 
approximation making it prohibitive for larger models. We can think of the simpli.cation problem as an 
optimization problem as well. A purely local algorithm may get trapped in a local min­imum of simpli.cation, 
while an ideal global algorithm will avoid all such minima. Figure 8 shows a two-dimensional example 
of a curve for which a local vertex removal algorithm might fail, but an algorithm that globally searches 
the solution space will suc­ceed in .nding a valid approximation. Any of the interior vertices we remove 
would cause a new edge to penetrate an envelope curve. But if we remove all of the interior vertices, 
the resulting edge is perfectly acceptable. This observation motivates a wide range of algorithms of 
which our local and global examples are the two extremes. We can easily imagine an algorithm that chooses 
nearby groups of vertices to remove simultaneously rather than sequentially. This could potentially lead 
to increased speed and simpli.cation performance. However, choosing such sets of vertices remains a challenging 
problem.  6 Additional Features Envelope surfaces used in conjunction with simpli.cation algorithms 
are powerful, general-purpose tools. As we will now describe, they implicitly preserve sharp edges and 
can be extended to deal with bordered surfaces and perform adaptive approximations. 6.1 Preserving Sharp 
Edges One of the important properties in any approximation scheme is the way it preserves any sharp edges 
or normal discontinuities present in the input model. Simpli.cation envelopes deal gracefully with sharp 
edges (those with a small angle between their adjacent faces). When the ftol­erance is small, there is 
not enough room to simplify across these sharp edges, so they are automatically preserved. As the tolerance 
is increased, it will eventually be possible to simplify across the edges (which should no longer be 
vis­ible from the appropriate distance). Notice that it is not necessary to explicitly recognize these 
sharp edges. 6.2 Bordered Surfaces A bordered surface is one containing points that are home­omorphic 
to a half-disc. For polygonal models, this corre­sponds to edges that are adjacent to a single face rather 
than two faces. Depending on the context, we may naturally think of this as the boundary of some plane-like 
piece of a surface, or a hole in an otherwise closed surface. The algorithms described in 5 are suf.cient 
for closed triangle meshes, but they will not guarantee our global er­ror bound for meshes with borders. 
While the envelopes constrain our error with respect to the normal direction of the surface, bordered 
surfaces require some additional constraints to hold the approximation border close to the original border. 
Without such constraints, the border of the approximation surface may creep in, possibly shrinking the 
surface out of existence. In many cases, the complexity of a surface s border curves may become a limiting 
factor in how much we can simplify the surface, so it is unacceptable to forgo simpli­fying these borders. 
We construct a set of border tubes to constrain the error in deviation of the border curves. Each border 
is actually a cyclic polyline. Intuitively speaking, a border tube is a smooth, non-self-intersecting 
surface around one of these polylines. Removing a border vertex causes a pair of border edges to be replaced 
by a single border edge. If this new border edge does not intersect the relevant border tube, we may 
safely attempt to remove the border vertex. To construct a tube we de.ne a plane passing through each 
vertex of the polyline. We choose a coordinate system on this plane and use that to de.ne a circular 
set of vertices. We connect these vertices for consecutive planes to con­struct our tube. Our initial 
tubes have a very narrow radius to minimize the likelihood of self-intersections. We then expand these 
narrow tubes using the same technique we used previously to construct our simpli.cation envelopes. The 
dif.cult task is to de.ne a coordinate system at each polyline vertex which encourages smooth, non-self­intersecting 
tubes. The most obvious approach might be to use the idea of Frenet frames from differential geometry 
to de.ne a set of coordinate systems for the polyline vertices. However, Frenet frames are meant for 
smooth curves. For a jagged polyline, a tube so constructed often has many self-intersections. Instead, 
we use a projection method to minimize the twist between consecutive frames. Like the Frenet frame method, 
we place the plane at each vertex so that the normal to the plane approximates the tangent to the polyline. 
This is called the normal plane. At the .rst vertex, we choose an arbitrary orthogonal pair of axes for 
our coordinate system in the normal plane. For subsequent vertices, we project the coordinate system 
from the previous normal plane onto the current normal frame. We then orthogonalize this projected coordinate 
system in the plane. For the normal plane of the .nal polyline vertex, we average the projected coordinate 
systems of the previous normal plane and the initial normal plane to minimize any twist in the .nal tube 
segment. 6.3 Adaptive Approximation For certain classes of objects it is desirable to perform an adaptive 
approximation. For instance, consider large ter­rain datasets, models of spaceships, or submarines. One 
would like to have more detail near the observer and less detail further away. A possible solution could 
be to sub­divide the model into various spatial cells and use a dif­ferent f-approximation for each cell. 
However, problems would arise at the boundaries of such cells where the f­approximation for one cell, 
say at a value f1 need not nec­essarily be continuous with the f-approximation for the neighboring cell, 
say at a different value f2. Since all candidate triangles generated are constrained to lie within the 
two envelopes, manipulation of these en­velopes provides one way to smoothly control the level of approximation. 
Thus, one could specify the fat a given vertex to be a function of its distance from the observer the 
larger the distance, the greater is the f. As another possibility, consider the case where certain features 
of a model are very important and are not to be approximated beyond a certain level. Such features might 
have human perception as a basis for their de.nition or they might have mathematical descriptions such 
as regions of high curvature. In either case, a user can vary the f associated with a region to increase 
or decrease the level of approximation. The bunny in Figure 9 illustrates such an approximation. Figure 
9: An adaptive simpli.cation for the bunny model. .varies from 1/64% at the nose to 1% at the tail. 
 7 Implementation and Results We have implemented both algorithms and tried out the local algorithm on 
several thousand objects. We will .rst discuss some of the implementation issues and then present some 
results. 7.1 Implementation Issues The .rst important implementation issue is what sort of input model 
to accept. We chose to accept only manifold triangle meshes (or bordered manifolds). This means that 
each edge is adjacent to two (one in the case of a border) triangles and that each vertex is surrounded 
by a single ring of triangles. We also do not accept other forms of degenerate meshes. Many mesh degeneracies 
are not apparent on casual in­spection, so we have implemented an automatic degener­acy detection program. 
This program detects non-manifold vertices, non-manifold edges, sliver triangles, coincident triangles, 
T-junctions, and intersecting triangles in a pro­posed input mesh. Note that correcting these degeneracies 
is more dif.cult than detecting them. Robustness issues are important for implementations of any geometric 
algorithms. For instance, the analytical method for envelope computation involves the use of bi­linear 
patches and the computation of intersection points. The computation of intersection points, even for 
linear el­ements, is dif.cult to perform robustly. The numerical method for envelope computation is much 
more robust be­cause it involves only linear elements. Furthermore, it requires an intersection test 
but not the calculation of inter­section points. We perform all such intersection tests in a conservative 
manner, using fuzzy intersection tests that may report intersections even for someclosebut non-intersecting 
elements. Another important issue is the use of a space-partitioning scheme to speed up intersection 
tests. We chose to use an octree because of its simplicity. Our current octree im­plementation deals 
only with the bounding boxes of the elements stored. This works well for models with trian­gles that 
are evenly sized and shaped. For CAD models, which may contain long, skinny, non-axis-alignedtriangles, 
a simple octree does not always provide enough of a speed­up, and it may be necessary to choose a more 
appropriate space-partitioning scheme. 7.2 Results We have simpli.ed a totalof 2636 objects from the 
auxiliary machine room (AMR) of a submarine dataset, pictured in Figure 10 to test and validate our algorithm. 
We reproduce the timings and simpli.cations achieved by our implemen­tation for the AMR and a few other 
models in Table 1. All simpli.cations were performed on a Hewlett-Packard 735/125 with 80 MB of main 
memory. Images of these simpli.cations appear in Figures 11 and 12. It is interest­ing to compare the 
results on the bunny and phone models with those of [7, 8]. For the same error bound, we are able to 
obtain much improved solutions. We have automated the process which sets the fvalue for each object by 
assigning it to be a percentage of the diagonal of its bounding box. We obtained the reductions presented 
in Table 1 for the AMR and Figures 11 and 12 by using this heuristic. For the rotor and AMR models in 
the above results, the ith1th level of detail was obtained by simplifying the i­level of detail. This 
causes to total fto be the sum of all previous f s, so choosing f0 sof 1, 2, 4, and 8 percent results 
in total f0 sof 1, 3, 7, and 15 percent. There are two advantages to this scheme: (a) It allows one to 
proceed incrementally, taking advantage of the work done in previous simpli.cations. (b) It builds a 
hierarchy of detail in which the vertices at the ith1th  level of detail are a subset of the vertices 
at the i­level of detail. One of the advantages of the setting fto a percent of the object size is that 
it provides an a way to automate the selection of switching points used to transition between the various 
representations. To eliminate visual artifacts, we switch to a more faithful representation of an object 
when fprojects to more than some user-speci.ed number of pixels on the screen. This is a function of 
the ffor that approximation, the output display resolution, and the corresponding maximum tolerable visible 
error in pixels.  8 Future Work There are still several areas to be explored in this research. We believe 
the most important of these to be the generation of correspondences between levels of detail and the 
moving of vertices within the envelope volume. Bunny Phone Rotor AMR f% # Polys Time f% # Polys Time 
f% # Polys Time f% # Polys Time 0 69,451 N/A 0 165,936 N/A 0 4,735 N/A 0 436,402 N/A 1/64 44,621 9 1/64 
43,537 31 1/8 2,146 3 1 195,446 171 1/32 23,581 10 1/32 12,364 35 1/4 1,514 2 3 143,728 61 1/16 10,793 
11 1/16 4,891 38 3/4 1,266 2 7 110,090 61 1/8 4,838 11 1/8 2,201 32 13/4 850 1 15 87,476 68 1/4 2,204 
11 1/4 1,032 35 33/4 716 1 31 75,434 84 1/2 1,004 11 1/2 544 33 73/4 688 1 1 575 11 1 412 30 15 3/4 674 
1 Table 1: Simpli.cation . s and run times in minutes 8.1 Generating Correspondences A true geometric 
hierarchy should contain not only repre­sentations of an object at various levels of detail, but also 
some correspondence information about the relationship between adjacent levels. These relationships are 
neces­sary for propagating local information from one level to the next. For instance, this information 
would be helpful for using the hierarchical geometric representation to perform radiosity calculations. 
It is also necessary for performing geometric interpolation between the models when using the levels 
of detail for rendering. Note that the envelope tech­nique preserves silhouettes when rendering, so it 
is also a good candidate for alpha blending rather than geometric interpolation to smooth out transitions 
between levels of detail. We can determine which elements of a higher level of detail surface are covered 
by an element of a lower level of detail representation by noting which fundamental prisms this element 
intersects. This is non-trivial only because of the bilinear patches that are the sides of a fundamental 
prism. We can approximate these patches by two or more triangles and then tetrahedralize each prism. 
Given this tetrahedralization of the envelope volume, it is possible to stab each edge of the lower level-of-detail 
model through the tetrahedrons to determine which ones they intersect, and thus which triangles are covered 
by each lower level­of-detail triangle. 8.2 Moving Vertices The output mesh generated by either of the 
algorithms we have presented has the property that its set of vertices is a subset of the set of vertices 
of the original mesh. If we can afford to relax this constraint somewhat, we may be able to reduce the 
output size even further. If we allow the vertices to slide along their normal vectors, we should be 
able to simplify parts of the surface that might otherwise be impossible to simplify for some choices 
of epsilon. We are currently working on a goal-based approach to mov­ing vertices within the envelope 
volume. For each vertex we want to remove, we slide its neighboring vertices along their normals to make 
them lie as closely as possible to a tangent plane of the original vertex. Intuitively, this should increase 
the likelihood of successfully removing the vertex. During this whole process, we must ensure that none 
of the neighboring triangles ever violates the envelopes. This approach should make it possible to simplify 
surfaces using smaller epsilons than previously possible. In fact, it may even enable us to use the original 
surface and a single en­velope as our constraint surfaces rather than two envelopes. This is important 
for objects with areas of high maximal curvature, like thin cylinders.   9Conclusion We have outlined 
the notion of simpli.cation envelopes and how they can be used for generation of multiresolution hi­erarchies 
for polygonal objects. Our approach guarantees non-self-intersecting approximations and allows the user 
to do adaptive approximations by simply editing the sim­pli.cation envelopes (either manually or automatically) 
in the regions of interest. It allows for a global error toler­ance, preservation of the input genus 
of the object, and preservation of sharp edges. Our approach requires only one user-speci.able parameter, 
allowing it to work on large collections of objects with no manual intervention if so de­sired. It is 
rotationally and translationally invariant, and can elegantly handle holes and bordered surfaces through 
the use of cylindrical tubes. Simpli.cation envelopes are gen­eral enough to permit both simpli.cation 
algorithms with good theoretical properties such as our global algorithm, as well as fast, practical, 
and robust implementations like our local algorithm. Additionally, envelopes permit easy gen­eration 
of correspondences across several levels of detail. 10 Acknowledgements Thanks to Greg Angelini, Jim 
Boudreaux, and Ken Fast at Electric Boat for the submarine model, Rich Riesen­feld and Elaine Cohen of 
the Alpha 1 group at the Uni­versity of Utah for the rotor model, and the Stanford Computer Graphics 
Laboratory for the bunny and tele­phone models.Thanks to Carl Mueller, Marc Olano, and Bill Yakowenko 
for many useful suggestions, and to the rest of the UNC Simpli.cation Group (Rui Bastos, Carl Erikson, 
Merlin Hughes, and David Luebke) for provid­ing a great forum for discussing ideas. The funding for this 
work was provide by a Link Foundation Fellowship, Alfred P. Sloan Foundation Fellowship, ARO Contract 
P­34982-MA, ARO MURI grant DAAH04-96-1-0013, NSF Grant CCR-9319957, NSF Grant CCR-9301259, NSF Ca­reer 
Award CCR-9502239, ONR Contract N00014-94-1­0738, ARPA Contract DABT63-93-C-0048, NSF/ARPA Center for 
Computer Graphics and Scienti.c Visualization, NIH Grant RR02170, an NYI award with matching funds from 
Xerox Corp, and a U.S.-Israeli Binational Science Foundation grant.  References [1] P. Agarwal and 
S. Suri. Surface approximation and geometric par­titions. In Proceedings Fifth Symposium on Discrete 
Algorithms, pages 24 33, 1994. [2] H. Br¨onnimann and M. Goodrich. Almost optimal set covers in .nite 
VC-dimension. In Proceedings Tenth ACM Symposium on Computational Geometry, pages 293 302, 1994. [3] 
K. L. Clarkson. Algorithms for polytope covering and approxima­tion. In Proc. 3rd Workshop Algorithms 
Data Struct., Lecture Notes in Computer Science, 1993. [4] M. Cosman and R. Schumacker. System strategies 
to optimize CIG image content. In Proceedings of the Image II Conference, Scotts­dale, Arizona, June 
10 12 1981. [5] G. Das and D. Joseph. The complexity of minimum convex nested polyhedra. In Proc. 2nd 
Canad. Conf. Comput. Geom., pages 296 301, 1990. [6] M. J. DeHaemer, Jr. and M. J. Zyda. Simpli.cation 
of objects rendered by polygonal approximations. Computers &#38; Graphics, 15(2):175 184, 1991. [7] T. 
D. DeRose, M. Lounsbery, and J. Warren. Multiresolution analysis for surface of arbitrary topological 
type. Report 93-10-05, Depart­ment of Computer Science, University of Washington, Seattle, WA, 1993. 
[8] M. Eck, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. Multiresolution analysis 
of arbitrary meshes. Computer Graphics: Proceedings of SIGGRAPH 95, pages 173 182, 1995. [9] T. A. Funkhouser 
and C. H. S´equin. Adaptive display algorithm for interactive frame rates during visualization of complex 
virtual en­vironments. In Computer Graphics (SIGGRAPH 93 Proceedings), volume 27, pages 247 254, August 
1993. [10] N. Greene, M. Kass, and G. Miller. Hierarchical z-buffer visibility. In Computer Graphics: 
Proceedings of SIGGRAPH 1993, pages 231 238. ACM SIGGRAPH, 1993. [11] T. He, L. Hong, A. Kaufman, A. 
Varshney, and S. Wang. Voxel­based object simpli.cation. In G. M. Nielson and D. Silver, editors, IEEE 
Visualization 95 Proceedings, pages 296 303, 1995. [12] P. Heckbert and M. Garland. Multiresolution modeling 
for fast rendering. Proceedings of Graphics Interface, 1994. [13] P. Hinker and C. Hansen. Geometric 
optimization. In Gregory M. Nielson and Dan Bergeron, editors, Proceedings Visualization 93, pages 189 
195, October 1993. [14] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. 
In James T. Kajiya, editor, Computer Graphics (SIGGRAPH 93 Proceedings), volume 27, pages 19 26, August 
1993. [15] A. D. Kalvin and R. H. Taylor. Superfaces: Polyhedral approxi­mation with bounded error. Technical 
Report RC 19135 (#82286), IBM Research Division, T. J. Watson Research Center, Yorktown Heights, NY 10958, 
1993. [16] J. Mitchell and S. Suri. Separation and approximation of polyhedral surfaces. In Proceedings 
of 3rd ACM-SIAM Symposium on Discrete Algorithms, pages 296 306, 1992. [17] Kevin J. Renze and J. H. 
Oliver. Generalized surface and volume decimation for unstructured tessellated domains. In Proceedings 
of SIVE 95, 1995. [18] J. Rossignac and P. Borrel. Multi-resolution 3D approximations for rendering. 
In Modeling in Computer Graphics, pages 455 465. Springer-Verlag, June July 1993. [19] H. E. Rushmeier, 
C. Patterson, and A. Veerasamy. Geometric sim­pli.cation for indirect illumination calculations. In Proceedings 
Graphics Interface 93, pages 227 236, 1993. [20] F. J. Schmitt, B. A. Barsky, and W. Du. An adaptive 
subdivision method for surface-.tting from sampled data. Computer Graphics (SIGGRAPH 86 Proceedings), 
20(4):179 188, 1986. [21] W. J. Schroeder, J. A. Zarge, and W. E. Lorensen. Decimation of triangle meshes. 
In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 65 70, July 
1992. [22] G. Taubin. A signal processing approach to fair surface design. In Proc. of ACM Siggraph, 
pages 351 358, 1995. [23] G. Turk. Re-tiling polygonal surfaces. In Computer Graphics (SIG-GRAPH 92 Proceedings), 
volume 26, pages 55 64, July 1992. [24] A. Varshney. Hierarchical geometric approximations. Ph.D. The­sis 
TR-050-1994, Department of Computer Science, University of North Carolina, Chapel Hill, NC 27599-3175, 
1994.  (a) bunny model: 69,451 triangles (b) phone model: 165,936 triangles (c) rotor model: 4,736 
triangles (a) f1/16%, 10;793 triangles (b) f1/32%, 12;364 triangles (c) f1/8%, 2;146 triangles (a) f1/4%, 
2;204 triangles (b) f1/16%, 4;891 triangles (c) f3/4%, 1;266 triangles  (a) f1%, 575 triangles (b) f1%, 
412 triangles (c) f33/4%, 716 triangles Figure 12: Level-of-detail hierarchies for three models. The 
approximation distance, ., is taken as a percentage of the bounding box diagonal. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237222</article_id>
		<sort_key>129</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Position-based physics]]></title>
		<subtitle><![CDATA[simulating the motion of many highly interacting spheres and polyhedra]]></subtitle>
		<page_from>129</page_from>
		<page_to>136</page_to>
		<doi_number>10.1145/237170.237222</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237222</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Linear programming</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Continuous</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010357</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Continuous simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716.10011138.10010041</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization->Continuous optimization->Linear programming</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716.10011138.10010041</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization->Continuous optimization->Linear programming</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
			<gt>Verification</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P290829</person_id>
				<author_profile_id><![CDATA[81100564498]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Milenkovic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematics and Computer Science, University of Miami, P.O. Box 249085, Coral Gables, FL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>74356</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Baraff. Analytical methods for dynamic simulation of nonpenetrating rigid bodies. Computer Graphics (Proceedings of SIGGRAPH), 23(3):223-232, 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97881</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Curved surfaces and coherence for nonpenetrating rigid body simulation. In Forest Baskett, editor, Computer Graphics (SIGGRAPH '90 Proceedings), volume 24, pages 19-28, August 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122722</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Coping with friction for non-penetrating rigid body simulation. In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH '91 Proceedings), volume 25, pages 31-40, July 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Issues in computing contact forces for nonpenetrating rigid bodies. Algorithmica, 10(2-4):292-352, 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192168</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Fast contact force computation for nonpenetrating rigid bodies. In Andrew Glassner, editor, Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 23-34. ACM SIGGRAPH, ACM Press, July 1994.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618272</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Interactive simulation of solid rigid bodies. IEEE Computer Graphics and Applications, 15(3):63-75, May 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[David Baraff and Andrew Witkin. Dynamic simulation of non-penetrating flexible bodies. In Edwin E. Catmull, editor, Computer Graphics (SIGRAPH '92 Proceedings), volume 26, pages 303-308, July 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378509</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[R. Barzel and A. H. Barr. A modeling system based on dynamics constraints. Computer Graphics (Proceedings of SIG- GRAPH), 22(4):179-187, 1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>203303</ref_obj_id>
				<ref_obj_pid>203297</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Jim X. Chen and Niels Da Vitoria Lobo. Toward interactiverate simulation of fluids with moving obstacles using navierstokes equations. Graphical Models and Image Processing, 57(2):107-116, March 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[N. Chiba, S. Sanakanishi, K. Yokoyama, I. Ootawara, K. Muraoka, and N. Saito. Visual simulation of water currents using a particle-based behavioural model. Journal of Visualization and Computer Animation, 6(3):155-172, July 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>88370</ref_obj_id>
				<ref_obj_pid>88307</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D. Dobkin, J. Hershberger, D. Kirkpatrik, and S. Suri. Implicitly searching convolutions and computing depth of collision. In Proceedings of the Second SIGAL, pages 165-180, 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Jeff Goldsmith and Alan H. Barr. Applying constrained optimization to computer graphics. SMPTE Journal - Society of Motion Picture and Television Engineers, 102(10):910-912, October 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[L. Guibas, L. Ramshaw, and J. Stolfi. A Kinetic Framework for Computational Geometry. In Proceedings of the 24th Annual IEEE Symposium on Foundations of Computer Science, pages 100-111. IEEE, 1983.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378530</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[James K. Hahn. Realistic animation of rigid bodies. In John Dill, editor, Computer Graphics (SIGGRAPH '88 Proceedings), volume 22, pages 299-308, August 1988.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218443</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Mikako Harada, Andrew Witkin, and David Baraff. Interactive physically-based manipulation of discrete/continuous models. In Robert Cook, editor, Proceedings of SIGGRAPH '95, Computer Graphics Proceedings, Annual Conference Series, pages 199-208. ACM SIGGRAPH, ACM Press, August 1995.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>207264</ref_obj_id>
				<ref_obj_pid>207261</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Vincent Hayward, Stephane Aubry, Andre Foisy, and Yasmine Ghallab. Efficient collision prediction among many moving objects. The International Journal of Robotics Research, 14(2):129-143, April 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97884</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Michael Kass and Gavin Miller. Rapid, stable fluid dynamics for computer graphics. In Forest Baskett, editor, Computer Graphics (SIGGRAPH '90 Proceedings), volume 24, pages 49-57, August 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[A. Kaul, M.A. O'Connor, and V. Srinivasan. Computing Minkowski Sums of Regular Polygons. In Thomas Shermer, editor, Proceedings of the Third Canadian Conference on Computational Geometry, pages 74-77, Vancouver, British Columbia, 1991. Simon Frasier University.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222772</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Z. Li. Compaction Algorithms for Non-Convex Polygons and Their Applications. PhD thesis, Harvard University, Division of Applied Sciences, 1994.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[A. Luciani, A. Habibi, and E. Manzotti. A multi scale physical model of granular materials. In Proceedings of Graphics Interface '95, pages 136-145, May 1995.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[H. Mallinder. The modelling of large waterfalls using string texture. Journal of Visualization and Computer Animation, 6(1):3-10, January 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[V. Milenkovic, K. Daniels, and Z. Li. Placement and Compaction of Nonconvex Polygons for Clothing Manufacture. In Proceedings of the Fourth Canadian Conference on Computational GeomeoT, pages 236-243, St. Johns, Newfoundland, August 1992. Department of Computer Science, Memorial University of Newfoundland.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[V.J. Milenkovic and Z. Li. A Compaction Algorithm for Nonconvex Polygons and Its Application. Eu~vpean Journal of Operations Research, 84:539-560, 1995.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[H. Minkowski. Volumen und Oberfl~iche. Mathematische Annalen, 57:447-495, 1903.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[M. Moore and J. Wilhelms. Collision detection and response for computer animation. Computer Graphics (Proceedings of SIGGRAPH), 22(4):289-298, 1988.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378524</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[J. C. Platt and A. H. Barr. Constraint methods for flexible models. Computer Graphics (Proceedings of SIGGRAPH), 22(4):279-287, 1988.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Serra. Image Analysis and Mathematical Morphology, volume 1. Academic Press, New York, 1982.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[J. Serra, editor. Image Analysis and Mathematical Morphology, volume 2: Theoretical Advances. Academic Press, New York, 1988.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Mikio Shinya and Marie-Claire Forgue. Layout out objects with geometric and pysical constraints. Visual Computer, 11:188-201, August 1995.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218444</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[John Snyder. An interactive tool for placing curved surfaces with interpenetration. In Robert Cook, editor, Proceedings of SIGGRAPH '95, Computer Graphics Proceedings, Annual Conference Series, pages 209-218. ACM SIGGRAPH, ACM Press, August 1995.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134037</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Richard Szeliski and David Tonnesen. Surface modeling with oriented particle systems. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 185-194, July 1992.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Demitri Terzopoulos, John Platt, and Kurt Fleischer. Heating and melting deformable models (from goop to glop). In Proceedings of Graphics Intelface '89, pages 219-226, June 1989.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192227</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Andrew P. Witkin and Paul S. Heckbert. Using particles to sample and control implicit surfaces. In Andrew Glassner, editor, Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 269-278. ACM SIGGRAPH, ACM Press, July 1994.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Position-Based Physics: Simulating the Motion of Many Highly Interacting Spheres and Polyhedra Victor 
J. Milenkovic. University of Miami Abstract This paper proposes a simpli.ed position-based physics that 
allows us to rapidly generate piles or clumps of many objects: local energy minima under a variety of 
potential energy functions. We can also generate plausible motions for many highly interacting ob­jects 
from arbitrary starting positions to a local energy minimum. We present an ef.cient and numerically stable 
algorithm for car­rying out position-based physics on spheres and non-rotating poly­hedra through the 
use of linear programming. This algorithm is a generalization of an algorithm for .nding tight packings 
of (non­rotating) polygons in two dimensions. This work introduces lin­ear programming as a useful tool 
for graphics animation. As its name implies, position-based physics does not contain a notion of velocity, 
and thus it is not suitable for simulating the motion of free-.ying, unencumbered objects. However, it 
generates realistic motions of crowded sets of objects in con.ned spaces, and it does so at least two 
orders of magnitude faster than other techniques for simulatingthephysicalmotionsofobjects. Evenforuncon.nedob­jects, 
the new algorithm can rapidly generate realistic piles and clumps. CR Descriptors: G.1.6 [Numerical Analysis]: 
Optimization -Linear Programming; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling 
-Physically based modeling; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -Animation; 
I.6.8 [Simulation and Modeling]: Types of Simula­tion -Continuous. 1 Introduction To generate realistic 
animation, recent work in computer graphics has focused on methods to simulate the motion of objects 
under the laws of physics. Suppose one wants to create an animated dancer. Instead of laboriously choosing 
a sequence of poses, one creates a model of a dancer with masses, joints, and forces, and lets the laws 
of physics do the dancing. The laws of physics are well understood (for this domain), and current computers 
can simulate physics for 1Department of Mathematics and Computer Science, University of Mi­ami, P.O. 
Box 249085, Coral Gables, FL 33124. The research of Victor J. Milenkovic was funded by the Textile/Clothing 
Technology Corporation from funds awarded to them by the Alfred P. Sloan Foundation and by NSF grants 
CCR-91-157993 and CCR-90-09272. Permission to make digital or hard copies of part or all of this work 
or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 these types of models 
in near to real time. The main dif.culty of this approach is choosing a set of forces (parameters for 
the model) that allow the dancer to dance and not fall on its face. However, there are other domains 
for which the forces are easily determined but the physics is very dif.cult to simulate. Consider the 
problem of generating a pile or clump of many objects under gravity or mutually attractive forces. Creating 
or even verifying an equilib­rium state of the physical system is a complex problem. Consider further 
the problems of animating the sand in an hourglass or the sand on the beach as someone sets foot on it. 
Consider even the problem of animating the molecules of .uid in a lava lamp. These models involve many 
highly interacting three-dimensional objects. For even a modest number of grains of sand or molecules, 
the sim­ulation outstrips our computational resources.1 This paper proposes a simpli.ed position-based 
physics that al­lows us to rapidly generate piles or clumps of many objects: local energy minima under 
a variety of potential energy functions. Position-based physics allows us to rapidly generate plausible 
mo­tions for sets of many highly interacting objects. We present an ef­.cient and numerically stable 
algorithm, based on linear program­ming, for carrying out position-based physics on spheres and non­rotating 
polyhedra. This algorithm is a generalization of an algo­rithm for .nding tight packings of (non-rotating) 
polygons in two dimensions. This work introduces linear programming as a useful tool for graphics animation. 
As its name implies, position-based physics does not contain a notion of velocity, and thus it is not 
suit­able for simulating the motion of free-.ying, unencumbered ob­jects. However, it generates realistic 
motions of crowded sets of objects in con.ned spaces, and it does so at least two orders of magnitude 
faster than other techniques for simulating the physical motions of objects. Even for uncon.ned objects, 
the new algorithm can rapidly generate realistic piles and clumps. Section 2 compares position-based 
physics to other methods of physical simulation such as velocity-based contact force methods. It describes 
two problems which severely slow down velocity-based methods: local and global rattling. Section 3 gives 
the formal def­inition of position-based physics and gives an algorithm to carry it out. For a system 
of translating objects that involves only sphere­sphere and sphere-polyhedron contacts, it is proved 
that the linear programming based algorithm converges to an equilibrium of the potential energy function: 
the algorithm cannot stick at a non­equilibrium con.guration. Section 4 describes how to implement the 
algorithm using linear programming to simulate position-based physics on a set of spheres inside a polyhedron, 
in particular, a set of 1000 spheres inside an hourglass . Section 5 shows how to han­dle non-gravitational 
potential functions such as attraction among the spheres. Section 6 presents results and running times, 
and Sec­tion 7 discusses the implication of this work and directions of future research. 1Also, numerical 
instability, which is a minor problem for the simulation of robots or dancers, becomes a serious impediment 
when the number of interacting objects rises into the hundreds, thousands, or beyond. 2 Techniques for 
Physical Simulation We categorize physical simulation techniques as acceleration­based, velocity-based, 
or position-based. Acceleration-based meth­ods come closest to simulating true physics, and they are 
the most expensive to carry out in computation. Velocity-based methods are farther divorced from reality 
but are faster. Position-based meth­ods are the farthest from reality and the fastest. Spring model methods 
(also called penalty methods) [25] [26] are typical acceleration-based methods. They allow the objects 
to overlap. For each pair of overlapping objects, there is a repulsive force proportional to the amount 
of overlap. The resulting repulsive forces cause the objects to accelerate. Numerical integration con­verts 
acceleration to velocity and then to position. These methods require many small time-steps when the acceleration 
is high. The large number of steps results in a high computational cost. Also, it is often dif.cult to 
determine the correct step size. Incorrect dis­cretization of time can cause unusual numerical results 
such as non­conservation of energy or momentum. Contact force model methods (also called analytical methods) 
[14] [1] [8] are examples of velocity-based methods. See [4] for a discussion of the many issues involved 
in contact force models. Rigid bodies are allowed to contact but not overlap. Given the cur­rent set 
of contacts, the method computes a set of consistent ve­locities such that no two contacting objects 
penetrate each other. The objects move with these velocities until a new contact occurs. The velocity-based 
method is much more stable and faster than the acceleration-based method for two reasons: 1) it can exactly 
com­pute the time of the next contact, and 2) the resulting time-step tends to be much larger than that 
needed to accurately carry out numerical integration. Even though these methods do not simu­late acceleration, 
they can handle increasingly sophisticated types of objects and forces: curved surfaces [2], friction 
[3], and .exible bodies [7]. Unfortunately, the velocity-based method is subject to two problems which 
cause small time-steps and thus high compu­tational cost. Local rattle occurs when one object bounces 
between two others (such as the rapid bouncing that occurs when you bring a paddle down on a bouncing 
ping-pong ball). Global rattle occurs when there are many interacting objects. Since there are so many, 
it is inevitable that some pair will make contact in a short amount of time. Even systems which can rapidly 
detect the next collision [16] cannot reduce the number of collisions. Each new contact forces us to 
recalculate the velocities. Just as a velocity-based method eliminates accelerations, a position-based 
method eliminates velocities (and also time, mo­mentum, and kinetic energy). The model only needs to 
have a po­tential energy function. Under position-based physics, the objects are allowed to move from 
their current con.guration (positions) to a lower energy con.guration along any valid (non-overlapping) 
energy-diminishing path. Under a linear programming based algo­rithm for position-based physics, the 
motion consists of a sequence of steps yielding a piecewise linear path. Each step diminishes the energy 
as much as possible within some maximal valid convex set of con.gurations surrounding the current con.guration. 
This con­vex set depends both on contacts that are occurring and on all con­tacts that might occur. Therefore 
the algorithm does not have to stop prematurely to handle a new contact, and there is no local rat­tle 
. Position-based physics also avoids global rattle since each object moves a maximal amount. Even if 
two objects in the model require only a small motion to come into contact, this does not pre­vent other 
objects in the model from moving farther, if they are able to. In his Ph.D. thesis [19] and in joint 
work with this author [22, 23], Li introduced the concept of position-based modeling. His application 
is compaction: .nding tight packings of polygo­nal objects in the plane. As he and others have noted 
[29, 30], for most layout applications the motion of the objects is immate­rial, and only the .nal con.guration 
matters. He attempted to carry out compaction using a velocity-based method similar to Baraff s [1], 
but he found this to be very expensive computationally and also numerically unstable. He formulated a 
position-based model and algorithm. This algorithm uses Minkowski sums [27, 13] and a locality heuristic 
to calculate a maximum convex region of the con.guration space visible to the current con.guration. Linear 
pro­gramming .nds the lowest energy con.guration in this region, and the model jumps to this con.guration. 
According to his experi­ments, the method typically reaches a local energy minimum in .ve or fewer jumps 
even for a layout of more than 100 polygons with up to 75 vertices per polygon. For the examples which 
were simple enough for him to carry out the velocity-based minimization, the position-based method was 
at least two orders of magnitude faster. The algorithms presented in this paper also use position-based 
physics. For sphere-sphere and sphere-polyhedron interactions, they do not require explicit calculation 
of the Minkowski sum (un­like Li s method for polygons). For polyhedron-polyhedron inter­actions, they 
do require calculation of the Minkowski sum, but un­like Li s locality heuristic, they do not require 
that the polyhedra be decomposed into star-shaped2 components. The new algorithms can solve for the motions 
of spheres and polyhedra, whereas Li s algorithm is restricted to two-dimensional polygons. Interestingly 
enough, some recent work has moved away from the use of large complex optimization systems [5]. Position-based 
physics reduces motion planning to linear programming. Instead of dealing with implementing optimization 
code ourselves, we sim­ply use a commercial linear programming library. Goldsmith et al. [12] discuss 
a number of ways optimization is applied in the .eld of graphics. Most of these involve either starting 
or .nishing prob­lems. Furthermore, most are non-linear. We believe that position­based physics is unique 
in the way it uses linear programming to generate a complex motion from start to .nish. Position-based 
physics yields realistic motion for crowded sets of objects. For free-.ying objects, the motion can be 
less reason­able. However, even in situations for which position-based methods give an unrealistic motion, 
there may be other applications. Recent work on manipulation of models makes use of non-physical motion 
[6] [15]. Finally, it is important to note the difference between position­based physics and particle-based 
systems. Examples of particle­based systems are too numerous to note all of them. See [32] [17] [31] 
[33] [10] [9] [21] for recent work. In general, particle­based systems model moving particles with forces 
between them, not rigid colliding objects. Simulating rigid objects using parti­cles requires a very 
steep repulsive energy gradient, and hence particle-based systems are subject to the same dif.culties 
as other acceleration-based methods: small time-steps and long running times.  3 Position-Based Physics 
The philosophy behind position-based physics is to dispense with acceleration, force, velocity, and time. 
The state of the system is the current con.guration (position). The system has a potential energy that 
depends only on the con.guration. There is no notion of kinetic energy. Section 3.1 de.nes position-based 
physics and describes how it can be used for modeling and animation. Section 3.2 gives an algorithm for 
simulating position-based physics on models with a smoothly convex decomposable overlap space: the set 
of forbidden con.gurations is a union of convex regions with smooth boundaries. 2Aregion is star-shaped 
if it contains at least one point which can see the entire boundary. This algorithm uses linear programming. 
Section 3.3 proves that several types of models have smoothly convex decomposable over­lap spaces. Among 
these are models involving translating spheres inside a .xed polyhedron. We describe what modi.cations 
are nec­essary to handle multiple translating polyhedra. 3.1 De.nitions As per typical usage, con.guration 
space denotes the concatenation of the degrees of freedom of the model. For a set of nspheres, the con.guration 
space has 3ndimensions. The free space is the set of con.gurations for which no pair of objects overlap.3 
These free con.gurations are also referred to as valid or non-overlapping.The complement of the free 
space is the set of overlapping, invalid,or forbidden con.gurations. We denote the free space by F. We 
assume that the model has a potential energy which is a con­tinuous and differentiable real-valued function 
on the con.guration space. For con.guration c, the energy is denoted E(c). A valid motion under position-based 
physics is an energy­diminishing path in F. Given a starting con.guration c2F,the goal is to generate 
a continuous and piecewise differentiable curve ,(t)such that 1) ,(0)=c,2) ,(t)2Ffor t.0,3) E(,(t))is 
monotonically decreasing for t.0,3) limt!1,(t)=c1is an equilibrium con.guration. A con.guration cequilis 
an equilibrium con.guration if every valid curve O(t)out of cequil(O(0)=cequil and O(t)2Ffor t.0) is 
non-energy-decreasing: dd t E(O(t)). 0. Note that tis not time and c1may not be the equilibrium state 
reached by a velocity-based or acceleration-based physics with the same potential energy function. However, 
a limiting value of ,(t) is a valid resting position: a pile or clump . To generate a de­cent looking 
animation, we can sample ,(t)in a way that makes it appear that the objects in the model are moving with 
constant or varying velocity, as desired. Also, as shown in Section 4.4, it is possible to add additional 
constraints on the path to make it appear more natural.  3.2 Linear Programming Based Algorithm This 
section gives an algorithm for performing position-based physics on models with a particular type of 
free/forbidden space. The algorithm is proved to have an equilibrium position as a limit point. If the 
energy function is linear, then the algorithm can be implemented using linear programming. 3.2.1 Smooth 
Convex Decompositions Aset S of Rnis de.ned to be convex decomposableif it is the union of a .nite number 
of convex sets. It is smoothly convex decompos­able if it is the union of a .nite number of convex sets 
with smooth boundaries. We use a standard de.nition of smooth: a convex set C is smooth if each point 
on the boundary has a unique tangent plane and outward unit normal vector and if this plane/normal is 
a con­tinuous function on the boundary. Actually, it is easy to show that if Cis convex and if each point 
on the boundary of Chas a unique outward unit normal, then the normal vector varies continuously on the 
boundary. 3.2.2 Algorithm The following is an algorithm for position-based physics when the overlap 
(forbidden) space O=Fis smoothly convex de­composable. Region Ois the union of msmooth convex regions 
O1;O2;O3;:::;Om. 3We consider the exterior of the container to be a .xed object, and hence the de.nition 
of free space includes the container constraint. Let c=c0be the initial con.guration. The algorithm sets 
,(0)=c. The algorithm proceeds in a sequence of steps. Dur­ing the ith step, it constructs the portion 
of the curve from ,(i-1) to ,(i). Let us suppose the algorithm has constructed the curve up to con.guration 
,(i)=ci. Here is how it performs the next step. For each convex region Oj, it constructs the half-space 
H(ci;Oj)as follows. First it projects cionto the nearest point PROJ(ci;Oj)on Oj. It computes the tangent 
plane to Ojat PROJ(ci;Oj). This tangent plane bounds two half-spaces, and H(ci;Oj)is the one which does 
not contain the interior of Oj.  The algorithm computes the convex region  m . I(ci;F)=H(ci;Oj): j=1 
Assuming Fis closed (objects are allowed to touch), it fol­lows that I(ci;F)F. The algorithm computes 
the minimum energy con.guration ci+12I(ci;F)which is reachable by an energy-diminishing path from ,(i)=cito 
,(i+1)=ci+1within I(ci;F). In general, the last step requires some sort of numerical integration. However, 
if the energy function is linear, it can be accomplished us­ing linear programming. Linear programming 
can easily compute the point ci+12I(ci;F)which minimizes the energy. The algo­rithm can set ,(t), i.t.i+1equal 
to the line segment from ci to ci+1.Since I(ci;F)is convex, this segment is valid.  3.2.3 Correctness 
and Convergence The following theorem states that the limit points of the algorithm are equilibrium points. 
This does not mean that the algorithm has a unique limit. If we have a room full of falling objects, 
some objects might already be on the .oor. Any arbitrary motion of an object along the .oor does not 
change the gravitational energy. For odd i, con.guration cicould have a particular object at one end 
of the room. For even i, it could be at the other end of the room. Thus, as the rest of the objects settled 
into a pile , this one object may never come to rest. Nothing in the de.nition of position-based physics 
prevents it from bouncing from one wall to the other forever. In this case, the system will have two 
limit points. Any particular linear programming library will break ties con­sistently. Therefore, it 
will not allow an object to bounce from wall to wall forever. It is also possible to introduce a conservative 
en­ergy term: each object is attracted to its current location. Both of these facts can be used to ensure 
that the algorithm converges to a unique limit. Section 4 illustrates both of these for spheres inside 
a polyhedron. It is probably possible to modify the de.nition of position-based physics and the algorithm 
to disallow bouncing . However, there may be cases in which we want this behavior. Note that bouncing 
is not like rattling. Bouncing does not affect the amount by which the energy is reduced in one step, 
and therefore it does not increase the number of steps required to reach an energy minimum. For velocity-based 
methods, frequent collisions dimin­ish the time-step and greatly increase the time required to reach 
a minimum. We remind the reader that the de.nition of an equilibrium point depends only on Fand the energy 
function E(). An equilibrium point for position-based physics is the same as an equilibrium point for 
any other type of physics. Therefore, the following theorem as­serts that the algorithm converges to 
physically correct piles and clumps . It does not get stuck at a non-equilibrium con.guration. However, 
most models have very many equilibrium con.gurations. The algorithm does not necessarily converge to 
the same equilib­rium as a true simulation of physics. Theorem 3.1 For compact (closed and bounded) F, 
the sequence c0;c1;c2;c3;:::generated by the algorithm has at least one limit point. Each limit point 
is an equilibrium point. Proof: The existence of a limit point is a property of any sequence in a compact 
set. Let climbe a limit point. Suppose climis not an equilibrium point. Therefore, there is a curve O(t)out 
of climwhich diminishes the energy. It follows that the energy decreases out of climin the direction 
v=O0(0)(the tangent vector to O(t)at t=0). For each j, climeither lies outside or on the boundary of 
Oj.If it lies outside, then there exists0such that clim+tv62Ojfor 0t<.If climlies on the boundary of 
Oj,then vmust lie in the tangent plane to Ojat climor it must point into H(clim;Oj). Hence, there exists0such 
that clim+tv2I(clim;F)for 0t< .Also, E(clim+tv)has the same derivative at t=0 as E(O(t)). The conclusion 
is that if the algorithm reached clim, the next step would be able to diminish the energy. Let 8limbe 
the amount of decrease. The algorithm has climas a limit point. The construction of I(ci;F)uses only 
continuous functions. For cisuf.ciently close to clim, the energy decrease 8i =E(ci)-E(ci+1)can be arbitrarily 
close to 8lim.Since E()is also continuous, there must exist some i such that 8lim E(ci)-E(clim)< 2 It 
follows that E(ci+1)<decreases the energy, clim 8 and E(ci)-E(ci+1)=8i lim : 2 E(clim). Since the algorithm 
always cannot be a limit point. This contradicts the assumption that climis not an equilibrium point. 
  3.3 Models with Convex Decompositions This section examines some models with smoothly convex decom­posable 
overlap spaces. It is shown how to apply the algorithm of Section 3.2.2 to the problems of animating 
and generating piles of spheres and polyhedra under translation. 3.3.1 Minkowski Sum. The Minkowski sum 
[24, 13, 27, 28] of two point-sets (of R3in the case of this paper) is de.ned AtB=fa+bja2A;b2Bg: For 
a point-set A,let Adenote the set complement of A and de.ne -A=f-aja2Ag. For a vector v,de.ne A+v=fa+vja2 
Ag. Note that A+v=Atfvg. Suppose we have ntranslating objects A1;A2;A3;:::;An.It can easily be shown 
that Ai+viand Aj+vjoverlap if and only if vj-vi2Ait-Aj. Lemma 3.2 If Cis convex and Sis smoothly convex, 
then CtS is smoothly convex. Proof: It is easy to show that CtSis convex. Suppose pis a point on the 
boundary of CtSsuch that phas two distinct unit normal vectors uand u 0.Since uis a normal at p, pmaximizes 
the dot product u.pover all points in CtS.Since p=c+swhere c2Cand s2Scan be chosen independently, smust 
maximize 000 u.sover all points of S.Yet, p=c+swhere c2Cand 0 s2Shave maximum dot products with u 0.Since 
shas a unique 0000 normal, uis not a normal at s,and u.s<u.s. Therefore 000 u.(c+s)<u.(c+s). Therefore, 
p=c+sdoes not maximize the dot product with u 0. This contradicts the assumption that phas two unit normals. 
Therefore, phas a unique unit normal vector. It is easily shown that if a convex region with unique unit 
normal vectors has continuous unit normal vectors: it is smooth. Corollary 3.3 If Cis convex decomposable 
and if Sis smoothly convex decomposable, then CtSis smoothly convex decompos­able. Proof: Let the decompositions 
be, C=C1 C2... Cl and S=S1 S2... Sm: It can easily be shown that, lm .. CtS= CgtSh: g=1h=1 In other words, 
the Minkowski sum is the union of the Minkowski sum of each possible pair. By Lemma 3.2, each of these 
sums is smoothly convex. 3.3.2 Good Models The following theorem describes the type of problem to which 
we can apply the algorithm of Section 3.2. Theorem 3.4 If 1) only translation is allowed, 2) all objects 
are convex decomposable, and 3) at most one object is not smoothly convex decomposable, then the overlap 
space is smoothly convex decomposable. Proof: For each pair of objects, Aiand Aj, at least one is smoothly 
convex decomposable. Corollary 3.3 implies that the pair­wise overlap space fhvi;vjijvj-vi2Ait-Ajg is 
smoothly convex decomposable. This transforms to a cylinder fhv1;v2;:::;vnijvj-vi2Ait-Ajg in the con.guration 
space for the entire model which is decompos­able into a union of smooth convex cylinders. 3.3.3 Applications 
The main application of this paper is a collection of translating spheres in a polyhedral container. 
Clearly each sphere is smoothly convex decomposable. The remaining object, the complement of the container, 
is polyhedral and therefore convex decomposable: simply cut it along every plane of every face. We can 
not directly apply Theorem 3.4 to the problem of mul­tiple translating polyhedra since a polyhedron is 
convex decom­posable but not smoothly convex decomposable. However, we can smooth a polyhedral region 
Pby adding a small spherical region S. By Corollary 3.3, PtSis smoothly convex decomposable. Of course, 
when we render the motion, we will display the original polyhedra, not the smoothed polyhedra. They will 
have small gaps between them equal to the diameter of S. For moving objects, this will not be noticeable. 
Once, the objects form a pile or clump , we can run the algorithm with smaller and smaller S, perhaps 
halv­ing the radius each time, until the desired accuracy is attained. Li and Milenkovic s algorithm 
(for translating polygons in the plane) uses a somewhat different framework than that of the algo­rithm 
in Section 3.2.2. In essence, it chooses an arbitrary tangent line/unit normal when the normal is not 
unique. This means, for instance, that our compaction algorithm for polygons in the plane might get stuck 
at a non-equilibrium con.guration. We have never seen it get stuck in practice (but we really have no 
indepen­dent way to verify an equilibrium other than visual inspection). It is likely that one could 
safely animate multiple translating polyhedra without getting stuck. However, we have not yet run any 
experi­ments. Baraff [1] indicates that correctly choosing a set of tangents at each non-unique contact 
is NP-complete.  4 Simulating an Hourglass using Position-Based Physics The .rst section of the accompanying 
video tape demonstrates the simulation of an hourglass or egg-timer using position-based physics. The 
main body of the hourglass is shaded as a curved sur­face, but it is actually a polyhedron. Each horizontal 
cross section is a 32-gon, and thus the sides of the hourglass have 160 faces. Ini­tially, 1000 spheres 
are arranged in a 10 by 10 by 10 grid in the upper part of the hourglass. Position-based physics calculates 
a (lo­cal) gravitational energy minimum for the spheres in the base. As a side-effect, it simulates the 
.ow through the narrow waist of the hourglass. The video presents two hourglass simulations. Both are 
shown at 30 frames per second. Each frame is an actual energy minimization step. Rendering was done with 
rendrib.4 The .rst video has 750 frames, and the second has 812 frames. Sections 4.1 through 4.3 describe 
how the .rst video was gen­erated. Section 4.4 shows how extra constraints where added to generate a 
more realistic motion in the second video. The modi.ed algorithm has acceleration and conservative forces. 
This video il­lustrates how the path generated by position-based physics can be controlled to increase 
the realism. 4.1 Pairwise Constraints Instead of working in 3n-dimensional space, we choose a convex 
subset of the pairwise free spaces. We must do this for each pair of spheres and each sphere with respect 
to the hourglass polyhe­dron. Taken together, these constraints are equivalent to I(c;F)of Section 3.2.2. 
For a pair of spheres Siand Sjwith radii riand rjand current cur cur positions (centers) piand pj,de.ne 
cur cur cur ji p-p u= cur cur ij jpj -pij to be the unit vector pointing from Sito Sj. The convex region 
Rij is the set of con.gurations satisfying cur (pj-pi).uij ri+rj;for 1i<jn:(1) The half-space constraint 
pj-pi2Rijprevents the spheres from overlapping, and it is exactly equivalent to one of the half-spaces 
in the .rst step of the algorithm in Section 3.2.2. For a sphere Siand the hourglass polyhedron G,we 
do the fol­lowing. Set h=1.Let qhbe the point on the boundary of Gclosest cur to pi.Let, cur cur i p 
-qh v = hi cur jp -qhj i cur be the unit vector from qhto pi. We create the constraint, -.v cur (2) (piqh)hiri; 
and we throw away all points pof the boundary of Gwhichdonot satisfy cur (p-qh).vhi0: 4The Blue Moon 
Rendering Tools by Larry I. Gritz. (Intersecting a half-space with a polyhedron is easy if the faces 
are all convex: the intersection of each face with the half-space is also convex.) If some part of the 
boundary remains, then we increment hand repeat this process. We stop when no point on the boundary of 
the polyhedron remains. The resulting set of linear constraints on pigiven by all instances of Equation 
2 de.nes a convex region Riwhich is a projection of I(ci;F)of Section 3.2.2 (actually, it can be a superset, 
but that can only improve the convergence). 4.2 Minimizing the Potential Energy For the hourglass example, 
the potential energy is the sum of the z-coordinates of the spheres. This corresponds to the gravitational 
energy of a set of spheres with equal mass. How do we .nd the next con.guration? We need to solve for 
the con.guration that minimizes the gravitational energy under the linear constraints of Equations 1 
and 2. This is linear programming. We simply pass this problem to a commercial linear programming package, 
CPLEX.5 Theorem 3.4 implies that the hourglass algorithm cannot stick unless the actually physical system 
would also. Hence, the spheres .ow down the hourglass without clogging in the middle. 4.3 Box Constraints 
We also bound each sphere to lie in a rotated cube centered at the current position of the sphere. The 
cube has width two times the sphere radius, and it is oriented to have a vertex at minimum z­coordinate. 
This constraint serves to put an upper bound on the maximum distance a sphere can move in any one step. 
This extra constraint serves two purposes. First, it keeps the spheres from falling too fast. In the 
absence of this constraint, a solitary sphere could fall to rest on the ground from an arbitrary height 
in a single step. Second, by limiting the motion of the spheres, we limit the pairs of spheres which 
can collide in the cur­rent step. That permits us to reduce the size of the linear program. We do not 
add a pairwise constraint for two spheres that are too far apart to collide in the next step. We can 
use bucketing to detect nearby pairs of spheres in nearly linear time. 4.4 Acceleration and Conservative 
Forces The reader will notice that the spheres do not accelerate as they fall. We could .x this by detecting 
if a sphere has fallen the maximum amount, and if so, increasing the height of its bounding box by a 
.xed amount. To make this work properly, we must use unrotated boxes, unlike the rotated cubes of the 
previous section. The reader will also notice that the spheres roll to the back of the hourglass. Using 
unrotated boxes makes this effect worse: the spheres fall to the lower-left-rear corner of the box, making 
them fall at an angle. This is an artifact of the simplex method used to solve the linear program. We 
can add a conservative energy term that penalizes each sphere for changing any of its coordinates. In 
particular, we express each variable xias xi =x + i -x . i,where both x + iand x . iare constrained to 
be positive (xicould represent the x, y,or zcoordinate of a sphere). To the objective function, we add 
X + . cconserve (x+x); ii i where cconserveis small compared to the gravitational constant . In our system, 
the gravitational constant is 1 and cconserve =0:001. The second hourglass video illustrates the addition 
of acceleration and conservative forces . 5Version 3.0. CPLEX Optimization Inc. Suite 279. 930 Tahoe 
Boule­vard, Building 802. Incline Village, Nevada 89451-9436. 4.5 Polyhedron-Polyhedron Constraints 
The hourglass example does not require polyhedron-polyhedron constraints. For the record, we describe 
how one could add them to the model. Li s method for constructing a convex subset of the free space, 
the locality heuristic, requires that the interacting polygons be star­shaped. If they are not, they 
must be decomposed into star-shaped components. Extra constraints must be added to ensure that the components 
move as one object. We describe here a method for selecting a convex free region Rijfor a pair of polyhedra 
Piand Pjunder translation. As in the case of modeling spheres, we can construct these regions for each 
pair of polyhedra instead of having to work in R3nas implied by the algorithm of Section 3.2.2. Note 
that this method does not require that Piand Pjbe star-shaped, which is an improvement of Li s result. 
Let Pirepresent the resting position of Pi,and let Pi+pirepresent Pitranslated by pifrom its resting 
position. As stated in Section 3.3.1 Pi+piand Pj+pjdo not overlap if and only if pj-pilies in F=Pit-Pj. 
This is all following theory developed by Li to handle the two-dimensional case. cur curcur For a given 
pairwise con.guration cij =hpi;pji, we con­struct convex region Rijas follows. Set h=1and let qhbe the 
cur cur point on the boundary of Fwhich is closest to pj -pi.De.ne cur cur cur ji p-p-qh w= cur cur hijjpj 
-pi -qhj cur cur to be the unit vector from qhto to pj -pi. Add the constraint, cur (pj-pi -qh).w0: (3) 
hij Throw away all points qof the boundary of Fthat do not satisfy cur (q-qh).whij0: Increment hand repeat 
until no points of the boundary of Fremain. The set of constraints given by all instances of Equation 
3 de.ne a convex region Rijwhich is a subset of the free space for these two polyhedra. This region is 
the projection of I(ci;F)of Section 3.2.2 into the con.guration space of these two polyhedra (actually, 
it can be a superset). For this reason, it is suf.cient to use these pairwise constraints.  5 Non-gravitational 
Potential Functions For some applications, it may be necessary to simulate a constant force, a spring 
force, or an inverse-square law force between spheres. In this section, we describe how this can be done, 
and give some examples. 5.1 Attraction between Spheres. To de.ne an attractive force between spheres, 
we must .rst de.ne the distance between spheres in a way that can be represented in a linear program. 
In the following, 1i<jn,where nis the number of spheres. Let Siand Sjbe spheres which are to attract 
each other. Create a new variable dijwhich represents an approximation to the distance from pito pj.The 
value of dijwill always be a lower bound on the Euclidean distance. Select a set Uof unit vectors. The 
cur set Ushould at least include uij(the unit vector from pito pj) and the six axis-parallel vectors 
(±1;0;0), (0;±1;0), (0;0;±1). Apply the following constraints on pi, pj,and dij: u.(pj-pi)dij;for u2U: 
(4) Adding more vectors to Umakes dija better approximation to the Euclidean distance jpj-pij. However, 
the given Uis suf.cient for cur realistic motion, and the presence of uijensures correct conver­gence. 
For a constant force fijof attraction between Siand Sj(inde­pendent of distance), we can add the term 
fijdijto the potential function for the model. Often, however, one desires a force which dies off with 
distance, such as the inverse-square law. The corre­sponding potential function -fij/dijis nonlinear. 
In this case, we use a linear approximation, dcur 1dij-ij Eapprox(dij)=fij(-+): dcur (dcur)2 ij ij For 
any convex potential function, such as the inverse-square law, the linear approximation is an upper bound 
on the actual potential energy. The con.guration to which the system jumps will there­fore have lower 
energy than expected, and thus the system will con­verge even if it uses this approximation. 5.2 Spring 
Force It is possible to model forces which increase with distance such as a spring force. In this case, 
the potential function is E(dij)=fijd2 ij. This type of function is concave (upwards), and thus the method 
in the previous paragraph does not work. To solve such a model using linear programming, we replace the 
function by a piecewise linear approximation. First, de.ne lvariables 0dij1;dij2;:::;dijl 1and add the 
constraint dij=dij1+dij2+...+dijl.The piecewise linear approximation to the energy function is l X Eapprox(dij)=fij(2k-1)dijk: 
k=1 For kdij<k+1, this energy is minimized when dij1 = dij2 =...=dijk=1and dij(k+1)=dij-k. The value 
of the approximate function is fij(k2+(2k+1)(dij-k))which is a good approximation to fijd2 ij. 5.3 Examples. 
The video illustrates the application of the constant force, inverse­square law force, and spring force. 
In the trampoline example, the border of the 30 by 30 grid of spheres is .xed. Each internal grid sphere 
is attracted to its four immediate neighbors under the constant force. The large sphere falls into this 
grid and comes to rest. We fake the bouncing by playing the sequence forwards and backwards. 5.4 Other 
Possible Applications. The examples we have implemented only scratch the surface of what one could do 
with the current formulation. For example, we could apply these new potential functions to two animations: 
a pearl falling in shampoo and a lava lamp . The shampoo or lamp .uid is a grid-like gas of spheres. 
A constant force at­tracts sphere Sito a .xed grid point gi. The algorithm for modeling attraction of 
a moving point pito a .xed point qiis straightforward from the math given above. The pearl is a single 
sphere in a gravitational potential falling through a shampoo. The lava lamp .uid uses the same model. 
We also add a rising blob of lava .uid subject to an upwards gravita­tional .eld. The beads in the blob 
.uid are subject to a mutually attractive force. For this we choose a potential function which rises 
linearly to a particular value and then stops increasing. This po­tential corresponds to a constant, 
short-range force. This potential function is convex, and thus linear programming can minimize it.  
6 Implementation and Results All examples in the video are a simple mapping of optimization steps to 
frames. We believe that these demonstrate a variety of re­alistic looking motions. If necessary, we could 
modulate the ve­locity by interpolating between frames. Since consecutive frames 0 correspond to motion 
from cto cin the same convex region, every interpolated con.guration would also be valid. 6.1 Running 
Times and Scalability As usual, our program is always in .ux, and it is dif.cult to gener­ate meaningful 
timings. The running time depends greatly on the settings of the parameters to the linear programming 
library. Cur­rently, we use the simplex method. We .nd that reaching 80-90% of optimum requires only 
about one-quarter the time as reaching the optimum objective value. Hence, each step can be accomplished 
in one-quarter the time using only 1/0:8=1:25times as many steps. This is clearly a good trade-off. At 
present, we run each linear pro­gram for 5000 simplex steps, and this always brings us to within 80% 
of optimal. Using these particular methods, we can compute each frame of the second hourglass video in 
about 1.1 minutes on a DEC Al­pha 3000/700 (tm). The entire video can be generated in about 19 hours. 
We ran the program with only 500 spheres, and each frame was computed 2.4 times faster than with 1000 
spheres. Some of the cost is a naive O(n 2)geometric algorithm, and the cost of this can be improved. 
In essence, the cost per step appears to be roughly linear in the number of spheres. Of course, about 
half as many steps are required to simulate the hourglass. Therefore, the cost appears to be roughly 
quadratic in the number of spheres. As we discuss in Section 7, this is not necessarily the last work. 
It is unlikely that distant spheres interact over small numbers of steps. Therefore it should be possible 
to decompose the problem and/or apply multi­scale methods. 6.2 Number of Iterations If we are simply 
generating a pile or clump , we usually do not have an obstruction like the neck of the hourglass through 
which the spheres must pass more or less sequentially. The following ta­ble gives the number of iterations 
for a kby kby kgrid of spheres falling to rest in a box. The number iterations rises surprisingly slowly. 
This compares very favorably with acceleration or velocity­based methods. For these methods time until 
next collision ap­pears to be inversely proportional to the number of spheres and the number of steps 
to be at least linear in the number of spheres. Number of Spheres 27 64 125 216 Number of Iterations 
21 25 39 54 6.3 Comparison with Velocity-Based Methods We attempted to simulate the hourglass with a 
velocity-based method. As expected, collisions caused the time steps to be very small (around 10.5). 
It takes about 150 steps to reduce the energy by one unit. For this example, it would require 2 or 3 
million steps to reach minimum energy. Each step is faster, but only by a factor of two. We estimate 
it would take about 1000 times as long to reach an energy minimum than it does using the position-based 
method of this paper.  7 Conclusion and Future Work Position-based physics and the linear programming 
algorithms we use to simulate it are very good ways to rapidly .nd local energy minima for many interacting 
objects. They are much faster than other physical simulation techniques, and they are certainly useful 
for CAD/CAM applications for which only the .nal con.guration matters. The current techniques do not 
allow rotation in three di­mensions (a moot issue for spheres but not for moving polyhedra), but Li [19] 
has found ways to allow rotation in two dimensions, and it may be possible to generalize this work to 
three dimensions or de­vise other methods. The algorithms presented here do not simulate true physical 
motion: 1) the physics is only semi-Newtonian, and 2) the algorithms use a number of approximations to 
allow us to apply linear programming. However, in graphics appearance and speed are really all that matters, 
and these methods rapidly generate mo­tions which appear realistic. Since no other method can currently 
generate such motions with so little computation, position-based physics and linear programming based 
simulations warrant consid­eration as useful tools of computer graphics. Even for 1000 spheres in a polyhedron, 
the simulation time is faster than the rendering time. For even larger number of spheres, one would have 
to break the set of spheres into zones and sim­ulate within each zone. By switching between overlapping 
zones, one could still generate a good animation. Multi-scale techniques might also be applicable [20]. 
The issue of non-convex sets of valid directions arises for sets of translating polyhedra. However, we 
believe that this potential dif.­culty will not have a practical impact. A dif.cult practical problem 
is that of explicitly computing con.guration spaces (Minkowski sums) for pairs of translating non-convex 
polyhedra. Dealing with rotations will be even more dif.cult. However, we believe these dif.culties can 
be overcome. Position-based physics may one day simulate many highly interacting, translating and rotating 
polyhe­dra, with links and attractive and repulsive forces. Another direction of future work is to handle 
the transition from crowding to freedom. Position-based physics does not do a good job on freely moving 
objects. Section 4.4 describes how to make the falling spheres appear to accelerate. We use another trick 
to make the ball appear to bounce in the trampoline video. A more gen­eral solution would somehow switch 
between the position-based method and an acceleration/velocity-based method in a way that is low-cost 
yet realistic. Acknowledgements: Thanks to Kevin Duffy for implementing many of the algorithms as a summer 
job. Thanks to Lisa Ventry Milenkovic for proofreading and literature search. Thanks to Karen Daniels 
for proofreading and suggestions.  References [1] D. Baraff. Analytical methods for dynamic simulation 
of non­penetrating rigid bodies. Computer Graphics (Proceedings of SIGGRAPH), 23(3):223 232, 1989. [2] 
David Baraff. Curved surfaces and coherence for non­penetrating rigid body simulation. In Forest Baskett, 
edi­tor, Computer Graphics (SIGGRAPH 90 Proceedings),vol­ume 24, pages 19 28, August 1990. [3] David 
Baraff. Coping with friction for non-penetrating rigid body simulation. In Thomas W. Sederberg, editor, 
Computer Graphics (SIGGRAPH 91 Proceedings), volume 25, pages 31 40, July 1991. [4] David Baraff. Issues 
in computing contact forces for nonpen­etrating rigid bodies. Algorithmica, 10(2 4):292 352, 1993. [5] 
David Baraff. Fast contact force computation for nonpenetrat­ing rigid bodies. In Andrew Glassner, editor, 
Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24-29, 1994),Com­puterGraphics Proceedings,AnnualConferenceSeries,pages 
23 34. ACM SIGGRAPH, ACM Press, July 1994. [6] David Baraff. Interactive simulation of solid rigid bodies. 
IEEE Computer Graphics and Applications, 15(3):63 75, May 1995. [7] David Baraff and Andrew Witkin. Dynamic 
simulation of non-penetrating .exible bodies. In Edwin E. Catmull, editor, Computer Graphics (SIGRAPH 
92 Proceedings), volume 26, pages 303 308, July 1992. [8] R. Barzel and A. H. Barr. A modeling system 
based on dy­namics constraints. Computer Graphics (Proceedings of SIG-GRAPH), 22(4):179 187, 1988. [9] 
Jim X. Chen and Niels Da Vitoria Lobo. Toward interactive­rate simulation of .uids with moving obstacles 
using navier­stokes equations. Graphical Models and Image Processing, 57(2):107 116, March 1995. [10] 
N. Chiba, S. Sanakanishi, K. Yokoyama, I. Ootawara, K. Mu­raoka, and N. Saito. Visual simulation of water 
currents using a particle-based behavioural model. Journal of Visualization and Computer Animation, 6(3):155 
172, July 1995. [11] D. Dobkin, J. Hershberger, D. Kirkpatrik, and S. Suri. Implic­itly searching convolutions 
and computing depth of collision. In Proceedings of the Second SIGAL, pages 165 180, 1990. [12] Jeff 
Goldsmith and Alan H. Barr. Applying constrained opti­mization to computer graphics. SMPTE Journal -Society 
of Motion Picture and Television Engineers, 102(10):910 912, October 1993. [13] L. Guibas, L. Ramshaw, 
and J. Stol.. A Kinetic Framework for Computational Geometry. In Proceedings of the 24th An­nual IEEE 
Symposium on Foundations of Computer Science, pages 100 111. IEEE, 1983. [14] James K. Hahn. Realistic 
animation of rigid bodies. In John Dill, editor, Computer Graphics (SIGGRAPH 88 Proceed­ings), volume 
22, pages 299 308, August 1988. [15] Mikako Harada, Andrew Witkin, and David Baraff. Inter­active physically-based 
manipulation of discrete/continuous models. In Robert Cook, editor, Proceedings of SIGGRAPH 95, Computer 
Graphics Proceedings, Annual Conference Se­ries, pages 199 208. ACM SIGGRAPH, ACM Press, August 1995. 
[16] Vincent Hayward, Stephane Aubry, Andre Foisy, and Yas­mine Ghallab. Ef.cient collision prediction 
among many moving objects. The International Journal of Robotics Re­search, 14(2):129 143, April 1995. 
[17] Michael Kass and Gavin Miller. Rapid, stable .uid dynamics for computer graphics. In Forest Baskett, 
editor, Computer Graphics (SIGGRAPH 90 Proceedings), volume 24, pages 49 57, August 1990. [18] A. Kaul, 
M.A. O Connor, and V. Srinivasan. Computing Minkowski Sums of Regular Polygons. In Thomas Sher­mer, editor, 
Proceedingsofthe ThirdCanadianConferenceon Computational Geometry, pages 74 77, Vancouver, British Columbia, 
1991. Simon Frasier University. [19] Z. Li. Compaction Algorithms for Non-Convex Polygons and Their Applications. 
PhD thesis, Harvard University, Division of Applied Sciences, 1994. [20] A. Luciani, A. Habibi, and E. 
Manzotti. A multi scale physi­cal model of granular materials. In Proceedings of Graphics Interface 95, 
pages 136 145, May 1995. [21] H. Mallinder. The modelling of large waterfalls using string texture. Journal 
of Visualization and Computer Animation, 6(1):3 10, January 1995. [22] V. Milenkovic, K. Daniels, and 
Z. Li. Placement and Com­paction of Nonconvex Polygons for Clothing Manufacture. In Proceedings of the 
Fourth Canadian Conference on Compu­tational Geometry, pages 236 243, St. Johns, Newfoundland, August 
1992. Department of Computer Science, Memorial University of Newfoundland. [23] V. J. Milenkovic and 
Z. Li. A Compaction Algorithm for Non­convex Polygons and Its Application. European Journal of Operations 
Research, 84:539 560, 1995. [24] H. Minkowski. Volumen und Ober.¨ache. Mathematische An­nalen, 57:447 
495, 1903. [25] M. Moore and J. Wilhelms. Collision detection and response for computer animation. Computer 
Graphics (Proceedings of SIGGRAPH), 22(4):289 298, 1988. [26] J. C. Platt and A. H. Barr. Constraint 
methods for .exible models. Computer Graphics (Proceedings of SIGGRAPH), 22(4):279 287, 1988. [27] J. 
Serra. Image Analysis and Mathematical Morphology,vol­ume 1. Academic Press, New York, 1982. [28] J. 
Serra, editor. Image Analysis and Mathematical Morphol­ogy, volume 2: Theoretical Advances. Academic 
Press, New York, 1988. [29] Mikio Shinya and Marie-Claire Forgue. Layout out objects with geometric and 
pysical constraints. Visual Computer, 11:188 201, August 1995. [30] John Snyder. An interactive tool 
for placing curved surfaces with interpenetration. In Robert Cook, editor, Proceedings of SIGGRAPH 95, 
Computer Graphics Proceedings, Annual Conference Series, pages 209 218. ACM SIGGRAPH, ACM Press, August 
1995. [31] Richard Szeliski and David Tonnesen. Surface modeling with oriented particle systems. In Edwin 
E. Catmull, editor, Com­puter Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 185 194, July 1992. 
[32] Demitri Terzopoulos, John Platt, and Kurt Fleischer. Heat­ing and melting deformable models (from 
goop to glop). In Proceedings of Graphics Interface 89, pages 219 226, June 1989. [33] Andrew P. Witkin 
and Paul S. Heckbert. Using particles to sample and control implicit surfaces. In Andrew Glassner, editor, 
Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994), Computer Graphics Proceedings, Annual 
Conference Series, pages 269 278. ACM SIGGRAPH, ACM Press, July 1994. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237226</article_id>
		<sort_key>137</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Linear-time dynamics using Lagrange multipliers]]></title>
		<page_from>137</page_from>
		<page_to>146</page_to>
		<doi_number>10.1145/237170.237226</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237226</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computations on matrices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor>Sparse, structured, and very large systems (direct and iterative methods)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010148.10010149.10010158</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms->Linear algebra algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Languages</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39038146</person_id>
				<author_profile_id><![CDATA[81100334025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baraff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robotics Institute, Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Baraff. Issues in computing contact forces for nonpenetrating rigid bodies. Algorithmica, 10:292-352, 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192168</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. Baraff. Fast contact force computation for nonpenetrating rigid bodies. Computer Graphics (Proc. SIGGRAPH), 28:23- 34, 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378509</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. Barzel and A.H. Barr. A modeling system based on dynamic constraints. In Computer Graphics (Proc. SIGGRAPH), volume 22, pages 179-188. ACM, July 1988.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Baumgarte. Stabilization of constraints and integrals of motion in dynamical systems. Computer Methods in Applied Mechanics, pages 1-36, 1972.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[K.E. Brenan, S.L. Campbell, and L.R. Petzold. Numerical Solution of Initial-value Problems in Differential-algebraic Equations. North-Holland, 1989.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>18753</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[I.S. Duff, A.M. Erisman, and J.K. Reid. Direct Methods for Sparse Matrices. Clarendon Press, 1986.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>576516</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[R. Featherstone. Robot Dynamics Algorithms. Kluwer, 1987.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>922879</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Gleicher. A Differential Approach to Graphical Manipulation. PhD thesis, Carnegie Mellon University, 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[G. Golub and C. Van Loan. Matrix Computations. John Hopkins University Press, 1983.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[C. Lanczos. The Variational Principles of Mechanics. Dover Publications, Inc., 1970.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[R.H. Lathrop. Constrained (closed-loop) robot simulation by local constraint propagation. In International Conference on Robotics and Automation, pages 689-694. IEEE, 1986.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. Negrut, R. Serban, and F.A. Potra. A topology based approach for exploiting the sparsity of multibody dynamics. Technical Report 84, Department of Mathematics, University of Iowa, December 1995.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91403</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[E Schr6der and D. Zeltzer. The virtual erector set: Dynamic simulation with linear recursive constraint propagation. In Proceedings 1990 Symposium on Interactive 3d Graphics, volume 24, pages 23-31, March 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[A. Shabana. Dynamics of Multibody Systems. Wiley, 1989.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134065</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[M.C. Surles. An algorithm with linear complexity for interactive, physically-based modeling of large proteins. Computer Graphics (Proc. SIGGRAPH), 26:221-230, 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A.F. Vereshchagin. Computer simulation of the dynamics of complicated mechansisms of robot manipulators. Engineering Cybernetics, 6:65-70, 1974.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91400</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A. Witkin, M. Gleicher, and W. Welch. Interactive dynamics. In Proceedings 1990 Symposium on Interactive 3d Graphics, volume 24, pages 11-21, March 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237229</article_id>
		<sort_key>147</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Efficient generation of motion transitions using spacetime constraints]]></title>
		<page_from>147</page_from>
		<page_to>154</page_to>
		<doi_number>10.1145/237170.237229</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237229</url>
		<keywords>
			<kw><![CDATA[computer animation]]></kw>
			<kw><![CDATA[cyclification]]></kw>
			<kw><![CDATA[human figure animation]]></kw>
			<kw><![CDATA[inverse kinematics]]></kw>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motion control]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor>Constrained optimization</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39078006</person_id>
				<author_profile_id><![CDATA[81332524180]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research and Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P32646</person_id>
				<author_profile_id><![CDATA[81100130209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guenter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15034387</person_id>
				<author_profile_id><![CDATA[81100500707]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bobby]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bodenheimer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15024219</person_id>
				<author_profile_id><![CDATA[81406592138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>574731</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BALAFOUTIS, C. A., AND PATEL, R. V. Dynamic Analysis of Robot Manipulators: A Cartesian Tensor Approach. Kluwer Academic Publishers, 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND WILLIAMS, L. Motion signal processing. In Computer Graphics (Aug. 1995), pp. 97-104. Proceedings of SIGGRAPH 95.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BURDETT, R. G., SKRINAR, G. S., ANDSIMON, S.R. Comparison of mechanical work and metabolic energy consumption during normal gait. Journal of Orhopaedic Research 1, 1 (1983), 63-72.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COHEN, M. F. Interactive spacetime control for animation. In Computer Graphics (July 1992), pp. 293-302. Proceedings of SIGGRAPH 92.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[GILL, P. E., MURRAY, W., AND WRIGHT, M. H. Practical Optimization. Academic Press, 1981.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[HOLLERBACH, J. M. A recursive lagrangian formulation of manipulator dynamics and a comparative study of dynamics formulation complexity. IEEE Transactions on Systems, Man, and Cybernetics SMC-IO, 11 (Nov. 1980).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[LIU, Z., AND COHEN, M. F. An efficient symbolic interface to constraint based animation systems. In Proceedings of the 5th EuroGraphics Workshop on Animation and Simulation (Sept. 1994).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192169</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[LIU, Z., GORTLER, S. J., AND COHEN, M. F. Hierarchical spacetime control. In Computer Graphics (July 1994), pp. 35- 42. Proceedings of SIGGRAPH 94.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[UNUMA, M., ANJYO, K., AND TEKEUCHI, R. Fourier principles for emotion-based human figure animation. In Computer Graphics (Aug. 1995), pp. 91-96. Proceedings of SIG- GRAPH 95.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND KASS, M. Spacetime constraints. In Computer Graphics (Aug. 1988), pp. 159-168. Proceedings of SIGGRAPH 88.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND POPOVfC, Z. Motion warping. In Computer Graphics (Aug. 1995), pp. 105-108. Proceedings of SIGGRAPH 95.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>195827</ref_obj_id>
				<ref_obj_pid>195826</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[ZHAO, J., AND BADLER, N. I. Inverse kinematics positioning using non-linear programming for highly articulated figures. ACM Transactions on Graphics 13, 4 (Oct. 1994), 313- 336.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ef.cient Generation of Motion Transitions using Spacetime Constraints Charles Rose Brian Guenter Bobby 
Bodenheimer Michael F. Cohen Princeton University Microsoft Research Microsoft Research Microsoft Research 
(currently at Microsoft Research) Abstract This paper describes the application of space time constraints 
to cre­ating transitions between segments of human body motion. The motion transition generation uses 
a combination of spacetime con­straints and inverse kinematic constraints to generate seamless and dynamically 
plausible transitions between motion segments. We use a fast recursive dynamics formulation which makes 
it possible to use spacetime constraints on systems with many degrees of free­dom, such as human .gures. 
The system uses an interpreter of a motion expression language to allow the user to manipulate motion 
data, break it into pieces, and reassemble it into new, more complex, motions. We have successfully used 
the system to create basis mo­tions, cyclic data, and seamless motion transitions on a human body model 
with 44 degrees of freedom. Additional Keywords and Phrases: computer animation, inverse kinematics, 
motion capture, motion control, human .gure anima­tion, cycli.cation. CR Categories and SubjectDescriptions: 
I.3.7 [Computer Graph­ics]: Three Dimensional Graphics and Realism: Animation; I.6.3 [Simulation and 
Modeling]: Applications; G.1.6 [Constrained Op­timization]; I.3.5 [Physically-Based Modeling]. 1 Introduction 
Existing 3D animation tools primarily provide support for creating a single linear stream of animation 
where the entire motion is planned in advance and computed off-line. Interactive 3D character anim­ation, 
however, is characterized by a degree of uncertainty that is not present in animation for .lm or television. 
Characters are un­der the control of the user and must be able to change the way they move at any time. 
Crafting animation for an interactive application presents a new set of problems and requires a different 
set of spe­cialized tools. One solution for these problems is to generate a set of high qual­ity motions, 
called basis motions in the remainder of the paper, and then create transitions between these motions 
so they can be strung together into animations of unlimited length and great variety. Basis Email: ft-chuckr, 
briangu, bobbyb, mcoheng@microsoft.com Permission to make digital or hard copies of part or all of this 
work or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 motions are typically 
short and can be combined into motions rep­resentative of the type of actions that the body has to perform. 
For example, basis motions might be walk cycles, arm waves, karate kicks, and so forth. Basis motions 
need not specify all the degrees of freedom in the body; they can specify just those of one limb or even 
a part of a limb. Using motion capture techniques, it is relat­ively easy to make high quality basis 
motions, but generating high­quality transitions among those basis motions is still dif.cult and involves 
signi.cant manual labor. The techniques presented work well with motion capture data, but would work 
equally well with hand-animated basis motions. We have developed an algorithm for generating these transitions 
semi-automatically, greatly reducing the time spent and the number of parameters an animator must specify. 
The system provides two semi-automatic mechanisms for generating motion: motion trans­ition generation 
and cycli.cation. Motion transition generation uses a combination of spacetime constraints [10] and inverse 
kinematic constraints [12] to generate transitions between basis motions. A fast dynamics formulation 
makes it practical to use spacetime trans­ition generation on high degree of freedom systems. With this 
dy­namics formulation, the algorithm achieves the lower bound time complexity for spacetime algorithms 
that use gradient based optim­ization techniques. The motion transitions satisfy both dynamic and kinematic 
con­straints. This differs from the work described in [2, 11, 9]. These papers described various mechanisms, 
such as dynamic time warp­ing, Fourier interpolation, and multi-resolution signal processing, for transforming 
existing motion data. Transitions between motion clips were achieved using linear combinations of the 
two motions being transitioned. This can result in motion which does not have realistic dynamic qualities 
and which may not necessarily satisfy kinematic or anthropomorphic constraints. The transition mechan­ism 
described here generates motion which minimizes the torque re­quired to transition from one motion to 
another while maintaining joint angle constraints. Inverse kinematics are used to ensure that kinematic 
constraints are satis.ed. Additionally, we have de.ned a motion expression language to allow the user 
to manipulate motion data, break it into pieces, and reassemble it into new, more complex motions. We 
have success­fully used the system to create basis motions, cyclic data, and mo­tion transitions on our 
human body model which has 44 degrees of freedom. Section 2 of the paper describes the human body model 
and how motion capture data is processed before it gets into the system. Section 3 describes the semi-automatic 
spacetime and inverse kin­ematic transition mechanism. This section also explains the fast dynamic formulation 
which allows spacetime constraint optimiza­tion to run quickly on systems with many degrees of freedom. 
Sec­tion 3 also describes our method of motion cycli.cation. Section 4 explains how body motion is internally 
represented and describes the motion expression language. Section 5 presents results of trans­itions 
generated with the system. Section 6 concludes the paper. Appendix A contains an explanation of the notation 
used in the dy­namics formulation, the dynamics equations, and their partial deriv­atives as they are 
used in the spacetime optimization.  2 Human Body Model Before motion data can be edited by the system 
it usually needs to be preprocessed to remove degrees of freedom which are not actu­ally present in humans. 
Most motion capture data, for example, has three rotational degrees of freedom at each joint. A handful 
of hu­man joints actually have three degrees of freedom but most have one or two. Anatomically extraneous 
degrees of freedom cause trouble when generating motion transitions since the synthetic body may move 
in ways that are impossible for a real human. We use an optim­ization procedure [12] which minimizes 
the angular and positional deviation between our internal model of a human and the motion capture data. 
Limb lengths are automatically extracted from the mo­tion data and used to scale the internal model appropriately. 
Our hu­man body model has 38 joint degrees of freedom and six degrees of freedom at the root, located 
between the hips, for positioning and orienting the entire body. As with most other animation work on 
human body models we assume that human joints can be accurately modeled as revolute joints. While this 
is not precisely the case, es­pecially for the knee and the shoulder, for most joints the errors in­troduced 
by making this assumption are typically small. Figure 1: Human body model illustrating degrees of freedom. 
 3 Semi-Automatic Motion Generation Two primary semi-automatic motion generation capabilities are provided. 
The .rst is motion transition generation and the second is motion cycli.cation. Motion transition generation 
creates new mo­tion to span unde.ned regions between two basis motions. Motion cycli.cation transforms 
a motion which may not be perfectly cyclic into one which is. 3.1 Function Representation Representations 
used in the past for the joint angle function, q(t)(q1(t);:::;qn(t)) where qi(t)is the angle of joint 
iat time t, include piecewise con­stant [10], B-splines [4], and B-spline wavelets [8]. B-spline wave­lets 
show good convergence properties for spacetime optimization when the number of basis functions in a single 
degree of freedom is large, e.g., more than 20 or 30. Since the transitions we are generat­ing are generally 
short, on the order of 1 second or less, good paths can be represented with 5 to 10 B-spline coef.cients. 
Our experi­ence has been that very few iterations are required to achieve con­vergence with a B-spline 
basis, so the extra complexity and com­putation of the B-spline wavelet basis was not justi.ed. For these 
reasons we use cubic B-splines as the basis functions for q(t). 3.2 Motion Transitions Motion transitions 
are generated using a combination of kinemat­ics and dynamics. The motion of the root of the body is 
determined kinematically while the motion of all the limbs which are not sup­porting the body is determined 
using spacetime constraints. Limbs which support the body during the transition are controlled using 
an optimization procedure to solve the inverse kinematics problem over the entire transition time interval 
instead of just at one point in time. A support limb is de.ned as the kinematic chain from the sup­port 
point (e.g., a foot on the .oor) back up the kinematic tree to the root. 3.2.1 Root Motion The x-zplane 
is de.ned to be coincident with the .oor with the y axis pointing upward. The xand zcomponents of the 
root position are interpolated based on either the velocities or accelerations avail­able at the beginning 
and end of the transition. The ycomponent of the root translation is linearly interpolated from the end 
of the .rst motion (at time t1) to the beginning of the second motion (at time t2). The root position 
in the x-zplane, p(t), during the transition time is Z   tn..o . t1 . t1 p(t)p(t1)+ v1 1 +v2 d. t2 
t1 t2 t1 t 1 where v1and v2are the vector velocities in the x-zplane of the root at time t1and t2. This 
expression can be easily evaluated analytic­ally and provides a C1path for the root. A C2path is more 
desirable and could be achieved by double integration of the accelerations of the root. However due to 
limitations in the motion capture process estimates of the acceleration are poor especially at the beginning 
and end of a motion capture data stream. 3.2.2 Inverse Kinematics Support limbs are controlled kinematically. 
The system attempts to locate support points automatically by .nding coordinate frames with motion that 
remains within a small bounding box over an ex­tended period of time. The animator has the option of 
overriding the system s guess, manually specifying a joint coordinate frame as being a support frame. 
During the transition this coordinate frame will be held .xed using inverse kinematics constraints. Enforcing 
kinematic constraints is done using an extension of the techniques presented in [12], optimizing for 
coef.cients in.uencing a range of time. The inverse kinematics constraint is enforced by minimizing the 
deviation rk(t)kpk(t) p^ k(t)k 2 of the constrained joint coordinate frame kfrom its desired position, 
where pkis the actual position of the constrained coordinate frame and p^kis the desired position. The 
total error, R, is given by the sum over all Kconstrained frames and integrated across the constrained 
time interval ZK t 2 X R rk(t)dt t 1k 1 Since a .nite and usually small number of B-spline coef.cients 
are used to represent the motion curves, minimizing this objective does not result in rapid oscillations. 
These can occur when inverse kin­ematic constraints are maintained independently at each frame time. 
Ris a function of the joint angles in the body Rf(q1(t);:::;qn(t)) which are themselves functions of 
the B-spline control points de.n­ing each joint angle function qi(t)g(bi;1;:::;bi;m) where the bi;jare 
the control points of the B-spline curve for joint angle function qi(t). We minimize Rusing the BFGS 
optimization algorithm described in more detail in Section 3.2.3. For the pur­poses of the present discussion 
the only relevant part of the BFGS algorithm is that it requires the gradient of Rat each iteration in 
the optimization @R @R rR;"""; @bi;1 @bn;m ZK t X @R 2 @rk dt @bi;j @bi;j t 1k1 @rk @qi 2(pk(t)p^ k(t))(uixdki)@bi;j@bi;j 
where uiis the axis of rotation of joint iand dkiis the vector from joint ito theconstrainedframe k. 
Figure2showstheeffectusingthe inverse kinematic constraint to .x the feet of the body on the ground during 
a motion transition period. The left leg is constrained to be on the ground during the entire transition 
interval while the right leg is constrained to be on the ground only at the end of the interval. In the 
image on the left the inverse kinematic constraints were turned off; the left leg drifts from its desired 
position and the right leg fails to meet its desired position. In the image on the right the inverse 
kinematic constraints are satis.ed. The left leg remains .xed during the transition period and the right 
leg touches down at the end of the interval. Figure 2: Effect of inverse kinematics constraint on placement 
of feet. 3.2.3 Spacetime Dynamics Formulation The energy required for a human to move along a path is 
actually a complex non-linear function of the body motion since energy can be stored in muscle and tendon 
in one part of the motion and released later on. As shown in [3], joint torques are a reasonable predictor 
of metabolic energy, so minimizing torque over time should be a reas­onable approximation to minimizing 
metabolic energy. Experience has shown that motion that minimizes energy looks natural. This leads to 
the minimization problem: Z t 2 X minimize eTi2(t)dt: t 1 i We use the BFGS optimization algorithm [5] 
to .nd a minimum of this integral equation. BFGS belongs to the class of quasi-Newton algorithms which 
progress toward a solution by using the gradient of the objective function gre: The gradient is used 
to incrementally update a matrix decomposition ofa pseudo-Hessianmatrix, H, and to compute a new step 
direction dH 1 g: The relative amount of computation for each subtask required at every iteration of 
the algorithm is common to several quasi-Newton algorithms: gradient computation, pseudo-Hessian update, 
and computation of the step direction. Since each of the Tiis potentially a function of all the qi, q_i, 
qqithe gradient requires the evaluation of O(n 2)partial derivatives where nis the number of degrees 
of freedom of the body. This is in fact a lower bound for the asymptotic time complexity of space time 
al­gorithms which use gradient-based optimization techniques. If mis the number of B-spline coef.cients 
used to de.ne the time function of each degree of freedom then the pseudo-Hessian is of size nmby nm. 
Theupdateofthepseudo-Hessianandcomputation of the step direction are both O((nm)2).For msmall, less than 
20, and nlarge, more than 30, the time required to compute gdominates all other computation thus an ef.cient 
formulation for g will pay the greatest dividends in reducing computation. Computing grequires .nding 
the joint torques and a variety of subsidiary quantities, such as angular velocity and acceleration. 
This is the inverse dynamics problem which has been extensively studied in the robotics literature. See 
[1] for a good overview of many of these algorithms. Many inverse dynamics formulations have been proposed 
in the robotics literature ranging from O(n 4) non-recursive to O(n)recursive algorithms. The inverse 
dynam­ics formulation we use is due to Balafoutis [1]. This is an O(n) recursive formulation which requires 
96n77multiplications and 84n77additions to solve the inverse dynamics problem for a robot manipulator 
with njoints. This is faster than the O(n)Lagrangian recursive formulation developed by Hollerbach [6] 
and used in [7], which requires 412n277multiplications and 320n201addi­tions. The ef.ciency of the Balafoutis 
algorithm derives from the com­putational ef.ciency of Cartesian tensors and from the recursive nature 
of the computations. These ef.ciencies carry over to the computation of the gradient terms. The Balafoutis 
algorithm proceeds in two steps. In the .rst step velocities, accelerations, net torques, and forces 
at each joint are computed starting from the root node and working out to the tips of allthechainsinthetree. 
Inthesecondstepthejointtorquesarecom­puted starting from the tips of the chains back to the root node. 
See the appendix for details. These recursive equations can be differen­tiated directly to compute gor 
one can use Cartesian tensor identit­ies to compute the derivatives. Since the differentiation is tedious 
and somewhat involved we have included some of the partial deriv­atives of the recursive equations in 
the appendix as an aid to those attempting to reproduce our results. 3.3 Motion Cycli.cation If cyclic 
motions, such as walking and running, come from motion capture data they will not be precisely cyclic 
due to measurement errors and normal human variation in movement. The discontinu­ities in motion will 
likely be small enough that the full power of a spacetime transition will not be necessary in order to 
splice a motion back onto itself smoothly. In this case we use a much simpler and faster algorithm for 
generating seamless cycles.  The cycli.cation algorithm proceeds in two steps. First the user marks 
the approximate beginning and end of a cycle. We create two time regions Isand Ifcentered about the markers 
the user has chosen. The time regions are set to be one-.fth the length of the time between markers. 
The system then .nds one time point in each in­terval that minimizes the difference between position, 
velocity, and acceleration of the body: minkabk 2 t12Is;t22If where a q(t1);q_(t1);qq(t1))Tand b q(t2);q_(t2);qq(t2))T 
. For most motions there will still be a discontinuity at the time where the motion cycles. We distribute 
this discontinuity error over the entire time interval so that the end points of the cycle match ex­actly 
by adding a linear offset to the entire time interval. We then construct a C2motion curve by .tting a 
least squares cyclic B-spline approximation to the modi.ed motion. Figure 3: Results of cycli.cation 
on a walk 4 Motion Representation To minimize the complexity of working with motions which involve many 
degrees of freedom we have developed a .exible functional expression language, and an interactive interpreter 
for the language, for representing and manipulating motions. Using this language it is a simple matter 
to interactively type in or procedurally generate complex composite motions from simpler constituent 
motions. We include a small example to demonstrate the simplicity of using the language. Motions are 
represented as a hierarchy of motion expressions. Motion exressions can be one of three types of objects: 
intervals; degrees of freedom (DOF); and motion units (MU). These primit­ives are described in a pseudo-BNF 
notation below: interval ! (f1;:::;f n ;ts;tf )je DOF ! intervaljDOF;intervalje MU ! array1:: :n DOF) 
 An interval is a list of scalar functions of time plus a start time tsand a stop time tf. A DOF is a 
list of intervals. A DOF de.nes the value over time of one of the angular or translational degrees of 
freedom of a body. An MU is an array of DOFs which de.nes the value over time of some, but not necessarily 
all, of the degrees of freedom of the body. There are three kinds of operations de.ned on these primitives: 
set operations, function operations, and insert and delete operations. The set operations are intersection, 
unde.ne, and composition, denoted ^, ,and +respectively. They are de.ned on intervals as follows (without 
loss of generality assume t3t1): I1 (f1;:::;fn;t1;t2) I2 (g1;:::;gn;t3;t4) e t1 t2 I^I(f;:::;fn;g;:::;gn;t;t) 
t<t;t<t 1211323224 (f;:::;fn;g;:::;gn;t;t) t<t;t<t 11343242 (f;:::;fn;t;t)(g;:::;gn;t;t) t t 11211223 
I1 I2(f;:::;fn;t;t)(g;:::;gn;t;t) t<t;t<t 1131243224 (f;:::;fn;t;t)(f;:::;fn;t;t) t<t;t<t 1131423242 
I1+I2 I1 I2;I1^I2 where the comma denotes list concatenation. The intersection operation takes two intervals 
as arguments and returns an interval. The unde.ne operator takes as arguments two intervals I1and I2and 
returns a DOF containing two intervals A and B. A diagrammatic representation of this operation is shown 
in Figure 4. The effect of the unde.ne operator is to unde.ne any portions of I1which overlap with I2. 
 Figure 4: Interval unde.ne operation. The set addition operator takes as arguments two intervals I1and 
I2and returns a DOF containing two intervals, Aand B,ifthe in­tersection of I1and I2is empty, or three 
intervals, A, B,and C,if the intersection of I1and I2is not empty. A diagrammatic repres­entation of 
this operator is shown in Figure 5. Figure 5: Interval addition operation. The effect of this operator 
is to replace the region of I1that was removed by the set unde.ne operator with a new interval C.Set 
operations are useful for .nding all the time regions over which two intervals, DOFs, or MUs are de.ned 
or those time regions where they are multiply de.ned, i.e., where composed motions con.ict. The function 
operations perform functional composition on the elements of an interval: functions and times. For example, 
one of the functional operators is afne(ts;d;I). This operator scales and translatesthe timecomponentsofaninterval,andimplicitly 
the time value at which the functions in that interval are evaluated. Other functional operations we 
have implemented include clip(ts;tf;I) which clips out a portion of an interval and translates the beginning 
of the clip back to time zero, clip-in-place(ts;tf;I)which performs the same operation as clipexcept 
that it leaves the clip time un­changed, and concatenate(I1;I2)which puts the two intervals in time sequence. 
Both the set and function operations can be applied to any inter­val, DOF, or MU. The operations are 
pushed down to the level of in­tervals at which point the primitive interval operations are invoked. 
For example if we intersect MU1and MU2, .rst we intersect all the DOFs of MU1and MU2and then we intersect 
all the intervals in all the DOFs. Complex motions can be easily created by functional composi­tion of 
simple motions using the set and function operations de.ned above. Figure 6 shows an example of a spacetime 
transition from a walk arm motion to a wave arm motion and then back to a walk arm motion. The arm wave 
MU de.nes only the DOFs of the arm and the walk MU de.nes all the DOFs of the body. First we perform 
an af.ne transformation on the arm wave MU and unde.ne this from the walk MU. This will unde.ne the arm 
degrees of freedom of the walk MU during a time that is slightly longer than the arm wave MU. When we 
add a time shifted version of the arm wave MU to the res­ulting MU there will be two unde.ned regions 
surrounding it which the spacetime operator will .ll in with spacetime transition motion. The result 
will be a smooth transition from a walking arm motion to an arm wave motion and back to a walking arm 
motion. This oper­ation is shown in both a time line view and an operator tree view in Figure 6. Letting 
SPdenote the spacetime optimization, the algeb­raic representation of this motion is SP(afne2(wave)+(walkafne1(wave))): 
 Time line view Function composition view Figure 6: Spacetime composition of motions. 5 Results Figure 
7: End position of motion 1 and beginning position of motion 2 for a motion transition We have successfully 
applied the motion transition algorithm on many motions. For this example the transition time was set 
to .6, and the number of B-spline coef.cients to 5. The resulting trans­ition is shown in Figure 8. Our 
experience has been that successful transitions are quite short, usually in the range of .3 to .6 seconds. 
Without a biomechanical model to guide a large motion, our min­imal energy model will often prove insuf.cient. 
The beginning of the transition is colored blue and the end is colored red with intermediate times a 
linear blend of the two colors. This motion is one transition from a longer animation which has 5 transitions 
between 6 motions. Figure 8: Multiple time exposure of transition generated from the motions in Figure 
7 Figure 9 shows an example of a motion transition which affects onlythearmdegreesoffreedomofthemotion. 
Thissequenceactu­ally consists of two space time transitions: one from a walking arm motion to the salute 
motion and another back to the walking arm motion. Each transition motion is .3 seconds long. Figure 
9: Arm walk motion transitioning to salute motion and back to walk motion. Arm degrees of freedom affected 
by the transition are colored green. Computation times for transitions are strongly dependent on the 
number of degrees of freedom involved since the spacetime formu­lation we use is O(n 2)in the number 
of degrees of freedom. For the transition of Figure 8 generating the spacetime transition mo­tion took 
72 seconds. This transition involved 44 degrees of free­dom. For the transition of Figure 9 generating 
the spacetime trans­ition took 20 seconds. All timings were performed on a 100 MHz Pentium processor. 
Spacetime transitions are more costly to generate than joint angle interpolation techniques, but they 
often produce more realistic mo­tion. One type of motion that demonstrates this superiority is mo­tion 
that has identical joint space beginning and ending conditions on some of the degrees of freedom of the 
.gure. An example of this type of motion is shown in Figure 10. This motion begins with the forearm nearly 
vertical, held close to the shoulder with zero initial velocity. The motion ends with the forearm held 
horizontal also with zero velocity. Because the upper arm and the wrist have identical joint space starting 
and ending conditions any simple interpolation technique, which would include linear interpolation, polynomial 
in­terpolation, and most other types of interpolation which simply take a weighted sum of the two endpoint 
conditions will yield a motion such as that shown on the left in Figure 10. This is an unnatural mo­tion 
since there is no joint space motion at the shoulder or the wrist. The spacetime motion, however, has 
motion at every joint and looks much more like the kind of motion a person might make.  6 Conclusion 
This paper has presented a powerful animation system for manipu­lating motion data using a motion expression 
interpreter. Data can be positioned in space and time, and complete control of the degrees of freedom 
in the system allows motions to be spliced and mixed in arbitrary manners. The system is capable of generating 
seamless transitions between segments of animations using spacetime constraints and inverse kin­ematics. 
Using a new, fast, dynamics formulation we can apply spacetime constraints to systems having a large 
number of degrees of freedom and still have reasonable computation time. An addi­tional capability of 
the system is the generation of arbitrary length periodic motions, such as walking, by cyclifying segments 
of mo­tion data which are approximately periodic. The results of using our system to generate animations 
starting from a base library of soccer motions are quite good. Cycli.cation of a segment of such motions 
as a walk produces a quite realistic walk of arbitrary length. The spacetime constraint and inverse kin­ematic 
optimization produce transitions between diverse motions which are seamless and invisible to the viewer. 
While the optimiza­tion cannot be done in real time, it is relatively fast, and quite usable by an animator 
designing motions. We plan to extend our motion model to more accurately model the dynamics of the human 
body model. The current approximation used for computing root motion works reasonably well for the class 
of transitions we have worked with but is not accurate for free body motion. Acknowledgements The authors 
thank Jessica Hodgins, College of Computing, Georgia Institute of Technology, for providing inertia matrices 
for the human body model. The Microsoft Consumer Division generously shared motion capture data with 
us for use in this project.  A Appendix: Dynamics Details A.1 List of Symbols oiorigin of the i-th link 
coordinate frame. cicenter of mass of the i-th link. i !angular velocity of the i-th link. i i zjoint 
axis of the i-thlink expressed in the i-thcoordinate frame. i i svector from oito ojexpressed in the 
i-th coordinate frame. i;j i rvector from oito cjexpressed in the i-th coordinate frame. i;j Ai3x3 coordinate 
(or 4x4 homogeneous) transformation relating the i-th coordinate frame to the (i1)-th frame. Ikinertia 
tensor of the i-th link about ciexpressed in the k-th co­ ci ordinate frame. JkEuler s inertia tensor 
of the i-th frame about ciexpressed in ci the k-th coordinate frame. 0iangular acceleration tensor of 
the i-th link expressed in the i-th i coordinate frame. Fiforce vector acting on ciexpressed in the i-th 
coordinate ci frame. Mimoment vectorabout ciexpressedin the i-th coordinate frame. ci fiforce vector 
exerted on link i by link (i1). i 1imoment vector exerted on link i by link (i1). i Titorque at joint 
i. ggravity. mimass of the i-th link. In the above, the subscript indicates the coordinate frame being 
represented and superscript the coordinate frame in which it is rep­resented. We use +and on index variables 
to denote relative placement in the joint hierarchy. Thus, iis the predecessor of i which is the i+ i 
predecessor of i+. For example, in the equation !AT!+ i+ii+ i i+ zq_i+, the variable !iis the angular 
velocity in the coordinate i+ i+ frame which precedes the coordinate frame of !.In other words, i+ coordinate 
frame i is closer to the root coordinate frame than is frame i+. Note that there is no guarantee of a 
uniquely de.ned suc­cessor. T g0:0;9:80655;0:0) 1 i ii JtraceI1I ci cici 2 "# dual(v) ~v 0v3 v2 v3 0v1 
v2 v0 1 ual(~v)d v i+ i^i+  A.2 Forward Dynamics Equations !_ !_ ! i+Tiii+ i+ jA+ q_ z i+i+i+ qj qj 
qj Base conditions at the root of the creature: ^ 00 !zq_ 0 00 00 !_z 00q 0 0 T qs0;0 A0g Recursive 
forward dynamics equations: i+ Tii+ !i+ Ai+!i+zq_i+ i+i+ Tii+i+ i+ !_i+ Ai+_i+!izi+q_i++zi+q ! ~i+ i+ 
i+ i+i+ _ 0~i++ !!~ ! ~ i+ i+i+ i+ Ti ii qs0;i+ Ai+qs0;i+0isi;i+ i+ i+i+ i+ qr0r+qs 0;i+ i+i+;i+0;i+ 
i+ i+ Fmi+qr ci+0;i+ T i+ i+i+ i+i+ ~ M0J0i+ ci+ i+ci+ Jci+  A.3 Backward Recursive Equations (Torque 
Equa­tions) At a joint controlling an end-effector: fiFi ici iiii1i ~ri;iFci +Mci ii Ti1i "z i At an 
internal joint: X ii i+ fiFci +Ai+fi+ i+ X iiii i+ ii 1i ~ri;iFci +Mci +Ai+1i++~si;i+fi+ i+ ii Ti1"z 
ii A.4 The Energy Function and its Partial ZX eTi2dt t R i P t Z t @2 T2dt2X @etii @T 1 i 2 Ti dt 
@qj @qj @qj t 1 A.5 Forward Partials with Initial Conditions i+ i !i+T ! i i+ jA i+ qj qj ! ii++ ATjj 
i+ j! qj qjj i+ T !_ AT A i+jj jjj i+ j!_+ !_zq_j qj qjj qjjj i+ i+ ^i+ i+ 0^ !_ ! ^ !!T i+i+i+i+ i+i+ 
+ !~+!~ i+i+ qj qj qj qj i+ i i qs0;i+T sq0;i 0ii i+i++si;i+ jA qj qj qj i+ sq AT 0;i+jj jj i+ jqs+0s 
0;jjj;j qj qj TMi+ i+ i+ 0 0 ci+i+i+ i+i+Jci+Jci+ qj qj qj i+ i+ Fc qr i+0;i+ m i+ qj qj  A.6 Reverse 
Partials with Initial Conditions fi Fi X i c Ai+ f iii+ i 9i++ fi++Ai+ qj qj qj qj i+ i Fi f ci i 69i+ 
qj qj i Fi ^Mi X 1i c Ai+i . icii+ 9i+r++ 1+ i;i+ i+ qj qj qj qj i+ i+ i+ 1g g f i+Ai+i+ i+ Ai++si f+siAi+ 
i;i+i+i;i+ qj qj qj i Fi ^i 1 ci Mc i i i . 69i+r+ qj i;i+ qj qj i Ti 1ii "z i qj qj  References [1] 
BALAFOUTIS,C.A., ANDPATEL,R.V. Dynamic Analysis of Robot Manipulators: A Cartesian Tensor Approach.Kluwer 
Academic Publishers, 1991. [2] BRUDERLIN,A., AND WILLIAMS, L. Motion signal pro­cessing. In Computer 
Graphics (Aug. 1995), pp. 97 104. Pro­ceedings of SIGGRAPH 95. [3] BURDETT,R.G.,SKRINAR,G.S., ANDSIMON,S.R.Com­parison 
of mechanical work and metabolic energy consump­tion during normal gait. Journal of Orhopaedic Research 
1,1 (1983), 63 72. [4] COHEN,M.F.Interactivespacetimecontrolforanimation.In Computer Graphics (July 1992), 
pp. 293 302. Proceedings of SIGGRAPH 92. [5] GILL,P.E., MURRAY,W., AND WRIGHT,M. H. Practical Optimization. 
Academic Press, 1981. [6] HOLLERBACH, J. M. A recursive lagrangian formulation of manipulator dynamics 
and a comparative study of dynamics formulation complexity. IEEE Transactions on Systems, Man, and Cybernetics 
SMC-10, 11 (Nov. 1980). [7] LIU,Z., AND COHEN, M. F. An ef.cient symbolic inter­face to constraint based 
animation systems. In Proceedings of the 5th EuroGraphicsWorkshopon Animation andSimulation (Sept. 1994). 
[8] LIU,Z., GORTLER,S.J., AND COHEN, M. F. Hierarchical spacetime control. In Computer Graphics (July 
1994), pp. 35 42. Proceedings of SIGGRAPH 94. [9] UNUMA,M., ANJYO,K., AND TEKEUCHI, R. Fourier prin­ciples 
for emotion-based human .gure animation. In Com­puter Graphics (Aug. 1995), pp. 91 96. Proceedings of 
SIG-GRAPH 95. [10] WITKIN,A., ANDKASS,M.Spacetimeconstraints.In Com­puter Graphics (Aug. 1988), pp. 159 
168. Proceedings of SIGGRAPH 88. [11] WITKIN,A., AND POPOV´IC, Z. Motion warping. In Com­puter Graphics 
(Aug. 1995), pp. 105 108. Proceedings of SIGGRAPH 95. [12] ZHAO,J., AND BADLER, N. I. Inverse kinematics 
position­ing using non-linear programming for highly articulated .g­ures. ACM Transactions on Graphics 
13, 4 (Oct. 1994), 313 336. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237231</article_id>
		<sort_key>155</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Limit cycle control and its application to the animation of balancing and walking]]></title>
		<page_from>155</page_from>
		<page_to>162</page_to>
		<doi_number>10.1145/237170.237231</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237231</url>
		<keywords>
			<kw><![CDATA[control]]></kw>
			<kw><![CDATA[human animation]]></kw>
			<kw><![CDATA[limit cycles]]></kw>
			<kw><![CDATA[locomotion]]></kw>
			<kw><![CDATA[physically-based modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P149166</person_id>
				<author_profile_id><![CDATA[81100081962]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laszlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Electrical and Computer Engineering, University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40036809</person_id>
				<author_profile_id><![CDATA[81319502903]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michiel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van de Panne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117278</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>225295</ref_obj_id>
				<ref_obj_pid>225294</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. AUSLANDER ET AL. Further Experience With Controller-based Automatic Motion Synthesis For Articulated Figures. ACM Transactions on Graphics, October 1995.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[N.I. BADLER, B. BARSKY and D. ZELTZER. Making them move. Morgan Kaufmann Publishers Inc., 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>113039</ref_obj_id>
				<ref_obj_pid>113034</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. BOULIC, N. M. THALMANN and D. THALMANN. A Global Human Walking Model With Real-time Kinematic Personification. The Visual Computer, 6, 1990, pp. 344-358.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74357</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. BRUDERLIN and T. W. CALVERT. Goal-Directed Animation of Human Walking. Proceedings of SIGGRAPH 89 (1989), In Computer Graphics 23, 4, (1989), pp. 233-242.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A. BRUDERLIN and T. W. CALVERT. Interactive Animation of Personalized Human Locomotion. Proceedings of Graphics Interface (1993), pp. 17-23.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[A. BRUDERLIN and L. WILLIAMS. Motion Signal Processing. Proceedings of SIGGRAPH 95 (Los Angeles, August,1995). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIG- GRAPH, pp. 97-104.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[H.C. CHANG, ET AL. A General Approach For Constructing The Limit Cycle Loci Of Multiple-Nonlinearity Systems. IEEE Transactions on Automatic Control, AC-32, 9, 1987, pp. 845-848.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[W.T. DEMPSTER and G. R. L. GAUGHRAN. Properties Of Body Segments Based On Size And Weight. American Journal of Anatomy, 1965, 120, 33-54.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. FURUSHO and M. MAUBUCHI. A Theoretically Motivated Reduced Order Model for the Control of Dynamic Biped Locomotion. Journal of Dynamic Systems, Measurement, and Control, 109, 1987, pp. 155- 163.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83534</ref_obj_id>
				<ref_obj_pid>83528</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. FURUSHO and A. SANO. Sensor-Based Control of a Nine-Link Robot. The International Journal of Robotics Research, 9, 2, 1990, pp. 83-98.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>31465</ref_obj_id>
				<ref_obj_pid>31462</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. GIRARD. Interactive Design Of Computer-animated Legged Animal Motion. IEEE Computer Graphics and Applications, 7, 6, June, 1987, pp. 39-51.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[C.L. GOLLIDAY and H. HEMAMI. An Approach to Analyzing Biped Locomotion Dynamics and Designing Robot Locomotion Controls. IEEE Transactions on Automatic Control, AC-22, 6, 1970, pp. 963- 972.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218411</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R. GRZESZCZUK and D. TERZOPOULOS. Automated Learning of Muscle-Actuated Locomotion Through Control Abstraction. Proceedings of SIGGRAPH 95 (Los Angeles, California, August 1995). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 63-70.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[H.M. HMAM and D. A. LAWRENCE. Robustness Analysis of Nonlinear Biped Control Laws Via Singular Perturbation Theory. Proceedings of the 31 st IEEE Conference on Decision and Control, 1992, pp. 2656-2661.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J.K. HODGINS ET AL. Animating Human Athletics. Proceedings of SIGGRAPH 95 (Los Angeles, August, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH, pp. 71-78.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. KATOH and M. MORI. Control Method of Biped Locomotion Giving Asymptotic Stability Of Trajectory. Automatica, 20, 1984, pp. 405-414.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[H. KO and N. I. BADLER. Straight Line Walking Animation Based on Kinematic Generalization that Preserves the Original Characteristics. Proceedings of Graphics Interface '93, 1993, pp. 9-16.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>137960</ref_obj_id>
				<ref_obj_pid>137957</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[D.E. KODITSCHEK and M. BOHLER. Analysis Of A Simplified Hopping Robot. The International Journal of Robotics Research, 10, 6, 1991, pp. 587-605.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[J.F. LASZLO. Controlling Bipedal Locomotion for Computer Animation, M.A.Sc. thesis, University of Toronto, 1996. URL: &lt;www.dgp.utoronto.ca/Njflaszlo&#62;
]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[J.M. LIN and K. W. HAN. Reducing The Effects Of Model Reduction On Stability Boundaries And Limit-Cycle Characteristics. IEEE Transactions on Automatic Control, AC-31, 6, 1986, pp. 567-569.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83533</ref_obj_id>
				<ref_obj_pid>83528</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[T. MCGEER. Passive Dynamic Walking. The International Journal of Robotics Research, 9, 2, 1990, pp. 62-82.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[T. MCGEER. Passive Walking with Knees. Proceedings of IEEE International Conference on Robotics and Automation, 1990, pp. 1640- 1645.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97882</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. MCKENNA and D. ZELTZER. Dynamic Simulation of Autonomous Legged Locomotion. Proceedings of SIGGRAPH 90 (1990). In Computer Graphics (1991), pp. 29-38.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[H. MIURA and I. SHIMOYAMA. Dynamic Walk Of A Biped. International Journal of Robotics Research, Summer 1984, pp. 60-74.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J.T. NGO and J. MARKS. Spacetime Constraints Revisited. Proceedings of SIGGRAPH 93 (1993). In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 343-350.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6152</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[M. H. RAIBERT. Legged Robots that Balance. MIT Press, 1986.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122755</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[M.H. RAIBERT and J. K. HODGINS. Animation Of Dynamic Legged Locomotion. Proceedings of SIGGRAPH 91 (1991). In Computer Graphics, 1991, pp. 349-358.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SYMBOLIC DYNAMICS INC. SD/Fast User's Manual, 1990.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[K. SIMS. Evolving Virtual Creatures. Proceedings of SIGGRAPH 94 (Orlando, Florida, July, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 15-22.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155326</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[A.J. STEWART and J. F. CREMER. Beyond Keyframing: An Algorithmic Approach to Animation. Proceedings of Graphics Interface '92, 1992, pp. 273-281.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[M. UNUMA, K. ANJYO and R. TAKEUCHI. Fourier Principles for Emotion-based Human Figure Animation. Proceedings of SIG- GRAPH 95 (Los Angeles, California, August, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIG- GRAPH, pp. 91-96.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166159</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[M. VAN DE PANNE and E. FIUME. Sensor-Actuator Networks. Proceedings of SIGGRAPH 93, (1993). In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 335- 342.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[M. VAN DE PANNE, R. KIM and E. FIUME. Virtual Wind-up Toys for Animation. Proceedings of Graphics Interface '94, 1994, pp. 208-215.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[H. G. VISSER and J. SHINAR. First-Order Corrections In Optimal Feedback Control Of Singularly Perturbed Nonlinear Systems. IEEE Transactions on Automatic Control, AC-31, 5, 1986, pp. 387-393.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>533579</ref_obj_id>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[VUKOBRATOVIC ET AL. Biped Locomotion: Dynamics, Stability, Control and Applications, Springer Verlag, 1990.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[A. WITKIN and Z. POPOVI'C. Motion Warping. Proceedings of SIG- GRAPH 95 (Los Angeles, California, August, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995 ACM SIG- GRAPH, pp. 105-107.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[W.L. WOOTEN and J. K. HODGINS. Simulation Of Human Diving. Proceedings of Graphics Interface '95, 1995, pp. 1-9.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Limit Cycle Control And Its Application To The Animation Of Balancing And Walking Joseph Laszlo Michiel 
van de Panne Eugene Fiume Department of Computer Science and Department of Electrical and Computer Engineering 
University of Toronto ABSTRACT Seemingly simple behaviors such as human walking are difficult to model 
because of their inherent instability. Kinematic animation techniques can freely ignore such intrinsically 
dynamic problems, but they therefore also miss modeling important motion character­istics. On the other 
hand, the effect of balancing can emerge in a physically-based animation, but it requires computing delicate 
con­trol strategies. We propose an alternative method that adds closed­loop feedback to open-loop periodic 
motions. We then apply our technique to create robust walking gaits for a fully-dynamic 19 de­gree-of-freedom 
human model. Important global characteristics such as direction, speed and stride rate can be controlled 
by chang­ing the open-loop behavior alone or through simple control param­eters, while continuing to 
employ the same local stabilization technique. Among other features, our dynamic human walking character 
is thus able to follow desired paths specified by the ani­mator. Keywords: control, limit cycles, physically-based 
modeling, loco­motion, human animation 1. INTRODUCTION As with any modeling endeavor, Nature has much 
to tell us about modeling motion. When observing the running motion of a charg­ing bull, before we take 
cover we can clearly see that motion is a product of both physics and muscular action. Physically based 
an­imation mimics Nature by modeling both. While techniques for simulating the basic physics of motion 
are well known, less is known about how to provide the necessary control over the muscles or actuators 
in order to produce a desired motion [1]. By analogy, for most adults, walking is a seemingly ef­fortless 
task. We know from watching toddlers that the apparent ease of walking, running, and maintaining balance 
is deceiving. It comes as little surprise, then, that walking has proven to be a diffi­cult motion to 
model. Indeed the most successful approach has been literally to watch Nature by capturing motion data 
from real walks and mapping this data to computer generated characters. The {jflaszlo | van | elf}@dgp.utoronto.ca 
We gratefully acknowledge the financial support of our research by ITRC (Ontario) and NSERC (Canada). 
The ongoing benevolence of Alias|Wavefront to our graphics lab is also appreciated. Permission to make 
digital or hard copies of part or all of this work or personal or classroom use is granted without fee 
provided that copies are not made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
 Figure 1. A 3-D dynamic walk using limit cycle control. work of [16, 5, 30, 35] is a good survey of 
recent techniques aimed at making the most flexible use of existing motion-capture and key­frame data. 
The advantages of motion capture are obvious: the im­mediate generation of realistic human motions. However, 
motion capture does not provide us with sufficient understanding to create more general walking motions, 
especially when conditions are un­predictable, when new motions need to be generated, or when deal­ing 
with non-human characters. Developing methods to control physical simulations can potentially provide 
us with a more general tool, and it is therefore the modeling paradigm we adopt. This paper proposes 
a solution for the control of periodic, un­stable motions. The technique automates the addition of feedback 
to otherwise unstable motions, such as walking and running (see Figure 1). We also present an application 
of our technique by dem­onstrating a human model capable of a variety of walking styles, as well as a 
second, imaginary creature. 2. BACKGROUND Many solutions to animation control problems have been proposed 
in the animation literature and elsewhere. In particular, there is a large body of work focussed on the 
important problem of locomo­tion control. What follows is a brief summary of some of these techniques, 
with an emphasis on algorithms for locomotion, includ­ing human walks and runs. Procedural methods have 
been popular for generating motion [10], especially for human walking [2, 3, 4]. These methods direct­ly 
generate walking motions by using a series of constraints, which are based on empirical data or on kinematic 
relationships. The work of [3] uses a mixed kinematic/dynamic model. A positive feature of such systems 
is that they give the animator direct control over useful gait characteristics, such as stride length, 
pelvic list, etc. The motions produced are typically parameterized in a way that is directly meaningful 
to an animator. In physical approaches to animation, one must solve for the control actions that will, 
upon simulation, produce a desired mo­tion. One approach has been to use a type of underconstrained in­verse 
dynamics with automatic addition and removal of constraints which can be hand-tailored to yield stable 
3D walks [29]. Several other approaches have treated control directly as a search problem, employing 
a particular choice of control representation and a choice of search algorithm, including genetic algorithms 
[24, 28], and sim­ulated annealing [23, 12, 31]. The results indicate that such tech­niques are surprisingly 
adept at finding novel modes of locomotion. Unfortunately, it is less clear that such global search techniques 
are an efficient means of finding control strategies for motions requir­ perturbation stable limit 
return tocycle stable limit cycle state space Figure 2. A passively-stable limit cycle. ing fine control 
and feedback, such as human walking and running. One would think that to create this type of controller, 
we should be able to do better than to explore a large solution space at random. Toward this end, some 
methods, including ours, draw upon previ­ous work in robotics and control. The hopping and running control 
strategies presented in [14, 26] represent a more methodical approach to arriving at a control strategy. 
The basic idea stems from earlier robotics work [25] and is a powerful control technique for hopping 
and running motions, given that certain assumptions are fulfilled. The key is an elegant decomposition 
of the control problem which arises from assuming the legs are lightweight with respect to the body mass. 
This as­sumption can be overcome to some extent as shown in the work of [14]. A more analytical examination 
of the remarkable robustness of this and a class of related strategies can be found in [17]. Other specific 
controller designs have also met with success. The simulated cockroach in [22] is a good example. Strategies 
pro­posed for walking robots also have possible applications for anima­tion [34, 9, 11, 23]. Demonstrations 
of passively-powered walking down a slight incline show that active walking may only require small amounts 
of energy [20, 21]. The point of departure for our work is the concept of periodic limit cycles. Consider 
for a moment the motion of a typical me­chanical toy, which drives its joints in a repetitive, periodic 
fashion and is oblivious to its environment. This type of open-loop control is sufficient for many types 
of animated motion, as has been point­ed out in [32]. However, it is insufficient as a control mechanism 
for unstable dynamic motions such as walking and running. The control technique we propose provides a 
general method of turning unstable open-loop motions into stable closed-loop motions. The basis of the 
technique is a process that perturbs the open-loop con­trol actions slightly in order to yield a desired 
stable, cyclic motion. A broad range of related work on limit cycles and periodic control can be found 
in the control systems literature [15, 13, 19, 6, 33]. While this paper provides all the essential information 
for imple­menting our technique, more details can be found in [18]. 3. LIMIT CYCLE CONTROL A mechanical 
toy owes its successful motion to a stable limit cycle that arises naturally from the interaction of 
the toy with its environ­ment. Typically, the environment acts on the toy in such a way that any disturbance 
to the motion, such as a small external push, is rap­idly damped. Figure 2 illustrates this concept using 
a phase dia­gram. The phase diagram is a projection of the path a motion traces through state-space over 
time.1 Unfortunately, recalling our wob­bly toddler, unstable motions such as walking cannot rely solely 
on 1. An object's state is the minimal set of parameters necessary to describe the position and velocity 
of all points on the object. end of one cycle, beginning of the next = g( x , )xn+1 u0+u nn 0x n+1 = 
xn+1 xn+1 state space 0 = g( x , u0) xn+1 n Figure 3. A limit cycle as a discrete dynamical system. passive 
damping of disturbances The goal of limit-cycle (LC) control is to actively drive mo­tions that would 
otherwise be unstable back to a fixed limit cycle. The key point to implementing LC control is to avoid 
dealing with the complexities of non-linear dynamics by representing the motion using a well-behaved 
discrete dynamical system. In general, the continuous equations of motion can be ex­pressed by the non-linear 
differential equation x. = f (x, u) where x is the system state, x. its time derivative, and u is the 
control input. The discrete dynamical system that we shall deal with has the following iterative form: 
x = g(x , +. u ) n + 1 n u0 n where x represents the initial state of the nth periodic limit cycle, nxn+1 
is the state on the next cycle, u0 is the periodic open-loop con­ trol, and . u is an applied control 
perturbation. The purpose of . u nn will be to drive the sequence x1, x2, ... ,xi to a desired value, 
xd. Fig­ure 3 depicts an abstraction of the discrete dynamical system. The advantage of dealing with 
the discrete dynamical system is that it is relatively smooth and therefore subject to linear approx­imation. 
There is no guarantee that the discrete system will be smooth, although we have in general found this 
to be the case for our experiments. For small control perturbations, . u, that will be napplied on each 
cycle, the resulting change in state, . xn+1, has a first order approximation of . x = J. u n + 1 n where 
J is a Jacobian relating the change in state after one cycle to the control perturbation. A first order 
approximation of the change of state makes it possible to calculate the control perturbation re­quired 
to bring a system back onto a desired limit cycle, namely - 1 . u = J. x nn + 1 where the desired . xn+1 
is calculated as . xn+1 = xd x0 n+1, xd being the desired state and x0 n+1 being the state achieved 
when the nomi­nal control, u0, is applied. 3.1 Linearity of Control Perturbations Our evidence for the 
above linear approximation is empirical, al­though this type of linearization has been justified more 
rigorously in the application of control theory to certain types of dynamical systems [8, 13]. Figure 
4 provides some experimental evidence that a linear model is a sufficient for modeling the effect of 
control per­turbations over a complete cycle of motion, despite the occurrence of discontinuous events 
such as foot-falls during the cycle itself. 1 0.8 0.6 0.4 0.2 -0.2 stance hip pitch (degrees) Figure 
4. Linearity of perturbation control. The vertical axis on this graph is a variable we would like to 
be able to control, namely some aspect of the system state. In this particular example, it is a measure 
of the forward pitch of the body of a simulated human model performing a walking movement, as measured 
at the end of a single step. The horizontal axis gives the magnitude of an applied control perturbation, 
in this case an alter­ation of the hip pitch angle during a particular part of the step. The graph thus 
tells us what type of forward pitch our simulated human body will have after having taken one step, using 
different varia­tions of applied control. Each line shows an example of this rela­tionship for a different 
step. It is evident in this figure that we can alter the forward pitch of the body as we desire and that 
a linear ap­proximation is a reasonable model for the effect of a control pertur­bation. It is important 
to note that the effects of control perturbations depend on the initial state, x. This is clear from 
Figure 4, which n shows that the relationship between control perturbation and change in state varies 
for different steps. What this means in prac­tice is that for each cycle of the motion, we need to recompute 
the Jacobian J, which defines the relationship between applied control perturbations and the resulting 
changes in state after one cycle. 3.2 Regulation Variables For many models, it is possible to make further 
simplifications and still effect proper LC control. It is typically neither necessary nor practical to 
work with a complete state vector in producing a con­trolled, stable limit cycle, as has thus far been 
implied. Instead, it can be sufficient to work with a small number of regulation vari­ables (RVs). Ensuring 
that these regulation variables are controlled to follow a limit cycle is sufficient to stabilize the 
limit cycle for the motion as a whole. The use of RVs instead of the complete state vector could also 
be considered advantageous for animation, as it potentially leads to a less-constrained, freer motion. 
This is loosely related to the use of reduced-order models in control theory [8, 13]. Along with choosing 
the regulation variables, one must provide a choice of desired target values for these variables to take. 
So as to make the notion of regulation variables more concrete, we introduce some of the possible choices 
that we know (through experiments) work well for a human walking model. A well chosen set of regulation 
variables should give a meaningful projection of the system state over a large range of possible states. 
Figure 5 shows two possible choices for sets of regulation variables, each based upon the definition 
of a particular type of vec­tor. The simplest of the two is the up-vector, so we begin by ex­plaining 
this choice. The up-vector is a fixed vector of unit length, defined in the coordinate frame of the pelvis, 
which can be used to measure the forward lean and sideways tilt of the pelvis. The regu­lation variables 
are the forward and lateral components of the up­vector, i.e., the projection of the up-vector onto a 
horizontal ground plane. Controlling the values of these two scalar variables using LC forward torso 
lean (unit vector projection)  Figure 5. Regulation variables for use in walking. From left to right: 
(a) Up vector, (b) Swing-COM vector. Figure 6. Control perturbations used for walking. control is sufficient 
to yield a balanced, fully three-dimensional walking motion for our human model. Other choices of regulation 
variable are also possible. The swing-center-of-mass (swing-COM) defines a vector from the swing foot 
to the center-of-mass, and thus measures where the cen­ter-of-mass will lie with respect to the future 
point of support. Once again, the two scalar components of the projection of the vector onto the ground 
plane form the set of regulation variables. In the presentation of the results, we primarily demonstrate 
the use of the up vector. 3.3 Control Perturbations Given that it is often sufficient to work with a 
limited number of regulation variables, we need to determine the type and number of control perturbations 
to effect the necessary control. For two regu­lation variables, as is the case for our walking example, 
we shall re­quire two appropriately-chosen control perturbations in order to yield a well-formed Jacobian 
J. The control perturbations we work with for our human model are twofold. First, one can use changes 
to the stance-hip pitch and roll, effected over a particular portion of the walk cycle. Figure 6 illustrates 
the effects of these perturbations in an exaggerated fash­ion. The two figures on the left demonstrate 
stance-hip roll, while those on the right demonstrate stance-hip pitch. Second, the use of alterations 
to swing-hip pitch and roll can be similarly used as a suitable pair of control perturbations. In this 
case, the resulting con­trol can be thought of as a type of foot-placement strategy. As with the choice 
of regulation variables, the choice of control perturba­tions is not unique, although they must be chosen 
to span the space of desired changes to the regulation variables  4. MODEL DESCRIPTIONS We choose human 
walking as our primary example to illustrate the limit cycle control technique for two reasons. The first 
is that it is typical of motions for which the open-loop control actions are rela­tively easy to construct 
and can thus benefit immediately from LC control in order to close the loop. The second reason is that 
the control of a dynamic human walking model demonstrates the effec­ 3 3 1 m 2 2  Figure 7. Construction 
of the human model. Joints are 1 DOF except where indicated. 2 1 m 2  Figure 8. Construction of the 
robot model. Joints are 1 DOF except where indicated. tiveness and scalability of this technique. The 
dynamic control of models with many degrees-of-freedom (DOF) of the type used in animation is problematic 
for many control techniques. 4.1 The Human Model The physical model we use has a mass and inertia distribution 
com­parable to those of a real human. The parameters are identical to those used in [36] and were originally 
obtained from [7]. The mod­el has 13 joints and 19 DOF, as shown in Figure 7. The hip joints have three 
rotational DOF, while the ankle joints have two rotation­al DOF. All other joints have one DOF. The equations 
of motion are calculated and integrated using a commercially-available simu­lation package [27]. The 
ground is modelled using a penalty meth­od. Stiff springs and dampers exert forces on a set of four points 
on the feet whenever they penetrate the ground. Each point is allowed to slip independently when the 
ratio of its applied horizontal and vertical component forces exceeds a user supplied threshold. The 
ground model thus uses no artificial constraints to hold the foot in place, which have in the past been 
used to simplify the simulation (and to some extent the control) of human motion. 4.2 The Robot Model 
As a second test case, we consider the robot shown in Figure 8. This figure has 11 joints and 15 rotational 
DOF. The lateral base of support is much wider than that of the human model, yielding a very different 
type of motion. Mass and inertia parameters for this mod­el are shown in Table 1.  5. APPLYING LC CONTROL 
In the following sections we present the details of the limit cycle control algorithm as applied to our 
human walking model. The dis- Link Mass (kg) Moment of Inertia (x, y, z kgm2) body head upper leg mid-leg 
lower leg ankle foot 14.72 20.9 1.2 1.6 2.2 1.4 2.52 0.47 1.11 0.001 0.023 0.057 0.0012 0.0098 0.26 0.92 
0.01 0.0013 0.0018 0.015 0.024 0.40 0.72 0.01 0.023 0.057 0.015 0.016 Table 1. Robot model mass and 
inertia parameters. The x, y and z axes are the forward, vertical and lateral axes respectively. S1 
S6 S5 Figure 9. Finite state machine employed for walking. cussion first looks at the open-loop control 
before proceeding on to examine how the LC-control algorithm can be superimposed on it. 5.1 Open-Loop 
Control Finite state machines (FSMs) combined with proportional-deriva­tive (PD) controllers are a common 
control mechanism in both physically-based animation [14, 22, 32] and robotics. The finite­state machine 
used as a basic controller for the walking motions is shown in Figure 9. Each state in the FSM provides 
a fixed set of desired angles to the individual PD joint controllers. In our FSM, the desired angles 
change as a step function when proceeding from one state to the next. The PD controllers calculate a 
torque according to t = kv(. d . ) kv.. where . d is the desired joint angle, . is the actual joint 
angle, .. is the angular velocity of the joint and kp and kv are gain constants which serve to define 
the strength of the joint. The state transitions in the finite state machine are time-based, with the 
exception of the transitions exiting states S1 and S4. These latter transitions are sensor-based and 
perform the simple job of en­suring that the proper stance foot is on the ground before complet­ing the 
current step. A basic open-loop motion can be constructed by defining the poses in states S2 and S3, 
where a pose consists of the set of desired joint angles to be used in a state. The pose for state S1 
is identical to the pose for state S2. The poses for states S4, S5, and S6 are the same as the poses 
for states S1, S2, and S3, respectively, with the left and right sides exchanging roles. In typical operation, 
state S2 (S5) raises and advances the swing leg and state S3 (S6) straightens it in anticipation of ground 
contact. Normally, the foot contacts the ground some time after entering state S3 (S6) and the remaining 
time in the state is spent in double-stance phase2. Since the next stance foot is already on the ground, 
the transition out of state S4 (S1) occurs immediately after entering it, essentially skipping the side 
view 1 rear view 1 side view 2 rear view 2 Figure 10. Walking with open-loop control (front/side 
views). state. The cycle then repeats for the other leg. The sensor-based transitions serve only to make 
for a more robust motion. They ef­fectively provide a way for the controlling FSM to remain synchro­nized 
with the actual motion and are typically only necessary during startup or when an FSM is dynamically 
altered to obtain a different motion. Note that strictly speaking, the FSM does not provide true open-loop 
control since the desired joint angles are realized using local PD controllers. Nevertheless, the motion 
is open-loop in the sense that no system-wide feedback is used to drive it towards the desired trajectory. 
The result of using the open-loop FSM of Fig­ure 9 for walking control is shown in Figure 10. It produces 
a mo­tion which takes several steps and then falls over. A certain amount of trial-and-error parameter 
tuning is re­quired to produce open-loop motions which can be balanced suc­cessfully. Although tedious, 
this process is relatively straightforward. Tasks might include ensuring that toes do not stub the ground 
and that the basic motion can produce movement in the desired directions. Once a good open loop controller 
has been gen­erated, it can be used to produce a wide variety of motions. 5.2 LC Control for Walking 
The first step in implementing LC control is to choose a set of reg­ulation variables and a set of control 
perturbations. We have exper­imented with two choices for the former, as shown in Figure 5, and two choices 
for the latter. As indicated in Figure 3, the final nature of the limit cycle is defined by the target 
state to be achieved at the end of the current cycle. It should be noted that LC control is not successful 
for all choices of target values. Target values should be similar to the val­ues that can be observed 
during the first few steps of the open-loop motion. This ensures that the generated limit cycle is close 
to the unstable open-loop limit cycle, thereby limiting the LC control to having to perform relatively 
small control corrections. The power of LC control lies in being able to predict the change in values 
of the regulation variables with respect to the ap­plied control perturbations. Using a linear model 
for this allows us to easily predict the required perturbation. The chosen pair of control perturbations 
for human walking, namely alterations to the desired hip pitch and roll angles, were de­signed to allow 
for more-or-less independent control of each of the regulation variables. The hip roll is effectively 
used to provide bal­ance in the coronal (side-to-side) plane, while the hip pitch provides 2. The double-stance 
phase is the part of the walking cycle during which both feet are in ground contact. x RV sample pointsx 
RVd obtained by simulation d u u u u0 u +u 00 u + k u Figure 11. Interpolating for a desired control 
perturbation. Figure 12. A falling motion illustrating the torso servo. balance in the sagittal (front-back) 
plane. In effect, this corre­sponds to only requiring the use of the diagonal elements of the Ja­cobian. 
We have not yet attempted to make use of the full Jacobian. Figure 11 illustrates the linear interpolation 
scheme, which is applied twice, once for sagittal balance, and once for coronal bal­ance. Carrying out 
one actual step in a walking motion requires per­forming five simulations of the step, each slightly 
different from one another. The first four simulations are used to capture the nec­essary data to construct 
a simple model of how control perturba­tions will affect the state of the body at the end of the step. 
This model is then used to estimate the necessary control perturbations to achieve the desired target 
state, and hence the desired limit cycle. The fifth simulation is required to produce the final balanced 
mo­tion for the current step before proceeding on to the next. The blind reconstruction of the RV-perturbation 
model each step in this fash­ion results in a five-fold increase in the required computation time compared 
to normal forward dynamic simulation. If the local per­turbation model can itself be predicted, true 
closed-loop control can be achieved. We are optimistic that this is possible. The robot model uses very 
similar choices to the human mod­el s to achieve a running motion. Stance hip variations provide the 
control perturbations and the chosen RVs are projections of an up vector attached to the creature s head. 
The primary difference be­tween the control for the two models is in the open loop FSM. Aside from differences 
in the particular poses, the robot s transition times are smaller than the human FSM (by about half) 
since the creature s wide stance makes it difficult to remain on one foot for long.  5.3 Torso Servo 
While the limit cycle control mechanism described thus far gener­ates stable walks, the resulting motions 
exhibit a characteristic bob­bing of the torso. This is an artifact of the simple open-loop motion chosen 
as a point of departure for our walking gait. We implement a simple vertical torso servo which not only 
smoothes the torso mo­tion (if desired), but also demonstrates the robustness of the limit cycle control 
in continuing to provide effective balance. For LC control, stabilizing a system which already contains 
some feedback components such as torso servoing is no different than stabilizing an open-loop motion. 
The torso servo consists of a PD controller applied to the one degree-of-freedom waist joint. The applied 
torque at this joint serves to force the torso to always remain upright with respect to the world coordinate 
frame. Note that the torso servoing does not pre­vent the biped from falling because the legs must still 
ultimately 0.35 0.3 10 0.25 5 0.2 forward component  0.15 0 0.1 -5 0.05 0 lateral component x Figure 
13. Regulation variable limit cycle (up-vector based). Figure 15. Dynamic walks using the COM-vector. 
for the forward component of the vector takes on different values. The quality of the final motion depends 
heavily on the open­loop motion on which it is based. The simple four pose FSM used 10 x  increasing 
the number of states and changing the timing of the swing leg motions. The use of motion captured data 
for tuning the open-loop gait for realism presents the possibility of further refin­ 0 -5 ing the motion 
while retaining both the guarantee of realistic mo­tion and the flexible autonomy LC control can provide. 
to generate most of the human model walks is quite robust, but pro­ 5 duces a motion more akin to a robot 
marching than to normal hu­man walking. A more human-like walk has been generated by -10 -30 -25 -20 
-15 -10 -5 0 y Figure 14. Dynamic walks using the up-vector provide for balanced support, as illustrated 
in Figure 12.  6. RESULTS Limit cycle control has been applied to obtain stable walking gaits for a 
19-DOF human model of realistic proportions. As a second example, we control the running motions of a 
two-legged robot with a bird-like skeleton. Figure 1 shows a sequence of frames from a typical dynamic 
walk, resulting from the application of limit cycle control to an open-loop walking motion, with torso 
servoing enabled. An illustration of the typical limit cycle which is achieved is shown in Figure 13. 
The path indicates the continuous-time projec­tion of the unit up vector onto the horizontal plane. This 
figure is the real analog of the earlier abstraction shown in Figure 3. For walking, the limit cycle 
consists of two roughly symmetric halves. This occurs because a single cycle consists of a left step 
and a right step, each forming half of the complete limit cycle for a stride. The desired values for 
the regulation variables lie at the center of this di­agram, at RVforward = 0.25, RVlateral = 0. Perturbation 
control is ap­plied on each step, thus forcing the limit cycle towards the desired point twice on each 
cycle. The startup phase of the motion is also evident from the figure, with the regulation variables 
eventually be­ing driven onto a stable limit cycle. While LC control is a general method of adding balance 
to a walk, it does not by itself ensure a straight walk. Figure 14 illus­trates different paths taken 
for different target values (RVd) of the regulation variable controlling the desired forward lean. The 
figure is a top view of the walking motion, showing only the position of the pelvis, enlarged in order 
to make the orientation of the body clearly visible during the walk. The correct scale for the walk is 
given by the axes, indicated in metres. An example of an alternative choice for the set of regulation 
variables is the use of the swing-center-of-mass (swing-COM) vec­tor. This also leads to stable walks, 
but not necessarily straight walks. Figure 15 shows how the path can vary as the target value 6.1 Additional 
Animator Control Control over the speed and direction of a walking gait is of obvious necessity to an 
animator. Because the stabilization (i.e., balance) of the walk is automated, it is a relatively simple 
matter to provide the necessary hooks to control these parameters, as well as other possi­ble stylistic 
variations.  6.2 Speed Control The speed of the walking gait can be controlled in several ways. One 
technique we have applied is to alter the underlying open-loop motion to produce particular faster and 
slower velocity walks and to use interpolations of these base controllers to achieve intermedi­ate speeds. 
Another successful approach makes use the fact that the forward speed is a function of the target values 
chosen for the reg­ulation variables. As we choose limit cycles for which the pelvis and torso lean forward 
more, the speed of the gait increases. In this case, a simple form of velocity feedback is used to give 
consistent, stable steady-state velocities. The RV target value for each step, i, is: RVdi = RV* + K(vd 
-v) d where v and vd are the actual and desired speeds respectively, K is a proportionality constant, 
and RVd* is a bias term used to relate RV values to speed. Either the bias term or the desired velocity 
can be used to vary walking speed. Figure 16 illustrates the walking speed for a set of walks obtained 
using this approach. The startup phase can be recognized during the first several steps and the longest 
walk demonstrates both positive and negative acceleration phases. All of the motions begin from rest. 
Only relatively small changes in the final RV target values are required to achieve a reasonable range 
of speeds. The fastest walk in Figure 16 has RVdi 0.30 while the slowest walk (nearly stationary) has 
RVdi 0.25. These correspond to forward pelvis angles of approximately 17 and 14 degrees from vertical 
respectively. This technique allows the underlying open loop motion to remain fixed, but tends to reduce 
motion quality somewhat at higher speeds. velocity (m/s)  1.2 20 5 1 RVd = 0.3 0.8 15 0.6 RVd = 0.285 
 y 10 0.4 0 0.2 0 5 10 15 RVd = 0.25 20 25 time (s) 30 35 40 45 5 3 1 4 6 Figure 16. Speed control for 
a dynamic walk. RVd = steady-state RVid. 02  -14-12-10 -8 -6 -4 -2 0 x Figure 19. Path following using 
six target points. S3S4S5 S6 S6 S1 S2 S3 S3 pelvis stance foot swing foot Figure 17. Hip rotations for 
turning during a walk. 12 Figure 20. Stylistic variation on a walk. 10 8 6 y 4 2 0 -2 -15-10 -5 0 5 
x Figure 18. Turning motions for a dynamic walk  6.3 Direction Control By changing the open-loop motion 
to include suitable hip twists, controllable turning motions can be achieved. Figure 17 shows a sequence 
of footprints which illustrate the effect of the hip joint ro­tations on the orientation of the pelvis 
for the case of a stationary walk. The figure indicates the hip rotations relative to the pelvis at key 
points in the walking cycle and the states of the basic FSM (Fig­ure 9) in which they occur. The turning 
motion works best when torso servoing is applied, in that turns of tighter radius can be per­formed. 
Figure 18 shows turns obtained using scaled versions of the twisting motion and stabilized using LC control. 
Once the turning radius can be controlled, it can be used to produce a path-following algorithm. Figure 
19 shows a dynamic walk following a desired trajectory. The algorithm makes use of a target point on 
the trajectory and chooses a turning rate proportion­al to the current error in direction. When the target 
point has almost been reached, its position is updated to be further along the desired path. 6.4 Other 
Variations Many other walking styles can be implemented by tailoring the open-loop control as desired. 
In many cases, transitions between different motions can be performed by simple linear interpolation 
of the underlying open-loop control over a period of a few steps. In some cases, a more gradual transition 
or more complex control of the desired RV values is necessary to avoid a fall. In either case, Figure 
21. A ducking motion. the basic LC control mechanism remains the same. Note that be­cause a dynamic simulation 
is always being used, ground con­straints and other physics constraints are always fulfilled, something 
that is not necessarily the case with direct kinematic in­terpolation of motion data, especially for 
large variations. Figure 19 illustrates a stylistic variation on a walk with the knees being bent and 
lifted high on each step. Figure 21 shows a ducking motion obtained by transitioning into and out of 
a bent­over walk. Stride rate variations can be achieved by changing the duration of time-based state 
transitions in the open-loop FSM. We have also simulated walking motions into a strong wind, for which 
the automatic feedback provided by the LC control visibly alters the motion to lean into the wind. In 
addition to these variations, LC control has proven capable of balancing lateral and backward walk­ing 
motions obtained using exactly the same open-loop FSM as the forward moving walks. When the human model 
s initial state has a sufficient backward or lateral velocity component, a balanced walk ensues in the 
direction of this initial nudge. The application of velocity control to effect transitions into and out 
of such motions has not yet been attempted but is expected to be relatively straight­forward. As a final 
example, Figure 22 shows our robot model which can run in a controlled fashion using LC control. This 
example serves to further illustrate the generality of LC control with respect to significant model and 
gait variations.  7. CONCLUSIONS Physically-based animation is difficult because of the lack of gen­eral 
control techniques. Motions such as walking are known to be Figure 22. Running motion for the robot. 
particularly difficult to control because of their unstable nature. The limit cycle control technique 
offers an automated way of add­ing closed-loop control to a basic desired open-loop motion. The open-loop 
component of the control can be tailored in a variety of ways to produce stylistic variations and useful 
parameterizations of the motion without any loss of physical realism. While the human walks obtained 
are not yet equivalent to mo­tion-capture quality, they are among the first demonstrations that general 
control techniques can indeed be developed for figures of relatively high complexity performing unstable 
motions such as walking. For imaginary creatures, physically-based simulation at present provides the 
best way of ensuring that motions abide by all the laws of physics. Thus, a general method of providing 
closed­loop control for such simulations is of considerable importance. In the near future, we foresee 
integrating the closed-loop mo­tion control developed here with an ever-growing library of other types 
of skilled motor control in order to produce simulated syn­thetic actors capable of a truly diverse set 
of physically-correct be­haviors. 8. REFERENCES [23] J. AUSLANDER ET AL. Further Experience With Controller-based 
Au­tomatic Motion Synthesis For Articulated Figures. ACM Transac­tions on Graphics, October 1995. [1] 
N. I. BADLER, B. BARSKY and D. ZELTZER. Making them move. Morgan Kaufmann Publishers Inc., 1991. [2] 
R. BOULIC, N. M. THALMANN and D. THALMANN. A Global Human Walking Model With Real-time Kinematic Personification. 
The Visu­al Computer, 6, 1990, pp. 344-358. [3] A. BRUDERLIN and T. W. CALVERT. Goal-Directed Animation 
of Human Walking. Proceedings of SIGGRAPH 89 (1989), In Computer Graphics 23, 4, (1989), pp. 233-242. 
[4] A. BRUDERLIN and T. W. CALVERT. Interactive Animation of Per­sonalized Human Locomotion. Proceedings 
of Graphics Interface (1993), pp. 17-23. [5] A. BRUDERLIN and L. WILLIAMS. Motion Signal Processing. 
Pro­ceedings of SIGGRAPH 95 (Los Angeles, August,1995). In Computer Graphics Proceedings, Annual Conference 
Series, 1994, ACM SIG-GRAPH, pp. 97-104. [6] H. C. CHANG, ET AL. A General Approach For Constructing 
The Limit Cycle Loci Of Multiple-Nonlinearity Systems. IEEE Transac­tions on Automatic Control, AC-32, 
9, 1987, pp. 845-848. [7] W. T. DEMPSTER and G. R. L. GAUGHRAN. Properties Of Body Seg­ments Based On 
Size And Weight. American Journal of Anatomy, 1965, 120, 33-54. [8] J. FURUSHO and M. MAUBUCHI. A Theoretically 
Motivated Reduced Order Model for the Control of Dynamic Biped Locomotion. Journal of Dynamic Systems, 
Measurement, and Control, 109, 1987, pp. 155­ 163. [9] J. FURUSHO and A. SANO. Sensor-Based Control of 
a Nine-Link Ro­bot. The International Journal of Robotics Research, 9, 2, 1990, pp. 83-98. [10] M. GIRARD. 
Interactive Design Of Computer-animated Legged Ani­mal Motion. IEEE Computer Graphics and Applications, 
7, 6, June, 1987, pp. 39-51. [11] C. L. GOLLIDAY and H. HEMAMI. An Approach to Analyzing Biped Locomotion 
Dynamics and Designing Robot Locomotion Controls. IEEE Transactions on Automatic Control, AC-22, 6, 1970, 
pp. 963­ 972. [12] R. GRZESZCZUK and D. TERZOPOULOS. Automated Learning of Muscle-Actuated Locomotion 
Through Control Abstraction. Proceed­ings of SIGGRAPH 95 (Los Angeles, California, August 1995). In Computer 
Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 63-70. [13] H. M. HMAM and D. 
A. LAWRENCE. Robustness Analysis of Nonlin­ear Biped Control Laws Via Singular Perturbation Theory. Proceed­ings 
of the 31st IEEE Conference on Decision and Control, 1992, pp. 2656-2661. [14] J. K. HODGINS ET AL. Animating 
Human Athletics. Proceedings of SIGGRAPH 95 (Los Angeles, August, 1995). In Computer Graphics Proceedings, 
Annual Conference Series, 1995, ACM SIGGRAPH, pp. 71-78. [15] R. KATOH and M. MORI. Control Method of 
Biped Locomotion Giv­ing Asymptotic Stability Of Trajectory. Automatica, 20, 1984, pp. 405-414. [16] 
H. KO and N. I. BADLER. Straight Line Walking Animation Based on Kinematic Generalization that Preserves 
the Original Characteristics. Proceedings of Graphics Interface '93, 1993, pp. 9-16. [17] D. E. KODITSCHEK 
and M. BÜHLER. Analysis Of A Simplified Hop­ping Robot. The International Journal of Robotics Research, 
10, 6, 1991, pp. 587-605. [18] J. F. LASZLO. Controlling Bipedal Locomotion for Computer Anima­tion, 
M.A.Sc. thesis, University of Toronto, 1996. URL: <www.dgp.utoronto.ca/~jflaszlo> [19] J. M. LIN and 
K. W. HAN. Reducing The Effects Of Model Reduction On Stability Boundaries And Limit-Cycle Characteristics. 
IEEE Transactions on Automatic Control, AC-31, 6, 1986, pp. 567-569. [20] T. MCGEER. Passive Dynamic 
Walking. The International Journal of Robotics Research, 9, 2, 1990, pp. 62-82. [21] T. MCGEER. Passive 
Walking with Knees. Proceedings of IEEE Inter­national Conference on Robotics and Automation, 1990, pp. 
1640­1645. [22] M. MCKENNA and D. ZELTZER. Dynamic Simulation of Autono­mous Legged Locomotion. Proceedings 
of SIGGRAPH 90 (1990). In Computer Graphics (1991), pp. 29-38. [23] H. MIURA and I. SHIMOYAMA. Dynamic 
Walk Of A Biped. Interna­tional Journal of Robotics Research, Summer 1984, pp. 60-74. [24] J. T. NGO 
and J. MARKS. Spacetime Constraints Revisited. Proceed­ings of SIGGRAPH 93 (1993). In Computer Graphics 
Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 343-350. [25] M. H. RAIBERT. Legged Robots 
that Balance. MIT Press, 1986. [26] M. H. RAIBERT and J. K. HODGINS. Animation Of Dynamic Legged Locomotion. 
Proceedings of SIGGRAPH 91 (1991). In Computer Graphics, 1991, pp. 349-358. [27] SYMBOLIC DYNAMICS INC. 
SD/Fast User's Manual, 1990. [28] K. SIMS. Evolving Virtual Creatures. Proceedings of SIGGRAPH 94 (Orlando, 
Florida, July, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, 
pp. 15-22. [29] A. J. STEWART and J. F. CREMER. Beyond Keyframing: An Algorith­mic Approach to Animation. 
Proceedings of Graphics Interface '92, 1992, pp. 273-281. [30] M. UNUMA, K. ANJYO and R. TAKEUCHI. Fourier 
Principles for Emotion-based Human Figure Animation. Proceedings of SIG-GRAPH 95 (Los Angeles, California, 
August, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIG-GRAPH, pp. 91-96. 
[31] M. VAN DE PANNE and E. FIUME. Sensor-Actuator Networks. Pro­ceedings of SIGGRAPH 93, (1993). In 
Computer Graphics Proceed­ings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 335­ 342. [32] M. VAN 
DE PANNE, R. KIM and E. FIUME. Virtual Wind-up Toys for Animation. Proceedings of Graphics Interface 
'94, 1994, pp. 208-215. [33] H. G. VISSER and J. SHINAR. First-Order Corrections In Optimal Feedback 
Control Of Singularly Perturbed Nonlinear Systems. IEEE Transactions on Automatic Control, AC-31, 5, 
1986, pp. 387-393. [34] VUKOBRATOVIC ET AL. Biped Locomotion: Dynamics, Stability, Control and Applications, 
Springer Verlag, 1990. [35] A. WITKIN and Z. POPOVI´C. Motion Warping. Proceedings of SIG-GRAPH 95 (Los 
Angeles, California, August, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995 
ACM SIG-GRAPH, pp. 105-107. [36] W. L. WOOTEN and J. K. HODGINS. Simulation Of Human Diving. Proceedings 
of Graphics Interface '95, 1995, pp. 1-9.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237238</article_id>
		<sort_key>163</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[SKETCH]]></title>
		<subtitle><![CDATA[an interface for sketching 3D scenes]]></subtitle>
		<page_from>163</page_from>
		<page_to>170</page_to>
		<doi_number>10.1145/237170.237238</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237238</url>
		<keywords>
			<kw><![CDATA[3D modeling]]></kw>
			<kw><![CDATA[direct manipulation]]></kw>
			<kw><![CDATA[gestural interface]]></kw>
			<kw><![CDATA[interaction techniques]]></kw>
			<kw><![CDATA[nonphotorealistic rendering]]></kw>
			<kw><![CDATA[sketching]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Input devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Modeling packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010391</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011070</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Application specific development environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P246989</person_id>
				<author_profile_id><![CDATA[81100099358]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Zeleznik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University site of the NSF Science and Technology Center for Computer Graphics and Scientific Visualization, PO Box 1910, Providence, RI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P160097</person_id>
				<author_profile_id><![CDATA[81332503677]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Herndon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University site of the NSF Science and Technology Center for Computer Graphics and Scientific Visualization, PO Box 1910, Providence, RI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024462</person_id>
				<author_profile_id><![CDATA[81100166298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University site of the NSF Science and Technology Center for Computer Graphics and Scientific Visualization, PO Box 1910, Providence, RI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Akeo, H. Hashimoto, T. Kobayashi, and T. Shibusawa. Computer graphics system for reproducing three-dimensional shape from idea sketch. Eurographics '94 Proceedings, 13(3):477-488, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Artifice, Inc. Design Workshop. Macintosh application.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192496</ref_obj_id>
				<ref_obj_pid>192426</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T. Baudel. A mark-based interaction paradigm for free-hand drawing. UIST '94 Proceedings, pages 185-192, Nov. 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91446</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[E.A. Bier. Snap-dragging in three dimensions. Computer Graphics (1990 Symposium on Interactive 3D Graphics), 24(2):193-204, Mar. 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[V. Branco, A. Costa, and F.N. Ferriera. Sketching 3D models with 2D interaction devices. Eurographics '94 Proceedings, 13(3):489-502, 1994.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199427</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[R. Bukowski and C. S~quin. Object associations: A simple and practical approach to virtual 3D manipulation. Computer Graphics (1995 Symposium on Interactive 3D Graphics), pages 131-138, Apr. 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147182</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. Butterworth, A. Davidson, S. Hench, and T.M. Olano. 3DM: A three dimensional modeler using a head-mounted display. Computer Graphics (1992 Symposium on Interactive 3D Graphics), 25(2):135-138, Mar. 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378497</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Chen, S. Joy Mountford, and Abigail Sellen. A study in interactive 3-D rotation using 2-D control devices. Computer Graphics (SIGGRAPH '88 Proceedings), 22(4):121-129, August 1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Clowes. On seeing things. Artificial Intelligence, (2):79-116, 1971. North-Holland.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>210087</ref_obj_id>
				<ref_obj_pid>210079</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Deering. Holosketch: A virtual reality sketching/animation tool. ACM Transactions on Computer-Human Interaction, 2(3):220-238, 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122747</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[T. Galyean and J. Hughes. Sculpting: An interactive volumetric modeling technique. Computer Graphics (SIGGRAPH '91 Proceedings), 25(4):267-274, July 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147194</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Gleicher. Integrating constraints and direct manipulation. Computer Graphics (1992 Symposium on Interactive 3D Graphics), 25(2):171-174, March 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142622</ref_obj_id>
				<ref_obj_pid>142621</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[K.P. Herndon, R.C. Zeleznik, D.C. Robbins, D.B. Conner, S.S. Snibbe, and A.van Dam. Interactive shadows. UIST '92 Proceedings, pages 1-6, Nov. 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>159731</ref_obj_id>
				<ref_obj_pid>159730</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D. Kurlander and S. Feiner. Inferring constraints from multiple snapshots. ACM Transactions on Graphics, 12(4):277-304, Oct. 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949541</ref_obj_id>
				<ref_obj_pid>949531</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D. Lamb and A. Bandopadhay. Interpreting a 3D object from a rough 2D line drawing. Visualization '90 Proceedings, pages 59-66, 1990.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>223910</ref_obj_id>
				<ref_obj_pid>223904</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J.A. Landay and B.A. Myers. Interactive sketching for the early stages of user interface design. Proceedings of CHI'95, pages 43-50, 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618268</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Lansdown and S. Schofield. Expressive rendering: A review of nonphotorealistic techniques. IEEE Computer Graphics &amp; Applications, pages 29-37, May 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97898</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J.D. Mackinlay, S.K. Card, and G.G. Robertson. Rapid controlled movement through a virtual 3d workspace. In Proceedings of the 1986 Workshop on Interactive 3D Graphics, pages 171-176, October 1986.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[G. Magnan. Using technical art: An industry guide. John Wiley and Sons, Inc., 1970.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147178</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[D. Pugh. Designing solid objects using interactive sketch interpretation. Computer Graphics (1992 Symposium on Interactive 3D Graphics), 25(2):117-126, Mar. 1992.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122753</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[D. Rubine. Specifying gestures by example. Computer Graphics (SIGGRAPH '91 Proceedings), 25(4):329-337, July 1991.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617702</ref_obj_id>
				<ref_obj_pid>616020</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[E. Sachs, A. Roberts, and D. Stoops. 3-draw: A tool for designing 3D shapes. IEEE Computer Graphics &amp; Applications, pages 18-25, Nov. 1991.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Salisbury, S. Anderson, R. Barzel, and D. Salesin. Interactive pen-and-ink illustration. Computer Graphics (SIGGRAPH '94 Proceedings), pages 101-108, July 1994.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[S. Sistare. Graphical interaction techniques in constraint-based geometric modeling. Proceedings of Graphics Interface '91, pages 85-92, June 1991.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134091</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[S.S. Snibbe, K.P. Herndon, D.C. Robbins, D.B. Conner, and A. van Dam. Using deformations to explore 3D widget design. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):351-352, July 1992.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134094</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[J.M. Snyder and J.T. Kajiya. Generative modeling: A symbolic system for geometric modeling. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):369-378, July 1992.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134089</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[P. Strauss and R. Carey. An object-oriented 3D graphics toolkit. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):341-349, July 1992.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[W. Wang and G. Grinstein. A survey of 3D solid reconstruction from 2D projection line drawings. Computer Graphics Forum, 12(2):137-158, June 1993.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617750</ref_obj_id>
				<ref_obj_pid>616023</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[L.R. Wanger, J.A. Ferwerda, and D.P. Greenberg. Perceiving spatial relationships in computer-generated images. IEEE Computer Graphics and Applications, 12(3):44-58, May 1992.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237244</article_id>
		<sort_key>171</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[OBBTree]]></title>
		<subtitle><![CDATA[a hierarchical structure for rapid interference detection]]></subtitle>
		<page_from>171</page_from>
		<page_to>180</page_to>
		<doi_number>10.1145/237170.237244</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237244</url>
		<keywords>
			<kw><![CDATA[collision detection]]></kw>
			<kw><![CDATA[contacts]]></kw>
			<kw><![CDATA[hierarchical data structure]]></kw>
			<kw><![CDATA[physically-based modeling]]></kw>
			<kw><![CDATA[shape approximation]]></kw>
			<kw><![CDATA[virtual prototyping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Trees</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10003634</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Languages</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39072503</person_id>
				<author_profile_id><![CDATA[81100288313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gottschalk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40035956</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC and U.S. Army Research Office]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40036029</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>176968</ref_obj_id>
				<ref_obj_pid>176962</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A.Garica-Alonso, N.Serrano, and J.Flaquer. Solving the collision detection problem. IEEE Computer Graphics and Applications, 13(3):36-43,1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>94794</ref_obj_id>
				<ref_obj_pid>94788</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Arvo and D. Kirk. A survey of ray tracing acceleration techniques. In An Introduction to Ray Tracing, pages 201-262,1989.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97881</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. Baraff. Curved surfaces and coherence for non-penetrating rigid body simulation. ACM Computer Graphics, 24(4):19-28,1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[B. Barber, D. Dobkin, and H. Huhdanpaa. The quickhull algorithm for convex hull. Technical Report GCG53, The Geometry Center, MN, 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>98741</ref_obj_id>
				<ref_obj_pid>93597</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[N. Beckmann, H. Kriegel, R. Schneider, and B. Seeger. The r*-tree: An efficient and robust access method for points and rectangles. Proc. SIGMOD Conf. on Management of Data, pages 322-331,1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[S. Cameron. Collision detection by four-dimensional intersection testing. P~vceedings of International Conference on Robotics and Automation, pages 291- 302, 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>112537</ref_obj_id>
				<ref_obj_pid>112515</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[S. Cameron. Approximation hierarchies and s-bounds. In P1vceedings. Symposium on Solid Modeling Foundations and CAD~CAM Applications, pages 129-137, Austin, TX, 1991.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>11790</ref_obj_id>
				<ref_obj_pid>11783</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J.F. Canny. Collision detection for moving polyhedra. IEEE Trans. PAMI, 8:200-209,1986.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>24036</ref_obj_id>
				<ref_obj_pid>7531</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[B. Chazelle and D. R Dobkin. Intersection of convex objects in two and three dimensions. J. ACM, 34:1-27,1987.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199437</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. Cohen, M. Lin, D. Manocha, and M. Ponamgi. I-collide: An interactive and exact collision detection system for large-scale environments. In P~vc. of ACM Interactive 3D Graphics Conference, pages 189-196,1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[R.O. Duda and RE. Hart. Pattern Classification and Scene Analysis. John Wiley and Sons, 1973.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134027</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Tom Duff. Interval arithmetic and recursive subdivision for implicit functions and constructive solid geometry. ACM Computer Graphics, 26(2):131-139, 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166158</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J. Snyder et. al. Interval methods for multi-point collisions between time dependent curved surfaces. In P1vceedings ofACM Siggraph, pages 321-334, 1993.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[E.G. Gilbert, D. W. Johnson, and S. S. Keerthi. A fast procedure for computing the distance between objects in three-dimensional space. IEEE J. Robotics and Automation, vol RA-4:193-203,1988.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[S. Gottschalk. Separating axis theorem. Technical Report TR96-024, Department of Computer Science, UNC Chapel Hill, 1996.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>180902</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[N. Greene. Detecting intersection of a rectangular solid and a convex polyhedron. In Graphics Gems IV, pages 74-82. Academic Press, 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378530</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J.K. Hahn. Realistic animation of rigid bodies. Computer Graphics, 22(4):pp. 299-308,1988.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. Held, J.T. Klosowski, and J.S.B. Mitchell. Evaluation of collision detection methods for virtual reality fly-throughs. In Canadian Conference on Computational Geometry, 1995.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97883</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[B. V. Herzen, A. H. Barr, and H. R. Zatz. Geometric collisions for timedependent parametric surfaces. Computer Graphics, 24(4) :39-48,1990.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[R M. Hubbard. Interactive collision detection. In P1vceedings oflEEE Symposium on Resealvh Frontiers in Virtual Reality, October 1993.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>920962</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[M.C. Lin. Efficient Collision Detection for Animation and Robotics. PhD thesis, Department of Electrical Engineering and Computer Science, University of California, Berkeley, December 1993.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M.C. Lin and Dinesh Manocha. Fast interference detection between geometric models. The Visual Computer, 11(10):542-561,1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Moore and J. Wilhelms. Collision detection and response for computer animation. Computer Graphics, 22(4):289-298,1988.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97892</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[B. Naylor, J. Amanatides, and W. Thibault. Merging bsp trees yield polyhedral modeling results. In P1vc. ofACM Siggraph, pages 115-124,1990.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. O'Rourke. Finding minimal enclosing boxes. Internat. J. Comput. Info~. Sci., 14:183-199,1985.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218076</ref_obj_id>
				<ref_obj_pid>218013</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[M. Ponamgi, D. Manocha, and M. Lin. Incremental algorithms for collision detection between general solid models. In P1vc. of ACM/Siggraph Symposium on Solid Modeling, pages 293-304,1995.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>4333</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[F.R Preparata and M. I. Shamos. Computational Geometry. Springer-Verlag, New York, 1985.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[S. Quinlan. Efficient distance computation between non-convex objects. In P1vceedings of International Conference on Robotics and Automation, pages 3324-3329,1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[A. Rappoport. The extended convex differences tree (ecdt) representation for n-dimensional polyhedra. International Journal of Computational Geometry and Applications, 1 (3):227-41,1991.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807479</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. Rubin and T. Whitted. A 3-dimensional representation for fast rendering of complex scenes. In P1vc. ofACM Siggraph, pages 110-116,1980.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[H. Samet. SpatiaI Data Structures: Quadtree, Octrees and Other Hieralvhical Methods. Addison Wesley, 1989.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>18559</ref_obj_id>
				<ref_obj_pid>18548</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[T.W. Sederberg and S.R. Parry. Comparison of three curve intersection algorithms. Computer-AidedDesign, 18(1):58-63,1986.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>98570</ref_obj_id>
				<ref_obj_pid>98524</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[R. Seidel. Linear programming and convex hulls made easy. In P1vc. 6th Ann. ACM Conf. on Computational Geometry, pages 211-215, Berkeley, California, 1990.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[W.Bouma and G.Vanecek. Collision detection and analysis in a physically based simulation. P1vceedings Eulvgraphics workshop on animation and simulation, pages 191-203,1991.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357335</ref_obj_id>
				<ref_obj_pid>357332</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[H. Weghorst, G. Hooper, and D. Greenberg. Improved computationalmethods for ray tracing. ACM Transactions on Graphics, pages 52-69,1984.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[E. Welzl. Smallest enclosing disks (balls and ellipsoids). Technical Report B 91-09, Fachbereich Mathematik, Freie Universitat, Berlin, 1991.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 OBBTree: A Hierarchical Structure for Rapid Interference Detection S. Gottschalk M. C. Lin. D. Manocha 
Department of Computer Science University of North Carolina Chapel Hill, NC 27599-3175 fgottscha,lin,manochag@cs.unc.edu 
http://www.cs.unc.edu/ geom/OBB/OBBT.html Abstract: We present a data structure and an algorithm for 
ef.cient and exact interference detection amongst com­plex models undergoing rigid motion. The algorithm 
is ap­plicable to all general polygonal models. It pre-computes a hierarchical representation of models 
using tight-.tting oriented bounding box trees (OBBTrees). At runtime, the algorithm traverses two such 
trees and tests for overlaps be­tween oriented bounding boxes based on a separating axis theorem, which 
takes less than 200 operations in practice. It has been implemented and we compare its performance with 
other hierarchical data structures. In particular, it can robustly and accurately detect all the contacts 
between large complex geometries composed of hundreds of thousands of polygons at interactive rates. 
CR Categories and Subject Descriptors: I.3.5 [Com­puter Graphics]: Computational Geometry and Object 
Modeling Additional Key Words and Phrases: hierarchical data structure, collision detection, shape approximation, 
con­tacts, physically-based modeling, virtual prototyping. Introduction The problems of interference 
detection between two or more geometric models in static and dynamic environments are fundamental in 
computer graphics. They are also con­sidered important in computational geometry, solid mod­eling, robotics, 
molecular modeling, manufacturing and computer-simulated environments. Generally speaking, we are interested 
in very ef.cient and, in many cases, real-time algorithms for applications with the following characteri­zations: 
1. Model Complexity: The input models are composed of many hundreds of thousands of polygons. 2. Unstructured 
Representation: The input models are represented as collections of polygons with no topol­ogy information. 
Such models are also known as polygonsoups andtheirboundariesmay havecracks, T-joints, or may have non-manifold 
geometry. No ro­bust techniques are known for cleaning such models.  .Also with U.S. Army Research Of.ce 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 3. Close Proximity: In the actual applications, the mod­els can 
come in close proximity of each other and can have multiple contacts. 4. Accurate Contact Determination: 
The applications need to know accurate contacts between the models (up to the resolution of the models 
and machine precision).  Many applications, like dynamic simulation, physically­based modeling, tolerance 
checking for virtual prototyping, and simulation-based design of large CAD models, have all these four 
characterizations. Currently, fast interference detection for such applications is a major bottleneck. 
Main Contribution: We present ef.cient algorithms for accurate interference detection for such applications. 
They make no assumptions about model representation or the motion. The algorithms compute a hierarchical 
repre­sentation using oriented bounding boxes (OBBs).An OBB is a rectangular bounding box at an arbitrary 
orientation in 3-space. The resulting hierarchical structure is referred to as an OBBTree. The idea of 
using OBBs is not new and many researchers have used them extensively to speed up ray tracing and interference 
detection computations. Our major contributions are: 1. New ef.cient algorithms for hierarchical representa­tion 
of large models using tight-.tting OBBs. 2. Use of a separating axis theorem to check two OBBs in space 
(with arbitrary orientation) for overlap. Based on this theorem, we can test two OBBs for overlap in 
about 100 operations on average. This test is about one order of magnitude faster compared to earlier 
al­gorithms for checking overlap between boxes. 3. Comparison with other hierarchical representations 
based on sphere trees and axis-aligned boundingboxes (AABBs). We show that for many close proximity sit­uations, 
OBBs are asymptotically much faster. 4. Robust and interactive implementation and demon­stration. We 
have applied it to compute all contacts between very complex geometries at interactive rates.  The rest 
of the paper is organized in the following man­ner: We provide a comprehensive survey of interference 
detection methods in Section 2. A brief overview of the algorithm is given in Section 3. We describe 
algorithms for ef.cient computation of OBBTrees in Section 4. Sec­tion 5 presents the separating-axis 
theorem and shows how it can be used to compute overlaps between two OBBs very ef.ciently. We compare 
its performance with hierar­chical representations composed of spheres and AABBs in Section 6. Section 
7 discusses the implementation and per­formance of the algorithms on complex models. In Section 8, we 
discussion possible future extensions.  Previous Work Interference and collision detection problems 
have been extensively studied in the literature. The simplest algo­rithms for collision detection are 
based on using bounding volumes and spatial decomposition techniques in a hier­archical manner. Typical 
examples of bounding volumes include axis-aligned boxes (of which cubes are a special case) and spheres, 
and they are chosen for to the simplicity of .nding collision between two such volumes. Hierar­chical 
structures used for collision detection include cone trees, k-d trees and octrees [31], sphere trees 
[20, 28], R­trees and their variants [5], trees based on S-bounds [7] etc. Other spatial representations 
are based on BSP s [24] and its extensions to multi-space partitions [34], spatial repre­sentations based 
on space-time bounds or four-dimensional testing [1, 6, 8, 20] and many more. All of these hierarchi­cal 
methods do very well in performing rejection tests", whenever two objects are far apart. However, when 
the two objects are in close proximity and can have multiple contacts, thesealgorithmseitherusesubdivisiontechniques 
or check very large number of bounding volume pairs for potential contacts. In such cases, their performance 
slows down considerably and they become a major bottleneck in the simulation, as stated in [17]. In computational 
geometry, many theoretically ef.cient algorithms have been proposed for polyhedral objects. Most of them 
are either restricted to static environments, convex objects, or only polyhedral objects undergoing rigid 
motion [9]. However, their practical utility is not clear as many of them have not been implemented in 
practice. Other approaches are based on linear programming and comput­ing closest pairs for convex polytopes 
[3, 10, 14, 21, 23, 33] and based on line-stabbing and convex differences for gen­eral polyhedral models 
[18, 26, 29]. Algorithms utilizing spatial and temporal coherence have been shown to be effec­tive for 
large environments represented as union of convex polytopes [10, 21]. However, these algorithms and systems 
are restrictive in terms of application to general polygo­nal models with unstructured representations. 
Algorithms based on interval arithmetic and bounds on functions have been described in [12, 13, 19]. 
They are able to .nd all the contacts accurately. However, their practical utility is not clear at the 
moment. They are currently restricted to objects whose motion can be expressed as a closed form function 
of time, which is rarely the case in most appli­cations. Furthermore, their performance is too slow for 
interactive applications. OBBs have been extensively used to speed up ray-tracing and other interference 
computations [2]. In terms of appli­cation to large models, two main issues arise: how can we compute 
a tight-.tting OBB enclosing a model and how quickly can we test two such boxes for overlap? For polygonal 
models, the minimal volume enclosing bound­ ing box can be computed in O(n 3)time, where nis the number 
of vertices [25]. However, it is practical for only small models. Simple incremental algorithms of linear 
time complexity are known for computing a minimal enclosing ellipsoid for a set of points [36]. The axes 
of the mini­mal ellipsoid can be used to compute a tight-.tting OBB. However, the constant factor in 
front of the linear term for this algorithm is very high (almost 3 x105) and thereby making it almost 
impractical to use for large models. As for ray-tracing, algorithms using structure editors [30] and 
modeling hierarchies [35] have been used to construct hier­archies of OBBs. However, they cannot be directly 
applied to compute tight-.tting OBBs for large unstructured mod­els. A simple algorithm for .nding the 
overlap status of two OBBs tests all edges of one box for intersection with any of the faces of the other 
box, and vice-versa. Since OBBs are convex polytopes, algorithms based on linear program­ming [27] and 
closest features computation [14, 21] can be used as well. A general purpose interference detection test 
between OBBs and convex polyhedron is presented in [16]. Overall, ef.cient algorithms were not known 
for comput­ing hierarchies of tight-.tting OBBs for large unstructured models, nor were ef.cient algorithms 
known for rapidly checking the overlap status of two such OBBTrees. 3 Hierarchical Methods &#38; Cost 
Equa­tion In this section, we present a framework for evaluating hier­archical data structures for interference 
detection and give a brief overview of OBBTrees. The basic cost function was taken from [35], who used 
it for analyzing hierarchical methods for ray tracing. Given two large models and their hierarchical 
representation, the total cost function for inter­ference detection can be formulated as the following 
cost equation: T NvxCv+NpxCp; (1) where T: total cost function for interference detection, Nv: number 
of bounding volume pair overlap tests Cv: cost of testing a pair of bounding volumes for overlap, Np: 
is the number primitive pairs tested for interference, Cp: cost of testing a pair of primitives for interference. 
 Given this cost function, various hierarchical data struc­tures are characterized by: Choice of Bounding 
Volume: The choice is governed by two con.icting constraints: 1. It should .t the original model as tightly 
as possible (to lower Nvand Np). 2. Testing two such volumes for overlap should be as fast as possible 
(to lower Cv).  Simple primitives like spheres and AABBs do very well with respect to the second constraint. 
But they cannot .t some primitives like long-thin oriented polygons tightly. On the other hand, minimal 
ellipsoids and OBBs provide tight .ts, but checking for overlap between them is relatively expensive. 
Hierarchical Decomposition: Given a large model, the tree of bounding volumes may be constructed bottom-up 
or top-down. Furthermore, different techniques are known for decomposing or partitioning a bounding volume 
into two or more sub-volumes. The leaf-nodes may correspond to different primitives. For general polyhedral 
models, they may be represented as collection of few triangles or convex polytopes. The decomposition 
also affects the values of Nv and Npin (1). It is clear that no hierarchical representation gives the 
best performance all the times. Furthermore, given two models, the total cost of interference detection 
varies considerably with relative placement of the models. In particular, when two models are far apart, 
hierarchical representations based on spheres and AABBs work well in practice. However, when two models 
are in close proximity with multiple num­ber of closest features, the number of pair-wise bounding volume 
tests, Nvincreases, sometimes also leading to an increase in the number pair-wise primitive contact tests, 
Np. For a given model, Nvand Npfor OBBTreestend to be smaller as compared to those of trees using spheres 
or AABBs as bounding volumes. At the same time, the best known earlier algorithms for .nding contact 
status of two OBBs were almost two orders of magnitude slower than checking two spheres or two AABBs 
for overlap. We present ef.cient algorithms for computing tight .tting OBBs given a set of polygons, 
for constructing a hierar­chy of OBBs, and for testing two OBBs for contact. Our algorithms are able 
to compute tight-.tting hierarchies ef­fectively and the overlap test between two OBBs is one order of 
magnitude faster than best known earlier methods. Given suf.ciently large models, our interference detection 
algorithm based on OBBTrees much faster as compared to using sphere trees or AABBs.  4 Building an OBBTree 
In this section we describe algorithms for building an OBB-Tree. The tree construction has two components: 
.rst is the placement of a tight .tting OBB around a collection of polygons, and second is the grouping 
of nested OBB s into a tree hierarchy. We want to approximate the collection of polygons with an OBB 
of similar dimensions and orientation. We triangu­late all polygons composed of more than three edges. 
The OBB computation algorithm makes use of .rst and second order statistics summarizing the vertex coordinates. 
They are the mean, p, and the covariance matrix, C, respectively [11]. If the vertices of the i th triangle 
are the points p i , q i,and r i, then the mean and covariance matrix can be expressed in vector notation 
as: n 1 X ii p (p+q+r i); 3n i.0 X 1 n ii ii ii C(pp+qq+rr);1 :j;k:3 jk jkjkjk 3n i.0 where nis the number 
of triangles, p i p i_p, q i q i_p,and r i r i_p. Each of them is a 3 x1 vector, iiiiT e.g. p(p;p;p)and 
Cjkare the elements of the 3 123 by 3 covariance matrix. The eigenvectors of a symmetric matrix, such 
as C,are mutuallyorthogonal. Afternormalizingthem,theyareused as a basis. We .nd the extremal vertices 
along each axis of this basis, and size the bounding box, oriented with the basis vectors, to bound those 
extremal vertices. Two of the three eigenvectors of the covariance matrix are the axes of maximum and 
of minimum variance, so they will tend to align the box with the geometry of a tube or a .at surface 
patch. The basic failing of the above approach is that vertices on the interior of the model, which ought 
not in.uence the selection of a bounding box placement, can have an arbitrary impact on the eigenvectors. 
For example, a small but very dense planar patch of vertices in the interior of the model can cause the 
bounding box to align with it. We improve the algorithm by using the convex hull of the vertices of the 
triangles. The convex hull is the smallest convex set containing all the points and ef.cient algorithms 
of O(nlg n)complexity and their robust implementations  Figure 1: Building the OBBTree: recursively 
partition the bounded polygons and bound the resulting groups. are available as public domain packages 
[4]. This is an im­provement, but still suffers from a similar sampling prob­lem: a small but very dense 
collection of nearly collinear vertices on the convex hull can cause the bounding box to align with that 
collection. One solution is to sample the surface of the convex hull densely, taking the mean and covariance 
of the sample points. The uniform sampling of the convex hull surface normalizes for triangle size and 
distribution. One can sample the convex hull in.nitely densely by integrating over the surface of each 
triangle, and allowing each differential patch to contribute to the covariance ma­trix. The resulting 
integral has a closed form solution. We let a point x iin the i th triangle be parameterized by sand 
tas in: iiiiii xp+s(q_p)+t(r_p);s;t2[0;1] The mean point of the convex hull is then ZZ X 11 t X 1 n 
1 i1 n1 ii p xdsdt(p+q+r i) nmi 6nmi 00 i.1i.1 i1 ii where marea of i th triangle 2 j(q_p i)x(r_p i)j: 
The elements of the covariance matrix Chave the following closed-form, n X iiiiii Cjk 1 m i[(pj+qj+rj)(pk+qk+rk) 
24n i.1 ii ii ii +p+q+r];1 :j;k:3 jpkjqkjrk iiiiii where pp_p, qq_p,and rr_p. Givenan algorithmtocomputetight-.ttingOBBsaround 
a group of polygons, we need to represent them hierarchi­cally. Most methods for building hierarchies 
fall into two categories: bottom-up and top-down. Bottom-up methods begin with a bounding volume for 
each polygon and merge volumes into larger volumes until the tree is complete. Top­down methods begin 
with a group of all polygons, and re­cursively subdivide until all leaf nodes are indivisible. In our 
current implementation, we have used a simple top­down approach. Our subdivision rule is to split the 
longest axis of a box with a plane orthogonal to one of its axes, partitioning the polygons according 
to which side of the plane their center point lies on (a 2-D analog is shown in Figure 1). The subdivision 
coordinate along that axis was chosen to be that of the mean point, p;of the vertices. If the longest 
axis cannot not be subdivided, the second longest axis is chosen. Otherwise, the shortest one is used. 
If the group of polygons cannot be partitioned along any axis by this criterion, then the group is considered 
indivisible. If we choose the partition coordinate based on where the median center point lies, then 
we obtain balanced trees. This arguably results in optimal worst-case hierarchies for collision detection. 
It is, however, extremely dif.cult to evaluate average-case behavior, as performance of collision detection 
algorithms is sensitive to speci.c scenarios, and no single algorithm performs optimally in all cases. 
Given a model with ntriangles, the overall time to build the tree is O(nlg2 n)if we use convex hull, 
and O(nlg n) if we don t. The recursion is similar to that of quicksort. Processing .tting a box to a 
group of ntriangles partitioning them into two subgroups takes O(nlg n)with convex hull and O(n)without 
it. Applying the process recursively creates a tree with leaf nodes O(lg n)levels deep.  5 Fast Overlap 
Test for OBBs Given OBBTrees of two objects, the interference algorithm typically spends most of its 
time testing pairs of OBBs for overlap. A simple algorithm for testing the overlap status for two OBB 
s performs 144 edge-face tests. In practice, it is an expensive test. Other algorithms based on linear 
programming and closest features computation exist. In this section, we present a new algorithm to test 
such boxes for overlap. One trivial test for disjointness is to project the boxes onto some axis (not 
necessarily a coordinate axis) in space. This is an axial projection. Under this projection, each box 
forms an interval on the axis. If the intervals don t overlap, then the axis is called a separating axis 
for the boxes, and the boxes must then be disjoint. If the intervals do overlap, then the boxes may or 
may not be disjoint further tests may be required. How many such tests are suf.cient to determine the 
con­tact status of two OBBs? We know that two disjoint convex polytopes in 3-space can always be separated 
by a plane which is parallel to a face of either polytope, or parallel to an edge from each polytope. 
A consequence of this is that two convex polytopes are disjoint iff there exists a separating axis orthogonal 
to a face of either polytope or orthogonal to an edge from each polytope. A proof of this basic theorem 
is given in [15]. Each box has 3 unique face orientations, and 3 unique edge directions. This leads to 
15 potential separating axes to test (3 faces from one box, 3 faces from the other box, and 9 pairwise 
combinations of edges). If the polytopes are disjoint, then a separating axis exists, and one of the 
15 axes mentioned above will be a separating axis. If the polytopes are overlapping, then clearly no 
separating axis exists. So, testing the 15 given axes is a suf.cient test for determining overlap status 
of two OBBs. To perform the test, our strategy is to project the centers of the boxes onto the axis, 
and also to compute the radii of the intervals. If the distance between the box centers as projected 
onto the axis is greater than the sum of the radii, then the intervals (and the boxes as well) are disjoint. 
This is shown in2D inFig. 2. We assume we are given two OBBs, Aand B, with B placed relative to Aby rotation 
Rand translation T.The half-dimensions (or radii ) of Aand Bare aiand bi,where i1;2;3. We will denote 
the axes of Aand Bas the unit  Figure 2: Lis a separating axis for OBBs Aand B because Aand Bbecome 
disjoint intervals under projection onto L. vectors Aiand Bi,for i1;2;3. These will be referred to as 
the 6 box axes. Note that if we use the box axes of A as a basis, then the three columns of Rare the 
same as the three Bivectors. The centers of each box projects onto the midpoint of its interval. By projecting 
the box radii onto the axis, and summing the length of their images, we obtain the radius of the interval. 
If the axis is parallel to the unit vector L,then the radius of box A s interval is X rA jaiAi.Lj i 
A similar expression is used for rB. The placement of the axis is immaterial, so we assume it passes 
through the center of box A. The distance between the midpoints of the intervals is jT.Lj.intervals.So, 
the intervals are disjoint iff XX jT.Lj>jaiAi.Lj+jbiBi.Lj ii This simpli.es when Lis a box axis or cross 
product of box axes. For example, consider LA1 xB2.The second term in the .rst summation is 21 22 ja2A.(AxB2)jja2B.(AxA1)j 
ja2B2 .A3j ja2B32j a2jR32j The last step is due to the fact that the columns of the rotation matrix 
are also the axes of the frame of B.The original term consisted of a dot product and cross product, butreducedtoamultiplicationandanabsolutevalue. 
Some terms reduce to zero and are eliminated. After simplifying all the terms, this axis test looks like: 
jT3R22 _T2R32j>a2jR32j+a3jR22j+b1jR13j+b3jR11j All 15 axis tests simplify in similar fashion. Among all 
the tests, the absolute value of each element of Ris used four times, so those expressions can be computed 
once before beginning the axis tests. The operation tally for all 15 axis tests are shown in Table 1. 
If any one of the expressions is satis.ed, the boxes are known to be disjoint, and the remainder of the 
15 axis tests are unnecessary. This permits early exit from the series of tests, so 200 operations is 
the absolute worst case, but often much fewer are needed. Degenerate OBBs: When an OBB bounds only a 
single polygon, it will have zero thickness and become a rectan­gle. In cases where a box extent is known 
to be zero, the expressions for the tests can be further simpli.ed. The oper­ation counts for overlap 
tests are given in Table 1, including when one or both boxes degenerate into a rectangle. Fur­ther reductions 
are possible when a box degenerates to a line segment. Nine multiplies and ten additions are eliminated 
for every zero thickness. OBBs with in.nite extents: Also, when one or more extents are known to be in.nite, 
as for a fat ray or plane, certain axis tests require a straight-forward modi.cation. For the axis test 
given above, if a2 is in.nite, then the inequality cannot possibly be satis.ed unless R32 is zero, in 
which case the test proceeds as normal but with the a2jR32jterm removed. So the test becomes, R32 0 and 
jT3R22 _T2R32j>a3jR22j+b1jR13j+b3jR11j In general, we can expect that R32 will not be zero, and using 
a short-circuit and will cause the more expensive inequality test to be skipped. Operation Box-Box Box-Rect 
Rect-Rect compare add/sub mult abs 15 60 81 24 15 50 72 24 15 40 63 24 Table 1: Operation Counts for 
Overlap Tests Comparisons: We have implemented the algorithm and compared its performance with other 
box overlap al­gorithms. The latter include an ef.cient implementation of closest features computation 
between convex polytopes [14] and a fast implementationof linear programming based on Seidel s algorithm 
[33]. Note that the last two implemen­tations have been optimized for general convex polytopes, but not 
for boxes. All these algorithms are much faster than performing 144 edge-face intersections. We report 
the average time for checking overlap between two OBBs in Table 2. All the timings are in microseconds, 
computed on a HP 735/125 . Sep. Axis Algorithm Closest Features Linear Programming 5 _7us 45 _105 us 
180 _230 us Table 2: Performance of Box Overlap Algorithms   OBB s vs. other Volumes The primary motivation 
for using OBBs is that, by virtue of their variable orientation, they can bound geometry more tightly 
than AABBTrees and sphere trees. Therefore, we reason that, all else being the same, fewer levels of 
an OBB-Tree need to be be traversed to process a collision query for objects in close proximity. In this 
section we present an analysis of asymptotic performance of OBBTrees versus AABBTrees and sphere trees, 
and an experiment which supports our analysis. In Fig. 9(at the end), we show the different levels of 
hierarchies for AABBTrees and OBBTrees while approxi­mating a torus. The number of bounding volumes in 
each tree at each level is the same. The ffor OBBTrees is much smaller as compared to ffor the AABBTrees. 
First, we de.ne tightness, diameter,and aspect ratio of a bounding volume with respect to the geometry 
it covers. The tightness, T, of a bounding volume, B, with respect to the geometry it covers, G,is B 
s Hausdorff distance from G. Formally, thinking of Band Gas closed point sets, this is Tmax min dist(b;g) 
b2Bg2G The diameter, d, of a bounding volume with respect to the bounded geometry is the maximum distance 
among all pairs of enclosed points on the bounded geometry, dmax dist(g;h) g;h2G The aspect ratio, p, 
of a bounding volume with respect to bounded geometry is pT/d. d Figure 3: Aspect ratios of parent volumes 
are similar to those of children when bounding nearly .at geometry. We argue that when bounded surfaces 
have low curva­ture, AABBTrees and sphere trees form .xed aspect ratio hierarchies, in the sense that 
the aspect ratio of a node in the hierarchy will have an aspect ratio similar to its children. This is 
illustrated in Fig. 3 for plane curves. If the bounded geometry is nearly .at, then the children will 
have shapes similar to the parents, but smaller. In Fig 3 for both spheres and AABBs, dand Tare halved 
as we go from parents to children, so pd/Tis approximately the same for both parent and child. For .xed 
aspect ratio hierarchies, Thas linear dependence on d. Note that the aspect ratio for AABBs is very dependent 
on the speci.c orientation of the bounded geometry if the geometry is conveniently aligned, the aspect 
ratio can be close to 0, whereas if it is inconveniently aligned, pcan be close to 1. But whatever the 
value, an AABB enclosing nearly .at geometry will have approximately the same pas its children. Since 
an OBB aligns itself with the geometry, the aspect ratio of an OBB does not depend on the geometry s 
orien­tation in model space. Rather, it depends more on the local curvature of the geometry. For the 
sake of analysis, we are assuming nearly .at geometry. Suppose the bounded geometry has low constant 
curvature, as on the surface of a large sphere. In Fig. 4 we show a plane curve of .xed radius of curvature 
rand bounded by an OBB. We have d2rsin e,and Tr_rcos e. Using the small angle approximation and eliminating 
e, we obtain Td2/8r.So ee  rd . d  Figure 4: OBBs: Aspect ratio of children are half that of parent 
when bounding surfaces of low constant curvature when bounding nearly .at geometry. Thas quadratic dependence 
on d.When dis halved, Tis quartered, and the aspect ratio is halved. We conclude that when bounding low 
curvature surfaces, AABBTrees and spheres trees have Twith linear depen­dence on d, whereas OBBTrees 
have Twith quadratic de­pendence on d. We have illustrated this for plane curves in the .gures, but the 
relationships hold for surfaces in three space as well. Suppose we use Nsame-sized bounding volumes to 
cover a surface patch with area Aand require each volume to cover O(A/N)surface area (for simplicity 
we are ignor­ing packing inef.ciencies). Therefore, for these volumes, p dO(A/N). For AABBs and spheres, 
Tdepends p linearly on d,so TO(A/N). For OBBs, quadratic de­pendence on dgivesusOBBs, TO(A/N). So, to 
cover a surface patch with volumes to a given tightness, if OBBs re­quire O(m)bounding volumes, AABBs 
and spheres would require O(m 2)bounding volumes. Most contact scenarios do not require traversing both 
trees to all nodes of a given depth, but this does happen when two surfaces come into parallel close 
proximity to one another, in which every point on each surface is close to some point on the other surface. 
This is most common in virtual prototyping and tolerance analysis applications, in which .tted machine 
parts are tested for mechanical con­sistency. Also, dynamic simulations often generate paths in which 
one object comes to rest against another. It should be also be noted that when two smooth, highly tessellated 
surfaces come into near contact with each other, the region of near contact locally resembles a parallel 
close proximity scenario in miniature, and, for suf.ciently tessellated mod­els, the expense of processing 
that region can dominate the overall collision query. So, while it may seem like a very special case, 
parallel close proximity is an abstract situation which deserves consideration when designing collision 
and evaluating collision detection algorithms. Experiments: We performed two experiments to support our 
analysis. For the .rst, we generated two concentric spheres consisting of 32;000 triangles each. The 
smaller sphere had radius 1, while the larger had radius 1+f. We performed collision queries with both 
OBBTrees and AABBTrees. The AABBTrees were created using the same process as for OBBTrees, except that 
instead of using the eigenvectors of the covariance matrix to determine the box orientations, we used 
the identity matrix. Figure 5: AABBs (upper curve) and OBBs (lower curve) for parallel close proximity 
(log-log plot) The number of bounding box overlap tests required to process the collision query are shown 
in Fig. 5 for both tree types, and for a range of fvalues. The graph is a log-log plot. The upper curve 
is for AABBTrees, and the lower, OBBTrees. The slopes of the the linear portions the upper curve and 
lower curves are approximately _2and _1, as expected from the analysis. The differing slopes of these 
curves imply that OBBTrees require asymptotically fewer box tests as a function of fthan AABBTrees in 
our experiment. Notice that the curve for AABBTrees levels off for the lowest values of f. For suf.ciently 
small values of f,even the lowest levels of the AABBTree hierarchies are inade­quate for separating the 
two surfaces all nodes of both are visited, and the collision query must resort to testing the triangles. 
Decreasing feven further cannot result in more work, because the tree does not extend further than the 
depth previously reached. The curve for the OBBTrees will also level off for some suf.ciently small value 
of f, which is not shown in the graph. Furthermore, since both trees are binary and therefore have the 
same number of nodes, the OBBTree curve will level off at the same height in the graph as the AABBTree 
curve. For the second experiment, two same-size spheres were placed next to each other, separated by 
a distance of f.We call this scenario point close proximity, where two nonpar­allel surfaces patches 
come close to touching at a point. We can think of the surfaces in the neighborhood of the closest points 
as being in parallel close proximity but this approximation applies only locally. We have not been able 
to analytically characterize the performance, so we rely instead on empirical evidence to claim that 
for this scenario OBBTrees require asymptotically fewer bounding box overlap tests as a function of fthan 
AABBTrees. The results are shown in Fig. 6. This is also a log-log plot, and the increasing gap between 
the upper and lower curves show the asymptotic difference in the number of tests as f decreases. Again, 
we see the leveling off for small values of f. Analysis: A general analysis of the performance of collision 
detection algorithms which use bounding volume hierarchies is extremely dif.cult because performance 
is so situation speci.c. We assumed that the geometry being bounded had locally low curvature and was 
.nely tessel­ Figure 6: AABBs (upper curve) and OBBs (lower curve) for point close proximity. (log-log 
plot) lated. This enabled the formulation of simple relationships between Tand d. We also assumed that 
the packing ef.­ciency of bounding volumes was perfect so as to formulate the relationships between dand 
the area of the surface cov­ered. We believe that the inaccuracies of these assumptions account for the 
deviations from theory exhibited in thegraph of Fig. 5. For surface patches with high curvature everywhere, 
such as a 3D fractal, we may not expect to see asymptotic per­formance advantages for OBBs. Similarly, 
a coarse tessel­lation of a surface will place a natural limit on the number, N, the number of volumes 
used to approximate the surface. For a coarse tessellation, OBB-, sphere-, and AABBTrees may have to 
traverse their entire hierarchies for suf.ciently close proximityscenarios, thusrequiringapproximatelythe 
same number of bounding volume overlap tests. Further­more, for scenarios in which parallel close proximity 
does not occur, we don t expect the quadratic convergence prop­erty of OBBs to be of use, and again don 
t expect to see superior asymptotic performance. Implementation and Performance The software for the 
collision detection library was written in C++. The primary data structure for an OBB is a box class 
whose members contain a rotation matrix and trans­lation vector, de.ning its placement relative to its 
parent, pointers to its parent and two children, the three box dimen­sions, and an object which holds 
a list of the triangles the box contains. The overall data structure for the box occu­pies 168 bytes. 
The tree formed from boxes as nodes, and the triangle list class, are the only compound data structures 
used. An OBBTree of ntriangles contains nleaf boxes and n_1 internal node boxes. In terms of memory require­ments, 
there are approximately two boxes per triangle. The triangle itself requires 9 double precision numbers 
plus an integer for identi.cation, totaling 76 bytes (based on 64-bit IEEE arithmetic). The memory requirement 
therefore to­tals 412 bytes per triangle in the model. This estimate does not include whatever overhead 
may exist in the dynamic memory allocation mechanism of the runtime environment. Using quaternions instead 
of rotation matrices (to represent box orientations), results in substantial space savings, but need 
13 more operations per OBB overlap test. Single precision arithmetic can also be used to save memory. 
7.1 Robustness and Accuracy The algorithm and the implementations are applicable to all unstructured 
polygonal models. The polygons are permit­ted to be degenerate, with two or even one unique vertex, have 
no connectivity restrictions. The algorithm requires no adjacency information. This degree of robustness 
gives the system wider applicability. For example, space curves can be approximated by degenerate triangles 
as line seg­ments the system will correctly .nd intersections of those curves with other curves or surfaces. 
The OBB overlap test is very robust as compared to other OBB overlap algorithms. It does not need to 
check for non­generic conditions such as parallel faces or edges; these are not special cases for the 
test and do not need to be handled separately. As a series of comparisons between linear combinations, 
the test is numerically stable: there are no divisions, square roots, or other functions to threaten 
domain errors or create conditioning problems. The use of an error margin, f, guards against missing 
intersections due to arithmetic error. Its value can be set by the user. Since the .ow of control for 
the overlap test is simple and the number of operations required is small, the overlap test is a good 
candidate for microcoding or implemented in assembly. The test could also be easily implemented in hardware. 
Since most of the collision query time is spent in the overlap tests, any such optimization will signi.cantly 
improve overall running time. The Qhull package [4] is optionally used for computing the OBB orientation. 
It has been found to be quite robust. If we do use Qhull, we have to ensure that the input to Qhull spans 
3 dimensions. If the input is rank de.cient, our current implementation skips the use of Qhull, and uses 
all the triangles in the group. A more complete solution would be to project the input onto a lower dimensional 
space, and compute the convex hull of the projection (Qhull works on input of arbitrary speci.ed dimension, 
but the input must be full rank). There is the issue of propagation of errors as we descend the hierarchies, 
performing overlap tests. When we test two boxes or two triangles, their placement relative to one another 
is the result of a series of transformations, one for each level of each hierarchy we have traversed. 
We have not found errors due to the cascading of transformation matrices, but it is a theoretical source 
of errors we are aware of. 7.2 Performance Our interference detection algorithm has been applied to 
two complex synthetic environments to demonstrate its ef­.ciency (as highlighted in Table 3). These .gures 
are for an SGI Reality Engine (90 MHz R8000 CPU, 512 MB). A simple dynamics engine exercised the collision 
detec­tion system. At each time step, the contact polygons were found by the collision detection algorithm, 
an impulse was applied to the object at each contact before advancing the clock. In the .rst scenario, 
the pipes model was used as both the environment and the dynamic object, as shown in Fig. 8. Both object 
and environment contain 140,000 polygons. The object is 15 times smaller in size than the environ­ment. 
We simulated a gravitational .eld directed toward the center of the large cube of pipes, and permitted 
the smaller cube to fall inward, tumbling and bouncing. Its path contained 4008 discrete positions, and 
required 16:9 Scenario Pipes Torus Environ Size 143690 pgns 98000 pgns Object Size 143690 pgns 20000 
pgns Num of Steps 4008 1298 Num of Contacts 23905 2266 Num of Box-Box Tests 1704187 1055559 Num of Tri-Tri 
Tests 71589 7069 Time 16.9 secs 8.9 secs Ave. Int. Detec. Time 4.2 msecs 6.9 msecs Ave. Time per Box 
Test 7.9 usecs 7.3 usecs Ave. Contacts per Step 6.0 1.7 Table 3: Timings for simulations seconds to 
determine all 23905 contacts along the path. This is a challenging scenario because the smaller object 
is entirely embedded within the larger model. The models contain long thin triangles in the straight 
segments of the pipes, which cannot be ef.ciently approximated by sphere trees, octrees, and AABBTrees, 
in general. It has no obvi­ous groups or clusters, which are typically used by spatial partitioning algorithms 
like BSP s. The other scenario has a complex wrinkled torus encir­cling a stalagmite in a dimpled, toothed 
landscape. Dif­ferent steps from this simulation are shown in Fig. 10. The spikes in the landscape prevent 
large bounding boxes from touching the .oor of the landscape, while the dimples provide numerous shallow 
concavities into which an object can enter. Likewise, the wrinkles and the twisting of the torus makes 
it impractical to decompose into convex poly­topes, and dif.cult to ef.ciently apply bounding volumes. 
The wrinkled torus and the environment are also smooth enough to come into parallel close proximity, 
increasing the number of bounding volume overlap tests. Notice that the average number of box tests per 
step for the torus sce­nario is almost twice that of the pipes, even though the number of contacts is 
much lower. We have also applied our algorithm to detect collision between a moving torpedo on a pivot 
model (as shown in Fig. 7). These are parts of a torpedo storage and handling room of a submarine. The 
torpedo model is 4780 triangles. The pivot structure has 44921 triangles. There are multiple contacts 
along the length of the torpedo as it rests among the rollers. A typical collision query time for the 
scenario shown in Fig. 7 is 100 ms on a 200MHz R4400 CPU, 2GB SGI Reality Engine. 7.3 Comparison with 
Other Approaches A number of hierarchical structures are known in the liter­ature for interference detection. 
Most of them are based on spheres or AABBs. They have been applied to a number of complex environments. 
However, there are no stan­dard benchmarks available to compare different algorithms and implementations. 
As a result, it is non-trivial to com­pare two algorithms and their implementations. More re­cently, 
[18] have compared different algorithms (based on line-stabbing and AABBs) on models composed of tens 
of thousands of polygons. On an SGI Indigo2 Extreme, the algorithms with the best performance are able 
to compute all the contacts between the models in about 1/7 _1/5of a second. Just based on the model 
complexity, we are able to handle models composed of hundreds of thousands of poly­gons (with multiple 
parallel contacts) in about 1/25 _1/75 of a second. We also compared our algorithm with an implementation 
of sphere tree based on the algorithm pre­sented in [28]. A very preliminary comparison indicates one 
order of magnitude improvement. More comparisons and experiments are planned in the near future. 7.4 
RAPID and benchmarks Our implementation of our algorithms is available as a soft­ware package called 
RAPID (Rapid and Accurate Poly­gon Interference Detection). It can be obtained from: http://www.cs.unc.edu/ 
geom/OBB/OBBT.html. Most of the models shown in this paper are also available, as well as precomputed 
motion sequences. Overall, we .nd that given two large models in close proximity, with Cv, Nv,and Npfrom 
the cost equation (1): .Cvfor OBBTrees is one-order of magnitude slower than that for sphere trees or 
AABBs. .Nvfor OBBTrees is asymptotically lower than that for sphere trees or AABBs. Likewise, Npfor OBBTrees 
is asymptotically lower. Thus, given suf.ciently large models in suf.ciently close proximity, using OBBTrees 
require less work to process a collision query than using AABBTrees or sphere trees.  8 Extensions 
and Future Work In the previous sections, we described the algorithm for interference detection between 
two polygonal models un­dergoing rigid motion. Some of the future work includes its specialization and 
extension to other applications. These include ray-tracing, interference detection between curved surfaces, 
view frustum culling and deformable models. As far as curve and surface intersections are concerned, 
current approaches are based on algebraic methods, subdivision methods and interval arithmetic [32]. 
Algebraic methods are restricted to low degree intersections. For high degree curve intersections, algorithms 
based on interval arithmetic have been found to be the fastest [32]. Such algorithms compute a decomposition 
of the curve in terms of AABBs. It will be worthwhile to try OBBs. This would involve sub­dividing the 
curve, computing tight-.tting OBBs for each segment, and checking them for overlaps. In terms of view 
frustum culling, most applications use hierarchies based on AABBs. Rather, we may enclose the object 
using an OBBTree and test for overlap with the view frustum. The overlap test presented in Section 5 
can be easily extended to test for overlap between an OBB and a view frustum. Libraries and Benchmarks: 
There is great need to de­velop a set of libraries and benchmarks to compare different algorithms. This 
would involve different models as well as scenarios. 9Conclusion In this paper, we have presented a hierarchical 
data structure for rapid and exact interference detection between polygo­nal models. The algorithm is 
general-purpose and makes no assumptions about the input model. We have presented new algorithms for 
ef.cient construction of tight-.tting OBB-Trees and overlap detection between two OBBs based on a new 
separating axis theorem. We have compared its perfor­mance with other hierarchies of spheres and AABBs 
and .nd it asymptotically faster for close proximity situations. The algorithm has been implemented and 
is able to detect all contacts between complex geometries (composed of a few hundred thousand polygons) 
at interactive rates. 10 Acknowledgements Thanks to Greg Angelini, Jim Boudreaux, and Ken Fast at Electric 
Boat for the model of torpedo storage and handling room. This work was supported in part by a Sloan foun­dation 
fellowship, ARO Contract P-34982-MA, NSF grant CCR-9319957, NSF grant CCR-9625217, ONR contract N00014-94-1-0738, 
ARPA contract DABT63-93-C-0048, NSF/ARPA Science and Technology Center for Computer Graphics &#38; Scienti.c 
Visualization NSF Prime contract No. 8920219 and a grant from Ford Motor company.  References [1] A.Garica-Alonso, 
N.Serrano, and J.Flaquer. Solving the collision detection problem. IEEE Computer Graphics and Applications, 
13(3):36 43,1994. [2] J. Arvo and D. Kirk. A survey of ray tracing acceleration techniques. In An Introduction 
to Ray Tracing, pages 201 262, 1989. [3] D. Baraff. Curved surfaces and coherence for non-penetrating 
rigid body simulation. ACM Computer Graphics, 24(4):19 28, 1990. [4] B. Barber, D. Dobkin, and H. Huhdanpaa. 
The quickhull algorithm for convex hull. Technical Report GCG53, The Geometry Center, MN, 1993. [5] N. 
Beckmann, H. Kriegel, R. Schneider, and B. Seeger. The r*-tree: An ef.cient and robust access method 
for points and rectangles. Proc. SIGMOD Conf. on Management of Data, pages 322 331, 1990. [6] S. Cameron. 
Collision detection by four-dimensional intersection testing. Pro­ceedingsof InternationalConferenceonRoboticsandAutomation, 
pages 291 302, 1990. [7] S. Cameron. Approximation hierarchies and s-bounds. In Proceedings. Sym­posium 
on Solid Modeling Foundations and CAD/CAM Applications, pages 129 137, Austin, TX, 1991. [8] J. F. Canny. 
Collision detection for moving polyhedra. IEEE Trans. PAMI, 8:200 209, 1986. [9] B. Chazelle and D. P. 
Dobkin. Intersection of convex objects in two and three dimensions. J. ACM, 34:1 27, 1987. [10] J. Cohen, 
M. Lin, D. Manocha, and M. Ponamgi. I-collide: An interactive and exact collision detection system for 
large-scale environments. In Proc. of ACM Interactive 3D Graphics Conference, pages 189 196, 1995. [11] 
R.O. Duda and P.E. Hart. Pattern Classi.cation and Scene Analysis. John Wiley and Sons, 1973. [12] Tom 
Duff. Interval arithmetic and recursive subdivision for implicit functions and constructive solid geometry. 
ACM Computer Graphics, 26(2):131 139, 1992. [13] J. Snyder et. al. Interval methods for multi-point collisions 
between time dependent curved surfaces. In Proceedings of ACM Siggraph,pages 321 334, 1993. [14] E.G.Gilbert,D.W.Johnson,andS.S.Keerthi.Afastprocedureforcomputing 
thedistance betweenobjectsin three-dimensionalspace. IEEE J. Robotics and Automation, vol RA-4:193 203, 
1988. [15] S. Gottschalk. Separating axis theorem. Technical Report TR96-024, Depart­ment of Computer 
Science, UNC Chapel Hill, 1996. [16] N. Greene. Detecting intersection of a rectangular solid and a convex 
polyhe­dron. In Graphics Gems IV, pages 74 82. Academic Press, 1994. [17] J. K. Hahn. Realistic animation 
of rigid bodies. Computer Graphics, 22(4):pp. 299 308, 1988. [18] M. Held, J.T. Klosowski, and J.S.B. 
Mitchell. Evaluation of collision de­tection methods for virtual reality .y-throughs. In Canadian Conference 
on Computational Geometry, 1995. [19] B. V. Herzen, A. H. Barr, and H. R. Zatz. Geometric collisions 
for time­dependent parametric surfaces. Computer Graphics, 24(4):39 48, 1990. [20] P. M. Hubbard. Interactive 
collision detection. In Proceedings of IEEE Sym­posium on Research Frontiers in Virtual Reality, October 
1993. [21] M.C. Lin. Ef.cient Collision Detection for Animationand Robotics.PhD thesis, Department of 
Electrical Engineering and Computer Science, University of California, Berkeley, December 1993. [22] 
M.C. Lin and Dinesh Manocha. Fast interference detection between geometric models. The Visual Computer, 
11(10):542 561, 1995. [23] M. Moore and J. Wilhelms. Collision detection and response for computer animation. 
Computer Graphics, 22(4):289 298, 1988. [24] B. Naylor, J. Amanatides, and W. Thibault. Merging bsp trees 
yield polyhedral modeling results. In Proc. of ACM Siggraph,pages 115 124, 1990. [25] J. O Rourke. Finding 
minimal enclosing boxes. Internat. J. Comput. Inform. Sci., 14:183 199, 1985. [26] M. Ponamgi, D. Manocha, 
and M. Lin. Incremental algorithms for collision detection between general solid models. In Proc.of ACM/SiggraphSymposium 
on Solid Modeling, pages 293 304, 1995. [27] F.P. Preparata and M. I. Shamos. Computational Geometry. 
Springer-Verlag, New York, 1985. [28] S. Quinlan. Ef.cient distance computation between non-convex objects. 
In Proceedings of International Conference on Robotics and Automation, pages 3324 3329, 1994. [29] A. 
Rappoport. The extended convex differences tree (ecdt) representation for n-dimensional polyhedra. International 
Journal of Computational Geometry and Applications, 1(3):227 41, 1991. [30] S. Rubin and T. Whitted. 
A 3-dimensional representation for fast rendering of complex scenes. In Proc. of ACM Siggraph, pages 
110 116, 1980. [31] H. Samet. Spatial Data Structures: Quadtree, Octrees and Other Hierarchical Methods. 
Addison Wesley, 1989. [32] T.W. Sederberg and S.R. Parry. Comparison of three curve intersection algo­rithms. 
Computer-Aided Design, 18(1):58 63, 1986. [33] R.Seidel.Linearprogrammingandconvexhullsmadeeasy.In Proc. 
6th Ann. ACM Conf. on ComputationalGeometry, pages 211 215,Berkeley, California, 1990. [34] W.BoumaandG.Vanecek.Collisiondetectionandanalysisinaphysicallybased 
simulation. ProceedingsEurographicsworkshop on animationand simulation, pages 191 203, 1991. [35] H.Weghorst,G.Hooper,andD.Greenberg.Improvedcomputationalmethods 
for ray tracing. ACM Transactions on Graphics, pages 52 69, 1984. [36] E. Welzl. Smallest enclosing disks 
(balls and ellipsoids). Technical Report B 91-09, Fachbereich Mathematik, Freie Universitat, Berlin, 
1991. Figure 7: Interactive Interference Detection for a Torpedo (shown in yellow) on a Pivot Structure 
(shown in green) Torpedo has 4780 triangles; Pivot has 44921 triangles; Average time to perform collision 
query: 100 msec on SGI Reality Engine with 200MHz R4400 CPU Figure 8: Interactive Interference Detection 
on Com­plex Interweaving Pipeline: 140;000 polygons each; Aver­age time to perform collision query: 4.2 
msec on SGI Reality Engine with 90MHz R8000 CPU Figure 9: AABBs vs. OBBs: Approximation of a Torus 
This Figure 10: Interactive Interference Detection for a Complex shows OBBs converging to the shape of 
a torus more rapidly Torus Torus has 20000 polygons; Environment has 98000 than AABBs. polygons; Average 
time to perform collision query: 6.9 msec on SGI Reality Engine with 90MHz R8000 CPU. High-resolution 
TIFF versions of these images can be found on the CD-ROM in: S96PR/papers/gottscha 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237247</article_id>
		<sort_key>181</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Free-form deformations with lattices of arbitrary topology]]></title>
		<page_from>181</page_from>
		<page_to>188</page_to>
		<doi_number>10.1145/237170.237247</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237247</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>E.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152.10003161</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems->Record storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010070.10010111.10011710</concept_id>
				<concept_desc>CCS->Theory of computation->Theory and algorithms for application domains->Database theory->Data structures and algorithms for data management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011024.10011028</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language features->Data types and structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10010031</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Data structures design and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10002971</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Data structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P248733</person_id>
				<author_profile_id><![CDATA[81100252041]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[MacCracken]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Research Laboratory, Department of Computer Science, University of California, Davis CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P160127</person_id>
				<author_profile_id><![CDATA[81100148606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[I.]]></middle_name>
				<last_name><![CDATA[Joy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Research Laboratory, Department of Computer Science, University of California, Davis CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>808573</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alan H. Barr. Global and local deformations of solid primitives. In Computer Graphics (SIGGRAPH '84 Proceedings), volume 18, pages 21-30, July 1984.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer-Aided Design, 10:350-355, September 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Chaikin. An algorithm for high speed curve generation. Computer Graphics and Image Processing, 3:346-349, 1974.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192220</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yu-Kuang Chang and Alyn E Rockwood. A generalized de Casteljau approach to 3D free-Form deformation. In Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29,1994), Computer Graphics Proceedings, Annual Conference Series, pages 257-260.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97900</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Sabine Coquillart. Extended free-form deformation: A sculpturing tool for 3D geometric modeling. In Computer Graphics (SIGGRAPH '90 Proceedings), volume 24, pages 187-196, August 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122720</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sabine Coquillart and Pierre Janc6ne. Animated free-form deformation: An interactive animation technique. In Computer Graphics (SIGGRAPH '91 Proceedings), volume 25, pages 23-26, July 1991.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Doo. A subdivision algorithm for smoothing down irregularly shaped polyhedrons. In Proceedings of the Int'l Conf. Interactive Techniques in ComputerAided Design, pages 157- 165, 1978.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. Doo and M. Sabin. Behaviour of recursive division surfaces near extraordinary points. Computer-Aided Design, 10:356- 360, September 1978.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Josef Griessmair and Werner Purgathofer. Deformation of solids with trivariate B-splines. In Eurographics '89, pages 137-148. North-Holland, September 1989.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Mark Halstead, Michael Kass, and Tony DeRose. Efficient, fair interpolation using Catmull-Clark surfaces. In Computer Graphics (SIGGRAPH '93 Proceedings), volume 27, pages 35-44, August 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614308</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Bernd Hamann, Donhua Wu, and Robert J. Moorhead II. On particle path generation based on quadrilinear interpolation and Bernstein-B6zier polynomials. IEEE Transactions on Visualization and Computer Graphics, 1(3):210-217, 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hugues Hoppe, Tony DeRose, Tom Duchamp, Mark Halstead, Hubert Jin, John McDonald, Jean Schweitzer, and Werner Stuetzle. Piecewise smooth surface reconstruction. In Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 295-302.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kenneth I. Joy and Ron MacCracken. The refinement rules for Catmull-Clark solids. Technical Report CSE-96-1, Department of Computer Science, University of California, Davis, January 1996.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>833881</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[David N. Kenwright and Davis A. Lane. Optimization oftimedependent particle tracing using tetrahedral decomposition. In Proceedings of Visualization '95, pages 321-328. IEEE Computer Society, 1985.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Charles Loop. Smooth subdivision surfaces based on triangles. Master's thesis, Department of Mathematics, University of Utah, August 1987.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222932</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Mike Lounsbery. Multiresolution Analysis for Smfaces of Arbitra~7 Topological Type. PhD thesis, Department of Computer Science and Engineering, University of Washington, Seattle, WA, June 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27628</ref_obj_id>
				<ref_obj_pid>27625</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[A. Nasri. Polyhedral subdivision methods for free-form surfaces. ACM Transactions on Graphics, 6:29-73, 1987.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R. Riesenfeld. On Chaikin's algorithm. Computer Graphics and Image Processing, 4(3):304-310, 1975.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Thomas W. Sederberg and Scott R. Parry. Free-form deformation of solid geometric models. In Computer Graphics (SIG- GRAPH '86 Proceedings), volume 20, pages 151-160, August 1986.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Kevin J. Weiler. Topological structures for geometric modeling. PhD thesis, Rensselaer Polytechnic Institute, August 1986.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Free-Form Deformations With Lattices of Arbitrary Topology Ron MacCracken Kenneth I. Joy Computer 
Graphics Research Laboratory Department of Computer Science University of California, Davis1 Abstract 
A new free-form deformation technique is presented that general­izes previous methods by allowing 3-dimensional 
deformation lat­tices of arbitrary topology. The technique uses an extension of the Catmull-Clark subdivision 
methodology that successively re.nes a 3-dimensional lattice into a sequence of lattices that converge 
uni­formly to a region of 3-dimensional space. Deformation of the lat­tice then implicitly de.nes a deformation 
of the space. An underly­ing model can be deformed by establishing positions of the points of the model 
within the converging sequence of lattices and then track­ing the new positions of these points within 
the deformed sequence of lattices. This technique allows a greater variety of deformable re­gions to 
be de.ned, and thus a broader range of shape deformations can be generated. 1 Introduction Ef.cient 
and intuitive methods for three-dimensional shape design, modi.cation, and animation are becoming increasingly 
important areas in computer graphics. The model-dependent techniques ini­tially used by designers to 
modify surfaces required each primitive type to have different parameters and/or control points that 
de.ned its shape. Model designers had to consider this mathematical model when making the desired modi.cations, 
and shape design could be dif.cult making simple changes to the surface required the mod­i.cation of 
many surface parameters. The process grew more dif.­cult when local changes, such as adding arbitrarily 
shaped bumps, or global changes, such as bending, twisting, or tapering were nec­essary. The free-form 
deformations [5, 6, 9, 19] were designed to deal with some of these problems. These methods embed an 
object in a deformable region of space such that each point of the object has a unique parameterization 
that de.nes its position in the region. The region is then altered, causing recalculation of the positions 
based upon their initial parameterization. If a deformable space can be de­.ned with great .exibility 
and with few control points relative to 1Department of Computer Science, University of California, Davis 
CA 95616. Email: fmaccrack,joyg@cs.ucdavis.edu Permission to make digital or hard copies of part or all 
of this work or personal or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, 
requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 the 
number in the surface model, then complex models comprised of thousands of control points can be deformed 
in many interesting ways with very little user-interaction. Barr [1] .rst introduced deformations by 
creating operations for stretching, twisting, bending and tapering surfaces around a central axis (x, 
y,or z). Operations that involved moving many control points could now be accomplished with the altering 
of as little as one parameter. However, this technique limits the possible de.ni­tions of the deformable 
space to that of a single coordinate system about an axis, and restricts the ways in which the space 
can be al­tered -the axis can not be modi.ed and the deformable space can only be modi.ed by a few parameters. 
Barr s deformations were followed by a more generalized ap­proach to the problem, the Free-Form Deformations 
(FFDs) of Sederberg and Parry [19]. This method imposes an initial deforma­tion lattice on a parallelepiped, 
and de.nes the deformable space as thetrivariateB´eziervolumede.nedbythelatticepoints. Theparal­lelepiped 
form of the lattice allows points of an embedded object to be quickly parameterized in the space of the 
lattice, and as the lattice is deformed, the deformed points can be calculated by simple sub­stitution 
into the de.ning equations of the trivariate volume. This method is widely used because of its ease of 
use and power to create many types of deformations with little user-interaction. Griessmair and Purgathofer 
[9] extended this technique by utilizing a trivari­ate B-Spline representation. Although both methods 
give the user many controls to alter the deformable space, both Sederberg and Parry s FFDs and Griessmair 
and Purgathofer s deformation tech­niques handle only a speci.c type of space de.nition, that de.ned 
initially by a parallelepiped lattice. In order to generate free-form deformations for a more general 
lattice structure, Coquillart introduced Extended Free-Form Defor­mations (EFFD) [5, 6]. This method 
uses the initial lattice points to de.neanarbitrarytrivariate B´eziervolume,andallowsthecombin­ing of 
many lattices to form arbitrary shaped spaces. Modifying the points of the de.ning lattice creates a 
deformation of space where one trivariate volume is deformed into another. This extension al­lows a greater 
inventory of deformable spaces, but loses some of the .exibility and stability of Sederberg and Parry 
s FFDs: While the corner control points of the joined lattices are user-controllable, the internal control 
points are constrained to preserve continuity; and, calculating the parameterization of a point embedded 
in the initial trivariate volume requires numerical techniques. A recent deformation technique developed 
by Chang and Rock­wood [4] generalizes Barr s technique in a different manner. In­stead of de.ning the 
space in a free-form manner, Chang s approach deals with increasing the .exibility of an axis-based approach 
by al­lowing modi.cations to the axis during the deformation. The tech­nique allows the user to de.ne 
the axis as a B´ezier curve with two user-de.ned axes at each control point of the curve. Repeated af.ne 
transformations using a generalized deCasteljau approach are used to de.ne the deformable space. This 
technique is very powerful, in­tuitive, and ef.cient, but again restricts the ways in which the space 
surrounding the curve can be altered. This paper introduces a further extension to these techniques by 
establishing deformation methods de.ned on lattices of arbitrary topology. In this case, the deformable 
space is de.ned by using a volume analogy of subdivision surfaces [2, 7, 8]. In these subdivi­sion methods, 
the lattice is repeatedly re.ned, creating a sequence of lattices that converge to a region in three-dimensional 
space. This re.nement procedure is used to de.ne a pseudo-parameterization of an embedded point. As the 
points of the lattice are modi.ed a de­formation of the space is created, and the embedded points can 
be relocated within the deformed space. This method has been found to be quite intuitive for the designer 
and dramatically increases the inventory of lattices that can be con­sidered in a free-form deformation. 
The twists and bends of Barr [1] and the cylindrical lattices of Coquillart [5] can be easily simulated. 
By allowing meshes of arbitrary topology, the continuity problems of adjoining lattices virtually disappear. 
In section 2, we give an overview of the subdivision methods that are used to de.ne the deformable space 
from the lattice. In our case these methods are based upon an extension of the Catmull-Clark re­.nement 
rules for surfaces [2]. In section 3, we modify the Catmull-Clark procedure to control the boundary surfaces 
and curves of the deformable region. This produces a deformable region that can be intuitively de.ned 
from the lattice. In section 4 we discuss the meth­odsthat give a correspondencebetweena point embeddedin 
the de­formable space and the sequence of lattices generated by the re.ne­ment procedure. In section 
5 we present an overview of the com­plete deformation procedure. Implementation details of the algo­rithm 
are discussed in section 6 and results are given in section 7.  2 De.ning the Deformable Space from 
the Lattice A lattice Lis de.ned to be a set of vertices fP0;P1;:::;Pngand anassociatedsimplicialcomplexwhichspeci.esthe 
connectivityof the vertices2.A subdivision method applied to a lattice is a function from the set of 
lattices into itself. A subdivision method is usually implemented as a set of re.nement rules which de.ne 
how to gen­erate the vertices of the resulting lattice, and also how to connect these new vertices. A 
set of re.nement rules can be repeatedly ap­plied to a lattice L, creating a sequence of lattices fL1;L2;L3;:::g. 
In many cases,this sequencecan be made to convergetoa region of 3-dimensional space. Todescribethe componentsof 
alattice, we will usethe following terms: .An edge of the lattice is de.ned by two vertices that are 
con­nected in the simplicial complex of the lattice. .A face of the lattice is de.ned by a minimal connected 
loop of vertices. .a cell of the lattice is the region of space bounded by a set of faces. A control 
polygon has vertices and edges, a control mesh has ver­tices, edges and faces, and a control lattice 
has vertices, edges, faces and cells. In the bivariate B-spline case, each face of the control mesh is 
de.ned by four vertices, and each vertex of the mesh has connectivity four (four edges radiating from 
the vertex). In the trivariate case, each cell of the control lattice is de.ned by six faces and each 
face by four vertices. Each vertex has connectivity six. 2For consistency, we will refer to a set of 
points that generates a volume as a lattice. The set of points generating a surface will be called a 
mesh.The set of points generating a curve will be called a control polygon. .gures/.g1.tif Figure 1: 
Lattice Structures We allow lattices of arbitrary topology with the following prop­erties: .The lattice 
is well-connected, i.e. no vertex lies on an edge not containing that vertex. .All cells are closed, 
meaning the faces comprising the cells do not form any holes. For example, a cube with one face missing 
is not a valid cell. .No two cells of the lattice intersect that is, we will not con­sider self-intersecting 
lattices. Figure 1 illustrates two sample lattices, one based upon a paral­lelepiped structure and one 
based upon a cylindrical structure. The uniform B-spline curves, surfaces and volumes can be de­.ned 
by subdivision methods. In the curve case, the re.nement rules were .rst presented by George Chaikin 
[3]. Riesenfeld [18] proceeded to show that Chaikin s curves were uniform quadratic B­spline curves. 
Doo and Sabin [7, 8] extended Chaikin s method to uniform quadratic B-spline surfaces and then extended 
the re.ne­ment rules for the quadratic case to meshes of an arbitrary topol­ogy. Catmull and Clark [2] 
developed a similar technique for the uniform cubic B-spline case. These methods have now come into widespread 
use in geometric modeling. They have been used for interpolation and fairing [10], approximation [12], 
and multireso­lution design [16]. In this paper, we consider lattices of arbitrary topology and de­velop 
a set of re.nement rules that subdivide this lattice to gener­ate a deformable region in three-dimensional 
space. To generate the deformable regions, we utilize an extension of the Catmull-Clark subdivision method 
to volumes. In the following sections, we sum­marize the Catmull-Clark re.nement rules for the uniform 
B-spline volume, along with the extensions of these methods to lattices of ar­bitrary topology. The complete 
details of the development of these re.nement rules can be found in [13]. 2.1 Subdivision Methods for 
Trivariate Cubic Uni­form B-Spline Volumes Given a control lattice Lthat de.nes a trivariate uniform 
B-spline volume, the subdivision method generates a new control lattice L1 which consists of the union 
of all the vertices generated by a binary .gures/.g2.tif .gures/.g3.tif Figure 2: Type-3, 4 and 5 cells 
generated by the subdivision process. subdivision of the trivariate volume. These points can be classi.ed 
into 1. cell points these points are the average of the vertices in the lattice that make up the cell. 
 2. face points these points can be written as  C0+2A+C1F= 4 where C0and C1are the cell points of the 
two adjacent hex­ahedral cells that contain the face and Ais the face centroid (the average of the vertices 
that surround the face). 3. edge points these points can be written as Cavg+2A+ME= avg4 where Cavgis 
the average of the cell points for those hexa­hedral cells that contain this edge, Aavgis the average 
of the face centroids for those faces that contain this edge, and Mis the midpoint of the edge. 4. vertex 
points these points can be written as Cavg+3Aavg+3Mavg+P V= 8 where Cavgis the average of the cell points 
for each of the hexahedral cells that contain this vertex, Aavgis the aver­age of the face centroids 
for the faces that contain this vertex, Mavgis the average of the midpoints for the edges that radiate 
from the vertex, and Pis the vertex itself. At each subdivision step, a cell point is inserted into the 
lattice for each cell according to the .rst rule, a face point is inserted for each face according to 
the second rule, an edge point is inserted for each edge according to the third rule, and each vertex 
s position is recalculated according to the fourth rule. To reconnect the lattice after these rules have 
been applied, we .rst connect each new cell point to the new face points generated for the faces de.ning 
the cell. Each new face point is connected to the new edge points of the edges de.ning the original face. 
Each new edge point is connected to the two vertex points de.ning the original edge. Figure 3: The type-4cells 
generated by repeated subdivision.  2.2 Catmull-Clark Volumes Extension of the above rules to lattices 
of arbitrary topology is straightforward, using an extension of the bivariate Catmull-Clark subdivision 
strategy[2]. We can classify the points of the re.nement into four types: 1. cell points these points 
are the average of the vertices of the lattice that bound the cell. 2. face points these points can 
be written as  C0+2A+C1F= 4 where C0and C1are the cell points of the two cells that con­tain the face 
and Ais the face centroid. 3. edge points these points can be written as Cavg+2Aavg+n 3)M E= n where 
Cavgis the average of the cell points for those cells that contain this edge, Aavgis the average of the 
face centroids for those faces contain this edge, and Mis the midpoint of the edge. nis the number of 
faces that contain the edge. 4. vertex points these points can be written as Cavg+3Aavg+3Mavg+P V= 8 
where Cavgis the average of the cell points for each of the cells that contain this vertex, Aavgis the 
average of the face centroids for the faces that contain this vertex, Mavgis the av­erage of the midpoints 
for the edges that radiate from the ver­tex, and Pis the vertex itself. These re.nement rules can be 
applied to a lattice of arbitrary topol­ogy creating a new set of vertex points, edge points, face points 
and cell points. The reconnection strategy is identical to that of the trivariate uniform B-spline lattice: 
the new cell point are connected to the new face points generated for the faces de.ning the cell; the 
newfacepoints are connectedto the newedgepointsfrom the edges .gures/.g4.tif .gures/.g5.tif Figure 4: 
Catmull-Clark Volumes de.ned by a rectangular and cylindrical lattice. de.ning the original face; and, 
each new edge point is connected to the two new vertex points from the original edge. To describe the 
cell structure of the subdivided lattice, we de.ne the valence of a vertex Vwithin a cell Cto be the 
number of edges in Cthat contain the vertex V. Given a cell Cof a lattice L, con­sider a vertex Vof the 
cell Cof valence n. The re.nement process creates a new cell from Vthat contains 2n4-sided faces, 2vertices 
of valence nand 2n2vertices of valence 3(see .gure 2). We call these characteristic cells type-ncells. 
After the .rst subdivision, all cells can be classi.ed as type-ncells. In the subdivision process, a 
type-ncell is re.ned into two type­ncells and 2n2type-3cells. The type-3cell is a hexahedral cell with 
4-sided faces and 3-valence vertices. After a few subdivisions, the bulk of the lattice will consist 
of type-3cells arranged in a reg­ular pattern that of the trivariate uniform B-spline case except around 
a .nite number of type-ncells where this regularity is dis­turbed. For n.3, the number of type-ncells 
doubles in each ap­plication of the subdivision algorithm (see .gure 3). Figure 4 illustrates the Catmull-Clark 
volumes generated from the lattices shown in .gure 1.  3 Boundary Control of the Subdivision Volume 
Designing a lattice that represents a particular region of space is a dif.cult task. The free-form deformations 
of Sederberg and Parry [19] were based upon an initial lattice that was formed on a paral­lelepiped, 
with the deformable space .lling the lattice completely. In our case, the region of space resulting from 
applying the trivari­ate Catmull-Clark subdivision method to an arbitrary lattice does not conform closely 
to the general shape of the lattice shrinking away from the boundary substantially. In .gure 4 for example, 
the cylindrical lattice does not re.ne into a cylindrical region of space. Thisfeaturecreatesanunusualburdenonthedesignerandlimits 
the usefulness of the technique. To solve this problem and create a tool that will construct regions 
of space that are intuitive to the designer, we modify the re.nement rules for those areas of the lattice 
that correspond to boundary sur­faces, sharp edges, and corner vertices. These rules are summarized as 
follows: Figure 5: Catmull-Clark volumes with boundary and edge control. The corner vertices are yellow, 
the sharp edges are red, the boundary edges are green and the internal edges are blue. .Corner vertices 
are identi.ed as those incident to only one cell of the lattice. In the re.nement procedure, the position 
of a corner vertex does not change. .Sharp edges are those edges joining vertices that are incident to 
only one or two cells of the lattice, including corner vertices. The edge and vertex points along the 
sharp edges of the lattice are calculated according to subdivision rules for uniform cubic B-spline curves[13]. 
.All other vertex, edge, and face points on the boundary are generated according to the Catmull-Clark 
rules for surfaces[2]. .Allinternalpointsare calculatedusingtheCatmull-Clark rules for volumes. Given 
a lattice based on a cube, these methods will generate a region of space that is the cube. In the case 
of the lattice approximating a cylinder, the region is .at at either end of the cylinder and rounded 
along the length of the cylinder as one would expect. These are shown in .gure 5. The corner vertices 
are yellow, the sharp edges are red, the boundary edges are green and the internal edgesare blue. These 
techniques have been previously used by Nasri [17] for Doo-Sabin surfaces, and are similar to the techniquesused 
by Hoppe et al. [12] in de.ning edges, creases, corners and darts on Loop Sur­faces [15]. When added 
to the subdivision procedure, these new rules generate deformable regions of space that closely represent 
their lattice. 4 Calculating the Location of Vertices Embedded in the Deformable Space Sederberg and 
Parry [19] impose the initial lattice on a paral­lelepiped in space and calculate the parameterization 
of a point within thedeformablespacebyusingthelocalcoordinatesofapoint within the parallelepiped. The 
location of the point under the de­formation is calculated by substituting these local coordinate values 
into the de.ning equation for the trivariate B´ezier volume. Coquil­lart [5] uses a similar method, but 
numerical iteration is required to calculate the local coordinate, as her initial lattices are not formed 
as parallelepipeds. In both these cases, the cells of the lattice are hex­ahedral. In the case that the 
lattice is of an arbitrary topology and the above subdivision procedure is used, a simple trivariate 
param­eterization is not available. Fortunately, the subdivision procedure itself can be used to establish 
a correspondence between points in the deformable region and points in the deformed region. Given an 
initial lattice L, the subdivision procedure generates a sequence of lattices fL1;L2;L3;L4:::;gthat converge 
to the de­formable region. Each lattice in the sequence induces a partitioning of the deformable space 
by its cells. We select a lattice, say Li,that has the property that the maximum volume of the individual 
cells of Liis small. We then identify the cell of Lithat contains a given point Pand assume that Pis 
deformed to a position within the cor­responding cell of the deformed lattice in the same relative posi­tion 
in the cell. Whereas this is an approximation, it can be made arbitrarily close to the actual deformation. 
To determine the relative position of a point in the cell, we take advantage of the fact that after the 
.rst re.nement all cells are type­ncells, and most are type-3(hexahedral). In the type-3case we can calculate 
a trilinear approximation of the position of the point in the cell and use this trilinear parameterization 
to adjust the position of the point in the deformed cell. In the type-ncase, we can calculate a piecewise 
trilinear approximation, by partitioning the cell into tetra­hedra, and use this to adjust the position 
in the deformed cell.  5 The Deformation Process To deform an object, we follow the 4-step procedure 
outlined by Co­quillart in [5]. First, the user must construct the lattice. This is nor­mally done by 
utilizing an inventory of lattices and a set of tools to merge and build new lattices from this inventory. 
A common tool is the extrusion tool that takes a mesh and extrudes it in a speci.ed direction to become 
a lattice (the cylinder of .gure 1 was generated in this manner.) At the lowest level, the user is allowed 
to to cre­ate cells one by one, attaching them face by face to form the lattice. Boundary surfaces, sharp 
edges and corner vertices can be marked automatically (as in section 3), or a manual marking procedure 
can determine them. Once the lattice has been constructed, the user must place the lattice around the 
object, or the part of the object, to be de­formed. When the lattice is oriented properly, it is frozen 
to the object. At this time, the lattice is re.ned, and the number of re.nement steps nis retained. Each 
point embedded in the deformable region can be tagged with a pointer to the cell of the re.ned lattice 
that contains the point, and a .nite number of parameters that de.nes its position in the cell. .For 
a type-3hexahedral cell, the parameters consist of a u;v;w)triple which de.nes the point s trilinear 
parameter­ization within the cell. .For a type-ncell, a new cell point is calculated and the cell is 
partitioned into 4ntetrahedra about this cell point, each face contributing two tetrahedra to the partition 
(see .gure 6). The parameters consist of an index into the tetrahedra containing the point and a u;v;w)triple 
which de.nes the point s pa­rameterization within the tetrahedra.  Finally, the original lattice is 
deformed by moving one or more of its vertices. The deformed lattice is then re.ned ntimes and the tag 
on each point is used to obtain the corresponding cell in the de­formed lattice. The vertices of this 
cell are used to calculate a posi­tion for the deformed point according to the parameters associated 
with the original. .gures/.g6.tif Figure 6: Partitioning of a type-ncell for approximation. The green 
edges represent the original edges of the cell. The blue edges are generated by the tetrahedral partition. 
 6 Implementation Details The data structure holding the lattice is implemented as an exten­sion of the 
half-edge data structure for surfaces much like the radial-edge structure of Weiler [20]. The primary 
difference be­tween halfedge structure for a mesh representing a surface and a lat­tice representing 
a volume is that the lattice structure may have sev­eral faces that contain each edge the mesh structure 
will have at most two. In addition, a real-time deformation algorithm requires that the sequence of lattices 
be stored hierarchically. The nsub­division steps are then executed only once during the initialization 
(or freezing) phase of the deformation. Subsequent deformations of the lattice then require only the 
recalculation of the re.nement rules for the vertices in lattices Lito Ln, without recreating the lattice 
structure. With each subdivision, the size of the data structure nearly triples and so deformations where 
the user desires a very small cell size can be quite memory intensive. Many numerical algorithms exist 
to generate the trilinear approx­imation of a point in a type-3cell. We have utilized an adaptation of 
an algorithm presented by Hamann, et al. [11]. Given a point Pin a cell, we generate a point P0as the 
trilinear point de.ned by 111 ;;). We then obtain 222 111 .1 u0;v0 ;w)= ;;)+PP0 )J ~ 0 222 ~ where Jis 
the transpose of a local approximate of the Jacobian at 111 ;;)obtained by interpolating the estimates 
of the Jacobians 222 at the vertices of the cell. In general, .1 ~ ui;vi;wi)=ui.1 ;vi.1 ;wi.1 )+PPi)J 
i ~ where Jiis obtained by performing trilinear interpolation at ;v;w)of the Jacobians at the vertices 
of the cell. The procedure is repeated until the distance between Piand Pis suf.ciently small. We found 
that few iterations were needed as the algorithm converges quickly and the cells are generally small. 
In the case of a type-ncell, we partition the cell into 4ntetrahe­dra by utilizing the cell point (average 
of the vertices of the cell). A simple interpolation function for tetrahedral cells can be written as 
ui.1i.1 i.1 Pu;v;w)=P0 +uP1 P0 )+vPP)+wP3 P0 ) 20 where P0, P1, P2,and P3, are the four vertices of the 
tetrahedra. This can be put into matrix form and solved directly [14]. With this implementation, we have 
found that the algorithm ex­ecutes in real time on an SGI Indigo2Extreme.  7 Results The primary motivation 
for moving from the hexahedral topological lattices of the trivariate B´ezier and B-spline representations 
of [5, 9, 19] was to increase the inventory of available lattices and thus the number of possible deformations. 
Figures 7 through 9 show the results of this algorithm with a variety of meshes and shapes. Figure 7 
exhibits a deformation by a cylindrical lattice, resulting in a surface deformation in the form of a 
star. Figure 8 illustrates a complex lattice in the shape of a barbell. This lattice was generated by 
creating a mesh in the shape of a bar­bell and extruding the shape to form the lattice. The subdivision 
methodology automatically handles the continuity between the seg­ments of the lattice. Figure 9 shows 
a deformation applied to the arm of the mobster which causes the arm to lift upward, and the hand to 
twist toward the viewer. This is an excellent example of our technique, as it was necessary to construct 
the lattice about the appendage that was to be moved. 8 Conclusions We have described a new free-form 
deformation technique that gen­eralizes previous methods by allowing 3-dimensional deformation lattices 
of arbitrary topology. The technique uses an extension of the Catmull-Clark subdivisionmethodologyto 
successivelyre.ne a 3-dimensional lattice into a sequence of lattices that converge uni­formly to a region 
of 3-dimensional space. Deformation of the lat­tice then implicitly de.nes a deformation of this region. 
An under­lying model can be deformed by establishing positions of the points of the model within the 
converging sequence of lattices, establishing the cell of the lattice that contains the point, establishing 
an approx­imation of the position of the point within the cell, and using this information to establish 
the new positions of these points within the deformed lattice. This method is very powerful in that it 
can be applied to virtually any geometric model, as it directly modi.es the vertices that de.ne the model. 
The variety of lattices that can be used with this tech­nique greatly increases the number of deformations 
that can be ac­complished. We have only discussed positional data of the embedded object in this paper. 
It is clear that the lattice could hold additional param­eters. For example, we could store, in the lattice 
points the parame­ters of a solid texture. As the lattice is deformed, the texture would be deformed 
along with the object. The careful reader will notice that, for the vertex points of the Catmull-Clark 
volume, we utilize the form Cavg+3Aavg+3Mavg+P V= 8 which does not contain an adjustment for the number 
of edges ra­diating from a vertex. We found that the Catmull-Clark surface methodology directly generalizes 
to edge points, but not to the ver­tex points of the re.nement rules. It was our purpose to use this 
re­.nement to generate a partitioning of the deformable space by its cells, and for this purpose, this 
calculation appears to work very well. A detailed theoretical analysis of the continuity of the deriva­tives 
of these volumes at the extraordinary points [2, 8]. will have to be addressed in a future paper.  9 
Acknowledgments We are very grateful to the anonymous referees for their many help­ful comments on the 
.rst version of this paper. We would also like to thank Bernd Hamann for pointing us toward the simple 
trivari­ate schemes that make this algorithm possible. Special thanks go to Justin Legakis for many useful 
critiques of the research, and who assisted us in the production of our .nal images. The data for the 
mobster in .gure 9 was contributed via the avalon cite at www.viewpoint.com. The research reported here 
was partially sup­ported by a grant through the University of California MICRO Pro­gram.  References 
[1] Alan H. Barr. Global and local deformations of solid primi­tives. In Computer Graphics (SIGGRAPH 
84 Proceedings), volume 18, pages 21 30, July 1984. [2] E. Catmull and J. Clark. Recursively generated 
B-spline sur­faces on arbitrary topological meshes. Computer-Aided De­sign, 10:350 355, September 1978. 
 [3] G. Chaikin. An algorithm for high speed curve generation. Computer Graphics and Image Processing, 
3:346 349, 1974. [4] Yu Kuang Chang and Alyn P. Rockwood. A generalized de Casteljau approach to 3D 
free Form deformation. In Proceed­ings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994), Computer 
Graphics Proceedings, Annual Conference Series, pages 257 260. [5] Sabine Coquillart. Extended free-form 
deformation: A sculp­turing tool for 3D geometric modeling. In Computer Graphics (SIGGRAPH 90 Proceedings), 
volume 24, pages 187 196, August 1990. [6] SabineCoquillartandPierreJanc´ene.Animatedfree-formde­formation: 
An interactive animation technique. In Computer Graphics (SIGGRAPH 91 Proceedings), volume 25, pages 
23 26, July 1991. [7] D. Doo. A subdivision algorithm for smoothing down irreg­ularly shaped polyhedrons. 
In Proceedings of the Int l Conf. InteractiveTechniquesin ComputerAidedDesign, pages 157 165, 1978. 
[8] D. Doo and M. Sabin. Behaviour of recursive division surfaces near extraordinary points. Computer-Aided 
Design, 10:356 360, September 1978. [9] Josef Griessmair and Werner Purgathofer. Deformation of solids 
with trivariate B-splines. In Eurographics 89, pages 137 148. North-Holland, September 1989.  [10] Mark 
Halstead, Michael Kass, and Tony DeRose. Ef.cient, fair interpolation using Catmull-Clark surfaces. In 
Computer Graphics (SIGGRAPH 93 Proceedings), volume 27, pages 35 44, August 1993. [11] Bernd Hamann, 
Donhua Wu, and Robert J. Moorhead II. On particle path generation based on quadrilinear interpolation 
and Bernstein-B´ezier polynomials. IEEE Transactions on Vi­sualization and Computer Graphics, 1(3):210 
217, 1995. [12] HuguesHoppe,TonyDeRose,TomDuchamp,MarkHalstead, Hubert Jin, John McDonald, Jean Schweitzer, 
and Werner Stuetzle. Piecewise smooth surface reconstruction. In Pro­ceedings of SIGGRAPH 94 (Orlando, 
Florida, July 24 29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 295 302. [13] 
Kenneth I. Joy and Ron MacCracken. The re.nement rules for Catmull-Clark solids. Technical Report CSE-96-1, 
Depart­ment of Computer Science, University of California, Davis, January 1996. [14] David N. Kenwright 
and Davis A. Lane. Optimization of time­dependentparticle tracing using tetrahedraldecomposition. In 
Proceedings of Visualization 95, pages 321 328. IEEE Com­puter Society, 1985. [15] Charles Loop. Smooth 
subdivision surfaces based on trian­gles. Master s thesis, Department of Mathematics, University of Utah, 
August 1987. [16] Mike Lounsbery. Multiresolution Analysis for Surfaces of Ar­bitrary Topological Type. 
PhD thesis, Department of Com­puter Science and Engineering, University of Washington, Seattle, WA, June 
1994. [17] A. Nasri. Polyhedral subdivision methods for free-form sur­faces. ACM Transactions on Graphics, 
6:29 73, 1987. [18] R. Riesenfeld. On Chaikin s algorithm. Computer Graphics and Image Processing, 4(3):304 
310, 1975. [19] Thomas W. Sederberg and Scott R. Parry. Free-form deforma­tion of solid geometric models. 
In Computer Graphics (SIG-GRAPH 86 Proceedings), volume 20, pages151 160, August 1986. [20] Kevin J. 
Weiler. Topological structures for geometric mod­eling. PhD thesis, Rensselaer Polytechnic Institute, 
August 1986. .gures/.g7a.tif .gures/.g7b.tif .gures/.g7c.tif Figure 7: Deforming a disk with a star-shaped 
lattice.  Figure 1: Lattice Structures Figure 4: Catmull-Clark Volumes defined by a rectangular and 
cylindrical lattice.  Figure 2: Type-3, 4 and 5 cells generated by the subdivision process. Figure 5: 
Catmull-Clark volumes with boundary and edge control. The corner vertices are yellow, the sharp edges 
are red, the boundary edges are green and the internal edges are blue.  Figure 3: The type-4 cells 
generated by repeated subdivision. Figure 6: Partitioning of a type-n cell for approximation. The green 
edges represent the original edges of the cell. The blue edges are generated by the tetrahedral partition. 
High-resolution TIFF versions of these images can be found on the CD-ROM in: S96PR/papers/joy   Figure 
7: Deforming a disk with a star-shaped lattice. High-resolution TIFF versions of these images can be 
found on the CD-ROM in: S96PR/papers/joy  Figure 8: Deforming a block with a barbell-shaped lattice. 
Figure 9: Deforming the mobster s arm. High-resolution TIFF versions of these images can be found on 
the CD-ROM in: S96PR/papers/joy  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237254</article_id>
		<sort_key>189</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Interpolating Subdivision for meshes with arbitrary topology]]></title>
		<page_from>189</page_from>
		<page_to>192</page_to>
		<doi_number>10.1145/237170.237254</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237254</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computation of transforms (e.g., fast Fourier transform)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Smoothing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Spline and piecewise polynomial approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003717</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computation of transforms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39037916</person_id>
				<author_profile_id><![CDATA[81100328351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zorin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, California Institute of Technology, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023875</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, California Institute of Technology, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P300467</person_id>
				<author_profile_id><![CDATA[81100340025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sweldens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lucent Technologies (formerly AT&T), Bell Laboratories, 700 Mountain, Avenue, Rm. 2C-175, Murray Hill, NJ and Department of Computer Science, K.U.Leuven, Belgium and NFWO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>42459</ref_obj_id>
				<ref_obj_pid>42458</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ball, A. A., and Storry, D. J. T. Conditions for tangent plane continuity over recursively generated B-spline surfaces. ACM Transactions on Graphics 7, 2 (1988), 83-102.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., and Clark, J. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer Aided Design 10, 6 (1978), 350-355.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dahmen, W., Micchelli, C. A., and Seidel, H.-P. Blossoming begets B-splines bases built better by B-patches. Mathematics of Computation 59, 199 (1992), 97-115.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Doo, D. A subdivision algorithm for smoothing down irregularly shaped polyhedrons. In P~vceedings on Interactive Techniques in Computer Aided Design (Bologna, 1978), pp. 157-165.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Doo, D., and Sabin, M. Analysis of the behaviour of recursive division surfaces near extraordinary points. Computer Aided Design 10, 6 (1978), 356-360.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Dubuc, S. Interpolation through an iterative scheme. J. Math. Anal. Appl. 114 (1986), 185-204.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dyn, N., Gregory, J. A., and Levin, D. A four-point interpolatory subdivision scheme for curve design. Computer Aided Geometric Design 4 (1987), 257-268.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Dyn, N., Hed, S., and Levin, D. Subdivision schemes for surface interpolation. In Workshop in ComputationaIGeometry (1993), A. C. et al., Ed., World Scientific, pp. 97-118.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Dyn, N., Levin, D., and Gregory, J. A. A butterfly subdivision scheme for surface interpolation with tension control. ACM Transactions on Graphics 9, 2 (1990), 160-169.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>87546</ref_obj_id>
				<ref_obj_pid>87526</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Dyn, N., Levin, D., and Micchelli, C. A. Using parameters to increase smoothness of curves and surfaces generated by subdivision. Computer Aided Geometric Design 7 (1990), 129-140.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Halstead, M., Kass, M., and DeRose, T. Efficient, fair interpolation using catmullclark surfaces. In Computer Graphics P1vceedings (1993), Annual Conference Series, ACM Siggraph, pp. 35-44.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hoppe, H., DeRose, T., Duchamp, T., Halstead, M., Jin, H., McDonald, J., Schweitzer, J., and Stuetzle, W. Piecewise smooth surface reconstruction. In Computer Graphics P1vceedings (1994), Annual Conference Series, ACM Siggraph, pp. 295-302.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kobbelt, L. Interpolatory subdivision on open quadrilateral nets with arbitrary topology. In Computer Graphics Forum (1996), vol. 15, Eurographics, Basil Blackwell Ltd. Eurographics '96 Conference issue.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Loop, C. Smooth subdivision surfaces based on triangles. Master's thesis, University of Utah, Department of Mathematics, 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192238</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Loop, C. Smooth spline surfaces over irregular meshes. In Computer Graphics P1vceedings (1994), Annual Conference Series, ACM Siggraph, pp. 303-310.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lounsbery, M., DeRose, T. D., and Warren, J. Multiresolution surfaces of arbitrary topological type. Department of Computer Science and Engineering 93-10-05, University of Washington, October 1993. Updated version available as 93-10-05b, January, 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>116112</ref_obj_id>
				<ref_obj_pid>116105</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Nasri, A. H. Surface interpolation on irregular networks with normal conditions. Computer Aided Geometric Design 8 (1991 ), 89-96.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>207497</ref_obj_id>
				<ref_obj_pid>207475</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Peters, J. C1 surface splines. SIAMJ. Numer. Anal. 32, 2 (1995), 645-666.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>226381</ref_obj_id>
				<ref_obj_pid>226379</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Peters, J. Curvature continuous spline surfaces over irregular meshes. Computer Aided Geometric Design (to appear).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>211166</ref_obj_id>
				<ref_obj_pid>211163</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Reif, U. A unified approach to subdivision algorithms near extraordinary points. Computer Aided Geometric Design 12 (1995), 153-174.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sabin, M. The use of Piecewise Fo1~s for the NumericaI Representation of Shape. PhD thesis, Hungarian Academy of Sciences, Budapest, 1976.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Schr/Sder, R, and Sweldens, W. Spherical wavelets: Efficiently representing functions on the sphere. In Computer Graphics P~vceedings (1995), Annual Conference Series, ACM Siggraph, pp. 161-172.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Sweldens, W. The lifting scheme: A construction of second generation wavelets. Tech. Rep. 1995:06, Industrial Mathematics Initiative, Department of Mathematics, University of South Carolina, 1995.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>580358</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Warren, J. Subdivision methods for geometric design. Unpublished manuscript, November 1995.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>865978</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Zorin, D., Schr/3der, R, and Sweldens, W. Interpolating subdivision for meshes of arbitrary topology. Tech. Rep. CS-TR-96-06, Caltech, Department of Computer Science, Caltech, 1996.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interpolating Subdivision for Meshes with Arbitrary Topology Denis Zoriny Peter Schr¨odery Wim Sweldens* 
Abstract Subdivision is a powerful paradigm for the generation of surfaces of arbitrary topology. Given 
an initial triangular mesh the goal is to produce a smooth and visually pleasing surface whose shape 
is controlled by the initial mesh. Of particular interest are interpolating schemes since they match 
the original data exactly, and play an important role in fast multiresolution and wavelet techniques. 
Dyn, Gregory, and Levin introduced the Butter.y scheme, which yields C1 surfaces in the topologically 
regular setting. Unfortunately it exhibits undesirable artifacts in the case of an irregular topology. 
We examine these failures and derive an improved scheme, which retains the simplicity of the Butter.y 
scheme, is interpolating, and results in smoother surfaces. Introduction Beginning with an initial mesh, 
subdivision produces a sequence of re.nements. New vertices are de.ned as local af.ne combinations ofnearbyvertices. 
Anattractivefeatureoftheseschemesis locality, i.e., no global system of equations needs to be solved. 
Examples of subdivision include classical spline constructions which are gener­ally not interpolating. 
The most common interpolating scheme is based on piecewise linears. Unfortunately this is not smooth 
enough for many applications. A scheme that achieves C1 continuity in the topologically regular setting, 
was pioneered by Dyn, Gregory, and Levin [9, 10] and has been applied to the construction of smooth surfaces1. 
The mathematical analysis of the surfaces resulting from sub­division is not always straightforward (see 
for example Reif [20]). However, the simplicity of the algorithms and associated data struc­tures makes 
subdivision attractive for large data sets and interactive applications. Recently, interpolating subdivision 
has been used for multires­olution analysis of complex geometries [16]. It provides a power­ful tool 
for the constructions of .nite analysis and synthesis .lters for wavelet algorithms on general manifolds 
[22] using the lifting scheme[23]. Further, adaptive subdivision is greatly simpli.ed with interpolating 
rules. Multiresolution decomposition algorithms are of importance in compression, progressive display 
and transmis­sion, multiresolution editing, and multigrid/wavelet based numeri­cal methods. While the 
Butter.y scheme of Dyn, Gregory and Levin can be used to generate smooth surfaces over regular triangular 
meshes (all vertices have valence 6), it exhibits degeneracies when applied yDepartment of Computer Science, 
California Institute of Technology, Pasadena CA 91125. Lucent Technologies (formerly AT&#38;T), Bell 
Laboratories, 700 Moun­tain Avenue, Rm. 2C-175, Murray Hill, NJ 07974; Department of Com­puter Science, 
K.U.Leuven, Belgium. (On leave from the NFWO) dzorin@gg.caltech.edu, ps@cs.caltech.edu, wim@bell-labs.com 
 1An alternative was recently proposed by Kobbelt [13]. Permission to make digital or hard copies of 
part or all of this work or personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
in a topologically irregular setting: undesirable creases can result at vertices of valence other than 
6. Figure 1 demonstrates such a failure for vertices of valence 3. The left picture shows the result 
of applying the original Butter.y scheme to a tetrahedron, the right picture is obtained using our modi.ed 
scheme. Motivated by these observations, we consider the construction of a subdivision scheme under the 
following constraints: Interpolation: The original mesh vertices are interpolated and all newly generated 
vertices are on the limit surface.  Locality: The neighborhood used to de.ne new vertex posi­tions from 
old ones should be as small as possible to enable fast algorithms.  Symmetry: The scheme should exhibit 
the same type of sym­metries as the local mesh topology.  Generality: The scheme should work for triangulations 
which are not topologically restricted, including the proper handling of boundaries.  Smoothness: We 
require the resulting scheme to reproduce poly­nomials up to some power a necessary but not suf.cient 
con­dition for higher order continuity.  Simplicity: The scheme should only require simple data struc­ 
 tures. Since the Butter.y scheme satis.es these requirements except for topological generality, we 
make it the starting point of our investi­gation. The main result of our work is a simple modi.cation 
of the Butter.y scheme around vertices of valence not equal to 6. It combats the cusp like artifacts 
exhibited by the unmodi.ed scheme in those circumstances. We use Fourier transform techniques [5, 1], 
which are typically used for analysis of subdivision, for the synthesis of our new interpolating subdivision 
scheme. In the next section we brie.y review related work. Then we describe the construction of our modi.ed 
scheme and present the results. We conclude with a discussion and outlook. 2RelatedWork We brie.y review 
subdivision algorithms for arbitrary topology con­trol meshes. They come in two .avors: approximating 
and interpo­lating. 2.1ApproximatingSubdivisionSchemes Approximating subdivision schemes for arbitrary 
topology meshes are typically modi.cations of spline based schemes. The algo­rithms of Doo and Sabin 
[4, 5, 21] and Catmull and Clark [2] are generalizations of quadratic and cubic B-splines respectively. 
The behavior around extraordinary vertices was analyzed by Doo and Figure 1 : A tetrahedron is subdivided 
according to the original Butter.y scheme (on the left) and with our modi.ed Butter.y scheme (right). 
Sabin [5] using Fourier transforms and an eigen analysis of the sub­division process. More recently Ball 
and Storry [1], Warren [24], and Reif [20] re.ned these techniques. A generalization of quartic box splines 
for arbitrary triangulations was given by Loop [14]. Alternatively one can directly derive a set of spline 
patches which globally achieve some order of continuity [15, 18, 19, 3]. The output of these algorithms 
is a set of patches of varying, at times rather high, polynomial order and varying shape. 2.2InterpolatingSubdivisionSchemes 
 Figure 2 : On the left a regular neighborhood in which all vertices have valence 6. On the right an 
example of a K =7 vertex. In both cases the dot represents a midpoint for which we compute a new value. 
In the regular Since the ability to control the resulting surface exactly is very important in many applications, 
modi.cations of approximating schemes have been proposed to force the limit surface to inter­polate particular 
points and normals. Nasri [17] presents such a modi.cation for the Doo-Sabin scheme, while Halstead et 
al. [11] do the same for the Catmull-Clark scheme. Both cases have a num­ber of limitations. For example, 
it is unclear under what conditions the linear system to be solved for the interpolation constraints 
is solvable. Additionally, the interpolation conditions are only satis­.ed in the limit. Among the patch 
based schemes only Peters [18] recently gave one which can incorporate interpolation constraints without 
requiring the solution of a global linear system. We choose a more direct route by considering subdivision 
schemes which are interpolating by design. The Butter.y scheme is interpolating, local, and simple to 
im­plement, but only leads to C1 surfaces in the regular setting [10] (all vertices of the mesh have 
valence 6.) Topological regularity is a rather severe restriction: the failure to be smooth for vertices 
of valence other than six can be quite noticeable (see Figures 1 and 4). InterpolatingSubdivisionSurfaces 
We .rst discuss surface smoothness, next present the main idea behind our construction and then give 
the description of the subdi­vision algorithm. Space limitations do not permit us to go into the details 
of the mathematical derivation. These are presented in [25]. 3.1Smoothness case the Butter.y stencil 
is used, while in the case K is performed leading to a local modi.cation of the weights. two and reconnecting. 
The interpolating subdivision scheme is used to de.ne a value associated with the new point of the planar 
trian­gulation by taking suitable weighted sums of nearby values. Note that all new planar vertices will 
be of valence 6. The neighbors participating in the computation are part of the subdivision stencil and 
their weights characterize the scheme. Since we assume that all our schemes will be local, we need to 
analyze only a small number of possible cases of the relationship between the new vertex and the topology 
of its graph neighborhood. The two cases of primary importance are the regular sites (all vertices are 
regular), and the extraordinary sites (adjacent to a non-regular vertex.) After several subdivision steps, 
at most one vertex in the neighborhoodhasvalencenotequalto 6,soit is suf.cientto analyze behaviorof the 
scheme only on regular and K-regular triangulations, with only one extraordinary vertex of valence K. 
For the regular case, we use the Butter.y scheme which repro­duces polynomials of degree 3. If initial 
values at the vertices of a regular triangulation are samples of a polynomial function on the plane, 
the limit of polynomial-reproducing subdivision will be that function. cb c s2 s 1 s 3 0 6 s d d s 
4 One of the advantages of patch based polynomial schemes is that their analytic smoothnessproperties 
are well understoodand closely correlate with what a human observer would call a smooth surface. This 
property is only partially covered by such notions as Ck con­tinuity, e.g., a C2 function may be quite 
wiggly. Absence of unnecessary undulations is often referred to as fairness. For ex­ample, Halstead, 
et al. [11] observed that enforcing interpolation conditions on Catmull-Clark surfaces resulted in a 
loss of fairness, an issue they addressed with a global optimization pass. In the present paper we do 
not consider the question of globally optimal fairness. Nonetheless we are attempting to build smooth 
interpolating subdivision schemes which yield surfaces whose shape is as pleasing as possible. Because 
no vertex is ever moved once it is computed, any distortion in the early stages of the subdivision will 
persist. This makes particularly the .rst few subdivision steps very important. Local smoothness of the 
surface requires the existence of smooth coordinate functions (x;y;z)(s;t). In the next section we discuss 
the smoothness of coordinate functions in greater detail. 3.2TheIdeaoftheConstruction Consider a single 
coordinate function. This function can be visual­ized as the graph of some function over the real parameter 
plane (not to be confused with the actual surface). More concretely, instead of thinking about generating 
new vertices in R3 by taking local aver­ages of vertices in the control polyhedron, we are now thinking 
of scalar values which are assigned to the vertices of a triangulation in R2. This triangulation is re.ned 
through splitting each edge in s 6 cb c s5 Figure 3 : a: Ten-point stencil. The dot indicates the midpoint 
of the edge for which a new value is computed. b: Stencil for a vertex in the 1-neighborhood of an extraordinary 
vertex For the K-regular case, we try to choose the coef.cients in such a way that the behavior of the 
scheme is similar to the regular case. To keep the support minimal we use only the immediate neighbors 
of an extraordinary vertex to compute the values at new sites next to it (see Figure 3(b).) We use a 
limited version of polynomial reproduction for the neighborhood of an extraordinary vertex: the values 
of a polynomial function of degree less than 3 have to be reproduced only for the immediate neighbors 
of the extraordinary vertex on each level, except the case K = 3, when only some second degree polynomials 
are reproduced. The Discrete Fourier Transform and an eigen analysis allow us to to construct subdivision 
rules with these constraints [25]. The subdivision matrices for extraordinary vertices have the same 
eigen­values as the matrices for regular vertices. 3.3TheModi.edSubdivisionScheme The subdivision scheme 
computes a new scalar value for each edge midpoint of the triangulation. We distinguish between four 
positions of the edge that we subdivide. 1. The edge connects two vertices of valence 6. In that case 
we use the extension of the Butter.y scheme to the ten point stencil [8] (see Figure 3a). This is the 
canonical setting and the weights are given by a =1/2 -w;b =1/8+2w;c = -1/16 -w;d = w where w can be 
chosen suitably small [8] (we used w =0). 2. The edge connects a K-vertex (K 6the 1­ =6) and a 6-vertex; 
neighbors of the K-vertex are used in the stencil as indicated in Figure 3b. For K .5 the weights are 
given by sj =(1/4+ cos(27j/K)+1/2cos(47j/K))/K with j =0;;K -1. For K =3 we take s0=5/12, s1;2= ... -1/12, 
and for K =4, s0=3/8, s2= -1/8, s1;3 =0. 3. The edge connects two extraordinary vertices; in this case 
we take the average of the values computed using the appropriate scheme of the previous paragraph for 
each endpoint. Since this case can only occur at the topmost level of subdivision the ultimate smoothness 
of the scheme is not in.uenced by this choice. However, we have found that the overall fairness of the 
resulting shapes tends to be better with this scheme. 4. Boundary edges are subdivided using the 1-dimensional 
4 point scheme (s.1= -1/16, s0=9/16, s1=9/16, s2= -1/16) [7, 6]. In this case only other edge points 
participate in the stencil. A consequence of this rule is that two separate meshes, whose boundary is 
identical, will have a matching boundary curve after subdivision. Edges which are not on the boundary 
but which have a vertex which is on the boundary are subdivided as before while any vertices in the stencil 
which would be on the other side of the boundary are replaced with virtual vertices. These are constructed 
on the .y by re.ecting vertices across the boundary.  Results We built an interactive application supporting 
general triangu­lar meshes and adaptive subdivision, using restricted triangular quadtrees. All subdivision 
coef.cients are precomputed and stored in a table indexed by the valence K of a given vertex. We present 
here some results obtained with this application. In the top half of Figure 4 we show how application 
of our scheme to an initial polyhedron (on the right) produces a smooth pipe joint.1 Note that the original 
control polyhedron contains vertices of valence 7 and 4 next to each other. Six levels of the original 
Butter.y subdivision lead to the shape that is shown in the middle. In several regions the surface has 
creases. In contrast on the top right is the shape resulting from applying our modi.ed scheme to the 
same original control polyhedron. These images also demonstrate the treatment of boundaries. We applied 
our scheme to a data set of a mannequin head (courtesy University of Washington) and a torso (a dataset 
from the Avalon site maintained by Viewpoint Datalab.) The bottom half of Figure 4 shows the original 
polyhedron and subdivision surfaces approximated with 3 subdivision levels. The mannequin dataset was 
obtained from the control mesh for the Loop subdivision scheme by moving each control vertex to its limit 
position on the surface. Then the surface was interpolated using the modi.ed Butter.y scheme. It is important 
to note the difference between averaging schemes like Loop s and interpolating schemes. While averaging 
schemes are able to produce relatively fair surfaces from highly irregular control polyhedra acting as 
low-pass .lters, interpolating schemes by their nature are much more sensitive to the irregularities 
in the initial mesh. Our examples demonstrate that if the initial mesh is sampled from a smooth surface, 
the interpolation scheme performs quite well. 1This con.guration was inspired by a similar con.guration 
of Jens Albrecht, Erlan­gen University. 5SummaryandFutureWork We have presented a simple interpolating 
subdivision scheme for meshes with arbitrary topology. Our scheme is based on the But­ter.y scheme, with 
special rules applied in the neighborhood of the extraordinary vertices. The proposed scheme has a number 
of properties that make it attractive: it is interpolating at all levels of subdivision;  the support 
of the scheme is minimal;  it is easy to implement;  limit surfaces have adequate smoothness;  subdivision 
can be performed adaptively;  explicit formulas for the normals exists [25]. Our scheme is especially 
convenient for multiresolution representa­tion of surfaces and wavelet representation of functions on 
surfaces as in [22].  There are several aspects which we believe to be worth investi­gating further: 
 Currently we perform only a limited adaptation of the scheme at the boundary. A more detailed analysis 
of K-vertices on or near the boundary is desirable.  By collapsing vertices and edges, this scheme can 
immediately accommodatemeshtaggingapproachessuchasin[12]. Further­more, the degree of smoothness of the 
surface can be continu­ously adjusted by manipulating the remaining degrees of freedom of the scheme. 
 Preliminary tests show that Hoppe s subdivision surface .tting methodology works well with the scheme 
proposed in this paper.  The question of C1 smoothness will be analyzed based on the work of Reif [20] 
and Warren [24] in a forthcoming paper.  We have observed that the fairness of the surface is determined 
by the behavior of the scheme at the .rst two subdivision steps. A more thorough analysis of the scheme 
in the .rst subdivision steps may shed more light on how to maintain fairness under the constraint of 
interpolation.  Acknowledgements This work was supported in part by an equipment grant from Hewlett 
Packard and fundsprovidedtothesecondauthorbytheCharlesLeePowellFoundation. Additional support was provided 
by NSF (ASC-89-20219), as part of the NSF/DARPA STC for Computer Graphics and Scienti.c Visualization. 
All opinions, .ndings, conclusions, or recommendations expressed in this document are those of the authors 
and do not necessarily re.ect the views of the sponsoring agencies. References [1] Ball, A. A., and Storry, 
D. J. T. Conditions for tangent plane continuity over recursively generated B-spline surfaces. ACM Transactions 
on Graphics 7,2 (1988), 83 102. [2] Catmull, E., and Clark, J. Recursively generated B-spline surfaces 
on arbitrary topological meshes. Computer Aided Design 10, 6 (1978), 350 355. [3] Dahmen, W., Micchelli, 
C. A., and Seidel, H.-P. Blossoming begets B-splines bases built better by B-patches. Mathematics of 
Computation 59, 199 (1992), 97 115. [4] Doo, D. A subdivision algorithm for smoothing down irregularly 
shaped poly­hedrons. In Proceedings on Interactive Techniques in Computer Aided Design (Bologna, 1978), 
pp. 157 165. [5] Doo, D., and Sabin, M. Analysis of the behaviour of recursive division surfaces near 
extraordinary points. Computer Aided Design 10, 6 (1978), 356 360. [6] Dubuc, S. Interpolation through 
an iterative scheme. J. Math. Anal. Appl. 114 (1986), 185 204. [7] Dyn, N., Gregory, J. A., and Levin, 
D. A four-point interpolatory subdivision scheme for curve design. Computer Aided Geometric Design 4 
(1987), 257 268. [8] Dyn, N., Hed, S., and Levin, D. Subdivision schemes for surface interpolation. In 
Workshop in Computational Geometry (1993), A. C. et al., Ed., World Scienti.c, pp. 97 118. [9] Dyn, N., 
Levin, D., and Gregory, J. A. A butter.y subdivision scheme for surface interpolation with tension control. 
ACM Transactions on Graphics 9, 2 (1990), 160 169. [10] Dyn,N.,Levin,D.,andMicchelli,C.A.Usingparameterstoincreasesmoothness 
of curves and surfaces generated by subdivision. Computer Aided Geometric Design 7 (1990), 129 140. [11] 
Halstead,M.,Kass,M.,andDeRose,T.Ef.cient,fairinterpolationusingcatmull­clark surfaces. In Computer Graphics 
Proceedings (1993), Annual Conference Series, ACM Siggraph, pp. 35 44.  Initial mesh Butter.y scheme 
interpolation Modi.ed Butter.y interpolation Initial mesh Modi.ed Butter.y interpolation Initial mesh 
Modi.ed Butter.y interpolation Figure 4 : Top row: pipe joint. Note the difference between Butter.y and 
Modi.ed Butter.y. Lower left: mannequin head. Lower right: torso. [12] Hoppe, H., DeRose, T., Duchamp, 
T., Halstead, M., Jin, H., McDonald, J., Schweitzer, J., and Stuetzle, W. Piecewise smooth surface reconstruction. 
In Computer Graphics Proceedings (1994), Annual Conference Series, ACM Sig­graph, pp. 295 302. [13] Kobbelt, 
L. Interpolatory subdivision on open quadrilateral nets with arbitrary topology. In Computer Graphics 
Forum (1996), vol. 15, Eurographics, Basil Blackwell Ltd. Eurographics 96 Conference issue. [14] Loop, 
C. Smooth subdivision surfaces based on triangles. Master s thesis, University of Utah, Department of 
Mathematics, 1987. [15] Loop, C. Smooth spline surfaces over irregular meshes. In Computer Graphics Proceedings 
(1994), Annual Conference Series, ACM Siggraph, pp. 303 310. [16] Lounsbery,M.,DeRose,T.D.,andWarren,J.Multiresolutionsurfacesofarbitrary 
topological type. Department of Computer Science and Engineering 93-10-05, University of Washington,October 
1993.Updated version available as 93-10-05b, January, 1994. [17] Nasri, A. H. Surface interpolation on 
irregular networks with normal conditions. Computer Aided Geometric Design 8 (1991), 89 96. [18] Peters, 
J. C1 surface splines. SIAM J. Numer. Anal. 32, 2 (1995), 645 666. [19] Peters,J. Curvaturecontinuoussplinesurfacesoverirregularmeshes. 
Computer Aided Geometric Design (to appear). [20] Reif, U. A uni.ed approach to subdivision algorithms 
near extraordinary points. Computer Aided Geometric Design 12 (1995), 153 174. [21] Sabin, M. The use 
of Piecewise Forms for the Numerical Representationof Shape. PhD thesis, Hungarian Academy of Sciences, 
Budapest, 1976. [22] Schr¨oder, P., and Sweldens, W. Spherical wavelets: Ef.ciently representing functions 
on the sphere. In Computer Graphics Proceedings (1995), Annual Conference Series, ACM Siggraph, pp. 161 
172. [23] Sweldens, W. The lifting scheme: A construction of second generation wavelets. Tech. Rep. 1995:06, 
Industrial Mathematics Initiative, Department of Mathemat­ics, University of South Carolina, 1995. [24] 
Warren, J. Subdivision methods for geometric design. Unpublished manuscript, November 1995. [25] Zorin,D.,Schr¨oder,P.,andSweldens,W. 
Interpolatingsubdivisionformeshesof arbitrary topology. Tech. Rep. CS-TR-96-06, Caltech, Department of 
Computer Science, Caltech, 1996. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237257</article_id>
		<sort_key>193</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Disney's Aladdin]]></title>
		<subtitle><![CDATA[first steps toward storytelling in virtual reality]]></subtitle>
		<page_from>193</page_from>
		<page_to>203</page_to>
		<doi_number>10.1145/237170.237257</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237257</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39045485</person_id>
				<author_profile_id><![CDATA[81100493478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Randy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pausch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Thornton Hall, University of Virginia, Charlottesville, VA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31095241</person_id>
				<author_profile_id><![CDATA[81332528745]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snoddy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31096981</person_id>
				<author_profile_id><![CDATA[81332531370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taylor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P261239</person_id>
				<author_profile_id><![CDATA[81100375623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P78147</person_id>
				<author_profile_id><![CDATA[81100126237]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haseltine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jon Airey, John Rohlf, Frederick Brooks, Towards Image Realism with Interactive Update Rates in Complex Virtual Building Environments, ACM SIGGRAPH Special Issue on 1990 Symposium on Interactive 3D Graphics 24:2, 1990, pages 41-50.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Walt Disney Home Video. Distributed by Buena Vista Home Video, Dept. CS, Burbank, CA, 91521. ISBN 1-55890- 663-0. Originally released in 1992 as a motion picture.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91409</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chuck Blanchard, Scott Burgess, Young Harvill, Jaron Lanier, Ann Lasko, Reality Built for Two: A Virtual Reality Tool, ACM SIGGRAPH 1990 Symposium on Interactive 3D Graphics, March 1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. P. Blauert, Spatial Hearing, MIT Press, Cambridge, MA, 1983.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319122</ref_obj_id>
				<ref_obj_pid>319120</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Frederick Brooks, Walkthrough -- A Dynamic Graphics System for Simulating Virtual Buildings, Proceedings of the 1986 Workshop on 3D Graphics, Chapel Hill, NC, October 23-24, 1986, ACM, pages 9-21.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617763</ref_obj_id>
				<ref_obj_pid>616024</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Steve Bryson, Creon Levit, The Virtual Wind Tunnel, IEEE Computer Graphics and Applications, July 1992, pages 25-34.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142825</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Christopher Codella et al., Interactive Simulation in a Multi-Person Virtual World, Proceedings of the ACM SIGCHI Human Factors in Computer Systems Conference, May 1992, pages 329-334.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[James Cutting and Peter Vishton, Perceiving Layout and Knowing Distances: The Integration, Relative Potency, and Contextual Use of Different Information About Depth, Handbook of Perception and Cognition: Perception of Space and Motion, Vol. 5, Academic Press (to appear).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>40687</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[R. Kent Dybvig. The Scheme Programming Language. Prentice-Hall, 1987.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319127</ref_obj_id>
				<ref_obj_pid>319120</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[S. S. Fisher, A M. McGreevy, J. Humphries, W. Robinett, Virtual Environment Display System, Proceedings on the 1986 Workshop on Interactive 3D Graphics, pages 77-87, October 23-24, 1986.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147158</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Thomas Funkhouser, Carlo Sequin, Seth Teller, Management of Large Amounts of Data in Interactive Building Walkthroughs, Proceedings of the 1992 ACM Symposium on Interactive Three-Dimensional Graphics, April, 1992, pages 11-20.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Thomas Furness, The Super Cockpit and Human Factors Challenges, Human Interface Technology (HIT) Laboratory of the Washington Technology Center, Tech Report HITL-M-886-1, October, 1986.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199421</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Tinsley A. Galyean, Guided Navigation of Virtual Environments, 1995 ACM Symposium on Interactive 3D Graphics, April 1995, pages 103-104, Monterey, CA.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Id Software, Inc.; information available via http://www.idsoftware.com/ .]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37407</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[John Lasseter, Principles of Traditional Animation Applied to 3D Computer Animation, Computer Graphics (SIGGRAPH '87 Proceedings) Volume 21, number 4, pages 35-44, July 1987.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Randy Pausch, et al, A Brief Architectural Overview of Alice, a Rapid Prototyping System for Virtual Reality, IEEE Computer Graphics and Applications, May 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin, Real Time Responsive Animation with Personality, IEEE Transactions on Visualization and Computer Graphics, Vol. 1, No. 1.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[A. Pope, BBN Report No 7102. The SIMNET Network and Protocols. BBN Systems and Technologies, Cambridge, Massachusetts, 1989.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[John Rohlf and James Helman, IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D Graphics, SIGGRAPH '94 Conference Proceedings, Computer Graphics, July, 1994.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[William J. Schroeder, Jonathan A. Zarge, William E. Lorensen, Decimation of Triangle Meshes, Computer Graphics (SIGGRAPH '92 Proceedings) Volume 26, pages 65-70, July 1992.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sense8 Corporation, 100 Shoreline Highway, Suite 282, Mill Valley, CA 94941, 415/331-6318, http://www.sense8.com/ , info@sense8.com .]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>173948</ref_obj_id>
				<ref_obj_pid>159161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Chris Shaw, Mark Green, Jiandong Liang, Yunqi Sun, Decoupled Simulation in Virtual Reality with the MR Toolkit, ACM Transactions on Information Systems, 11:3, pages 287-317, July 1993.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Ivan Sutherland, The Ultimate Display, Proceedings of IFIP (International Federation of Information Processing) '65, Vol. 2, pages 506-508.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1476686</ref_obj_id>
				<ref_obj_pid>1476589</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Ivan Sutherland, A Head-mounted Three-dimensional Display, Proceedings of the Fall Joint Computer Conference, AFIPS, Vol. 33, pages 757-764.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Virtuality, http://www.virtuality.com, UK office: Virtuality House, 3 Oswin Road, Brailsford Industrial Park, Leicester LE3 1HR, United Kingdom, Tel: +44(0) 116 233 7000, enquiries@virtuality.com . USA office: 7801-7805 Mesquite End Drive, Suite 105, Irving, Texas 75063, USA 001-214-556-1800, 1-800-ILLUSION, enquiries@tx.viruality.com.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[E. Wenzel, F. Wightman, S. Fisher, A Virtual Display System for Conveying Three-dimensional Acoustic Information, Proceedings of the Human Factors Society, 32nd Annual Meeting, 1988, pages 86-90.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Disney s Aladdin: First Steps Toward Storytelling in Virtual Reality Randy Pausch1, Jon Snoddy2, Robert 
Taylor2, Scott Watson2, Eric Haseltine2 1University of Virginia 2Walt Disney Imagineering Figure 1: 
A Guest s View of the Virtual Environment ABSTRACT Disney Imagineering has developed a high-fidelity 
virtual reality (VR) attraction where guests fly a magic carpet through a virtual world based on the 
animated film Aladdin. Unlike most existing work on VR, which has focused on hardware and systems software, 
we assumed high fidelity and focused on using VR as a new medium to tell stories. We fielded our system 
at EPCOT Center for a period of fourteen months and conducted controlled experiments, observing the reactions 
of over 45,000 guests. contact author: Randy Pausch, Computer Science Department, Thornton Hall, University 
of Virginia, Charlottesville, VA 22903. Pausch@virginia.edu, 804/982-2211 Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
Riders filled out an exit survey after the experience, and with select groups we used a number of other 
data-gathering techniques, including interviews and mechanically logging where guests looked and flew. 
Our major finding is that in a high fidelity VR experience, men and women of all ages suspend disbelief 
and accept the illusion that they are in a different place. We have found that in VR, as in all media, 
content matters. Novices are unimpressed with the technology for its own sake; they care about what there 
is to do in the virtual world. We can improve the experience by telling a pre-immersion background story 
and by giving the guest a concrete goal to perform in the virtual environment. Our eventual goal is to 
develop the lexicon for this new storytelling medium: the set of communication techniques shared between 
directors and the audience. We conclude with a discussion of our second version of the Aladdin project, 
which contains a large number of synthetic characters and a narrative story line. INTRODUCTION Most 
existing work on virtual reality (VR) has focused on hardware and system software [1, 3, 5, 6, 7, 10, 
12, 23, 24]. The price of a high quality system has placed it out of reach for most people interested 
in content. Building high quality, low cost VR systems is important, but we believe the exciting challenge 
in VR is learning what to do with the medium. We believe that the content questions are the really hard 
ones. The goal of this project has been to allow the content producers, or authors, to assume the existence 
of satisfactory technology and to focus directly on authoring in the new medium of VR. We produced high-quality 
content based on flying a magic carpet in the animated film Aladdin [2]. Figure 1 shows a screen shot 
from the system. We field-tested the system on over 45,000 guests at EPCOT Center. In this paper we repor 
t our detailed observations, the guests exit surveys, and data we recorded during guest experiences. 
This is not a systems implementation paper; we describe the hardware and software only as context for 
describing the guest experience. In addition to guest experiences, we also describe industrial design 
solutions to the problems of high volume usage. In early 1996, we will deploy a second version with a 
narrative story line and a large number of reactive characters. We conclude with lessons learned from 
creating virtual environments and characters for our second version, especially controlling the narrative 
in an interactive medium. Our underlying premise is that VR is a new medium, as film, radio, and television 
once were. As motion pictures matured, directors and audiences developed a lexicon including close ups, 
cross cuts, flash backs, etc. Over time a common language, or lexicon, will evolve for VR; this project 
is our first step towards that goal.  SYSTEM DESCRIPTION In each of our field trials, four guests donned 
head-mounted displays and piloted a flying carpet. Because they were running on separate systems, the 
pilots could neither see nor interact with each other. We designed the system for robustness, high volume 
usage, and high accessibility. Unlike research setups, theme park equipment is used extensively, continuously, 
and abusively. Failures with a one-in-a-million chance of happening can occur once a week in a typical 
theme park attraction. The Head Mounted Display The system used an internally developed head-mounted 
display (HMD), shown in Figure 2. The two main design constraints were to provide high image quality 
and to make it easy to put the HMD on quickly, to support the high throughput of guests in a theme park 
attraction. In early trials we learned that having adjustments such as a focus knob on the HMD confused 
guests, since they had no baseline to distinguish between high and low image quality. Therefore, we designed 
a system that would accommodate a large variation in where a guest s eyes sit with respect to the optics. 
Figure 2: The Head Mounted Display Image quality considerations drove us to use CRTs instead of LCDs, 
a tradeoff that increased the HMD s weight and extended its center of mass. We partially compensated 
for this by providing spring-suspension of the HMD from the ceiling. Major design challenges in the HMD 
included avoiding visible pixel boundaries, obtaining high contrast, minimizing inter-ocular rivalry, 
and addressing the weight balance and packaging issues. For head-tracking, we used a magnetic position/orientation 
tracker. Unlike many other VR systems, our HMD display was bi­ocular, not stereo. We rendered a single, 
horizontally wide graphics window and fed partially overlapping view windows to each of the CRTs in the 
HMD. For applications such as ours, stereoscopy is surprisingly unimportant as a depth cue [8]. We addressed 
hygiene issues by having the HMD snap onto a per-guest inner cap that can be cleaned separately. The 
inner liner also allowed us to adjust tightness to each guest s head before monopolizing the more expensive 
HMD and image generator. The HMD fit comfortably over eyeglasses; the only notable issue was guests with 
hair tied in buns. Sound The HMD contained two speakers that rested close to, but not in physical contact 
with, the guest s ears. We used a combination of stereo ambient sound, binaural recorded sound, and eight 
channels of localized sound. We recorded the binaural sound track via a high quality binaural head (essentially, 
microphones placed in a mannequin head). The binaural soundtrack included background voices, animals, 
and other clutter sounds. We recorded multiple binaural tracks, and then mixed those layers to form a 
composite recording. When the binaural recording was played during the VR experience, even though those 
sounds moved with the head, they established a believable background sound field. It is in this context 
that the eight special channels were convolved to localize in real-time based on head tracking [26]. 
The localized channels provided main characters and large sound effects. The stereo sound (primarily 
music) established emotional context, the binaural sound established the believable three-dimensional 
space, and the localized sounds gave strong, specific cues for orientation. The three levels increasingly 
traded recording quality for localization, and the binaural and localized sounds worked well together 
because they employed the same head-related transfer functions [4].  Seating, Controls, and Motion Base 
Guests were seated straddling a motorcycle-style seat as shown in Figure 3. A benefit of this design 
is that the guests were firmly grounded, with weight on their buttocks, knees, and feet. Additionally, 
this design accommodated a wide range of heights. Guests gripped a steering mechanism representing the 
front of a magic carpet. Turning left or right controlled yaw of the carpet, and tilting controlled the 
pitch of the carpet Imagine a car s steering wheel; pulling the top of the wheel toward the driver pitched 
the carpet up, pushing it pitched the carpet down. Pushing the entire mechanism forward controlled velocity. 
Figure 4 shows a schematic diagram of the carpet controls. Figure 3: The Physical Setup  Figure 4: Schematic 
Diagram of Carpet Controls We mounted the seat on a movable base that pitched up and down in response 
to the steering control. Originally, the motion base also tilted side-to-side, but this caused discomfort 
during early testing so we removed the side-to-side tilt. Surprisingly, the presence or absence of a 
motion base had no substantial effect on guest satisfaction, or anything else we measured with exit surveys. 
An early version of the system simulated wind with a rate­controlled fan blowing air over the guests. 
Much to our disappointment, most guests wearing the HMD did not notice it.  Image Generation For each 
guest, we used a custom Silicon Graphics computer with 512 megabytes of RAM, 16 megabytes of texture 
memory, eight 150 MHz MIPS R4400 CPUs and three Reality Engine pipelines with four RM5 raster managers 
each. We rendered 20 frames per second on each pipe, interleaving the frames to achieve a 60 Hz frame 
rate. Although the frame rates could vary between 15 and 60 during a flight, the overwhelming majority 
of the time the system rendered at 60 Hz. Because hardware lighting can draw attention to edges in models 
with low polygon count, our artists decided to render all polygons with hand-painted textures, with no 
hardware lighting. This also improves rendering time slightly, but we did it for image quality, not speed. 
 Model Management And Show Programming A custom software system, called the player, provided scene management 
and character animation playback. The player provided a Scheme interface on top of a C/C++ layer, all 
on top of SGI Performer [19]. The player used Performer s support for multiple levels of detail. Unlike 
a flight simulator, most of our scene was close, so we used only two levels of detail per object. Artists 
created both models for each object because degrading a model by hand still produces better results than 
automatic means [20]. We sometimes used large texture flats for distant objects, and switched to three 
dimensional models as the guest approached. Programming of various show elements, such as an object s 
reaction when hit by the carpet, was performed in the topmost layer of the player, a locally developed 
Story Animation Language. This SAL layer implemented cooperative lightweight threads on top of Chez Scheme 
[9], an incrementally compiled Scheme. In our second version, the database is much larger, and is partitioned 
into distinct scenes. The player software pre-fetches geometry and texture maps as guests fly from one 
scene to another [11]. Between scenes, we include explicit transition areas, such as hallways and caverns. 
Transition areas have a smaller number of polygons, which buys us time to pre-fetch textures. Transitions 
bend and twist, thus ensuring that at no point can the guest quickly look back to the previous scene. 
  GUEST SELECTION We deployed the system at EPCOT Center in Orlando, Florida, from July 1, 1994 until 
September 8, 1995. Every twenty minutes a group of up to 120 guests was given a brief technical lecture 
about VR followed by a demonstration where four guests were selected to fly a magic carpet. The attraction 
was intentionally hidden in a remote area of the park. Most guests entered not because they had a strong 
interest in VR, but because our attraction was the next thing to do. Guests could not volunteer to fly; 
they were selected by the ride operators. The operators maintained a strict policy of avoiding guests 
who showed an active interest in VR. Therefore, rather than pertaining to a small subset of VR enthusiasts, 
we believe that our results are essentially a fair cross section of the theme park population. Some guests 
did decline the invitation to fly. Interviews revealed this was primarily due to stage fright, not an 
aversion to trying VR. The selected pilots did not hear the technical lecture about VR. We gave them 
a background story that they would be stepping into the feature film Aladdin. We instructed them that 
the main goal was to have fun, but that they were also searching for a character from the film. The marketplace 
scene was chosen because it 1) contains familiar objects such as doors which establish scale, 2) is a 
brightly lit daytime scene, and 3) contains wide variety, encouraging exploration. There was typically 
time for a one-to-three minute practice flight followed by a few minutes of rest before the audience 
entered and the four minute flight began. NOVICES EXPERIENCES We exposed a large, non-self selected, 
population of guests to a high-fidelity virtual experience in a controlled environment. At least one 
other system has exposed large numbers of novices to VR [25]. However, Virtuality s users were self­selected. 
Their users wanted to try VR, and paid for the experience. Our sample is much more diverse. Our findings 
are drawn from a variety of sources, including written post-flight guest surveys, logged flight data, 
extensive conversations with the day-to-day attraction operators, observations of guests flights, and 
interviews of guests before, during and after their flights. Technologists should be aware that most 
guests were not impressed by the technology itself; guests assumed VR was possible and had an expectation 
of extremely high quality. Many had seen the holodeck on Star Trek, and expected that level of quality. 
Once in a virtual environment, guests focused on what there was to do in the virtual world - content 
matters! General Observations We were able to sustain the illusion that the guests were in another place. 
Men and women of all ages suspended disbelief and a large number reported the sensation that they were 
in the scene. This is hard to conclude from exit surveys, but guests also provided unsolicited cues, 
such as panicking or ducking their heads as they approached obstacles. Guests cared about the experience, 
not the technology. Most guests had no concept of how VR works, nor did they care. They focused on the 
sensation, which was exhilarating for most guests. Many guests shouted Wow! or Whee! in their first thirty 
seconds. The experience was overwhelming. Between sensory overload and the task of trying to control 
the carpet s flight, many guests were so cognitively taxed that they had trouble answering questions 
early in their flights. Guests needed a goal. If not given a specific goal, guests would ask What should 
I be doing? Guests needed a background story. We found that giving as much context as possible about 
the scene helped reduce the severity of the transition from the real to the virtual environment. Background 
story is the set of expectations, goals, major characters, and set of rules that apply to the virtual 
world. Ironically, in lower fidelity, less believable VR systems, this need for background story may 
not be as evident. We believe it is the abrupt transition into a believable virtual world that is problematic. 
Performing a good transition from the real to the virtual world is an open challenge. Guests liked exploring, 
and seeing new spaces. Most guests did not spend much time studying detail in a given place; they tended 
to move on quickly to new vistas. Guests did not turn their heads very much. This could be because they 
were piloting a vehicle, or because they were not accustomed to being able to turn their heads when looking 
at displayed images. For many, we believe the latter. Guests often watched characters walk out of frame, 
as would happen with television or movies. Our strongest indication came from many pilots where we waited 
90 seconds into their flight, then explicitly told them to turn their heads. At that point, they clearly 
had the aha of the head-tracking experience. While we suspect that different content would be more conducive 
to head turning, head tracking is far enough from most guests experiences with film and television that 
we suspect this will be a problem for many systems. Controlling the carpet was a problem for many guests. 
This prompted the addition of test flights before the show began. Many guests flew out into the desert 
or up above the city to find a space where there were fewer obstacles, making flight easier. Although 
we could have had the magic carpet fly itself, our surveys indicated that the control and freedom are 
important parts of the experience. Six-axis control is a very difficult problem and an important design 
challenge is finding appropriate control constraints. VR must be personally experienced. In addition 
to the 45,000 guests who piloted carpets, we had over one million audience members who observed the pilots 
progress on display monitors. The audience members enjoyed the show and understood that something fascinating 
was going on with the pilots, but it was clear that VR is foreign enough that most people can not fully 
comprehend it without direct personal experience. Audience members often asked if the pilots could see 
or interact with each other.  Presence and Immersion Although it is difficult to formally measure, we 
believe that most guests suspended disbelief and had the experience of being in a new place. Our choice 
of an animated world underscored that believability is different from photo-realism. In fact, we reject 
the term simulation, as we provide an experience not possible in reality. Our virtual environment was 
not realistic, but it was consistent with the large number of animated worlds that guests had seen before. 
Guests flew, but had no fear of heights; guests reacted to the characters, but were not afraid of a guard 
who brandished a sword. In many ways, this environment was compelling without being disturbing. A common 
sight in a 3-D theater is to see large numbers of guests reaching out to grab the projected image. We 
speculate that they are compelled to conduct this test because their perceptual and cognitive systems 
are in conflict; their eyes tell them the object is within arm s length, but their brain tells them it 
is just a projection. In our system, we saw no evidence of the need to test. Guests did not intentionally 
run into objects to see if the objects really existed. In fact, guests did the opposite, often involuntarily 
ducking when they felt they could not avoid a collision. In general, we believe that the need for high 
fidelity can be reduced by engaging the user in a complex, real-time task. For example, the desktop DOOM 
game [14] and the SIMNET tank simulator [18] both get users to the point where the interface becomes 
transparent and the user focuses on task performance, which requires a sense of presence. Our system 
did so with the mildest of tasks, that of searching for a character. At first, we suspected that the 
difficult task of piloting the carpet might lower our fidelity requirements. Therefore, we ran experiments 
where the carpet flew itself. During those tests guests achieved the same suspension of disbelief, with 
the only task being to look around. Our metric for suspension of disbelief was their reactions to the 
environment, such as ducking when flying near objects. What produced the effect of immersion is difficult 
to know. Even for guests who did not turn their heads much, the HMD physically blocked out the real world. 
Sound was also very important, as many guests remarked that the sound of wind when they flew high, or 
the crashing noises when they ran into walls strongly reinforced their sense of being there. In post 
flight interviews, guests told us that their illusion of presence was destroyed when characters did not 
react. Reaction to Virtual Characters It is more difficult to build a believable character than a believable 
scene. Although our major focus was on building the environments, we were pleased that a few of our guests 
did respond to characters. The show began with instructions from a parrot who told the pilots to nod 
their heads. Some guests actually heeded his command. Another character covered his head and shouted 
Don t you have a horn on that thing?! when guests flew near him. Many guests shouted back at this character. 
One young girl finished the attraction in tears because she had spent several minutes attempting to apologize 
to him, but instead continually triggered hostile responses whenever she approached him. (All the characters 
had a small set of dialog sequences that could be triggered). The key to a successful character is the 
suspension of disbelief; one must talk to the puppet, not the puppeteer. Most guests flew at high speed, 
zooming past the characters. When guests did slow down, they expected the characters to respond and were 
very disappointed when the characters did not. At the very least, characters should orient their heads 
and eyes and look at the guest. Our next system is incorporating this feature. We suspect that the limited 
believability of our first system s characters is due to low fidelity. All characters in the first show, 
such as those shown in Figure 5, were animated with motion capture, where sensors recorded an actor s 
body motions in real time, and those values were used to drive the animation. Our second version uses 
higher quality key frame animation. While testing of the second version is not yet complete, early indications 
are that we will cross a fidelity threshold in character animation much as this project crossed one in 
environment fidelity. Figure 5: Animated Characters Men vs. Women One of our original objectives was 
to discover whether VR appealed only to the narrow (primarily young male) video game market, or was more 
like feature films, appealing to males and females of all ages. While content will still matter, the 
technology itself did not turn away any guests. On post flight surveys, the reaction of both genders 
and all age groups was almost identical on all questions. One major difference was that many women are 
afraid that they would not be able to operate the equipment properly. This surfaced both as a pre­flight 
concern and as a post-flight comparison. They often asked how they performed relative to the other pilots. 
Also, during in-flight interviewing men were more likely to talk about the technology, whereas women 
were more likely to talk about the experience and emotional impact. Neither men nor women complained 
about having to wear the HMD. VR for the Disabled Everyone involved with the project noted the impact 
on both the pilots and the audience when motion-impaired guests flew. Accessibility is a fundamental 
design constraint at Disney parks, and we have a substantial wheelchair population. One of our four stations 
could be converted for wheelchair access in about ten seconds, and we had several wheelchair fliers per 
day. The sense of mobility and the joy it brought them was overwhelming. Motion Sickness We did not 
find motion sickness to be as significant an issue as we had feared. During selection, we asked guests 
if they were prone to motion sickness, and warned that they might feel motion sick during the experience. 
We also told them they could stop at any time and remove the HMD. Post flight surveys indicated that, 
as with many theme park attractions, some guests reported discomfort or dizziness, but they mostly described 
it as a mild sensation. We do not know if guests who felt discomfort or dizziness self-limited their 
head motion; our logged data showed no such correlation. Reports of discomfort went up when the room 
was warmer, which is consistent with discomfort reports from platform-based simulator rides. We were 
careful to limit the duration of the experience. As with any thrill experience, discomfort increases 
with ride length.  GUEST POST-RIDE SURVEYS After their flights, we asked guests to complete a one page 
survey with about five multiple choice questions. Guests were identified on the survey only by first 
name, and over 95 percent of the guests completed a survey. Most who declined did so because of low English 
skills. We asked many questions and report here a relevant subset. Our sample was 48.5 percent female, 
and included all ages. We tried to ask questions that would yield different responses by gender and age. 
However, we were unable to design questions where the responses were not reasonably consistent across 
all groups. Thus, we conclude that VR experiences have broad appeal. Responses are presented here by 
gender (M = male, F = female); breakdown by age is equally similar. The possible responses are listed 
in the same order as they appeared on the printed survey form. Because we made ongoing changes to the 
surveys, the number of total responses to any question is variable --after each question is the total 
number of responses. What did you LIKE the most? (N=25,038) all M F characters 11% 10% 12% helmet fit 
4% 4% 3% motion 32% 32% 32% picture quality 17% 19% 15% sound 8% 7% 9% steering control 21% 21% 21% town 
7% 7% 7% What did you DISLIKE the most? (N=22,479) all M F characters 5% 6% 4% helmet fit 20% 20% 20% 
motion 13% 14% 13% picture quality 13% 13% 13% sound 6% 6% 6% steering control 34% 33% 36% town 8% 8% 
7% Guest rating of the Experience (N=1,903) all M F terrible 1% 1% 1% okay 4% 4% 5% good 11% 9% 13% 
great 54% 49% 57% best thing at Disney 23% 28% 20% best thing in my life 7% 9% 5% As an absolute answer, 
we take this with a grain of salt. It is unlikely that our system is really the best thing in seven percent 
of our guests lives. However, the scale is useful for comparing males and females; again, we found an 
overwhelming similarity.  Would You Recommend it To a Friend? (N=273) all M F yes 99% 100% 98% no 1% 
0% 2%  It Made Me Feel Like I Was... (N=1,336) all M F visiting a town 14% 15% 14% playing a video 
game 23% 19% 25% being inside a movie 45% 49% 43% in the middle of a dream 17% 16% 17% invisible 1% 1% 
2%  Had You Heard About Virtual Reality Before Today? (N=307) all M F no 16% 12% 18% I had read about 
it 36% 37% 34% seen on TV or movies 49% 50% 47%  On My Next Ride, I Would Most Like To... (N=324) all 
M F see more characters 35% 32% 40% see more towns/places 38% 37% 38% see the other pilots 27% 31% 22% 
 0.05 0.045  The Best Thing About it Was... (N=439) portion of time at angle 0.04 0.035 0.03 0.025 
0.02 0.015 0.01 0.005 0 all M F the characters 5% 3% 6% flying 42% 41% 43% exploring/seeing new things 
23% 23% 23% being able to go where I wanted 30% 33% 28%  I would most like to... (N=426) all M F have 
the carpet fly itself 9% 5% 12% fly the carpet myself 84% 90% 80% ride while a friend is flying 6% 4% 
8%  LOGGED DATA head yaw angle Figure 7: Conventional Histogram of Head Yaw For over two thousand 
guests we recorded the position and orientation of the pilot s head and the carpet twenty times each 
second. Our original hope was that we could see patterns of where guests flew and what they found interesting. 
In fact, we discovered that guests flew almost indiscriminately; no obvious patterns of travel emerged 
from the data. The analysis of head turning data was more interesting. Our Figure 8 shows that the difference 
between male and female head yaw is negligible. In fact, every category that we examined (gender, age, 
which lab technician instructed them, whether or not they experienced motion discomfort, how much they 
enjoyed the ride) yielded essentially the same profile. 0.05 first question was How much do guests turn 
their heads? The data confirmed what many researchers describe as the dirty secret of VR. In many scenarios, 
people in HMDs do not turn their heads very much. Figure 6 shows a top-view polar histogram of head yaw; 
for a guest facing right, the length of each line shows the proportional amount of time spent at each 
angle. Figure 7 shows a conventional histogram of guest head yaw; the height of each bar is the portion 
of total time spent at that angle. portion of time at angle 0.045 0.04 0.035 0.03 0.025 0.02 0.015 0.01 
0.005 0 Figure 8: Male vs. Female Head Yaw Figure 9 shows that head pitch, or up/down tilt, is even more 
confined than head yaw. portion of time at angle 0.05 Figure 6: Polar Histogram of Head Yaw 0.04 0.03 
0.02 0.01 0 head pitch angle Figure 9: Head Pitch We were not surprised that guests looked straight 
ahead most of the time. However, we were surprised by the following: instead of portion of total time, 
Figure 10 graphs the widest yaw angle guests ever experienced. One way to read this graph is that 90% 
of the guests never looked more than 75 degrees to either side. One could infer that building a 150 degree 
wide, screen-based display would be as good as an HMD for 90% of the guests. That conclusion would ignore 
that the HMD field of view must be added to the head yaw, and that the HMD also prevents visual intrusion 
from the real world.  TELLING STORIES IN VR Given that VR can present a compelling illusion, researchers 
can and should pursue its uses for education, training, medical applications, games, and many other purposes. 
As a storytelling company, we are focusing on using VR as a story­telling medium. Script vs. Guest Controlled 
Cameras Our first system was the first of many steps towards telling stories in VR. Our next show contains 
over twenty scenes, approximately fifteen high-fidelity characters, and a narrative story line that includes 
the ability to alter the sequence of events. Our guest assumes a role in an immersive feature film. The 
major challenge of allowing the guest to become a character is that the director gives up control of 
the narrative. While this is true of many interactive, non-HMD based games, the problem becomes acute 
with an HMD. Because we let the guest control the viewpoint we must build characters and scenes that 
look good from all vantage points. By establishing entrances to scenes we control the initial view of 
each scene, a technique used in well-designed theme parks. The inability to cut from scene to scene or 
view to view is very frustrating for content authors. We have experimented with having characters that 
are attached to the guest s head, and appear to be hanging off the front of the HMD. This allows us to 
interject a brief scene including that character. There is an intrinsic conflict between a pre-constructed 
narrative and a guest-controlled exploration. An interactive system can dynamically re-configure the 
story to avoid omission of critical portions. As our director said, "It's as if you decide to leave a 
movie early, and the projectionist edits the film to make sure you see the important ending before you 
leave." In other perceptually intensive theme park attractions such as effects-laden stereoscope films 
or platform simulators, we have learned to keep the story line simple and clear. We must do the same 
for VR. Our initial experience indicates that VR is good at placing guests in an environment, and we 
look forward to seeing its storytelling capacities evolve. All our experiences to date have been with 
a novice audience. Filmmakers once used devices such as tearing away calendar pages to show flashbacks 
or passage of time. As the audience became more experienced, these devices became unnecessary. Controlling 
the Narrative We are fairly successful at composing a scene that draws the guest s attention to a desired 
spot. We have also experimented with using characters to direct attention. In some scenes characters 
point where we want the guest to look, and in others, we have a character move to be in line with another 
object we want in view. All these techniques can be quickly tried; the key is to test them on novices. 
We have experimented with explicit techniques for controlling the guest s position, such as having a 
character grab the carpet and drag the guest to a desired location. Another coarse grain technique is 
to close doors behind guests to keep them from back-tracking. We have also experimented with implicit 
techniques such as a water skiing tow rope metaphor [13], where an invisible boat is controlling the 
eventual position and the guest is free to fly within a moving envelope. Sound In films, the sound track, 
particularly the musical score, tends to carry the emotional tone for most scenes. Because we no longer 
control timing we must choose sound tracks that work with wide variation in duration, and we must be 
able to make the transition smoothly from one ambient sound to the next based on guest actions. Many 
VR system architects are concerned with the underlying technology for localizing sound. In our experience, 
the careful selection/creation of ambient sounds and music, i.e. the content, is much more important 
than the specific details, or even the use of, sound localization.  AUTHORING In the process of building 
our first show we have learned a number of lessons regarding the process of authoring in VR. Rapid prototyping 
is essential for authoring. Flight simulator technologies often guarantee rendering frame rates, but 
require long (many hour) periods to change the show s content. Our SAL/Scheme layer allows code interpretation 
at run time, similar to MR/OML [22], Alice [16], and World Toolkit [21]. We could not have developed 
the show without this interpreted layer. Character animation in our second system approaches the look 
of traditional animation. This is not surprising, since the principles of animation apply regardless 
of the medium [15]. The key to achieving this was involving the artists in the development of the underlying 
technology, rather treating it as a given. We can now generate a new scene or character animation in 
under a week. The fidelity trap for VR is that unlike many other media, a low-quality quick and dirty 
mockup is often misleading. Since a partial or low-quality mockup may not yield accurate results when 
we test guests on it, we often must build systems to completion before we know what works well. This 
is partially because there are not yet good tools for sketching three dimensional scenes and animations. 
Motion capture vs. key frame animation: Our first system used motion capture to animate characters. This 
allowed us to produce a large amount of animation quickly, but the quality was not as high as the key 
frame-based animation we are using for the second version. Motion capture is troublesome for non­human 
characters, often seems too realistic, and requires laborious post-processing of the data. Branching 
story : In a linear narrative, a character s behavior is completely pre-planned. When the guest s actions 
can cause a character to perform different pre-animated sequences, we refer to that as a branch. In the 
original show most characters performed a repose animation until the guest approached, and then branched 
to perform a reaction animation. While this makes an interesting and active scene, in most cases it does 
not provide enough different branches to allow the guest to easily suspend disbelief. Autonomous characters 
vs. authored branching: Artificially intelligent characters are an interesting concept, but it will be 
a long time before they are believable in any but the simplest background role. For the next few years, 
we feel that believable character performances will be made up of branches of pre-animated material rather 
than computer programs for several reasons: 1) thinking characters are far enough into the future to 
be off the planning horizon, 2) characters who can construct decent sounding sentences are not much closer, 
3) a good animator can achieve a much more believable dramatic performance than a computer program, and 
4) even simple branching works when properly done. In our second version characters have multiple possible 
behaviors that are triggered by context. Even our simplest characters have a default behavior, a reaction 
to the guest s presence, and a reaction to the guest s departure. A major technical challenge is to smoothly 
blend between various pre­defined animations [17]. Rotation of characters to face guests is important 
to present the illusion that characters are real. We find that first turning a character s head, then 
his or her body, works best. The technical challenge is to avoid bad interactions between the automatic 
rotation and the character s key frame animation. RESEARCH CHALLENGES Based on our experiences, we present 
the following as open challenges to the research community: 1) Finding mechanisms that allow guests to 
self-calibrate the intensity of the experience. Currently we must keep the experience tame enough to 
be enjoyable for our more sensitive guests. 2) Developing constraints to solve the six-degree-of-freedom 
problems in controlling flight; i.e. navigation and motion through virtual spaces. 3) Development of 
software to better support animators, especially in the sketching phase. Animators use onion skin paper 
to superimpose views from multiple frames; this ability is lacking in most software tools. 4) The automatic 
generation of mouth animation from sound source. This is currently labor intensive and not particularly 
creative work. CONCLUSIONS This project gave over 45,000 people a first exposure to virtual reality 
(VR). While we have made what we consider to be substantial advances in HMD and rendering technology, 
our major advances have been in learning how to create and present compelling virtual environments. We 
stress that this is an exercise that requires both artistic and engineering talent and creativity. Our 
guests completed written surveys, and with subsets we logged head and carpet motions. Based on that data 
and interviews before, during and after guest flights, we conclude that: Guests suspend disbelief. The 
illusion is compelling enough that most guests accept being in a synthetically generated environment 
and focus on the environment, not the technology. VR appeals to everyone. Both genders and all ages had 
similar responses to our attraction. This leads us to conclude that VR is like feature films in that 
different content may segment the market, but the basic technology does not. We also note that wheelchair 
guests find mobility within VR extremely exciting. VR must be personally experienced. VR is foreign enough 
that most people can not comprehend it without direct personal experience. Fidelity matters. To get most 
guests to suspend disbelief requires extremely high fidelity. We provide 60 frames per second (at the 
expense of stereo), for polygonal models with hand-painted texture maps, and we do not use hardware lighting. 
Texture quality matters much more than polygon count. Content matters. People love the experience of 
VR, but even at high fidelity VR by itself is not enough. The public, unlike the developers, is not impressed 
with the technology. In fact, the public assumes that high fidelity VR exists and immediately focuses 
on what there is to do in the synthetic environment. The illusion of presence is fragile. Although guests 
suspend disbelief, inconsistencies can instantly shatter the illusion. For example, objects inter-penetrating, 
or characters not responding to the guest s presence completely shatter the sense of presence. Guests 
need a background story. VR is an overwhelming experience of being thrust into a new environment. A good 
way to soften this transition is to provide a background story that familiarizes the guest with the new 
environment before the immersion. This is a standard technique in theme park attractions, typically provided 
in a pre-show. Guests need a goal. Guests need to know why they are in the virtual world and what they 
are supposed to do. Guests do not turn their heads much. We were surprised at how little people turned 
their heads in this flight-based experience. We attribute this to the mass of the HMD, the need to look 
where one is flying, and guests inexperience with a head-tracked medium. Input controls are hard. We 
developed a novel input mechanism for controlling flight. Since no one flies magic carpets in the real 
world we could not transfer everyday skills. After many design iterations we believe that six axis control 
is a phenomenally difficult problem and conclude that designers must limit degrees of freedom. Tell a 
straightforward story. As we have learned with other intensive media, such as effects laden stereoscopic 
films and motion-base simulators, when the guest is perceptually overwhelmed it helps to keep the story 
short and clear. Aladdin is a beginning, not an end. Our original goal was to move past the technology. 
Our first system produces a compelling illusion and our next efforts are to examine whether we can tell 
stories in this new medium. Our second version of the project, scheduled for release in early 1996, contains 
a large number of characters and a narrative story line. ACKNOWLEDGMENTS This work is the effort of 
many talented people over several years; we mention here only a subset, but express our gratitude to 
all involved. Special thanks, in alphabetical order, to: Daniele Colajacomo, for managing the character 
modeling in the EPCOT show; Dave Fink, for helping start the project; Phillip Freer and Gary Daines, 
for their art direction and world design; Andy Ogden, for his industrial design on the steering and HMD; 
George Scribner, for his work on story and character in the EPCOT show; and Dave Spencer, for his management 
of the EPCOT show installation. We thank all the other artists and engineers who worked on this project, 
and we would especially like to express our deepest gratitude to the families and significant others 
who supported these individuals in their efforts. We would also like to thank Evans &#38; Sutherland, 
Silicon Graphics, NASA Ames Research Center, the staff who ran the attraction at EPCOT Center, and Matt 
Conway.  REFERENCES [1] Jon Airey, John Rohlf, Frederick Brooks, Towards Image Realism with Interactive 
Update Rates in Complex Virtual Building Environments, ACM SIGGRAPH Special Issue on 1990 Symposium on 
Interactive 3D Graphics 24:2, 1990, pages 41-50. [2] Walt Disney Home Video. Distributed by Buena Vista 
Home Video, Dept. CS, Burbank, CA, 91521. ISBN 1­55890-663-0. Originally released in 1992 as a motion 
picture. [3] Chuck Blanchard, Scott Burgess, Young Harvill, Jaron Lanier, Ann Lasko, Reality Built for 
Two: A Virtual Reality Tool, ACM SIGGRAPH 1990 Symposium on Interactive 3D Graphics, March 1990. [4] 
J. P. Blauert, Spatial Hearing, MIT Press, Cambridge, MA, 1983. [5] Frederick Brooks, Walkthrough --A 
Dynamic Graphics System for Simulating Virtual Buildings, Proceedings of the 1986 Workshop on 3D Graphics, 
Chapel Hill, NC, October 23-24, 1986, ACM, pages 9-21. [6] Steve Bryson, Creon Levit, The Virtual Wind 
Tunnel, IEEE Computer Graphics and Applications, July 1992, pages 25-34. [7] Christopher Codella et al., 
Interactive Simulation in a Multi-Person Virtual World, Proceedings of the ACM SIGCHI Human Factors in 
Computer Systems Conference, May 1992, pages 329-334. [8] James Cutting and Peter Vishton, Perceiving 
Layout and Knowing Distances: The Integration, Relative Potency, and Contextual Use of Different Information 
About Depth, Handbook of Perception and Cognition: Perception of Space and Motion, Vol. 5, Academic Press 
(to appear). [9] R. Kent Dybvig. The Scheme Programming Language. Prentice-Hall, 1987. [10] S. S. Fisher, 
A M. McGreevy, J. Humphries, W. Robinett, Virtual Environment Display System, Proceedings on the 1986 
Workshop on Interactive 3D Graphics, pages 77-87, October 23-24, 1986. [11] Thomas Funkhouser, Carlo 
Sequin, Seth Teller, Management of Large Amounts of Data in Interactive Building Walkthroughs, Proceedings 
of the 1992 ACM Symposium on Interactive Three-Dimensional Graphics, April, 1992, pages 11-20. [12] Thomas 
Furness, The Super Cockpit and Human Factors Challenges, Human Interface Technology (HIT) Laboratory 
of the Washington Technology Center, Tech Report HITL-M-886-1, October, 1986. [13] Tinsley A. Galyean, 
Guided Navigation of Virtual Environments, 1995 ACM Symposium on Interactive 3D Graphics, April 1995, 
pages 103-104, Monterey, CA. [14] Id Software, Inc.; information available via http://www.idsoftware.com/ 
. [15] John Lasseter, Principles of Traditional Animation Applied to 3D Computer Animation, Computer 
Graphics (SIGGRAPH '87 Proceedings) Volume 21, number 4, pages 35-44, July 1987. [16] Randy Pausch, et 
al, A Brief Architectural Overview of Alice, a Rapid Prototyping System for Virtual Reality, IEEE Computer 
Graphics and Applications, May 1995. [17] Ken Perlin, Real Time Responsive Animation with Personality, 
IEEE Transactions on Visualization and Computer Graphics, Vol. 1, No. 1. [18] A. Pope, BBN Report No 
7102. The SIMNET Network and Protocols. BBN Systems and Technologies, Cambridge, Massachusetts, 1989. 
[19] John Rohlf and James Helman, IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 
3D Graphics, SIGGRAPH '94 Conference Proceedings, Computer Graphics, July, 1994. [20] William J. Schroeder, 
Jonathan A. Zarge, William E. Lorensen, Decimation of Triangle Meshes, Computer Graphics (SIGGRAPH '92 
Proceedings) Volume 26, pages 65-70, July 1992. [21] Sense8 Corporation, 100 Shoreline Highway, Suite 
282, Mill Valley, CA 94941, 415/331-6318, http://www.sense8.com/ , info@sense8.com . [22] Chris Shaw, 
Mark Green, Jiandong Liang, Yunqi Sun, Decoupled Simulation in Virtual Reality with the MR Toolkit, ACM 
Transactions on Information Systems, 11:3, pages 287-317, July 1993. [23] Ivan Sutherland, The Ultimate 
Display, Proceedings of IFIP (International Federation of Information Processing) 65, Vol. 2, pages 506-508. 
[24] Ivan Sutherland, A Head-mounted Three-dimensional Display, Proceedings of the Fall Joint Computer 
Conference, AFIPS, Vol. 33, pages 757-764. [25] Virtuality, http://www.virtuality.com, UK office: Virtuality 
House, 3 Oswin Road, Brailsford Industrial Park, Leicester LE3 1HR, United Kingdom, Tel: +44(0) 116 233 
7000, enquiries@virtuality.com . USA office: 7801-7805 Mesquite End Drive, Suite 105, Irving, Texas 75063, 
USA 001-214-556-1800, 1-800-ILLUSION, enquiries@tx.viruality.com. [26] E. Wenzel, F. Wightman, S. Fisher, 
A Virtual Display System for Conveying Three-dimensional Acoustic Information, Proceedings of the Human 
Factors Society, 32nd Annual Meeting, 1988, pages 86-90.  Figure 1: A Guest s View of the Virtual Environment 
 Figure 2: The Head Mounted Display High-resolution TIFF versions of these images can be found on the 
CD-ROM in S96PR/papers/pausch  Figure 3: The Physical Setup Figure 5: Animated Characters High-resolution 
TIFF versions of these images can be found on the CD-ROM in S96PR/papers/pausch  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237258</article_id>
		<sort_key>205</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Improv]]></title>
		<subtitle><![CDATA[a system for scripting interactive actors in virtual worlds]]></subtitle>
		<page_from>205</page_from>
		<page_to>216</page_to>
		<doi_number>10.1145/237170.237258</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237258</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.5</cat_node>
				<descriptor>Expert system tools and techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003241.10003243</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Decision support systems->Expert systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39077159</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Research Laboratory, Department of Computer Science, New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14172646</person_id>
				<author_profile_id><![CDATA[81100493386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Athomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Research Laboratory, Department of Computer Science, New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>111154</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[N. Badler, B. Barsky, D. Zeltzer, Making Them Move: Mechanics, Control, and Animation of Articulated Figures Morgan Kaufmann Publishers, San Mateo, CA, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>162261</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[N. Badler, C. Phillips, B. Webber, Simulating Humans: Computer Graphics, Animation, and Control Oxford University Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Bates, A. Loyall, W. Reilly, Integrating Reactivity, Goals and Emotions in a Broad Agent, Proceedings of the 14th Annual Conference of the Cognitive Science Society, Indiana, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218405</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[B. Blumberg, T. Galyean, Multi-Level Direction of Autonomous Creatures for Real-Time Virtual Environments Computer Graphics (SIGGRAPH '95 Proceedings), 30(3):47--54, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A. Bruderlin, L. Williams, Motion Signal Processing, Computer Graphics (SIGGRAPH '95 Proceedings), 30(3) :97-- 104, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[R. Brooks. A Robust Layered Control for a Mobile Robot, /EEE Journal of Robotics and Automation, 2(1):14--23, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74358</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. Chadwick, D. Haumann, R. Parent, Layered construction for deformable animated characters. Computer Graphics ( SIGGRAPH '89 Proceedings), 23(3):243--252, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. Ebert and et. al., Texturing and Modeling, A Procedural Approach Academic Press, London, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325244</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Girard, A. Maciejewski, Computational modeling for the computer animation of legged figures. Computer Graphics (SIGGRAPH '85 Proceedings), 20(3):263--270, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. Hodgins, W. Wooten, D. Brogan, J O'Brien, Animating Human Athletics, Computer Graphics (SIGGRAPH '95 Proceedings), 30(3):71--78, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>239975</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Johnson, WavesWorld: PhD Thesis, A Testbed for Three Dimensional Semi-Autonomous Animated Characters, MIT, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Karaul, personal communication]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791451</ref_obj_id>
				<ref_obj_pid>791214</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[P. Maes, T. Darrell and B. Blumberg, The Alive System: Full Body Interaction with Autonomous Agents in Computer Animation'95 Conference, Switzerland, April 1995 .IEEE Press, pages 11-18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[M. Minsky, Society of Mind, MIT press, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>93274</ref_obj_id>
				<ref_obj_pid>93267</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[C. Morawetz, T. Calvert, Goal-directed human animation of multiple movements. Proc. Graphics Interface}, pages 60--67, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[K. Perlin, An image synthesizer. Computer Graphics (SIGGRAPH '85 Proceedings)}, 19(3):287--293, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[K. Perlin, Danse interactif. SIGGRAPH '94 Electronic Theatre, Orlando.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[K. Perlin, Real Time Responsive Animation with Personality, IEEE Transactions on Visualization and Computer Graphics, 1(1), 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[K. Perlin, A. Goldberg, The Improv System Technical Report NYU Department of Computer Science, 1996. (online at http://www.mrl.nyu.edu/improv)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[K. Sims, Evolving virtual creatures. Computer Graphics (SIGGRAPH '94 Proceedings) }, 28(3):15--22, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[N. Stephenson, Snow Crash Bantam Doubleday, New York, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>145886</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[S. Strassman, Desktop Theater: Automatic Generation of Expresssive Animation, PhD thesis, MIT Media Lab, June 1991 (online at http://www.method.com/straz/straz-phd.pdf)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667945</ref_obj_id>
				<ref_obj_pid>1667943</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos, X. Tu, and R. Grzesczuk Artificial Fishes: Autonomous Locomotion, Perception, Behavior, and Learning in a Simulated Physical World, Artificial Life, 1(4):327-351, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[A. Witkin, Z. Popovic, Motion Warping Computer Graphics (SIGGRAPH '95 Proceedings), 30(3):105-108, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Improv: A System for Scripting Interactive Actors in Virtual Worlds Ken Perlin / Athomas Goldberg Media 
Research Laboratory Department of Computer Science New York University ABSTRACT Improv is a system for 
the creation of real-time behavior-based animated actors. There have been several recent efforts to build 
network distributed autonomous agents. But in general these efforts do not focus on the author s view. 
To create rich interactive worlds inhabited by believable animated actors, authors need the proper tools. 
Improv provides tools to create actors that respond to users and to each other in real-time, with personalities 
and moods consistent with the author s goals and intentions. Improv consists of two subsystems. The first 
subsystem is an Animation Engine that uses procedural techniques to enable authors to create layered, 
continuous, non-repetitive motions and smooth transitions between them. The second subsystem is a Behavior 
Engine that enables authors to create sophisticated rules governing how actors communicate, change, and 
make decisions. The combined system provides an integrated set of tools for authoring the "minds" and 
"bodies" of interactive actors. The system uses an english-style scripting language so that creative 
experts who are not primarily programmers can create powerful interactive applications. INTRODUCTION 
 Believability And Interaction Cinema is a medium that can suspend disbelief; the audience enjoys the 
psychological illusion that fictional characters have an internal life. When this is done properly, these 
characters can take the audience on a compelling emotional journey. Yet cinema is a linear medium; for 
any given film, the audience s journey is always the same. Likewise, the experience is inevitably a passive 
one as the audience s reactions can have no effect on the course of events. This suspension of disbelief, 
or believability, does not require realism. For example, millions of people relate to Kermit the Frog 
and to Bugs Bunny as though they actually exist. Likewise, Bunraku puppet characters can create for their 
audience a deeply profound and moving psychological experience. NYU-MRL, 719 Broadway 12th Floor, New 
York, NY 10003 Fax: (212) 995-4122 Web: http://www.mrl.nyu.edu Email: perlin@nyu.edu | athomas@mrl.nyu.edu 
Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 All of these media have one thing in common. Every moment of the 
audience s journey is being guided by talented experts, whether an screenwriter and actor/director, a 
writer/animator, or a playwright and team of puppeteers. These experts use their judgment to maintain 
a balance: characters must be consistent and recognizable, and must respond to each other appropriately 
at all times. Otherwise believability is lost. In contrast, current computer games are non-linear, offering 
variation and interactivity. While it is possible to create characters for these games that convey a 
sense of psychological engagement, it is extremely difficult with existing tools. One limitation is that 
there is no expert, no actor, director, animator or puppeteer, actually present during the unfolding 
drama, and so authors using existing techniques are limited by what they can anticipate and produce in 
advance. In this paper, we discuss the problem of building believable characters that respond to users 
and to each other in real-time, with consistent personalities, properly changing moods and without mechanical 
repetition, while always maintaining an author s goals and intentions. We describe an approach in which 
actors follow scripts, sets of author-defined rules governing their behavior, which are used to determine 
the appropriate animated actions to perform at any given time. We also describe a behavioral architecture 
that supports author-directed multi-actor coordination as well as run-time control of actor behavior 
for the creation of user-directed actors or avatars. Next, we describe how the system has been implemented 
using an "english-style" scripting language and a network distribution model to enable creative experts, 
who are not primarily programmers, to create powerful interactive applications. Finally, we discuss our 
experiences with the system and future work. Related Work The phrase "Desktop Theater" was coined by 
Steve Strassman [Strassman91]. His philosophy was quite similar to ours. Yet because his work slightly 
predated the age of fast graphical workstations, it did not deal with real time visual interaction. But 
there was already the emphasis on expressive authoring tools for specifying how characters would respond 
to direction. characters would respond to direction. Stephenson also influenced this work. That novel 
posits a "Metaverse", a future version of the Internet which appears to its participants as a quasi-physical 
world. Participants are represented by fully articulate human figures, or avatars. Body movements of 
avatars are computed automatically by the system. Snow Crash specifically touches on the importance of 
proper authoring tools for avatars, although it does not describe those tools. Our system takes these 
notions further, in that it supports autonomous figures that do not directly represent any participant. 
Most autonomous actor simulation systems follow the parallel layered intelligence model of [Minsky86], 
which was partially implemented by the subsumption architecture of [Brooks86] as well as in [Bates92] 
and [Johnson94]. Several systems have been developed which share this layered architecture with Improv, 
yet which solve distinctly different problems. The Jack system of [Badler93] focuses on proper task planning 
and biomechanical simulation, as does [Hodgins95] The general goal is to produce accurate simulations 
of biomechanical robots. Similarly, the simulations of Terzopoulis et. al [Terzopoulos94] has simulated 
autonomous animal behaviors that respond to their environment according to biomechanical rules. Autonomous 
figure animation has been studied by [Badler91], [Girard85], [Morawetz90] and [Sims94]. The Alive system 
of [Maes95] and [Blumberg95] focuses on self-organizing embodied agents, which are capable of making 
inferences and of learning from their experiences. Instead of maximizing an authors ability to express 
personality, the Alive system use ethological mechanisms to maximize the actor s ability to reorganize 
its own personality, based on its own perception and accumulated experience. APPROACH Improv: An Expert 
System For Authors As an authoring system, Improv must provide creative experts with tools for constructing 
the various aspects of an interactive application. These must be intuitive to use, allow for the creation 
of rich, compelling content, and produce behavior at run-time which is consistent with the author s vision 
and intentions. Animated actors must be able to respond to a wide variety of user-interactions, in ways 
that are both appropriate and non-repetitive. This is complicated by the fact that in applications involving 
several characters, these actors must be able to work together while faithfully carrying out the author 
s intentions. The author needs to control the choices an actor makes and how the actors move their bodies. 
 ARCHITECTURE The behavior model used by Improv is similar to that proposed by [Blumberg95] in that 
it consists of geometry that is manipulated in real-time, an Animation Engine which utilizes descriptions 
of atomic animated actions (such as Walk or Wave) to manipulate the geometry, and a Behavior Engine which 
is responsible for higher-level capabilities, (such as going to the store, or engaging another actor 
in a conversation), and decisions about which animations to trigger. In addition, the Behavior Engine 
maintains the internal model of the actor, representing various aspects of an actor s moods, goals and 
personality. The Behavior Engine constitutes the mind of the actor. An run-time, an actor s movements 
and behavior are computed by iterating an update cycle that alternates between the Animation and Behavior 
Engines. is a generalization of the system presented in [Perlin 95]. Actors are able to move from one 
animated motion to another in a smooth and natural fashion in real time. Motions can be layered and blended 
to convey different moods and personalities. The Animation Engine controls the bodyofthe actor. Geometry 
An animator can build any variety of articulated character. Actors can be given the form of humans, animals, 
animate objects, or fantasy creatures. An actor consists parts that are connected by rotational joints. 
The model can be deformable, which is useful for muscle flexing or facial expressions as illustrated 
in [Chadwick89]. Degrees Of Freedom Authors specify individual actions in terms of how those actions 
cause changes over time to each individual degree of freedom (DOF) in the model. The system then combines 
these DOF values to make smooth transitions and layerings among actions. There are various types of DOFs 
that an author can control. The simplest are the three rotational axes between any two connected parts. 
Examples of this are head turning and knee bending. The author can also simply position a part, such 
as a hand or a foot. The system automatically does the necessary inverse kinematics to preserve the kinematic 
chain. From the author s point of view, the x,y,z coordinates of the part are each directly available 
as a DOF. The author can also specify part mesh deformations as DOFs. To make a deformation, the author 
must provide a "deformation target," a version version of the model (or just some parts of the model) 
in which some vertices have been moved. For each deformation target, the Improv system detects which 
vertices have been moved, and builds a data structure containing the x,y,z displacement for each such 
vertex. For example, if the author has provided a smiling face as a deformation target, then the (s)he 
can declare SMILE to be a DOF. The author can then specify various values for SMILE between 0. (no smile) 
and 1. (full smile). The system handles the necessary interpolation between mesh vertices. In the particular 
case of smiling, the author can also specify negative values for SMILE, to make the face frown. figure 
2. Flexing a deformable mesh. Continuous Signal Generation The author defines an action simply as a list 
of DOFs, together with a range and a time varying expression for each DOF. Most actions are constructed 
by varying a few DOFs over time via combinations of sine, cosine and coherent noise. For example, sine 
and cosine signals are used together within actions to impart elliptical rotations. One of the key ingredients 
to realism in Improv characters is the ability to apply coherent noise. This mechanism was originally 
developed for procedural textures [Perlin85][Ebert94]. In the current work it is used in essentially 
the same way. Using noise in limb movements allows authors to give the impression of naturalistic motions 
without needing to incorporate complex simulation models. For example, coherent noise can be used to 
convey the small motions of a character trying to maintain balance, the controlled randomness of eye 
blinking, or the way a character s gaze wanders around a room. Although in real life each of these examples 
has a different underlying mechanism, viewers do not perceive the mechanism itself. Instead they perceive 
some statistics of the motion it produces. When coherent noise is applied in a way that matches those 
statistics, the actor s movements are believable. The author can also import keyframed animation from 
commercial modeling systems, such as Alias or SoftImage. The Improv system internally converts these 
into actions that specify time varying values for various DOFs. To the rest of the system, these imported 
actions look identical to any other action. Defining Actions The author uses DOF s to build actions. 
Below are three different actions that define how an actor might gesture with his arm while talking. 
Each one uses several frequencies of noise to modulate arm movement. The first two are general hand waving 
gestures, while the third shakes the arm more emphatically, as though pointing at the listener. On each 
line of an action, the part name is followed first by three angular intervals, and then by three time-varying 
interpolants in braces. Each interpolant is used to compute a single angle in its corresponding interval. 
The results are applied to the part as Pitch, Roll and Yaw rotations respectively. The angle intervals 
are constant over time, whereas the time varying interpolants are reevaluated at each update cycle. For 
example, in the first line below, if N0 possesses the value 0.5 at some time step then the resulting 
Pitch rotation at that time step will be 0.5 of the way between 25 degrees and 55 degrees, or 40 degrees. 
define ACTION "Talk Gesture1" { R_UP_ARM 25:55 0 -35:65 { N0 0 N0 } R_LO_ARM 55:95 0 0 {N10 0} R_HAND 
-40:25 75:-25 120 { N1 N2 0 } } define ACTION "Talk Gesture2" { R_UP_ARM 10:47 0 -10:45 { N0 0 N0 } 
R_LO_ARM 35:77 0 0 {N1 0 0} R_HAND -53:55 -40:15 120 { N1 N2 0 } } define ACTION "Talk_Gesture3" { R_UP_ARM 
45 20:15 0 { 0 N0 N0 } R_LO_ARM 70:120 0 0 { N1 0 0 } R_HAND 40:15 0 120 {N2 0 0} } The variables N0, 
N1 and N2 are shorthand that the Improv system provides the author to denote time varying coherent noise 
signals of different frequencies. N1 is one octave higher than N0, and N2 is one octave higher than N1. 
The value of each signal varies between 0.0 and 1.0. Note that the upper arm movement is controlled by 
N0, whereas the lower arm movement is controlled by N1. The result is that the upper arm will, on the 
average, swing back and forth about the shoulder once per second, whereas the lower arm will, on the 
average, swing back and forth about the elbow twice per second. Meanwhile, the hand will make small rapid 
rotations about the wrist. These frequencies were chosen simply because they looked natural. In our tests, 
frequency ratios that varied significantly from these did not look natural. Presumably this frequency 
ratio reflects the fact that the lower arm has about half as much mass as the total arm, and therefore 
tends to swing back and forth about twice as frequently. Action Compositing An Improv actor can be doing 
many things at once, and these simultaneous activities can interact in different ways. For example, an 
author may want an actor who is waving to momentarily scratch his head with the same hand. It would be 
incorrect for the waving movement to continue during the time the actor is scratching his head. The result 
could be strange. For example, actor might try to feebly to wave while his arm while making vague scratching 
motions about his cranium. Clearly in this case we want to decrease the amount of waving activity as 
we increase the scratching activity. Some sort of ease-in/out transition is called for. In contrast, 
suppose we want an actor to scratch his head for a moment while walking downstage. It would be incorrect 
if the Improv system were to force the actor to stop walking every time he scratched his head. In this 
case, an ease-in/out transition would be inappropriate. The difference between these two examples is 
that the former situation involves two actions which cannot coexist, whereas the latter situation involves 
two actions that can gracefully coexist. The authoring system should provide a mechanism to allow authors 
to make these distinctions in an easy and unambiguous way. To do this, Improv contains a simple set of 
rules. The approach we take is borrowed from image compositing methods. The Improv author thinks of motion 
as being layered, just as composited images can be layered back to front. The difference is that whereas 
an image maps pixels to colors, an action maps DOFs to values. The author can place actions in different 
groups, and these groups are organized into a "back-to-front" order. Also the author may "select" any 
action. Given this structure, the two compositing rules are as follows: (1) Actions which are in the 
same group compete with each other. At any moment, every action possesses some weight, or opacity. When 
an action is selected, its weight transitions smoothly from zero to one. Meanwhile, the weights of all 
other actions in the same group transition smoothly down to zero. (2) Actions in groups which are further 
forward obscure those in groups which are further back.  Using this system, authors place actions which 
should compete with each other in the same group. Some actions, such as walking, are fairly global in 
that they involve many DOFs through the body. Others, such as head scratching, are fairly localized and 
involve relatively few DOFs. The author places more global actions in the rear-most groups. More localized 
actions are placed in front of these. Also, some actions are relatively persistent. Others are generally 
done fleetingly. Groups of very fleeting or temporary actions (like scratching or coughing) are placed 
still further in front. For the author, this makes it easy to specify intuitively reasonable action relationships. 
For example, suppose the author specifies the following action grouping: GROUP Stances ACTION Stand ACTION 
Walk GROUP Gestures ACTION No_waving ACTION Wave_left ACTION Wave_right GROUP Momentary ACTION No_scratching 
ACTION Scratch_head_left Then let s say actions are selected in the following order: Stand Walk Wave_left 
Scratch_head_left No_scratching Wave_right The actor will start to walk. While continuing to walk he 
will wave with his left hand. Then he will scratch his head with his left hand, and resume waving again. 
Finally he will switch over to waving with his right hand. Because of the grouping structure, the author 
has easily imparted to the actor many convenient rules. For example, the actor knows to wave with either 
one hand or the other (not both at once), that he doesn t need to stop walking in order to wave or to 
scratch his head, and that after he s done scratching he can resume whatever else he was doing with that 
arm. Applying Actions To The Model At any animation frame, the run time system must assign a unique value 
to each DOF for the model, then move the model into place and render it. To compute these DOFs, the algorithm 
proceeds as follows. Within each group, a weighted sum is taken over the contribution of each action 
to each DOF. The values for all DOFs in every group are then composited, proceeding from back to front. 
The result is a single value for each DOF, which is then used to move the model into place. There are 
subtleties in this algorithm, such as correctly compositing inverse kinematic DOFs over direct rotational 
DOFs. But these are beyond the space limitations of this paper. For a full treatment of the DOF compositing 
algorithm, the reader is referred to [Perlin96]. The author is given tools to easily synchronize movements 
of the same DOF across actions; transitions between two actions that must have different tempos are handled 
by a morphing approach: During the time of the transition, speed of a master clock is continuously varied 
from the first tempo to the second tempo, so that the phases of the two actions are always aligned. This 
is similar to the approach taken by [Bruderlin95] and [Witkin95]. Action Buffering Sometimes it would 
be awkward for an actor to make a direct transition between two particular actions in a group. For example, 
let s say the actor has his hands behind his back, and then claps his hands. Because DOFs are combined 
linearly, the result would be that the actor passes his hands through his body! We allow the author to 
avoid such situations by declaring that some action in a group can be a buffering action for another. 
The system implements this by building a finite state machine that forces the actor to pass through this 
buffering action when entering or leaving the troublesome action. For example, the author can declare 
that the action hands-at-the-sides is a buffering action for hands-behind-the-back. Then when the actor 
transitions between hands-behind-the back and any other action, he will always first move his hands around 
the sides of his body. figure 3: Otto demonstrating action buffering. BEHAVIOR ENGINE Motivation Improv 
authors cannot create deterministic scenarios, because the user is a variable in the run-time system. 
The user s responses are always implicitly presenting the actor with a choice of what to do next. Because 
of this variability, the user s experience of an actor s personality and mood must be conveyed largely 
by that actor s probability of selecting one choice over another. As a very simple example, suppose the 
user often goes away for awhile, keeping an actor waiting for various amounts of time. If the actor usually 
sits down or naps before the user returns, then the actor will appear to the user as a lazy or tired 
character. The user is forming an impression based on probabilities. The influence of the author lies 
in carefully tuning of such probabilities. The goal of the behavior engine is to help the author to do 
so in the most expressive way possible. Mechanism The behavior engine provides several authoring tools 
for guiding an actor s behavioral choices. The most basic tool is a simple parallel scripting system. 
Generally speaking, at any given moment an actor will be executing a number of scripts in parallel. In 
each of these scripts the most common operation is to select one item from a list of items. These items 
are usually other scripts or actions for the actor (or for some other actor) to perform. The real power 
of the behavior engine comes from "probability shaping" tools we provide authors for guiding an actor 
s choices. The more expressive the tools for shaping these probabilities, the more believable actors 
will be, in the hands of a talented author. In the following sections we describe the working of the 
behavior engine. First we describe the basic parallel scripting structure. After that, we will describe 
the probability shaping tools. Scripts For an Interactive World If actions are the mechanism for continuous 
control of the movements made by an actor s body, then scripts are the mechanism for discrete control 
of the decisions made by the actor s mind. The author must assume that the user will be making unexpected 
responses. For this reason, it is not sufficient to provide the author with a tool for scripting long 
linear sequences. Rather, the author must be able to create layers of choices, from more global and slowly 
changing plans, to more localized and rapidly changing activities, that take into account the continuously 
changing state of the actor s environment, and the unexpected behavior of the human participant. In the 
next two sections, we first discuss how scripts are organized into layers, and then how an individual 
script operates. Grouping Scripts Like actions, scripts are organized into groups. However unlike actions, 
when a script within a group is selected, any other script that was running in the same group immediately 
stops. In any group at any given moment, exactly one script is running. Generally, the author organizes 
into the same group those scripts that represent alternative modes that an actor can be in at some level 
of abstraction. For example, the group of activities that an actor performs during his day might be: 
ACTIVITIES Resting Working Dining Conversing Performing In general, the author first specifies those 
groups of scripts that control longer term goals and plans. These tend to change slowly over time, and 
their effects are generally not immediately felt by the user. The last scripts are generally those that 
are most physical. They tend to choose actual body actions, in response to the user and to the state 
of higher level scripts. For example, an actor might contain the following groups of scripts, in order, 
within a larger set of scripts: ... DAY_PLANS Waking Morning Lunch Afternoon Dinner Evening ... ACTIVITIES 
Resting Working Dining Conversing Performing ... BEHAVIOR Sleeping Eating Talking Joking Arguing Listening 
Dancing We can think of the Animation Engine, with its groups of continuous actions, as an extension 
of this grouping structure to even lower semantic levels. Individual Scripts A script is organized as 
a sequence of clauses. At run-time, the system runs these clauses sequentially for the selected script 
in each group. At any update cycle, the system may run the same clause that it ran on the previous cycle, 
or it may move on to the next clause. The author is provided with tools to "hold" clauses in response 
to events or timeouts. The two primary functions of a script clause are 1) to trigger other actions or 
scripts and 2) to check, create or modify the actor s properties Triggering Actions and Scripts The simplest 
thing an author can do within a script clause is trigger a specific action or script, which is useful 
when the author has a specific sequence of activities (s)he wants the actor to perform. In the following 
example, the actor walks onstage, turns to the camera, bows, and then walks offstage again. define SCRIPT 
"Curtain Call" { "walk to center" } { continue until { my location equals center } } { "turn to camera" 
} { continue until { "turn to camera" is done } } { "bow" } { continue for 3 seconds } { "walk offstage" 
} In this case, phrases in quotes represent scripts or actions. Each of these scripts might, in turn, 
call other scripts and/or actions. The other information (continue, etc) is used by Improv to control 
the timing of the scene. Layered Behavior Through layering, an author can create complex behaviors from 
simpler scripts and actions. Take the following example: define SCRIPT "greeting" { { "enter" } { wait 
4 seconds } { "turn to camera" } { wait 1 second } { "wave" for 2 seconds "talk" for 6 seconds } { wait 
3 seconds } { "sit" } { wait 5 seconds } { "bow" toward "Camera" } { wait 2 seconds } { "leave" } } In 
this example, the actor first activates the "enter" script (which instructs the actor to walk to center). 
The "enter" script and "greeting" script are now running in parallel. The "greeting" script waits four 
seconds before activating the "turn to camera" script. This tells the actor to turn to face the specified 
target, which in this case is the camera. The script then waits one second, before instructing the actor 
to begin the "wave" and "talk" actions. The script waits another 3 seconds before activating the "sit" 
action during which time the "wave" action has ended, returning to the default "No Hand Gesture" action 
in its group. Meanwhile, the "talk" action continues for another three seconds after the actor sits. 
Two seconds later the actor bows to the camera, waits another two seconds and then leaves. Non-Deterministic 
(Stochastic) Behavior In addition to commands that explicitly trigger specific actions and scripts, Improv 
provides a number of tools for generating the more non-deterministic behavior required for interactive 
non-linear applications. An author may specify that an actor choose randomly from a set of actions or 
scripts. as in the following example: SCRIPT "Rock Paper Scissors" { choose from { "Rock" "Paper" "Scissors" 
} } Once an action or script is chosen it is executed as though it had been explicitly specified. Alternately, 
the author can specify weights associated with each item in the choice. These weights are used to affect 
the probability of each item being chosen, as in the following example: define SCRIPT "Rock Paper Scissors2" 
{ choose from { "Rock" .5 "Paper" .3 "Scissors" .1 } } In this case, there is a 5/9 chance the actor 
executing this script will choose the "Rock" action, 3/9 that the actor will choose "Paper", and a 1/9 
chance the actor will pick "Scissors". The decision is still random, but the author has specified a distinct 
preference for certain behaviors over others. In order to create believable characters, the author also 
needs to be able to have these decisions reflect an actor s mental state as well as the state of the 
actor s environment. An actor s decision about what to do may depend on any number of factors, including 
mood, time of day, what other actors are around and what they re doing, what the user is doing, etc. 
In Improv, authors can create decision rules which take information about an actor and his environment 
and use this to determine the actor s tendencies toward certain choices over others. The author specifies 
what information is relevant to the decision and how this information influences the weight associated 
with each choice. As this information changes, the actor s tendency to make certain choices over others 
will change as well. Decision Rules Properties The information about an actor and his relationship to 
his environment are stored in an actor s properties. These properties may be used to describe aspects 
of an actor s personality, such as assertiveness, temperament or dexterity, an actor s current mood, 
such as happiness or alertness, or his relationship to other actors or objects, such as his sympathy 
toward the user or his attitude toward strained peas. These properties are specified by the author either 
when the actor is created, or else within a clause of a script, to reflect a change in the actor due 
to some action or event. The latter case is shown in the following example: define SCRIPT "Eat Dinner" 
{ "Eat" } { set my "Appetite" to 0 } { "Belch" } In this case, the author specifies how an actor s behavior 
is reflected in his personality by reducing the actor s appetite after eating. An author can also use 
properties to provide information about any aspect of an actor s environment, including inanimate props 
and scenery and even the scripts and actions an actor chooses from. An author can assign properties to 
actions and scripts describing the various semantic information associated with them, such as aggressiveness, 
formality, etc The author can then uses these values in the construction of decision rules. Decision 
rules allow actors to make decisions that reflect the state of the world the author has created. What 
Decision Rules Do When a decision rule is invoked, a list of objects is passed to it. The system then 
uses the decision-rule to generate a weight between zero and one for each object. This list can then 
be used to generate a weighted decision. Each decision rule consists of a list of author-specified factors: 
pieces of information that will influence the actor s decision. Each of these factors is assigned a weight 
which the author uses to control how much influence that piece of information has upon the decision. 
This information can simply be the value of a property of an object as in the following example: { choose 
from { "Steph" "Bob" "Sarah" } based on "who s interesting" } define DECISION-RULE "who s interesting" 
factor { his/her "Charisma" } influence .8 factor { his/her "Intelligence" } influence .2 In this example, 
the decision rule will use the "Charisma" and "Intelligence" properties of the three actors to generate 
a weight for each actor that will used in the decision. In this case, the author has specified that the 
value of an actor s "Charisma" will have the greatest influence in determining that weight, with "Intelligence" 
having a lesser role. The influence is optional and defaults to 1.0 if unspecified. The equations for 
determining these weights can be found in Appendix A: Decision Rule Equations. An author can also use 
the relationship between the actor and the various choices to influence a decision, by making "fuzzy" 
comparisons between their properties. For example: { choose from ("Fight" "Flee") based on "how courageous" 
} define DECISION-RULE: "how courageous" { factor { my "Courage" equals its "Courage Level" to within 
.5 } } Here, the author is comparing the actor s "Courage" property with the "Courage Level" property 
associated with the scripts "Fight" and "Flee". If the actor s "Courage" equals the script s "Courage 
Level" the decision rule will assign a weight of 1 to that choice. If the values aren t equal, a weight 
between 0 and 1 will be assigned based on the difference between them, dropping to 0 when the difference 
is greater than the "within" range. In this case, .5 . (The equations for this can be found in Appendix 
B: Fuzzy Logic Equations) As the actor s "Courage" increases or decreases, so will the actor s tendency 
toward one option or the other. An author may want an actor to choose from a set of options using different 
factors to judge different kinds of items. A list of objects passed to the decision rule may be divided 
into subsets using author-defined criteria for inclusion. The weights assigned to a given subset may 
be scaled, reflecting a preference for an entire group of choices over another. For example: { choose 
from ("Steph" "Bob" "Sarah") based on "who s interesting2" } define DECISION-RULE: "who s interesting2" 
{ subset "Those I d be attracted to" scale 1 factor { his/her "Intelligence" equals my "Confidence" to 
within .4 } subset "Those I wouldn t be attracted to" scale .8 factor { his/her "Intelligence" equals 
my "Intelligence" to within .4 } } define SUBSET: "Those I d be attracted to" { his/her "Gender" equals 
my "Preferred Gender" } define SUBSET: "Those I wouldn t be attracted to" { his/her "Gender" doesn t 
equal my "Preferred Gender" } Let s assume the actor is considered a heterosexual male (ie his "Gender" 
is "Male" and his "Preferred Gender" is "Female"). The weight assigned to "Steph" and "Sarah" will depend 
on how closely their intelligence matches our actor s confidence (being put off by less intelligent women 
and intimidated by more intelligent ones, perhaps). The factor used to judge "Bob" reflects a sympathy 
toward men who are his intellectual equal, unaffected by the actor s confidence. The scale values reflect 
a general preference for one gender over the other. Coordination Of Multiple Actors Ideally we would 
prefer to give an author the same control over groups of actors that (s)he has over individual actors. 
The proper model is that the author is a director who can direct the drama via pre-written behavior rules. 
To the author, all of the actors constitute a coordinated "cast", which in some sense is a single actor 
that just happens to have multiple bodies. For this reason, we allow actors to modify each other s properties 
with the same freedom with which an actor can modify his own properties. From the author s point of view, 
this is part of a single larger problem of authoring dramatically responsive group behavior. If one actor 
tells a joke, the author may want the other actors to respond, favorably or not, to the punchline. By 
having the joke teller cue the others actors to respond, proper timing is maintained, even if the individual 
actors make their own decisions about how exactly to react. In this way, an actor can give the impression 
of always knowing what other actors are doing and respond immediately and appropriately in ways that 
fulfill the author s goals. figure 4: Actors communicate with each other through a shared blackboard. 
This communication occurs through the use of a shared blackboard. The blackboard allows actors to be 
coordinated, even when running on a single processor, on multiple processors or across a network. USER-INTERACTION 
Multi-Level Control Of Actor State Creating and Modifying User Interface Elements An author can also 
include user-interface specifications in a actor s scripts, enabling widgets to be easily generated at 
run-time in response to actor behavior or to serve the needs of the current scene or interaction. The 
user can employ these widgets to trigger actions and scripts at any level of an actor s behavioral hierarchy. 
This enables users to enter the virtual environment, by allowing them to direct the actions of one (or 
more) animated actor(s). By making this interface a scriptable element, Improv enables authors to more 
easily choreograph the interaction between the virtual actors and the human participant. Controlling 
An Actor From Multiple Levels of Abstraction One important feature of Improv is ability for the user 
to interaction with the system at different semantic levels. The result of the user s actions can cause 
changes in the system anywhere from high level scripts to low level actions. This means that the author 
can give the user the right kind of control for every situation. If the user requires a very fine control 
over actors motor skills, then the author can provide direct access to the action level. On the other 
hand, if the user is involved in a conversation, the author might let the user specify a set of gestures 
for the actor to use, and have the actor decide on the specific gestures from moment to moment. At an 
even higher level, the author may want to have the user directing large groups of actors, such as an 
acting company or an army, in which case (s)he might have the user give the entire group directions and 
leave it to the individual actors to carry out those instructions. Since any level of the actor s behavior 
can be made accessible to the user, the author is free to vary the level of control, as necessary, at 
any point in the application. figure 5: Users interact with both the Behavior Engine and the Animation 
Engine through an author-defined user-interface. IMPLEMENTATION English-Style Scripting Language Many 
of the authors and artists interested in creating interactive content are not primarily programmers, 
and therefore we have developed a number of "english-style" scripting language extensions to Improv that 
make it easier for authors and artists to begin scripting interactive scenarios. For example, all of 
the code examples shown in this paper were written in the current Improv syntax. Because the scripting 
language is written as an extension of the system language, as users become more experienced they can 
easily migrate from scripting entirely using the high-level english-style syntax, to extending the system 
through low-level algorithmic control. Network Distribution Improv is implemented as a set of distributed 
programs in UNIX, connected by TCP/IP socket connections, multicast protocols and UNIX pipes. The participating 
processes can be running on any UNIX machines. This transport layer is hidden from the author. All communication 
between participant processes is done by continually sending and receiving programs around the network. 
These are immediately parsed into byte code and executed. At the top of the communication structure are 
routing processes. There must be at least one routing process on every participating Local Area Network. 
The router relays information among actors and renderer processes. For Wide Area Network communication, 
the router opens sockets to routers at other LAN s. In our current implementation, each actor maintains 
a complete copy of the blackboard information for all actors. If an actor s behavior state changes between 
the beginning and end of a time step, then these changed are routed to all other actors. Virtual Simultaneity 
Typical Wide Area Network (WAN) latencies can be several seconds. This poses a problem for two virtual 
actors interacting in a distributed system. From the viewpoint of believability, some latency is acceptable 
for high level decisions but not for low level physical actions. For example, when one character waves 
at another, the second character can get away with pausing for a moment before responding. But two characters 
who are shaking hands cannot allow their respective hands to move through space independently of each 
other. The hands must be synchronized to at least the animation frame rate. The blackboard model allows 
us to deal with this situation gracefully. We can split the Behavior Engine and Animation Engine for 
an actor across a Wide Area Network, and have these communicate with each other through the blackboard. 
For the DOFs produced by the Animation Engine, we allow the blackboard to contain different values at 
each LAN. For the states produced by the Behavioral Engine, the actor maintains a single global blackboard. 
Computationally, each actor runs the Behavioral Engine at only a single Local Area Network (LAN), but 
duplicates Animation Engine calculations at each LAN. When two characters must physically coordinate 
with each other, then they use the local versions of their DOFs. In this way, an actor is always in a 
single Behavioral State everywhere on the WAN, even though at each LAN he might appear to be in a slightly 
different position. In a sense, the actor has one mind, but multiple bodies, each inhabiting a parallel 
universe. Although these bodies may differ slightly in their position within their own universe, they 
are all consistent with this one mind. figure 6: Wide Area Network Distribution Model This leads to 
an interesting and fundamental property. Let us suppose that our Improv actor Gregor is dancing while 
balancing a tray in an Improv scene. Further, suppose that the scene is being watched at the same time 
by people in Sao Paulo, Brazil, and in Manhattan, New York. Perhaps some of these people are interacting 
with Gregor. The connection is through the Internet. In this scene, Gregor s Behavior Engine makes all 
the choices about whether to dance, whether to keep balancing the tray, how much joy and abandon versus 
self-conscious restraint he puts into the dance. His Animation Engine must set all the DOFs that determine 
how he moves when doing these things, so as to be responsive and coordinated. If the people in NY and 
those in SP are talking on the telephone, they will report seeing the same thing. Yet, if a high speed 
dedicated video link were established, and participants could see the two Gregors side by side, they 
would see two somewhat different animations. In one, Gregor s hand might thrust up to balance the tray 
half a second sooner, in the other he might have his other arm extended a bit further out. He might be 
rocking right to left on one screen, while he is rocking from left to right on the other. Thus, everywhere 
in the world there is only one social Gregor. He has a single mood, a single personality, he is only 
engaged in one task. Yet Gregor can have many slightly different physical realities, differing only up 
to the threshold where they might disrupt the social unity of his Behavioral State. figure 7: Two versions 
of Gregor dancing, each on different networked computer. In fact, if communication lag exceeds several 
seconds, significant differences may have occurred between the various Gregor instances. This can lead 
to problems. For example, suppose two actors that are temporarily out of communication each try to pick 
up some physical object. This is a standard collaborative work dilemma. The only reliable solution is 
to make the object itself an actor (albeit a light weight one). As odd as it seems, the object itself 
must agree to be picked up, since it too must maintain a consistent physical reality. This was also independently 
observed by [Karaul95]. Communicating with Improv Actors From Outside The System The blackboard protocol 
has a great advantage in terms of flexibility. To take full advantage of this flexibility, we provide 
a C support library that gives access to the blackboard. This allows researchers who know nothing about 
the Improv system, except for the names of actions and scripts, to begin immediately to control Improv 
actors. For example, a researcher can write a standalone C program that links with the support library. 
The program can pass string arguments such as "Gregor Sit" or "Otto Walk_To_Door" to an output function. 
This is all that the program needs to do, in order to modify actors behavior states. Since the system 
treats the standalone program as just another actor, the program can also listen for messages by calling 
an input routine. These messages contain the information that updates the blackboard, saying where various 
actors are, what they are doing, what their moods are, etc. In practice, this allows researchers and 
students at other institutions who know nothing about Improv except its GUI to immediately begin to use 
the system for their own applications. In our research collaborations we find that this is a highly successful 
way for our collaborators to bootstrap. Improv also has several audio subsystems. These subsystems are 
used for speech generation, music generation, allowing actors to follow musical cues, and generating 
ambient background noise. Extended Example The following is an example of a scene involving multiple 
actors involved in a social interaction with a user. define SCRIPT "Tell Joke" { { do "Turn to Face" 
to choose from { others except player } } { cue { others except player } to "Listen To Joke" to me } 
{ do "No Soap, Radio" do "Joke Gestures" } { wait until { current "Joke" is "completed" } } { do "Laugh" 
for 3 seconds } { cue { others except player } to "React To Joke" } { wait 3 seconds } { do "React To 
Player" } } In this example, the actor executing the script randomly chooses one of the actors not being 
controlled by the user, and turns to him or her. The actor then cues the other non-user actors to execute 
the "Listen To Joke" script, in which the actor chooses the appropriate gestures and body language that 
will give the appearance of listening attentively. define SCRIPT "Listen To Joke" { { choose from { entire 
set of "Stances" } based on "appropriate listening gestures" choose from { entire set of "Gestures" } 
based on "appropriate listening gestures" } { continue for between 3 and 12 seconds } { repeat } } Here, 
the actor chooses from the actions in the of "Stances" and "Gestures" using the decision rule "appropriate 
listening gestures" define DECISION_RULE "appropriate listening gestures" { subset "Listening?" scale 
1 factor { my "confidence" is greater than its "confidence" to within 0.3 } influence .5 factor { my 
"self control" is less than its "self control" to within 0.3 } influence .5 } define SUBSET "Listening?" 
{ it is "reactive" and "conversational" or "generic" } In this rule, the actor narrows the list down 
to those actions that are reactive and conversational, or generic actions that can be used in any context. 
The rule than compares the "confidence" and "self control" of the actor those assigned to each action, 
creating a weighted list favoring actions that match the fuzzy criteria. After choosing from the list 
the actor will wait from 3 to 12 seconds before repeating the script and choosing another gesture. Meanwhile, 
The actor telling the joke then executes the "No Soap, Radio" script which contains a command to an external 
speech system to generate the text of the joke. At the same time, the actor executes the "Joke Gestures" 
script which, like the "Listen To Joke" script chooses appropriate gestures based on the actor s personality. 
The actor continues until the joke is finished (the speech system sends a command to set the script s 
"completed" property to true) and then laughs, cuing the other actors to execute the "React To Joke" 
script. define SCRIPT "React To Joke" { { choose from { "Laugh" "Giggle" "Ignore" "Get Upset" } based 
on "feelings toward player" } } define DECISION_RULE "feelings toward player" { factor { my "sympathy 
toward" player does not equal its "mood" to within .4 } } Simply put, the more sympathy actor have for 
the player, the less likely they are to react positively to the joke. Finally, the actor executes the 
"React To Player" script in which the actor chooses an appropriate reaction to the player, depending 
on whether or not the player tells his actor to laugh. if he does, the joke teller laughs, maliciously 
if her sympathy for the player is low, playfully if her sympathy for the player is high. If the player 
s actor doesn t laugh the joke teller executes the "Get It?" script, taunting the player until he gets 
mad and/or leaves.  figure 8: Izzy tells Otto (the user) and Elli a joke. Elli is amused, Otto isn t. 
EXPERIENCE SIGGRAPH 95 At SIGGRAPH 95 we demonstrated an interactive embodied actor named Sam who responded 
to spoken statements and requests. Voice recognition was provided by DialecTech, a company that has developed 
an interface for an IBM continuous speech recognition program. In our demonstration, untrained participants 
could conduct a game of "Simon Says". Sam would follow requests only if they were preceded by the words 
"Simon Says". To make it more interesting we programmed Sam so that sometimes he would also follow requests 
not preceded by "Simon Says", but then he would act embarrassed at having been fooled. Our experience 
was that the sense of psychological involvement by participants was very great and compelling. Participants 
appeared to completely "buy into" Sam s presence. We believe that this was due to several factors: (i) 
participants could talk with Sam directly, (ii) participants knew Sam was not being puppeteered (the 
participant was the only human in the interaction loop), and  (iii) Sam s motions were relatively lifelike 
and never repeated themselves precisely. We have also found that allowing the participant to appear as 
an embodied avatar enhances the participant s sense of fun and play, and therefore involvement. We had 
positive experience of this at SIGGRAPH 95. We presented the participant with a large rear projection 
of a room full of embodied conversational agents. The participant s position, as well as simple arm gestures, 
were tracked by an overhead video camera. The participant appeared in the scene as a flying bat. As the 
participant walked around, the bat flew around accordingly. The nearest agent would break out of conversing 
with the other agents, and begin to play with the bat. When the participant flapped his/her arms, the 
bat would fly higher in the scene, and the camera would follow, which gave the participant a sense of 
soaring high in the air. We found that participants, and children in particular, enjoyed this experience 
very much, and would spend long periods of time "being" the bat and flying in turn around the heads of 
each of the embodied agents.  figure 9: Participant interacting with Improv actors as a bat. From SIGGRAPH 
95 Interactive Entertainment Exhibit. Other Users We have also provided a copy of Improv to a number 
of researchers at other Universities. These researchers are pursuing their own research on top of our 
actor embodiment substrate. In at least one case, they plan to do comparisons with their own existing 
agent embodiment system. Feedback from these collaborators on the use of Improv indicates that it is 
a useful tool for the embodiment of intelligent actors, especially for study of social interaction. In 
particular, it was suggested as a good tool for building educational VR environments, when used in conjunction 
with research software for virtual Interactive Theater. The combination can be used to simulate behaviors 
that would be likely to engage children to respond to, identify with and learn from knowledge agents. 
We have added extensions to Improv so that animators can use commercial tools, such as Alias and SoftImage, 
to create small atomic animation components. Trained animators can use these tools to build up content. 
Such content can include various walk cycles, sitting postures, head scratching, etc. The procedural 
animation subsystem is designed in such a way that such action styles can be blended. For example, two 
or three different styles of walks can be separately designed from commercial key frame animation packages, 
and then blended together, or else blended with various procedural walks, to create continuously variable 
walk styles that reflect the actor s current mood and attitude, as well as the animator s style. FUTURE 
DIRECTIONS It is well known in traditional animation that human motions are created from combinations 
of temporarily overlapping gestures and stances. One of our current goals is to use Improv s ability 
to tie into commercial animation tools to build up a library of component motions, and to classify these 
motions in a way that makes them most useful as building blocks. We have begun to embed Improv into a 
client-based application for a Java compatible browser (such as Netscape version 2.0). For use in educational 
and home settings, we plan to augment the full 3D subsystem with a "nearly 3D" version. This would run 
on a low end platform, such as a PC with an Intel processor. The user would still be able to see a view 
into a three dimensional world, but the visual representations of the actors would be simpler and largely 
two dimensional. For example, two participants to a graphical MUD, one with an SGI Onyx, and one with 
an Intel/486 based PC, could interact in the same scene. They would both see the same actors at the same 
locations, actions and personality. The only difference would be that the first participant would see 
a much more realistic quality of rendering. We plan to integrate Improv s voice recognition and english-like 
behavioral sub-systems. This will allow a user to fully exploit the object substrate, giving access to 
direction of goals, mood changes, attitudes and relationships between actors, all via spoken English 
sentences. CONCLUSION We have described an interactive system that lets authors of various abilities 
create remarkably lifelike, responsively animated character interactions that run over networks in real 
time. We believe these techniques have the potential to have a large impact on many areas. These include: 
computer Role Playing Games, simulated conferences, "clip animation," graphical front ends for MUDs, 
synthetic performance, shared virtual worlds, interactive fiction, high-level direction for animation, 
digital puppetry, computer guides and companions, point to point communication interfaces, true non-linear 
narrative TV, and large scale deployment of bots for the Metaverse. As Improv is a very large system, 
we could not cover many of its details in this paper. We refer the reader to [Perlin96] for a more in-depth 
treatment. ACKNOWLEDGEMENTS We gratefully acknowledge the support of Microsoft Corporation (and especially 
Dan Ling), the National Science Foundation, the New York State Science and Technology Foundation, and 
Silicon Graphics Incorporated (especially Keith Seto). Daniel Wey and Jon Meyer have both made important 
contributions to the Improv substrate. Mauricio Oka designed the flexible face model. Many other people 
have helped with this effort in many ways. In particular, we d like to thank Cynthia Allen, Clilly Castiglia, 
Troy Downing, Steve Feiner, Laura Haralyi, Mehmet Karaul, Sabrina Liao, Marcelo Tocci More, Ruggero Ruschioni, 
Eduardo Toledo Santos, Jack Schwartz, Gerry Seidman, Eric Singer, Michael Wahrman, and Marcelo Zuffo. 
Also everyone at the, CAT and MRL at NYU, and LSI at USP. E Emi, com beijos. APPENDICES A. Decision Rules 
Equation When an object is passed through a decision rule, a weighted sum is made of each of the values 
returned from the associated factors, modified by the scale assigned to the set of choices. This becomes 
the final weight assigned to the object that is used in making the decision. The formula for this is 
as follows: FinalWeight=Scale(factor1influence1factor2influence2...factor ninfluencen) B. Fuzzy Logic 
Equations The function compares how close the Input Value comes to the Target Value (or Target Range); 
returning a value of 1 at the Target Value (or inside the Target Range), dropping to0ata distance of 
Spread from the TargetValue. The fuzzy comparison is implemented as follows: where: y is the Fuzzy Value 
· w is a bell curve weighting kernel (we use a raised cos function) A high and low spread may be specified, 
in which case input values greater than the target value (or range) will use the high spread in the calculation, 
while input values lower than the target value (or range) will apply the low spread. The returned value 
is then modified based on the type of fuzzy operation as follows: equals y Value not equals 1-y, its 
complement greater than y, high spread defaults to infinity not greater than 1-y, high spread defaults 
to infinity less than y, low spread defaults to -infinity not less than 1-y, low spread defaults to -infinity 
 REFERENCES N. Badler, B. Barsky, D. Zeltzer, Making Them Move: Mechanics, Control, and Animation of 
Articulated Figures Morgan Kaufmann Publishers, San Mateo, CA, 1991. N. Badler, C. Phillips, B. Webber, 
Simulating Humans: Computer Graphics, Animation, and Control Oxford University Press, 1993. J. Bates, 
A. Loyall, W. Reilly, Integrating Reactivity, Goals and Emotions in a Broad Agent, Proceedings of the 
14th Annual Conference of the Cognitive Science Society, Indiana, July 1992. B. Blumberg, T. Galyean, 
Multi-Level Direction of Autonomous Creatures for Real-Time Virtual Environments Computer Graphics (SIGGRAPH 
95 Proceedings), 30(3):47--54, 1995. A. Bruderlin, L. Williams, Motion Signal Processing, Computer Graphics 
(SIGGRAPH 95 Proceedings), 30(3):97--104, 1995. R. Brooks. A Robust Layered Control for a Mobile Robot, 
IEEE Journal of Robotics and Automation, 2(1):14--23, 1986. J. Chadwick, D. Haumann, R. Parent, Layered 
construction for deformable animated characters. Computer Graphics (SIGGRAPH 89 Proceedings), 23(3):243--252, 
1989. D. Ebert and et. al., Texturing and Modeling, A Procedural Approach Academic Press, London, 1994. 
M. Girard, A. Maciejewski, Computational modeling for the computer animation of legged figures. Computer 
Graphics (SIGGRAPH 85 Proceedings), 20(3):263--270, 1985. J. Hodgins, W. Wooten, D. Brogan, J O Brien, 
Animating Human Athletics, Computer Graphics (SIGGRAPH 95 Proceedings), 30(3):71--78, 1995. M. Johnson, 
WavesWorld: PhD Thesis, A Testbed for Three Dimensional Semi-Autonomous Animated Characters, MIT, 1994. 
M. Karaul, personal communication P. Maes, T. Darrell and B. Blumberg, The Alive System: Full Body Interaction 
with Autonomous Agents in Computer Animation 95 Conference, Switzerland, April 1995 .IEEE Press, pages 
11-18. M. Minsky, Society of Mind, MIT press, 1986. C. Morawetz, T. Calvert, Goal-directed human animation 
of multiple movements. Proc. Graphics Interface}, pages 60--67, 1990. K. Perlin, An image synthesizer. 
Computer Graphics (SIGGRAPH 85 Proceedings)}, 19(3):287--293, 1985. K. Perlin, Danse interactif. SIGGRAPH 
94 Electronic Theatre, Orlando. K. Perlin, Real Time Responsive Animation with Personality, IEEE Transactions 
on Visualization and Computer Graphics, 1(1), 1995. K. Perlin, A. Goldberg, The Improv System Technical 
Report NYU Department of Computer Science, 1996. (online at http://www.mrl.nyu.edu/improv) K. Sims, 
Evolving virtual creatures. Computer Graphics (SIGGRAPH 94 Proceedings)}, 28(3):15--22, 1994. N. Stephenson, 
Snow Crash Bantam Doubleday, New York, 1992. S. Strassman, Desktop Theater: Automatic Generation of Expresssive 
Animation, PhD thesis, MIT Media Lab, June 1991 (online at http://www.method.com/straz/straz-phd.pdf) 
D. Terzopoulos, X. Tu, and R. Grzesczuk Artificial Fishes: Autonomous Locomotion, Perception, Behavior, 
and Learning in a Simulated Physical World, Artificial Life, 1(4):327-351, 1994. A. Witkin, Z. Popovic, 
Motion Warping Computer Graphics (SIGGRAPH 95 Proceedings), 30(3):105-108, 1995.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237259</article_id>
		<sort_key>217</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[The virtual cinematographer]]></title>
		<subtitle><![CDATA[a paradigm for automatic real-time camera control and directing]]></subtitle>
		<page_from>217</page_from>
		<page_to>224</page_to>
		<doi_number>10.1145/237170.237259</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237259</url>
		<keywords>
			<kw><![CDATA[camera placement]]></kw>
			<kw><![CDATA[cinematography]]></kw>
			<kw><![CDATA[hierarchical finite state machines]]></kw>
			<kw><![CDATA[screen acting]]></kw>
			<kw><![CDATA[virtual environments]]></kw>
			<kw><![CDATA[virtual worlds]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.1.1</cat_node>
				<descriptor>Automata (e.g., finite, push-down, resource-bounded)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003766</concept_id>
				<concept_desc>CCS->Theory of computation->Formal languages and automata theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14119987</person_id>
				<author_profile_id><![CDATA[81451600003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Li-wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[He]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, One Microsoft Way, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15024219</person_id>
				<author_profile_id><![CDATA[81406592138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, One Microsoft Way, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Daniel Arijon. Grammar of the Film Language. Communication Arts Books, Hastings House, Publishers, New York, 1976.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617421</ref_obj_id>
				<ref_obj_pid>616000</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[James Blinn. Where am I? What am I looking at? IEEE Computer Graphics and Applications, pages 76-81, 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1892897</ref_obj_id>
				<ref_obj_pid>1892875</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[David B. Christianson, Sean E. Anderson, Li-wei He, David H. Salesin, Daniel S. Weld, and Michael F. Cohen. Declarative camera control for automatic cinematography. In Proceedings of the AAAI-96, August 1996.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147166</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Steven M. Drucker, Tinsley A. Galyean, and David Zeltzer. CINEMA: A system for procedural camera movements. In David Zeltzer, editor, Computer Graphics (1992 Symposium on Interactive 3D Graphics), volume 25, pages 67-70, March 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199428</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Steven M. Drucker and David Zelter. CamDroid: A system for implementing intelligent camera control. In Michael Zyda, editor, Computer Graphics (1995 Symposium on Interactive 3D Graphics), volume 28, pages 139-144, April 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Steven M. Drucker and David Zeltzer. Intelligent camera control in a virtual environment. In Proceedings of Graphics Interface '94, pages 190-199, Banff, Alberta, Canada, May 1994. Canadian Information Processing Society.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer Graphics, Principles and Practice. Addison- Wesley Publishing Company, Reading, Massachusetts, second edition, 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134088</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher and Andrew Witkin. Through-the-lens camera control. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 331-340, July 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>34886</ref_obj_id>
				<ref_obj_pid>34884</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[David Harel. Statecharts: A visual formalism for complex systems. Science of Computer Programming, pages 231-274, 1987.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6251</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Donald Hearn and M. Pauline Baker. Computer Graphics. Prentice Hall, Englewood Cliffs, New Jersey, second edition, 1994.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93272</ref_obj_id>
				<ref_obj_pid>93267</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Peter Karp and Steven Feiner. Issues in the automated generation of animated presentations. In Proceedings of Graphics Interface '90, pages 39-48, May 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Peter Karp and Steven Feiner. Automated presentation planning of animation using task decomposition with heuristic reasoning. In Proceedings of Graphics Interface '93, pages 118-127, Toronto, Ontario, Canada, May 1993. Canadian Information Processing Society.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Christopher Lukas. Directing for Film and Television. Anchor Press/Doubleday, Garden City, N.Y., 1985.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97898</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Jock D. Mackinlay, Stuart K. Card, and George G. Robertson. Rapid controlled movement through a virtual 3D workspace. In Forest Baskett, editor, Computer Graphics (SIGGRAPH '90 Proceedings), volume 24, pages 171-176, August 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Joseph V. Mascelli. The Five C's of Cinematography. Cine/Grafic Publications, Hollywood, 1965.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147167</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Cary B. Phillips, Norman I. Badler, and John Granieri. Automatic viewing control for 3D direct manipulation. In David Zeltzer, editor, Computer Graphics (1992 Symposium on Interactive 3D Graphics), volume 25, pages 71-74, March 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Warren Sack and Marc Davis. IDIC: Assembling video sequences from story plans and content annotations. In IEEE International Conference on Multimedia Computing and Systems, Boston, MA, May 1994.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Patrick Tucker. Secrets of Screen Acting. Routledge, New York, 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Virtual Cinematographer: A Paradigm for Automatic Real-Time Camera Control and Directing Li-wei 
He * Michael F. Cohen * David H. Salesin y Microsoft Research Microsoft Research University of Washington 
Abstract This paper presents a paradigm for automatically generating com­plete camera speci.cations for 
capturing events in virtual 3D en­vironments in real-time. We describe a fully implemented system, called 
the Virtual Cinematographer, and demonstrate its application in a virtual party setting. Cinematographic 
expertise, in the form of .lm idioms, is encoded as a set of small hierarchically organized .nite state 
machines. Each idiom is responsible for capturing a par­ticular type of scene, such as three virtual 
actors conversing or one actor moving across the environment. The idiom selects shot types and the timing 
of transitions between shots to best communicate events as they unfold. A set of camera modules, shared 
by the id­ioms, is responsible for the low-level geometric placement of spe­ci.c cameras for each shot. 
The camera modules are also respon­sible for making subtle changes in the virtual actors positions to 
best frame each shot. In this paper, we discuss some basic heuristics of .lmmaking and show how these 
ideas are encoded in the Virtual Cinematographer. CR Categories and Subject Descriptors: I.3.3 [Computer 
Graph­ics]: Picture/Image Generation viewing algorithms; I.3.6 [Com­puter Graphics]: Methodology and 
Techniques interaction tech­niques. Additional Keywords: cinematography, virtual worlds, virtual en­vironments, 
screen acting, camera placement, hierarchical .nite state machines  Introduction With the explosive 
growth of the internet, computers are increas­ingly being used for communication and for play between 
multiple participants. In particular, applications in which participants con­trol virtual actors that 
interact in a simulated 3D world are becom­ing popular. This new form of communication, while holding 
much promise, also presents a number of dif.culties. For example, partic­ipants often have problems comprehending 
and navigating the vir­tual 3D environment, locating other virtual actors with whom they wish to communicate, 
and arranging their actors in such a way that they can all see each other. *Microsoft Research, One Microsoft 
Way, Seattle, WA 98052. Email: fa-liweih mcoheng@microsoft.com yDepartment of Computer Science and Engineering, 
University of Wash­ington, Seattle, WA 98195. Email: salesin@cs.washington.edu Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
In fact, these same types of problems have been faced by cinematog­raphers for over a century. Over the 
years, .lmmakers have devel­oped a set of rules and conventions that allow actions to be commu­nicated 
comprehensibly and effectively. These visual conventions are now so pervasive that they are essentially 
taken for granted by audiences. This paper addresses some of the problems of communicating in 3D virtual 
environments by applying rules of cinematography. These rules are codi.ed as a hierarchical .nite state 
machine, which is ex­ecuted in real-time as the action unfolds. The .nite state machine controls camera 
placements and shot transitions automatically. It also exerts subtle in.uences on the positions and actions 
of the vir­tual actors, in much the same way that a director might stage real actors to compose a better 
shot. Automatic cinematography faces two dif.culties not found in real­world .lmmaking. First, while 
informal descriptions of the rules of cinematography are mentioned in a variety of texts [1, 13, 15], 
we have not found a description that is explicit enough to be directly encoded as a formal language. 
Second, most .lmmakers work from a script that is agreed upon in advance, and thus they have the op­portunity 
to edit the raw footage as a post-process. In constrast, we must perform the automatic camera control 
in real time. Thus, live coverage of sporting events is perhaps a better analogy to the prob­lem we address 
here, in that in neither situation is there any explicit knowledge of the future, nor is there much opportunity 
for later edit­ing. In this paper, we discuss an implementation of a real-time camera controller for 
automatic cinematography, called the Virtual Cine­matographer (VC). We demonstrate its operation in the 
context of a virtual party, in which actors can walk, look around, converse, get a drink, and so on. 
The various actors are controlled either by human users over a network, or by a party simulator, which 
can control certain actors automatically. Each user runs his or her own VC, which conveys the events 
at the party from the point of view of that user s actor, or protagonist. The Virtual Cinematographer 
paradigm is applicable to a number of different domains. In particular, a VC could be used in any applica­tion 
in which it is possible to approximately predict the future ac­tions of virtual actors. For example, 
in virtual reality games and interactive .ction, the VC could be used to improve upon the .xed point-of-view 
shots or ceiling-mounted cameras that such applica­tions typically employ. 1.1 Related work There are 
a number of areas in which related work has been ex­plored. Karp and Feiner [11, 12] describe an animation-planning 
system that can customize computer-generated presentations for a particular viewer or situation. Sack 
and Davis [17] present the IDIC system, which assembles short trailers from a library ofStar Trek, The 
Next Generation footage. Christianson et al. [3] have developed an interactive story-telling system that 
plans a camera sequence based on a simulated 3D animation script. All of these techniques A line of interest 
B external internal external apex Figure 1 (Adapted from .gure 4.11 of [1].) Camera placement is speci.ed 
relative to the line of interest. use an off-line planning approach to choose the sequence of camera 
positions. In this paper, by contrast, we are concerned with real-time camera placement as the interactively-controlled 
action proceeds. A number of other systems concentrate on .nding the best camera placement when interactive 
tasks are performed [8, 14, 16]. In par­ticular, Drucker et al. [4, 5, 6] show how to set up the optimal 
cam­era positions for individual shots by solving small constrained opti­mization problems. For ef.ciency 
reasons, in the real-time setting we select shots from a small set of possible camera speci.cations so 
that camera positions can be computed using closed-form methods. The mathematics for de.ning low-level 
camera parameters, given the geometry of the scene and the desired actor placements, is de­scribed in 
a number of texts [7, 10]. We found Blinn s treatment [2] to be the most helpful single source.  2 
Principles of cinematography It is useful to consider a .lm as a hierarchy. At the highest level, a .lm 
is a sequence of scenes, each of which captures a speci.c sit­uation or action. Each scene, in turn, 
is composed of one or more shots. A single shot is the interval during which the movie camera is rolling 
continuously. Most shots generally last a few seconds, al­though in certain cases they can go on much 
longer. 2.1 Camera placement Directors specify camera placements relative to the line of interest, an 
imaginary vector either connecting two actors, directed along the line of an actor s motion, or oriented 
in the direction that an actor is facing. Common camera placements include external, internal,and apex 
views, as shown in Figure 1. Cinematographers have identi.ed that certain cutting heights make for pleasing 
compositions while others yield ugly results (e.g., an image of a man cut off at the ankles). There are 
approximately .ve useful camera distances [1]. An extreme closeup cuts at the neck; a closeup cuts under 
the chest or at the waist; a medium view cuts at the crotch or under the knees; a full view shows the 
entire person; and a long view provides a distant perspective. Individual shots also require subtly different 
placement of actors to look natural on the screen. For example, the closeup of two actors in Figure 2(a) 
looks perfectly natural. However, from a distance it is clear that the actors are closer together than 
expected. Similarly, shots with multiple actors often require shifting the actor positions to properly 
frame them (Figure 2(b)). 2.2 Cinematographic heuristics and constraints Filmmakers have articulated 
numerous heuristics for selecting good shots and have informally speci.ed constraints on successive shots 
for creating good scenes. We have incorporated many of these  (a) (b) Figure 2 (Adapted from Tucker 
[18, pp. 33, 157].) Actor positions that look natural for a particular closeup look too close together 
when viewed from further back (a). Correctly positioning three actors for a shot may require small changes 
in their positions (b). heuristics in the design of the Virtual Cinematographer. Some ex­amples are: 
.Don t cross the line: Once an initial shot is taken from the left or right side of the line of interest, 
subsequent shots should remain on that side. This rule ensures that successive shots of a moving actor 
maintain the direction of apparent motion. .Avoid jump cuts: Across the cut there should be a marked 
differ­ence in the size, view, or number of actors between the two se­tups. A cut failing to meet these 
conditions creates a jerky, sloppy effect. .Use establishing shots: Establish a scene before moving to 
close shots. If there is a new development in the scene, the situation must be re-established. .Let the 
actor lead: The actor should initiate all movement, with the camera following; conversely, the camera 
should come to rest a little before the actor. .Break movement: A scene illustrating motion should be 
broken into at least two shots. Typically, each shot is cut so that the actor appears to move across 
half the screen area. A more complete survey of these heuristics can be found in Chris­tianson et al. 
[3]. 2.3 Sequences of shots Perhaps the most signi.cant invention of cinematographers is a col­lection 
of stereotypical formulas for capturing speci.c scenes as se­quences of shots. Traditional .lm books, 
such as The Grammar of the Film Language by Arijon [1], provide an informal compila­tion of formulas, 
along with a discussion of the various situations in which the different formulas can be applied. As 
an example, Figure 3 presents a four-shot formula that will serve as an extended example throughout the 
remainder of this paper. The formula provides a method for depicting conversations among three actors. 
The .rst shot is an external shot over the shoulder of actor Ctoward actors Aand B. The second and third 
shots are external shots of actors Aand Balone. The fourth shot is an internal reaction shot of actor 
C. Arijon [1] stipulates that an editing order for a typ­ical sequence using this setup would be to alternate 
between shots 1 and 4 while actors Aand Btalk to actor C.When Aand Bbegin to talk to each other, the 
sequence shifts to an alternation between shots 2 and 3, with an occasional reaction shot 4. Shot 1 should 
be introduced every now and then to re-establish the whole group. Figure 3 (Adapted from Figure 6.29 
of Arijon [1].) A common for­mula for depicting a conversation among three actors. The particular formulas 
prefered by any individual director lend a certain .avor or style to that director s .lms. In the Virtual 
Cin­ematographer, the style is dictated by the particular formulas en­coded. (In fact, as each of the 
authors of this paper worked on the VC, a slightly different style emerged for each one.)  3 The Virtual 
Cinematographer The Virtual Cinematographer is one part of the overall architecture shown in Figure 4. 
The other two parts consist of the real-time ap­plication and the renderer. The real-time application 
supplies the renderer with any static geometry, material properties, and lights. At each time tick (i.e., 
at each frame of the resulting animation), the following events occur: 1. The real-time application sends 
the VC a description of events that occur in that tick and are signi.cant to the protagonist. Events 
are of the form (subject, verb, object). The subject is al­ways an actor, while the object may be an 
actor, a current con­versation (comprising a group of actors), a .xed object (e.g., the bar), or null. 
 2. The VC uses the current events plus the existing state of the an­imation (e.g., how long the current 
shot has lasted) to produce an appropriate camera speci.cation that is output to the renderer. The VC 
may query the application for additional information, such as the speci.c locations and bounding boxes 
of various ac­tors. The VC may also make subtle changes in the actors po­sitions and motion, called acting 
hints. These are also output to the renderer. 3. The scene is rendered using the animation parameters 
and de­scription of the current environment sent by the application, and the camera speci.cation and 
acting hints sent by the VC.  3.1 The VC architecture The cinematography expertise encoded in the Virtual 
Cinematogra­pher is captured in two main components: camera modules and id­ioms (see Figure 5). Camera 
modules implement the different cam­era placements described in Section 2.1. Idioms describe the formu­las 
used for combining shots into sequences, as described in Sec­tion 2.3. The idioms are organized hierarchically, 
from more gen­eral idioms near the top, to idioms designed to capture increasingly speci.c situations. 
This structure allows each idiom to simply re­ Real-time application events, geometric information queries 
Virtual Cinematographer camera , acting hints animation parameters, static geometry, actor models, lights, 
etc. Renderer  Figure 4 System including the Virtual Cinematographer. Figure 5 The Virtual Cinematographer 
structure. turn control back to a more general idiom when unforseen events are encountered. 3.2 Camera 
modules Each camera module takes as input a number of actors, called pri­mary actors; the exact number 
depends on the particular camera module. Each camera module automatically positions the camera so as 
to place the actors at particular locations on the screen and allow for pleasing cutting heights. In 
addition, the camera module may decide to reposition the actors slightly to improve the shot. Finally, 
the camera placement is automatically chosen so as to not cross the line of interest. 3.2.1 Example camera 
modules Sixteen different camera modules have been implemented, several of which are shown in Figure 
6. The most heavily used camera mod­ules include: .apex(actor1, actor2): The apexcamera module takes 
two ac­tors as input and places the camera so that the .rst actor is cen­tered on one side of the screen 
and the second actor is centered on the other. The camera distance is thus a function of the distance 
between the two actors. .closeapex(actor1, actor2): This camera module also imple­ments an apex camera 
placement. However, it differs from the previous camera module in that it always uses a close-up camera 
distance. To compose a more pleasing shot, this camera module may move the actors closer together, as 
discussed in Section 2.1 and illustrated by A0and B0in Figure 6. .external(actor1, actor2): The external 
camera module takes as input two actors and places the camera so that the .rst actor is seen over the 
shoulder of the second actor, with the .rst actor occupying two-thirds of the screen and the second actor 
the other third. .internal(actor1,[actor2]): The internalcamera module places the camera along the same 
line of sight as the external camera module, but closer in and with a narrower .eld of view, so that 
only the .rst actor is seen. If only one actor is speci.ed, line of interest external(B,A) other instance 
of external(B,A) BA internal(B,A) A B A B apex(B,A) closeapex(B,A) A B A B 1 A B track(A,B) pan(A,B) 
2 follow(A,B) Figure 6 Some camera modules. then the line of interest is taken to be along the direction 
the actor is facing. .ext1to2(actor1, actor2, actor3): This camera module imple­ments an external camera 
placement between one actor and two others. It places the camera so that the .rst two actors are seen 
over the shoulder of the third actor, with the .rst two actors oc­cupying two-thirds of the screen, and 
the third actor the rest of the screen (see camera 1 in Figure 3). This camera module may also sometimes 
perturb the actors positions to compose a better shot. .ftrack pan followg(actor1, actor2, mindist, 
maxdist): These three related camera modules are used when actor1 is mov­ing (Figure 6). They differ 
from the preceding modules in that they de.ne a moving camera that dynamically changes position and/or 
orientation to hold the actor s placement near the center of the screen. The trackcamera sets the camera 
along a perpen­dicular from the line of interest and then moves with the actor, maintaining the same 
orientation. The panmodule sets itself off the line of interest ahead of the actor and then pivots in 
place to follow the motion of the actor. The followmodule combines these two operations. It .rst behaves 
like a panning camera, but as the actor passes by it begins to follow the actor from behind rather than 
allowing the actor to move off into the distance. .fixed(cameraspec): This camera module is used to specify 
a particular .xed location, orientation, and .eld of view. We use it in our application to provide an 
overview shot of the scene. .null(): This camera module leaves the camera in its previous position. 
3.2.2 Respecting the line of interest Recall that the line of interest is de.ned relative to the two 
actors in a shot. Most of the camera modules can choose one of two in­stances, corresponding to symmetric 
positions on opposite sides of the line of interest (Figure 6). The rules of cinematography dictate that 
when the line of interest remains constant, the camera should re­main on the same side of the line. When 
the line of interest changes, for example, when one of the two actors in the shot changes position, the 
choice is not as well de.ned. We have found that a good rule is to choose the instance in which the camera 
orientation with respect to the new line of interest is closest to the orientation of the previous shot. 
 3.2.3 In.uencing the acting The camera modules are also able to subtly improve a shot by in­.uencing 
the positions of the actors in the scene. Since the real­time application is primarily in charge of manipulating 
the actors, the changes made by the VC must be subtle enough to not disturb the continuity between shots 
(Figure 10). For example, the closeapexcamera module moves the two pri­mary actors closer together if 
their distance is greater than some minimum, as in Figure 6. The ext1to2 camera module adjusts the positions 
of the three primary actors so that no actor is obscured by any other in the shot. Some camera modules 
remove actors al­together from the scene, to avoid situations in which an actor ap­pears only part-way 
on screen or occludes another primary actor in the scene. For example, the internal camera module removes 
the second actor from the scene. 3.2.4 Detecting occlusion Camera modules are also responsible for detecting 
when one or more of the primary actors becomes occluded in the scene. In the case of occlusion, at each 
time tick, the camera module increments an occlusion counter, or resets the counter to zero if the occluded 
actors become unoccluded. This counter can be used by the idioms to decide whether to change to a different 
shot.  3.3 Idioms At the core of the Virtual Cinematographer is the .lm idiom.A sin­gle idiom encodes 
the expertise to capture a particular type of situ­ation, such as a conversation between two actors, 
or the motion of a single actor from one point to another. The idiom is responsible for deciding which 
shot types are appropriate and under what condi­tions one shot should transition to another. The idiom 
also encodes when the situation has moved outside the idiom s domain of exper­tise for example, when 
a third actor joins a two-person conver­sation. In the VC, an idiom is implemented as a hierarchical 
.nite state ma­chine (FSM) [9]. Each state invokes a particular camera module. Thus, each state corresponds 
to a separate shot in the animation be­ing generated. Each state also includes a list of conditions, 
which, when satis.ed, cause it to exit along a particular arc to another state. Thus, a cut is implicitly 
generated whenever an arc in the FSM is tra­versed to a state that uses a different camera module. The 
FSM s are hierarchically arranged through call/return mechanisms. We will introduce the concepts involved 
in constructing idioms by way of examples. In the .rst example, we construct an idiom for depicting a 
conversation between two actors, called 2Talk.In the second example, we use this idiom as a primitive 
in building a more complex idiom, called 3Talk, for depicting a conversation among three actors. 3.3.1 
The 2Talk idiom The 2Talkidiom (Figure 7) encodes a simple method for .lming two actors as they talk 
and react to each other. It uses only external shots of the two actors. The 2Talkprocedure takes as parameters 
the two actors Aand Bwho are conversing. It has four states. The .rst state uses an externalcamera module, 
which shows Atalk­ing to B. The second state is used for the opposite situation, when Btalks to A. The 
third and fourth states use external camera place­ments to capture reaction shots of each of the actors. 
When the idiom is activated, it follows one of two initial arcs that originate at the small circle in 
the diagram of Figure 7, called the Figure 7 The 2Talkidiom. entry point. The arc to be used is determined 
by the following code: DEFINE_IDIOM_IN_ACTION(2Talk) WHEN ( talking(A, B) ) DO ( GOTO (1); ) WHEN 
( talking(B, A) ) DO ( GOTO (2); ) END_IDIOM_IN_ACTION This code, like the rest of the code in this 
paper, is actual C++ code, rather than pseudocode. The keywords written in all-caps are macros that are 
expanded by the C preprocessor. This code tests whether Ais talking to B,or Bis talking to A, and transitions 
im­mediately to the appropriate state, in this case, either 1or 2, respec­tively. As a state is entered, 
it .rst executes a set of in-actions.The in­actions are often null, as is the case for all of the states 
in the 2Talk idiom. Once the state is entered, the state s camera module is called to position the camera. 
The state then executes a sequence of actions at every clock tick. The actions can be used to affect 
conditional transitions to other states. Finally, when exiting, the state executes aset of out-actions, 
again, often null. In the 2Talkidiom, the camera modules are de.ned as follows: DEFINE_SETUP_CAMERA_MODULES(2Talk) 
 MAKE_MODULE(1, external, (A, B)) MAKE_MODULE(2, external, (B, A)) LINK_MODULE(1, 1, "A talks") LINK_MODULE(2, 
2, "B talks") LINK_MODULE(3, 1, "A reacts") LINK_MODULE(4, 2, "B reacts") END_SETUP_CAMERA_MODULES 
 MAKE MODULE(module id, type, parameter list) creates a new cam­era module of the designated type with 
the speci.ed parameters and gives it an identifying number. LINK MODULE(state id, module id, name) associates 
the speci.ed camera module with the speci.ed state. Thus for example, whenever state 1 is entered, an 
external shot of actor A over the shoulder of actor B will be used. The .rst action code to be executed 
in each state is speci.ed in a block common to all states, called the common actions. Thisispri­marily 
a shorthand mechanism to avoid having to respecify the same (condition, arc) pairs in each state of the 
idiom. The common ac­tions in the 2Talkidiom are: DEFINE_STATE_ACTIONS(COMMON) WHEN (T < 10) DO( STAY;) 
WHEN (!talking(A, B) &#38;&#38; !talking(B, A)) DO ( RETURN; ) END_STATE_ACTIONS The .rst statement 
checks to see whether the total time Tspent so far in this state is less than 10 ticks; if so, the current 
state remains un­changed. (However, an exception mechanism takes precedence and may in fact pre-empt 
the shot, as discussed in Section 3.3.2.) If the shot has lasted at least 10 ticks, but A and B are no 
longer conversing, then the idiom should return to the idiom that called it. The action statements are 
evaluated sequentially. Thus, earlier statements take precedence over statements listed later in the 
code. The variable Tis a global variable, which is accessible to any state. There are several other global 
variables that can be used in state ac­tions: .Occluded, the number of consecutive ticks that one or 
more of the primary actors has been occluded; .IdiomT, the total number of ticks spent so far in this 
idiom; .D[A,B], the distance between the actors (measured in units of head diameters ); .forwardedge[x], 
rearedge[x], centerline[x],the edges of the bounding box of actor x, relative to the screen coor­dinates. 
There are also a number of prede.ned control structures: .STAY, remain in the same state for another 
tick; .GOTO(x), transition to state x; .RETURN, return to the parent state; .CALL(idiom, parameter list), 
execute the speci.ed idiom by passing it the speci.ed list of parameters. Finally, the actions code above 
makes use of a domain-speci.c sub­routine called talking(A,B), which returns true if and only if the 
current list of events includes (A,talk,B). State 1of the 2Talkidiom is used to depict actor Atalking 
to B.In addition to the common actions, the list of actions executed at each clocktickwheninstate 1are: 
DEFINE_STATE_ACTIONS(1) WHEN ( talking(B, A) ) DO ( GOTO (2); ) WHEN( T> 30) DO ( GOTO (4); ) END_STATE_ACTIONS 
 If B is now talking to A, then a transition to state 2 is required to capture this situation. Otherwise, 
if an actor has been in the same shot for more than 30 ticks, there should be a transition to state 4to 
get a reaction shot from the other actor. State 2, which addresses the case of actor B talking to actor 
A,is completely symmetric: the code is exactly the same except that A and B are swapped and states 1and 
3 are used in place of states 2 and 4. For completeness, the action code for state 3is shown below: DEFINE_STATE_ACTIONS(3) 
WHEN ( talking(A, B) ) DO ( GOTO (1); ) WHEN ( talking(B, A) T > 15 ) DO ( GOTO (2); ) END_STATE_ACTIONS 
 Note that this state can make a transition back to state 1, which uses the same camera module as is 
used here in state 3. In this case, the two shots are really merged into a single shot without any cut. 
Fi­nally, state 4is symmetric to state 3in the same way that state 2is symmetric to state 1. Figure 
8 The 3Talkidiom. Since the out-actions for 2Talkare null, we have now completely described the 2Talk 
idiom. The next section shows how 2Talk can be used as a subroutine for a higher-level idiom that handles 
con­versations among three actors. 3.3.2 The 3Talk idiom The .nite state machine for 3Talk, which handles 
conversations among three actors, is shown in Figure 8. This idiom implements the cinematic treatment 
of three actors sketched in Figure 3 and il­lustrated in Figure 10. The 3TalkFSM has the same types of 
com­ponents as 2Talk: it has states and arcs representing transitions be­tween states. In addition, this 
FSM also uses the exception mecha­nism, as discussed below. The idiom has four states. The .rst state, 
labeled 1, is an establish­ing shot of all three actors, corresponding to the .rst camera posi­tion in 
Figure 8 and the second shot in Figure 10. The second state, labeled 2and3, is a parent state that calls 
the 2Talkidiom, corre­sponding to cameras 2 and 3 in Figure 3. Finally, the last two states, labeled 
4aand 4b, capture the reaction shot of the .rst actor; these two states correspond to camera 4 of Figure 
3. All four states have actions that are similar to the ones described in 2Talk. The two states 4a and 
4b have been implemented as separate states because they function differently in the idiom, even though 
they both shoot the scene from the same camera. State 4a is used in the opening sequence or after a new 
establishing shot, al­lowing shots of all three actors to be alternated with reaction shots of actor 
C. By contrast, state 4bis used only after a two-way con­versation between actors A and B has been established, 
in this case to get an occasional reaction shot of actor C and then quickly return to the two-way conversation 
between A and B. The one state that differs signi.cantly from the states considered earlier is the state 
labeled 2and3. First, unlike the previous states, state 2and3does have in-actions: DEFINE_STATE_IN_ACTION(2and3) 
 REG_EXCEPTION(left_conv, C, LEFT_CONV); REG_EXCEPTION(too_long, 100, TOO_LONG); REG_EXCEPTION(reacts, 
C, GET_REACTION); CALL( 2Talk, (A, B) ); END_STATE_IN_ACTION These in-actions set up a number of exceptions, 
which, when raised, will cause a child idiom to exit, returning control to the parent state. Each REG 
EXCEPTION command takes three parameters: the name of a function to call to test whether or not the exception 
should be raised; an arbitrary set of parameters that are passed to that function; and the exception 
name, which is an enumerated type. The .nal in-action of state 2and3calls the 2Talkidiom, passing it 
actors A and B as parameters. The 2Talkidiom is then executed at once. All of the exceptions are implicitly 
tested before the actions in every state of the child idiom are executed. The 2Talkidiom will return 
either when it executes a RETURNin one of its actions or when one of the exceptions is raised. In either 
case, control is returned to the parent state at that point, and its ac­tions are executed. The actions 
for state 2and3are: DEFINE_STATE_ACTIONS(2and3) WHEN(EXCEPTION_RAISED(LEFT_CONV)) DO( GOTO(1); ) WHEN(EXCEPTION_RAISED(TOO_LONG)) 
DO( GOTO(1); ) OTHERWISE DO( GOTO(4b); ) END_STATE_ACTION If either the LEFT CONVor TOO LONGexception 
has been raised, then a transition is made back to state 1to get another establishing shot. Otherwise, 
a transition is made to get a reaction shot. The out-actions of state 2and3, evaluated just before the 
transition to the new state is made, are used to remove the exceptions that were set up by the in-actions: 
DEFINE_STATE_OUT_ACTION(2and3) DELETE_EXCEPTION(LEFT_CONV); DELETE_EXCEPTION(TOO_LONG); DELETE_EXCEPTION(GET_REACTION); 
 END_STATE_OUT_ACTION  3.3.3 Idioms for movement Capturing screen motion (Figure 9) presents special 
problems. In particular, it may be desirable to end a shot not only when an event is triggered by the 
real-time system, but also when an actor reaches a certain position on the screen (such as the edge of 
the screen). The global variables forwardedge[x], rearedge[x], centerline[x] are used to facilitate these 
kinds of tests. These variables are measured in a screen-coordinate system of the actor, which is set 
up relative to the orientation of each actor x. The edge of the screen that the actor is facing is de.ned 
to be at +1, and the edge to the actor s rear, .1.The center line of the screen is at 0. Thus, for example, 
a state can see if actor x has just reached the edge of the screen by testing whether forwardedge[x]is 
greater than 1. It can test whether the actor has walked completely off the screen by testing whether 
rearedge[x]is greater than 1.  4 The Party application For illustration, the Virtual Cinematographer 
has been applied to a simulated party environment. The party application runs over a network, so that 
multiple participants can interact in a single virtual environment. Each participant controls a different 
actor (their pro­tagonist) at the party. The actors can walk, look around, converse with each other, 
or go to the bar where they can drink and talk to the bartender. The user interface allows the user to 
invoke (verb, object) pairs, which are translated into (subject, verb, object) triples in which the protagonist 
is the subject. Current verbs include talk, react, goto, drink, lookat,and idle. Each invocation of a 
verb causes a change in the action of the protagonist shortly after the corresponding but­ton is pushed. 
An additional interface button allows the actors who stand alone or in a conversation to vote on whether 
to accept or reject a new actor signaling to join in the conversation. The signal verb is implicitly 
generated when an actor approaches within a short distance of the object of the goto verb. At each time 
tick, the party application running on each client work­station sends an update of the current actions 
of that client s protag­onist to a server. The server then broadcasts a list of (subject, verb, object) 
triples of interest to each protagonist s private VC. Triples of interest are those involving the protagonist 
(or others in the same conversation as the protagonist) as subject or object. The party ap­plication 
is responsible for all low-level motion of the actors, includ­ing walking, mouth movements, head turning, 
etc. 5Results The party application, renderer, and Virtual Cinematographer run on a Pentium PC. They 
are implemented in Visual C++, except for the user interface code, which is written in Visual Basic. 
The ren- R derer uses Rendermorphics®to generate each frame. The full sys­tem runs at a rate of approximately 
5 ticks per second, of which the majority of time is spent in the renderer. Figures 9 and 10 depict the 
Movingidiom in action and the hier­archy of Converse, 3Talk,and 2Talk. Individual frames are shown on 
the left, with a corresponding shot from above on the right (which includes the camera itself circled 
in black.) In Figure 10, the arcs are labeled with the condition causing a transition to occur be­tween 
states. (Heavy lines indicate the path taken through the idiom. Dotted lines indicate jumps between idioms 
caused by the calling and exception mechanisms.) Figure 10 depicts the use of the hierarchy of .lm idioms. 
The se­quence begins in the generic Converseidiom, which speci.es the groupshot. The converse idiom then 
calls 3Talk, which follows an ext1to2 shot by calling 2Talk. Eventually 2Talk is inter­rupted by an exception 
and by the 3Talk idiom. Note the subtle rearrangement of the characters and the removal of extraneous 
char­acters in the 3Talkidiom. Conclusion and future work This paper has described a Virtual Cinematographer 
whose archi­tecture is well suited to a number of real-time applications involv­ing virtual actors. The 
VC has been demonstrated in the context of a networked virtual party application. By encoding expertise 
de­veloped by real .lmmakers into a hierarchical .nite state machine, the VC automatically generates 
camera control for individual shots and sequences these shots as the action unfolds. There are a number 
of areas for future work. Although the camera modules have proved quite robust, they can fail for a few 
frames due to unexpected occlusions, or they may miss a critical action due to minimum-length shot constraints. 
Some of these issues can be re­solved by redesigning the idioms in the current structure. We are also 
looking into incorporating simple constraint solvers, such as the ones proposed by Drucker [4, 5, 6] 
and Gleicher and Witkin [8]. In addition, we would like to expand the input to the VC to include such 
ancillary information as the emotional state of the scene or of indi­vidual actors. For example, if the 
situation is tense, faster cuts might be made, or if one actor is scared, the camera might be lowered 
to give the other actors a looming appearance. We would also like to apply similar rules for automatically 
lighting the scenes and actors in a cinematographic style. Acknowledgements We would like to thank Daniel 
Weld and Sean Anderson for their signi.cant contributions during an earlier phase of this work. We would 
also like to thank Jutta M. Joesch for her help in editing the paper.  References [1] Daniel Arijon. 
Grammar of the Film Language. Communication Arts Books, Hastings House, Publishers, New York, 1976. [2] 
James Blinn. Where am I? What am I looking at? IEEE Computer Graphics and Applications, pages 76 81, 
1988. [3] David B. Christianson, Sean E. Anderson, Li-wei He, David H. Salesin, Daniel S. Weld, and Michael 
F. Cohen. Declarative camera control for automatic cinematography. In Proceedings of the AAAI-96, August 
1996. [4] Steven M. Drucker, Tinsley A. Galyean, and David Zeltzer. CINEMA: A system for procedural camera 
movements. In David Zeltzer, edi­tor, Computer Graphics (1992 Symposium on Interactive 3D Graph­ics), 
volume 25, pages 67 70, March 1992. [5] Steven M. Drucker and David Zelter. CamDroid: A system for imple­menting 
intelligent camera control. In Michael Zyda, editor, Computer Graphics (1995 Symposium on Interactive 
3D Graphics), volume 28, pages 139 144, April 1995. [6] Steven M. Drucker and David Zeltzer. Intelligent 
camera control in a virtual environment. In Proceedings of Graphics Interface 94, pages 190 199, Banff, 
Alberta, Canada, May 1994. Canadian Information Processing Society. [7] James D. Foley, Andries van Dam, 
Steven K. Feiner, and John F. Hughes. Computer Graphics, Principles and Practice. Addison-Wesley Publishing 
Company, Reading, Massachusetts, second edition, 1990. [8] Michael Gleicher and Andrew Witkin. Through-the-lens 
camera con­trol. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH 92 Proceedings), volume 26, 
pages 331 340, July 1992. [9] David Harel. Statecharts: A visual formalism for complex systems. Science 
of Computer Programming, pages 231 274, 1987. [10] Donald Hearn and M. Pauline Baker. Computer Graphics. 
Prentice Hall, Englewood Cliffs, New Jersey, second edition, 1994. [11] Peter Karp and Steven Feiner. 
Issues in the automated generation of an­imated presentations. In Proceedings of Graphics Interface 90, 
pages 39 48, May 1990. [12] Peter Karp and Steven Feiner. Automated presentation planning of animation 
using task decomposition with heuristic reasoning. In Pro­ceedings of Graphics Interface 93, pages 118 
127, Toronto, Ontario, Canada, May 1993. Canadian Information Processing Society. [13] Christopher Lukas. 
Directing for Film and Television. Anchor Press/Doubleday, Garden City, N.Y., 1985. [14] Jock D. Mackinlay, 
Stuart K. Card, and George G. Robertson. Rapid controlled movement through a virtual 3D workspace. In 
Forest Bas­kett, editor, Computer Graphics (SIGGRAPH 90 Proceedings), vol­ume 24, pages 171 176, August 
1990. [15] Joseph V. Mascelli. The Five C s of Cinematography. Cine/Gra.c Pub­lications, Hollywood, 1965. 
[16] Cary B. Phillips, Norman I. Badler, and John Granieri. Automatic viewing control for 3D direct manipulation. 
In David Zeltzer, editor, Computer Graphics (1992 Symposium on Interactive 3D Graphics), volume 25, pages 
71 74, March 1992. [17] Warren Sack and Marc Davis. IDIC: Assembling video sequences from story plans 
and content annotations. In IEEE International Con­ference on Multimedia Computing and Systems, Boston, 
MA, May 1994. [18] Patrick Tucker. Secrets of Screen Acting. Routledge, New York, 1994. Moving Converse 
3Talk 2Talk Call 3Talk(A,B,C) Call 2Talk(A,B) external(B,A) T>8 T>8 T>12 T>12 D<10 D<10 D<5 external(A,B) 
apex(A,B) pan(A) track(A) apex(A,B) ext1to2(A,B,C) group(A,B,C) external(B,A) external(A,B) internal(C) 
Figure 9: Idiom State Transitions Figure 10: Idiom Hierarchy  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237260</article_id>
		<sort_key>225</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Comic Chat]]></title>
		<page_from>225</page_from>
		<page_to>236</page_to>
		<doi_number>10.1145/237170.237260</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237260</url>
		<keywords>
			<kw><![CDATA[Internet]]></kw>
			<kw><![CDATA[World Wide Web]]></kw>
			<kw><![CDATA[automated presentation]]></kw>
			<kw><![CDATA[chat programs]]></kw>
			<kw><![CDATA[comics]]></kw>
			<kw><![CDATA[graphical histories]]></kw>
			<kw><![CDATA[illustration]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[user interfaces]]></kw>
			<kw><![CDATA[virtual worlds]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.1</cat_node>
				<descriptor>Data communications</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.2</cat_node>
				<descriptor>User interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Internet</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011069</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Integrated and visual development environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14026309</person_id>
				<author_profile_id><![CDATA[81100041571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurlander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, One Microsoft Way, Redmond, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31095154</person_id>
				<author_profile_id><![CDATA[81344498343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Skelly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, One Microsoft Way, Redmond, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63624</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Washington, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anderson, M. Joe's Dope: If Ya Gotta Do It.. Do It Right! PS: The Preventive Maintenance Monthly. Issue 279, February 1976.29-36.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barr, A. and Feigenbaum, E. A. The Handbook of Artificial Intelligence. volume 1. William Kaufmann, Inc., Los Altos, CA. 1981. 285-287.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192272</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn, B., Beckett, T., Douville, B., Prevost, S., and Stone, M. Automated Conversation: Rule-based Generation of Spoken Expression, Gesture, and Spoken Intonation for Multiple Conversational Agents. Proceedings of SIGGRAPH '94 (Orlando, FL, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conf. Series, ACM, New York. 413-420.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CD-ROMIX! Inc., FREEX #1. P. O. Box 2961, Torrance, CA 90509. 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Eisner, W. Comics &amp; Sequential Art. Poorhouse Press. Tamarac, FL. 1985.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Feiner, S. APEX: An Experiment in the Automated Creation of Pictorial Explanations. IEEE Computer Graphics and Applications, 5, 11. November 1985.29-37.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gornick, L. The Cartoon History of the Universe. Doubleday. New York. 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Haeberli, P. Paint by Numbers: Abstract Image Representations. Proceedings of SIGGRAPH '90 (Dallas, TX, Aug. 6- 10). In Computer Graphics, 24, 4 (Aug. 1990), ACM, New York. 1990. 207-214.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>168134</ref_obj_id>
				<ref_obj_pid>168080</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kurlander, D. and Feiner, S. A History of Editable Graphical Histories. In Watch What I Do: Programming by Demonstration. Allen Cypher, ed. MIT Press, Cambridge, MA. 1993. 405-413.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lent, J. Oh, What a Time It Was: The Early Days of the Funnies. Witty World: International Cartoon Magazine. No. 19. Summer/Autumn 1995.16-22.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lent, J. "Yellow Kid" Celebrates Century with Hectic Schedule in 1995. Witty World: International Cartoon Magazine. No. 19. Summer/Autumn 1995.18-19.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>22950</ref_obj_id>
				<ref_obj_pid>22949</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Mackinlay, J. Automating the Design of Graphical Presentations of Relational Information. ACM Trans. on Graphics, 5, 2. April 1986. 110-141.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[McCloud, S. Understanding Comics. Kitchen Sink Press. Northampton, MA. 1993.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Microsoft Corp., Microsoft Introduces V-Chat Communications for MSN, The Microsoft Network. Nov. 30, 1995. Microsoft Press Release. Redmond, WA 98052.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Montgomery R. and Gilligan, S. Comic Creator. Spark Interactive, 112 W. San Francisco St., Sante Fe, NM 87501, 1995.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>114792</ref_obj_id>
				<ref_obj_pid>114772</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Morningstar, C. and Farmer, F. R. The Lessons of Lucasfilm' s Habitat. In Cyberspace: First Steps. Benedikt, M., ed. MIT Press, Cambridge, MA. 1991. 273-301.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Oikarinen, J. and Reed, D. Internet Relay Chat Protocol. Internet RFC #1459. May 1993.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M. P., Anderson, S. E., Barzel, R., and Salesin, D.H. Interactive Pen and Ink Illustration. Proceedings of SIG- GRAPH '94 (Orlando, FL, July 24-29). In Computer Graphics Proceedings, Annual Conf. Series, ACM, New York. 1994. 101-108.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122732</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Seligmann, D. D., and Feiner, S. Automated Generation of Intent-Based 3D Illustrations. Proceedings of SIGGRAPH '91 (Las Vegas, Nevada, July 28 - Aug. 2). In Computer Graphics, 25, 4 (July 1991) ACM, New York. 1991, 123-132.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Trupin, 3. The Visual Programmer Puts ActiveX Documents Through Their Paces. Microsoft Systems Journal, 11, 6. June 1996.55-76.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G., and Salesin, D. Computer Generated Pen and Ink Illustration. Proceedings of SIGGRAPH '94 (Orlando, FL, July 24-29). In Computer Graphics Proceedings, Annual Conf. Series, ACM, New York. 1994, 91-100.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Worlds, Inc. Worlds Chat: Meet Your New Cyberfriends. http: //www.worlds.net/products/wchat/readme.html.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Comic Chat David Kurlander Tim Skelly David Salesin Microsoft Research Microsoft Research Department 
of Computer Science One Microsoft Way One Microsoft Way University of Washington Redmond, WA 98052 Redmond, 
WA 98052 Seattle, WA 98195 djk@microsoft.com timsk@microsoft.com salesin@cs.washington.edu ABSTRACT 
Comics have a rich visual vocabulary, and people find them appeal­ing. They are also an effective form 
of communication. We have built a system, called Comic Chat, that represents on-line commu­nications 
in the form of comics. Comic Chat automates numerous aspects of comics generation, including balloon 
construction and layout, the placement and orientation of comic characters, the de­fault selection of 
character gestures and expressions, the incorpora­tion of semantic panel elements, and the choice of 
zoom factor for the virtual camera. This paper describes the mechanisms that Comic Chat uses to perform 
this automation, as well as novel as­pects of the program s user interface. Comic Chat is a working pro­gram, 
allowing groups of people to communicate over the Internet. It has several advantages over other graphical 
chat programs, in­cluding the availability of a graphical history, and a dynamic graph­ical presentation. 
CR Categories: I.3.2 [Computer Graphics]: Graphics Systems distributed /network graphics; K.8.1 [Personal 
Computing]: Appli­cation Packages; D.2.2 [Information Interfaces and Presentation]: Tools and Techniques 
user interfaces. Keywords: Non-photorealistic rendering, comics, chat programs, virtual worlds, graphical 
histories, automated presentation, illustra­tion, user interfaces, Internet, World Wide Web. 1. INTRODUCTION 
Computers are rapidly changing the way that people communicate, as is evidenced by the proliferation 
of electronic mail and the ad­vent of the World Wide Web. As the Internet and on-line services have grown 
in popularity, so has another form of computer-assisted communication: electronic chat rooms. Although 
electronic chat programs originally supported only text-based communications, fast modems and network 
connections have enabled the authors of these programs to create a richer user experience through the 
addi­tion of graphics [14, 16, 22]. Participants in these new chat rooms communicate with one another 
not only by typing text, but also by changing the gesture or facial expression of an avatar, which serves 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 as their graphical representation. In these chat rooms, graphics 
serves as a first-class form of communication it is essential, not merely ornamental. Considering the 
importance of graphics to modern chat rooms, it is a true failing that these programs provide only textual 
history tran­scripts. Histories are crucial in chat programs, since participants commonly interleave 
several tasks while chatting on-line. For ex­ample, people often participate in chats while writing documents, 
taking care of children, compiling programs, or monitoring other chat rooms. Any graphical communication 
that occurs while a par­ticipant is tending to another task is either lost or translated into tex­tual 
form. Since there is no graphical history of the conversation, it is difficult to share the experience 
with non-participants, or to re­view the chat at a later time. A second problem with current graphical 
chat programs is the re­quirement that participants must spend a significant amount of time doing things 
other than chatting. Most graphical chat programs re­quire that participants navigate the room (or world), 
looking for an interesting conversation. Gestures must be specified either by hit­ting memorized function 
keys, or navigating menus or buttons. Care must be taken not to obstruct the view of other avatars, or 
to keep them in view. In 2D chat rooms avatars often overlap, and in 3D rooms they can accidently move 
behind or between other partic­ipants in the conversation. A third aspect of graphical chat programs 
that can be improved upon is the relatively static nature of the graphical presentation. Al­though some 
chat rooms have animated backgrounds, the partici­pants tend to see the same exact scene composition, 
at the same exact point of view, unless they or others move. Much can be done to make the view and scene 
composition more dynamic, and to make them reflect and enhance aspects of the conversation. To address 
these problems, we have developed a new graphical representation of electronic chat rooms, based on the 
visual con­ventions of comics. Comics have a very rich tradition, and a dis­tinctive, compelling, and 
entertaining visual vocabulary. Nearly everyone is familiar with the comic form, and many of its conven­tions 
have become second nature, even to the few that rarely read comics. Many of the same visual conventions 
are shared by comics throughout the world, which is important when choosing a graphi­cal presentation 
for chat rooms on worldwide electronic networks. We have built a system, called Comic Chat, that automatically 
generates comics to depict on-line graphical chats. Relying on the rules of comic panel composition, 
this system chooses which ava­tars (presented as comic characters) to include in each panel, deter­mines 
their placement and orientation, constructs word balloons of multiple types, and places these word balloons 
according to the rules for proper reading order. The system also chooses an appro­priate camera zoom 
factor for each panel, decides when to begin a new panel, and selects default gestures and expressions 
that the par­ticipants can override. To make the chat rooms more lively and fun, Comic Chat can adjust 
the background or scene elements to reflect the topic of the conversation. All of the comics appearing 
in this pa­per were generated automatically by the Comic Chat system. In several instances, we felt it 
important to show examples of what can go wrong in constructing comics, and in these cases we gener­ated 
comics by disabling components of the system. Although the origins of comics are very old, this project 
was under­taken to coincide with the centennial of the modern comic strip, as commonly measured from 
the publication of Richard Outcault s Yellow Kid, first appearing in 1895 in the New York World [10]. 
This past year, the comic strip centennial has been celebrated in the U. S. with a set of commemorative 
stamps, the opening of the Na­tional Gallery of Caricature and Cartoon Art, and even a series of textual 
chat room discussions with professional comic artists [11]. We consider it a fitting time to explore 
how on-line communica­tions might benefit from the comic strip. The next section describes related work. 
Section 3 explains the methodology that we used to select the features of comics to auto­mate in our 
system. Section 4 describes how Comic Chat decides which characters to include in each panel, and how 
it positions and draws them. Issues related to balloon construction and layout are covered in Section 
5. Section 6 addresses other important consider­ations in constructing panels. Section 7 discusses implementation 
issues. Several examples of our system s output are provided in Section 8. Finally Section 9 presents 
our conclusions and describes possible directions for future work.  2. RELATED WORK Some of the best 
analyses of the visual language of comics have been written by comic artists. Both McCloud [13] and Eisner 
[5] authored excellent books on the subject. Several comics have been published electronically, some 
of which use techniques like color­table animation and staged presentation, to achieve effects beyond 
the scope of traditional comics [4]. Other commercial software pro­grams allow people to author their 
own comics [15]. However, these programs are essentially drawing programs that allow users to assemble 
comics from clip art they do not attempt to automate the process of comics production. Several computer 
graphics researchers have built systems to pro­duce illustrations automatically that satisfy a specified 
goal. Feiner s APEX system generates 3D illustrations depicting the steps of tasks, such as equipment 
repair [6]. Mackinlay s APT sys­tem determines the form of graph that best represents a given type of 
quantitative information, selecting which graphical attributes are best suited to represent that information 
[12]. The IBIS system, de­veloped by Seligmann and Feiner, employs a generate-and-test mechanism with 
backtracking, to create illustrations that satisfy a given communicative intent [19]. Comic Chat also 
automates the construction of illustrations, but it is more special purpose, relying on domain-specific 
techniques. Cassel et al. developed techniques to automate gestures and facial expressions in the animation 
of conversations between animated characters [3]. Their system relies on a database of facts, goals, 
and beliefs, not only to generate aspects of the characters animations, but also to generate the text 
spoken by the characters. Comic Chat also generates default gestures and facial expressions for its charac­ters, 
but because the system receives unconstrained textual input, it is difficult to produce sophisticated 
inferences about the conversa­tion to control gesture and expression. Instead, Comic Chat uses simple 
rules, based on comics and on-line chat conventions, to choose default gestures and expressions, which 
can be overridden by the user. Research aimed at achieving aesthetic, non-photorealistic images is receiving 
increasing interest in the graphics community. Haeberli developed techniques to produce artfully painted 
pictures interac­tively from image data [8]. Winkenbach and Salesin [21] and Salis­bury et al. [18] devised 
methods for generating pen and ink illustrations from 3D models and images. Today, comics are also gaining 
recognition as an artform, and although a few comic artists are beginning to use computer-based tools, 
comics have not previ­ously been targeted for computer graphics research. A benefit of depicting chat 
sessions in comics form is that the pri­mary visual representation also serves as a history transcript. 
Kurlander s editable graphical histories also represent the history of graphical user interaction as 
a series of panels [9]. However, that history representation adopts no other conventions from the visual 
language of comics. Comic Chat was motivated by the appearance of several graphical chat programs in 
recent years. Habitat, built by members of Lucas­film s game group [16], is a virtual multi-user world, 
rather than simply a chat system, although providing a new means for social interaction and communication 
was one of the project s major goals. Habitat places a very simple word balloon over the head of the 
last avatar to speak. When the next avatar speaks, the earlier word balloon loses its tail and scrolls 
upwards. As avatars move about, their balloons do not follow. Balloon construction in Habitat is very 
coarse, only slightly reminiscent of comics, and no real lay­out is performed. Yet Habitat was revolutionary 
in showing that on­line graphical virtual communities are not only possible but worth­while, and later 
incarnations of the system are popular today, in­cluding Fujitsu Habitat in Japan, and CompuServe s WorldsAway. 
More recently, several companies have deployed three-dimen­sional graphical chat rooms. In Worlds Inc. 
s WorldsChat, partici­pants navigate a virtual space station, and see the environment from either a first-person 
or third-person perspective [22]. All communi­cation, however, is textual. On the Microsoft Network, 
V-Chat (for Virtual Chat ) provides several 2D and 3D environments. Partici­pants can gesture and change 
their avatars expressions, as well as type text to one another [14]. 3. METHODOLOGY To help us determine 
how to best represent chat sessions in the comic form, we first gathered numerous chat transcripts from 
an on-line service. We then read through the transcripts and annotated them with any information that 
we felt a computer would be able to extract easily, and that might be useful in composing the strips. 
Jim Woodring, a professional comic artist, took these transcripts, and il­lustrated a representative 
session. Prior to seeing Woodring s illustrations, we were concerned that an effective comic chat representation 
might need to reflect deep se­mantics of the conversation, semantics beyond the reach of current natural 
language understanding technology. After reviewing the prototype illustrations, it became clear that 
we could produce inter­esting comics with only very limited semantics, and without any natural language 
processing. Comic styles vary so dramatically that a single system capable of producing them all is inconceivable. 
Instead, we undertook to mimic the style of Woodring s prototype illustrations as precisely as possible 
in the Comic Chat system, and we designed the archi­tecture so that it could be readily extended to include 
additional styles. Carefully reviewing the artist-drawn prototypes, we determined the design elements 
that require automation. These elements fall under three broad categories: characters, balloons, and 
panels. The fol­lowing three sections address each category in turn. 4. CHARACTERS In our system, comic 
characters serve as avatars for the chat partici­pants. Participants select their own character from 
a presupplied list. In composing the panels, several aspects of the characters need to be determined, 
including their gestures and expressions, which characters to draw, and their positions and orientations. 
 4.1 Gestures and expressions We refer to gesture as a character s body pose, and expression as its 
facial pose. Most of the characters in Comic Chat have a set of heads that can fit on a set of bodies; 
hence their expressions can be independent of their gestures. Since one of our design goals is to minimize 
the amount of user interaction necessary, Comic Chat picks default gestures and expressions, which the 
user is free to override. When a participant types in dialogue for his or her character, Comic Chat uses 
this dialogue to determine a default gesture and expres­sion. An analysis of chat transcripts revealed 
a number of chat-spe­cific conventions that we could effectively exploit, and our comic artist suggested 
a few additional indicators: 1. Emoticons: for example, :-) and :-( , indicate happy and sad. 2. Chat 
acronyms: a. LOL (Laughing Out Loud), ROTFL (Rolling On The Floor Laughing) indicate laughter b. IMHO 
(In My Humble Opinion) results in character point­ing to self. c. BRB (Be Right Back) results in character 
waving. d. <g> (or <grin>) indicates smiling.  3. Typesetting: All caps indicates shouting. 4. Punctuation: 
!!! indicates shouting. 5. Greetings: Hi, hello, bye, goodbye, welcome (at beginning of sentence), result 
in character waving. 6. Self-references: I (at beginning of sentence), I ll, I will, I m, I am, I d, 
I would, etc... result in character pointing to self. 7. Other-references: You (at beginning of sentence), 
are you, will you, did you, don t you, etc... result in character pointing to other.  Initially, we 
had thought that simple punctuation would be a much better indicator for expressions in comics (for example, 
a ? for questioning, or a single ! for exclaiming). However, typically punctuation is an indicator of 
fairly subtle expressions, and such subtlety tends to get lost in comics. In contrast, characters pointing 
and waving, which occur relatively infrequently in real life, come off well in comics. Note that since 
gesture and expression can be controlled indepen­dently, multiple indicators can be extracted and used 
from the same input. For example, I can t make it :-( , results in a character pointing to itself and 
frowning. Since multiple indicators can be ex­tracted from a single input, they may also conflict. For 
example, Hi Sue, how are you? , would suggest that the character both wave and point outward. Since constructing 
composite gestures from many individual 2D bitmaps of body parts (left arm pose, right arm pose, etc...) 
makes for a nearly impossible art authoring process, Comic Chat instead uses a prioritization scheme 
to choose the most important gesture. (a) (b) (c) Figure 1. Emotion wheel and three bodies. a) a neutral 
pose; b) somewhat angry; c) very angry. When no expression or gesture indicators are found in the utter­ance, 
the system chooses a default neutral expression and gesture. The system has multiple neutral expressions 
and gestures, and cy­cles through these to increase the visual richness of the comics pro­duced. Often 
there are multiple versions of other expressions and gestures, too. Participants can see the system-selected 
expression and gesture for their character as they enter text. If they would pre­fer to override these, 
they can do so with a user-interface widget that we call an emotion wheel. The emotion wheel is a circle, 
with emotions distributed along its perimeter: coy, happy, laughing, shouting1, angry, sad, scared, and 
bored. At the center of the circle is neutral, and the further from the center, the greater the intensity 
of the emotion. The emotion wheel is similar to color choosers that distribute fully-saturated hues along 
the perimeter and have a shade of gray at the center. A nice aspect of the emotion wheel is that although 
emotion type and in­tensity are controlled with a single mouse movement, the results are still very intuitive. 
Figure 1 shows the emotion wheel, along with the character feed­back pane from Comic Chat s user interface. 
Initially, in Figure 1a, the control is at the center, and the character is in a neutral pose. In Figure 
1b, the character becomes slightly angry, as the control is moved towards the angry direction on the 
perimeter. The charac­ter becomes much angrier in Figure 1c, as the control is moved fur­ther toward 
the perimeter. Emotions can affect both expression and gesture, as seen in Figure 1. Having both expression 
and gesture contribute to the portrayal of an emotion can provide many more shades of an emotion, given 
a limited supply of art. In comics as in life, intense emotions can turn to actions, so it may make sense 
to add actions to the emotion wheel as well. For exam­ple, if the character becomes extremely angry, 
he or she might punch another. While this may be amusing at first, we fear it would become overly disruptive. 
Actions that do not correspond to emotions, such as waving and pointing, are not accessible from the 
wheel. Instead, users can in­voke them by mousing on the target character in any panel, and 1. Technically, 
shouting is not an emotion, but an action. Howev­er, we have found that it works best to include it on 
the wheel. (a) (b) (c) (d) Figure 2. Three poorly composed panels, and one well-composed panel. a) 
missing speaker; b) conversational participants face away from each other; c) outer conversational pair 
is separated; d) well-composed panel. then choosing the action from a pop-up menu. Since much of com­munication 
is non-verbal, Comic Chat allows gestures and expres­sions to be transmitted with or without accompanying 
text.  4.2 Characters for inclusion Only about five characters can fit in one of our panels, and as 
in television and film, it is best not to show every character in every shot. It is, however, critical 
to show characters whenever they are speaking. Professional comic artists break this rule extremely rarely, 
and then typically only to make a stylistic statement. Figure 2a shows an example of breaking this rule. 
If a character starts speaking to someone new, it is also important to include the character (or characters) 
being addressed. Once the ad­dressee has been established, he or she can optionally be omitted from subsequent 
panels. When a participant reacts to what is said with either a gesture or expression, the system must 
show this as well. When new characters enter the chat world, they are shown immedi­ately, whether or 
not they say something. This makes it clear to the other participants that they have someone new monitoring 
their conversation, as well as someone new with whom they may wish to speak. 4.3 Position and orientation 
Once it is determined which characters should be included in the panel, the next step is to position 
the characters and determine the direction they should face. In comics, as in life, people talking to 
each other tend to face one another and be situated together. Hence, the characters in Figures 2b and 
2c are poorly arranged, and Figure 2d presents a better configuration. Of course, this rule suggests 
that it is important to know who is speaking to whom when rendering a conversation as comics. Since the 
intended recipient of an utterance is not always apparent, participants in textual chat rooms often follow 
the convention of addressing each of their statements to particular individuals. Comic Chat looks for 
the nicknames of each of the chat participants in ev­ery utterance and tries to infer who is being addressed. 
(The nick­name of a character appears whenever the cursor is placed over a character.) Alternatively, 
the user can also make the addressees ex­plicit by selecting them with the mouse. Since there is a great 
deal of coherence in who is talking to whom in a conversation, this is not a significant hassle for the 
participants, and it also allows us to pro­vide an option to hide all utterances not explicitly addressed 
to a specific participant. If someone chooses not to address their com­ment to specific individuals, 
it is interpreted as a general statement to the group, and the character is ideally made to face as many 
par­ticipants as possible. We have constructed an evaluation function that considers these criteria, 
as well as some additional ones, and returns a lower value for better layouts. Keep in mind that the 
actual numbers in this function are arbitrary, but the function does capture the criteria we feel are 
important as well as the relative significance of the criteria. Let C be the set of characters to be 
included in the panel. Let a and b range over every member of C. We search for an ordering and ori­entation 
of the characters that minimizes the sum: S (Facing (a, b) + Neighbors (a, Left (a), Right (a))) a, b 
. C | a . b The Facing function penalizes conversational pairs in which the characters do not face one 
another or are not proximal. It is the sum of the numbers on the left whose predicate on the right is 
true: 4: if a has not addressed his utterance, and is not facing b. 2: if a has not addressed his utterance, 
and b is not facing a. 4: if a has addressed his utterance to b, and b is not facing a. 40: if a has 
addressed his utterance to b, and a is not facing b. 4n: if a has addressed his utterance to b, and the 
number of characters between them is n. The Neighbors function discourages the system from shuffling 
the positions of characters from panel to panel. For each character in the sequence, we consider the 
characters to the left and the right, and for each of these that is different from the character last 
appear­ing there, a single point penalty is assessed. Note that maintaining consistency of positions 
from panel to panel is the least important consideration, and some comic artists do not feel this to 
be impor­tant at all. Now that we have an evaluation function, we try to find the posi­tions and orientations 
of the characters that minimize it. Instead of trying every combination, we use a greedy algorithm. After 
placing the first character, we find which of the two possible positions (and two possible orientations 
in each position) is best for the sec­ond. After placing the second, we consider the three possible posi­tions 
for the third, and so on. Although this technique is not guaranteed to find the best possible arrangement, 
it tends to do an adequate job and is fast. 4.4 Rendering The process of rendering a character consists 
of composing to­gether a head bitmap with a body bitmap at the proper offset, and flipping them if necessary 
to make the character face the proper di­rection. There is however one subtle yet important detail. Comic 
artists that draw complicated backgrounds often leave a little white space around the character. This 
makes the character much more visible, (a) (b) and makes it pop out of the background. We call this 
area around the character, the halo. Figure 3a shows two characters drawn with­out halos, and Figure 
3b shows two characters with halos. For each head and body bitmap, we also save a halo mask. It is im­portant 
to draw both of the halos before drawing either the head or body, since the head s halo must not overwrite 
any of the body, and the body s halo must not overwrite any of the head. 5. BALLOONS Word balloons are 
one of the most distinctive and readily recogniz­able elements of the comic medium. Most comic artists 
represent dialogue in word balloons, and these balloons are clearly critical in Comic Chat, since much 
of the communications between partici­pants is still textual. This section addresses the various types 
of word balloons, as well as issues in their layout and construction.  5.1 Types The appearance of balloons 
varies dramatically from artist to artist. Some artists draw balloons as ovals, others draw rectangular 
bal­loons. Certain artists omit the traditional balloon outline, and draw a dash from the character to 
the words. Woodring draws balloons that flow around his text. We felt that these balloons would be ideal 
for Comic Chat, because every balloon is different, and the result­ing variety and visual richness assist 
in hiding the machine-con­structed origin of the panels. Even for a given artistic style, there are several 
types of balloons within the comic vocabulary. Figure 4 shows three different bal­loon types in the style 
that we are emulating. The most basic type is the speech balloon, an example of which appears in Figure 
4a. The outline is solid, with the upper part (the body of the balloon) sur­rounding the dialogue. Emanating 
from the body is the tail, which points towards the speaker. Figure 4b illustrates a thought balloon. 
It is similar to the speech balloon, except instead of having a solid tail, its tail is composed of ovals. 
In some styles, the body of the thought balloon appears more like a cloud. (a) (b) (c) Figure 4. Three 
types of balloons: a) speech balloon; b) thought balloon; c) whisper balloon. Figure 4c shows a whisper 
balloon. It differs from speech balloons in having a dashed outline, which is made more visible by the 
pres­ence of a halo. The text in whisper balloons is often italicized. In chat terminology, whispering 
means sending a message only to a select subset of the participants, and it is convenient that the comic 
vocabulary already has a convention for this. A fourth type of bal­loon is the shout balloon, which has 
a jagged outline (and is yet to be implemented in Comic Chat). Although other types of balloons do exist, 
these four are the most prevalent.  5.2 Layout Before constructing balloons, we run a layout algorithm 
that deter­mines each balloon s dimensions and placement. In the style that we are emulating, all balloons 
appear above the tallest character s head. However, additional constraints must be considered as well. 
In comics, the relative placement of balloons determines the read­ing order. Because it is important 
in Comic Chat to convey the or­der of the utterances, we strictly follow the appropriate comic conventions. 
The rule is that balloons are read in a top-down order, and when multiple balloons are at the same height, 
they are read from left to right. Note that the order in which balloons are read is independent of where 
characters are placed in the panel. A third point that helps constrain the placement of the balloons 
in Comic Chat is the style-specific requirement that some part of each balloon float at least partially 
over the center of the speaker s face. We have designed a layout algorithm that takes all of these rules 
into account, while adding a little randomness to each layout to avoid the appearance of machine-generated 
regularity. We use a greedy algorithm for placing the bodies of balloons; once the algorithm chooses 
a body location, it never alters it. However, the tail placement is not greedy; instead, the algorithm 
defers plac­ing the tails until all the bodies have been placed. Note that using a greedy algorithm is 
suitable for body placement because it is fast, does a good job, and because there is no real need to 
pack the maxi­mum amount of text into a panel, as cartoonists do not typically do this either. On the 
other hand, we originally tried using a greedy al­gorithm for tail placement, with poor results. Our 
layout algorithm makes use of intervals, called routing chan­nels, for ensuring that there will be enough 
room to place the tails once the bodies are positioned. Routing channels are disjoint they are a partitioning 
of the space available for routing balloon tails. Each routing channel is associated with a balloon and 
indicates where that balloon s tail could be placed. For example, in Figure 5a, the darkly shaded area 
is the original routing channel for the topmost balloon, spoken by the center character. This region 
spans the width of the balloon; initially, the tail could go anywhere in this region and be guaranteed 
not to intersect another balloon or tail. When a second character speaks (in this case, the leftmost 
charac­ter), the first routing channel is decreased in size in order to accom­modate the second balloon, 
as shown in Figure 5b. The third balloon, in its original position, completely covers what remains of 
the first routing channel, leaving no place for the first balloon s tail. Our algorithm recognizes this 
situation and shifts the third balloon right in order to leave enough space for the first balloon s tail. 
The final routing channel for all three balloons is shown in Figure 5c. Now that we have run through 
this example, let s define the routing algorithm more formally. To describe our algorithm, we will first 
introduce some notation. Let B = (B1,...,Bn) be an array of balloons in the order in which they appear, 
with T = (T1,...,Tn) representing the text in the balloons. Let R = (R1,...,Rn) be a corresponding array 
of routing channels, with each routing channel Ri specified by left and right endpoints Ri.l and Ri.r. 
Each balloon Bi is spoken by a  Figure 5. Routing Channels. (a) Lower left balloon requests channel 
allocation from top balloon; (b) Lower right balloon re­quests channel allocation from top balloon; (c) 
Lower right balloon shifts right, routing regions successfully allocated. character, and the x coordinate 
of the center of that character s face is represented by xi. The following procedure computes the horizontal 
placement for an array of n balloons, and sets up the routing channels R1,...,Rn. It re­turns the number 
of balloons that is placed successfully. function PlaceBalloons(B, R, x, T) for j = 1 to n do wj := FindWidth(Bj); 
Rj := [xj -wj, xj+wj] for i = 1 to j-1 do: Rj := MaxAllowable (Ri, xi, Rj, xj); end for if width (Rj) 
>= wj then Rj := Position (Bj, Rj) else if not SqueezeBalloon (Rj, Tj) then return j-1; end if end if 
for i=1 to j-1 do Ri := ReduceChannel (Ri, xi, Rj, xj) end for end for return n end function The PlaceBalloons 
function loops through each balloon, placing them one at a time. Let us assume that balloons B1 through 
Bj-1 have been placed, and let s look at how to place balloon Bj. The first step is to choose a target 
width for the balloon (described in de­tail later), and set the largest possible routing channel that 
would al­low the balloon s horizontal extent to pass above the center of the character s face. Next, 
we trim the routing channel Rj just enough to ensure that all of the previous routing channels R1,...,Rj-1 
remain wide enough to support a tail. If the resulting routing channel Rj is wider than the target width, 
then we have to choose a horizontal position for the balloon in the routing channel (we do this randomly); 
we then set the routing channel to be the horizontal extent of the balloon. Oth­erwise, we attempt to 
squeeze the balloon into a narrower channel than the original target width. If the amount of text in 
the balloon is too large to fit, then we give up and return the number of balloons previously placed. 
Finally, we reduce all of the previous routing channels to accommodate the new one. This describes the 
heart of the algorithm. All that is left are the de­tails. To choose the target width, we initially estimate 
the area that the body will cover, by computing the area of a single typeset line, and scaling it up 
by a factor of a third to account for linebreaks and leading. If the line is short, it should not be 
broken into multiple lines, and we assume a height of one text line. Otherwise, we deter­mine a conservative 
estimate of the allowable height of this balloon by calculating the distance from the bottom of the lowest 
previ­ously placed balloon to the bottom of the rectangle in which bal­loons can be placed. The minimum 
allowable width of the balloon text is the maximum of the widest word, and the balloon area di­vided 
by the allowable height. We randomly select a width between this minimum and the width of the panel. 
The function below is used to trim the routing channel Rj just enough to ensure that routing channel 
Ri remains wide enough to support a tail. It ensures that the width of routing region Ri is at least 
t, and that Ri continues to contain xi: function MaxAllowable (Ri, xi, Rj, xj): R := Rj if xi < xj then 
R.l := max{Ri.l + t, xi} else R.r := min{Ri.r -t, xi} end if return R end function Finally, the ReduceChannel 
function reduces the interval Ri so that it no longer overlaps Rj: function ReduceChannel (Ri, xi, Rj, 
xj) R := Ri if xi < xj then Ri.r := min{Ri.r, Rj.l} else Ri.l := max{Ri.l, Rj.r} end if return R end 
function As mentioned earlier, the above algorithm finds the horizontal placement of a set of balloons. 
Next we calculate the vertical place­ment. The rules for proper reading order require that the new bal­loon 
be no higher than the bottom of any balloons already placed to its right, and no higher than the top 
of any balloons already placed to its left. We place each balloon as high as possible, such that proper 
reading order is maintained. If the function fails, at least one balloon could not be successfully placed 
in the current panel. We check to see if the balloon can fit in a panel by itself. If it cannot, we break 
up the text of the balloon into smaller balloons that do fit the panel size constraint, and add ellipses 
to each of the new balloons to indicate that a split occurred.  5.3 Balloon body construction Since 
the bodies of our balloons flow around the text that they con­tain, the first step of constructing balloons 
consists of computing the layout of this text. The artist we are emulating typically centers text in 
word balloons, so we first compute word breaks and then center the text on each line. Because our goal 
is to create balloons that wrap smoothly around text, it makes sense to construct the bodies of these 
balloons with splines. After experimenting with cardinal splines and B-splines, we chose the latter, 
since they produce the best effect. Our artist draws moderately sharp bends in his balloon outlines, 
which we were able to mimic using a high B-spline tension of 5.0. When comic artists draw balloons, they 
always leave a margin between the text and the balloon outline, so we expand the boundaries of the text 
lines outward before computing the spline. Our original attempt at balloons appeared amoeba-like, following 
the text too closely, responding to its every turn. We prefer balloons that flow around the text, but 
do not draw too much attention to themselves. In studying a set of sample balloons drawn by our art­ist, 
we determined that he avoids this problem in two ways. First, he never dips the balloon inwards towards 
the text on one line, just to move outwards again on the next. Second, he tends not to shift the balloon 
outline inwards or outwards by very small increments in­stead he responds only to larger changes in the 
text outline. We implemented these two additional rules, and the resulting bal­loons were much improved, 
yet still inferior to balloons that we wished to emulate. The large remaining difference between our 
bal­loons and Woodring s was the lack of small-scale perturbation in the outline. Whenever there is a 
long segment of the outline that does not bend in or outward with the text, Woodring adds small, low 
frequency waves to give the balloon additional richness. We have been able to mimic this effect by placing 
additional control points along these large segments, which alternately move towards and away from the 
text. The resulting balloons produced by Comic Chat faithfully capture Woodring s style, and examples 
of these balloons appear in figures throughout the paper. 5.4 Balloon tail construction Now that we 
have placed and constructed the bodies of the balloons and computed routing channels for the tails, it 
is time to construct the tails themselves. The tails of speech and whisper balloons are composed of arcs 
or straight segments, and the tails of thought bal­loons consist of ovals forming either an arc or a 
line. The best looking balloons have tails emanating from under the bot­tom line of text. Depending upon 
the placement of the characters and balloons in a panel, it is sometimes necessary to choose a non­optimal 
tail connection point, as comic artists do from time to time. To choose the connection point of a tail, 
we first see if a large enough part of the last line of text spans the routing channel. If so, we choose 
a location further than a specified distance from the edge of the routing channel, to insure that the 
tail will not be exactly ad­jacent to other balloon bodies or tails. Failing this, we attach the tail 
to a piece of the balloon within the routing channel, at a small hori­zontal distance from the center 
of the speaking character s head. This allows us to give the tail a definite arc, but not an arc that 
will span diagonally over a large part of the illustration. We construct balloon tails so that they come 
to a point at roughly the same height, (which of course must be below the lowest bal­loon), and always 
at least in the lowest third of the region reserved for balloons. Since the best balloon tails head directly 
and clearly toward the speaker, it is preferable not to have tail arcs cross over the head of the character 
responsible for the balloon. In our comic style, tails emanating from a part of the balloon to the left 
of the speaker curve counterclockwise and end above the center of the speaker s face. Tails beginning 
to the right of the speaker curve clockwise and end at the same point. 5.5 Rendering Rendering a balloon 
is simply a matter of filling its interior (to hide a background that was drawn first), scan-converting 
its outline, and drawing the text. There are two important details worth mention­ing. Whisper balloons 
have halos, and these are easily rendered by initially scan-converting the outline of the balloon with 
a thick, solid, white pen, before rendering the true outline with a smaller, black, dashed pen. Also, 
most comic artists draw balloon text in all­caps, with a very distinctive (though varying) comic lettering. 
We also display the text in all-caps, no matter how it was typed. A wide variety of comic fonts is available 
commercially and as share-ware. 6. PANELS Several additional issues should be taken into account when 
com­posing and rendering panels. These include choosing when to begin a new panel, adjusting the zoom 
factor of the virtual camera, and adding semantic elements to the panels.  6.1 Panel breaks As participants 
specify new dialogue and body poses, Comic Chat proceeds to redraw the last panel, incorporating this 
additional in­formation. At some point, however, it is necessary to start a new panel, and Comic Chat 
has several rules for choosing when to do so. First, Comic Chat always begins a new panel whenever the 
balloon layout algorithm cannot successfully fit an additional balloon in the panel. A second rule is 
that Comic Chat always starts a new panel when­ever the additional input would result in too many characters 
in the current panel. We limit the number of characters per panel to five, since facial expressions are 
otherwise difficult to read. In the cur­rent implementation, Comic Chat will not draw more than one bal­loon 
for a character in a single panel, so multiple utterances from a given character is also reason to start 
a new panel. Third, we also start a new panel whenever rendering the new data in the current panel would 
result in a loss of information. For exam­ple, if a participant specifies two different facial expressions 
for his or her character, then these two expressions cannot be represented in the same panel. Finally, 
since we believe it desirable to have the system occasion­ally draw a single character in a panel, we 
also will break to a new panel with a given probability (15%) whenever the first utterance of the panel 
is longer than a few words.  6.2 Camera zoom If movies and television were always shot with the same 
camera pa­rameters, they would quickly become visually tedious, and though comics technically do not 
have cameras, it is still important to add variety by changing the scale at which characters and the 
back­  (a) (b) (c) Figure 6. Semantic elements. a) map; b) scene object; c) Greek Chorus. ground appear. 
Word balloons are unaffected by the virtual zoom factor. As in the movies, it is often important to have 
an establishing shot. Whenever a new person enters a chat room, that person sees a panel showing the 
immediate surroundings. From time to time (about ev­ery 15 panels), we remind participants of their surroundings 
by generating another establishing shot. Other times, we pull in as close as possible, with the following 
caveats. First, we will not zoom in so far as to cut off a character at the neck; it nearly always looks 
better to include part of the character s shoulders. Second, we will not let an important character (one 
that we decided earlier must be included in the panel) be cut by the sides of the panel. Third, it is 
also best not to cut characters off at the ankles. In such cases it is better to pull back a little, 
and show the characters in their entirety. Some comic artists also prefer not to cut off characters at 
their knees either, but this is far from a universal rule, and we currently allow it. Subject to the 
above restrictions, we pull in to the tightest shot pos­sible. Since the number of characters selected 
to be included in the panels varies, as discussed in Section 4.2, the scale chosen for the panels also 
varies, contributing to the resulting visual richness.  6.3 Semantic elements The pictures in human-drawn 
comics often reflect the words and stories that the comics tell. Since it is impossible given the current 
state of natural language technology to extract the semantics of ar­bitrary chat text, it is also impossible 
to automatically represent the meaning of this dialogue in pictures. However, things can still be done 
in composing the comics panels to reflect very simple seman­tic elements. In fact, we have already discussed 
how simple ges­tures and expressions can be selected automatically. Comic Chat reflects semantics in 
additional ways. There is a collection of topics that seem to come up repeatedly in chat sessions. For 
example, where people are from, what they do for a living, sports teams, pets, and kids are all very 
popular topics. For each of these common topics, we can define keywords to look for, which when found 
alter the composition of the scene. Figure 6 shows three different ways that we change the panel composition 
whenever Ohio is mentioned. In Figure 6a, the background is changed to a map of Ohio for the duration 
of a single panel. Comic artists often use such non-realistic, representational backgrounds [13]. A disadvantage 
to this approach is that it dramatically in­creases the amount of art necessary to run the application, 
but it is still reasonable if the user has a CD-ROM drive. A second ap­proach is shown in Figure 6b, 
where instead of changing the entire background, a new element is added to an existing background. Here 
we draw a banner for the soon-to-be-former-Cleveland Browns, which appropriately states, Go Browns! In 
Figure 6c, we introduce a little meta-character at the bottom of the panel that makes smart-allecky remarks. 
Our comic artist refers to him as the Greek Chorus . Here, in response to Ohio being mentioned, he asks 
What s round on the ends and hi in the mid­dle? This character s size and placement and its balloon position 
distinguish him from human participants. The beauty of the keyword approach is that it tends to work 
well, independent of the meaning of the entire sentence. For example, it is fine to show a map of Ohio 
whether a participant said I was born in Ohio, I ve never been to Ohio, or My plane got stuck in Ohio. 
A problem with this approach is that it requires a large effort to generate enough pictures and clever 
remarks to have reasonable coverage. 7. IMPLEMENTATION Comic Chat is implemented in C++ and runs on both 
the Microsoft Windows 95 and Windows NT operating systems. Currently Comic Chat uses the Internet Relay 
Chat (IRC) protocol [17], and it interoperates with the many IRC servers already on the Internet. Non-textual 
information, such as gesture and expression choice, is encoded as a small string at the beginning of 
each message. Users of existing textual chat programs can communicate with Comic Chat participants, except 
they receive a text-only view of the pro­ceedings, and are represented by a randomly selected character 
to Comic Chat participants. An early version of Comic Chat used Mi­crosoft Network Protocols, which allowed 
text to be transmitted in specially-marked text packets, and all other information to be trans­mitted 
in data packets. This enabled Comic Chat to make the non­textual information totally invisible to text 
clients. Unfortunately, IRC lacks this capability. Information communicated between clients includes 
indices of the specific bitmaps used, as well as the symbolic gestures and expres­sions. When a receiving 
client has the same character art used by the sender, it can simply consult the bitmap indices to render 
the ex­act pose chosen by the sender. However, if the receiver does not have the same character art, 
the symbolic gesture and expression information is applied to a different character that does reside 
on the system to yield a pose with the same intent. Comic Chat has an extensible rule set for inferring 
a pose from typed text. End users can define their own rules. These rules are only applied to the text 
that the sender types, and the resulting pose information is communicated to other clients. However, 
when a message is received from a text-only client, the rules are applied to that message as well to 
assign a reasonable gesture and pose to the text client s character. Each Comic Chat client makes its 
own panel composition deci­sions, which are not communicated from client to client. Hence, the participants 
in a single conversation can see different panels repre­senting the same communications. Although on 
the surface this may seem to be a poor choice, it actually makes panel display more flexible, because 
each participant sees a custom view of the conver­sation. Currently, there are several factors that affect 
this custom view. First, participants can customize their panel size, and since more conversation fits 
in larger panels, this affects panel composi­tion. Also, because Comic Chat displays an establishing 
shot for new participants, the panel appearance can differ according to when the participant joined the 
chat. In addition, individuals can whisper to one another, and these utterances appear only to those 
involved. Comic Chat works as a standalone application or in conjunction with Web browsers. People can 
place pointers to chat rooms on their Web pages, and clicking on such a pointer will automatically launch 
Comic Chat to the chat room. Because it was written as an ActiveX Document [20], Comic Chat appears as 
a dynamic Web page within the window frame of browsers that support this inter­face, such as Microsoft 
Internet Explorer 3.0. Performance of Comic Chat meets our original goal of requiring less than a second 
to compose and draw panels on a Pentium class machine. The background and character art are kept resident 
on each client machine, so that they need not be transmitted over the network. Artwork for a single character 
can vary in size according to the size and color depth of the bitmaps, as well as the number of poses 
provided. We have found 50 KBytes (compressed) to be suf­ficient for representing a character with ten 
to fifteen head and body drawings. New characters and backgrounds can be loaded dynami­cally. Although 
we focused much of our efforts on recreating a particular comics style, the system architecture was designed 
to allow new styles to be plugged in. Comic Chat has an object-oriented struc­ture, and includes classes 
for comics pages, panels, balloons, char­acters, character poses, and backgrounds. Elements of our artist 
s style are encapsulated in sub-classes of these. However, it is impor­tant to note that large amounts 
of effort went into defining these sub-classes, so reproducing additional artists styles will be easier 
in the future, yet far from trivial. 8. EXAMPLES This section shows three different types of chat interactions 
de­picted by Comic Chat. The first, Figure 7, is an ordinary chat ex­change between two of the authors. 
It contains six panels of comics, including a title panel. The title panel lists the most active partici­pants 
(both of them in this case), and this information is also avail­able interactively. All aspects of these 
panels, including character placement, gesture and expression choice, balloon construction and layout, 
panel breaks, and panel zoom, were chosen automatically by the system. The second example, Figure 8, 
shows one person s interactions with a bot. Bots are simple computer programs, often masquerad­ing as 
humans on chat channels. In some cases they perform useful functions, in others they exist to irritate 
other chat participants. One conversational bot is loosely modeled after the Eliza program, orig­inally 
written by Joseph Weizenbaum at MIT to mimic a human psychoanalyst [2]. By joining an Eliza room, chat 
users can receive free (and valueless) psychoanalytical help. Figure 8 was generated by a volunteer from 
a Cub Scout troop, getting a demonstration of Comic Chat. The Cub Scout appears on the left, Eliza appears 
on the right. We halted the demonstration after the eighth panel. People often use chat programs for 
role playing and interactive fic­tion. Figure 9 is an excerpt from such a gathering on the Internet. 
The participants were all using textual chat clients, and their dia­logue was rendered using Comic Chat. 
Note that the third panel of Figure 9 also includes a narration box, an additional comics ele­ment supported 
by our program. Narration boxes help to describe what is taking place in a panel. Balloons can be placed 
to the right and below the narration box, according to the algorithm presented earlier. IRC clients typically 
support a special kind of message to indicate what a participant is doing. Comic Chat looks for such 
messages, and places them in narration boxes. Other kinds of infor­mation could be placed in narration 
boxes as well, such as who is entering and leaving. The character wearing a tie in this figure is in 
charge of the role playing game, and is providing narration through dialogue that is rendered in balloons. 
Much of this could be placed in narration boxes to produce a different effect.  9. CONCLUSIONS AND FUTURE 
WORK The panels produced by the methods described in this paper are ac­ceptably composed according to 
comic conventions. Jim Woodring is very pleased with the results in general, and occasionally points 
out panels that (he claims) he would be proud to have drawn. How­ever, we feel that reproducing the creativity 
and variability of a hu­man comic artist, particularly one of Woodring s abilities, is in all practicality 
an unachievable goal. Nevertheless, people seem to take delight in the output of Comic Chat, and we look 
forward to performing user studies to compare Comic Chat to other graphical chat systems. There are still 
numerous additional features that we would like to add to the system. Professional comic book artists 
(in contrast to most comic strip artists) tend to show a great deal of imagination in how they lay out 
panels on the page. Currently, Comic Chat places square panels regularly on an infinitely long page. 
To create more variability, we would like to alter the size and shapes of our panels, and lay them out 
more creatively. This could either be done algo­rithmically, or by selection from a pool of template 
pages. Although we have distinct locations where characters can enter our comics world (including a room 
in a house, a pastoral scene, a bal­cony, a fantasy world, and a pond scene), we should have a mecha­nism 
to allow our characters to graphically transition from one scene to another. This could happen when a 
character decides to join a conversation elsewhere; however, the system itself, to add variety and unpredictability, 
might choose to move a conversation to a new location. We would also like to make the system capable 
of showing participants performing various activities, like having tea or swimming in a pond. This need 
not be under user control, and in fact putting it under user control might be a distraction to the pri­mary 
chatting activity. Having the system control such activities would add variety and unpredictability, 
and give the participants even more to chat about. One of the innovations of Comic Chat is that it provides 
a graphical transcript of an on-line conversation in which a person has partici­pated. It could also 
provide a transcript of a conversation that a per­son might want to join. In current chat systems, after 
entering a chat room, prospective participants wait to see if the conversation inter­ests them. This 
not only creates a distraction for the other partici­pants, but it is potentially a waste of time if 
the conversation turns out not to be of interest. Instead, we could provide a graphical tran­script of 
conversations for prospective participants to review imme­diately. We can take this idea further, and 
intersperse meanwhile panels during gaps in a Comic Chat conversation. These panels  Figure 7. A page 
from a session with Comic Chat. Figure 8. The first panels from a chat session with an Eliza bot.  Figure 
9. The beginning of a fantasy role playing quest. Participants are using text IRC clients, monitored 
by Comic Chat. would show snippets of conversations occurring elsewhere in the Comic Chat world (and 
be labeled MEANWHILE... , following another comics convention). Of course, there is a potential privacy 
issue here that needs consideration. Using more powerful technology to identify the semantics of the 
conversation would certainly allow us to generate better comics. Natural language processing systems 
would allow us to improve our choice of default expressions and gestures for the characters, and would 
allow Comic Chat to provide additional types of (and better targeted) semantic feedback beyond that described 
in Section 6.3. Many comic artists emphasize certain words in their balloons by setting them in bold 
type, and natural language technology could help select which words to make bold. Comics are used for 
a wide variety of purposes other than entertain­ment, and computer-generated comics can be applied to 
far more than chat rooms. For example, human-drawn comics have in­structed people about history [7] and 
the repair of military equip­ment [1]. Computer-generated comics could be used for education as well, 
but they could also serve as a tool in the production of inter­active, collaborative fiction, a visual 
presentation for MUDs and other virtual worlds, and a graphical history of interactions with so­cial 
or agent-based interfaces. The examples presented in Figures 8 and 9 hint at these possibilities. Automatically-generated 
comics could be used in computer-based help and instruction. They could also provide a graphical transcript 
or serve as the primary visual representation for adventure games. The leader of a large com­puter-supported 
cooperative work project tells us that he believes computer-generated comics could help people separated 
in space and time to collaborate on joint projects. Comics are a rich form of communication, with a visual 
vocabulary that most of us have already internalized. They can also be wonder­fully entertaining. We 
have found comics to be ideally suited as a visual representation for on-line chat rooms, but the potential 
appli­cation of comics to other computer-based applications is equally promising. The possibilities are 
vast, and we look forward to inves­tigating other ways of combining comics and computer graphics. A free 
copy of Comic Chat can be downloaded from our Web site, http://www.research.microsoft.com/comicchat.htm. 
 ACKNOWLEDGMENTS Foremost, we would like to thank Jim Woodring for sharing his knowledge of comics, 
and creating such wonderful art for this project. Rick Rashid has been a great advocate (and occasional 
hacker). Thanks to George Robertson, Maarten Van Dantzich, Ben Slivka, David Thiel, Dan Ling, Jim Kajiya, 
and Steve Drucker for helpful comments. Comic Chat is publicly available on the Internet, largely due 
to the efforts of Linda Stone, Ross M. Brown, Diana Murray, Eric George, Mark Mecham, Bob Tabscott, and 
Albert Tan. Interaction with members of Microsoft Research and Mi­crosoft s Virtual Worlds Group contributed 
to both the design and deployment of Comic Chat. REFERENCES 1. Anderson, M. Joe s Dope: If Ya Gotta 
Do It... Do It Right! PS: The Preventive Maintenance Monthly. Issue 279, February 1976. 29-36. 2. Barr, 
A. and Feigenbaum, E. A. The Handbook of Artificial Intelligence. volume 1. William Kaufmann, Inc., Los 
Altos, CA. 1981. 285-287. 3. Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn, B., Beckett, 
T., Douville, B., Prevost, S., and Stone, M. Auto­mated Conversation: Rule-based Generation of Spoken 
Expression, Gesture, and Spoken Intonation for Multiple Con­versational Agents. Proceedings of SIGGRAPH 
94 (Orlando,  FL, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conf. Series, ACM, New 
York. 413-420. 4. CD-ROMIX! Inc., FREEX #1. P. O. Box 2961, Torrance, CA 90509. 1993. 5. Eisner, W. 
Comics &#38; Sequential Art. Poorhouse Press. Tama­rac, FL. 1985. 6. Feiner, S. APEX: An Experiment 
in the Automated Creation of Pictorial Explanations. IEEE Computer Graphics and Applications, 5, 11. 
November 1985. 29-37. 7. Gornick, L. The Cartoon History of the Universe. Doubleday. New York. 1990. 
 8. Haeberli, P. Paint by Numbers: Abstract Image Representa­tions. Proceedings of SIGGRAPH 90 (Dallas, 
TX, Aug. 6­10). In Computer Graphics, 24, 4 (Aug. 1990), ACM, New York. 1990. 207-214. 9. Kurlander, 
D. and Feiner, S. A History of Editable Graphical Histories. In Watch What I Do: Programming by Demonstra­tion. 
Allen Cypher, ed. MIT Press, Cambridge, MA. 1993. 405-413. 10. Lent, J. Oh, What a Time It Was: The 
Early Days of the Fun­nies. Witty World: International Cartoon Magazine. No. 19. Summer/Autumn 1995. 
16-22. 11. Lent, J. Yellow Kid Celebrates Century with Hectic Sched­ule in 1995. Witty World: International 
Cartoon Magazine. No. 19. Summer/Autumn 1995. 18-19. 12. Mackinlay, J. Automating the Design of Graphical 
Presenta­tions of Relational Information. ACM Trans. on Graphics, 5, 2. April 1986. 110-141. 13. McCloud, 
S. Understanding Comics. Kitchen Sink Press. Northampton, MA. 1993. 14. Microsoft Corp., Microsoft Introduces 
V-Chat Communica­tions for MSN, The Microsoft Network. Nov. 30, 1995. Microsoft Press Release. Redmond, 
WA 98052. 15. Montgomery R. and Gilligan, S. Comic Creator. Spark Inter­active, 112 W. San Francisco 
St., Sante Fe, NM 87501, 1995. 16. Morningstar, C. and Farmer, F. R. The Lessons of Lucasfilm s Habitat. 
In Cyberspace: First Steps. Benedikt, M., ed. MIT Press, Cambridge, MA. 1991. 273-301. 17. Oikarinen, 
J. and Reed, D. Internet Relay Chat Protocol. Inter­net RFC #1459. May 1993. 18. Salisbury, M. P., Anderson, 
S. E., Barzel, R., and Salesin, D.H. Interactive Pen and Ink Illustration. Proceedings of SIG-GRAPH 
94 (Orlando, FL, July 24-29). In Computer Graphics Proceedings, Annual Conf. Series, ACM, New York. 1994. 
101-108. 19. Seligmann, D. D., and Feiner, S. Automated Generation of Intent-Based 3D Illustrations. 
Proceedings of SIGGRAPH 91 (Las Vegas, Nevada, July 28 - Aug. 2). In Computer Graphics, 25, 4 (July 1991) 
ACM, New York. 1991, 123-132. 20. Trupin, J. The Visual Programmer Puts ActiveX Documents Through Their 
Paces. Microsoft Systems Journal, 11, 6. June 1996. 55-76. 21. Winkenbach, G., and Salesin, D. Computer 
Generated Pen and Ink Illustration. Proceedings of SIGGRAPH 94 (Orlando, FL, July 24-29). In Computer 
Graphics Proceedings, Annual Conf. Series, ACM, New York. 1994, 91-100. 22. Worlds, Inc. Worlds Chat: 
Meet Your New Cyberfriends. http: //www.worlds.net/products/wchat/readme.html.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237261</article_id>
		<sort_key>237</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Reproducing color images as duotones]]></title>
		<page_from>237</page_from>
		<page_to>248</page_to>
		<doi_number>10.1145/237170.237261</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237261</url>
		<keywords>
			<kw><![CDATA[Neugebauer model]]></kw>
			<kw><![CDATA[color printing]]></kw>
			<kw><![CDATA[color reproduction]]></kw>
			<kw><![CDATA[duotone]]></kw>
			<kw><![CDATA[gamut mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P141715</person_id>
				<author_profile_id><![CDATA[81332521647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joanna]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Power]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P31946</person_id>
				<author_profile_id><![CDATA[81100513825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brad]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[West]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P78500</person_id>
				<author_profile_id><![CDATA[81100083423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Stollnitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[F. R. Clapper and J. A. C. Yule. The effect of multiple internal reflections on the densities of half-tone prints on paper. Journal of the Optical Society of America, 43(7):600-603, July 1953.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer Graphics: Principles and Practice. Addison- Wesley, Reading, MA, second edition, 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ronald S. Gentile and Jan P. Allebach. A comparison of techniques for color gamut mismatch compensation. InHuman Vision, Visual Processing, and Digital Display, volume 1077 of Proceedings of the SPIE, pages 342-354. SPIE, Bellingham, WA, 1989.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Andrew S. Glassner. Principles of Digital Image Synthesis, volume 1. Morgan Kaufmann, San Francisco, 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>200977</ref_obj_id>
				<ref_obj_pid>200972</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Andrew S. Glassner, Kenneth P. Fishkin, David H. Marimont, and Maureen C. Stone. Device-directed rendering. ACM Transactions on Graphics, 14(1):58-76, January 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ed Granger. Light Source Computer Images, Inc. Personal communication, December 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Arthur C. Hardy and F. L. Wurzburg, Jr. Color correction in color printing. Journal of the Optical Society of America, 38(4):300-307, April 1948.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Steven J. Harrington and R. Victor Klassen. Color printing having a highlight color image mapped from a full color image. U.S. Patent 5,237,517, August 17, 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ewald Hering. Outlines of a Theory of the Light Sense. Harvard University Press, Cambridge, MA, 1964.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R. W. G. Hunt. The Reproduction of Colour. Fountain Press, Kingstonupon-Thames, England, fifth edition, 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kansei Iwata and Gabriel Marcu. Computer simulation of printed colors on textile materials. InColor Hard Copy and Graphic Arts III, volume 2171 of Proceedings of the SPIE, pages 228-238. SPIE, Bellingham, WA, 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Henry R. Kang. Applications of color mixing models to electronic printing. Journal of Electronic Imaging, 3(3):276-287, July 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74345</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Bruce J. Lindbloom. Accurate color reproduction for computer graphics applications. In Proceedings of SIGGRAPH '89, pages 117-126. ACM, New York, 1989.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Gabriel Marcu and Satoshi Abe. Color designing and simulation in non-conventional printing process. In Applications of Digital Image Processing XVII, volume 2298 of Proccedings of the SPIE, pages 216- 223. SPIE, Bellingham, WA, 1994.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[H. E. J. Neugebauer. Die Theoretischen Grundlagen des Mehrfarbenedruckes (The theoretical foundation for multicolor printing).Z. Wiss Photogr., pages 73-89, 1937. Reprinted in Sayanai {20}, pages 194- 202.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Victor Ostromoukhov. Chromaticity gamut enhancement by heptatone multi-color printing. InDevice-Independent Color Imaging and Imaging Systems Integration, volume 1909 of Proceedings of the SPIE, pages 139-151. SPIE, Bellingham, WA, 1989.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218445</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Victor Ostromoukhov and Roger D. Hersch. Artistic screening. InProceedings of SIGGRAPH 95, pages 219-228. ACM, New York, 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>66503</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R. H. J. M. Otten and L. P. P. P. van Ginneken. The Annealing Algorithm. Kluwer Academic Publishers, Boston, 1989.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Warren Rhodes. Fifty years of the Neugebauer equations. In Sayanai {20}, pages 7-18.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Kazuo Sayanai, editor. Neugebauer Memorial Seminar on Color Reproduction, volume 1184 of Proceedings of the SPIE. SPIE, Bellingham, WA, 1990.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>48045</ref_obj_id>
				<ref_obj_pid>46165</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Maureen C. Stone, William B. Cowan, and John C. Beatty. Color gamut mapping and the printing of digital color images. ACM Transactions on Graphics, 7(4):249-292, October 1988.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Johannes von Kries. Chromatic adaptation. In David L. MacAdam, editor, Sources of Color Science, pages 109-119. MIT Press, Cambridge, MA, 1970.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[John A. C. Yule. Principles of Color Reproduction. Wiley, New York, 1967.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Reproducing Color Images as Duotones y Joanna L. PowerBrad S. West Eric J. Stollnitz David H. Salesin 
University of Washington Abstract Given: A color image C, a paper color, the set of all available printing 
inks , and a subset of k = 0,1,or2 IaI We investigate a new approach for reproducing color images. inks 
in chosen by the user. Rather than mapping the colors in an image onto the gamut of colors Find: The 
best 2 k additional inks for reproducing Cthat can be printed with cyan, magenta, yellow, and black inks, 
we on the selected paper, along with the appropriate color choose the set of printing inks for the particular 
image being repro­separations. duced. In this paper, we look at the special case of selecting inks for 
duotone printing, a relatively inexpensive process in which just two inks are used. Speci.cally, the 
system we describe takes an image as input, and allows a user to select 0, 1, or 2 inks. It then chooses 
the remaining ink or inks so as to reproduce the image as accurately as possible and produces the appropriate 
color separations automat­ically. CR Categories: I.3.4 [Computer Graphics]: Graphics Utilities Additional 
Keywords: color reproduction, color printing, duotone, gamut mapping, Neugebauer model 1 Introduction 
Modern color reproduction typically employs a .xed set ofprocess­color inks: cyan, magenta, yellow, and 
sometimes black. Placed on top of one another and in juxtaposition, these inks can be used to re­produce 
a range of colors, called theirgamut. Although the standard process colors were carefully chosen to provide 
a relatively large gamut, this gamut is nevertheless quite limited when compared to the full range of 
colors visible to the human eye. Thus, color .delity must generally be compromised when reproducing images 
with pro­cess colors (or any other small, .xed set of inks). A number of approaches have been suggested 
for mitigating this problem, including methods for smoothly mapping the original im­age colors to the 
process color gamut [21], and, in the case of three­dimensional computer-synthesized imagery, rede.ning 
the original object colors so that the rendered image will lie inside the process­color gamut [5]. However, 
to our knowledge, there has been no re­search to date on a very different but equally promising approach: 
allowing the inks used for printing to be selected for the particular image being reproduced. In this 
paper, we investigate a .rst step toward this goal: selecting inks for duotone color reproduction, a 
less expensive printing pro­cess in which just two inks are used. In particular, the problem we address 
can be stated formally as follows: y Department of Computer Science and Engineering, University of Washington, 
Box 352350, Seattle, WA 98195-2350 Permission to make digital or hard copies of part or all of this work 
or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to While duotones obviously have more limited color gamuts than three-and four-ink 
processes, they can nevertheless be used to re­produce a surprising range of color especially when the 
inks are chosen for the particular image being reproduced, as we show in this paper. Indeed, a duotone 
print is really a combination of four colors: the paper color alone, the colors of each of the inks individually, 
and the color of the two inks superimposed on each another. These four colors de.ne a bilinear surface 
in color space that may span a broad range of the full color gamut. Whereas duotone printing has traditionally 
been used almost ex­clusively to enhance monochrome gray-scale images with a tint of color, the kind 
of duotone color reproduction we explore in this pa­per has a variety of new applications. First, duotone 
printing is sig­ni.cantly less expensive than process color, typically about two­thirds the cost. Thus, 
there is a clear economic advantage in us­ing duotones for images that are adequately reproduced in this 
way. Second, the general form of the problem expressed above allows a user to select one or both of the 
two printing inks. This formula­tion is useful when other requirements of the page design, such as matching 
the precise colors of a company logo, already constrain the choice of inks. Also, because a large number 
of printing presses are two-color presses, printed documents are often designed for just two inks, generally 
black (for text) and one additional color. In these cases, we show how duotone separations can still 
be computed to match the colors of an image as well as possible with the inks avail­able. Finally, our 
formulation also allows duotone separations to be computed for a colored paper, which may be useful in 
a variety of situations. As one example, this form of the problem could be useful for creating duotone 
separations of full-color images for a yellow pages telephone directory. 1.1 Related work To our knowledge, 
there has been no previous work that proposes a general approach to reproducing images using duotones. 
The most closely related work in the .eld of computer graphics addresses the problem of gamut mapping, 
or smoothly mapping the colors of an original image to those available on an output device. Fundamen­tally, 
a duotone gamut is much more restricted than the gamut of a typical output device. Our duotone mapping 
differs from the ap­proaches taken by Stoneet al. [21] and Gentile and Allebach [3] pri­marily in that 
we require a mapping from three dimensions to two. Harrington et al. [8] describe a technique for creating 
a highlight color image, a specialized duotone in which one ink is black. There are a few articles in 
the optical engineering literature on cal­ republish, to post on servers, or to redistribute to lists, 
requires prior culating halftone separations for inks other than the standard four­specific permission 
and/or a fee. color process inks. Marcu and colleagues [11, 14] describe how to &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
compute separations when printing with an arbitrary number of inks. However, the inks and the order in 
which they will be printed must be speci.ed by the user. Furthermore, Marcu et al. do not specify how 
to handle a color that is out of gamut (the common case for duo­tone mapping). Ostromoukhov [16] describes 
how the traditional process-color printing gamut can be extended by introducing addi­tional basic colors. 
In the printing industry, a number of empirical studies have de­veloped improved models for halftone 
color reproduction [1, 7, 19]. Instead of complicating our duotone mapping with the correc­tion factors 
described in these works, we adjust the output of the model according to our own empirically determined 
correction fac­tor. Kang [12] takes a similar approach, though he assumes that each ink can be corrected 
independently, while we adjust the duo­tone gamut as a whole.  1.2 Overview of paper The remainder of 
the paper is organized as follows: Section 2 pro­vides relevant background information on color, color 
printing, and the Neugebauer model of halftone printing. Section 3 describes the duotone mapping in detail, 
and Section 4 discusses the ink-selection process. Section 5 describes some variations on duotone printing 
that are easily accommodated by our algorithm. Section 6 presents some actual duotones as well as some 
rough timing results. We con­clude in Section 7 with a discussion of possible directions for future work. 
 2 Background Before we dive into the details of our duotone mapping, we present some background material 
on color and color spaces, color halftone printing, and the Neugebauer halftone model. 2.1 Color and 
color spaces Color is determined by the intensity of light in the range of visible wavelengths from about 
400nm to 700nm. According to the tris­timulus theory of color perception [9], all colors can be reproduced 
using combinations of three primary wavelengths (roughly corre­sponding to red, green, and blue). Thus, 
color can be expressed as a function of wavelength, known as a spectral re.ectance, orasa three-dimensional 
quantity. More information about color can be found in standard graphics texts [2, 4]. The XYZ color 
space was developed in 1931 by the Commission In­ternationale de l ´ Eclairage (CIE) to standardize color 
speci.cation. The XYZ color space is additive, meaning that the color resulting from the superposition 
of two colored light sources can be calcu­lated by simply adding the coef.cients of the two known colors. 
A spectral re.ectance can be converted to XYZ coordinates by inte­grating the spectral information against 
three functionsx, y and z [2, pages 579 580]. The computer graphics community is more famil­iar with 
the RGB color space, an additive color space that is device­dependent. Conversion between RGB and XYZ 
coordinates can be accomplished by a linear transform if theXYZ coordinates of the de­vice s red, green 
and blue primaries are known [2, pages 585 587]. In contrast to additive color spaces, perceptually uniform 
color spaces allow the difference between two colors (as perceived by the human eye) to be measured as 
the distance between points. For ex­ample, two colors c1 and c2 separated by some distance d in a per­ceptually 
uniform color space appear about as different as two other colors c3 and c4 separated by the same distance 
d. The CIE devel­ rrrrrr oped two perceptually uniform spaces: Laband Luv. Both color spaces require 
the de.nition of areference white, which is usu­ally taken to be a standard light source de.ned by the 
CIE. In both 2 spaces, Lindicates brightness and has a value of 100 for reference rrrrrrr white. Though 
neither Labnor Luvis perfectly perceptually uniform, both come close to satisfying the condition that 
colors sep­arated by the same distance appear equally similar [4, pages 59 66].  2.2 Color halftone 
printing In color halftone printing, a continuous-tone image is reproduced by printing a number of versions 
of the image atop one another. Each version, known as a halftone separation, consists of various sized 
dots of a single ink. Color halftone printing differs from color dithering on monitors in that subtractive 
effects as well as additive effects play a role. The subtractive effect of superimposing dots of different 
color produces the set ofprinting primaries for a particular set of inks. For example, for cyan, magenta, 
and yellow ink printed on white paper, the set of printing primaries is cyan, magenta, yel­low, blue 
(cyan + magenta), green (cyan + yellow), red (magenta + yellow), black (cyan + magenta + yellow), and 
white (no ink). The additive effect of juxtaposing dots of different sizes produces the en­tire set, 
or gamut, of colors that can be achieved by printing halftone separations using a particular set of inks. 
Figure 1 illustrates the re­production of a color image using cyan, magenta, yellow, and black inks. 
More information on color reproduction can be found in the classic texts by Hunt [10] and Yule [23]. 
 2.3 Neugebauer halftone model In 1937, Neugebauer developed a series of equations that, given ink and 
paper colors, describe the amount of each ink needed to repro­duce a given color [15]. Intuitively, the 
model says that the overall color of a small area is a weighted average of the printing primaries, with 
each primary weighted by the relative area it covers. For ex­ample, in a square printed with cyan, magenta 
and yellow ink on white paper, the contribution of blue is given by the fraction of the square that is 
covered by cyan and magenta but not yellow. If1, 2, and 3 are the amounts of cyan, magenta and yellow 
ink printed,XXXX a XX then the contribution of blue is 12(13). The Neugebauer equations express colors 
in terms of their coordi­nates in the XYZ color space. The model was originally designed to describe 
three-color printing, though it can be generalized to handle any number of inks. Let g0 be the color 
of the paper, gi the color of ink i on the paper, gi,j the color of inks i and j superimposed on the 
paper, gi,j,k the color of all three inks i, j and k superimposed on the paper, and i the amount of ink 
i (between 0 and 1). For three inks, X the Neugebauer model describes c, a color in the printing gamut, 
in terms of the eight printing primaries and the amounts of the three inks required to achieve c:  
 ma2 h h h h h h h 3 (11)(1 2)(1 3) 1(1 2)(13) (11)2(1 3) (1 1)(12)3 c = g0 g1 g2 g3 g1,2 g1,3 g2,3 
g1,2,3 12(1 3)1(1 2)3 4 h 7 (1 1)23 6 h 5 123 3 Duotone mapping We are now prepared to tackle the 
following problem: Given 0, 1, or 2 inks out of a set of available inks, choose the remaining 0, 1, or 
2 inks and compute the separations that most accurately reproduce a given image. In this section we present 
the duotone mapping, our technique for computing the separations for an image once the two inks are known. 
In Section 4 we will describe the process of choos­ing inks. Figure 1 Color halftoning of an image. 
 3.1 Neugebauer model for duotone printing For two-color printing, the Neugebauer model describes c, 
a color in the printing gamut, in terms of the four printing primaries and the amounts of the two inks: 
(1 1)(1 2) m a246 a XX a X a X X a XX 357 1(1 2) c = g0 g1 g2 g1,2(1 2 1 2 The gamut described by the 
above equation is a bilinear surface in an additive color space such as XYZ or RGB, as shown in Figure 
2. Finding the duotone gamut The duotone gamut is fully speci.ed by the Neugebauer model given the printing 
primaries g0, g1, g2 and g1,2. We obtained spec­tral re.ectance data for inks and papers using a ColortronTM 
spec­trophotometer. The ColortronTM software also provides data for . 1)PANTONER ink sets. In order to 
.nd the printing primaries, we must estimate the color of inks printed on the selected paper. We must 
also estimate the color of two inks superimposed on the se­lected paper. We used a simple model to approximate 
the effect of printing ink on paper. The data for a layer of ink or paper can be expressed in two parts: 
an overall re.ectance spectrum R and a Fresnel re.ectance spectrum F. The overall re.ectance spectrum 
indicates how much light of each wavelength is re.ected back by the layer and can be directly measured 
with a spectrophotometer. The Fresnel informa­tion indicates how much light of each wavelength bounces 
off the surface without entering the layer. Inks act very much like .lters, which are purely subtractive 
layers. A paper acts like an opaque layer. This simple behavior allows us to approximate layer composition 
by multiplying re.ectance spec­tra and adjusting for Fresnel effects [6]. To approximate the result of 
superimposing a subtractive layeri on another layer j, we remove the Fresnel component from each layer 
s overall re.ectance, multi­ply the altered spectra, and add back the Fresnel re.ectance of the top layer: 
aa Ri,j =(Ri Fi)(Rj Fj)+ Fi Fi,j = Fi Since we cannot directly measure the Fresnel component of re­.ectance, 
we approximate the Fresnel spectrum of an ink by the re­.ectance spectrum of black ink. The intuition 
behind this approx­imation is that black absorbs nearly all incident light; thus, any re­.ectance from 
a layer of black ink is primarily due to a Fresnel effect 3 g0 XX g2 g1,2 Figure 2 Bilinear surface 
described by the Neugebauer halftone model. at the surface. For simplicity, we use the same Fresnel 
spectrum for paper, and we ignore any Fresnel effect between ink and paper. The layering model described 
above does not completely solve our problem, as the measured spectral re.ectance data for an ink in­cludes 
information about the stock paper on which the ink was orig­inally printed. To compute the result of 
removing one layer from another, we simply invert the above equations: Fi = Fi,j (Ri,j Fi,j) Ri =+ Fi 
a (Rj Fj) Division by zero does not arise in practice, as we only remove inks from papers, and the 
papers have high overall re.ectance spectra and low Fresnel components. Once we have the spectral re.ectance 
information for the paper, the individual inks on paper, and the com­bination of both inks on paper, 
we can .nd the printing primariesg0, g1, g2, and g1,2 by .rst converting the appropriate spectral quantities 
to XYZ or RGB coordinates [2, pages 579 587]. 3.2 Goals of duotone mapping The core of our algorithm 
is the mapping that transforms image col­ors onto the duotone gamut. There are many characteristics of 
col­ors and of sets of colors that one could seek to preserve in design­ing a mapping. The fundamental 
tradeoff is between mapping colors exactly and maintaining overall relationships between colors. Our 
approach places more importance on overall relationships than on exact matches, and weighs certain relationships 
more heavily than others. The basic idea behind our mapping technique is to de.ne an orthog­onal axis 
system for each duotone gamut, then transform the im­age colors along two directions and use parallel 
projection along the third. The choice of directions is clearly critical to the effectiveness of the 
mapping and corresponds to preserving certain relationships between image colors at the expense of other 
relationships. Here are the choices we made: 1. Preserve relative luminance. The eye is more sensitive 
to light at some wavelengths than others. The luminance-ef.ciency curve, which describes the relationship 
between wavelength and vi­sual sensitivity, corresponds to a direction in a three-dimensional color space. 
Preserving separation of image colors along this di­rection of greatest sensitivity, called the luminance 
direction, is the primary goal of our duotone mapping. TheY axis of the XYZ color space corresponds exactly 
to the luminance direction [2], and therefore we take Y to be the direction of our .rst transfor­mation. 
(Stone et al. also used preservation of relative luminance in their gamut mapping work [21].) 2. Preserve 
ink-spread separation. The second relationship our  3.3.1 Calculating the luminance transformation g0 
 The .rst transformation applied to the image colors is a mapping along Y to bring all colors within 
the luminance range of the current YP duotone gamut. Although this mapping could be any monotonically 
increasing function, for the sake of simplicity we use a linear func- S g2 g1 a g1,2 Figure 3 Axes associated 
with a typical duotone gamut. mapping preserves is separation in the direction of most color variation 
on the duotone gamut. The curve on the gamut between the two individual ink primaries describes the widest 
variation in color achievable with the selected paper and inks. Thus, the vec­tor g2 g1 is the ideal 
direction for the second transformation. However, our duotone mapping requires an orthogonal axis sys­tem, 
and we have already chosen the Y axis. Therefore we or­thogonalize g2 g1 with respect to Y and use the 
resulting ink­ a spread direction S as the direction of the second transformation. 3. Sacri.ce normal 
separation. Separation must be sacri.ced in some direction in order to map points from three-dimensional 
space onto a surface. As we have chosen two axes and require mutual orthogonality, the projection direction 
P is already de­.ned by YS. The P axis approximates the average normal of o the bilinear surface, which 
is the direction of least color variation on the gamut. It is therefore a good direction in which to 
sacri.ce separation. Given a duotone gamut, the constraints listed above completely de­.ne the orthogonal 
axis system we will use for our duotone map­ping. A typical example is illustrated in Figure 3.  3.3 
Computing the duotone mapping The duotone mapping takes image colors c1,, cn and maps them to colors 
c1,, cn on the duotone gamut. The mapping of image color ci takes place in three steps, with each step 
affecting a differ­ent orthogonal component of ci. We will denote the components as follows: cYi ci Y 
cSi ci S cPi ci P   tion. If we let cYi be the y-value of the original image color ci, and cYi the 
y-value of the transformed color, the luminance transformation can be written as follows: ymin = min 
ciY i ymax = max cYi i YYYY y min = max ymin, min g0, g1, g2, g1,2 YYYY  f c g aaf agggg y = minymax, 
maxg0, g1, g2, g max 1,2 Y Y ciymin y c i = y min + max y min ymax ymin Figure 4 shows a set of image 
colors before and after their luminance values have been transformed to lie within the range of luminance 
values available in a typical duotone gamut. 3.3.2 Calculating the ink-spread transformation The second 
transformation is along the ink-spread axis S. This transformation depends upon luminance, since at some 
luminance values the duotone gamut is wide and at others it consists of a sin­gle point. Consider a particular 
luminance value y between ymin and ymax. We de.ne s0 and s1 to be the s-values of the points found by 
intersecting the edges of the duotone gamut with a plane of constant luminance y. The ink-spread transformation 
at lumi­nance value y brings the s-values of all image colors with luminance value y into the range [ 
s0, s1]. Figure 5 illustrates the effect of an ink-spread transformation on a set of image colors. In 
our current implementation, the non-uniformity along Y is han­dled by separating colors into bins according 
to y-value and calcu­lating a different transformation for each bin. The coherence among colors in most 
natural images prevents this discrete approach from introducing noticeable discontinuities into a duotone. 
The ink-spread transformation, like the luminance transformation, can be any monotonically increasing 
function that maps thes-values of image colors to the s-values available on the gamut. In what fol­lows, 
we present both a simple linear mapping and a more complex mapping based on B´ezier curves. Linear mapping 
We will use the same superscript notation to indicate the orthogonal components of the printing primaries 
g0, g1, g2 and g1,2. Y P S Figure 4 Image colors before and after a uniform transformation in the luminance 
direction Y. The simplest transformation of s-values is a linear mapping, which we can de.ne in a manner 
similar to the luminance transformation. If we consider only colors within a particular luminance bin, 
the lin- P Y S Figure 5 Image colors before and after a non-uniform transforma­tion in the ink-spread 
direction S. B1 B1 10s max s max B1 2 B1 S c 3 i B0 3 S c i B0 2s min s min B0 B0 01 smin Ssmax smin 
Ssmax cc ii (a) (b) Figure 6 Two possible ink-spread transformations: (a) a linear mapping; (b) a B´ezier 
curve mapping. ear mapping is given by: smin = min ciS i smax = max cSi i s min = max smin, min s0, 
s1  f c g af gggga s max = minsmax, maxs 0, s1 S S smin smin) c i = smin + ci ( smax smax smin a An 
example of a linear ink-spread transformation for a single lumi­nance bin is illustrated in Figure 6(a). 
B´ezier-curve mapping The linear mapping given above does not attempt to preserve thes­values of image 
colors, despite the fact that many such values may be available in the duotone gamut. Instead, the linear 
mapping pre­serves the relationships betweens-values of the image colors. At the other extreme, we could 
map each s-value to the closest value in the interval of available values [ s0, s1], thereby accurately 
reproducing some colors while clamping others to the interval endpoints. We are interested in obtaining 
some of the bene.ts of both these alternatives. As a compromise, we construct a mapping based on B´ezier 
curves, as illustrated in Figure 6(b) for a typical luminance bin. In the interest of brevity, we will 
only describe the B´ezier-curve mapping qualitatively here. As with the linear mapping, the two endpoints 
of the curve are constrained to map smin to smin and smax to smax. However, because we are using two 
cubic B´ezier curves that meet with C1 continuity, we have .ve additional control points with which we 
can alter the behavior of the mapping. These additional degrees of freedom are utilized to meet the constraints 
that follow. First, the slope of the B´ezier-curve mapping at either end (controlled by B01 and B11) 
is constrained to be zero, while the tangent at the mid­dle of the curve (controlled by the segment B12 
B20) is constrained a P Y S Figure 7 Image colors before and after being projected onto the duo­tone 
gamut in the direction P. to be parallel to the line from B01 to B11. Together, these constraints impose 
a nonlinearity on the mapping that tends to preserve separa­tion of s-values for colors that are in gamut 
while squeezing together the colors that are out of gamut. This approach strikes a balance be­tween a 
linear mapping and an approach that clamps values. Second, the center of the curve is constrained to 
pass through the point B03 = B31, which is chosen to lie on the line s = s so as to guar­antee that s-values 
near the center of the mapping are preserved. We determine the precise location of this center constraint 
by intersect­ing the line from g0 to g1,2 with a plane of constant luminance y. Asa result, the constraint 
at the center of the B´ezier curves prevents col­ors that are closer in hue to one ink from mapping to 
a color on the opposite side of the gamut. Our implementation of the B´ezier-curve mapping relies on 
a lookup table in order to ef.ciently transform a value cSi to a new value cSi . The table corresponding 
to each luminance bin is calculated before any colors are transformed. We evaluate a curve at a large 
number of parameter values, and store the resulting (s, s) coordinates in a table. Then, to map a value 
ciS we need only .nd the correct interval of s-values and use linear interpolation to approximate the 
result cSi . 3.3.3 Calculating the projection Image colors transformed by the luminance and ink-spread 
map­pings above are guaranteed to project onto the gamut in theP direc­tion. The projection point corresponding 
to a color can be calculated analytically by solving for the intersection of a line and the bilinear 
surface de.ned by the Neugebauer equation. In the appendix, we derive a solution for 1 and 2, the amounts 
of the two inks required XX to produce the projected color. Thus, in addition to computing the desired 
color, the projection step calculates the halftone separations required for the duotone printing process. 
Figure 7 illustrates the result of projecting a set of image colors onto a duotone gamut. 4 Selecting 
inks The duotone mapping described in the previous section assumed that two inks were given. Now we consider 
more general cases in which neither ink or just one ink is speci.ed by the user. Our goal is to .nd a 
few good pairs of inks for a given image, in addition to .nd­ing the halftone separations. We de.ne a 
good pair of inks as one for which the duotone mapping algorithm produces an image that is as close as 
possible to the original full-color image. We de.ne the closeness of two images as the pixel-wise L2 
distance measured in a perceptually uniform color space. Thus, the selection of one or two inks amounts 
to an optimization problem whose goal (or objective) function is a function of the original image and 
two inks. Simulated annealing is a heuristic optimization technique designed to avoid local minima [18]. 
Transitions from the current state to an­other state are generated randomly and accepted with some proba-close 
to black. Instead, the .rst two inks can be chosen to reproduce bility. This acceptance probability depends 
both on the relative the image s hues, while the black ink permits .ne gradations in lu­scores of the 
two states, as rated by the objective function, and on minance. the value of a control parameter T. Unfavorable 
moves are likely when T is high but are accepted with decreasing probability as T decreases. The cooling 
schedule describes the rate at whichT de­creases and the number of moves made at each value of T. In 
our problem, a state consists of a pair of inks, each of which has a set of neighbor inks. Taken together, 
the neighbor sets describe a fully connected, symmetric graph. The set of legal moves from a state is 
the set of all possible combinations of neighbor moves for each ink. Simulated annealing is an ideal 
optimization technique for our par­ticular problem. We require a heuristic technique because most in­ 
. teresting ink sets, such as the PANTONER inks, contain hundreds of inks. In addition, our preliminary 
experiments indicated that our objective function has many local minima. Finally, the large num­ber of 
parameters associated with simulated annealing allow us to tune the optimizer to our needs. In particular, 
by adjusting the cool­ing schedule and starting conditions, we can cause the optimizer to .nd several 
relatively deep local minima instead of a single global minimum. This adjustment is useful because it 
allows us to present several alternative choices of ink pairs to the user. Evaluating the objective function 
tends to be costly because it re­quires that we transform every pixel color in the original image and 
compare the resulting color to its original value. In order to solve the optimization problem ef.ciently, 
the annealer uses a low-resolution version of the color information present in the original image. Such 
low-resolution information can be obtained either by quantizing the colors in the original image or by 
reducing the size of the original image. The low-resolution information is provided to the duotone mapping 
step instead of the original image colors. 5 Variations There are several variations on the basic algorithm 
that lead to im­provements in the duotones of certain images. 5.1 Outlying clusters In our implementation, 
clustering of image colors is performed as a preprocessing step. Clusters of small size and large average 
distance from other clusters are marked as outlying clusters. Colors that have been identi.ed as members 
of outlying clusters are ignored when calculating the ink-spread transformation. The rationale is that 
such colors matter less to the overall image, and that mapping them well at the expense of more prevalent 
colors is not justi.ed. Colors that are ignored in calculating the ink-spread transformation may not 
project onto the gamut. We transform each such color to a gray of the appropriate luminance, and then 
project that gray onto the gamut. 5.2 Black enhancement Full-color printing relies on three colored inks 
cyan, magenta, and yellow that combine to make black. However, a black halftone separation is usually 
printed in addition to the three color separa­tions. Adding a black separation permits denser blacks 
than three colors can produce, allows more detail to be expressed in shadowed areas, substitutes inexpensive 
black ink for more expensive colored inks, and avoids a thick accumulation of ink [23, page 282]. Adding 
a black separation to duotone printing offers a further ben­e.t: for images with a wide range of luminance 
values, the two col­ored inks no longer need to be chosen so that their combination is Extending the 
duotone process to support a black separation is sim­pler than the general problem of using three arbitrary 
inks (called a tritone process), thanks to the nearly accurate assumption that com­posing black ink with 
any other ink results in black. This assump­tion simpli.es the problem in two ways. First, whereas a 
general tri­tone gamut is a trilinear volume according to the Neugebauer equa­tions, a black-enhanced 
duotone is a volume bounded by four trian­gles, (g0, g1, black), (g0, g2, black), (g1, g1,2, black), 
(g2, g1,2, black), and one bilinear surface, as illustrated in Figure 8. Second, black enhancement can 
be implemented as a simple extension to the duo­tone algorithm using the following steps: 1. Subtract 
some amount of black from each color in the original image. 2. Apply the optimization and duotone mapping 
algorithm to pro­duce two color separations. 3. Calculate a black separation that adds back the appropriate 
amount of black to each color.  These three steps are illustrated in Figure 9. Removing black from an 
image in order to create a better duotone is similar to the four-color process of under-color removal 
(UCR), though slightly more complicated. Applying UCR to a color is fairly simple: a fraction of the 
minimum component of cyan, magenta, and yellow is removed from all three components, and then replaced 
by an equal amount of black [13]. The success of this technique hinges on the fact that cyan, magenta, 
and yellow combine to black, a prop­erty that is not necessarily true for the two inks used in a duotone. 
For duotones, removal of black from anRGB color c is performed by moving c in a straight line away from 
the RGB position of black ink 0 to a new position c, as shown in Figure 10(a). The amount of shift depends 
on the color s saturation: a fully saturated color does not bene.t from using any black, while a fully 
desaturated (gray) color bene.ts most from using pure black ink. Performing this operation on every color 
in the original image yields a new target image for the duotone mapping. Once we have applied the duotone 
mapping algorithm to obtain new colors, we can compute how much black to use by comparing the duotone 
colors to the original colors. Because we are primarily con­cerned with preserving the luminance of image 
colors, our goal is to match the luminance of the corresponding color in the original im­age. We cannot 
reproduce the complete range of original luminance values, though; instead we attempt to match a luminance 
scaled to lie between that of the paper and that of the black ink. For each color c in our duotone, we 
create a line segment from that color to the position of black ink. We then shift c toward black until 
it 0 achieves the desired luminance at some position c, as illustrated in g0 g2 g1,2 Figure 8 The gamut 
of two inks and black. Figure 9 Steps of black enhancement (counterclockwise from top left): original, 
original with black removed, duotone without black, and duotone with black replaced. Figure 10(b). The 
amount of shifting along the line through black determines the amount of black to print. 5.3 Minimized 
hue difference For some very colorful images, even the best duotone will not be able to adequately reproduce 
all colors. Mapping an unachievable original color to gray may produce a more pleasing duotone than mapping 
it to another color. To achieve this effect we introduced the idea of an optional second pass over the 
duotone, performed af­ter the desired pair of inks is selected. During the second pass, we perform a 
pixel-by-pixel comparison of the duotone and the original image, calculating the perceptual hue difference 
of each pixel. Hues are measured as hue angles in a perceptually uniform color space. rrrrrrr In both 
Laband Luv, a plane perpendicular to the Laxis and passing through a particular brightness value contains 
a disk of all the hues present at that brightness, ranging from gray at the center to saturated colors 
on the edge. Thus the hue angle of a color is de.ned .r . r.r . r as huv = tan1(vu) or hab = tan1(ba) 
[4, page 65]. The per­ceptual hue difference between two colors is the difference between the hue angles 
of the colors. Our algorithm for reducing hue differences proceeds by desaturating (moving toward gray) 
each pixel in the original image by an amount parameterized by the magnitude of the hue difference computed 
for that pixel. The duotone mapping is then applied to the grayed-out image. In addition to reducing 
offensive hue mappings, graying-out parts of the original may improve the ink-spread transformation by 
bringing smin and smax closer to smin and smax. Effectively, minimizing hue difference treats some potentially 
signi.cant image colors like outliers, sacri.cing them so that other colors will be mapped better. 0 
black cc (a) 0 black c c 5.4 Colored paper The paper used in a duotone need not be white. For an input 
image without much white, using a colored paper can greatly improve a duotone reproduction by providing 
an additional color essentially for free. Our implementation allows the user to specify a paper color 
or leave the choice of paper up to the optimizer. We modi.ed the simulated annealing algorithm to select 
a paper color by extending the de.nition of a state to include two inks and a paper. It is important 
to note that the colors in a duotone surrounded by an expanse of colored paper (as might occur in a yellow 
pages di­rectory) appear quite different from the colors in the same duotone printed to cover all of 
the paper (as in a postcard). The difference is caused by adaptation effects in the visual system, which 
allow a sub­tle color to be perceived as white when nothing perceptually closer to white is present [22]. 
We are currently investigating quantitative models of adaptation that might improve a duotone surrounded 
by an expanse of colored paper. 6 Results We have implemented an application that allows a user to create 
duotone separations. In this section we discuss some details of our implementation, as well as the results 
we obtained by printing duo­tones on a two-color press. 6.1 Performance Our na¨ive implementation of 
the optimization loop takes approxi­mately .ve minutes on a 133 MHz SGI Indy to .nd three good ink pairs, 
using the clustered version of an image and an ink set of more than 400 inks. While the optimization 
process is not interactive, it only needs to be run once for a given image. For a 300300 image, o producing 
full-size color separations and a preview duotone takes approximately four seconds on the same machine. 
6.2 Printing (b) Testing our method requires that we print duotones on an actual Figure 10 Transformations 
of black enhancement: (a) reducing printing press and compare the results to reference prints. While 
black of a color in the original image by moving away from black; ordinarily a reproduction should look 
as much as possible like a (b) replacing black in the duotone by moving toward black. photograph or an 
image displayed on a monitor, for the purposes of this paper we chose to compare duotone prints to four-color 
pro­cess prints. As a consequence, when we provide an input image to our duotone mapping algorithm, its 
colors should be those that are printed by a four-color press. All of our original images are stored 
asRGB colors; in order to trans­form those colors to match the colors obtained from a four-color printing 
process, we use an RGB correction function. We gener­ated data for the RGB correction by printing (on 
a four-color press) a square for each color in a regular three-dimensional grid of col­ors spanning the 
RGB cube. We scanned each printed square us­ing a spectrophotometer and then converted the spectral data 
to an RGB color. For this set of colors, we constructed a mapping from in­tended RGB values to actual 
printed RGB values. Using piecewise­trilinear interpolation between these discrete colors, we can trans­form 
any given RGB color to the corresponding RGB color that pro­cess printing would produce. There is a second 
aspect of our color printing experiments that also requires color correction. The Neugebauer model is 
not entirely ac­curate in its prediction of colors obtained by halftoning two inks. Therefore, in keeping 
with the empirical spirit of printing, we de­veloped an empirical correction method for adjusting duotone 
sep­arations. This duotone correction is similar to the RGB correction, but we need to measure data for 
only two dimensions. Using the two selected inks, we print a regular grid of values for 1 and 2. Once 
XX again, we measure the RGB color of each square in the grid. Lo­cating each measured RGB color on the 
duotone gamut associates intended percentages with printed percentages of inks. Using bilin­ear interpolation, 
we construct a mapping from intended amounts to actual printed amounts. Applying the inverse of this 
function to a pair of separations before printing compensates for the simplicity of the Neugebauer model. 
It should be noted that while the RGB transformation is valid for all images, the duotone correction 
is only accurate for the particular pair of inks that were measured. Since one of our goals is to make 
printing duotones inexpensive and easy, we do not want to require two trips to the printer. We therefore 
calculated an average correc­tion function based on the duotone correction functions for three dif­ferent 
pairs of inks. This average correction was used for most of the results described below. 6.3 Examples 
Example 1 depicts a painting by C´ezanne printed with process inks and as three duotones. The .rst two 
duotones, both printed with black and gold, demonstrate the difference between the traditional approach 
to printing duotones and our approach. (The traditional duotone was created using Adobe PhotoshopTM.) 
The last image in the series, printed with black and two inks selected by optimization, demonstrates 
black enhancement. Some very different images reproduce well using a single pair of inks. Conversely, 
certain images reproduce well using several re­markably different pairs of inks. Example 3 shows a photograph 
of a koala printed in process color and as a duotone. Example 4 depicts a painting by Renoir in process 
color and as two duotones. The .rst duotone of the Renoir painting uses the same pair of user-selected 
inks as the duotone of the koala photograph. Choosing an unrelated pair of inks for the second duotone 
of the Renoir painting results in a very different, but still successful reproduction. Though skin tones 
are notoriously dif.cult to reproduce, pho­tographs of people are not beyond the scope of two-color reproduc­tion. 
Examples 4, 5, and 6 show three portrait photographs printed in process color and as duotones. All three 
duotones are printed with blue and orange-brown, inks that were selected by the optimizer for the photograph 
in Example 4. Example 7 shows a photograph of a sunset in process color and as a black-enhanced duotone 
printed with hand-picked inks. Viewed on a monitor, the original image contains colors not present in 
the pro­cess gamut. These colors are better achieved in a duotone that has not undergone the correction 
described in the previous section. Us­ing the same inks, we printed an uncorrected black-enhanced duo­tone 
as the .nal image in the series. Example 8 shows a painting by Schiele printed in process color and as 
a duotone on yellow paper with optimizer-selected inks. Using colored paper allows us to reproduce the 
three dominant hues (yel­low, red, and green) present in the original image. 7 Discussion and future 
work We have presented an algorithm for reproducing color images with duotones. Our hope is that our 
approach will be of interest to the desktop publishing community as a high-quality, economical alter­native 
to full-color printing. While the limited number of examples we have sent to a printing press may not 
be perfect reproductions of the original images, they are certainly vast improvements over tra­ditional 
duotones. The core of our method is the duotone mapping: the transformation of a set of scattered image 
colors onto a surface in three-dimensional color space. Our mapping preserves relationships between image 
colors at the expense of matching exact colors. One can imagine many other mappings; below are some of 
the dif.culties we found with two alternate approaches: . Finding the closest color. The obvious problem 
with the ap­proach of mapping an image color to the closest point on the duo­tone gamut is that out-of-gamut 
image colors will be clamped. While clamping may be acceptable for certain images and duo­tone gamuts, 
in general clamping results in arti.cial discontinu­ities and a loss of information. . Using orthogonal 
projection. Using orthogonal rather than par­allel projection would complicate the ink-spread transformation, 
making it a function of three variables rather than two: s = f (y, s, p). A more fundamental dif.culty 
with orthogonal projec­tion is the absence of an obvious continuous mapping, since there are colors for 
which there is no orthogonal projection onto the gamut. Our research suggests several intriguing directions 
for future work. Some of the methods presented in this paper might apply to the more general problem 
of .nding the best n inks with which to reproduce an image. Our extension of the optimization algorithm 
to choose a paper as well as inks suggests that we should adjust our algorithm to take advantage of psychophysical 
effects such as von Kries adapta­tion [22] and simultaneous contrast. These effects may allow us to effectively 
expand the printing gamut by tricking the eye into seeing colors that are not actually achievable. Another 
possible extension to our implementation is a system that facilitates the production of two-color brochures 
by optimizing over images, inks, and papers. The system would take several images as input and choose 
two inks, one paper, and a speci.ed number of the images that would reproduce well with the selected 
inks and paper. Our algorithm has no sense of what parts of images are semanti­cally or aesthetically 
important. Because the creation of a duotone from an image frequently requires loss of color information, 
our al­gorithm would bene.t from user input indicating which colors in the original image are most important 
to preserve. Finally, we are con­sidering combining our duotone mapping algorithm with the artistic screening 
approach to halftoning presented by Ostromoukhov and Hersch [17]. Acknowledgments We wish to thank the 
University of Washington Publications Ser­vices for assisting us with our printing experiments. This 
work was supported by an Alfred P. Sloan Research Fellowship (BR­3495), an NSF Presidential Faculty Fellow 
award (CCR-9553199), an ONR Young Investigator award (N00014-95-1-0728), a NASA Space Grant, an NSF Graduate 
Research Fellowship, and industrial gifts from Interval, Microsoft, and Xerox. Appendix: Parallel projection 
onto a bilinear surface Suppose we have a color c that we want to project in the direction P onto a duotone 
gamut de.ned by the colorsg0, g1, g2, and g1,2. The solution, which we will denote by c, must lie on 
the line that passes through c in the direction P, so we know c = c + tP for some real number t. The 
solution must also lie on the bilinear surface de.ning the duotone gamut: (1 1)(1 2) m a246 a XX a X 
a X X a XX 357 1(1 2) c = c + tP = g0 g1 g2 g1,2 (1) (1 1)2 12 for some values of 1 and 2. In what follows, 
we will solve for the  XXXX unknown ink amounts 1 and 2, rather than the projection point c. First, 
let s rewrite equation (1) by grouping the terms differently: aaa X a X aa XX g0 ctP+(g1 g0)1+(g2 g0)2+(g0+g1,2 
g1 g2)12=0 Notice that if we take the dot product of both sides of this equation with either the S direction 
or the Y direction, we eliminate t because P, S, and Y are de.ned to be mutually orthogonal. We can write 
the two equations that result from these dot products as follows: u1+ u2 1+ u3 2+ u4 1 2=0 (2)  X X 
X X v1+ v21+ v32+ v412=0 (3) where the constants u1, , u4 and v1, , v4 are given by u1 =(g0 c) S v1=(g0 
c) Y u2 =(g1 g0) S v2=(g1 g0) Y u3 =(g2 g0) S v3=(g2 g0) Y a aa X a X aa u4 =(g0+ g1,2 g1 g2) S v4=(g0+ 
g1,2 g1 g2) Y Next, we can solve equation (2) for 2 in terms of 1 to get u1+ u21 X a XX 2 = (4)u3+ 
u41 When we substitute this expression for2 into equation (3) and sim­plify, we get a quadratic equation 
for 1 alone: XXX w121+ w21+ w3=0 where w1 = u4v2 u2v4 X a a w2 = u4v1 u1v4+ u3v2 u2v3 w3 = u3v1 u1v3 
Therefore, the solution for 1 is given by w2 X o8 a U p a6 w2 24w1w3 if w1=0 1= 2w1 w3 if w1=0 w2 When 
w1 = 0, there are two possible projections of c onto the bi­linear surface; we choose the solution for 
1 that results in a value 6 XXX between zero and one. Once we obtain1, we can compute 2 from equation 
(4) and the projection point c from equation (1). References [1] F. R. Clapper and J. A. C. Yule. The 
effect of multiple internal re.ec­tions on the densities of half-tone prints on paper. Journal of the 
Op­tical Society of America, 43(7):600 603, July 1953. [2] James D. Foley, Andries van Dam, Steven K. 
Feiner, and John F. Hughes. Computer Graphics: Principles and Practice. Addison-Wesley, Reading, MA, 
second edition, 1990. [3] Ronald S. Gentile and Jan P. Allebach. A comparison of techniques for color 
gamut mismatch compensation. InHuman Vision, Visual Pro­cessing, and Digital Display, volume 1077 of 
Proceedings of the SPIE, pages 342 354. SPIE, Bellingham, WA, 1989. [4] Andrew S. Glassner. Principles 
of Digital Image Synthesis, volume 1. Morgan Kaufmann, San Francisco, 1995. [5] Andrew S. Glassner, Kenneth 
P. Fishkin, David H. Marimont, and Maureen C. Stone. Device-directed rendering. ACM Transactions on Graphics, 
14(1):58 76, January 1995. [6] Ed Granger. Light Source Computer Images, Inc. Personal communi­cation, 
December 1995. [7] Arthur C. Hardy and F. L. Wurzburg, Jr. Color correction in color print­ing. Journal 
of the Optical Society of America, 38(4):300 307, April 1948. [8] Steven J. Harrington and R. Victor 
Klassen. Color printing having a highlight color image mapped from a full color image. U.S. Patent 5,237,517, 
August 17, 1993. [9] Ewald Hering. Outlines of a Theory of the Light Sense. Harvard Uni­versity Press, 
Cambridge, MA, 1964. [10] R. W. G. Hunt. The Reproduction of Colour. Fountain Press, Kingston­upon-Thames, 
England, .fth edition, 1995. [11] Kansei Iwata and Gabriel Marcu. Computer simulation of printed col­ors 
on textile materials. InColor Hard Copy and Graphic Arts III, vol­ume 2171 of Proceedings of the SPIE, 
pages 228 238. SPIE, Belling­ham, WA, 1994. [12] Henry R. Kang. Applications of color mixing models to 
electronic printing. Journal of Electronic Imaging, 3(3):276 287, July 1994. [13] Bruce J. Lindbloom. 
Accurate color reproduction for computer graph­ics applications. In Proceedings of SIGGRAPH 89, pages 
117 126. ACM, New York, 1989. [14] Gabriel Marcu and Satoshi Abe. Color designing and simulation in non-conventional 
printing process. In Applications of Digital Image Processing XVII, volume 2298 of Proccedings of the 
SPIE, pages 216 223. SPIE, Bellingham, WA, 1994. [15] H. E. J. Neugebauer. Die Theoretischen Grundlagen 
des Mehrfarben­edruckes (The theoretical foundation for multicolor printing).Z. Wiss Photogr., pages 
73 89, 1937. Reprinted in Sayanai [20], pages 194 202. [16] Victor Ostromoukhov. Chromaticity gamut 
enhancement by heptatone multi-color printing. InDevice-Independent Color Imaging and Imag­ing Systems 
Integration, volume 1909 of Proceedings of the SPIE, pages 139 151. SPIE, Bellingham, WA, 1989. [17] 
Victor Ostromoukhov and Roger D. Hersch. Artistic screening. InPro­ceedings of SIGGRAPH 95, pages 219 
228. ACM, New York, 1995. [18] R. H. J. M. Otten and L. P. P. P. van Ginneken. The Annealing Algo­rithm. 
Kluwer Academic Publishers, Boston, 1989. [19] Warren Rhodes. Fifty years of the Neugebauer equations. 
In Sayanai [20], pages 7 18. [20] Kazuo Sayanai, editor. Neugebauer Memorial Seminar on Color Re­production, 
volume 1184 of Proceedings of the SPIE. SPIE, Belling­ham, WA, 1990. [21] Maureen C. Stone, William B. 
Cowan, and John C. Beatty. Color gamut mapping and the printing of digital color images. ACM Trans­actions 
on Graphics, 7(4):249 292, October 1988. [22] Johannes von Kries. Chromatic adaptation. In David L. MacAdam, 
ed­itor, Sources of Color Science, pages 109 119. MIT Press, Cambridge, MA, 1970. [23] John A. C. Yule. 
Principles of Color Reproduction. Wiley, New York, 1967.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237262</article_id>
		<sort_key>249</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[A model of visual adaptation for realistic image synthesis]]></title>
		<page_from>249</page_from>
		<page_to>258</page_to>
		<doi_number>10.1145/237170.237262</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237262</url>
		<keywords>
			<kw><![CDATA[adaptation]]></kw>
			<kw><![CDATA[realistic image synthesis]]></kw>
			<kw><![CDATA[vision]]></kw>
			<kw><![CDATA[visual perception]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P132198</person_id>
				<author_profile_id><![CDATA[81100459651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Ferwerda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Frank H. T. Rhodes Hall, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14020467</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Frank H. T. Rhodes Hall, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083338</person_id>
				<author_profile_id><![CDATA[81100449948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Frank H. T. Rhodes Hall, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Frank H. T. Rhodes Hall, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adelson E.H. (1982). Saturation and adaptation in the rod system. Vision Research, 22, 1299-1312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Aguilar, M. ~z Stiles, W.S. (1954). Saturation of the rod mechanism of the retina at high levels of stimulation. Optica Acta, I, 59-65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baker, H.D. (1949). The course of foveal adaptation measured by the threshold intensity increment. Journal of the Optical Society of America, 39, 172-179.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[CIE (1981). An analytic model for describing the influence of lighting parameters upon visual performance, vol. I. Technical foundations. CIE 19/2.1, Technical committee 3.1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Crawford, B.H. (1947). Visual adaptation in relation to brief conditioning stimuli. Proceedings of the Royal Society of London, Series B, 128, 283-302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Granit R., Munsterhjelm, A., and Zewi, M. (1939). The relation between concentration of visual purple and retinal sensitivity to light during dark adaptation. Journal of Physiology, 96, 31-44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hecht, S. (1934). Vision II: the nature of the photoreceptor process. In C. Murchison (Ed.), A handbook of general experimental psychology. Worchester, Massachusetts: Clark University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hood, D.C. &amp; Finkelstein M.A. (1986). Visual sensitivity. In K.Boff, L. Kaufman, ~z J. Thomas (Eds.), Handbook of Perception and Human Performance (Volume I). 5-I-5-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[IES (1993). Lighting handbook:reference and application volume, (8th edition). Mark S. Rea (Ed.). New York: Illuminating Engineering Society of North America.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>912931</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Meyer, G.W. (1986) Color calculation for and perceptual assessment of computer graphic images. Ph.D. thesis, Cornell University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Mueller, C.G. (1951). Frequency of seeing functions for intensity discrimination at various levels of adapting intensity. Journal of General Physiology, 34, 463-474.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Minnaert, M. (1954) The nature of light and color in the open air. New York: Dover.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Pugh, E.N. (1988). Vision: physics and retinal physiology, In R.C. Atkinson (Ed.), Steven's handbook of experimental psychology, (2nd edition). New York: Wiley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Riggs, L.A. (1971) Vision. In J.W. Kling &amp; L.A. Riggs (Eds.), Woodworth and Schlosberg's Experimental Psychology , (3rd edition). New York: Holt, Rinehart, and Winston.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Shaler, S. (1937) The relation between visual acuity and illumination. Journal of General Physiology, 21, 165-188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218466</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Spencer, G., Shirley P., Zimmerman, K. ~z Greenberg, D. (1995). Physically-based glare effects for computer generated images, Proceedings ACM SIGGRAPH '95, 325-334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Spillman, L., ~z Werner, J.S. (Eds.) (1990). Visual perception: the neurophysiological foundations. SanDiego: Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Stevens, S.S., ~z Stevens, J.C. (1960). Brightness function: parametric effects of adaptation and contrast, Journal of the Optical Society of America , 53, 1139.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Tumblin, J., and Rushmeier, H. (1993). Tone Reproduction for Realistic Images, IEEE Computer Graphics and Applications, 13(6), 42-48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>180934</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ward, G. (1994). A contrast-based scalefactor for luminance display. In P.S. Heckbert (Ed.), Graphics Gems IV, Boston: Academic Press Professional.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Wyszecki G. ~z Stiles W.S. (1982). Color science: concepts and methods, quantitative data and formulae (2nd edition). New York: Wiley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Model of Visual Adaptation for Realistic Image Synthesis James A. Ferwerda Sumanta N. Pattanaik Peter 
Shirley Donald P. Greenberg Program of Computer Graphics, Cornell University* Abstract In this paper 
we develop a computational model of vi­sual adaptation for realistic image synthesis based on psy­chophysical 
experiments. The model captures the changes in threshold visibility, color appearance, visual acuity, 
and sensitivity over time that are caused by the visual system s adaptation mechanisms. We use the model 
to display the results of global illumination simulations illuminated at in­tensities ranging from daylight 
down to starlight. The re­sulting images better capture the visual characteristics of scenes viewed over 
a wide range of illumination levels. Be­cause the model is based on psychophysical data it can be used 
to predict the visibility and appearance of scene fea­tures. This allows the model to be used as the 
basis of perceptually-based error metrics for limiting the precision of global illumination computations. 
CR Categories and Subject Descriptors: I.3.0 [Computer Graphics]: General; I.3.6 [Computer Graphics]: 
Methodology and Techniques. Additional Key Words and Phrases: realistic image synthesis, vision, visual 
perception, adaptation. 1 Introduction The goal of realistic image synthesis is to produce im­ages that 
capture the visual appearance of modeled scenes. Physically-based rendering methods make it possible 
to ac­curately simulate the distribution of light energy in scenes, but physical accuracy in rendering 
does not guarantee that the displayed images will have a realistic visual appearance. There are at least 
two reasons for this. First, the range of light energy in the scene may be vastly di.erent from the range 
that can be produced by the display device. Second, the visual states of the scene observer and the display 
ob­server may be very di.erent. To produce realistic images we need to model not only the physical behavior 
of light propagation, but also the pa­rameters of perceptual response. This is particularly true of the 
visual system s adaptation to the range of light we encounter in the natural environment since visual 
function changes dramatically over the range of environmental illu­mination. *580 Frank H. T. Rhodes 
Hall, Ithaca, NY 14853, USA. http://www.graphics.cornell.edu. Permission to make digital or hard copies 
of part or all of this work or personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that copies bear this notice and the 
full citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
-6-4-20 2 4 6 8Luminance (log cd/m2) starlight moonlight indoor lighting sunlight Range of Illumination 
Visual function poor acuity good acuity Figure 1: The range of luminances in the natural environ­ment 
and associated visual parameters. After Hood (1986). Very little work has been done in computer graphics 
on adaptation. Earlier work has focused primarily on overcom­ing the limits of conventional CRT displays 
and determin­ing how to best display simulated environments within the limited dynamic range available. 
Tumblin and Rushmeier (1993) introduced the concept of tone reproduction to the computer graphics community 
and developed a tone repro­duction operator that preserves the apparent brightness of scene features. 
Ward (1994) has taken a somewhat di.er­ent approach and has developed a tone reproduction opera­tor that 
preserves apparent contrast and visibility. Spencer (1995) has developed a psychophysical model of glare 
and has implemented a glare .lter that increases the apparent dynamic range of images. The model of adaptation 
presented herein deals with many more visual parameters than dynamic range. We develop a model that includes 
the e.ects of adaptation on threshold visibility, color appearance, visual acuity,and changes in visual 
sensitivity over time. The algorithm we derive from our model is based on the psychophysics of adap­tation 
measured in experimental studies. Therefore, it can be used predictively for illumination engineering 
work, and can be used to develop perceptually-based approaches to ren­dering and display. 1.1 Background 
The range of light energy we experience in the course of a day is vast. The light of the noonday sun 
can be as much as 10 million times more intense than moonlight. Figure 1 shows the range of luminances 
we encounter in the natural environment and summarizes some visual parameters asso­ciated with this luminance 
range. Our visual system copes with this huge range of luminances by adapting to the pre­vailing conditions 
of illumination. Through adaptation the visual system functions over a luminance range of nearly 14 log 
units. Adaptation is achieved through the coordinated action of mechanical, photochemical, and neural 
processes in the vi­sual system. The pupil, the rod and cone systems, bleaching and regeneration of receptor 
photopigments, and changes in neural processing all play a role in visual adaptation. Although adaptation 
provides visual function over a wide range of ambient intensities, this does not mean that we see equally 
well at all intensity levels. For example, under dim illumination our eyes are very sensitive, and we 
are able to detect small di.erences in luminance, however our acuity for pattern details and our ability 
to distinguish colors are both poor. This is why it is di.cult to read a newspaper at twilight or to 
correctly choose a pair of colored socks while dressing at dawn. Conversely, in daylight we have sharp 
color vision, but absolute sensitivity is low and luminance di.erences must be great if we are to detect 
them. This is why it is impossible to see the stars against the sunlit sky. Further, adaptation does 
not happen instantaneously. Nearly everyone has experienced the temporary blindness that occurs when 
you enter a dark theatre for a matinee. It can sometimes take a few minutes before you can see well enough 
to .nd an empty seat. Similarly, once you have dark adapted in the theatre and then go out into the daylight 
af­ter the show, the brightness is at .rst dazzling and you need to squint or shield your eyes, but within 
about a minute, you can see normally again. To produce realistic synthetic images that capture the ap­pearance 
of actual scenes, we need to take adaptation-related changes in vision into account. In this paper we 
develop a computational model of visual adaptation and apply it to the problem of rendering scenes illuminated 
at vastly di.erent levels. The model predicts the visibility of object features and colors at particular 
illumination levels, and simulates the changes in visibility and appearance that occur over the time-course 
of light and dark adaptation.  2 Physiological foundations of adaptation Through adaptation the visual 
system functions over a lumi­nance range of nearly 14 log units, despite the fact that the individual 
neural units that make up the system have a re­sponse range of only about 1.5 log units (Spillman 1990). 
Through four distinct adaptation mechanisms, the visual system moderates the e.ects of changing levels 
of illumi­nation on visual response to provide sensitivity over a wide range of ambient light levels. 
2.1 The pupil The most obvious mechanism available to regulate the amount of light stimulating the visual 
system is the pupil. Over a 10 log unit range of luminance, the pupil changes in diameter from approximately 
7 mm down to about 2 mm (Pugh 1988). This range of variation produces a little more than a log unit change 
in retinal illuminance so pupillary ac­tion alone is not su.cient to completely account for visual adaptation 
(Spillman 1990). In fact, rather than playing a signi.cant role in adaptation it is thought that variation 
in pupil size serves to mitigate the visual consequences of aberrations in the eye s optical system. 
At high levels where there is plenty of light to see by, the pupil stops down to limit the e.ects of 
the aberrations. At low levels where catching enough light to allow detection is more essential than 
opti­mizing the resolution of the retinal image, the pupil opens to allow more light into the eye. 2.2 
The rod and cone systems There are somewhere between 75 and 150 million rod and 6 to 7 million cone photoreceptors 
in each retina (Riggs 1971). The rods are extremely sensitive to light and provide achro­matic vision 
at scotopic levels of illumination ranging from 10-6 to 10 cd/m2. The cones are less sensitive than the 
rods, but provide color vision at photopic levels of illumination in log Relative Efficiency 1 0 -1 -2 
-3 -4 -5 Wavelength (nm) Figure 2: Scotopic V. . and photopic V. luminous e.ciency functions. After Wyszecki 
(1982). the range of 0.01 to 108 cd/m2. At light levels from 0.01 to 10 cd/m2 both the rod and cone systems 
are active. This is known as the mesopic range. Relatively little is known about vision in the mesopic 
range but this is increasingly a topic of interest because computer-based o.ce environments with CRT 
displays and subdued lighting exercise the visual system s mesopic range. The rod and cone systems are 
sensitive to light with wave­lengths from about 400nm to 700nm. The rods have their peak sensitivity 
at approximately 505nm. Spectral sensitiv­ity of the composite cone system peaks at approximately 555 
nm. The rod and cone systems are not equally sensitive to light at all wavelengths. Luminous e.ciency 
functions show how e.ective light of a particular wavelength is as a visual stimulus. Di.erences between 
the rod and cone systems lead to separate photopic and scotopic luminous e.ciency func­tions that apply 
to typical daytime and nighttime illumi­nation levels. Figure 2 shows the normalized scotopic and photopic 
luminous e.ciency functions developed by the CIE (Wyszecki 1982). 2.3 Bleaching and regeneration of 
photopigments At high light intensities, the action of light depletes the pho­tosensitive pigments in 
the rods and cones at a faster rate than chemical processes can restore them. This makes the receptors 
less sensitive to light. This process is known as pigment bleaching. Early theories of adaptation were 
based the idea that light adaptation was produced by pigment bleaching and dark adaptation was produced 
by pigment restoration (Hecht 1934). However pigment bleaching can­not completely account for adaptation 
for two reasons: .rst, a substantial amount of adaptation takes place in both the rod and cone systems 
at ambient levels where little bleach­ing occurs (Granit 1939); and second, the time courses of the early 
phases of dark and light adaptation are too rapid to be explained by photochemical processes alone (Crawford 
1947).  2.4 Neural processes The neural response produced by a photoreceptor cell de­pends on chemical 
reactions produced by the action of light on the cell s photopigments. The cell s response to light is 
limited by the maximum rate and intensity of these chemical reactions. If the reactions are occurring 
near their maximum levels, and the amount of light striking the photopigments is increased, the cell 
may not be able to fully signal the in­crease. This situation is known as saturation. The result of saturation 
is response compression: above a certain level incremental increases in light intensity will produce 
smaller and smaller changes in the cell s response rate. The rod and cone photoreceptors connect through 
a net­work of neurons in the retina to ganglion cells whose axons form the optic nerve. Adaptive processes 
sited in this neural network adjust the base activity and gain of the early visual system to mitigate 
the e.ects of response compression in the photoreceptors. A multiplicative process adjusts the gain of 
the system by e.ectively scaling the input by a constant re­lated to the background luminance. This process 
acts very rapidly and accounts for changes in sensitivity over the .rst few seconds of adaptation. A 
slower acting subtractive pro­cess reduces the base level of activity in the system caused by a constant 
background. This process accounts for the slow improvement in sensitivity measured over minutes of adaptation 
(Adelson 1982).  3 A psychophysical model of adaptation The physiological mechanisms described above 
provide the basis for visual adaptation. The action of these mechanisms is re.ected in the changes in 
visibility, color appearance, vi­sual acuity, and sensitivity over time that can be observed in everyday 
experience and measured in psychophysical exper­iments. In this section we will review a series of experiments 
that measure the changes in visual function that accompany adaptation. The results of these experiments 
will serve as the basis of our computational model of adaptation. 3.1 Threshold studies Visual sensitivity 
is often measured psychophysically in a detection threshold experiment. In the typical experimental paradigm, 
an observer is seated in front of a blank screen that .lls their .eld of view. To determine the absolute 
threshold the screen is made dark. To determine the con­trast threshold a large region of the screen 
is illuminated to a particular background luminance level. Before testing be­gins, the observer .xates 
the center of the screen until they are completely adapted to the background level. On each trial a disk 
of light is .ashed near the center of .xation for a few hundred milliseconds. The observer reports whether 
they see the disk or not. If the disk is not seen its intensity is increased on the next trial. If it 
is seen, its intensity is decreased. In this way, the detection threshold for the target disk against 
the background is measured. There are many stimulus parameters that a.ect detection thresholds. Background 
and target size, color, duration, and position all a.ect threshold magnitude. To allow comparison of 
the di.erent experiments in this section we have summa­rized the experimental parameters in insets on 
each graph. We have also converted from the diverse range of luminance and illuminance units used in 
the literature to a standard scale of log cd/m2 taking into account the changes in retinal log Threshold 
Luminance (cd/m^2) 5 4 3 2 1 0 -1 -2 -3 log Background Luminance (cd/m^2) Figure 3: A psychophysical 
model of detection thresholds over the full range of vision. illuminance due to changes in pupil size 
and di.erences in the luminous e.ciency of the rod and cone systems.  3.2 Changes in threshold sensitivity 
As the luminance of the background in a detection thresh­old experiment is increased from zero, the luminance 
dif­ference between target and background required for detec­tion increases in direct proportion to the 
background lu­minance. Plotting the detection threshold against the cor­responding background luminance 
gives a threshold-versus­intensity (t.v.i.) function. Figure 3 shows the psychophysi­cally measured t.v.i. 
functions for the rod and cone systems. At luminance levels below about -4 log cd/m2,the rod curve .attens 
to a horizontal asymptote. This indicates that the luminance of the background has little e.ect on the 
threshold which approaches the limit for detecting a stimu­lus in the dark. At levels above 2 log cd/m2 
the curve ap­proaches a vertical asymptote. This indicates that the rod system is being overloaded by 
the background luminance with the result that no amount of luminance di.erence be­tween the background 
and target will allow detection. Over a wide middle range covering 3.5 log units of back­ground luminance 
the function is linear, this relationship can be described by the function .L = kL. This relationship 
is known as Weber s law (Riggs 1971). Weber s law behavior is indicative of a system that has constant 
contrast sensitivity, since the proportional increase in threshold with increasing background luminance 
corresponds to a luminance pattern with constant contrast. The other curve in Figure 3 shows the t.v.i. 
function for the cone system. In many ways the rod and cones show sim­ilar patterns of response. At levels 
below -2.6 log cd/m2 , the t.v.i function is essentially .at indicating that the back­ground has no e.ect 
on the response threshold. In this region the cones are operating at their absolute levels of sensitiv­ity. 
At background levels above 2 log cd/m2 the function is linear, indicating Weber s law behavior and constant 
con­trast sensitivity. One important di.erence between the rod and cone functions is that the cone system 
never saturates in the upper reaches of the luminance range. Instead, pig­ment bleaching gradually lowers 
sensitivity all the way up (b) (c) 2 0 (a) rods -4  -6 -8 -12 400 500 600 700 400 500 600 700 400 500 
600 700 Wavelength Figure 4: Changes in the spectral sensitivity of the visual system at (a) scotopic, 
(b) mesopic, and (c) photopic illumination levels. After Hood (1986). to damaging intensity levels. We 
have placed the rod and cone t.v.i. functions on the same graph to show the relative sensitivities of 
the systems -1 and to show how threshold sensitivity varies over a wide range of scotopic and photopic 
background luminances. At background luminances from about -6 to 0 log cd/m2 the rod system is more sensitive 
than the cone system. In this range the rods account for the magnitude of the detection thresh­ old. 
As the background luminance is increased, the rod sys­ tem loses sensitivity and the detection threshold 
rises. At a background level around 0 log cd/m2 the rod and cone t.v.i. functions cross. Above this level 
the cone system is more sensitive than the rod system and it accounts for the detec­ log Sensitivity 
(1/ [cd/m^2]) -2 -3 -4 -5 -6 -7 -8 tion threshold. Over a wide range of background luminances -9 the 
visual system s threshold sensitivity can be described by -10 the envelope of the rod and cone t.v.i. 
curves. -11 -4 -2-1 0 1 2 3 4  3.3 Changes in color appearance The spectral sensitivities of the rod 
and cone systems are described by the scotopic and photopic luminous e.ciency functions. When presented 
graphically, the functions are typically normalized which masks the fact that the rod and cone systems 
di.er greatly in sensitivity and operate over di.erent luminance ranges. Figure 4 (a) shows the visual 
system s spectral sensitivity at scotopic levels. At these levels detection is dominated by the rod system. 
Absolute sensitivity is quite high, but since the rod system is achromatic, color will not be apparent. 
Figure 4 (b) shows spectral sensitivity at mesopic levels. Here the rod and cone systems are nearly equal 
in abso­lute sensitivity. Detection at a particular wavelength will be served by the more sensitive system. 
The graph shows that the rods will detect wavelengths below about 575 nm and the cones will detect wavelengths 
above this point. Figure 4 (c) shows the visual system s spectral sensitivity at photopic levels. At 
these levels detection is dominated by the cone system. Absolute sensitivity has dropped consider­ably, 
but due to the trichromatic nature of the cone system, colors will now be seen. Figure 5 shows the luminous 
e.ciency functions as sur­faces positioned with respect to the rod and cone system threshold sensitivities 
at di.erent luminance levels. This 3d graph shows how the visual system s spectral sensitivity changes 
with changing luminance levels and which system is dominant at a particular level. The sub.gures show 
cross log Background Luminance Figure 5: A model of threshold sensitivity as a function of wavelength 
and background luminance for the rod and cone systems. sections of these spectral sensitivity vs. luminance 
surfaces. This model of the changes in spectral sensitivity with changing luminance levels can account 
for a number of di.er­ent color appearance phenomena observed over the scotopic to photopic range. First, 
at low luminance levels vision will be achromatic since detection at all wavelengths is served by the 
rod system. As the luminance level is raised into the mesopic range, the cone system will become active 
and col­ors will begin to be seen beginning with the long wavelength reds and progressing toward the 
middle wavelength greens. Only at relatively high luminances will short wavelength blue targets begin 
to appear colored. 3.4 Changes in visual acuity Acuity is a measure of the visual system s ability to 
resolve spatial detail. Acuity is often measured clinically with the Snellen chart. A portion of the 
Snellen chart is shown in Figure 6. The letters of the chart are constructed such that the strokes of 
each character subtend precise visual angles when viewed from a distance of 20 feet. The bottom line 
 Figure 6: The Snellen acuity chart. of the chart is taken as the standard of normal acuity. At 20 feet 
each character stroke in the bottom line (8) subtends one minute of visual angle. A viewer who can correctly 
iden­tify the characters on this line is said to have 20/20 vision. The upper lines in the chart have 
progressively wider stroke widths. These lines are used to assess subnormal acuity. For example each 
stroke in the characters on line 5 is twice as big as those on line 8. A person with normal acuity can 
identify the characters in this line from a distance of 40 feet. If you can just identify this line at 
the standard 20 foot viewing distance then you have 20/40 vision. The large E on line 1 of the chart 
is equivalent to a visual acuity of 20/200. Acuity is lower at scotopic levels of illumination than at 
photopic levels. The curve in Figure 7 shows how visual acuity changes with background luminance. The 
data cover the range from daylight down to starlight. The experiment measured acuity by testing the detectability 
of square wave gratings of di.erent spatial frequencies. The graph shows that the highest frequency grating 
that can be resolved drops from a high of about 50 cycles/degree at 3 log cd/m2 down to about 2 cycles/degree 
at -3.3 log cd/m2. Thisisequivalent to a change from almost 20/10 vision at daylight levels down to nearly 
20/300 under starlight. This curve can be used to predict the visibility of scene details at di.erent 
levels of illumination. At low levels of illumination it should be di.cult to resolve detailed patterns, 
like the smaller lines on the Snellen chart or .ne textures. Figure 7: Changes in grating acuity as 
a function of back­ground luminance. After Shaler (1937).  3.5 The time-course of adaptation 3.5.1 
Light adaptation Adaptation does not happen instantaneously. If you are seated in a dark room and the 
lights are suddenly switched on it takes several seconds before you adjust to seeing at the new level 
of illumination. This process is known as light adaptation. Figure 8 shows the results of an experiment 
on the time course of light adaptation in the rod system (Adel­son 1982). Prior to the experiment the 
observer was dark adapted. At the beginning of the experiment a large back­ground .eld of 0.5 log cd/m2 
was switched on and from that moment forward the threshold was measured repeatedly. In the instant after 
the background .eld was switched on the detection threshold jumped from its dark adapted level to about 
0.1 log cd/m2, but after 2 seconds the threshold has dropped back to about -1.7 log cd/m2 . The graph 
shows that light adaptation in the scotopic range of the rod system is extremely rapid. More than 80% 
of sensitivity recovery occurs within the .rst 2 seconds, and nearly 75% happens within the .rst 200 
ms. Figure 9 shows the results of a similar experiment on the time-course of light adaptation in the 
cone system (Baker 1949). As with the rod system, thresholds are highest im­mediately after the onset 
of the background .eld. At a 3.75 log cd/m2 background level, the instantaneous threshold is about 3.5 
log cd/m2. The threshold decreases over time and reaches a minimum after about 3 minutes of exposure. 
The threshold drops more than 0.5 log units during this period. After 3 minutes the threshold rises again 
slightly (due to interactions between neural and photochemical processes in adaptation) and reaches its 
fully adapted level at about 10 minutes. This experiment also shows that the time course of light adaptation 
in the cone system is slower than the rod system. Visually, light adaptation provides a distinctive experi­ence. 
When we go quickly from low to high levels of illumi­nation, at .rst everything is painfully glaring 
and we squint or close one eye to reduce the discomfort. However over time the overall brightness of 
the visual .eld diminishes to more comfortable levels and normal vision is restored.  Time (Seconds) 
Figure 8: The time course of light adaptation in the rod system. After Adelson (1982).  3.5.2 Dark adaptation 
Figure 10 shows the time-course of dark adaptation as mea­sured by Hecht (1934). In this experiment, 
the observer was .rst adapted to a high background luminance and then plunged into darkness. Detection 
thresholds were measured continuously over more than 30 minutes. The graph shows the detection threshold 
as a function of time in the dark. The kinked threshold curve is actually the envelope of the curves 
for the separately tested rod and cone systems. In the .rst 5 minutes after the adapting .eld is switched 
o., the threshold drops rapidly, but then it levels o. at a relatively high level because the cone system 
has reached its greatest sensitiv­ity, but the rod system has still not recovered signi.cantly. After 
about 7 minutes rod system sensitivity surpasses that of the cone system and the threshold begins to 
drop again. This point is known as the Purkinje break (Riggs 1971) and indicates the transition from 
detection by the cone system to detection by the rods. Changes in the threshold can be measured out to 
about 35 minutes, at which point the visual system has reached its absolute levels of sensitivity, and 
the threshold has dropped nearly 4 log units. Visually, dark adaptation is experienced as the tempo­rary 
blindness that occurs when we go rapidly from pho­topic to scotopic levels of illumination. The relatively 
slow time-course of dark adaptation means that vision can be im­paired for several minutes when we move 
quickly from high illumination levels to low ones.  3.6 Summary The cumulative achievement of adaptation 
is that the visual system is sensitive over a vast range of ambient light levels despite severe limits 
on the dynamic ranges of the individ­ual neural units that make up the system. However this does not 
mean that we see equally well at all levels of illu­mination. The experiments show that threshold visibility, 
color appearance, and visual acuity are di.erent at di.erent illumination levels, and that these visual 
parameters change over the time-course of light and dark adaptation. We will now develop a computational 
model of the changes in threshold visibility, color appearance, visual acuity, and sensitivity over time 
that are given by the experiments de­ 0 2 4 6 810 Time (mins) Figure 9: The time course of light adaptation 
in the cone system. After Baker (1949). scribed above. This computational model will allow us to produce 
synthetic images that better capture the appearance of scenes illuminated at di.erent levels. Because 
the compu­tational model is based on psychophysical data, it will allow us to predict the visibility, 
color appearance, and clarity of scene features at a given level of illumination and to describe the 
changes in these visual parameters over the time-courses of light and dark adaptation.  4 Implementation 
We implement our model in a program that maps image .les with photopic luminance (CIE Y), scotopic luminance, 
and CIE XZ channels to displayable images in a .xed RGB color space. Since this is fundamentally a tone 
reproduction problem, our algorithm draws on the state-of-the-art in this area. Tumblin and Rushmeier 
(1993) introduced the concept of tone reproduction to the computer graphics community. Tone reproduction 
addresses the goal of making an image that is a faithful visual representation of the photometric properties 
of a scene. Tone reproduction operators describe the mapping from scene to display in terms of physical 
pro­cesses in the display system and psychophysical processes in hypothetical scene and display viewers 
that a.ect the .delity of the displayed image to the scene. Tumblin and Rushmeier developed a tone reproduction 
operator that preserves brightness relationships. Their op­erator uses a psychophysical model of brightness 
perception developed by Stevens and Stevens (1960) to produce a map­ping from scene luminances to display 
luminances such that the perceived brightness of a region on the display will match the perceived brightness 
of a region in the scene. A somewhat di.erent approach to tone reproduction has been developed by Ward 
(1994). Ward s operator di.ers from Tumblin and Rushmeier s in that it preserves perceived contrast rather 
than perceived brightness. Ward s operator is based on threshold contrast sensitivity data collected 
by Blackwell (CIE 1981). The operator maps just noticeable contrast di.erences (JND s) in the scene to 
just noticeable di.erences in the image. From a psychophysical point of view, Tumblin and -1 -2 -3 -4 
-5 Time (mins) Figure 10: The time course of dark adaptation. After Riggs (1971). Rushmeier and Ward 
have taken fundamentally di.erent approaches to the tone reproduction process. Tumblin and Rushmeier 
s brightness-based operator seeks to match suprathreshold brightness appearance across the range of scene 
luminances. On the other hand, Ward s contrast-based operator, seeks to match contrast visibility at 
threshold and scales suprathreshold values relative to the threshold mea­sure. Each approach has its 
strengths and weaknesses. A brightness-based operator may better capture the subjective appearance of 
surfaces in a scene, but it may not correctly capture the visibility of surfaces near threshold. Conversely, 
a contrast-based operator will correctly predict threshold visibility, but may not account well for suprathreshold 
ap­pearance. A more complete model of tone reproduction for computer graphics may have to combine these 
two ap­proaches to correctly account for visibility at threshold as well as suprathreshold appearance. 
Because we have based our adaptation model on thresh­old data, our implementation is based on Ward s 
concept of matching just noticeable di.erences for the world and dis­play observers. Ward s tone reproduction 
operator is: Ld(Lw)= mLw, (1) where Lw is the luminance seen by the world observer, and Ld is the luminance 
that Lw is mapped to on the display device. The multiplier m is chosen to achieve matches in visibility 
for the world and display observers. To achieve this, Ward assumes that we have a t.v.i. function t(L)that 
gives a threshold luminance that is barely visible for a given adaptation luminance L. Hefurther assumes 
thatwehavea way to estimate the adaptation luminance Lwa for the world observer and Lda for the display 
observer. This means that his multiplier m is a function of the adaptation levels of world and display 
observer. So he chooses m(Lwa,Lda)such that: t(Lda)= m(Lwa,Lda)t(Lwa), (2) so m(Lwa,Lda)= t(Lda)/t(Lwa 
). (3) This determines how luminance is mapped. Ward assumes that the chromatic channels follow the same 
mapping. log Threshold Luminance (cd/m^2) To construct our operator we .rst apply Ward s model without 
change using our cone t.v.i data from Figure 3, which is approximated by: log tp(La)=. -0.72 (4) if log 
La =-2.6, log La -1.255 (0.249 log La +0.65)2.7 -0.72 if log La =1.9, otherwise. This operator would 
work much the same as Ward s model, although Equation 4 is slightly di.erent from Ward s because it is 
derived from di.erent experimental data. We chose this data because we did not have access to the raw 
data Ward used, and because the di.erences are small enough that they are probably not signi.cant. We 
now extend Ward s model to include the rod t.v.i. function shown in Figure 3. Our approximation to this 
data is: log ts(La)= (5) -2.86 if log La =-3.94, log La -0.395 if log La =-1.44, 6)2.18 (0.405 log La 
+1.-2.86 otherwise. Since we expect an achromatic response from the rod sys­tem, we produce only a grayscale 
mapping. We do this by applying Equation 3 using the t.v.i. curves from Equation 5 for the world observer 
and Equation 4 for the display ob­server, because the display observer is in a photopic state of adaptation. 
This would be a plausible tone reproduction operator to preserve visibility for a coneless observer (a 
rod monochromat ). This technique was inspired by Meyer s (1986) model for simulating the visual experience 
of color­defective viewers. For photopic conditions we can apply the photopic tone reproduction operator. 
For scotopic conditions we can apply the scotopic operator. But what do we do for mesopic con­ditions? 
Simply adding the results of the photopic and sco­topic operators would be a mistake, because for high 
mesopic levels the rods would produce quite a bright image, when in fact they are shutting down due to 
saturation. Instead, we generate both a photopic display luminance Ldp and a sco­topic luminance Lds, 
and combine them with the formula: Ld = Ldp + k(La)Lds, (6) where k is a constant that varies from 1 
to 0 as the sco­topic world adaptation level goes from the bottom to the top of the mesopic range. The 
rods rather than the cones have a multiplier because the rod system is losing sensitivity as the intensity 
increases toward the photopic range, while the cones are quiescent and in a ready state. Because the 
cones are ready to respond, we apply Equation 6 for all scene adaptation levels. This way, a red stoplight 
in a night scene will be displayed properly. 4.1 Acuity Just as we want threshold contrast to be mapped 
between the world and display observers, we would like resolvable de­tail to be preserved as well. From 
the data shown in Figure 7, we can determine what spatial frequencies are visible to the world observer. 
We simply remove all spatial frequencies above this in the image we present to the display observer. 
Because we don t want ringing in the displayed image, we use a Gaussian convolution .lter whose power 
spectrum am­plitude at the cuto. frequency is matched to the observer s threshold. Thus we remove frequencies 
in the image which would not be discernable to the world observer: t(Lwa )f. (.c(Lwa )) = , (7) Lwa where 
f. is the Fourier transform of the convolution .lter, and .c(Lwa ) is the threshold frequency for the 
world adap­tation of the viewer. This way a high contrast scene grating at frequency .c(Lwa ) will be 
displayed at the threshold of visibility for the display viewer. 4.2 Light Adaptation The detection 
threshold for an observer who suddenly enters a bright environment, is high relative to the threshold 
for an observer adapted to the bright environment. The t.v.i. data for this situation is shown in Figures 
8 and 9. We can apply Equation 3 to make sure that only high contrasts are visible. Although this would 
be valid in terms of visibility, it would produce a dim appearance, which is the opposite of qualita­tive 
experience. This is because whenever Equation 3 raises the contrast threshold for the world observer, 
it does this by making m small, thus assuring a dim appearance on the display. To combat this we note 
that any linear model can be put in place of Equation 3, and visibility can still be pre­served, but 
we gain a free parameter that can increase the qualitative accuracy of appearance. We keep the m multi­plier 
and add an o.set, so that contrast can be reduced and screen brightness can be adjusted separately: Ld(Lw)= 
mLw + b. (8) The multiplier m will still be set by the same formulas as above. However, b will be a function 
of time. Because we have no quantitative data to set b, we do the simplest thing possible: we set b such 
that Ld(Lwa ) = constant over time. (9) This means the overall luminance of the display will not change 
during the light adaptation process. We could adjust the value of b in an ad-hoc manner to create a .ash 
im­mediately after the viewer changes viewing conditions. We choose not to do this because we want to 
preserve a hands­o. objective model. 4.3 Dark Adaptation The detection threshold for an observer who 
suddenly enters a dim environment is high relative to the threshold of an observer adapted to the dim 
environment. The procedure we used for light adaptation can be applied without change. 4.4 Determining 
adaptation luminances When applying display equations such as Equation 3, the result depends on the choices 
of the adaptation states Lwa and Lda. In the absence of any obviously correct answer, we opt for the 
simplest choice. For the world adaptation we choose half the highest visible luminance. For the display 
ob­server we use half the maximum screen luminance (typically 80/2=40cd/m2). We have observed that the 
appearance of many displayed images can be improved by tuning the adaptation luminances, but we purposely 
avoid doing this because we want to maintain an automatic process based on psychophysical data.  5 Results 
The panels of Figure 11 show the results of applying our model of visual adaptation to a simulated scene. 
The scene is an o.ce that contains a Snellen chart, and a Macbeth Colorchecker chart used as a standard 
in color reproduction. The rendered image .le was created using Monte Carlo path tracing with a spectral 
color model, di.use illumination that is uniform across the visible spectrum, and the standard re­.ectivities 
for the Macbeth chart (Wyszecki 1982). Panel (a) shows the image produced by our model for a scene il­luminated 
at 1000 cd/m2 . This image simulates what the scene looks like under photopic conditions that approximate 
normal daylight levels. Notice that all the colors in Macbeth chart are bright and saturated, and that 
all the letters in the Snellen chart can be recognized. Panel (b) shows the scene illuminated at 10 cd/m2 
. This approximates dim interior lighting and is near the top of the mesopic range. Notice that the scene 
is darker overall, that some contrast has been lost, and that the colors are less saturated, but acuity 
is still good since all the lines on the Snellen chart are recognizable. Panel (c) shows the scene illuminated 
at 0.04 cd/m2.This is a moonlight level near the the mesopic/scotopic transition. Notice that the saturation 
of all the colors in the Macbeth chart is greatly reduced, and that the blues and greens have become 
completely achromatic. Notice also that visual acu­ity has dropped signi.cantly, and that the smaller 
letters on the Snellen chart can no longer be identi.ed. Panel (d) shows the scene illuminated at starlight 
levels of 0.001 cd/m2 near the lower threshold of vision. At this level detection is the primary function 
of vision. The ability to distinguish col­ors and details has been lost. Only the largest and highest 
contrast forms can be discerned. The di.erences in con­trast, color appearance, and spatial resolution 
that can be observed across this set of images are a consequence of the adaptation-related changes in 
visual function that are cap­tured by our model. One particular visual phenomenon predicted by our model 
is the Purkinje shift in the relative lightness of reds and blues in the mesopic range (Minnaert 1954). 
The shift is due to the re-ordering of the relative sensitivities of the rod and cone systems at mesopic 
levels. The e.ect can be seen in the reversal of the lightnesses of the red and blue squares in the Macbeth 
chart. In panel (b) the scene is illuminated at 10 cd/m2 near the top end of the mesopic range. At this 
level the red square appears lighter than the blue square. In panel (c), illuminated at 0.04 cd/m2 near 
the bottom of the mesopic range the blue square now appears lighter than the red. Figure 12 (a) shows 
an image sequence that simulates the changes in visual function over the time course of light adap­tation. 
In the .rst frame of the sequence the scene is illumi­nated to a level of 0.1 cd/m2. In the second frame 
the light level has just been raised to 5623 cd/m2. Notice that much of the scene is washed out. Apparent 
contrast is reduced and the colors in the Macbeth chart appear desaturated. The subsequent frames show 
how the scene appears at intervals following the change in illumination level. Notice that ap­parent 
contrast and color gamut increase over time. The .nal frame shows the scene after 75 seconds of light 
adap­tation. After this time, adaptation is almost complete and visibility, color appearance, and acuity 
are near their steady state photopic levels. Figure 12 (b) shows an image sequence that simulates the 
changes in visual function over the time course of dark adaptation. In the .rst frame of the sequence 
the scene is il­luminated to a level of 1412 cd/m2. In the second frame the light level has just been 
reduced to 0.1 cd/m2.Notice that at .rst the appearance is low contrast and that only the major scene 
features can be distinguished. The subsequent frames show how the scene appears at intervals during dark 
adapta­tion. The .nal frame shows the appearance after more than 3 minutes of dark adaptation. At this 
time adaptation is almost complete, and visibility, color appearance and acuity are close to their steady 
state scotopic levels.  6 Conclusions/Future Work In this paper we have developed a computational model 
of the changes in visual function that are produced by adapta­tion. By applying this model to global 
illumination solutions we have generated images that better capture the visual ap­pearance of scenes 
illuminated over a wide range of intensity Figure 11: Visual function across the range of light. levels. 
Because this model is based on psychophysical exper­iments, the images produced are visually faithful 
representa­tions and can be used predictively. However we must caution that since our data is derived 
from experiments conducted under widely varying conditions, none of which are likely to match typical 
viewing conditions, our images should be taken as approximations to a precisely accurate simulation. 
There is still much work to be done in this area. While we have modeled the visual consequences of adaptation 
we still do not have a good model for a viewer s state of adaptation. This is a complex problem because 
the retina adapts locally and because eye movements cause the state of adaptation to change continuously. 
We are also less than completely satis.ed with our simu­lations of light and dark adaptation. While we 
believe our model predicts the changes in threshold visibility that oc­cur over the time course of adaptation, 
we feel that our images do not completely capture the appearance of the early phases of light and dark 
adaptation. Our algorithm is based on Ward s contrast-threshold tone reproduction op­erator. Tumblin 
and Rushmeier have presented an alter­native tone reproduction operator based on suprathreshold brightness 
measurements. Perhaps a more complete solution to the questions of adaptation-related changes in visibility 
and appearance will come from some combination of these models. A more complete model of adaptation will 
be important for advances in realistic image synthesis. The quest for real­ism in computer graphics is 
pushing advanced software and hardware technology toward a convergence. On the soft­ware side are physically-based 
global illumination rendering methods that produce accurate simulations of the distribu­tion of light 
energy in scenes. On the hardware side are high-resolution immersive displays in which computer gen­erated 
images .ll the visual .eld. True visual realism in im­age synthesis will eventually occur with the merging 
of these advanced technologies, but two problems stand in the way. First, current rendering methods are 
too slow to accommo­date the real-time update rates required for immersive envi­ronments. Second, we 
do not know how to correctly display the results of global illumination simulations to produce re­alistic 
visual appearance in immersive display systems. A better model of visual adaptation can help solve both 
of these problems. In the .rst case, an adaptation model can be used as the basis of perceptual error 
metrics to limit the precision of global illumination calculations based on visibility and appearance 
criteria. This could lead to time­critical global illumination rendering algorithms that achieve real-time 
rates. In the second case an adaptation model can be used to determine how to properly display images 
in immersive display systems where the display output .lls the visual .eld and provides all the viewer 
s visual stimulation. Bringing these two techniques together in algorithms where rendering computations 
can be tightly constrained because the viewer s visual state is precisely known could lead to even greater 
computational e.ciencies and greater visual realism.  Acknowledgements Dan Kartch and Steve Marschner 
helped with the preparation of the paper and color .gures. This work was supported by the NSF/ARPA Science 
and Technology Center for Computer Graphics and Scien­ti.c Visualization (ASC-8920219) and by NSF CCR-9401961 
and per­formed on workstations generously provided by the Hewlett-Packard Corporation. Bibliography 
 Adelson E.H. (1982). Saturation and adaptation in the rod system. Vision Research, 22, 1299-1312. Aguilar, 
M. &#38; Stiles, W.S. (1954). Saturation of the rod mechanism of the retina at high levels of stimulation. 
Optica Acta, 1, 59-65. Baker, H.D. (1949). The course of foveal adaptation measured by the threshold 
intensity increment. Journal of the Optical Society of America, 39, 172-179. CIE (1981). An analytic 
model for describing the in.uence of light­ing parameters upon visual performance, vol. 1. Technical 
foun­dations. CIE 19/2.1, Technical committee 3.1. Crawford, B.H. (1947). Visual adaptation in relation 
to brief con­ditioning stimuli. Proceedings of the Royal Society of London, Series B, 128, 283-302. Granit 
R., Munsterhjelm, A., and Zewi, M. (1939). The relation between concentration of visual purple and retinal 
sensitivity to light during dark adaptation. Journal of Physiology, 96, 31-44. Hecht, S. (1934). Vision 
II: the nature of the photoreceptor process. In C. Murchison (Ed.), A handbook of general experimental 
psychology. Worchester, Massachusetts: Clark University Press. Hood, D.C. &#38; Finkelstein M.A. (1986). 
Visual sensitivity. In K.Bo., L. Kaufman, &#38; J. Thomas (Eds.), Handbook of Perception and Human Performance 
(Volume 1). 5-1-5-66. IES (1993). Lighting handbook:reference and application volume, (8th edition). 
Mark S. Rea (Ed.). New York: Illuminating Engineering Society of North America. Meyer, G.W. (1986) Color 
calculation for and perceptual assessment of computer graphic images. Ph.D. thesis, Cornell University. 
Mueller, C.G. (1951). Frequency of seeing functions for intensity discrimination at various levels of 
adapting intensity. Journal of General Physiology, 34, 463-474. Minnaert, M. (1954) The nature of light 
and color in the open air. New York: Dover. Pugh, E.N. (1988). Vision: physics and retinal physiology, 
In R.C. Atkinson (Ed.), Steven s handbook of experimental psychology, (2nd edition). New York: Wiley. 
Riggs, L.A. (1971) Vision. In J.W. Kling &#38; L.A. Riggs (Eds.), Wood­worth and Schlosberg s Experimental 
Psychology , (3rd edition). New York: Holt, Rinehart, and Winston. Shaler, S. (1937) The relation between 
visual acuity and illumina­tion. Journal of General Physiology, 21, 165-188. Spencer, G., Shirley P., 
Zimmerman, K. &#38; Greenberg, D. (1995). Physically-based glare e.ects for computer generated images, 
Proceedings ACM SIGGRAPH 95, 325-334. Spillman, L., &#38; Werner, J.S. (Eds.) (1990). Visual perception: 
the neurophysiological foundations. SanDiego: Academic Press. Stevens, S.S., &#38; Stevens, J.C. (1960). 
Brightness function: para­metric e.ects of adaptation and contrast, Journal of the Optical Society of 
America , 53, 1139. Tumblin, J., and Rushmeier, H. (1993). Tone Reproduction for Real­istic Images, IEEE 
Computer Graphics and Applications, 13(6), 42-48. Ward, G. (1994). A contrast-based scalefactor for luminance 
display. In P.S. Heckbert (Ed.), Graphics Gems IV, Boston: Academic Press Professional. Wyszecki G. &#38; 
Stiles W.S. (1982). Color science: concepts and methods, quantitative data and formulae (2nd edition). 
New York: Wiley.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237263</article_id>
		<sort_key>259</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Blue screen matting]]></title>
		<page_from>259</page_from>
		<page_to>268</page_to>
		<doi_number>10.1145/237170.237263</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237263</url>
		<keywords>
			<kw><![CDATA[alpha channel]]></kw>
			<kw><![CDATA[backing impurities]]></kw>
			<kw><![CDATA[backing shadows]]></kw>
			<kw><![CDATA[blue screen matte creation]]></kw>
			<kw><![CDATA[blue spill]]></kw>
			<kw><![CDATA[chromakey]]></kw>
			<kw><![CDATA[compositing]]></kw>
			<kw><![CDATA[flare]]></kw>
			<kw><![CDATA[separating surfaces]]></kw>
			<kw><![CDATA[triangulation matting]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP311402800</person_id>
				<author_profile_id><![CDATA[81536477056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alvy]]></first_name>
				<middle_name><![CDATA[Ray]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation, One Microsoft Way, Redmond, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P131722</person_id>
				<author_profile_id><![CDATA[81100294395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Blinn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation, One Microsoft Way, Redmond, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BEYER, W. Traveling Matte Photography and the Blue Screen System. American Cinematographer, May 1964, p. 266. The second of a four-part series.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617920</ref_obj_id>
				<ref_obj_pid>616032</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLINN, J. F. Jim Blinn's Corner: Compositing Part 1: Theory. IEEE Computer Graphics &amp; Applications, September 1994, pp. 83-87.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617933</ref_obj_id>
				<ref_obj_pid>616033</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLINN, J. F. Jim Blinn's Corner: Compositing Part 2: Practice. IEEE Computer Graphics &amp; Applications, November 1994, pp. 78-82.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DADOURIAN, A. Method and Apparatus for Compositing Video Images. U. S. Patent 5,343,252, August 30, 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FELLINGER, D. F. Method and Apparatus for Applying Correction to a Signal Used to Modulate a Background Video Signal to be Combined with a Foreground Video Signal. U. S. Patent 5,202,762, April 13, 1993.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FIELDING, R. The Technique of Special Effects Cinematography. Focal/Hastings House, London, 3rd edition, 1972, pp. 220-243.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[LANCZOS, C. Applied Analysis. Prentice Hall, Inc., Englewood Cliffs, NJ, 1964, pp. 156-161.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[MISHIMA, Y. A Software Chromakeyer Using Polyhedric Slice. Proceedings of NICOGRAPH 92 (1992), pp. 44-52 (Japanese). See http://206.155.32.1/us/primatte/whitepaper.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[PORTER, T. and DUFF, T. Compositing Digital Images. Proceedings of SIGGRAPH 84 (Minneapolis, Minnesota, July 23-27, 1984). In Computer Graphics 18, 3 (July 1984), pp. 253-259.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[PORTER, T. Matte Box Design. Lucasfilm Technical Memo 63, November 1986. Not built.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42249</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[PRESS, W. H., TEUKOLSKY, S. A., VETTERLING, W. T., and FLANNERY, B. P. Numerical Recipes in C. Cambridge University Press, Cambridge, 1988, p. 59.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[SMITH, A. R. Analysis of the Color-Difference Technique. Technical Memo 30, Lucasfilm Ltd., March 1982.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[SMITH, A. R. Math of Mattings. Technical Memo 32, Lucasfilm Ltd., April 1982.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[SMITH, A. R. Image Compositing Fundamentals. Technical Memo 4, Microsoft Corporation, June 1995.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[SMITH, A. R. Alpha and the History of Digital Compositing. Technical Memo 7, Microsoft Corporation, August 1995.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. Composite Photography Utilizing Sodium Vapor Illumination. U. S. Patent 3,095,304, May 15, 1958. Expired.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. Composite Color Photography. U. S. Patent 3,158,477, November 24, 1964. Expired.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. Electronic Composite Photography. U. S. Patent 3,595,987, July 27, 1971. Expired.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. Electronic Composite Photography with Color Control. U. S. Patent 4,007,487, February 8, 1977. Expired.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. Comprehensive Electronic Compositing System. U. S. Patent 4,100,569, July 11, 1978. Expired.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. and TAYLOR, B. Traveling Matte Composite Photography. American Cinematographer Manual. American Society of Cinematographers, Hollywood, 7th edition, 1993, pp. 430-445.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. Comprehensive Electronic Compositing System. U. S. Patent 4,344,085, August 10, 1982.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. Encoded Signal Color Image Compositing. U. S. Patent 4,409,611, October 11, 1983.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P., VLAHOS, P. and FELLINGER, D. F. Automatic Encoded Signal Color Image Compositing. U. S. Patent 4,589,013, May 13, 1986.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[VLAHOS, P. Comprehensive Electronic Compositing System. U. S. Patent 4,625,231, November 25, 1986.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Blue Screen Matting Alvy Ray Smith and James F. Blinn1 Microsoft Corporation ABSTRACT A classical problem 
of imaging the matting problem is sepa­ration of a non-rectangular foreground image from a (usually) 
rectangular background image for example, in a film frame, extraction of an actor from a background scene 
to allow substitu­tion of a different background. Of the several attacks on this diffi­cult and persistent 
problem, we discuss here only the special case of separating a desired foreground image from a background 
of a constant, or almost constant, backing color. This backing color has often been blue, so the problem, 
and its solution, have been called blue screen matting. However, other backing colors, such as yellow 
or (increasingly) green, have also been used, so we of­ten generalize to constant color matting. The 
mathematics of con­stant color matting is presented and proven to be unsolvable as generally practiced. 
This, of course, flies in the face of the fact that the technique is commonly used in film and video, 
so we demonstrate constraints on the general problem that lead to solu­tions, or at least significantly 
prune the search space of solutions. We shall also demonstrate that an algorithmic solution is possible 
by allowing the foreground object to be shot against two constant backing colors in fact, against two 
completely arbitrary backings so long as they differ everywhere. Key Words: Blue screen matte creation, 
alpha channel, compositing, chromakey, blue spill, flare, backing shadows, backing impurities, separating 
surfaces, triangulation matting. CR Categories: I.3.3, I.4.6, J.5.  DEFINITIONS A matte originally meant 
a separate strip of monochrome film that is transparent at places, on a corresponding strip of color 
film, that one wishes to preserve and opaque elsewhere. So when placed together with the strip of color 
film and projected, light is allowed to pass through and illuminate those parts desired but is blocked 
everywhere else. A holdout matte is the complement: It is opaque in the parts of interest and transparent 
elsewhere. In both cases, partially dense regions allow some light through. Hence some of 1 One Microsoft 
Way, Redmond, WA 98052-6399. alvys@microsoft.com, blinn@microsoft.com. Permission to make digital or 
hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
v2.37 the color film image that is being matted is partially illuminated. The use of an alpha channel 
to form arbitrary compositions of images is well-known in computer graphics [9]. An alpha channel gives 
shape and transparency to a color image. It is the digital equivalent of a holdout matte a grayscale 
channel that has full value pixels (for opaque) at corresponding pixels in the color image that are to 
be seen, and zero valued pixels (for transparent) at corresponding color pixels not to be seen. We shall 
use 1 and 0 to represent these two alpha values, respectively, although a typi­cal 8-bit implementation 
of an alpha channel would use 255 and 0. Fractional alphas represent pixels in the color image with par­tial 
transparency. We shall use alpha channel and matte interchangeably, it being understood that it is really 
the holdout matte that is the analog of the alpha channel. The video industry often uses the terms key 
and keying as in chromakeying rather than the matte and matting of the film industry. We shall consistently 
use the film terminology, after first pointing out that chromakey has now taken on a more sophisticated 
meaning (e.g., [8]) than it originally had (e.g., [19]). We shall assume that the color channels of an 
image are premultiplied by the corresponding alpha channel and shall refer to this as the premultiplied 
alpha case (see [9], [14], [15], [2], [3]). Derivations with non-premultiplied alpha are not so elegant. 
 THE PROBLEM The mixing of several pictures the elements to form a single resulting picture the composite 
is a very general notion. Here we shall limit the discussion to a special type of composite fre­quently 
made in film and television, the matte shot. This consists of at least two elements, one or more foreground 
objects each shot against a special backing color typically a bright blue or green and a background. 
We shall limit ourselves to the case of one foreground element for ease of presentation. The matting 
problem can be thought of as a perceptual proc­ess: the analysis of a complex visual scene into the objects 
that comprise it. A matte has been successfully pulled, if it in combi­nation with the given scene correctly 
isolates what most humans would agree to be a separate object in reality from the other ob­jects in the 
scene, that we can collectively refer to as the back­ground. Note that this analysis problem is the reverse 
of classic 3D geometry-based computer graphics that synthesizes both the object and its matte simultaneously, 
and hence for which there is no matting problem. There is also no matting problem of the type we are 
consider­ing in the case of several multi-film matting techniques such as the sodium, infrared, and ultraviolet 
processes [6], [16]. These record the foreground element on one strip of film and its matte simulta­neously 
on another strip of film. The problem we address here is that of extracting a matte for a foreground 
object, given only a composite image containing it. We shall see that, in general, this is an underspecified 
problem, even in the case where the background consists of a single back­ing color. Note that a composite 
image contains no explicit infor­mation about what elements comprise it. We use the term composite to 
convey the idea that the given image is in fact a representation of several objects seen simultaneously. 
The prob­lem, of course, is to determine, the objecthood of one or more of these objects. In the film 
(or video) world, the problem is to ex­tract a matte in a single-film process that is, one with no special 
knowledge about the object to be extracted, such as might be contained in a separate piece of film exposed 
simultaneously in a multi-film process. Now a formal presentation of the problem: The color C = [R G 
B a] at each point of a desired composite will be some func­tion of the color Cf of the foreground and 
color Cb of the new background at the corresponding points in the two elements. We have for convenience 
extended the usual color triple to a quadru­ple by appending the alpha value. As already mentioned, each 
of the first three primary color coordinates is assumed to have been premultiplied by the alpha coordinate. 
We shall sometimes refer to just these coordinates with the abbreviation c = [R G B], for color C. For 
any subscript i, Ci = [Ri Gi Bi ai] and ci = [Ri Gi Bi]. Each of the four coordinates is assumed to lie 
on [0, 1]. We shall always assume that af = ab = 1 for Cf and Cb i.e., the given foreground and new background 
are opaque rectangular images. The foreground element Cf can be thought of as a composite of a special 
background, all points of which have the (almost) constant backing color Ck, and a foreground Co that 
is the fore­ground object in isolation from any background and which is transparent, or partially so, 
whenever the backing color would show through. We sometimes refer to Co as the uncomposited foreground 
color. Thus Cf = f(Co, Ck) expresses the point-by-point foreground color as a given composite f of Ck 
and Co. We shall always take ak = 1 for Ck. We assume that f is the over function [9], Ca + (1 aa) Cb, 
combining Cb with (premultiplied) Ca by amount aa, 0 =aa = 1. One of the features of the premultiplied 
alpha formulation is that the math applied to the three primary color coordinates is the same as that 
applied to the alpha coordinate. An alpha channel holds the factor aa at every point in an image, so 
we will use channel and coordinate synonymously. This facilitates: The Matting Problem Given Cf and 
Cb at corresponding points, and Ck a known backing color, and assuming Cf = Co + (1 ao)Ck , determine 
Co which then gives composite color C = Co + (1 ao)Cb at the corre­sponding point, for all points that 
Cf and Cb share in common. We shall call Co that is, the color, including alpha, of a fore­ground object 
a solution to the Matting Problem. Once it is known at each point, we can compute C at each point to 
obtain the desired result, a composite over a new background presumably more interesting than a single 
constant color. We shall refer to the equation for Cf above as the Matting Equation. We sometimes refer 
to an uncomposited foreground object (those pixels with ao > 0) as an image sprite, or simply a sprite. 
 PREVIOUS WORK Blue screen matting has been used in the film and video industries for many years [1], 
[6], [21] and has been protected under patents [17], [18], [19], [20] until recently. The most recent 
of these ex­pired July, 1995. Newer patents containing refinements of the process still exist, however. 
Any commercial use of the blue screen process or extensions should be checked carefully against the extant 
patents e.g., [22], [23], [24], [25], [5], [4]. An outstanding inventor in the field is Petro Vlahos, 
who de­fined the problem and invented solutions to it in film and then in video. His original film solution 
is called the color-difference technique. His video solution is realized in a piece of equipment, common 
to the modern video studio, called the Ultimatte®. It is essentially an electronic analog of the earlier 
color-difference film technique. He was honored in 1995 with an Academy Award for lifetime achievement, 
shared with his son Paul. Vlahos makes one observation essential to his work. We shall call it the Vlahos 
Assumption: Blue screen matting is performed on foreground objects for which the blue is related to the 
green by Bo = a2Go. The usual range allowed by his technique is .5 = a2 = 1.5 [20]. That this should 
work as often as it does is not obvious. We shall try to indicate why in this paper. The Vlahos formula 
for ao, abstracted from the claims of his earliest electronic patent [18] and converted to our notation, 
is ao = 1 a1(Bf a2Gf), clamped at its extremes to 0 and 1, where the ai are tuning ad­justment constants 
(typically made available as user controls). We will call this the First Vlahos Form. The preferred embodiment 
described in the patent replaces Bf above with min(Bf, Bk), where Bk is the constant backing color (or 
the minimum such color if its intensity varies, as it often does in practice). In the second step of 
the Vlahos process, the foreground color is further modified be­fore compositing with a new background 
by clamping its blue component to min(Bf, a2Gf). A more general Vlahos electronic patent [20] introduces 
ao = 1 a1(Bf a2(a5 max(r, g) + (1 a5)min(r, g))), where r = a3Rf, g = a4Gf, and the ai are adjustment 
parameters. Clamping again ensures 0 and 1 as limiting values. We shall call this the Second Vlahos Form. 
Again the blue component of the foreground image is modified before further processing. A form for ao 
from a recent patent [4] (one of several new forms) should suffice to show the continued refinements 
intro­duced by Vlahos and his colleagues at Ultimatte Corp.: ao = 1 ((Bf a1) a2 max(r, g) max(a5(Rf 
 Gf), a6(Gf Rf))), with clamping as before. They have continually extended the number of foreground 
objects that can be matted successfully. We believe Vlahos et al. arrived at these forms by many years 
of experience and experiment and not by an abstract mathematical approach such as presented here. The 
forms we derive are related to their forms, as we shall show, but more amenable to analysis. With these 
patents Vlahos defined and attacked several prob­lems of matting: blue spill or blue flare (reflection 
of blue light from the blue screen on the foreground object), backing shadows on the blue screen (shadows 
of the foreground object on the backing, that one wishes to preserve as part of the foreground object), 
and backing impurities (departures of a supposedly pure blue backing screen from pure blue). We shall 
touch on these issues further in later sections. Another contribution to matting [8] is based on the 
following thinking: Find a family of nested surfaces in colorspace that sepa­rate the foreground object 
colors from the backing colors. Each surface, corresponding to a value of ao, is taken to be the set 
of colors that are the ao blend of the foreground and backing colors. See Fig. 4. The Primatte® device 
from Photron Ltd., based on this concept, uses a nested family of convex multi-faceted polyhedra (128 
faces) as separating surfaces. We shall discuss problems with separating surface models in a later section. 
 THE INTRINSIC DIFFICULTY We now show that single-film matting, as typically practiced in a film or video 
effects house, is intrinsically difficult. In fact, we show that there is an infinity of solutions. This 
implies that there is no algorithmic method for pulling a matte from a given fore­ground element. There 
must be a human or perhaps someday a sufficiently intelligent piece of image processing software in the 
loop who knows a correct matte when he (she or it) sees one, and he must be provided with a sufficiently 
rich set of controls that he can successfully home in on a good matte when in the neighborhood of one. 
The success of a matting machine, such as the Ultimatte or Primatte, reduces then to the cleverness of 
its designers in selecting and providing such a set of controls. The argument goes as follows: We know 
that Rf is an inter­polation from Rk to Ro with weight ao, or Rf = Ro + (1 ao)Rk, and that similar relations 
hold for Gf and Bf. This is cf = co + (1 ao)ck in our abbreviated notation. (We ignore the relation 
for af because it is trivial.) A complete solution requires Ro, Go, Bo, and ao. Thus we have three equations 
and four unknowns, an incom­pletely specified problem and hence an infinity of solutions, un­solvable 
without more information. There are some special cases where a solution to the matting problem does exist 
and is simple. SOLUTION 1: NO BLUE If co is known to contain no blue, co = [Ro Go 0], and ck contains 
only blue, ck = [0 0 Bk], then cf =co +- 1 ao )ck =[Ro Go ( -ao )Bk ] ( 1. Thus, solving the Bf = (1 
-ao) Bk equation for ao gives solution . B . Co =.Rf Gf 01 - f ., if Bk .0. B . k . This example is 
exceedingly ideal. The restriction to fore­ground objects with no blue is quite serious, excluding all 
grays but black, about two-thirds of all hues, and all pastels or tints of the remaining hues (because 
white contains blue). Basically, it is only valid for one plane of the 3D RGB colorspace, the RG plane. 
The assumption of a perfectly flat and perfectly blue backing color is not realistic. Even very carefully 
prepared blue screens used in cinema special effects as backings have slight spatial brightness variations 
and also have some red and green impurities (backing impurities). A practical solution for brightness 
varia­tions, in the case of repeatable shots, is this: Film a pass without the foreground object to produce 
a record of Bk at each point to be used for computing Co after a second pass with the object. We rather 
arbitrarily chose pure blue to be the backing color. This is an idealization of customary film and video 
practice (although one sees more and more green screens in video). We shall soon show how to generalize 
to arbitrary and non-constant backing colors and hence do away with the so-called backing impurities 
problem in certain circumstances. SOLUTION 2: GRAY OR FLESH The matting problem can be solved if co is 
known to be gray. We can loosen this claim to say it can be solved if either Ro or Go equals Bo. In fact, 
we can make the following general statement: There is a solution to the matting problem if Ro or Go = 
aBo + bao, and if ck is pure blue with aBk + b .0. To show this, we derive the solution Co for the green 
case, since the solution for red can be derived similarly: The conditions, rewritten in color primary 
coordinates, are: c =[R aB +ba B +- (1 a)B ]. f oo oo ok Eliminate Bo from the expressions for Gf and 
Bf to solve for ao: . Gf -aB .. Co =.RfGf B.+ao Bk ., if aBk + b .0. . aB +b . . k . Here we have introduced 
a very useful definition C.= Cf Ck . The special case Co gray clearly satisfies Solution 2, with a = 
1 and b = 0 for both Ro and Go. Thus it is not surprising that sci­ence fiction space movies effectively 
use the blue screen process (the color-difference technique) since many of the foreground objects are 
neutrally colored spacecraft. As we know from prac­tice, the technique often works adequately well for 
desaturated (towards gray) foreground objects, typical of many real-world objects. A particularly important 
foreground element in film and video is flesh which typically has color [d .5d .5d]. Flesh of all races 
tends to have the same ratio of primaries, so d is the darkening or lightening factor. This is a non-gray 
example satisfying Solution 2, so it is not surprising that the blue screen process works for flesh. 
Notice that the condition Go = aBo + bao, with 2/3 =a =2 and b = 0, resembles the Vlahos Assumption, 
Bo =a2Go. In the special case b = 0, our derived expression for ao can be seen to be of the same form 
as the First Vlahos Form: 1 . 1 . ao =-1 .Bf - Gf .. Bk .a . Thus our Bk is Vlahos 1/ a1 and our a is 
his 1/ a2. Careful read­ing shows that Bk = 1/ a1 is indeed consistent with [18]. By using these values, 
it can be seen that Vlahos replacement of Bf by min(Bf, a2Gf) is just his way of calculating what we 
call Bo. The next solution does not bear resemblance to any technique used in the real world. We believe 
it to be entirely original. SOLUTION 3: TRIANGULATION Suppose co is known against two different shades 
of the backing color. Then a complete solution exists as stated formally below. It does not require any 
special information about co. Fig. 1(a-d) demonstrates this triangulation solution: Let Bk1 and Bk2 be 
two shades of the backing color i.e., B =cB and B =dB for 0 =d < c =1. Assume co is known kk k k 12 
 against these two shades. Then there is a solution Co to the mat­ting problem. N.B., ck2 could be black 
i.e., d = 0. The assumption that co is known against two shades of Bk is equivalent to the following: 
c RGB +- 1) = ( a B f [ooo ok ] 11 . c = oG o (1 a B f [R oB +- o ) k ] 22 The expressions for Bf1 and 
Bf2 can be combined and Bo elimi- B -B 12 nated to show a=- ff , where the denominator is not 0 o 1 
Bk -Bk 12 since the two backing shades are different. Then BB- BB fk fk 21 12 R= R = RG= G = GB= off 
off o 12 12 B- B kk 12 completes the solution. No commonly used matting technique asks that the fore­ground 
object be shot against two different backgrounds. For computer controlled shots, it is a possibility 
but not usually done. If passes of a computer controlled camera are added to solve the problem of nonuniform 
backing mentioned earlier, then the trian­gulation solution requires four passes. Consider the backing 
shadows problem for cases where the triangulation solution applies. The shadow of a foreground object 
is part of that object to the extent that its density is independent of the backing color. For a light-emitting 
backing screen, it would be tricky to perform darkening without changing the shadows of the foreground 
objects. We will give a better solution shortly. Figure 1. Ideal triangulation matting. (a) Object against 
known constant blue. (b) Against constant black. (c) Pulled. (d) Com­posited against new background. 
(e) Object against a known backing (f). (g) Against a different known backing (h). (i) Pulled. (j) New 
composite. Note the black pixel near base of (i) where pixels in the two backings are identical and the 
technique fails. GENERALIZATIONS The preceding solutions are all special cases of the generalization 
obtained by putting the Matting Equation into a matrix form: .100 t1 . .010 t. 2 Co. .=[R. G. B. T], 
001 t 3 . . -R -G -Bt . kkk 4 . where a fourth column has been added in two places to convert an underspecified 
problem into a completely specified problem. Let t=[t1 ttt]. 234 The matrix equation has a solution 
Coif the determinant of the 4x4 matrix is non-0, or tR+tG +tB +t=· 0 tC. . 1 k 2 k 3 k 4 k Standard 
linear algebra gives, since a. =0 always, T-(tR+tG +tB ) TtC tCf-T -· · 1 . 2 . 3 .. a= ==- . o 1 k 
kk tC · tC · tC · Then c =c.+acby the Matting Equation. o ok Thus Solutions 1 and 2 are obtained by 
the following two choices, respectively, for tand T, where the condition on t·Ckis given in parentheses: 
t= [0 0 1 0]; T= 0; (Bk.0) t= [0 -1 ab]; T= 0; (-Gk+ aBk+ b.0). The latter condition reduces to that 
derived for Solution 2 by the choice of pure blue backing color i.e., Gk = 0. We state the gen­eral result 
as a theorem of which these solutions are corollaries: Theorem 1. There is a solution Coto the Matting 
Problem if there is a linear condition t·Co= 0 on the color of the uncomposited foreground object, with 
t·Ck.0. · Proof. T= 0 in the matrix equation above gives a1 tCf =- . ¦ o tC · k The Second Vlahos Form 
can be shown to be of this form with a1 proportional to 1/( t·Ck). Geometrically, Theorem 1 means that 
all solutions Colie on a plane and that Ckdoes not lie on that plane. Solution 3 above can also be seen 
to be a special case of the general matrix formulation with these choices and condition, where by extended 
definition C.=Cf-Ck, i= 1 or 2: i ii t=001 -Bk ; = ;(Bk TB -B .0) ,   [ ]. k 2 212 with C=[00 Bk 1]and 
right side [R GfB. B.]. k f 1 1112 The condition is true by assumption. This solution too is a corol­lary 
of a more general one, Cnot restricted to a shade of blue: k 1 Theorem 2. There is a solution Coto the 
Matting Problem if the uncomposited foreground object is known against two distinct backing colors Cand 
C, where Cis arbitrary, Cis a shade kkkk 121 2 of pure blue, and Bk-Bk .0. 12 Proof. This is just the 
matrix equation above with tand Tas for Solution 3, but with Ckgeneralized to RGB ]1 and the [kk k 1 
11 right side of the matrix equation being [R. G. B. B.]. 1 11 2 B -BB -B .. ff 21 12 Thus, as for Solution 
3, a= 1 =- . ¦ o B -BB -B kk kk 12 12 The following generalization of Theorem 2 utilizes all of the Cbacking 
color information. Let the sum of the color coordi­ k 2 nates of any color Cbe S=R+G+B. aa a aa Theorem 
3. There is a solution Coto the Matting Problem if the uncomposited foreground object is known against 
two distinct backing colors Cand C, where both are arbitrary and kk 12 S-S =(R -R ) +(G -G ) +(B -B ) 
.0. kkkk kk kk 1212 12 12 Proof. Change tand Tin the proof of Theorem 2 to t=111 -Sk ; T=S.  [ ].2 2 
 This gives tC· =S-aS =S -S , which is exactly what oook fk 2 22 you get by adding together the three 
primary color equations in the Matting Equation, C aC =C.. The solution is o-ok 22 S -S S-S .. ff 12 
12 a= 1 =- o S-S S-S kk kk 12 12 (R -R ) +(G -G ) +(B -B ) ff ffff 12 1212 =- , 1 (R -R ) +(G -G ) +(B 
-B ) kk kkkk 12 1212 c=c +ac =c (1 a)c , or c=c (1 a)c . ¦ -- -- o .1 ok f1 ok of2 ok 2 11 The conditions 
of Theorem 3 are quite broad only the sums of the primary color coordinates of the two backing colors 
have to differ. In fact, a constant backing color is not even required. We have successfully used the 
technique to pull a matte on an object against a backing of randomly colored pixels and then against 
that same random backing but darkened by 50 percent. Fig. 1(e-j) shows another application of the technique, 
but Fig. 2 shows more realistic cases. See also Fig. 5. The triangulation problem, with complete information 
from the two shots against different backing colors, can be expressed by this non-square matrix equation 
for an overdetermined system: .1001 0 0 . .0100 1 0 . C = o.0010 0 1 . -R -G -B -R -G -B . kkkk kk. . 
1112 22 . RGBRG B . [.... ..] 1112 22 The Theorem 3 form is obtained by adding the last three columns 
of the matrix and the last three elements of the vector. The standard least squares way [7] to solve 
this is to multiply both sides of the equation by the transpose of the matrix yielding: . 200 -(Rk+Rk) 
. . 12 .020 -(G +G ) . k1 k2 . Co = . 002 -(B +B ). kk . 12 . .-(Rk+Rk) -(Gk+Gk) -(Bk+Bk) .. . 12 1212 
. R +RG +GB +B G [.. ....] 121 212 222 2 22 where .=R +G +B +R +G +B and kkk k kk 111 2 22 G=-(RR +GG 
+BB +RR +GG +BB ). k1 .1 k1 .1 k1 .1 k1 .1 k1 .1 k1 .1 Inverting the symmetric matrix and multiplying 
both sides by the inverse gives a least squares solution Co if the determinant of the 2 22 matrix, 4(( 
R - R ) + (G - G ) + (B - B ) ) , is non-0. kk kkkk 12 1212 Thus we obtain our most powerful result: 
Theorem 4. There is a solution Co to the Matting Problem if the uncomposited foreground object is known 
against two arbitrary backing colors Ck1 and Ck2 with nonzero distance between them 2 22 (R - R ) + (G 
- G ) + (B - B ) .0 (i.e., distinct). kk kkkk 12 1212 The desired alpha ao can be shown to be one minus 
(R - R )( R - R ) + (G - G )( G - G ) + (B - B )( B - B ) f fkk f fkk f fkk 1212 1212 1212 . 2 22 (R 
- R ) + (G - G ) + (B - B ) kk kkkk 12 1212  Figure 2. Practical triangulation matting. (a-b) Two different 
backings. (c-d) Objects against the backings. (e) Pulled. (f) New composite. (g-i) and (j-l) Same triangulation 
process applied to two other objects (backing shots not shown). (l) Object com­posited over another. 
The table and other extraneous equipment have been garbage matted from the shots. See Fig. 5. The Theorem 
3 and 4 expressions for ao are symmetric with re­spect to the two backings, reflected in our two expressions 
for co (in the proof of Theorem 3). Theorems 2 and 3 are really just special cases of Theorem 4. For 
Theorem 2, the two colors are required to have different blue coordinates. For Theorem 3, they are two 
arbitrary colors that do not lie on the same plane of constant S. In practice we have found that the 
simpler conditions of Theorem 3 often hold and permit use of computations cheaper than those of Theorem 
4. Theorem 4 allows the use of very general backings. In fact, two shots of an object moving across a 
fixed but varied back­ground can satisfy Theorem 4, as indicated by the lower Fig. 1 example. If the 
foreground object can be registered frame to frame as it moves from, say, left to right, then the background 
at two different positions can serve as the two backings. Notice that the Theorem 3 and 4 techniques 
lead to a backing shadows solution whereas simple darkening might not work. The additional requirement 
is that the illumination levels and light­emitting directions be the same for the two backing colors 
so that the shadows are the same densities and directions. The overdetermined linear system above summarizes 
all in­formation about two shots against two different backing colors. A third shot against a third backing 
color could be included as well, replacing the 4×6 matrix with a 4×9 matrix and the 1×6 right­hand vector 
with a 1×9 vector. Then the same least squares solu­tion technique would be applied to find a solution 
for this even more overdetermined problem. Similarly, a fourth, fifth, etc. shot against even more backing 
colors could be used. An overdeter­mined system can be subject to numerical instabilities in its solu­tion. 
We have not experienced any, but should they arise the tech­nique of singular value decomposition [11] 
might be used. IMPLEMENTATION NOTES The Fig. 1(a-d) example fits the criteria of Theorem 2 (actually 
the Solution 3 special case) perfectly because the given blue and black screen shots were manufactured 
by compositing the object over perfect blue and black backings. As predicted by the theo­rem, we were 
able to extract the original object in its original form, with only small least significant bit errors. 
Similarly Fig. 1(e-j) illustrates Theorem 3 or 4. Fig. 2 is a set of real camera shots of real objects 
in a real stu­dio. Our camera was locked down for the two shots required by Theorem 3 and 4 plus two 
more required for backing color cali­brations as mentioned before. Furthermore, constant exposure was 
used for the four shots, and a remote-controlled shutter guarded against slight camera movements. The 
results are good enough to demonstrate the effectiveness of the algorithm but are nevertheless flawed 
from misregistration introduced during the digitization process pin registration was not used and from 
the foreground objects having different brightnesses relative one another, also believed to be a scanning 
artifact. Notice from the Theorem 3 and 4 expressions for ao that the technique is quite sensitive to 
brightness and misregistration er­rors. If the foreground colors differ where they should be equal, then 
ao is lowered from its correct value of 1, permitting some object transparency. In general, the technique 
tends to err towards increased transparency. Another manifestation of the same error is what we term 
the fine line problem. Consider a thin dark line with bright sur­roundings in an object shot against 
one backing, or the comple­ment, a thin bright line in a dark surround. Such a line in slight misregister 
with itself against the other backing can differ dra­matically in brightness at pixels along the line, 
as seen by our algorithm. The error trend toward transparency will cause the appearance of a fine transparent 
line in the pulled object. The conclusion is clear: To effectively use triangulation, pin­registered 
filming and digitization should be used to ensure posi­tional constancy between the four shots, and very 
careful moni­toring of lighting and exposures during filming must be under­taken to ensure that constant 
brightnesses of foreground objects are recorded by the film (or other recording medium). Since triangulation 
works only for non-moving objects (excluding rigid motions, such as simple translation), it should be 
possible to reduce brightness variations between steps of the process due to noise by averaging several 
repeated shots at each step. A LOWER BOUND The trouble with the problems solved so far is that the premises 
are too ideal. It might seem that the problems which have Solu­tions 1 and 2, and Theorem 1 generalizations, 
are unrealistically restrictive of foreground object colors. It is surprising that so much real-world 
work approaches the conditions of these solu­tions. Situations arising from Solution 3, and Theorems 
2-4 gen­eralizations, require a doubling of shots, which is a lot to ask even if the shots are exactly 
repeatable. Now we return to the general single-background case and derive bounds on ao that limit the 
search space for possible solutions. Any Co offered as solution must satisfy the physical limits on color. 
It must be that 0 = Ro =ao (since Ro is premultiplied by ao) and similarly for Go and Bo. The Matting 
Equation gives Rf = Ro + (1 ao)Rk. The inequalities for Ro applied to this expression give (1 -a )R 
= R = (1 -a )R +a , okf oko with the left side being the expression for Ro = 0 and the right for Ro 
= ao. Similar inequalities apply to Gf and Bf. Fig. 3 shows all regions of valid combinations of ao, 
Rf, Gf, and Bf using equality in the relationship(s) above as boundaries. The color ck for this figure 
is taken to be the slightly impure blue [.1 .2 .98]. The dashed vertical lines in Fig. 3 represent a 
given cf in this figure, [.8 .5 .6]. The dotted horizontal lines represent the mini­mum ao for each of 
Rf, Gf, and Bf which gives a valid Ro, Go, and Bo, respectively. Let these three ao s be called aR, aG, 
and aB. Since only one ao is generated per color, the following relation­ship must be true: ao = max(aR, 
aG, aB). We shall call the ao which satisfies this relationship at equality amin, and any ao =amin will 
be called a valid one. Notice that al­though the range of possible ao s is cut down by this derivation, 
there are still an infinity of valid ones to choose from, hence an infinity of solutions. If Rf > Rk, 
as in the Fig. 3 example, then aR corresponds to Ro = ao, the right side of the inequalities above for 
Rf and ao. If Rf < Rk then aR corresponds to Ro = 0, the left side. Thus . R .1- f , if Rf < Rk . k R 
. R . aR =.1- R , if Rf > Rk . . k . . fk 0, if R = R . In the example of Fig. 3, amin .78. For the 
special case of pure blue backing, amin = max(Rf, Gf, 1 Bf). So long as a valid ao exists, a foreground 
object color can be derived from the given cf by c =c.+ac as before. o ok  AN UPPER BOUND Tom Porter 
pointed out (in an unpublished technical memo [10]) that an upper bound could also be established for 
ao, by taking lessons from Vlahos. The Vlahos Assumption, when valid, has Bo=a2Go. The rear­rangement 
of the Matting Equation above for the green channel is Go =Gf (1 ao)Gk. -- Another rearrangement, this 
time for the blue channel, gives us B-B aG-B of 2 of a=+1 =+ . o 1 kk BB Combining these two, by substituting 
the equation for Gointo the inequality for aoand solving, gives B -aG f 2 f a=- , o 1 k 2 k B-aG clamped 
to [0, 1] if necessary. Recall that .5 =a2=1.5 typically. Let aoat equality be amax. Then, in our Fig. 
3 example, a2= 1 yields amax .87, which constrains the possible solutions a bit more: .78 =ao=.87. BLUE 
SPILL Vlahos tackled the very important blue spill (blue flare) problem of backing light reflecting off 
the foreground object in [19]. He solved it for an important class of objects, bright whites and flesh 
tones, by making what we call the Second Vlahos Assumption: Foreground objects have max(Bo-Go, 0) =max(Go-Ro, 
0). If this is not true, the color is assumed to be either the backing color or flare from it. Object 
transparency is taken, as before, to be pro­portional to Bo-Go, and this distinguishes the two cases. 
Our statement of the Matting Problem needs to be altered to include the blue spill problem. Our current 
model says that the foreground color Cfis a linear combination of the uncomposited foreground object 
color Coand the backing color Ck, Cf = Co+ (1 ao)Ck. The Extended Matting Problem would include a term 
Cs for the backing spill contribution. For example, it might be mod­eled as a separate foreground object, 
with its own alpha as, in linear combination with the desired foreground object color Co: Cf = Cs+(1 
 as)(Co+ (1 ao)Ck). Now the problem becomes the more difficult one of determining both Csand Cofrom 
the given information Cfand Ck. A simplification is to assume that the spill color is the same as the 
backing color, Cs = asCk. Thus Cf= (1 as)Co+ (1 ao+ aoas)Ck. For brevity, let C.=C./(1 -as) . Then 
this spill s model can be put into a matrix equation of the same form as be­fore (but notice the as= 
1 singularity): .100 t1 . C. 2 .=RGBT. o . . 00 01 10 tt . [.ss .s ] 3 . . -R -G -Bt . k kk 4 . Hence, 
since a.=0 always, the solutions are of the same form as s TtC -· .s before: a= and c=c.+ac. This does 
not solve o ook s · tC k the problem since asis still unknown. We shall not pursue the spill problem 
further here but recommend it for future research. SEPARATING SURFACE PROBLEMS Fig. 4 illustrates the 
separating surface approach to the general matting problem. A single plane of colorspace is shown for 
clar­ity. A family of three separating surfaces for different values of ao have been established between 
the body of backing colors Ckand the body of object colors Co. A given foreground color Cfis shown at 
the point of intersection with the ao= .5 locus along the straight line through object colors Aand B. 
The Vlahos (or Ultimatte) matting solutions can be cast into the separating surface model. In the First 
Vlahos Form (as well as in our Solutions 1 and 2 and Theorem 1), each dotted line of Fig. a 1 amax a=a 
R min 0 R 0Rk Rf1 a 1 aG 0 G 0GkGf 1 a 1 aB 0 B 0 Bf Bk Figure 3. Shaded areas show solution space. 
Black areas are constrained by upper and lower alpha limits to valid alphas for the given foreground 
color. Valid alphas for Co lie along intersection of Cf (dashed lines) with black areas. 4 would simply 
be a straight line (a plane in RGB). In the Second Vlahos Form it would be a line with two straight segments 
(two polygons sharing an edge in RGB). The third form simply adds a third segment (polygon) to this shape. 
The Primatte solution ex­tends this trend to many (up to 128) segments (faces of a convex polyhedron). 
Fig. 4 illustrates a general problem with the separating surface model. All mixtures of A with the backing 
color will be correctly pulled if they indeed exist in the foreground object. However, all mixtures of 
B with the backing will not be correctly pulled be­cause they have been disguised as mixtures of A. Another 
problem is that it is not always possible to have fore­ground object colors disjoint from backing colors. 
Another is the assumption that a locus of constant ao is a surface rather than a volume, connected rather 
than highly disconnected, and planar or convex. SUMMARY The expiration of the fundamental Vlahos patents 
has inspired us to throw open the very interesting class of constant color matting problems to the computer 
graphics community. Thus one of our purposes has been to review the problems of the field the gen­eral 
one of pulling a matte from a constant color shot plus related subproblems such as blue spill, backing 
impurities, and backing shadows. The mathematical approach introduced here we believe to be more understandable 
than the ad hoc approach of the Vlahos pat­ents, the standard reference on blue screen matting. Furthermore, 
we believe that the treatment here throws light on why the process should work so well as it does in 
real-world applications (gray, near-gray, and flesh tones), surprising in light of the proof herein that 
the general problem has an infinity of solutions. Consistent with the lack of a general algorithmic solution 
is the fact that hu­man interaction is nearly always required in pulling a matte in film or video. Our 
principal idea is that an image from which a matte is to be pulled can be represented by a model of two 
images, an uncom­posited foreground object image (a sprite) and a backing color Magenta Red  Blue Black 
 Figure 4. A slice through a (non-convex) polyhedral family of surfaces of constant alpha separating 
backing colors from foreground object colors. Given color Cf will be inter­preted as A with alpha of 
.5 whereas the object might actu­ally be B with an alpha of .25. image, linearly combined using the alpha 
channel of the fore­ground object. Our main results are deduced from this model. In each case, the expression 
for the desired alpha channel a o is a function of the two images in the model, Cf, the given image a 
composite by our model and Ck, the given backing image. This may be compared to the Vlahos expressions 
for alpha which are functions of the given image Cf only. We have introduced an algorithmic solution, 
the triangulation solution, by adding a new step to the blue screen process as usu­ally practiced: Another 
shot of the foreground object against a second backing color. This multi-background technique cannot 
be used for live actors or other moving foreground objects because of the requirement for repeatability. 
Whenever it is applicable, how­ever, it is powerful, the only restriction on the two backings being that 
they be different pixel by pixel. Hence the backing colors do not even have to be constant or pure the 
backing impurities problem does not exist. However, to solve the backing shadows problem, illumination 
level and direction must be the same for both backings, particularly important if they are generated 
by light emission rather than reflection. We have bounded the solution space for the general non­algorithmic 
problem, a new extension to the Vlahos oeuvre. Hopefully, this will inspire further researches into this 
difficult problem. See the Vlahos patents (including [4] and [5]) for further inspiration. We have touched 
on the blue spill (blue flare) problem and suggest that additional research be aimed at this important 
prob­lem. We have sketched a possible model for this research, gener­alizing the idea of the given image 
being a composite of others. In particular, we propose that the idea of modeling blue spill by an additional 
blue spill image, with its own alpha, might lead to fur­ther insight. Finally, we have briefly reviewed 
the modeling of the matting problem with separating surface families (cf. [8]), shown how to cast the 
Vlahos work in this light, and discussed some problems with the general notion. We urge that this class 
of solutions be further explored and their fundamental problems be elucidated beyond the initial treatment 
given here.  ACKNOWLEDGMENTS To Tom Porter for his alpha upper limit. To Rob Cook for an early critical 
reading of [12] and [13] on which much of this paper is based. To Rick Szeliski for use of his automatic 
image registra­tion software. To Jack Orr and Arlo Smith for studio photography and lighting. REFERENCES 
[1] BEYER, W. Traveling Matte Photography and the Blue Screen System. American Cinematographer, May 1964, 
p. 266. The second of a four-part series. [2] BLINN, J. F. Jim Blinn s Corner: Compositing Part 1: Theory. 
IEEE Computer Graphics &#38; Applications, September 1994, pp. 83-87. [3] BLINN, J. F. Jim Blinn s Corner: 
Compositing Part 2: Prac­tice. IEEE Computer Graphics &#38; Applications, November 1994, pp. 78-82. [4] 
DADOURIAN, A. Method and Apparatus for Compositing Video Images. U. S. Patent 5,343,252, August 30, 1994. 
[5] FELLINGER, D. F. Method and Apparatus for Applying Cor­rection to a Signal Used to Modulate a Background 
Video Signal to be Combined with a Foreground Video Signal. U. S. Patent 5,202,762, April 13, 1993. 
[6] FIELDING, R. The Technique of Special Effects Cinematogra­phy. Focal/Hastings House, London, 3rd 
edition, 1972, pp. 220-243. [7] LANCZOS, C. Applied Analysis. Prentice Hall, Inc., Engle­wood Cliffs, 
NJ, 1964, pp. 156-161. [8] MISHIMA, Y. A Software Chromakeyer Using Polyhedric Slice. Proceedings of 
NICOGRAPH 92 (1992), pp. 44-52 (Japanese). See http://206.155.32.1/us/primatte/whitepaper. [9] PORTER, 
T. and DUFF, T. Compositing Digital Images. Pro­ceedings of SIGGRAPH 84 (Minneapolis, Minnesota, July 
23-27, 1984). In Computer Graphics 18, 3 (July 1984), pp. 253-259. [10] PORTER, T. Matte Box Design. 
Lucasfilm Technical Memo 63, November 1986. Not built. [11] PRESS, W. H., TEUKOLSKY, S. A., VETTERLING, 
W. T., and FLANNERY, B. P. Numerical Recipes in C. Cambridge Uni­versity Press, Cambridge, 1988, p. 59. 
[12] SMITH, A. R. Analysis of the Color-Difference Technique. Technical Memo 30, Lucasfilm Ltd., March 
1982. [13] SMITH, A. R. Math of Mattings. Technical Memo 32, Lucas­film Ltd., April 1982. [14] SMITH, 
A. R. Image Compositing Fundamentals. Technical Memo 4, Microsoft Corporation, June 1995. [15] SMITH, 
A. R. Alpha and the History of Digital Compositing. Technical Memo 7, Microsoft Corporation, August 1995. 
[16] VLAHOS, P. Composite Photography Utilizing Sodium Vapor Illumination. U. S. Patent 3,095,304, May 
15, 1958. Expired. 10 [17] VLAHOS, P. Composite Color Photography. U. S. Patent 3,158,477, November 
24, 1964. Expired. [18] VLAHOS, P. Electronic Composite Photography. U. S. Patent 3,595,987, July 27, 
1971. Expired. [19] VLAHOS, P. Electronic Composite Photography with Color Control. U. S. Patent 4,007,487, 
February 8, 1977. Expired. [20] VLAHOS, P. Comprehensive Electronic Compositing System. U. S. Patent 
4,100,569, July 11, 1978. Expired. [21] VLAHOS, P. and TAYLOR, B. Traveling Matte Composite Photography. 
American Cinematographer Manual. Ameri­can Society of Cinematographers, Hollywood, 7th edition, 1993, 
pp. 430-445. [22] VLAHOS, P. Comprehensive Electronic Compositing System. U. S. Patent 4,344,085, August 
10, 1982. [23] VLAHOS, P. Encoded Signal Color Image Compositing. U. S. Patent 4,409,611, October 11, 
1983. [24] VLAHOS, P., VLAHOS, P. and FELLINGER, D. F. Automatic Encoded Signal Color Image Compositing. 
U. S. Patent 4,589,013, May 13, 1986. [25] VLAHOS, P. Comprehensive Electronic Compositing System. U. 
S. Patent 4,625,231, November 25, 1986.  Figure 5. A composite of nine image sprites pulled from studio 
photographs using the triangulation technique shown in Fig. 2. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237264</article_id>
		<sort_key>269</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Combining frequency and spatial domain information for fast interactive image noise removal]]></title>
		<page_from>269</page_from>
		<page_to>276</page_to>
		<doi_number>10.1145/237170.237264</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237264</url>
		<keywords>
			<kw><![CDATA[POCS]]></kw>
			<kw><![CDATA[projections into convex sets]]></kw>
			<kw><![CDATA[scratch and wire removal]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computation of transforms (e.g., fast Fourier transform)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003717</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computation of transforms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P19106</person_id>
				<author_profile_id><![CDATA[81100297226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anil]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Hirani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation, Research Center, Sony Corporation, 6-7-35 Kitashinagawa Shinagawa-ku, Tokyo 141, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31097598</person_id>
				<author_profile_id><![CDATA[81332532185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Totsuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation, Research Center, Sony Corporation, 6-7-35 Kitashinagawa Shinagawa-ku, Tokyo 141, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[FERREIRA, RJ.S.G. and PINHO, A.J. "Errorless Restoration Algorithms for Band-Limited Images", Proc. IEEE Intl. Conf. Image Proc. (ICIP), III, 157-161, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[FERREIRA, EJ.S.G. "Interpolation and the Discrete Papoulis- Gerchberg Algorithm", IEEE Trans. Sig. Proc., 42, No. 10, 2596- 2606, Oct 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[FIENUR J.R. "Phase Retrieval Algorithms: a Comparison", Appl. Opt., 21, No. 15, 2758-2769, 1982.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[HEEGER, D.J. and BERGEN, J.R. "Pyramid-Based Texture Analysis/Synthesis", Proc. SIGGRAPH 95, 229-238, 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[MALZBENDER, T. and SPACH, S. "A Context Sensitive Texture Nib", Communicating with Virtual Worlds, Thalmann, N.M. and D. Thalmann (eds.), Springer-Verlag Tokyo, 1993.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[PAPOULIS, A. "A New Algorithm in Spectral Analysis and Band- Limited Extrapolation", IEEE Trans. Cir. &amp; Sys., 22, No. 9, 735- 742, 1975.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[PITAS, I. and VENETSANOPOULOS, A.N. "Order Statistics in Digital Image Processing", Proc. IEEE, 80, No. 12, Dec 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[SEZAN, M. I. and STARK, H. "Image Restoration by the Method of Convex Projections: Part 2 - Applications and Numerical Results", IEEE Trans. Med. Imag., 1, No. 2, 95-101, 1982.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[STROHMER, T. "On Discrete Band-Limited Signal Extrapolation", In Mathematical Analysis, Wavelets, and Signal Processing, Ismail, M. et. al. (eds.), AMS Contemporary Mathematics, 190, 323-337, 1995. Also available from http://tyche.mat.univie.ac.at, the home page of NUHAG in University of Vienna.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2321997</ref_obj_id>
				<ref_obj_pid>2319109</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[SUN, H. and KWOK, W. "Concealment of Damaged Block Transform Coded Images Using Projections onto Convex Sets", IEEE Trans. Image Proc., 4, No. 4, April 1995.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[YOULA, D.C. and WEBB, H. "Image Restoration by the Method of Convex Projections: Part 1 - Theory", IEEE Trans. Med. Imag., 1, No. 2, 81-94, 1982.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Combining Frequency and Spatial Domain Information for Fast Interactive Image Noise Removal Anil N. 
Hirani, Takashi Totsuka Sony Corporation Abstract Scratches on old .lms must be removed since these 
are more notice­able on higher de.nition and digital televisions. Wires that suspend actors or cars must 
be carefully erased during post production of special effects shots. Both of these are time consuming 
tasks but can be addressed by the following image restoration process: given the locations of noisy pixels 
to be replaced and a prototype image, restore those noisy pixels in a natural way. We call it image noise 
removal and this paper describes its fast iterative algorithm. Most existing algorithms for removing 
image noise use either frequency domain information (e.g low pass .ltering) or spatial domain infor­mation 
(e.g median .ltering or stochastic texture generation). The few that do combine the two domains place 
the limitation that the image be band limited and the band limits be known. Our algorithm works in both 
spatial and frequency domains without placing the limitations about band limits, making it pos­sible 
to fully exploit advantages from each domain. While global features and large textures are captured in 
frequency domain, local continuity and sharpness are maintained in spatial domain. With a judicious choice 
of operations and domains in which they work, our dual-domain approach can reconstruct many contiguous 
noisy pixels in areas with large patterns while maintaining continuity of features such as lines. In 
addition, the image intensity does not have to be uniform. These are signi.cant advantages over exist­ing 
algorithms. Our algorithm is based on a general framework of projection onto convex sets (POCS). Any 
image analysis technique that can be described as a closed convex set can be cleanly plugged into the 
iteration loop of our algorithm. This is another important advantage of our algorithm. CR Categories: 
I.3.3 [Computer Graphics]: Picture / Image Gen­eration; Display Algorithms; I.3.6 [Computer Graphics]: 
Method­ology and Techniques Interaction techniques; I.4.4 [Image Pro­cessing]: Restoration; I.4.9 [Image 
Processing]: Applications. Additional Keywords: scratch and wire removal, projections onto convex sets, 
POCS. fhirani | totsukag@av.crl.sony.co.jp Research Center, Sony Corporation 6-7-35 Kitashinagawa Shinagawa-ku, 
Tokyo 141, Japan Permission to make digital or hard copies of part or all of this work or personal or 
classroom use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 1 INTRODUCTION The proliferation of television 
channels and increasing use of mul­timedia viewing platforms means that older .lms are likely to see 
increased use. In addition, higher de.nition and digital television formats mean that imperfections in 
old .lm stock are going to be­come more noticeable. Removal of scratches from old .lms and photographs 
is one motivation for this paper. Another motivation comes from needs of .lm and video post production. 
In some special effects scenes in .lms, actors or objects are suspended from wires. These wires are later 
removed in post production either by using an optical process or by processing the digitized .lm. The 
digital process is much more commonly used now. As a result, increasing the ef.ciency of tools for digital 
wire removal has become important. All these factors indicate the need for ef.cient and accurate tools 
for removing scratch, wire and other unwanted noise from images. All these problems can be addressed 
by the following image restoration process. Given (i) the locations of noisy pixels and (ii) a proto­ 
type (sample) image, restore those noisy pixels in a nat­ ural way. By natural, we mean that the continuity 
of intensity and features (e.g., textures, lines) with the surrounding area is maintained. For the scratch 
removal and the wire removal applications described above, pixels to be restored are those under a scratch 
or a wire, and the sample image is usually taken from a nearby region. In this paper, we refer to this 
image restoration process as image noise removal . Although image restoration is not a new concept, existing 
noise removal algorithms have dif.culty with noise which (i) consists of many contiguous pixels and (ii) 
is in a textured area of image or areas with prominent lines. Note that by texture we mean not only small 
stochastic texture but also small patterns like fabric texture as shown in Fig. 10(b). In addition there 
can be prominent system­atic lines or lines placed randomly in the image. The brick wall in Fig. 10(a) 
and the stone wall in Fig. 10(c) are examples. Our algo­rithm for removal of noise is based on the theory 
of projections onto convex sets. Ours is a fast iterative algorithm that uses the available information 
from both frequency and spatial domain. The pixels determined by the algorithm to replace the noise are 
(a) as sharp as the surrounding area (b) maintain continuity of prominent lines running across the noise 
pixels and (c) haveatex­ture matching the surrounding texture. While some previous algo­rithms were able 
to remove such noise from images with stochastic texture or small regularly patterned textures, ours 
works on those as well as the more dif.cult cases of systematic or randomly placed prominent lines. To 
our knowledge it is the .rst application of POCS for interactive image noise removal. Ours is also the 
.rst image noise removal algorithm that combines frequency and spa­tial domain information in an extendible 
way. It does this by using the clean and well understood formalism of POCS and without re­quiring that 
the images be band limited. In addition it works even when the noisy pixels are contiguous and numerous. 
Another key  a bc Figure 1: Problems with copying from another area of image. (a) Image with noise (noise 
is the black diagonal line). (b) The area s1has been copied into the area r1in an attempt to cover up 
the noise. Note the alignment and shading mismatch. The horizontal cement line appears broken in r1now 
and the area is darker than surrounding. (c) Using a cloning brush the area s2has been sampled and copied 
carefully into r2to maintain alignment of the horizontal cement line. But such a tool cannot solve the 
problems of mismatched intensity. The cloning source is darker than the destination area. Results of 
order statistics non linear .lters like median and results of low pass .lters are not shown here. But 
they do not work on images and types of noise shown here. See text for detail. advantage of our algorithm 
is that the image intensity does not have 2.2 Spatial Domain Only to be uniform across the image. One 
problem shared by all spatial-only methods is that they have lo-The rest of the paper is organized as 
follows. Section 2 summa­cal control and information but do not have any information about rizes related 
previous works. Following a brief overview of POCS the global structure of the image. The limitation 
to local neighbor­in section 3, our algorithm is described in section 4. Section 5 hood is due to practical 
computational constraints in some cases. shows results of our algorithm and section 6 gives conclusions 
and In addition, some of these methods like median .ltering etc. are possible future directions. inherently 
incapable of using the global information meaningfully. Cloning tools of popular commercial image manipulation 
pro­  2 PREVIOUS WORK grams allow copying from another area of the image using brush like strokes. However, 
aligning reconstructed lines with existing Previous work on image noise removal can be divided into intra­lines 
by this method is time consuming and error prone. An even frame and inter-frame techniques depending 
on where the informa­bigger problem is when the image has uneven intensity due to light­tion needed for 
removal comes from. Inter-frame algorithms copy ing conditions or inter re.ections. In such cases, .nding 
the same needed pixels from preceding or succeeding frames. They may intensity source area can be dif.cult. 
compensate for motion of object or camera by tracking key points Fig. 1 demonstrates these problems with 
copying from another of an image. Inter-frame methods fail when scratches run across area of the image. 
Fig. 1(b) shows how the shading as well as the many frames (such scratches are common because of the 
vertical alignment can be different between source (s1) and destination (r1)motion of the .lm through 
projectors) or when there is too much areas. Fig. 1(c) shows how careful use of a manual cloning tool 
camera activity. In either of these cases, the needed pixels cannot can somewhat ameliorate the alignment 
problem in this case. But be found easily in the preceding or succeeding frames. without using the frequency 
domain information, such a tool can do This paper describes an intra-frame algorithm assuming that nothing 
about shading mismatch as shown in the .gure. other frames do not have the needed pixels. Previous intra-frame 
A recent survey of median and similar non-linear order statis­methods can be further classi.ed as follows 
based on which infor­tics .lters describes the advantages and shortcomings of these [7]. mation they 
use. The problem with order statistics .lters is when not enough cor­ 1. Use frequency domain information 
only (e.g low pass .lter).  rect information is available for meaningful order statistics. This is 
2. Use spatial domain information only typically the case when numerous contiguous pixels are noisy. 
Spatial domain texture synthesis algorithms [5] have shown re­ (a) Median and similar order statistics 
.lters. markable results for stochastic type or small regular texture. How­(b) Spatial statistical texture 
synthesis. ever, these methods fail when the image has long range structure (c) Cloning by copying pixels. 
as in the image of brick wall in Fig. 10(a). The computational cost 3. Use spatial and frequency domain 
information increases prohibitively for long range image structure because such algorithms use second 
order statistics. (a) Projections onto convex sets for band limited images. (b) Matrix algorithms for 
band limited images.  (c) Spatial and frequency based statistical texture synthesis. 2.3 Spatial And 
Frequency Domains  In the case of texture synthesis it is possible to use multi-resolution 2.1 Frequency 
Domain Only directional .lters and then work with only single order statistics Frequency domain algorithms 
such as low pass .ltering can capture (histograms) as in [4]. This could be considered a spatial and 
fre­global structure of the image but lose local control (line continu-quency domain algorithm. However 
this method works only for ity, sharpness). As a result lines and other details become blurred. stochastic 
texture or small regular texture. In addition [4] is not a Since human visual system is very sensitive 
to details of an image noise removal algorithm. It is designed for generating large texture like those 
conveyed by the lines, the result is unacceptable for re-areas from sample images. It cannot be used 
to generate pixels that moving noise that consists of many contiguous pixels. We have not maintain continuity 
of prominent lines crossing noise pixels while shown results of low pass .ltering here because such .lters 
per-retaining the noise free pixels. form very poorly for the kind of noise and images shown in Fig. 
10 Gerchberg-Papoulis and related algorithms [6, 2] are POCS (many contiguous noisy pixels in textured 
areas or in textured areas based algorithms that use frequency and spatial domain informa­with prominent 
lines). tion. However, they work only for band limited images and the  Convex set 2 Region of acceptable 
images Figure 2: Pictorial representation of POCS. See section 3 for details. band limits must be known. 
In addition, recent extensions like [10] require expensive calculations of lines that intersect noise 
pixels. In other recent work [1] shows how to reduce band-limited inter­polation and extrapolation problems 
for .nite-dimensional signals to solution of a set of linear equations. Further, they show that the corresponding 
matrix is positive-de.nite with a spectral radius less than 1. The authors then derive properties of 
convergence of algo­rithms for different types of noises. Another matrix based method is [9]. These methods 
require that the image be band limited and the limits be known.  3 PROJECTIONS ONTO CONVEX SETS Papoulis 
[6] introduced an algorithm for reconstructing band lim­ited signals by alternating between signal and 
transform domains and applying the constraints of each domain. The constraints are preservation of known 
pixel values and enforcement of band limits. This approach was later generalized and given a geometric 
interpre­tation. A further generalization [11, 8] has come to be known as the method of projections onto 
convex sets (POCS). It allows the use of any information about the image (or any other signal) as long 
as the information can be represented as a closed convex set. Although the POCS theory was developed 
in the context of Hilbert spaces, for digital image restoration, it is convenient to restrict our atten­tion 
to .nite dimensional spaces. This space might be, for example, the space of all MxNcomplex matrices where 
the image has M rows and Ncolumns. Given a set Cin such a space, x;y2C,we say the Cis con­vex iff for 
any 0:f:1;fx+1 f y2C. Cis closed if it contains all its limit points. See [11] for details. We' ll use 
the words closed convex set and constraints interchangeably since the only constraints we' ll be working 
with will select closed con­vex sets from a larger set. Projection onto a convex set consists of .nding 
an image satisfying the constraint and closest to the im­age being projected. Intuitively, this can be 
thought of as making the least possible change to satisfy the constraints. See Fig. 2 for a pictorial 
representation of POCS. Repeated projection onto all the convex sets is guaranteed to .nd an image that 
satis.es all the constraints if at least one such image exists. See the classic Youla and Webb paper 
[11] for more details. The advantages of POCS come from the fact that .nding a direct projection onto 
the desired intersection is usually very dif.cult and expensive, while an ef.cient projection onto each 
set is more likely to be found. This is why formulating and solving a problem as POCS can be quite attractive 
computationally. Note that POCS is a general algorithm, with potential applications in many areas besides 
image restoration.  4 OUR ALGORITHM The Fourier transform is an integration over the entire signal. 
Af­ter a transform, many of the essential global features of an image Image with noise User paints mask 
Binary noise mask on noise passed to algorithm Figure 3: The creation of noise mask for algorithm A1. 
Left image shows the actual noise (the dark uneven line running diagonally). Middle image shows the image 
with a binary mask that user has painted over the noise, to cover the noise. This does not have to be 
a straight line or rectangle, although it happens to be so in this case. The right most image shows the 
binary mask that will be passed to the algorithm. The middle image will become the r0input of the algorithm. 
See section 4 and Fig. 4 for details. become localized, i.e come closer in the spectrum. These can in­clude 
repeating patterns, overall image intensity, slow variation in intensity due to inter re.ection or shading 
etc. On the other hand, rapidly varying stochastic texture or sharpness of lines and edges appear scattered 
in the spectrum. These are features that are local­ized in the spatial domain. Clearly there is a need 
to combine these two for noise removal and texture synthesis. As we will show, POCS is a way of doing 
this in a clean and extendible fashion. In this section we .rst describe our basic algorithm A1, which 
combines the frequency and spatial domains in a POCS framework. Then we show how the use of POCS allows 
us to easily and cleanly extend A1 to solve important practical problems. The ef.ciency of the algorithm 
comes from the fact that each iteration requires fast operations on small subimages, not on the entire 
image. 4.1 Information Needed No algorithm can restore an image or generate new texture without information, 
every algorithm needs some hint. Existing algorithms take a sample subimage (can be the entire image 
in some cases), which is usually taken from nearby pixels, analyze it, and extract hint information. 
Our algorithm is no exception. It also needs some hint. In our case, a neighborhood of the noise (called 
repair subimage) is se­lected by the user to provide hint about the local spatial informa­tion. A nearby 
or similar subimage (called sample subimage) is selected by the user to provide a hint for the frequency 
information. The noise is located by the user creating a binary mask that covers it completely (the mask 
can be larger than the actual noise). Ex­ample of binary noise mask can be seen in Fig. 3. The black 
line in the noisy images in Fig. 10 can also be thought of as the noise mask covering the actual noise 
underneath. The algorithm starts with these images as the noisy image input. This does not necessarily 
mean we need more information. We use two subimages, one for extracting global features, and one for 
maintaining local continuity. The algorithm does not place any re­striction on choosing the location 
of the sample subimage. If these pieces of information can be obtained from one place, the sam­ple and 
the repair subimages can overlap as in several subimages shown in Fig. 6 and 7. Or they can be far apart 
as in the case of some subimages in the brick wall, the .rst image in Fig. 6.  4.2 Base Algorithm (A1) 
Fig. 4 gives a .owchart of the base algorithm A1. First, the global features are restored. This is best 
done in frequency domain by cor­ Figure 4: Details of our base algorithm A1. See section 4 for details. 
recting the spectrum magnitude. The .rst step is to Fourier trans­form the repair and sample subimages. 
Since the repair-spectrum is corrupted due to noise, the sample-spectrum is used as a template for improving 
the repair-spectrum. It is very important to use this sample information correctly and this is where 
the theory of convex projections is important. The information must be represented as a convex set and 
an orthogonal projection to this set must be used. In addition this must be an ef.cient projection. Several 
obvious ways of using the sample-spectrum are actu­ally incorrect, in that they will yield algorithms 
that diverge because of non convexity. For example, one may think of using the sample spectrum to replace 
the repair spectrum completely. The practical objection to this is that good information is thrown away 
along with the bad. Theoretically too, this is unworkable because replacing the spectrum magnitude is 
not a projection onto a convex set. Another plausible improvement might be to use a mixture of the two 
spectra, perhaps weighted by an aand 1arespectively, where ais chosen by the user. This too leads to 
a diverging algorithm which is incapable of noise removal. Again the reason is non con­vexity of the 
underlying set. Other plausible choices like using the high peaks of the sample spectrum etc. are also 
incorrect due to the same reason. Before we start the description of our projections and convex sets, 
a note about notation. In the following equations r0is the starting repair subimage multiplied by the 
binary noise mask and sis the sample subimage (thus these are real matrices). ris an arbitrary complex 
matrix. r0, sand rall have the same dimension. Rand Sare the Fourier transforms of rand srespectively 
i.e in a typical implementation R=FFT r, S=FFT swhere FFT stands for the Fast Fourier Transform operation. 
IFFT is the inverse FFT. ab c d e f Figure 5: Part of one iteration of A1 in spatial and frequency domains. 
(a)-(c) in .rst row show magnitudes of the Fourier transforms; (a) Repair subimage with binary noise 
mask ; (b) Sample subimage ; (c) Minimum of the .rst two, except at DC (center of FT) where the value 
from .rst one is used. The high energy in (a) due to the noise mask is seen as a diagonal white brightness. 
This has been considerably reduced in (c). Note that (a) and (b) are similar, except for the high energy 
due to the noise mask. This is because the repair and sample subimages are approximately translated versions 
of each other. (d)-(f) in second row show corresponding spatial domain data. See section 4.2.1 for details. 
 4.2.1 Using Global Frequency Information The .rst projection operation that we use iphase(R) r=IFFT 
MSe:(1) Pmin-dc involves MSwhich is nearly a MIN operation, hence the name Pmin-dc.The MS in the above 
equation is min jRu;vj;jS u;vj if u;v 6= 0 ;0 MS r = jR0;0j if u= 0 ;v = 0 (2) Noise in general adds 
magnitude to the spectrum. Taking MIN ef­fectively reshapes the repair spectrum into the sample spectrum. 
Our projection, Pmin-dc has this nice property, and it is a projec­tion onto a closed convex set (see 
below). MSde.ned in Eq.2 is a kind of minimum taking operation on jRu;vjand jSu;vj.The only exception 
is at DC, u=0;v=0where the value of jR0;0j is retained. The motivation for not modifying the DC value 
of the repair-spectrum is that it contains the value of the overall repair subimage intensity. Also note 
that the phase is retained in Eq. 1, i.e while reshap­ing spectrum magnitude we leave the phase of the 
repair spectrum untouched. It turns out that the phase is reconstructed automat­ically over several iterations 
as in the phase reconstruction algo­rithms used in astronomy and other .elds [3]. Phase reconstruction 
results in the automatic alignment of global features, e.g the align­ment of the cement line in subimage 
rain Fig. 6. Doing this in frequency domain is easy. In spatial domain, an alignment would have required 
expensive block matching. The underlying set :jRu;vj:jSu;vj;u;v=60;0g:(3) Cmin-dc =fr is closed and convex 
and this can be proved similarly to proof on pp. 86 of [11] after making straightforward adjustments 
to go to the discrete case. In that proof, set M!1;!2=Sand L= f!1;!2=60;0g. Note that Pmin-dc is a projection 
because it makes the least change possible to make its input satisfy Cmin-dc.  Figure 6: Repair and 
sample subimages used for examples in Fig. 10(a)­ (c) with algorithm A1. Black line is scratch, sample 
and repair subimages are shown as dark patches. outlines. Prominent lines in sample and repair subimage 
don' t have to be aligned. See e.g the thick horizontal line between bricks in saand ra. Thus the complete 
operation Pmin-dc rconsists of (i) taking an FFT of r(ii) creating a new spectrum magnitude by taking 
a mini­mum of jRjand jSjat all frequencies except DC where jR0;0jis retained and and retaining the phase 
of Rand, (iii) taking an IFFT using the new magnitude and the phase of Runchanged. See Fig. 4. As described 
above, this projection Pmin-dc thus has two pur­poses to reshape the spectrum magnitude to match the 
prototype in order to get the global information correct and to align the promi­nent global features 
like prominent lines correctly. See Fig. 5 to see effect of one application of Pmin-dc in frequency and 
spatial do­mains (for the purpose of displaying, Fig. 5(f) is actually shown after clipping the output 
of Pmin-dc to real values between 0 and 255).  4.2.2 Using Local Spatial Information At the end of Pmin-dc 
(Fig. 4) we are back in spatial domain. The result is now closer to the answer. But since we modi.ed 
the spec­trum magnitude it is possible that after IFFT we now have imag­inary component in the image 
matrix. Some values may also be outside the feasible range of .0;255.. To bring the values back into 
the feasible range, the values of the spatial domain matrix are made real and clipped to .0;255.. In 
addition, since the operation Pmin-dc was in frequency domain it affects even the pixels outside of the 
scratch. These must now be corrected in spatial domain. This is Figure 7: Repair and sample subimages 
used for examples in Fig. 10 (d) and (e) for algorithm A2 and A3 respectively. Black line is scratch, 
sample and repair subimages are shown as white or black outlines. For A3 (and to a lesser extent, for 
all algorithms), the repair and sample subimages can have very different shading. See for example sband 
rb. done simply by copying the known pixel values around the noise from the original repair subimage. 
These two rather simple projec­tions are given below as equations, along with the closed convex sets. 
Proof of their convexity is simple and can be found in [11]. The convex set corresponding to the clipping 
to real values in .0;255.is Creal =fr:rj;k2.;0:rj;k:255g:(4) The corresponding projection Preal qclips 
the input to a real value between 0 and 255. Let Wbe the set of coordinate pairs where the binary noise 
mask is 0,i.e Wis the set of locations under the noise mask pixels. The convex set corresponding to known 
pixel replacement is Crep =fr:rj;k=r0j;k;j;k62Wg:(5) Let wbe the binary mask which is 0at noise pixel 
locations and 1otherwise. Then the appropriate projection corresponding to the convex set Crep is Prep 
r=r1w+rw:(6) 0 4.2.3 Iterations After applying Pmin-dc;Preal and Prep we come to the end of the .rst 
iteration of A1. This process is then repeated. Thus the algo­rithm A1 can be written as r 0 =initial 
repair subimage xnoise mask (7) rn+1 =(8) PrepPrealPmin-dc rn: In the current implementation, the user 
sets the number of itera­tions. It is easy to implement other termination criteria. The algo­rithm is 
fast because it usually converges in under 10 iterations and each iteration requires 1 FFT, 1 IFFT and 
copying, all performed on a small neighborhood of the noise and not on the entire image. Results of A1 
are shown in Fig. 10(a)-(c). Figure 8: Algorithm A3. See section 4.4 for details.  4.3 Soft Scratch 
Algorithm (A2) In this and Sec. 4.4 we present two extensions to the basic algorithm A1. Our purpose 
in doing this is twofold. First, these two exten­sions solve some practical shortcomings of algorithm 
A1. Equally important, we show that by working in a POCS framework and us­ing a dual-domain approach, 
important and substantial extensions can be made fairly easily. Hopefully, these extensions will also 
serve as guides for someone trying to extend the basic algorithm A1 in other ways. The continuity of 
large prominent lines crossing the binary noise mask is generated by Pmin-dc. But a transition in the 
local high frequency texture near the noise mask edge might be noticeable to human eye since the mask 
is sharp edged. It would be useful to use a soft edged mask for the noise to .x this potential problem. 
This is easy to do with a slight modi.cation in A1. In the .nal projection Prep in each iteration of 
A1, use a soft edged noise mask instead of a binary mask. This new projection that we will call Psoft-rep 
can be written as Psoft-rep r=r1+r0w(9) wsoftsoft where wsoft is a soft edged noise mask. The underlying 
convex set Csoft-rep can be written as Csoft-rep =frj;k:r0j;k=pj;kwa+qj;k1wag (10) where qis an arbitrary 
image and wais 1 outside binary noise mask, 0 inside binary noise mask and adin soft noise mask edge 
region, and 0<=ad<=1is a nice smoothly rising function like 1 ­gaussian, depending on the distance dfrom 
the binary noise mask edge. Thus the algorithm can be now written similar to A1 us­ing Psoft-rep instead 
of Prep. Showing that Csoft-rep is convex is straightforward using simple algebra. The results of using 
A2 on a color image by applying it to the r, g, b channels is shown in Fig. 10 (d). More explanation 
of results is in section 5. 4.4 Split Frequency Algorithm (A3) Notice that the example images used for 
A1 and A2 have had nearly uniform shading across the image, as in Fig. 10 (a)-(d). The next extension 
we describe removes this restriction of uniform shading. The resulting algorithm is A3, shown in Fig. 
8. The results of this algorithm A3 are shown in Fig. 10 (e). A comparison between A1 andA3onavery unevenly 
shadedimage is showninFig. 9. Al­though it may not be visible in the .nal printed paper, the circled 
area shows remnants of noise in the left image on which A1 has been applied. This problem is absent in 
the result of A3 on right in Fig. 9. Note that overall variation in shading of an image is a global feature 
and so we choose the frequency domain to attack this prob­lem. The basic idea is very simple ignore 
the shading (which is a very large, global feature) by ignoring the low frequency com­ponents. Then, 
to the high frequency image, apply the projections similar to A2 followed by merging the effect of the 
shading. Now we go through the algorithm step by step. As shown in Fig. 8, the algorithm can be written 
as r 0 =initial repair subimage xnoise mask (11) = rn+1 Psoft-repPrealPmergePsoft-repPminPsplit rn:(12) 
The main new projections are Psplit and Pmerge. The .rst of these splits the input image rinto two images, 
a high pass .ltered hpf rand a low pass .ltered rhpf r. We use a gaussian .l­ter to create hpf rand rhpf 
r. Pmerge does the reverse of Psplit by simply adding the output of previous projections, which is the 
processed high pass .ltered component of the repair subimage, with the unprocessed low pass .ltered repair 
subimage as shown in Fig. 8. Note that since the lower frequencies are being ignored during processing 
we can simplify Pmin-dc to Pmin by simplifying MS (2) of algorithm A1 to MSr=min jRu;vj;jSu;vj. Thus 
now we are taking MIN across the entire spectrum, including the DC unlike A1. After this, a replace operation 
will be performed. Since we are using a high pass .ltered repair subimage, the effect of the noise will 
be seen outside the noise mask after Psplit. Therefore when we replace the known pixel values, we should 
use a better estimate in each iteration. This is done by using the latest hpf rninstead of hpf r0as input 
for Psoft-rep. Thus the .rst Psoft-rep of A3 is similar to equation 9 of A2 except that hpf rnis used 
instead of r0in equation 9. After merging the result with the low frequency image using Pmerge, Preal 
is applied which is the same as in A1 or A2. Finally the known values are replaced using the original 
r0just as in A1 or A2. It is easy to show convexity of the underlying sets for Pmerge and Psplit using 
linearity of the Fourier transform. Figure 9: Comparison of A1 and A3 on an image with intensity variation. 
In the circled area some leftover noise is visible in the A1 result on the left whereas the A3 result 
on the right is cleaner [the difference may not be obvious in the .nal printed version]. See section 
4.4 for details.  5 RESULTS Fig. 10 shows noise removal using our algorithms A1, A2 and A3. The images 
shown have stochastic and regular textures and promi­nent systematic and random lines. The black line 
running across the .rst image of each group of images in Fig. 10 is the noise. (a)­ (c) show the result 
of algorithm A1, (d) shows result of A2 and (e) shows result of A3. The .rst four images, shown as group 
(a) are clockwise from top left image with noise, image after 1, 2 and 10 iterations of our algorithm 
A1. The other groups shown in (b)-(e) show only the noisy image and the image after 10 iterations of 
our algorithms. The sample and repair subimages used for (a)-(c) are shown in Fig. 6 as darkened patches. 
It is important to note that the formu­lation of the algorithm makes the selection of subimages easy. 
No manual alignment of features is necessary during subimage selec­tion. This can be seen in the Fig. 
6. Note for example that the subimage rahas the cement line running towards the bottom of it while that 
horizontal line is nearly in the middle in the correspond­ing sample subimage sa. The next improvement 
to A1 is the algorithm A2 which uses a soft edged mask instead of a binary mask. Fig. 10 (d) shows noise 
removal from a color image using our algorithm A2. A2 is applied to r, g and b channels separately. The 
subimage selection is shown in Fig. 7. Our last and most powerful algorithm is A3 which is able to han­dle 
images with varying intensity which might have been caused by shading or inter-re.ection etc. The result 
is shown in Fig. 10 (e) and the subimage selection is shown in Fig. 7. Note that for example, in Fig. 
7, sband rbare two subimages with very different intensity, one is darker than the other. To some extent 
variation in intensity is tolerated by all three algorithms, though A3 is best able to deal with that. 
See sections 4.2, 4.3 and 4.4 for algorithms A1, A2 and A3 re­spectively. Although not shown here, removing 
non-straight con­tiguous noise, or noise removal from synthetic images with pre­cisely repeating patterns 
(and no stochastic texture) requires no ex­tra work for our algorithm. In fact synthetic images with 
exactly repeating patterns are reconstructed perfectly. The limitations of our approach are that the 
contents of sample and repair subimages must be approximately translated versions of each other. The 
can be seen in the subimage selections in Fig. 6 and Fig. 7. If the prominent lines and texture in repair 
and sample subimages are rotated versions of each other or there is perspec­tive or other distortion 
between the two, then the algorithm will not work. Thus for example, we can' t use a vertical feature 
as in sbof Fig. 7 to .x the noise in an area which has a horizontal feature. 6 CONCLUSIONS AND FUTURE 
WORK A fast iterative algorithm for image noise removal has been de­scribed. While most existing algorithms 
have worked solely in spa­tial or frequency domain, our algorithm works in both domains, making it possible 
to fully exploit the advantages from each do­main. Although a few previous algorithms combine frequency 
and spatial domain information [6, 2], they required the image to be band limited, required that the 
band limits be known. Our algo­rithm does not place this limitation. As shown in the results, with a 
judicious choice of operations (in terms of constraints and projections) and domains in which the op­erations 
work, our dual-domain approach can (1) reconstruct many contiguous noisy pixels, (2) reconstruct textures 
even when they are large featured, (3) maintain sharpness, (4) maintain continuity of features (e.g., 
lines) across the noisy region. These advantages make the algorithm very useful in many areas. Important 
applications of this algorithm are in the .eld of .lm and video post production: for removing wires used 
in special ef­fects scenes and for restoring old .lms and photographs that have become scratched. Our 
algorithm is based on a general framework of POCS and can be extended in a clean way. Besides the constraints 
and the projections described in this paper, any image analysis and/or fea­ture extraction techniques 
that are described in a closed convex set can be plugged into the iteration loop. Also, the choice of 
domain is not limited to the spatial and frequency domains. For example, one could choose the Wavelet 
transform if multiresolutional analysis is desired. One of our motivations for presenting this work was 
to increase awareness about the general and powerful method of Projections Onto Convex Sets in the graphics 
community. To us, it appears to be an interesting way of thinking about various problems and until now 
it has been popular only amongst the image processing com­munity. It is possible to imagine other uses 
for POCS besides image restoration, by using appropriate convex sets. Examples related to our work could 
be : restoring missing 3D geometry data acquired by range data acquisition systems, .lling occluded information 
dur­ing image based rendering etc. In the context of image restoration there are a few areas that need 
attention. An interactive brush implementation in which the sam­ple and the repair subimages are automatically 
selected based on a brush stroke, would be useful. A multi-frame extension which allows better inter-frame 
continuity is another important extension. We have found that when we restore multiple frames of a still 
image in which the wire noise is moving some kind of moving noise is vis­ible when the images are observed 
in sequence. But when the movie is stopped, the restorations seems good and the noise disappears. A straightforward 
extension by using 3D Fourier transform using a few frames at a time has not worked. Our color image 
processing is also rather naive and needs more attention. A study of how the vari­ation in binary noise 
mask size affects the performance would also be desirable. Finally, what to do about rotation of features 
or per­spective distortion between sample and repair subimages are other areas that need attention. We 
are currently working on these issues.  References [1] FERREIRA, P.J.S.G. and PINHO, A.J. Errorless 
Restoration Al­gorithms for Band-Limited Images , Proc. IEEE Intl. Conf. Image Proc. (ICIP), III, 157-161, 
1994. [2] FERREIRA, P.J.S.G. Interpolation and the Discrete Papoulis-Gerchberg Algorithm , IEEE Trans. 
Sig. Proc., 42, No. 10, 2596­2606, Oct 1994. [3] FIENUP, J.R. Phase Retrieval Algorithms: a Comparison 
, Appl. Opt., 21, No. 15, 2758-2769, 1982. [4] HEEGER, D.J. and BERGEN, J.R. Pyramid-Based Texture Anal­ysis/Synthesis 
, Proc. SIGGRAPH 95, 229-238, 1995. [5] MALZBENDER, T. and SPACH, S. A Context Sensitive Texture Nib 
, Communicating with Virtual Worlds, Thalmann, N.M. and D. Thalmann (eds.), Springer-Verlag Tokyo, 1993. 
[6] PAPOULIS, A. A New Algorithm in Spectral Analysis and Band-Limited Extrapolation , IEEE Trans. Cir. 
&#38; Sys., 22, No. 9, 735­742, 1975. [7] PITAS, I. and VENETSANOPOULOS, A.N. Order Statistics in Digital 
Image Processing , Proc. IEEE, 80, No. 12, Dec 1992. [8] SEZAN, M. I. and STARK, H. Image Restoration 
by the Method of Convex Projections: Part 2 Applications and Numerical Results , IEEE Trans. Med. Imag., 
1, No. 2, 95-101, 1982. [9] STROHMER, T. On Discrete Band-Limited Signal Extrapolation , In Mathematical 
Analysis, Wavelets, and Signal Processing, Ismail, M. et. al. (eds.), AMS Contemporary Mathematics, 190, 
323-337, 1995. Also available from http://tyche.mat.univie.ac.at, the home page of NUHAG in University 
of Vienna. [10] SUN, H. and KWOK, W. Concealment of Damaged Block Trans­form Coded Images Using Projections 
onto Convex Sets , IEEE Trans. Image Proc. , 4, No. 4, April 1995. [11] YOULA, D.C. and WEBB, H. Image 
Restoration by the Method of Convex Projections: Part 1 Theory , IEEE Trans. Med. Imag.,1, No. 2, 81-94, 
1982.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237265</article_id>
		<sort_key>277</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Consequences of stratified sampling in graphics]]></title>
		<page_from>277</page_from>
		<page_to>280</page_to>
		<doi_number>10.1145/237170.237265</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237265</url>
		<keywords>
			<kw><![CDATA[antialiasing]]></kw>
			<kw><![CDATA[discrepancy]]></kw>
			<kw><![CDATA[sampling]]></kw>
			<kw><![CDATA[stratification]]></kw>
			<kw><![CDATA[variance reduction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Sampling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14128889</person_id>
				<author_profile_id><![CDATA[81100360165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Don]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Beck and W. W. L. Chen. Irregularities of Distribution, Cambridge University Press, 1987.]]></ref_text>
				<ref_id>Beck87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. L. Cook, Stochastic sampling in computer graphics. ACM Trans. Graphics 5:1 (1986) 51-72.]]></ref_text>
				<ref_id>Cook86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. A. Cross. Sampling Patterns Optimized for Uniform Distributed Edges. Graphics Gems V, Academic Press, 1995, 359-363.]]></ref_text>
				<ref_id>Cross95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325182</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M.A.Z. Dippe and E. H. Wold. Antialiasing through stochastic sampling. Computer Graphics 19:3 (1985) 69-78.]]></ref_text>
				<ref_id>Dippe85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. P. Dobkin and D. P. Mitchell. Random-edge discrepancy of supersampling patterns. Graphic Interface, York, Ontario (1993).]]></ref_text>
				<ref_id>Dobkin93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>234536</ref_obj_id>
				<ref_obj_pid>234535</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D. P. Dobkin, D. Eppstein and D. P. Mitchell. Computing the Discrepancy with Applications to Supersampling Patterns. Trans. Graphics (to appear).]]></ref_text>
				<ref_id>Dobkin94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. H. Halton. A retrospective and prospective survey of the Monte Carlo method. SIAM Review 12 (1970) 1-63.]]></ref_text>
				<ref_id>Halton70</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. M. Hammersley and D. C. Handscomb. Monte Carlo Methods. Methuen &amp; Co., London, 1964.]]></ref_text>
				<ref_id>Hammersley64</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. T. Kajiya. The Rendering Equation. Computer Graphics 20 (1986) 143-150.]]></ref_text>
				<ref_id>Kajiya86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325179</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Lee, R. A. Redner, and S. P. Uselton. Statistically optimized sampling for distributed ray tracing. Computer Graphics 19:3 (1985) 61-67.]]></ref_text>
				<ref_id>Lee85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37410</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D. P. Mitchell. Generating antialiased images at low sampling densities. Computer Graphics 21:4 (1987) 65-72.]]></ref_text>
				<ref_id>Mitchell87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74362</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. Painter and K. Sloan. Antialiased ray tracing by adaptive progressive refinement. Computer Graphics 23:3 (1989) 281-288.]]></ref_text>
				<ref_id>Painter89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[W. Purgathofer. A statistical model for adaptive stochastic sampling. Proc. Eurographics (1986) 145- 152.]]></ref_text>
				<ref_id>Purgathofer86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[P. Shirley. Discrepancy as a quality measure for sample distributions. Proc. Eurographics (1991) 183-193.]]></ref_text>
				<ref_id>Shirley91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Spanier and E. M. Gelbard. Monte Carlo Principles and Neutron Transport Problems. Addison-Wesley, Reading, MA, 1969.]]></ref_text>
				<ref_id>Spanier69</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Consequences Of Stratified Sampling In Graphics Don P. Mitchell Microsoft Corporation ABSTRACT Antialiased 
pixel values are often computed as the mean of N point samples. Using uniformly distributed random samples, 
the central limit theorem predicts a variance of the mean of O(N-1). Stratified sampling can further 
reduce the variance of the mean. This paper investigates how and why stratification effects the convergence 
to mean value of image pixels, which are observed to converge from N-2 to N-1, with a rate of about N-3/2 
in pixels containing edges. This is consistent with results from the theory of discrepancy. The result 
is generalized to higher dimensions, as encountered with distributed ray tracing or form-factor computation. 
CR Categories and Descriptors: I.3.3 [Computer Graphics]: Picture/Image generation. Additional Key Words: 
Sampling, Stratification, Discrepancy, Antialiasing, Variance Reduction.  INTRODUCTION One of the most 
general solutions to the aliasing problem in image synthesis is to supersample, compute many sample values 
within a pixel area and average them to estimate the actual integral of the image over an area. Several 
different theories have been applied to this sampling problem. Shannon s sampling theorem provides the 
justification for sampling at a higher density when an image is not sufficiently bandlimited for sampling 
at the pixel rate. The signal-processing viewpoint is not perfectly suited for treating the sampling 
of discontinuous signals (i.e., an image with sharp edges) or for understanding nonuniform sampling methods, 
although nonuniform sampling has been analyzed from this standpoint [Dippe85, Cook86, Mitchell87]. Another 
point of view is the theory of statistical sampling or Monte Carlo integration [Lee85, Kajiya86, Purgathofer86, 
Painter89]. The pixel value is estimated by the mean of a Permission to make digital or hard copies of 
part or all of this work or personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
number of samples taken within the pixel area. If the pixel area is sampled at uniformly distributed 
random locations, the central limit theorem implies that the variance of the mean is O(N-1). This is 
true even if the pixel area contains edges or if the domain being sampled has an unusual topology (e.g., 
sampling a function on the surface of a sphere) cases where signal processing theory is difficult to 
apply. A third viewpoint is the theory of discrepancy, which deals with the ability of a sampling pattern 
to estimate areas of subregions in a pixel. Quasi Monte Carlo methods [Halton70] are based on deterministic 
sampling patterns with low discrepancy, typically optimized to estimate the area of arbitrary axis-aligned 
rectangles within a square pixel area. Shirley introduced this sample­pattern quality measure to computer 
graphics [Shirley91], and Dobkin et al. introduced and analyzed a discrepancy measure based on arbitrary 
edges through a pixel [Dobkin93]. The most commonly used sampling strategy in ray tracing and distributed 
ray tracing is stratified sampling (often equivalent to the so-called jittered sampling patterns). This 
type of sampling has been studied from all three theoretical viewpoints mentioned above. This paper presents 
some theory and observations about the consequences of stratified sampling in computer graphics. EXPERIMENTAL 
OBSERVATIONS In the statistical viewpoint, a pixel value is the mean value of a small square area in 
an image. This assumes the use of a box filter, which is not ideal. Using a better filter simply involves 
computing a weighted mean, so for simplicity we will restrict the discussion to pixel-area averaging. 
A pixel value is estimated by a sample mean, the average of a number of point samples within the pixel 
area. The variance of the sample mean is a measure of the accuracy of this pixel estimate. The variance 
of the mean can be directly measured by repeatedly estimating the same pixel with M independent trials 
of N samples xi: N x = x 1 . ji N i=1 M 1 s 2 = xx- )2 x .( j M -1 j =1 general rough working rule that 
stratification gives a variance of the mean of O(N-3) [Hammersley64]. Hammersley was commenting on the 
numerical integration of one-dimensional functions. Looking at multi-dimensional radiation transport, 
Spanier reports that stratification doesn t give much benefit [Spanier69]. 02468 0 -2 -4 -6 -8 -10 -12 
 Figure 1. One of several ray-traced images analyzed. A simple experiment demonstrates the O(N-1) behavior 
predicted by the central limit theorem. Choose a set of pixels to study in a ray traced image such as 
the one shown in Figure 1. For progressively increasing values of N, measure the variance of the mean 
by performing M trials of uniformly distributed random samples. Plotting the log of the variance versus 
the log of N shows points fitting closely to a line of slope 1, for any pixel area in the image. Figure 
2 shows a histogram of the measured slopes (derived from least-square fits) for test pixels. 30 25 20 
15 10 5 0 0.75 0.9 1.05 1.2 1.35 1.5 1.65 1.8 1.95 2.1 2.25 Figure 3. Measured variance of mean for 
a pixel versus It seems appropriate then to actually measure the variance of the mean in the pixels of 
an image. Using several ray-traced images, M = 50 trials were performed with stratified sampling for 
N taking on values of 1, 4, 9, 16, up to 256, and performed a least-square fit in log space to measure 
the closest fit to a convergence of the mean of the form O(N-p). Figure 3 shows the data from one pixel 
and the least-square fit. Figure 4 shows the histogram of resulting values of p, using pixels from the 
image in Figure 1. The result is typical: a range of values from 1.0 to 2.0. 9 8 7 6 5 4 3 2 1 0 0.75 
0.9 1.05 1.2 1.35 1.5 1.65 1.8 1.95 2.1 2.25 Figure 4. Convergence of the Mean for Stratified Sampling 
of Pixels. Figure 2. Convergence of the Mean for Uniformly Distributed Random Sampling of Pixels. A standard 
variance-reducing technique is stratified sampling. Instead of distributing N random samples uniformly 
within the pixel area, the area can be divided into a grid of N × N cells, with one sample placed randomly 
within each cell. The literature of Monte Carlo methods contains varying comments about the effectiveness 
of stratification. Hammersley reports a By inspecting various pixels and their associated rates of convergence, 
three fairly distinct types can be found. Areas containing extremely complex features converge with a 
variance proportional to N-1 . Areas containing smooth regions of the image exhibit a variance of the 
mean that converges more rapidly, as N-2 . Pixels that exhibit N-3/2 variance are found to contain smooth 
areas delimited by a few edges. Figure 5 shows several typical examples: Figure 5. Pixels with p = 1.89, 
1.45, and 1.12. THEORETICAL ANALYSIS The N-1 behavior in highly complex regions of the image is not 
surprising, since this is what would result from sampling a randomly varying function, according to the 
central limit theorem. There is no benefit from stratification in this case. The N-3/2 behavior in pixels 
with edges is consistent with the arbitrary-edge discrepancy of stratified/jittered sample patterns, 
as proven in Beck and Chen [Beck87]. In their derivation of the discrepancy, they note that edge discontinuities 
are one­dimensional features. As N (the number of samples and the number of strata) increases, an edge 
intersects only ON ) ( strata. In computing the variance of the mean, the samples in the edge-crossing 
strata will dominate. Each sample xi is given a weight of 1/N, and so the total variance of the mean 
is: 21 N 2 s 2 s= .si 2 15. xN N i=1 This is equivalent to a convergence of the standard deviation (square 
root of variance) proportional to N-3/4. This is in line with Beck and Chen s bounds for arbitrary edge 
discrepancy are N -34/ -34/ 12 / O() (log N and ON ) [Beck87]. Dobkin et al. and Cross have used simulated 
annealing to generate sampling patterns with nearly half the discrepancy of jittered patterns [Dobkin94, 
Cross95]. These patterns may yield smaller pixel error, but Beck and Chen s lower bound proves that no 
pattern will be asymptotically better than jittered samples. This analysis can be generalized to the 
case of N strata in d dimensions, with a sharp discontinuity of k dimensions. In this case, we expect 
O(Nk/d) strata to be cut by the discontinuity and dominate the variance. In that case, the variance of 
the mean should converge as O(Nk/d-2). The O(N-2) convergence in smooth regions of the image can be justified 
if we make the fairly general assumption that the image obeys a Lipshitz smoothness condition within 
the pixel area. That is, the range of values of the image function f(x) (where x is a point in d dimensions) 
is no more than a constant factor times the diameter of the region: fx - f () () y = C xy - In d dimensions, 
the diameter of each strata is proportional to N-1/d and so we expect the standard deviation (root of 
the variance) of the function to be lower by the same proportion. The variance of the average of samples 
taken from each strata should therefore be: 1 N 2 -1/ d 2 (1 -- 2/ d .si (N ) = ON ) 2 N i =1 For d=2, 
this agrees with the observed result of O(N-2). It may also explain Hammersley s working rule for one-dimensional 
smooth functions, while Spanier was working with radiation transport problems of high dimension and saw 
less benefit from stratification. A FOUR-DIMENSIONAL EXPERIMENT An additional experiment was done to 
test these results in higher dimensions. The calculation of the form factor between two parallel unit 
squares (two units apart and aligned) was computed by Monte Carlo integration in four dimensions. Once 
again, the result was recomputed with M = 50 independent trials for values of N = 14, 24, 104, and an 
estimation of the variance of the mean was found for each value of N. Since this is a smooth function 
with a four-dimensional domain, we expect a convergence of O(N-3/2). The measured least-square fit to 
the data (in log space) gave p = 1.430. Removing the first (least accurate) point from the set gave p 
= 1.501. The same experiment was then performed, with a smaller occluding square between the two planes. 
For each point on a given (two-dimensional) plane, the perimeter of the occluding square presents a one-dimensional 
discontinuity in the differential form factor. Thus, the overall discontinuity in the form factor is 
three-dimensional. Thus for d = 4 and k = 3, we expect a convergence of the mean of O(N-5/4). The measured 
result was p = 1.233, and with the N = 1 point removed we found p = 1.245. CONCLUSIONS Stratified sampling 
is commonly used in ray tracing and distributed ray tracing, but its benefit has not been fully analyzed. 
Pixel accuracy is strictly improved by using stratification. For N = 1 samples per pixel, uniformly distributed 
random sampling and stratified sampling are the same, and as N increases, stratified sampling will often 
converge to the mean asymptotically faster than uniform random sampling. The improvement in pixel accuracy 
depends on the nature of the image within the pixel area. In the worst case, stratification is no better 
(but no worse) than uniform random sampling, . If a finite number of edges pass through the pixel area, 
we expect an variance of the mean to be lower by a factor of N1/2. If the image is smooth within the 
pixel area, we expect a variance of the mean to be lower by a factor of N. The absolute pixel error will 
actually be proportional to the square root of the variance (ie., the standard deviation). The asymptotic 
reduction of error due to stratification is a little less impressive when we take the square root. The 
benefits of stratification are probably a mix of genuine error reduction and the spectral consequences 
of jittering as described by Dippe, Cook and Mitchell (i.e., the tendency of these sampling patterns 
to distribute the error in a high frequency pattern over the image). ACKNOWLEDGEMENTS I would like to 
thank Steve Drucker, Steven Gortler, Mike Marr, and my SIGGRAPH reviewers for their helpful comments. 
Thanks also to Josef Beck, Bernard Chazelle and David Dobkin for discussions about discrepancy. REFERENCES 
[Beck87] J. Beck and W. W. L. Chen. Irregularities of Distribution, Cambridge University Press, 1987. 
[Cook86] R. L. Cook, Stochastic sampling in computer graphics. ACM Trans. Graphics 5:1 (1986) 51-72. 
[Cross95] R. A. Cross. Sampling Patterns Optimized for Uniform Distributed Edges. Graphics Gems V, Academic 
Press, 1995, 359-363. [Dippe85] M.A.Z. Dippe and E. H. Wold. Antialiasing through stochastic sampling. 
Computer Graphics 19:3 (1985) 69-78. [Dobkin93] D. P. Dobkin and D. P. Mitchell. Random-edge discrepancy 
of supersampling patterns. Graphic Interface, York, Ontario (1993). [Dobkin94] D. P. Dobkin, D. Eppstein 
and D. P. Mitchell. Computing the Discrepancy with Applications to Supersampling Patterns. Trans. Graphics 
(to appear). [Halton70] J. H. Halton. A retrospective and prospective survey of the Monte Carlo method. 
SIAM Review 12 (1970) 1-63. [Hammersley64] J. M. Hammersley and D. C. Handscomb. Monte Carlo Methods. 
Methuen &#38; Co., London, 1964. [Kajiya86] J. T. Kajiya. The Rendering Equation. Computer Graphics 20 
(1986) 143-150. [Lee85] M. Lee, R. A. Redner, and S. P. Uselton. Statistically optimized sampling for 
distributed ray tracing. Computer Graphics 19:3 (1985) 61-67. [Mitchell87] D. P. Mitchell. Generating 
antialiased images at low sampling densities. Computer Graphics 21:4 (1987) 65-72. [Painter89] J. Painter 
and K. Sloan. Antialiased ray tracing by adaptive progressive refinement. Computer Graphics 23:3 (1989) 
281-288. [Purgathofer86] W. Purgathofer. A statistical model for adaptive stochastic sampling. Proc. 
Eurographics (1986) 145­ 152. [Shirley91] P. Shirley. Discrepancy as a quality measure for sample distributions. 
Proc. Eurographics (1991) 183-193. [Spanier69] J. Spanier and E. M. Gelbard. Monte Carlo Principles and 
Neutron Transport Problems. Addison-Wesley, Reading, MA, 1969.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237266</article_id>
		<sort_key>281</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Multiresolution video]]></title>
		<page_from>281</page_from>
		<page_to>290</page_to>
		<doi_number>10.1145/237170.237266</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237266</url>
		<keywords>
			<kw><![CDATA[clip-art]]></kw>
			<kw><![CDATA[compositing]]></kw>
			<kw><![CDATA[image pyramids]]></kw>
			<kw><![CDATA[multigrid methods]]></kw>
			<kw><![CDATA[multimedia]]></kw>
			<kw><![CDATA[scientific visualization]]></kw>
			<kw><![CDATA[video editing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Time-varying imagery</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Video (e.g., tape, disk, DVI)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Hierarchical</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010230</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Video summarization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P10635</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P43568</person_id>
				<author_profile_id><![CDATA[81100068731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Jacobs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Adams, R. Garcia, B. Gross, J. Hack, D. Haidvogel, and V. Pizzo. Applications of multigrid software in the atmospheric sciences. Monthly Weather Review, 120(7):1447- 1458, July 1992.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Deborah E Berman, Jason T. Bartell, and David H. Salesin. Multiresolution painting and compositing. In Proceedings of SIGGRAPH '94, Computer Graphics Proceedings, Annual Conference Series, pages 85-90, July 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen. Quicktime VRian image-based approach to virtual environment navigation. In Proceedings of SIGGRAPH 95, Computer Graphics Proceedings, Annual Conference Series, pages 29-38, August 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John E Hughes. Computer Graphics: Principles and Practice. Prentice-Hall, 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>103090</ref_obj_id>
				<ref_obj_pid>103085</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. Le Gall. MPEG: A video compression standard for multimedia applications. CACM, 34(4):46-58, April 1991.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Randy LeVeque and Marsha Berger. AMRCLAW: adaptive mesh refinement + CLAWPACK. http ://www. amath, washington, edu/~rjl/amrclaw/]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. S. Lewis and G. Knowles. Video compression using 3D wavelet transforms. Electronics Letters, 26(6):396-398, 15 March 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>179021</ref_obj_id>
				<ref_obj_pid>179015</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[S. McCormick and U. Rude. A finite volume convergence theory for the fast adaptive composite grid methods. Applied Numerical Mathematics, 14(1-3):91-103, May 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan and Gary Bishop. Plenoptic modeling: An image-based rendering system. InProceedings of SIGGRAPH 95, Computer Graphics Proceedings, Annual Conference Series, pages 39-46, August 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Arun N. Netravali and Barry G. Haskell. Digital Pictures. Plenum Press, New York, 1988.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218437</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin and Luiz Velho. Live paint: Painting with procedural multiscale textures. In Proceedings of SIGGRAPH 95, Computer Graphics Proceedings, Annual Conference Series, pages 153-160, August 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Thomas Porter and Tom Duff. Compositing digital images. In Hank Christiansen, editor, Computer Graphics (SIGGRAPH '84 Proceedings), volume 18, pages 253-259, July 1984.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Steven Radecki. Multimedia With Quicktime. Academic Press, 1993. ISBN 0-12-574750-0.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>77587</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hanan Samet. Applications of Spatial Data Structures. Addison-Wesley, Reading, Massachusetts, 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[E S. Sathyamurthy and S. V. Patankar. Block-correction-based multigrid method for fluid flow problems. Numerical Heat Transfer, Part B (Fundamentals), 25(4):375-94, June 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[I. Suisalu and E. Saar. An adaptive multigrid solver for highresolution cosmological simulations. Monthly Notices of the Royal Astronomical Society, 274(1):287-299, May 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>215265</ref_obj_id>
				<ref_obj_pid>217279</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Jonathan Swartz and Brian C. Smith. A resolution independent video language. InACM Multimedia 95, pages 179-188. ACM, Addison-Wesley, November 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[S.L. Tanimoto and Theo Pavlidis. A hierarchical data structure for picture processing. Computer Graphics and Image Processing, 4(2):104-119, June 1975.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Pyramidal parametrics. In Computer Graphics (SIGGRAPH '83 Proceedings), volume 17, pages 1-11, July 1983.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[L. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Trans. Inform.Theory, Vol.IT-23, (3), May 1977.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multiresolution Video Adam Finkelstein Charles E. Jacobs David H. Salesin  Department of Computer Science 
and Engineering University of Washington Abstract We present a new representation for time-varying image 
data that allows for varying and arbitrarily high spatial and temporal res­olutions in different parts 
of a video sequence. The representation, called multiresolution video, is based on a sparse, hierarchical 
en­coding of the video data. We describe a number of operations for creating, viewing, and editing multiresolution 
sequences. These op­erations support a variety of applications: multiresolution playback, including motion-blurred 
fast-forward and reverse ; constant­speed display; enhanced video scrubbing; and video clip-art edit­ing 
and compositing. The multiresolution representation requires little storage overhead, and the algorithms 
using the representation are both simple and ef.cient. CR Categories and Subject Descriptors: H.5.1 [Information 
Interfaces]: Multimedia Information Systems video I.3.3 [Computer Graphics]: Picture/Image Generation 
display algorithms I.4.10 [Image Processing]: Image Representation hierarchical Additional Keywords: 
clip-art, compositing, image pyramids, multigrid methods, multimedia, scienti.c visualization, video 
editing   1 Introduction Scientists often run physical simulations of time-varying data in which different 
parts of the simulation are performed at differing spatial and temporal resolutions. For example, in 
a simulation of the air .ow about an airplane wing, it is useful to run the slowly­varying parts of the 
simulation generally, the portion of space fur­ther from the wing at a fairly coarse scale, both spatially 
and tem­porally, while running the more complex parts say, the region of turbulence just aft of the wing 
at a much higher resolution. The multigrid techniques used frequently for solving large-scale prob­lems 
in physics [15], astronomy [16], meteorology [1], and applied mathematics [8] are a common example of 
this kind of computation. In this paper, we present a new approach for representing the time­varying 
data produced by such algorithms, called multiresolution video. The multiresolution video representation 
provides a means of capturing time-varying image data produced at multiple scales, both spatially and 
temporally. In addition, we introduce ef.cient al­gorithms for viewing multiresolution video at arbitrary 
scales and speeds. For example, in a sequence depicting the .ow of air about i For project updates, addresses, 
and email see our Web page: http://www.cs.washington.edu/research/graphics/mrvideo Permission to make 
digital or hard copies of part or all of this work or personal or classroom use is granted without fee 
provided that copies are not made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
a wing, a user can interactively zoom in on an area of relative tur­bulence, computed at an enhanced 
spatial resolution. Analogously, fast-changing components in a scene can be represented and viewed at 
a higher temporal resolution, allowing, for example, a propeller blade to be represented and viewed in 
slow motion. Moreover, we have found that multiresolution video has applica­tions that are useful even 
for conventional uniresolution video. First, the representation facilitates a variety of viewing applications, 
such as multiresolution playback, including motion-blurred fast­forward and reverse ; constant-speed 
viewing of video over a network with varying throughput; and an enhanced form of video scrubbing. The 
representation also provides a controlled degree of lossy compression, particularly in areas of the video 
that change little from frame to frame. Finally, the representation supports the assembly of complex 
multiresolution videos from either uniresolu­tion or multiresolution video clip-art elements. 1.1 Related 
work The multiresolution video representation described in this paper generalizes some of the multiresolution 
representations that have previously been proposed for images, such as image pyramids [18] and MIP maps 
[19]. It is also similar in spirit to the wavelet­based representations for images described by Bermanet 
al. [2] and Perlin and Velho [11]. In particular, like these latter works, our rep­resentation is sparse, 
and it supports ef.cient compositing opera­tions [12] for assembling complex frames from simpler elements. 
Several commercially available video editing systems support many of the operations described in this 
paper for uniresolution video. For example, Adobe After Effects allows the user to view video seg­ments 
at low resolution and to construct an edit list that is later ap­plied to the high-resolution frames 
off-line. Discrete Logic s Flame and Flint systems also provide digital video compositing and many other 
digital editing operations on videos of arbitrary resolution. Swartz and Smith [17] describe a language 
for manipulation of video segments in a resolution-independent fashion. However, the input and output 
from all of these systems is uniresolution video. Multiresolution video also allows the user to pan and 
zoom to ex­plore a .at video environment. This style of interaction is simi­lar in spirit to two image-based 
environments: Apple Computer s e QuickTimeR VR [3] and the plenoptic modeling system of McMillan and 
Bishop [9]. These methods provide an image-based representation of an environment that surrounds the 
viewer. In sec­tion Section 4.5, we describe how such methods can be combined with multiresolution video 
to create a kind of multiresolution video QuickTime VR, in which a viewer can investigate a panoramic 
en­vironment by panning and zooming, with the environment changing in time and having different amounts 
of detail in different locations. While not the emphasis of this work, we also describe a simple form 
of lossy compression suitable for multiresolution video. Video compression is a heavily-studied area, 
with too many papers to ad­equately survey here. MPEG [5] and QuickTime [13] are two in­dustry standards. 
Other techniques based on multiscale transforms [7, 10] might be adapted to work for multiresolution 
video.  1.2 Overview image trees  [0,8] The rest of this paper is organized as follows. Section 2 
describes our representation for multiresolution video, and Section 3 describes how it is created and 
displayed. Section 4 describes a variety of ap­plications of the multiresolution video representation, 
and Section 5 provides some concrete examples. Finally, Section 6 outlines some  [0,4]  [4,8] areas 
for future work. The appendix provides additional low-level operations useful for editing multiresolution 
video.  [0,2] [2,4] [4,6] [6,8]  2 Representation Our goals in designing a multiresolution video 
representation were .vefold. We wanted it to: 2 (with a small constant of proportionality); permit 
lossy compression; and require only a small working storage overhead, so that video could be streamed 
in from disk as it is needed. The rest of this section describes the multiresolution video format we 
chose and an analysis of the storage required. The basic multiresolution video format [4,5] [5,6] support 
varying spatial and temporal resolutions; require overall storage proportional only to the detail present 
 ef.ciently support a variety of primitive operations for creating, viewing, and editing the video; 
0 1 2 3 4 5 6 7 } } } } }  time s Figure 1 Binary tree of quadtrees. Each node of the Time Tree 
points to a sparse quadtree, called an image tree, which represents the multiresolution image content 
of a single frame of the video sequence. In analogy to the Time Tree, leaves of an image tree correspond 
to pixels at the highest spatial resolution for which information is present in the particular frame 
being represented. Internal nodes of an image tree correspond, once again, to box-.ltered averages of 
their children in this case, to a 2 2 block of higher-resolution pixels. Note that the image tree sup-Perhaps 
the most obvious choice for a multiresolution video format would be a sparse octree [14], whose three 
dimensions were used to encode the two spatial directions and time. Indeed, such a represen­ tation was 
our .rst choice, but we found that it did not adequately ad­ dress a number of the goals enumerated above. 
Put brie.y, the prob­ lem with such a representation is that it couples the dimensions of space and time 
too tightly. In an octree structure, each node would correspond to a cube with a .xed extent in space 
and time. Thus, it would be ef.cient to rescale a video to, say, twice the spatial res­olution only if 
it were equally rescaled in time that is, played at half the speed. We therefore needed to develop a 
representation that, while still making it possible to take advantage of temporal and spa­tial coherence, 
could couple space and time more loosely. The structure we ultimately chose is a sparse binary tree of 
sparse quadtrees. The binary tree encodes the .ow of time, and each quadtree encodes the spatial decomposition 
of a frame (Figure 1). In the binary tree, called the Time Tree, each node corresponds to a single image, 
or frame, of the video sequence at some temporal res­olution. The leaves of the Time Tree correspond 
to the frames at the highest temporal resolution for which information is present in the video. Internal 
nodes of the Time Tree correspond to box-.ltered av­erages of their two children frames. Visually, these 
frames appear as motion-blurred versions of their children. Note that this representa­tion supports video 
sequences with varying degrees of temporal res­olution simply by allowing the Time Tree to grow to different 
depths in different parts of the sequence. For convenience, we will call the child nodes of the Time 
Tree child time nodes and their parents par­ent time nodes. We will use capitalized names for any time 
node. Time Tree nodes are represented by the following data structure: type TimeNode = record frame: 
pointer to ImageNode Half1, Half2: pointer to TimeNode end record ports varying spatial resolution simply 
by allowing the quadtree to type ImageNode = record type: TREE COLOR j uplink: UpLinkInfo union tree: 
pointer to ImageSubtree color: PixelRGBA end union end record type ImageSubtree = record avgcolor: PixelRGBA 
child[0..1, 0..1]: array of ImageNode end record Each subtree contains both the average color for a region 
of the im­age, stored as an RGBA pixel, and also image nodes for the four quadrants of that region. We 
compute the average of the pixels as if each color channel were premultiplied by alpha as prescribed 
by Porter and Duff [12] but we do not actually represent the pix­els that way in our image nodes, in 
order to preserve color .delity in highly-transparent regions. Each image node generally contains a pointer 
to a subtree for each quadrant. However, if a given quadrant only has a single pixel s worth of data, 
then the color of the pixel is stored in the node directly, in place of the pointer. (This trick works 
nicely, since an RGBA pixel value is represented in our system with 4 bytes, the same amount of space 
as a pointer. Packing the pixel in­ leaf nodes of the image tree are boxed in yellow. Speci.cally, here 
is how we encode each node in the image tree: parent time node P time parent p parent image node  time 
child c child image node (a) Time tree (b) Image tree Figure 2 Parent-child relationships in the trees. 
formation into the pointer space allows us to save a large amount of memory that we might otherwise waste 
on null pointers at the leaves.) There is also an uplink .eld, whose use we will discuss in the next 
section. There is an additional relationship between image nodes that is not represented explicitly in 
the structure, but which is nevertheless cru­cial to our algorithms. As described already, there are 
many differ­ent image nodes that correspond to the same region of space, each hanging from a different 
time node. We will call any two such image nodes time-relatives. In particular, for a given image node 
c hanging from a time node C, we will call the time-relative p hanging from the parent time node P of 
C the time-parent of c. In this case, the image node c is also called the time-child of p. (See Figure 
2.) Note that a given node does not necessarily have a time-parent or a time-child, as the quadtree structures 
hanging from P and C may differ. 2.2 Temporal coherence Recall that the representation of each frame 
exploits spatial coher­ence by pruning the image tree at nodes for which the image content is nearly 
constant. We can take advantage of temporal coherence in a similar way, even in regions that are spatially 
complex. Consider an image node p and its two time-children c1 and c2. Whenever the images in c1 and 
c2 are similar to each other, the im­age in p will be similar to these images as well. Rather than triplicat­ing 
the pixels in all three places, we can instead just store the image in the time-parent p and allow c1 
and c2 to point to this image di­rectly. We call such pointers up-links. See Figure 3 for a schematic 
example. Figure 5 shows a frame from a multiresolution video clip in which all up-link regions (which 
cover most of the frame) are shaded red. The up-links are described by the following structure: type 
UpLinkInfo = record linked: Boolean type: FIRST MIDDLE LAST jj end record The linked .eld tells whether 
or not there is an up-link. There is also a type .eld, which we will describe in Section 3.2. 2.3 Storage 
complexity Now that we have de.ned the multiresolution video data structure, we can analyze its storage 
cost. The type and uplink .elds require very few bits, and in practice these two .elds for all four children 
may be bundled together in a single 4-byte .eld in theImageSubtree structure. Thus, each ImageSubtree 
contains 4 bytes (for the aver­age color), 44 bytes (for the children), and 4 bytes (for the .ags), s 
yielding a total of 24 bytes. Each leaf node of an image tree com­prises 4 pixels, and there are 4/3 
as many total nodes in these trees Figure 3 Exploiting temporal coherence. The quadrants containing 
the Luxo lamp are not duplicated in the lower six frames of the Time Tree. Instead, the right two quadrants 
in all six frames contain up­links to the corresponding quadrants in the Time Tree s root. as there are 
leaves. Assuming P pixels per time node, we get: 24 bytes 4 nodes 1 leaves P pixels 8P bytes sss = node 
3 leaf 4 pixels time node time node Furthermore, there are twice as many time nodes as there are leaves 
(or frames) in the Time Tree, so the storage complexity is really 16P bytes/frame. In addition, each 
TimeNode contains 34 = 12 bytes, s and there are twice as many nodes in this tree as there are leaves. 
Thus, the Time Tree needs an additional 24 bytes/frame. However, since 16P is generally much larger than 
24, we can ignore the latter term in the analysis. The overall storage is therefore 16 bytes/pixel. In 
the worst case a complete tree with no up-links we have as many pixels in the tree as in the original 
image. Thus, the tree takes 4 times as much space as required by just the highest-resolution pixel information 
alone. It is worthwhile to compare this overhead with the cost of directly storing the same set of time-and 
space-averaged frames, without allowing any space for pointers or .ags. Such a structure would essentially 
involve storing all powers-of-two time and spatial scales of each image, requiring a storage overhead 
of 83. Thus, our storage overhead of 4 is only slightly larger than the . minimum overhead required. 
However, as will be described in Sec­tion 3.1, the very set of pointers that makes our worst-case overhead 
larger also permits both lossless and lossy compression by taking ad­vantage of coherence in space and 
time.  Figure 4 Quadtree. Figure 5 Up-links. 2.4 Working storage One of the goals of our representation 
was to require a small work­ing storage overhead, so that video could be streamed in from disk only as 
it is needed. This feature is crucial for viewing very large se­quences, as well as for the editing operations 
we describe in the ap­pendix. As we will see when we discuss these operations in detail, this goal is 
easily addressed by keeping resident in memory just the image trees that are currently being displayed 
or edited, along with all of their time-ancestors. For a video clip with 2k frames, the num­ber of time-ancestors 
required is at most k. 2.5 Comparison with wavelets Wavelets have been successfully used to represent 
multiresolution functions in a variety of domains. Our .rst impulse and, in fact, our .rst implementation 
was to use a 3D wavelet representation for the video. However, we eventually moved to the data structure 
described in this section for several reasons. First, the box basis functions we use now are simpler, 
making it faster to render a frame of video. Second, wavelet coef.cients require increasing numbers of 
bits at .ner levels of detail, so we had to use .oating-point num­bers (rather than bytes) to store the 
color channels a factor of four expansion. Third, the wavelets we were using (nonstandard tensor­product 
Haar wavelets) made it dif.cult to separate the spatial and temporal dimensions; thus, it was expensive 
to extract a frame in which the time and space dimensions were scaled differently. While it is possible 
to construct a wavelet basis that avoids this problem, we were not able to .nd an ef.cient compositing 
algorithm for it. Fi­nally, the current representation takes advantage of areas of a video sequence that 
have temporal coherence but no spatial coherence; the wavelets we used were unable to compress such sequences. 
  3 Basic algorithms In this section we describe algorithms for creating and displaying multiresolution 
video. Algorithms for translating, scaling, and com­positing multiresolution video clips appear in the 
appendix. 3.1 Creating multiresolution video We begin with the problem of creating multiresolution video 
from conventional uniresolution video. We break this process into two parts: creating the individual 
frames, and linking them together into a multiresolution video sequence. 3.1.1 Creating the individual 
frames Given a 22source frame S we construct an image tree by calling the following function with arguments 
(S, 0, 0,): function CreateFrame(S, x, y,): returns ImageNode if=0 then return ImageNode(COLOR, S[x, 
y])   0 s 2f g 0 0 d for each i, j 0, 1do x2x + i y2y + j subtree.child[i, j]CreateFrame(S, x, y,1) 
 end for subtree.avgcolorAverageChildren(subtree.child[0..1, 0..1]) return ImageNode(TREE, subtree) 
end function Image trees built from images that are not of dimension 22are s implicitly padded with 
transparent, black pixels. The quadtree constructed by CreateFrame() is complete. The next step is to 
take advantage of spatial coherence by culling redundant information from the tree. The following function 
recursively tra­verses the image tree p and prunes any subtree whose colors differ from its average color 
a by less than a threshold:  function PruneTree(p, a,): returns Boolean if p.type = COLOR then return 
(ColorDiff(p.color, a)) pruneTRUE 2fg for each i, j 0, 1do pruneprune and PruneTree(p.child[i, j], p.avgcolor,) 
end for if prune = FALSE then return FALSE free(p.child[0..1, 0..1]) pImageNode(COLOR, p.avgcolor) return 
TRUE end function Choosing= 0 yields lossless compression, whereas using0 permits an arbitrary degree 
of lossy compression at the expense of image degradation. The function ColorDiff() measures the distance 
between two colors (r1, g1, b1, a1) and (r2, g2, b2, a2). We chose to measure the distance as the sum 
of the distances between color com­ponents, weighted by their luminance values: jdjjdjjdj 0.299r1a1 
r2a2+ 0.587g1 a1 g2a2+ 0.114b1 a1 b2a2 In practice, the source material may be multiresolution in nature. 
For example, the results of some of the scienti.c simulations de­scribed in Section 5 were produced via 
adaptive re.nement. It is easy to modify the function CreateFrame() to sample source mate­rial at different 
levels of detail in different parts of a frame. In this case, the recursive function descends to varying 
depths, depending on the amount of detail present in the source material. 3.1.2 Linking the frames together 
The next step is to link all the frames together into the Time Tree. We .rst insert all the image trees 
at the leaves of the Time Tree, and then compute all of the internal nodes by averaging pairs of frames 
in a depth-.rst recursion. Now that the complete Time Tree is built, the following two procedures discover 
and create all the up-links: procedure MakeMRVideo(Timetree,):  62f g for each Half Half1, Half2of 
Timetree do if Half = NULL then MakeUpLinks(Half.frame, Timetree.frame,) MakeMRVideo(Half,) end if end 
for end procedure function MakeUpLinks(p, c,): returns Boolean c.uplink.linkedFALSE 6 if p = NULL or 
p.type = c.type then return FALSE else if c.type = COLOR then c.uplink.linked(ColorDiff(p.color, c.color)) 
 return c.uplink.linked end if linkTRUE 2fg for each i, j 0, 1do link(link and MakeUpLinks(p.child[i, 
j], c.child[i, j]),) end for if link = FALSE then return FALSE free(c.tree) c.treep.tree c.uplink.linkedTRUE 
return TRUE end function The MakeMRVideo() routine works by .nding all of the up-links between the root 
of the Time Tree and its two child time nodes. The routine then calls itself recursively to .nd up-links 
between these children and their descendents in time. Because of the preorder re­cursion, up-links may 
actually point to any time-ancestor, not just a time-parent. (See Figure 3.) The MakeUpLinks() function 
attempts to create an up-link from a time-child c to its time-parent p. An up-link is created if the 
two nodes are both subtrees with identical structure, and all of their de­scendent nodes are suf.ciently 
close in color. The function returns TRUE if such an up-link is created. It also returns TRUE if the 
two nodes are colors and the two colors are suf.ciently close; it further­more sets the child node s 
uplink .ag, which is used to optimize the display operation in the following section. After executing 
MakeMRVideo(), we traverse the entire Time Tree in a separate pass that sets the type .eld of the uplink 
structure, whose use is explained in the following section. 3.2 Display Drawing a frame at an arbitrary 
power-of-two spatial or tempo­ral resolution is simple. Displaying at a particular temporal resolu­tion 
involves drawing frames at the corresponding level in the Time Tree. Displaying at a particular spatial 
resolution involves drawing the pixels situated at the corresponding level in the image trees. The up-links 
that were used in the previous section to optimize stor­age can also play a role in optimizing the performance 
of the display routine when playing successive frames. We would like to avoid re­freshing any portion 
of a frame that is not changing in time; the up­links provide exactly the information we need. In particular, 
if we have just displayed frame t, then we do not need to render portions of frame t + 1 (at the same 
time level) that share the same up-links. We will use the type .eld in the UpLinkInfo structure to specify 
the .rst and last up-links of a sequence of frames that all share the same parent data. When playing 
video forward, we do not need to render any region that is pointed to by an up-link, unless it is a FIRST 
up­link. Conversely, if we are playing backward, we only need to render LAST up-links. To render the 
image content c of a single multiresolution video frame at a spatial resolution 2 2 , we can call the 
following re­ s cursive routine, passing it the root c of an image tree and other pa­rameters (0, 0, 
): procedure DrawImage(c, x, y, ): if c.uplink.linked and c.uplink.type = FIRST then return if c.type 
= COLOR then DrawSquare(x, y,2, c.color) else if =0 then DrawPixel(x, y, c.avgcolor) else  2fg6d for 
each i, j 0, 1do DrawImage(c.child[i, j], 2x + i,2y + j, 1) end for end if end procedure The routine 
DrawSquare() renders a square at the given location and sizein our application window, while DrawPixel()renders 
a single pixel. Note that DrawImage() assumes that the video is being played in the forward direction 
from beginning to end. A routine to play the videoinreversewouldhavetouseLAST inplaceof FIRST inthe.rst 
line. A routine to display a single frame that does not immediately follow the previously displayed frame 
(for example, the .rst frame to be played) would have to omit the .rst line of code entirely. One further 
optimization is that we actually keep track of the bound­ing box of non-transparent pixels in each frame. 
We intersect this bounding box with the rectangle containing the visible portion of the frame on the 
screen, and only draw this intersection. Thus, if only a small portion of the frame is visible, we only 
draw that portion. The DrawImage() routine takes time proportional to the number of squares that are 
drawn, assuming that the time to draw a square is constant. Fractional-level zoom The DrawImage() routine 
as described displays multiresolution video at any power-of-two spatial resolution. Berman et al. [2] 
de­scribe a simple method to allow users to view multiresolution im­ages at any arbitrary scale. We have 
adapted their method to work for multiresolution video. The basic idea is that if we want to dis­play 
a frame of video at a fractional level between integer levels 1 sd and , we select pixels from the image 
tree as though we were draw­ing a 2 2 image, and then display those pixels at locations appro­priate 
to the fractional level. So if a pixel would be drawn at location ss 00 (x, y) in a 2 2 image, then it 
would be drawn at location (x, y) in an MM image, where x = xM2 y = yM2 0 b . c 0 b . c Furthermore, 
as with MIP maps [19], we interpolate between the colors appearing at levels and 1 in the image tree 
in order to d reduce point-sampling artifacts. Drawing at this fractional level is only slightly more 
expensive than drawing pixels at level . Similarly, even though we are selecting frames from the Time 
Tree corresponding to power-of-two intervals of time, we can achieve fractional rates through the video, 
as will be described in Sec­tion 4.2. 4 Applications We now describe several applications of the primitive 
operations presented in the last section. These applications include mul­tiresolution playback, with 
motion-blurred fast-forward and re­verse ; constant perceived-speed playback; enhanced video scrub­bing; 
video clip-art editing and compositing; and multiresolution video QuickTime VR. These applications of 
multiresolution video serve as tools that can be assembled in various combinations into higher-level 
applica­tions. We describe our prototype multiresolution video editing and viewing application in Section 
5. 4.1 Multiresolution playback The primary application of multiresolution video is to support play­back 
at different temporal and spatial resolutions. To play a video clip at any temporal resolution 2k and 
spatial resolution 2 2 we s simply make successive calls to the procedure DrawImage(), pass­ing it a 
series of nodes from level k of the Time Tree, as well as the spatial level . We can zoom in or out of 
the video by changing the level . Similarly, for motion-blurred fast-forward and reverse, we use a smaller 
time level k. In our implementation the motion-blur effect comes from simple box .ltering of adjacent 
frames. Though box­.ltering may not be ideal for creating high-quality animations, it does appear to 
be adequate for searching through video. Sometimes it may be desirable to have a limited degree of motion 
blur, which might, for example, blur the action in just the .rst half of the frame s time interval. This 
kind of limited motion blur can be implemented by descending one level deeper in the Time Tree, dis­playing 
the .rst child time node rather than the fully motion-blurred frame. 4.2 Constant perceived-speed playback 
During video playback, it is useful to be able to maintain a con­stant perceived speed, despite variations 
in the network throughput or CPU availability. Multiresolution video provides two ways of ad­justing 
the speed of play, which can be used to compensate for any such variations in load. First, by rendering 
individual frames at a .ner or coarser spatial resolution, the application can adjust the ren­dering 
time up or down. Second, by moving to higher or lower levels in the Time Tree, the application can also 
adjust the perceived rate at which each rendered frame advances through the video. These two mechanisms 
can be traded off in order to achieve a con­stant perceived speed. One possibility is to simply adjust 
the spa­tial resolution to maintain a suf.ciently high frame rate, say 30 frames/second. If, however, 
at some point the degradation in spa­tial resolution becomes too objectionable (for instance, on account 
of a large reduction in network bandwidth), then the application can drop to a lower frame rate, say, 
15 frames/second, and at the same time move to the next higher level of motion-blurred frames in the 
Time Tree. At this lower frame rate, the application has the liberty to render more spatial detail, albeit 
at the cost of more blurred tem­poral detail. Note that by alternating between the display of frames 
at two adja­cent levels in the Time Tree, we can play at arbitrary speeds, not just those related by 
powers of two. 4.3 Scrubbing Conventional broadcast-quality video editing systems allow a user to search 
through a video interactively by using a slider or a knob, in a process known as scrubbing. In such systems, 
frames are simply dropped to achieve faster speeds through the video. Multiresolution video supports 
a new kind of scrubbing that shows all of the motion-blurred video as the user searches through it, rather 
than dropping frames. In our implementation, the user interface pro­vides a slider whose position corresponds 
to a position in the video sequence. As the user moves the slider, frames from the video are displayed. 
The temporal resolution of these frames is related to the speed at which the slider is pulled: if the 
slider moves slowly, frames of high temporal detail are displayed; if the slider moves quickly, blurred 
frames are displayed. The bene.ts of this approach are similar to those of the constant perceived-speed 
playback mechanism described above. If the slider is pulled quickly, then the application does not have 
an opportu­nity to display many frames; instead, it can use the motion-blurred frames, which move faster 
through the video sequence. In addition, the motion blur may provide a useful visual cue to the speed 
at which the video is being viewed. 4.4 Clip-art In our multiresolution video editor, the user may load 
video frag­ments, scale them, arrange them spatially with respect to each other, and preview how they 
will look together. These input fragments may be thought of as video clip-art in the sense that the user 
constructs the .nal product as a composite of these elements. Since the .nal composition can take a long 
time to construct, our ap­plication provides a preview mode, which shows roughly how the .- Figure 6 
The application. nal product will appear. The preview may differ from the .nal com­posite in that it 
performs compositing on the images currently be­ing displayed rather than on the underlying video, which 
is poten­tially represented at a much higher resolution. (The degree to which the preview differs from 
the .nal composite corresponds exactly to the degree to which the compositing assumption [12] is violated.) 
When viewing the motion-blurred result of compositing two video sequences, there is a similar difference 
between the preview pro­vided in our editor and the actual result of the compositing opera­tion. Once 
the desired effect is achieved, the user can press a button that translates, scales, and composites the 
various clip-art elements into a single multiresolution video, employing the operations described in 
the appendix. This video may be saved for subsequent viewing, or it may be combined with other elements 
as clip-art to form an even more elaborate multiresolution video. 4.5 Multiresolution video QuickTime 
VR Apple Computer s QuickTime VR (QTVR) allows a user to explore an environment by looking from a .xed 
camera position out into a virtual world in any direction. Chen [3] proposes a potential aug­mentation 
of QTVR based on quadtrees that would provide two ben­e.ts. First, it would allow users to zoom into 
areas where there is more detail than in other areas. Second, it would reduce aliasing when the user 
zooms out. We implemented this idea, and extended it in the time dimension as well. Two simple modi.cations 
to mul­tiresolution video were all that were required to achieve this mul­tiresolution video QuickTime 
VR (MRVQTVR?!). First, we treat the video frames as panoramic images, periodic in the x direction. Second, 
we warp the displayed frames into cylindrical projections based on the view direction.  5 Results We 
have implemented all of the operations of the previous section as part of a single prototype multiresolution 
video editing and view­ing application, shown in Figure 6. Using the application, a user can zoom in 
and out of a video either spatially or temporally, pan across a scene, grab different video clips and 
move them around with re­spect to each other, play forward or backward, and use several slid­ers and 
dials to adjust the zoom factor, the speed of play through the video, the desired frame rate, and the 
current position in time. Figure 11 Astrophysical simulation of a galaxy. Figure 7 Julia set. Figure 
8 Van Gogh room. Figure 7 illustrates how multiresolution video can be used for vi­sualization of multiresolution 
data, in this case, an animation of the Julia set [4]. The data were generated procedurally, with higher 
spa­tial resolution in places of higher detail, as described in Section 3.1. The top three cells show 
increasing spatial detail, and the lower two cells show increasing motion blur. Figure 8 shows the result 
of arranging and compositing the many clip-art elements from the work area of the application shown in 
Figure 6 into a single multiresolution video, and then viewing this video at different spatial and temporal 
resolutions. (Apologies to Vincent Van Gogh.) Figure 9 shows wind stress, the force exerted by wind over 
the earth s surface, measured for 2000 days over the Paci.c Ocean by the National Oceanographic and Atmospheric 
Administration (NOAA). Wind stress is a vector quantity, which we encoded in multiresolution video using 
hue for direction and value for magni­tude. The left image shows a leaf time node (re.ecting a single 
day s measurements), while the right image shows the root time node (re­.ecting the average wind stress 
over the 2000-day period). Note the emergence of the dark continents in the right image, which reveals 
the generally smaller magnitude of wind stress over land. Figure 9 Wind stress over the Paci.c Ocean. 
Figure 10 Fluid dynamics simulation. The left side of Figure 10 shows a frame from a computational .uid 
dynamics simulation in which two .uids (one heavy, one light) in­teract in a closed tank. The simulation 
method [6] adaptively re.nes its sample grid in regions where the function is spatially complex, so the 
resolution of the data is higher at the interface between the two .uids than it is in the large, constant 
regions containing just one .uid. This re.nement also occurs in time, providing higher temporal resolution 
in areas that are changing rapidly. The right image shows a close-up of the boxed region in the left 
image. One more scienti.c visualization is shown in Figure 11. In this sim­ulation, a galaxy is swept 
about a cluster of other astronomical bod­ies and is eventually ripped apart by their gravitational forces. 
The left image shows a close-up late in the entire simulation focused on the galaxy. The right image 
shows an even closer view of a sin­gle frame in which some whimsical high-resolution detail has been 
added. (However, creating the entire video sequence at this level of detail would be prohibitively expensive.) 
Finally, Figure 12 shows a QTVR panoramic image that we have adapted for use with multiresolution video 
QuickTime VR. Over the picture frame on the wall we have composited the entire Van Gogh room video from 
Figure 8.  Figure Video Disk Size Memory Size Unires Size 7 8 9 10 11 12 Julia set Van Gogh Wind stress 
Fluids Galaxy Panorama 23,049 46,738 68,566 40,091 37,222 47,723 58,926 98,798 134,201 106,745 315,098 
100,804 67,109 34,359,730 33,554 536,870 137,438,953 2,199,023,256 Table 1 Sizes (in Kb) of some example 
multiresolution video clips. Table 1 reports information about the storage space for the exam­ples in 
Figures 7 12. The Disk Size column gives the total amount of space required to store the entire structure 
on disk, with aver­ages and pointers included, after it has been compressed without loss using a Lempel-Ziv 
compressor [20]. The next column, Memory Size gives the total space required in memory, including all 
aver­ages, pointers, and .ags. The Unires Size column reports the total space that would be required 
to store the raw RGBA pixel values, as­suming the entire video had been expanded to its highest spatial 
and temporal resolution present anywhere in the multiresolution video, but not including spatial or temporal 
averages. With the exception of the wind stress data, all of the video clips were smaller (in several 
cases much, much smaller) in the multiresolution video format than they would be in a uniresolution format, 
despite the overhead of the spatial and temporal averages. The wind stress data was dif.cult to consolidate 
because it has very little spatial or temporal coherence. The galaxy data compressed very well on disk 
because all of the col­ors stored in our structure (most of which were black) were selected from a small 
palette of very few colors.  6 Future work This investigation of multiresolution video suggests a number 
of ar­eas for future work: User-interface paradigms. As in the multiresolution image work of Berman et 
al. [2], there is an important user-interface issue to be addressed: How does the user know when there 
is more spatial or temporal detail present in some part of the video? We have consid­ered changing the 
cursor in areas where there is more spatial detail present than is currently being displayed. Perhaps 
a timeline show­ing a graph of the amount of temporal detail present in different parts of the video 
would address the corresponding temporal problem. Environment mapping. Multiresolution video could be 
used for en­vironment maps that change in time, allowing, for example, the ren­dering of a crystal glass, 
with animated objects in the environment re.ecting in the glass. One bene.t of using a multiresolution 
rep­resentation is that as the viewpoint and curvature of the glass sur­face vary, an accurate rendering 
may require more or less informa­tion from the surrounding environment. Better compression. We currently 
require the up-links in our rep­resentation to point to a time-ancestor, primarily because coher­ence 
is fairly easy to discover this way. However, by relaxing this restriction that is, by allowing up-links 
to point to any other place in the structure we might be able to achieve much better compres­sion, particularly 
for areas that have spatially repeating patterns. Un­fortunately, .nding the optimal set of up-links 
in this more general setting could be very expensive. Spatial and temporal antialiasing. So far we only 
have used box­basis functions to represent the colors in multiresolution video. When the user zooms in 
to view a region at a higher spatial resolu­tion than is present in the frame, large blocky pixels are 
displayed. Furthermore, if the user zooms in in time to view frames at higher temporal detail than is 
present in the video sequence, the motion is choppy. It would be interesting to explore the use of higher-order 
.l­ters to produce smoother interpolations when the user views regions at higher resolution than is represented. 
Acknowledgements We would like to thank David Herbstman and David Simons for useful discussions during 
the initial phase of the project; Richard Anderson, Ronen Barzel, and Richard Ladner for valuable advice 
along the way; Neal Katz and Tom Quinn for the astronomical sim­ulation; Randy LeVeque for the computational 
.uid dynamics; Xuri Yu for the wind stress data; Sean Anderson for the bee model; and Lightscape Technologies 
for the hotel lobby panorama. This work was supported by an Alfred P. Sloan Research Fellow­ship (BR-3495), 
an NSF Presidential Faculty Fellow award (CCR­9553199), an ONR Young Investigator award (N00014-95-1-0728), 
an Intel Graduate Research Fellowship, and industrial gifts from In­terval, Microsoft, and Xerox. A 
Algorithms for combining video clips This appendix describes a set of linear-time algorithms for translat­ing, 
scaling and compositing multiresolution video sequences. Such operations are useful, for example, in 
the video clip-art application described in Section 4.4. A.1 Translating a video clip When combining 
video sequences, the various elements may need to be registered with respect to one another, requiring 
that they be translated and scaled within their own coordinate frames. The basic operations of translation 
and scaling are well-understood for quadtrees [14]. However, as with drawing frames, we want these operations 
to take advantage of the temporal coherence encoded in the up-links of our data structure. For example, 
suppose we wanted to translate the fan and lamp video of Figure 3 a bit to the left. The re­gions of 
the video that contain the lamp should only be translated in the root node of the Time Tree, and all 
the time-children must some­how inherit that translation. The following routine translates a multiresolution 
video clip, rooted at time node C, by an amount (dx, dy) at level tran to produce a re­ 0 sulting Time 
Tree C. In order to handle up-links, the routine is also 000 passed the parent time node P of C, as 
well as the result Pof (pre­viously) translating P by the given amount. In the top-level call to the 
procedure, the parameters P and Pare passed as NULL, and the Time Tree Cinitially points to an image 
node containing just a sin­gle clear pixel. As the procedure writes its result into C, the trans­lated 
image tree is developed (and padded with clear pixels as it is extended). 00 procedure TranslateTimeTree(C, 
C , P, P, dx, dy,tran): TranslateFrame(C.frame, C .frame, P.frame, P.frame, dx, dy,tran, 0, 0, 0) ComputeSpatialAverages(C 
.frame)   62f 00 g 000 0 for each Half Half1, Half2of Timetree do if C.Half = NULL then TranslateTimeTree(C.Half, 
C .Half, C, C , dx, dy,tran) end if end for end procedure The call to ComputeSpatialAverages() in the 
above procedure cal­culates average colors in the internal nodes of the image tree, using code similar 
to the CreateFrame() routine from Section 3. The TranslateFrame() routine translates a single image tree 
c by an amount (dx, dy) at level tran. In general the translation can cause large regions of constant 
color (leaves high in c) to be broken up across many nodes in the resulting tree c. To handle the up-links, 
we must pass into the procedure the time-parent p of c, as well as the result pof (previously) translating 
p. We also pass into the pro­cedure arguments x, y and (initially all 0), which keep track of the location 
and level of node c. 00 procedure TranslateFrame(c, c, p, p, dx, dy,tran, x, y,): if c.Type = COLOR or 
c.uplink.linked ortran =then w 2 tran a 0r 2f 0 ng 0 n 00 0 r Rectangle(wx + dx, wy + dy, w, w,tran) 
PutRectInTree(c, c, p, r, 0, 0, 0) else for each i, j 0, 1do TranslateFrame(c.child[i, j], c, p.child[i, 
j], p, dx, dy,tran, 2x + i,2y + j,+1) end for end if end procedure The above procedure recursively descends 
image treec, pausing to copy any terminal squares that it encounters as it goes. There are three kinds 
of terminal squares: large regions of constant color, sub­trees that hang from level tran, and up-links. 
In the .rst two cases, we copy the source from the original tree, whereas in the latter case we copy 
from the time-parent. A square s size and position are com­bined in a single structure Rectangle(x, y, 
width, height, r), the co­ordinates of which are relative to the level r. When the procedure .nds one 
of these squares, it copies it into the resulting tree using the following procedure: a 00 procedure 
PutRectInTree(c, c, p, r, x, y,): coverage CoverageType(r, x, y,) if coverage = COMPLETE then if c.type 
= COLOR or not c.uplink.linked then cc else cp c.uplink.linked TRUE end if else if coverage = PARTIAL 
then   0 a 0 2fga 00 for each i, j 0, 1do PutRectInTree(c, c.child[i, j], p.child[i, j], r,2x + i,2y 
+ j,+1) end for end if end procedure 0 The above procedure recursively descends the result treecto .nd 
those nodes that are completely covered by the given rectangle, an approach reminiscent of Warnock s 
algorithm [4]. The function CoverageType(r, x, y, ) returns a code indicating whether rectan­gle r completely 
covers, partially covers, or does not cover pixel (x, y) at level . For those nodes that are completely 
covered, Put-RectInTree() copies either a color or a pointer, depending on the type of node being copied. 
If the node is a color, then the color is simply copied to its new position. If the node is a pointer 
but not an up-link, the routine copies the pointer, which essentially moves an entire subtree from the 
original tree. Finally, if the node is an up­link, the routine copies the corresponding pointer from 
the (already translated) time-parent p. Thus, we have to descend the result tree 000 cand its time-parent 
pin lock-step in the recursive call. As with DrawImage(), the complexity of TranslateFrame() is re­lated 
to the number of nodes it copies using PutRectInTree(). The latter procedure is dependent on the number 
of nodes it encounters when copying a rectangle. Since the former call makes a single pass over the source 
quadtreec, and the collective calls to the latter proce­ 00 dure make a single pass over the resulting 
image treec, the overall complexity is proportional to the sum of the complexities ofc and c. A.2 Scaling 
a video clip Here, we consider scaling a Time Tree by some integer factorssx in the x direction and sy 
in y. Note that to shrink a video frame by any power of two in both x and y we simply insert more image 
parent nodes above the existing image root, .lling in any new siblings with clear. Conversely, to magnify 
a video frame by any power of two, we simply scale all other videos down by that factor, since we are 
only interested in their relative scales. Thus, scaling both x and y by any power of two is essentially 
free, and we can really think of the .. scales as being sx2 and sy2 for any (positive or negative) . 
For ef.ciency, it is best to divide both sx and sy by their greatest common power-of-two divisor. The 
algorithms for scaling multiresolution video are structurally very similar to those for translation. 
The two main differences are that they copy scaled (rather than translated) versions of the source tree 
into the destination tree, and that they must descend down to the leaves of the image trees. We omit 
the speci.c pseudocode for scal­ing a video clip for lack of space. The time complexity of scaling is 
the same as translation: linear in the size of the input and output. A.3 Compositing two video clips 
The .nal operation addressed in this appendix is compositing two Time Trees A and B using the compositing 
operation op [12]: function CompositeTimeTrees(A, B, op): returns TimeTree for each Half Half1, Half2do 
Result.Half NULL else Ahalf A.Half Bhalf B.Half if Ahalf = NULL then Ahalf NewUplinkNode(A) end if if 
Bhalf = NULL then Bhalf NewUplinkNode(B) end if 2fag if A.Half = NULL and B.Half = NULL then  a aa 
 a Result.Half CompositeTimeTrees(Ahalf , Bhalf , op) end if end for Result.frame CompositeFrames(A.frame, 
B.frame, FALSE, FALSE, Result.Half1.frame, Result.Half2.frame, op) return Result end function This function 
recursively traverses A and B in a bottom-up fashion, compositing child time nodes .rst, then their parents. 
If one ofA or B has more temporal resolution than the other, then a temporary node iscreated bythefunction 
NewUplinkNode().Invoking thisfunction with the argument A creates a new TimeNode containing a single 
ImageNode, each of whose four children is an up-link pointing to its time-parent inA. The following function 
works on two image trees a and b, taking a pair of arguments aUp and bUp that are set to FALSE in the 
top­level call; these .ags are used to keep track of whether a and b are really parts of a time-parent. 
The function also takes a pair of argu­ments c1 and c2 that are the time-children of the resulting tree. 
In order to pass c1 and c2, the CompositeTimeTrees() function must have already computed these time-children, 
which is why it makes a bottom-up traversal of the Time Tree. function CompositeFrames(a, b, aUp, bUp, 
c1, c2, op): returns ImageNode if a.uplink.linked then aUp TRUE end if if b.uplink.linked then bUp TRUE 
end if a if aUp and bUp then return NULL end if if a.Type = COLOR and b.Type = COLOR then if c1 = NULL 
or c2 = NULL then return ImageNode(COLOR, CompositePixels(a, b, op)) else return ImageNode(COLOR, Average(c1.avgcolor, 
c2.avgcolor)) end if end if for each i, j 0, 1do  2fga result.child[i, j] CompositeFrames(GC(a, i, j), 
GC(b, i, j), aUp, bUp, GC(c1, i, j), GC(c2, i, j), op) end for result.avgcolor AverageChildColors(result) 
a return result end function We composite two image trees by traversing them recursively, in lock-step, 
compositing any leaf nodes. Child colors are propagated up to parents to construct internal averages. 
The helper function GC() (for GetChild or GetColor ) simply returns its argument node if it is a color, 
or the requested child if it is a subtree. There are two subtleties to this algorithm. The .rst is that 
when the routine .nds some region of the result for which both a and b are up-links (or subtrees of up-links), 
then it can assume that the result will be an up-link as well; in this case it simply returns NULL. Later, 
after all of the frames in the Time Tree have been composited, we invoke a simple function that traverses 
the Time Tree once, replac­ingall NULL pointerswiththeappropriate up-link.(Thisassignment cannot occur 
in CompositeFrames() because the nodes to which the up-links will point have not been computed yet.) 
The second sub­tlety is that if time-child c1 or c2 is NULL it means that the resulting image node has 
no time-children: either the node is part of an im­age tree that hangs from a leaf of the Time Tree, 
or its children are up-links. In either case we perform the compositing operation. If, on the other hand, 
c1 and c2 exist, then we are working on an internal node in the Time Tree and we can simply average c1 
and c2. The compositing operation described in this section creates a new Time Tree that uses up-links 
to take advantage of any temporal co­herence in the resulting video. Since this resulting Time Tree is 
built using two bottom-up traversals, the complexity of creating it is lin­ear in the size of the input 
trees. References [1] J. Adams, R. Garcia, B. Gross, J. Hack, D. Haidvogel, and V. Pizzo. Applications 
of multigrid software in the atmo­spheric sciences. Monthly Weather Review, 120(7):1447 1458, July 1992. 
[2] Deborah F. Berman, Jason T. Bartell, and David H. Salesin. Multiresolution painting and compositing. 
In Proceedings of SIGGRAPH 94, Computer Graphics Proceedings, Annual Conference Series, pages 85 90, 
July 1994. [3] Shenchang Eric Chen. Quicktime VR an image-based ap­proach to virtual environment navigation. 
In Proceedings of SIGGRAPH 95, Computer Graphics Proceedings, Annual Conference Series, pages 29 38, 
August 1995. [4] James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer Graphics: 
Principles and Practice. Prentice-Hall, 1990. [5] D. Le Gall. MPEG: A video compression standard for 
multi­media applications. CACM, 34(4):46 58, April 1991. [6] Randy LeVeque and Marsha Berger. AMRCLAW: 
adaptive mesh re.nement + CLAWPACK. http://www.amath.washington.edu/ rjl/amrclaw/ . [7] A. S. Lewis 
and G. Knowles. Video compression using 3D wavelet transforms. Electronics Letters, 26(6):396 398, 15 
March 1990. [8] S. McCormick and U. Rude. A .nite volume convergence the­ory for the fast adaptive composite 
grid methods. Applied Nu­merical Mathematics, 14(1 3):91 103, May 1994. [9] Leonard McMillan and Gary 
Bishop. Plenoptic modeling: An image-based rendering system. InProceedings of SIGGRAPH 95, Computer Graphics 
Proceedings, Annual Conference Se­ries, pages 39 46, August 1995. [10] Arun N. Netravali and Barry G. 
Haskell. Digital Pictures. Plenum Press, New York, 1988. [11] Ken Perlin and Luiz Velho. Live paint: 
Painting with proce­dural multiscale textures. In Proceedings of SIGGRAPH 95, Computer Graphics Proceedings, 
Annual Conference Series, pages 153 160, August 1995. [12] Thomas Porter and Tom Duff. Compositing digital 
images. In Hank Christiansen, editor, Computer Graphics (SIGGRAPH 84 Proceedings), volume 18, pages 253 
259, July 1984. [13] Steven Radecki. Multimedia With Quicktime. Academic Press, 1993. ISBN 0-12-574750-0. 
[14] Hanan Samet. Applications of Spatial Data Structures. Addison-Wesley, Reading, Massachusetts, 1990. 
[15] P. S. Sathyamurthy and S. V. Patankar. Block-correction-based multigrid method for .uid .ow problems. 
Numerical Heat Transfer, Part B (Fundamentals), 25(4):375 94, June 1994. [16] I. Suisalu and E. Saar. 
An adaptive multigrid solver for high­resolution cosmological simulations. Monthly Notices of the Royal 
Astronomical Society, 274(1):287 299, May 1995. [17] Jonathan Swartz and Brian C. Smith. A resolution 
indepen­dent video language. In ACM Multimedia 95, pages 179 188. ACM, Addison-Wesley, November 1995. 
[18] S. L. Tanimoto and Theo Pavlidis. A hierarchical data struc­ture for picture processing. Computer 
Graphics and Image Processing, 4(2):104 119, June 1975. [19] Lance Williams. Pyramidal parametrics. In 
Computer Graph­ics (SIGGRAPH 83 Proceedings), volume 17, pages 1 11, July 1983. [20] L. Ziv and A. Lempel. 
A universal algorithm for sequential data compression. IEEE Trans. Inform.Theory, Vol.IT-23, (3), May 
1977.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237267</article_id>
		<sort_key>291</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[A cellular texture basis function]]></title>
		<page_from>291</page_from>
		<page_to>294</page_to>
		<doi_number>10.1145/237170.237267</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237267</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31101908</person_id>
				<author_profile_id><![CDATA[81100065604]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Worley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[405 El Camino Real Suite 121, Menlo Park, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[EBERT, D.E. Texturing and Modeling: A Procedural Approach. Academic Press, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808572</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[GARDNER, G.Y. Simulation of natural scenes using textured quadric surfaces. In Computer Graphics (SIGGRAPH '84 Proceedings) (July 1984), H. Christiansen, Ed., vol. 18, pp. 11-20.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74360</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[LEWIS, J.-P. Algorithms for solid noise synthesis. In Computer Graphics (SIGGRAPH '89 Proceedings) (July 1989), J. Lane, Ed., vol. 23, pp. 263-270.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97921</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[MIYATA, K. A method of generating stone wall patterns. In Computer Graphics (SIGGRAPH '90 Proceedings) (Aug. 1990), F. Baskett, Ed., vol. 24, pp. 387-394.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[PEACHEY, D. Solid texturing of complex surfaces. In Computer Graphics (SIGGRAPH '85 Proceedings) (July 1985), B. A. Barsky, Ed., vol. 19, pp. 279-286.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[PEACHEY, D. Writing renderman shaders. In 1992 Course 21 Notes. ACM SIGGRAPH, 1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. An image synthesizer. In Computer Graphics (SIGGRAPH '85 Proceedings) (July 1985), B. A. Barsky, Ed., vol. 19, pp. 287-296.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74359</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K., AND HOFFERT, E. M. Hypertexture. In Computer Graphics (SIGGRAPH '89 Proceedings) (July 1989), J. Lane, Ed., vol. 23, pp. 253-262.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[SCHACHTER, B. J., AND AHUIA, N. Random pattern generation processes. Computer Graphics and Image Processing 10 (1979), 95-114.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Generating textures for arbitrary surfaces using reaction-diffusion. In Computer Graphics (SIGGRAPH '91 Proceedings) (July 1991), T. W. Sederberg, Ed., vol. 25, pp. 289-298.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237268</article_id>
		<sort_key>295</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[A framework for interactive texturing on curved surfaces]]></title>
		<page_from>295</page_from>
		<page_to>302</page_to>
		<doi_number>10.1145/237170.237268</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237268</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Paint systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31089767</person_id>
				<author_profile_id><![CDATA[81332520351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[K&#248;hling]]></middle_name>
				<last_name><![CDATA[Pedersen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University and Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Catmull. A hidden-surface algorithm with anti-aliasing. In Computer Graphics (SIGGRAPH '78 P1vceedings), volume 12, pages 6-11, August 1978.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Richard Shoup. SuperPaint. Xerox PARC, 1974.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Thomas Porter and Tom Duff. Compositing digital images. In Hank Christiansen, editor, Computer Graphics (SIGGRAPH' 84 Proceedings), volume 18, pages 253-259, July 1984.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[1990 computer graphics achievement award. Computer Graphics (SIGGRAPH '90 P1vceedings), 24:17-18, August 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Paul E. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. In Forest Baskett, editor, Computer Graphics (SIGGRAPH '90 P1vceedings), volume 24, pages 215-223, August 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199429</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Maneesh Agrawala, Andrew C. Beers, and Marc Levoy. 3d painting on scanned surfaces. In P1vceedings 1995 Symposium on Interactive 3D Graphics (Monterey, California, April 9-12,1995), pages 145-152.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J6r6me Maillot, Hussein Yahia, and Anne Verroust. Interactive texture mapping. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH '93 P1vceedings), volume 27, pages 27-34, August 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hans K. Pedersen. Decorating implicit surfaces. In Robert Cook, editor, P1vceedings of SIGGRAPH ' 95 (Los Angeles, California, August 6-11,1995), Computer Graphics Proceedings, Annual Conference Series, pages 291-300. ACM SIGGRAPH, ACM Press, August 1995.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218447</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kurt Fleisher, David H. Laidlaw, Bena L. Currin, and Alan H. Barr. Cellular texture generation. In Robert Cook, editor, P~vceedings of SIGGRAPH '95 (Los Angeles, California, August 6-11, 1995), Computer Graphics Proceedings, Annual Conference Series, pages 239-248. ACM SIGGRAPH, ACM Press, August 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>223682</ref_obj_id>
				<ref_obj_pid>223355</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Julie Daily and Kenneth Kiss. 3d painting: Paradigms for painting in a new dimension, chi '95 conference proceedings (denver colorado, may 7-11,1995).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Venkat Krishnamurthy and Marc Levoy. Fitting smooth surfaces to dense polygonal meshes for computer animation. In P~vceedings ofSIGGRAPH' 96 (New Orleans, Louisiana, August4-9,1996), august 1996.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237271</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Matthias Eck and Hugues Hoppe. Automatic reconstruction of b-spline surfaces of arbitrary topological type. In P1vceedings of SIGGRAPH '96 (New Orleans, Louisiana, August 4-9,1996), august 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192227</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Andrew R Witkin and Paul S. Heckbert. Using particles to sample and control implicit surfaces. In Andrew Glassner, editor, P1vceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29,1994), Computer Graphics Proceedings, Annual Conference Series, pages 269-278. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0-89791-667-0.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>172376</ref_obj_id>
				<ref_obj_pid>172372</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[R. Dietz, J. Hoschek, and B. Jtittler. An algebraic approach to curves and surfaces on the sphere and on other quadrics. Computer Aided Geometric Design, 10(3):211-230, August 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Chakib Bennis, Jean-Marc V6zien, G6rard Igl6sias, and Andr6 Gagalowicz. Piecewise surface flattening for non-distorted texture mapping. In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH '91 P1vceedings), volume 25, pages 237-246, July 1991.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Matthias Eck et al. Multiresolution analysis of arbitrary meshes. In Robert Cook, editor, P1vceedings of SIGGRAPH '95 (Los Angeles, California, August 6-11,1995), Computer Graphics Proceedings, Annual Conference Series, pages 173-182. ACM SIGGRAPH, ACM Press, August 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Brian Curless and Marc Levoy. A volumetric method for building complex models from range images. In P1vceedings of SIGGRAPH '96 (New Orleans, Louisiana, August 4-9,1996), august 1996.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Framework for Interactive Texturing on Curved Surfaces Hans Køhling Pedersen Computer Science Department 
Stanford University* Abstract Existing techniques for texturing curved surfaces are either only applicable 
for a limited subset of surface representations (3d paint­ing of parametric patches or dense polygonal 
meshes for exam­ple), ordonotlendthemselvesnaturally to interactivetexture edit­ing (e.g. procedural 
and solid textures). Although such methods have been used to produce stunning effects, there is a lack 
of .ex­ible and general purpose tools, such as those provided by 2d im­age processing applications. This 
work argues that interactive tex­turing could bene.t from a more cohesive paradigm built around a kernel 
of powerful and general operations. Using an analogy to the evolution of 2d painting algorithms, the 
paper motivates a framework for interactive texturing operations on curved surfaces and describes an 
approach for translating, rotating, and warping regions of texture (patchinos) on a surface. These ideas 
have been implemented for parametric and implicit surfaces. As an interest­ing side effect, this more 
uni.ed framework also opens the door to a number of new interactive 3d texturing techniques that have 
no natural counterparts in two dimensions.  Introduction In the past decade, 2d painting systems have 
revolutionized the .eld of desktop publishing. The success of these products has stimulated an intense 
research interest in interactive image pro­cessing tools and a diverse range of applications based on 
highly specialized operations has been absorbed into this thriving market, such as Adobe PhotoShop , 
Fractal Design Painter and Kai s Power Tools . However, underneath this diversity is an under­lying framework 
consisting of a few general and powerful algo­rithms, most prominently the concepts of digital compositing 
[1] and copy and paste [2]. The idea behind compositing is to reduce the complexity of ren­deringbyseparatingtheimageinto 
anumberoflayers,generatean image for each layer, and subsequently synthesize the layers into one composite 
image using mattes and alpha blending [3]. Copy and paste operations allow portions of an image to be 
extracted, moved, warped and repositioned interactively. The two ideas sup­plement each other well as 
they share the underlying philosophy of looking at an image as a combination of layers that can be com­bined 
using high level operators. The two methods will henceforth be referred to under the common term of image 
compositing. . I can be reached at Laboratory for Computer Science, Mas­sachusetts Institute of Technology. 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 Today, image compositing forms the foundation for all com­mercial 
painting packages, a success which can be attributed to a convenient and intuitive user interface, simplicity 
and elegance of the underlying theoretical model, and computational ef.ciency that makes the idea practical. 
1.1 Interactive 3d texturing Compared to traditional framebuffer systems, which date back to the early 
1970s [4], 3d painting is a new discipline [5]. Recently, however, the market for these products has 
experienced an explo­sive growth that rivals that of 2d systems a decade ago. Unfortu­nately, the literature 
on 3d painting algorithms exhibits a tendency to focus on one particular type of surface representation, 
such as parametric patches [5], scanned polygonal meshes [6], parameter­ized meshes [7], or implicit 
surfaces [8], thus polarizing the spec­trum of painting algorithms rather than moving towards a unify­ing 
standard. Although the fundamental differences between the underlying surface representations make certain 
operations bet­ter suited for one particular class of surfaces than another (cubic patches, for example, 
are ideal for patterns utilizing trim curves, while polygonal meshes often provide a more compact represen­tation 
for low frequency textures), it would be desirable to iden­tify a nucleus of operations, a lowest common 
denominator, that could serve as a general framework for all painting systems. In order to design such 
an architecture, we need to identify a suit­able core of generic operations. Fleisher et al. [9] introduced 
a novel framework for cellular texturing compatible with all stan­dard surface representations. However, 
since our focus is on more general operations supported by an intuitive interface, the proven concept 
of image compositing forms an even better source of in­spiration. Furthermore, Daily and Kiss s [10] 
recent study of users of painting systems concluded that artists are more likely to em­brace a 3d application 
if its interface resembles that of the well known 2d packages. Motivated by these observations, this 
paper proposes a framework for interactive texturing of general smooth surfaces that extends the power 
of the interactive image composit­ing paradigm to three dimensions. 1.2 Overview Section 2 presents 
an architecture for texturing operations on smooth surfaces, followed by a generalization of the basic 
con­cepts of image compositing to surfaces in section 3. Section 4 dis­cussesmore advancedhigh level 
operations, followed by a conclu­sion and ideas for future work.  2 Architecture Before moving on to 
texture compositing on surfaces, we will start by developing an architecture that will allow such operations 
to be applied on general smooth models. Pedersen [8] took a step towards this goal with an approach for 
placing images arbitrarily on implicit surfaces in lieu of extending the method to other types of surfaces. 
In this section, a more elaborate architecture will be presented. 2.1 Surfaces For some operations, 
such as dragging curves smoothly across a surface, it is most convenient to work with a representation 
that offers continuous derivatives, but for others a polyhedral approx­imation is more practical (estimating 
coordinate transformations between a surface and a patch on it, for example). To get the best of both 
worlds, surfaces are represented by a differentiable func­tion as well as a parameterized polygonal mesh. 
For our study, we will assume that both representations are readily available. This assumptionis reasonablebecausesuchtechniquesexistforwidely 
used surface representations, such as spline patches (the trivial case), implicit surfaces [8], polyhedral 
surfaces [7, 11] and dense uniform point clouds [12]. Class Vector2 Real u,v Class Vector3 Real x,y,z 
Class Surface private: DifferentiableSurface smooth ; Differentiable function Mesh mesh; Parameterized 
polygonal mesh public: ComputeNormal(Sample p) 7!Vector3 Slide(Sample p,Vector3 v) ComputeCurve(Sample 
p1,Sample p2) 7!Curve GetTextureCoordinates(Sample p) 7!(u,v,patch id) Class DifferentiableSurface virtual 
Vector3 ComputeNormal(Sample p) virtual Slide(Sample p,Vector3 v) Class Parametric : public DifferentiableSurface 
private: Patch patches[MAX SIZE] ; Parameterizations for individual patches public: ComputeNormal(Sample 
p) 7!Vector3 Slide(Sample p,Vector3 v) Class Implicit : public DifferentiableSurface private: Gradient(Vector3 
x) 7!Vector3 AttractPoint(Vector3 x) 7!Vector3 ; Prevents points from drifting away. ; See [13] for details. 
public: ComputeNormal(Sample p) 7!Vector3 Slide(Sample p,Vector3 v) Figure 1: Representation of surfaces. 
See section 2.1 for further comments. More speci.cally, surfaces are represented by the data struc­ture 
shown in .gure 1. Slide moves a sample point pwith velocity vconstrained to the surface1, Surface : ComputeCurve 
computes a curve between two arbitrary points on the surface, and GetTex­tureCoordinates determines the 
surface texture coordinates corre­sponding to a point on the surface. Internally, ComputeNormal and Move 
are implemented using only the differentiable function, while the two latter procedures utilize both 
the smooth and polyg­onal representations. Any high level texturing algorithm that can be implemented 
in terms of these primitives can be applied for any class for surfaces for which the above library is 
available.  2.2 Differentiable surfaces Class DifferentiableSurface is used for operations that require 
a continuous derivative. In principle, it could have any number of sub-classes corresponding to subsets 
of surfaces with different mathematical representations, but we will focus on the important cases of 
implicit and parametric surfaces. Because different types of surfaces possess inherently different internal 
representations, surface samples are also represented as derived classes. The cor­responding data structures 
are shown in .gure 2. Class Sample Vector3 x; Position Class ParametricSample : public Sample Real u,v;(u,v) 
coordinates Integer patchid; Index of surface patch Implicit: ComputeNormal(Sample p) 7!Vector3 g=Gradient(p:x) 
return g jgj Implicit: Slide(Sample p,Vector3 v) vproj=p:ProjectToTangentPlane(v) p:x=p:x+vproj p:x= 
AttractPoint(p:x) Parametric: ComputeNormal(Sample p) 7!Vector3 <u= patches[p.patch id].U Derivative(p:u;p:v) 
<v= patches[p.patch id].V Derivative(p:u;p:v) return ¢uX¢v j¢uX¢vj Parametric: Slide(Sample p,Vector3 
v) vproj= p:ProjectToTangentPlane(v) hhUpdate (p:u,p:v,p.patch id) by vprojii ; See section 2.2 for detailed 
comments Figure 2: Implementation of smooth surfaces. In Parametric:Slide, the velocity vector vis projected 
to the tangent plane and the partial derivatives are used to estimate a uv­offset, which is added to 
(p:u,p:v). Care must be taken when a sample moves across a patch boundary: .rst, the point is moved 1For 
simplicity, it is assumed that jvjis small enough for the differen­tiable surface to be approximated 
by its tangent plane within this distance. Larger velocity vectors should be subdivided into a number 
of smaller steps. to the boundary of the patch, then (p:u,p:v,p:patchid)is set to the corresponding triple 
in the adjacent patch s coordinates and, .nally, a recursive call to Slide completes the operation. Notice 
that although procedures like ComputeNormaland Slide may be implemented very differently for parametric 
and implicit surfaces, we do not have to worry about such low level details when we move on to design 
higher level operations. Thus, from now on we will no longer distinguish between different types of surfaces 
but formulate all algorithms in terms of the methods of the Surface superclass.  3 Texture compositing 
Whereas digital images can be represented by a simple uniform grid of samples, the corresponding problem 
of sampling a texture on a 3d surface currently has no equally convenient solution. We choose to sample 
the texture signal in texture maps correspond­ing to parameterized polygonal patches (see [8] for a motivation 
of this representation). Similar to image compositing, our approach consists of three simple steps: 1. 
Copy a region of texture. 2. Move it. 3. Paste it back.  This section will describe how each step 
can be generalized to surfaces. 3.1 Copying and pasting 2d painting systems offer a variety of interactive 
and automatic tools for outlining regions of an image. After a region has been selected, a rectangular 
bounding region is copied along with an alpha channel specifying the opaqueness of each pixel. Whereas 
computing bounding boxes and copying pixels is uncomplicated for images, the corresponding problems for 
curved surfaces are a little less straightforward. In our system, the region to be extracted is outlined 
by one or more closed polygonal curves on the surface using the Com­puteCurve operation. Ideally, it 
would be convenient to compute a bounding box for the region automatically, but we leave this problem 
to future work and currently draw a rectangular bound­ing patch interactively. From now on, such a patch 
will be re­ferred to as a patchino to distinguish it from the patches that con­stitute the polygonal 
mesh. After the patchino has been parame­terized (see section 4 of [8] for details), the closed curves 
are pro­jected to it ([8], section 5.1) and a matte is computed by perform­ing an inside-outside test 
for each texel (in practice, we tessellate the closed regions and scan-convert the resulting triangles 
directly into the alpha channel). Finally, a coordinate transformation be­tween the surface patches and 
the patchino is computed and used to copy the texels from the surface into the texture space of the lat­ter. 
In section 5.2.1 of [8], an algorithm for pasting textures from a patchino to a surface was outlined 
(see also .gure 9a), and the corresponding cut operation can be implemented similarly using the GetTextureCoordinates 
primitive (.gure 9b). Although these algorithms are somewhat more time consuming to implement for surface 
textures than for images, the procedures are completely analogous and the simple and intuitive user interface 
is preserved. 3.2 Moving textures -overview In two dimensions, regions of an image can be translated, 
rotated and scaled using simple af.ne transformations. Unfortunately, the attractive simplicity of planar 
geometry does not generalize di­rectly to curved surfaces. In differential geometry, the literature on 
curves and surfaces in surfaces present various approaches to the problem of describing regions of curved 
surfaces independently of the surface representation. In some cases, such as cubic spline patches on 
low degree implicit surfaces [14], it is possible to de­rive expressions for regions of a surface analytically, 
but, unfortu­nately, the range of mathematical tools for analyzing this problem is limited, and existing 
results are too special case to be practical for our problem: to slide patches freely across general 
surfaces re­liably and at interactive speed. Due to these shortcomings of existing analytical tools, 
we choose to make some simplifying assumptions: 1. A patchino is approximated by a mesh of coupled springs 
connecting a grid of sample points. 2. As the patchino moves across the surface, only the sample points 
are constrained to remain on the surface.  Given these assumptions, moving a patchino can be formulated 
as a constrained optimization problem, namely that of minimizing metric distortion relative to some rest 
shape subject to the con­straint that all samples in the grid must remain on the surface. Al­though parts 
of the patchino thus may not lie exactly on the sur­face, the accuracy with which it approximates the 
surface geome­try can be chosen arbitrarily by increasing or decreasing the den­sity of the samples in 
the mesh. Since most smooth surfaces are eventually rendered as a set of polygons instead of a differentiable 
function, it is not unreasonable to use a polygonal approximation to the patches in the texturing stage 
as well. We will return with a further discussion of the implications of these assumptions in sec­tion 
6. 3.3 Moving textures -theory Let c:U7!Sbe a parameterization of a regular patch, R,on smooth surface, 
S, that has a normal vector .eld with continuous derivative (notice that Sis not required to have a parameteriza­tion). 
Finding a reparameterization such that the metric distortion ZZ E(U)= Error(cu;cv)dudv U is minimal, 
where cuand cvdenote the partial derivatives and Erroris some objective function measuring the distortion 
within the patch, is a standard problem in graphics research [15, 7, 16, 11]. Various functionals have 
been proposed, weighting the preservationofanglesanddistancesindifferentways. TheGreen-Lagrange deformation 
tensor is a simple example: ZZ EGL(U)= jjG¢(u;v)-Ijj2dudv U ZZ =(cu 2 -1)2 +2(cucv) 2 +(cv 2 -1)2dudv 
U where G¢denotes the metric tensor of c. Our problem, however, is slightly different: instead of mini­mizing 
distortion relative to a plane, it is minimized relative to a curved rest shape parameterization B: ZZ 
E(U)= jjG¢(u;v)-Ge(u;v)jj2dudv U ZZ 22 22 22 =(cu-Bu)+2(cucv-BuBv)+(cv-Bv)dudv; U subject to the constraint 
that c(U).S. Just like EGLpenalizes the deviation between the metric tensor to cand the identity ma­trix, 
Emeasures the difference between the metric tensor of cand that of B. Naturally, there is a tradeoff 
between minimizing metric distortion and the stick-to-surface constraint, and, aside from a few simple 
surfaces, it is impossible to avoid some degree of dis­tortion. Although conceptually simple, the problem 
of implementing these ideas feasibly in an interactive system is challenging. To make the approach practical, 
we will have to replace Ewith a slightly different functional. 3.3.1 Discretization Given a patch on 
a surface, its parameterization, B, issaidtobe the rest shape of any other patch (with parameterization 
c)for which E(U)=0. As patchinos are stored as a discrete grid of samples rather than a continuous function, 
the rest shape is represented as a list of spring coordinates, (ku;kv), for each node, pi, in the grid: 
ij ij qij:xproj=pi:x+kupi:Bu+kvpi:Bv where qij:xprojdenotes the projection of pi s j th neighbor onto 
the tangent plane at pi:x(see also Sample in .gure 7), and (kijij u;kv)are the rest coordinates of spring 
jemanating from node i. Each (ku;kv)pair thus represents the coordinates of an adjacent node in the tangent 
plane through p:xand spanned by the basis p:Bu;p:Bv(see .gure 3). Figure 3: The local coordinates of 
the rest lengths of the springs are measured in the (Bu;Bv)coordinate frame at p. The reason for measuring 
the rest lengths of the springs in lo­cal coordinates rather than in absolute distances is that the partial 
derivatives inevitably will change as the patchino moves across the surface, and .ghting this distortion 
by trying to keep the metric tensors identical everywhere (see Eabove) is a losing battle that will quickly 
result in the patchino folding onto itself. Our solution to the tradeoff between minimizing Eand preserving 
the structural integrity of the grid is to assign a priority to each sample point, in­dicating the relative 
importance of minimizing distortion in this particular region. Since the base surface was assumed to 
be rel­atively smooth, it is reasonable to allow a greater amount of dis­tortion near the edges of a 
patchino than at its center, and this can be accomplished using local coordinates as described above 
and controlling the order in which the nodes are updated in each itera­tion. More speci.cally, the algorithm, 
which will be described in detail in section 3.4, proceeds in a spiral pattern emanating from the center 
as shown in .gure 4. The discretization of the modi.ed Eis XX . ij ij2 ij ij2 E(U)= (k-k)+(k-k) u u v 
v ij Figure 4: The order in which the nodes are visited forms a spiral emanating from the center node. 
where the greek subscript denotes the parameterization for which the derivatives de.ne the basis vectors 
where the spring coordi­nates are measured. In essence, the procedure adapts the stiffness of the springs 
to the geometry of the surface, but still preserves the shape of the patch in the sense that no matter 
how far it moves from the location where its rest shape was speci.ed, it will reattain its original shape 
when it returns. To summarize, the algorithm works by .rst estimating the par­tial derivatives at each 
sample point using .nite differences, then minimizing E.by updating the positions of the samples in a 
spe­ci.c order subject to the constraint that all the samples must re­main on the surface. These two 
steps are repeated continously, only interrupted by external forces exerted on a patchino by user interaction. 
We will return with more details on the implementa­tion in a moment. 3.3.2 Moving patchinos Motion can 
be integrated into the optimization procedure by re­peating two steps. First, each sample point pi:xis 
moved with a velocity vector pi:v, perturbing E.slightly from a local minimum, and the objective function 
is then pulled back towards a minimum as described in the previous section. As with any optimization 
technique, the robustness depends on the step size, and the trick is to pick the sample velocities so 
that the updated positions will remain in the proximity of a local min­imum for E. .  3.4 Moving textures 
-implementation Figure 7 outlines an implementation of the most basic operations needed for interacting 
with patchinos. In the following, each step will be described in more detail. 3.4.1 Freezing the rest 
shape Before a patchino can be moved, its rest shape must be initialized to the parameterization of a 
patchino. The algorithm for freezing thecurrentshapeofapatchtoits restshapeissketchedas Patchino : Freeze 
in .gure 7. Given the resulting data structure and the par­tial derivatives at each node, the mesh can 
be optimized towards the rest shape as shown in procedure Pathino : ReduceDistortion. 3.4.2 Translation 
In our interface, the user selects a patch by clicking on a control point. As the projection of this 
point is dragged in screen space, the patch moves constrained to the surface so that the control point 
on the patch follows the cursor. Per default, the control point is taken to be the center vertex, but 
it could be any .xed point on the patch. Thus, the input to the translation algorithm is simply a three-dimensional 
vector.  Figure 5: Propagating transformation velocities through a patch. When a transformation is applied 
to the center point in the grid, similar transformations are applied to its neigh­bors. If this velocity 
vector only was applied to the center node, it might take many iterations for the operation to propagate 
to the entire patchino grid, and this lag makes the idea infeasible in an interactive system. A naive 
alternative would be to add the same velocity vector to every node, but this idea fails because it violates 
the stick-to-surface constraint in regions of high curvature. In­stead, the velocity vector estimates 
are propagated outwards from the control point in a spiral order (see .gure 4): First, the location of 
the control sample, p:x, is moved with velocity vconstrained to the surface using Surface : Slide, and 
the tangent plane com­ponent of the difference between the new and the old position is stored in p:v. 
Second, for each immediate neighbor, q, to the con­trol point, p:vis projected to the tangent plane at 
q:xand restored to its original length, and the resulting velocity vector is used to update q:xconstrained 
to the surface. Finally, the velocity vector q:vis computed as described for p:v. This spiral continues 
until every vertex has been visited. 3.4.3 Rotation Rotations are performed by pulling a handle vector 
(see .gure 9c) emanating from the center node and constrained to the tangent plane. As the user moves 
the cursor, the patchino is updated so that the center node remains .xed in screen space and the projec­tion 
of the handle vector always points towards the cursor. The input to the rotation operation is thus an 
angular offset in the tan­gent plane to the center node. Guessing a set of velocity vectors for a rotation 
is a little more tricky than the similar problem for translations, as even a tiny rotation angle could 
potentially result in large displacements at nodes further from the center, thus causing numerical dif.culties. 
Our solution is to propagate the rotation velocity vectors using the same maximum step size for every 
node in the grid. This yields a faster angular velocity for nodes near the center of a patchino than 
for nodes at its border, bounding the maximum step size at the ex­pense of introducing a delay for nodes 
further away from the cen­ter. In practice, this heuristic has performed well, and it provides a sensible 
solution to the tradeoff between interactive feedback and robustness. The details of the algorithm are 
outlined as Patchino : Rotate on .gure 7. 3.4.4 Warping While scaling and shearing are standard features 
in traditional painting systems, it appears that these operations may not be as useful on curved surfaces. 
Instead, more general non-linear warp­ing operations have proven to be convenient. Patches are not restricted 
to remain the same shape while they are being translated and rotated on the surface. The user can inter­actively 
deform a patch by manipulating control points. This type Figure 6: Propagating rotations to the samples 
in a patch. Given a rotation angle !in the tangent plane, the neighbors to ptry to rotate around this 
point, while remaining con­strained to the surface. For example, sample qmoves to q 0 . The samples are 
updated in the same order as shown on .g­ure 4. of warping is simple to implement given the framework 
described for the above operations:all that needs to be done is to modify the rest shape of the patch 
as it is being deformed, and the optimiza­tion process described in section 3.3.1 will automatically 
make the patch return to its new rest shape. It is best to use a separate mesh of texture springs to 
store the warped patch coordinates. The original mesh of geometry springs should remain relatively undistorted, 
as the translation and rota­tion operations require this mesh to be as regular as possible to assure 
maximum robustness. Therefore, the warped coordinate system is speci.ed independently of the geometry 
coordinates (see .gure 8) with the exception that geometry nodes at patchino boundariescanberepositionedfreely 
(textureandgeometrynodes coincide at the boundaries). At any time, the user can map the warped texture 
into the texture space of the patch using the Sur­face : GetTextureCoordinates operation. In our current 
implementation, the interface lets the user click on a number of feature points inside the patch and 
move these freely. The key points can be constrained to remain at .xed po­sitions in the patch, allowing 
detailed feature alignment between texture and geometry (see .gure 10). These basic operations could 
be extended to a more sophisticated library of warping tools, al­lowing points, curves and regions inside 
a patch to be dragged and constrained. In our experience, surface warping is a natural and useful ex­tension 
of the translation and rotation operations, and it forms the last element in our interactive texturing 
framework.   4 Higher level operations Just as there are 2d image processing methods that have no mean­ing 
on curved surfaces, there are interactive texturing algorithms unique to surfaces. An interesting aspect 
of the architecture de­scribed in the previous sections is that it has served as inspira­tion for a number 
of new texturing operations that have no natural counterparts in traditional 2d painting systems. 4.1 
Cylindrical patchinos As many interesting shapes have parts that are relatively cylindri­cal, a cylindrical 
patchino feature has been implemented. Cylin­drical patchinos are de.ned by three boundary curves: two 
cyclic geodesic curvature minimizing interpolants and one geodesic ar­ranged in the shape of an oldfashioned 
pair of eye-glasses (see .g­ures 9d and 11) and parameterized using a straightforward exten­sion of the 
techniques described in [8] and [11]. The optimization Class Sample Vector3 x;n;v; Position, normal 
and velocity Vector3 <u,<v ; Derivatives Sample neighbors[8] ; Spring coordinates Vector2 k[8] Class 
Patchino private: DifferentiableSurface surface ; Pointer to surface Sample grid[DIM U][DIM V] ; Grid 
of samples public: Freeze() ; Set B=< Translate(Vector3 v) ; Slide in direction v Rotate(Real !) ;Rotate 
by angle !around center ReduceDistortion() ; Minimize E*(U) Patchino : Freeze() for pi2grid for qij2pi:neighbors 
hh(pi:k[j]:u;pi:k[j]:v)= coordinates of qij:x-pi:xprojected to tangent plane at qij:x as a linear combination 
as qij:<uand qij:<uii Patchino : ReduceDistortion() newp:x:x=newp:x:y=newp:x:z=0 for p2gridin spiral 
order for q2p:neighbors newp:x=newp:x+q:x+ q:k[j]:u·q:<u+q:k[j]:v·q:<v ; Here, q:neighbors[j]=p for p2grid 
p:x=newp:x/#p:neighbors hhRecompute n, <:uand <:vat each sample ii Patchino : Translate(Vector3 v) center=grid[DIM 
U/2][DIM V/2] surface.Slide(center,v) center:v= hhcenter:x-center:xold projected to tangent plane at 
center:xii for p2grid/fcentergin spiral order ; See .gure 4 q=p:neighbors["interior00]; See section 3.4.2 
surface.Slide(p;q:v) p:v= hhp:x-p:xoldprojected to tangent plane at p:xii hhRecompute n, <:uand <:vat 
each sample ii Patchino : Rotate(Real !) center=grid[DIM U/2][DIM V/2] hhRotate center:<uand center:<v 
by !in tangent plane at center:xii for p2grid/fcentergin spiral order q=p:neighbors["interior00];See 
text v=hhp:xrotated by !around q:xin tangent plane at q:xii-p:x surface.Slide(p,v) hhEstimate p:<uand 
p:<vbased on p:xand q:xii hhRecompute n, <:uand <:vat each sample ii  Figure 8: Texture warping. The 
regular grid illustrates the geometric coordinates of the patch, which should remain as undistorted as 
possible at all times. The curved lines shows the separate texture coordinate grid for the patch. procedure 
from rectangular patchinos generalizes almost directly, except that the order in which the nodes are 
visited is speci.ed by circular curves, emanating from middle of the cylinder and pro­ceeding up and 
down, rather than a spiral. Experiments have shown that compositing using cylindrical patchinos is a 
highly useful addition to the lower level opera­tions from section 3. Moreover, the general idea of working 
with patches of different topologies present an interesting new set of sub-problems, such as how to apply 
transformations between rect­angular and cylindrical patches interactively and how to deal with rotations. 
Potential applications of spherical and Mobius strip patchinos are left as a thought experiment for the 
interested reader.  4.2 Multi-layered compositing In our system, any number of texture patches can exist 
on the sur­face at any time, and the user is free to translate, rotate, scale and deform these by clicking 
on them. Motivated by the image com­positing paradigm, the patches reside in different layers and can 
be lowered or raised similar to the way windows can be manipulated on graphically oriented operating 
systems. At any time, textures can be copy and pasted between any patch and the surface. Aside from the 
special case of mappings be­tween a texture patch and the surface as described in section 3.1, textures 
can also be mapped between arbitrary combinations of patchinos and the surface (see .gure 9e). This facilitates 
general image processing operations between sets of patches equivalent to [3] and thus the entire digital 
compositing paradigm to be ap­plied on curved surfaces. Current texture mapping hardware sup­ports alpha 
blending and allows the various textures to be rendered at interactive rates, making the approach practical 
on widely used high-end graphics workstations.  5 Future work We will conclude with a few ideas for 
other new and potentially useful texturing operations. Geometric patterns. A look at almost any intriguing 
3d sur­face will reveal a correlation between geometry and textures. Tools allowing artists to take advantage 
of symmetries and con­strain the position and extent of patchinos relative to each other would probably 
be very useful. Swiss cheese patchinos. Some surfaces have regions that are relatively .at except for 
a hole or a branch extending outwards. In such cases, it would be convenient to operate with patchinos 
that would ignore the highly irregular regions. This might be accom- Figure 7: Implementation of patchinos. 
plished by drawing feature curves around the base of a branch and constraining a surrounding patchino 
not to move within these. Copy and paste of surface geometry. Recent progress in 3d data acquisition 
and surface .tting techniques ([17] and [11], [12]), present an interesting challenge in how to texture 
models of a hitherto unseen level of complexity. This problem might be alleviated by a new range of interactive 
applications between tra­ditional modeling and painting systems. Operations supported by such systems 
could include features for dragging free-form­deformation lattices constrained to a surface while minimizing 
volumetric distortion subject to appropriate constraints, and copy and paste of actual geometric features 
extending from the surface. 6 Limitations The underlying assumption of this work is that the base surface 
has to be smooth . If the user tries to drag a patchino over a sharp spike or across any region containing 
frequencies that are high compared to the sampling density of the spring mesh, the patchino mesh may 
start to fold onto itself. In this case, the user currently looses the rest shape and has to reparameterize 
the interior of the patchino. Although improved robustness would obviously be de­sirable, the prototype 
implementation of these ideas is capable of dealing with a suf.ciently general range of surfaces to make 
the approach feasible (see .gures 9f, 12 and 13). In a nutshell, the more irregular the surface, the 
less favorable texture compositing is going to be compared to brush painting, and just like copy and 
paste has not replaced hand painting in 2d systems, the same is unlikely to happen for 3d surfaces. However, 
severe distortion is unavoidable on highly irregular surfaces, and as severely distorted texture mappings 
rarely look attractive, it is questionable whether this capability would even be desirable. Instead, 
it appears that texture compositing of relatively smooth surfaces could be a much more feasible endeavor. 
 7 Conclusion Considering the demand for interactive texturing algorithms for 3d painting applications, 
there is a need for a more cohesive method­ology for texturing of 3d surfaces. This work has proposed 
a ker­nel of powerful and general operations in the hope that this will serve as a starting point for 
a process that could eventually lead to more friendly 3d painting applications. Furthermore, a stronger 
underlying framework for texturing of general surfaces could po­tentially help point towards new interesting 
directions of research and thus accelerate the development of exciting new interactive tools. 8 Acknowledgments 
Thanks to Pat Hanrahan for a fun year in California, to Venkat Kr­ishnamurthy for many helpful and enjoyable 
discussions and for allowing me to use his excellent parameterization package [11], to Julie Dorsey for 
lots of inspiration and cool ideas, to Tamara Munzner for kindly helping with an important video tape 
in the last minutes before a challenging deadline, to Brian Curless for allow­ing me to use his friendly 
3d scanning software [17], and to all of the reviewers for their much appreciated advise. Finally, I 
would like to thank everyone in the Stanford graphics group who helped make my visit as pleasant as it 
was, i særdeleshed et stort tak til Bill Lorensen for being the best sort-of-Danish of.cemate I have 
ever had. References [1] E. Catmull. A hidden-surface algorithm with anti-aliasing. In Computer Graphics 
(SIGGRAPH 78 Proceedings), volume 12, pages 6 11, August 1978. [2] Richard Shoup. SuperPaint. Xerox PARC, 
1974. [3] Thomas Porter and Tom Duff. Compositing digital images. In Hank Chris­tiansen, editor, Computer 
Graphics (SIGGRAPH 84 Proceedings), volume18, pages 253 259, July 1984. [4] 1990computergraphicsachievementaward. 
Computer Graphics (SIGGRAPH 90 Proceedings), 24:17 18, August 1990. [5] Pat Hanrahan and Paul E. Haeberli. 
Direct WYSIWYG painting and texturing on 3D shapes. In Forest Baskett, editor, Computer Graphics (SIGGRAPH 
90 Proceedings), volume 24, pages 215 223, August 1990. [6] ManeeshAgrawala,AndrewC.Beers,andMarcLevoy.3dpaintingonscanned 
surfaces. In Proceedings 1995 Symposium on Interactive 3D Graphics (Mon­terey, California, April 9 12, 
1995), pages 145 152. [7] J´er ome Maillot, Hussein Yahia, and Anne Verroust. Interactive texture map­ping. 
In James T. Kajiya, editor, Computer Graphics (SIGGRAPH 93 Pro­ceedings), volume 27, pages 27 34, August 
1993. [8] Hans K. Pedersen. Decorating implicit surfaces. In Robert Cook, editor, Proceedings of SIGGRAPH 
95 (Los Angeles, California, August 6 11, 1995), Computer Graphics Proceedings, Annual Conference Series, 
pages 291 300. ACM SIGGRAPH, ACM Press, August 1995. [9] Kurt Fleisher, David H. Laidlaw, Bena L. Currin, 
and Alan H. Barr. Cellular texture generation. In Robert Cook, editor, Proceedings of SIGGRAPH 95 (Los 
Angeles, California, August 6 11, 1995), Computer Graphics Proceed­ings, Annual Conference Series, pages 
239 248. ACM SIGGRAPH, ACM Press, August 1995. [10] Julie Daily and Kenneth Kiss. 3d painting: Paradigms 
for painting in a new dimension,chi 95conferenceproceedings(denvercolorado,may 7 11,1995). [11] Venkat 
Krishnamurthy and Marc Levoy. Fitting smooth surfaces to dense polygonal meshes for computer animation. 
In Proceedings of SIGGRAPH 96 (New Orleans, Louisiana, August 4 9, 1996), august 1996. [12] Matthias 
Eck and Hugues Hoppe. Automatic reconstruction of b-spline sur­faces of arbitrary topological type. In 
Proceedings of SIGGRAPH 96 (New Orleans, Louisiana, August 4 9, 1996), august 1996. [13] Andrew P. Witkin 
and Paul S. Heckbert. Using particles to sample and control implicit surfaces. In Andrew Glassner, editor, 
Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994), Computer Graphics Proceedings, An­nual 
Conference Series, pages 269 278. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0-89791-667-0. [14] R. Dietz, 
J. Hoschek, and B. J¨uttler. An algebraic approach to curves and sur­faces on the sphere and on other 
quadrics. Computer Aided Geometric Design, 10(3):211 230, August 1993. [15] Chakib Bennis, Jean-Marc 
V´ezien, G´erard Igl´esias, and Andr´e Gagalowicz. Piecewise surface .attening for non-distorted texture 
mapping. In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH 91 Proceedings), vol­ume 25, pages 
237 246, July 1991. [16] Matthias Eck et al. Multiresolution analysis of arbitrary meshes. In Robert 
Cook, editor, Proceedings of SIGGRAPH 95 (Los Angeles, California, Au­gust 6 11, 1995), Computer Graphics 
Proceedings, Annual Conference Series, pages 173 182. ACM SIGGRAPH, ACM Press, August 1995. [17] Brian 
Curless and Marc Levoy. A volumetric method for building complex models from range images. In Proceedings 
of SIGGRAPH 96 (New Orleans, Louisiana, August 4 9, 1996), august 1996. Figure 9: Patchinos on an implicit 
surface. a) Pasted tex­ture. b) Copied texture. c) Interactive handle for rota­tions. d) Cylindrical 
patchino. e) Layered operations: tex­ture mapped from one patchino to another. f) This and the other 
patchinos were dragged from the back of the dog in less than 10 seconds. g) Warped texture.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237269</article_id>
		<sort_key>303</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[A volumetric method for building complex models from range images]]></title>
		<page_from>303</page_from>
		<page_to>312</page_to>
		<doi_number>10.1145/237170.237269</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237269</url>
		<keywords>
			<kw><![CDATA[isosurface extraction]]></kw>
			<kw><![CDATA[range image integration]]></kw>
			<kw><![CDATA[surface fitting]]></kw>
			<kw><![CDATA[three-dimensional shape recovery]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Least squares approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Size and shape</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14202804</person_id>
				<author_profile_id><![CDATA[81100587184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curless]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>218424</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[C.L. Bajaj, F. Bernardini, and G. Xu. Automatic reconstruction of surfaces and scalar fields from 3D scans. In Proceedings of SIGGRAPH '95 (Los Angeles, CA, Aug. 6-11,1995), pages 109-118. ACM Press, August 1995.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357349</ref_obj_id>
				<ref_obj_pid>357346</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J.-D. Boissonnat. Geometric structures for three-dimensional shape representation. ACM Transactions on Graphics, 3(4):266-286, October 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C.H. Chien, Y.B. Sire, and J.K. Aggarwal. Generation of volume/surface octree from range data. In The Computer Society Conference on Computer Vision and Pattern Recognition, pages 254-60, June 1988.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C.I. Connolly. Cumulative generation of octree models from range data. In P1vceedings, Intl. Conf. Robotics, pages 25-32, March 1984.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[B. Curless. Better optical triangulation and volumetric reconstruction of complex modelsfivm range images. PhD thesis, Stanford University, 1996.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>840039</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[B. Curless and M. Levoy. Better optical triangulation through spacetime analysis. In P1vceedings of lEEE International Conference on Computer Vision, pages 987- 994, June 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. Dolenc. Software tools for rapid prototyping technologies in manufacturing. Acta Polytechnica Scandinavica: Mathematics and Computer Science Series, Ma62:l-lll, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>209499</ref_obj_id>
				<ref_obj_pid>209495</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. Eberly, R. Gardner, B. Morse, S. Pizer, and C. Scharlach. Ridges forimage analysis. Journal of Mathematical Imaging and Vision, 4(4):353-373, Dec 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147153</ref_obj_id>
				<ref_obj_pid>147130</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[H. Edelsbrunner and E.R Mticke. Three-dimensional alpha shapes. In Workshop on Volume Visualization, pages 75-105, October 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[A. Elfes and L. Matthies. Sensor integration for robot navigation: combining sonar and range data in a grid-based representation. In P~vceedings of the 26th IEEE Conference on Decision and Contlvl, pages 1802-1807, December 1987.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[H. Gagnon, M. Soucy, R. Bergevin, and D. Laurendeau. Registration of multiple range views for automatic 3-D model building. In P~vceedings 1994 IEEE Computer Society Conference on Computer Vision and Pattern Recogn#ion, pages 581- 586, June 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[E. Grosso, G. Sandini, and C. Frigato. Extraction of 3D information and volumetric uncertainty from multiple stereo images. In P~vceedings of the 8th Eu~vpean Conference on Artificial Intelligence, pages 683-688, August 1988.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R Hebert, D. Laurendeau, and D. Poussart. Scene reconstruction and description: geometric primitive extraction from multiple viewed scattered data. In P~vceedings of lEEE Confelvnce on Computer Vision and Pattern Recognition, pages 286-292, June 1993.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>648979</ref_obj_id>
				<ref_obj_pid>645309</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[A. Hilton, A.J. Toddart, J. Illingworth, and T. Windeatt. Reliable surface reconstruction from multiple range images. In Fourth Emvpean Confe~vnce on Cornputer Vision, volume I, pages 117-126, April 1996.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Tsai-Hong Hong and M. O. Shneier. Describing a robot's workspace using a sequence of views from a moving camera. IEEE Transactions on Pattern Analysis and Machine Intelligence, 7 (6):721-726, November 1985.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134011</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Surface reconstruction from unorganized points. In Computer Graphics (SIGGRAPH '92 P~vceedings), volume 26, pages 71-78, July 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[V. Krishnamurthy and M. Levoy. Fitting smooth surfaces to dense polygonmeshes. In these proceedings.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192283</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R Lacroute and M. Levoy. Fast volume rendering using a shear-warp factorization of the viewing transformation. In Proceedings of SIGGRAPH '94 (Orlando, FL, July 24-29,1994), pages 451-458. ACM Press, July 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A. Li and G. Crebbin. Octree encoding of objects from range images. Pattern Recognition, 27(5):727-739, May 1994.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[W.E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3D surface construction algorithm. In Computer Graphics (SIGGRAPH ' 87 Proceedings), volume 21, pages 163-169, July 1987.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[W.N. Martin and J.K. Aggarwal. Volumetric descriptions of objects from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5 (2): 150- 158, March 1983.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[C. Montani, R. Scateni, and R. Scopigno. A modified look-up table for implicit disambiguation of marching cubes. Visual Computer, 10(6):353-355,1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>31124</ref_obj_id>
				<ref_obj_pid>31123</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Potmesil. Generating octree models of 3D objects from their silhouettes in a sequence of images. Computer Vision, Graphics, and lmage Processing, 40(1):1- 29, October 1987.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[M. Rutishauser, M. Stricker, and M. Trobina. Merging range images of arbitrarily shaped objects. In P1vceedings 1994 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 573-580, June 1994.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>203422</ref_obj_id>
				<ref_obj_pid>203416</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[M. Soucy and D. Laurendeau. A general surface approach to the integration of a set of range views. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(4):344-358, April 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>112699</ref_obj_id>
				<ref_obj_pid>112687</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[G. Succi, G. Sandini, E Grosso, and M. Tistarelli. 3D feature extraction from sequences of range data. In Robotics Research. Fifth International Symposium, pages 117-127, August 1990.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>167383</ref_obj_id>
				<ref_obj_pid>171245</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[R. Szeliski. Rapid octree construction from image sequences. CVGIP: Image Understanding, 58(1):23-32, July 1993.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[G.H Tarbox and S.N. Gottschlich. IVIS: An integrated volumetric inspection system. In P1vceedings of the 1994 Second CAD-Based Vision Workshop, pages 220- 227, February 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[G. Taubin. A signal processing approach to fair surface design. In P~vceedings of SIGGRAPH '95 (Los Angeles, CA, Aug. 6-11,1995), pages 351-358. ACM Press, August 1995.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[G. Turk and M. Levoy. Zippered polygon meshes from range images. In P~vceedings of SIGGRAPH '94 (Orlando, FL, July 24-29, 1994), pages 311-318. ACM Press, July 1994.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Robert Weinstock. The Calculus of Variations, with Applications to Physics and Engineering. Dover Publications, 1974.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Volumetric Method for Building Complex Models from Range Images Brian Curless and Marc Levoy Stanford 
University Abstract A number of techniques have been developed for reconstructing sur­faces by integrating 
groups of aligned range images. A desirable set of properties for such algorithms includes: incremental 
updating, rep­resentation of directional uncertainty, the ability to .ll gaps in the re­construction, 
and robustness in the presence of outliers. Prior algo­rithms possess subsets of these properties. In 
this paper, we present a volumetric method for integrating range images that possesses all of these properties. 
Our volumetric representation consists of a cumulative weighted signed distance function. Working with 
one range image at a time, we .rst scan-convert it to a distance function, then combine this with the 
data already acquired using a simple additive scheme. To achieve space ef.ciency, we employ a run-length 
encoding of the volume. To achieve time ef.ciency, we resample the range image to align with the voxel 
grid and traverse the range and voxel scanlines synchronously. We generate the .nal manifold by extracting 
an isosurface from the volumetric grid. We show that under certain assumptions, this isosur­face is optimal 
in the least squares sense. To .ll gaps in the model, we tessellate over the boundaries between regions 
seen to be empty and regions never observed. Using this method, we are able to integrate a large number 
of range images (as many as 70) yielding seamless, high-detail models of up to 2.6 million triangles. 
CR Categories: I.3.5 [Computer Graphics] Computational Geome­try and Object Modeling Additional keywords: 
Surface .tting, three-dimensional shape re­covery, range image integration, isosurface extraction  
1 Introduction Recent years have witnessed a rise in the availability of fast, accurate range scanners. 
These range scanners have provided data for applica­tions such as medicine, reverse engineering, and 
digital .lm-making. Many of these devicesgenerate range images; i.e., they produce depth values on a 
regular sampling lattice. Figure 1 illustrates how an op­tical triangulation scanner can be used to acquire 
a range image. By connecting nearest neighbors with triangular elements, one can con­struct a range surface 
as shown in Figure 1d. Range images are typi­cally formed by sweeping a 1D or 2D sensor linearly across 
an object or circularly around it, and generally do not contain enough informa­tion to reconstruct the 
entire object being scanned. Accordingly, we require algorithms that can merge multiple range images 
into a sin- Authors Address: Computer Science Department, Stanford University, Stanford, CA 94305 E-mail: 
fcurless,levoyg@cs.stanford.edu World Wide Web: http://www-graphics.stanford.edu Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
 gle description of the surface. A set of desirable properties for such a surface reconstruction algorithm 
includes: .Representation of range uncertainty. The data in range images typically have asymmetric error 
distributions with primary di­rections along sensor lines of sight, as illustrated for optical tri­angulation 
in Figure 1a. The method of range integration should re.ect this fact. .Utilization of all range data, 
including redundant observations of each object surface. If properly used, this redundancy can re­duce 
sensor noise. .Incremental and order independent updating. Incremental up­dates allow us to obtain a 
reconstruction after each scan or small set of scans and allow us to choose the next best orientation 
for scanning. Order independence is desirable to ensure that results are not biased by earlier scans. 
Together, they allow for straight­forward parallelization. .Time and space ef.ciency. Complex objects 
may require many range images in order to build a detailed model. The range images and the model must 
be represented ef.ciently and pro­cessed quickly to make the algorithm practical. .Robustness. Outliers 
and systematic range distortions can create challenging situations for reconstruction algorithms. A robust 
algorithm needs to handle these situations without catastrophic failures such as holes in surfaces and 
self-intersecting surfaces. .No restrictions on topological type. The algorithm should not assume that 
the object is of a particular genus. Simplifying as­sumptions such as the object is homeomorphic to a 
sphere yield useful results in only a restricted class of problems. .Ability to .ll holes in the reconstruction. 
Given a set of range images that do not completely cover the object, the surface re­construction will 
necessarily be incomplete. For some objects, no amount of scanning would completely cover the object, 
be­cause some surfaces may be inaccessible to the sensor. In these cases, we desire an algorithm that 
can automatically .ll these holes with plausible surfaces, yielding a model that is both wa­tertight 
and esthetically pleasing. In this paper, we present a volumetric method for integrating range images 
that possesses all of these properties. In the next section, we review some previous work in the area 
of surface reconstruction. In section 3, we describe the core of our volumetric algorithm. In sec­tion 
4, we show how this algorithm can be used to .ll gaps in the re­construction using knowledge about the 
emptiness of space. Next, in section 5, we describe how we implemented our volumetric approach so as 
to keep time and space costs reasonable. In section 6, we show the results of surface reconstruction 
from many range images of com­plex objects. Finally, in section 7 we conclude and discuss limitations 
and future directions. 2 Previous work Surface reconstruction from dense range data has been an active 
area of research for several decades. The strategies have proceeded along two basic directions: reconstruction 
from unorganized points, and Surface Object sz Direction of travel sx Laser sheet CCD CCD image plane 
Cylindrical lens Laser CCD Laser (a) (b) (c) (d) Figure 1. From optical triangulation to a range surface. 
(a) In 2D, a narrow laser beam illuminates a surface, and a linear sensor images the re.ection from an 
object. The center of the image pulse maps to the center of the laser, yielding a range value. The uncertainty, 
cx, in determining the center of the pulse results in range uncertainty, czalong the laser s line of 
sight. When using the spacetime analysis for optical triangulation [6], the uncertainties run along the 
lines of sight of the CCD. (b) In 3D, a laser stripe triangulation scanner .rst spreads the laser beam 
into a sheet of light with a cylindrical lens. The CCD observes the re.ected stripe from which a depth 
pro.le is computed. The object sweeps through the .eld of view, yielding a range image. Other scanner 
con.gurations rotate the object to obtain a cylindrical scan or sweep a laser beam or stripe over a stationary 
object. (c) A range image obtained from the scanner in (b) is a collection of points with regular spacing. 
(d) By connecting nearest neighbors with triangles, we create a piecewise linear range surface. reconstruction 
that exploits the underlying structure of the acquired data. These two strategies can be further subdivided 
according to whether they operate by reconstructing parametric surfaces or by re­constructing an implicit 
function. A major advantage of the unorganized points algorithms is the fact that they do not make any 
prior assumptions about connectivity of points. In the absence of range images or contours to provide 
connec­tivity cues, these algorithms are the only recourse. Among the para­metric surface approaches, 
Boissanat [2] describes a method for De­launay triangulation of a set of points in 3-space. Edelsbrunner 
and M¨ucke [9] generalize the notion of a convex hull to create surfaces called alpha-shapes. Examples 
of implicit surface reconstruction in­clude the method of Hoppe, et al [16] for generating a signed distance 
function followed by an isosurface extraction. More recently, Bajaj, et al [1] used alpha-shapes to construct 
a signed distance function to which they .t implicit polynomials. Although unorganized points al­gorithms 
are widely applicable, they discard useful information such as surface normal and reliability estimates. 
As a result, these algo­rithms are well-behaved in smooth regions of surfaces, but they are not always 
robust in regions of high curvature and in the presence of systematic range distortions and outliers. 
Among the structured data algorithms, several parametric ap­proaches have been proposed, most of them 
operating on range im­ages in a polygonal domain. Soucy and Laurendeau [25] describe a method using Venn 
diagrams to identify overlapping data regions, followed by re-parameterization and merging of regions. 
Turk and Levoy [30] devised an incremental algorithm that updates a recon­struction byerodingredundantgeometry,followedbyzipperingalong 
the remaining boundaries, and .nally a consensus step that rein­troduces the original geometry to establish 
.nal vertex positions. Rutishauser, et al [24] use errors along the sensor s lines of sight to es­tablish 
consensus surface positions followed by a re-tessellation that incorporates redundant data. These algorithms 
typically perform bet­ter than unorganized point algorithms, but they can still fail catas­trophically 
in areas of high curvature, as exempli.ed in Figure 9. Several algorithms have been proposed for integrating 
structured data to generate implicit functions. These algorithms can be classi.ed as to whether voxels 
are assigned one of two (or three) states or are samples of a continuous function. Among the discrete-state 
volumet­ric algorithms,Connolly[4]castsraysfrom arangeimageaccessedas a quad-tree into a voxel grid stored 
as an octree, and generates results for synthetic data. Chien, et al [3] ef.ciently generate octree models 
under the severe assumption that all views are taken from the direc­tions corresponding to the 6 faces 
of a cube. Li and Crebbin [19] and Tarbox and Gottschlich [28] also describe methods for generating bi­nary 
voxel grids from range images. None of these methods has been used to generate surfaces. Further, without 
an underlying continuous function, there are no mechanism for representing range uncertainty or for combining 
overlapping, noisy range surfaces. The last category of our taxonomy consists of implicit function methods 
that use samples of a continuous function to combine struc­tured data. Our method falls into this category. 
Previous efforts in this area include the work of Grosso, et al [12], who generate depth maps from stereo 
andaveragetheminto a volume with occupancyramps of varying slopes corresponding to uncertainty measures; 
they do not, however, perform a .nal surface extraction. Succi, et al [26] create depth maps from stereo 
and optical .ow and integrate them volumet­rically using a straight average. The details of his method 
are unclear, but they appear to extract an isosurface at an arbitrary threshold. In both the Grosso and 
Succi papers, the range maps are sparse, the di­rections of range uncertainty are not characterized, 
they use no time orspaceoptimizations,andthe.nalmodelsareoflowresolution. Re­cently, Hilton, et al [14] 
have developed a method similar to ours in that it uses weighted signed distance functions for merging 
range im­ages, but it does not address directions of sensor uncertainty, incre­mental updating, space 
ef.ciency, and characterization of the whole space for potential hole .lling, all of which we believe 
are crucial for the success of this approach. Other relevant work includes the method of probabilistic 
occu­pancy grids developed by Elfes and Matthies [10]. Their volumetric space is a scalar probability 
.eld which they update using a Bayesian formulation. The results have been used for robot navigation, 
but not for surface extraction. A dif.culty with this technique is the fact that the best description 
of the surface lies at the peak or ridge of the proba­bility function, and the problem of ridge-.nding 
is not one with robust solutions [8]. This is one of our primary motivations for taking an iso­surface 
approach in the next section: it leverages off of well-behaved surface extraction algorithms. The discrete-state 
implicit function algorithms described above also have much in common with the methods of extracting 
volumes from silhouettes [15] [21] [23] [27]. The idea of using backdrops to help carve out the emptiness 
of space is one we demonstrate in sec­tion 4. 3 Volumetric integration Our algorithm employs a continuous 
implicit function, D(x),repre­sented by samples. The function we represent is the weighted signed distance 
of each point xto the nearest range surface along the line of Range surface x x Sensor Zero-crossing 
New zero-crossing (isosurface) (a) (b) Figure 2. Unweighted signed distance functions in 3D. (a) A range 
sen­sor looking down the x-axis observes a range image, shown here as a re­constructed range surface. 
Following one line of sight down the x-axis, we can generate a signed distance function as shown. The 
zero crossing of this function is a point on the range surface. (b) The range sensor re­peats the measurement, 
but noise in the range sensing process results in a slightly different range surface. In general, the 
second surface would inter­penetrate the .rst, but we have shown it as an offset from the .rst surface 
for purposes of illustration. Following the same line of sight as before, we obtain another signed distance 
function. By summing these functions, we arrive at a cumulative function with a new zero crossing positioned 
mid­way between the original range measurements. sight to the sensor. We construct this function by 
combining signed distance functions d1(x), d2(x), ... dn(x)and weight functions w1(x), w2(x), ... wn(x)obtained 
from range images 1... n.Our combining rules give us for each voxel a cumulative signed distance function, 
D(x), and a cumulative weight W(x). We represent these functions on a discrete voxel grid and extract 
an isosurface corre­sponding to D(x)0. Under a certain set of assumptions, this iso­surface is optimal 
in the least squares sense. A full proof of this op­timality is beyond the scope of this paper, but a 
sketch appears in ap­pendix A. Figure 2 illustrates the principle of combining unweighted signed distances 
for the simple case of two range surfaces sampled from the same direction. Note that the resulting isosurface 
would be the sur­face created by averaging the two range surfaces along the sensor s lines of sight. 
In general, however, weights are necessary to repre­sent variations in certainty across the range surfaces. 
The choice of weights should be speci.c to the range scanning technology. For op­tical triangulation 
scanners, for example, Soucy [25] and Turk [30] make the weight depend on the dot product between each 
vertex nor­mal and the viewing direction, re.ecting greater uncertainty when the illumination is at grazing 
angles to the surface. Turk also argues that the range data at the boundaries of the mesh typically have 
greater uncertainty, requiring more down-weighting. We adopt these same weighting schemes for our optical 
triangulation range data. Figure 3 illustrates the construction and usage of the signed dis­tance and 
weight functions in 1D. In Figure 3a, the sensor is posi­tioned at the origin looking down the +x axis 
and has taken two mea­surements, r1and r2. The signed distance pro.les, d1(x)and d2(x) may extend inde.nitely 
in either direction, but the weight functions, w1(x)and w2(x), taper off behind the range points for 
reasons dis­cussed below. Figure 3b is the weighted combination of the two pro.les. The combination rules 
are straightforward: D(x)  w i (w x )di(x i() x ) (1) W(x) w i ( x ) (2)  d(x) D(x) 1  d(x) 2W(x) 
w(x) 2    x Sensor R 12  (a) (b) Figure 3. Signed distance and weight functions in one dimension. 
(a) The sensor looks down the x-axis and takes two measurements, r1and r2. d1(x)and d2(x)are the signed 
distance pro.les, and w1(x)and w2(x) are the weight functions. In 1D, we might expect two sensor measure­ments 
to have the same weight magnitudes, but we have shown them to be of different magnitude here to illustrate 
how the pro.les combine in the general case. (b) D(x)is a weighted combination of d1(x)and d2(x), and 
W(x)is the sum of the weight functions. Given this formulation, the zero-crossing,R, becomestheweightedcombinationof 
r1and r2and rep­resentsourbestguessofthelocationofthesurface. Inpractice,wetruncate the distance ramps 
and weights to the vicinity of the range points. where, di(x)and wi(x)are the signed distance and weight 
functions from the ith range image. Expressed as an incremental calculation, the rules are: Wi(x)Di(x)+wi+1(x)di+1(x)Di+1(x) 
(3) Wi(x)+wi+1(x) Wi+1(x)Wi(x)+wi+1(x) (4) where Di(x)and Wi(x)are the cumulative signed distance and 
weight functions after integrating the ith range image. In the special case of one dimension, the zero-crossing 
of the cu­mulative function is at a range, Rgiven by: wiri R (5) wi i.e., a weighted combination of 
the acquired range values, which is what one would expect for a least squares minimization. In principle, 
the distance and weighting functions should extend in­de.nitely in either direction. However, to prevent 
surfaces on oppo­site sides of the object from interfering with each other, we force the weighting function 
to taper off behind the surface. There is a trade-off involved in choosing where the weight function 
tapers off. It should persist far enough behind the surface to ensure that all distance ramps will contribute 
in the vicinity of the .nal zero crossing, but, it should also be as narrow as possible to avoid in.uencing 
surfaces on the other side. To meet these requirements, we force the weights to fall off at a distance 
equal to half the maximum uncertainty interval of the range measurements. Similarly, the signed distance 
and weight functions need not extend far in front of the surface. Restricting the functions to the vicinity 
of the surface yields a more compact representation and reduces the computational expense of updating 
the volume. In two and three dimensions, the range measurements correspond to curves or surfaces with 
weight functions, and the signed distance ramps have directions that are consistent with the primary 
directions of sensor uncertainty. The uncertainties that apply to range image in­tegration include errors 
in alignment between meshes as well as er­rors inherent in the scanning technology. A number of algorithms 
for aligning sets of range images have been explored and shown to yield excellent results [11][30]. The 
remaining error lies in the scanner it­self. For optical triangulation scanners, for example, this error 
has been shown to be ellipsoidal about the range points, with the major axis of the ellipse aligned with 
the lines of sight of the laser [13][24]. Figure 4 illustrates the two-dimensional case for a range curve 
de­rived from a single scan containing a row of range samples. In prac­tice, we use a .xed point representation 
for the signed distance func­ Sensor Sensor (d) (e) (f) Figure 4. Combination of signed distance and 
weight functions in two di­mensions. (a) and (d) are the signed distance and weight functions, respec­tively, 
generated for a range image viewed from the sensor line of sight shown in (d). The signed distance functions 
are chosen to vary between Dminand Dmax, as shown in (a). The weighting falls off with increas­ing obliquityto 
thesensorandattheedgesofthe meshesasindicatedbythe darker regions in (e). The normals, n1and n2shown 
in (e), are oriented at a grazing angle and facing the sensor, respectively. Note how the weight­ing 
is lower (darker) for the grazing normal. (b) and (e) are the signed dis­tance and weight functions for 
a range image of the same object taken at a 60 degree rotation. (c) is the signed distance function D(x)correspond­ing 
to the per voxel weighted combination of (a) and (b) constructed using equations 3 and 4. (f) is the 
sum of the weights at each voxel, W(x).The dotted green curve in (c) is the isosurface that represents 
our current esti­mate of the shape of the object. tion, which bounds the values to lie between Dminand 
Dmaxas shown in the .gure. The values of Dminand Dmaxmust be negative and positive, respectively, as 
they are on opposite sides of a signed distance zero-crossing. For three dimensions, we can summarize 
the whole algorithm as follows. First, we set all voxel weights to zero, so that new data will overwrite 
the initial grid values. Next, we tessellate each range im­age by constructing triangles from nearest 
neighbors on the sampled lattice. We avoid tessellating over step discontinuities (cliffs in the range 
map) by discarding triangles with edge lengths that exceed a threshold. We must also compute a weight 
at each vertex as described above. Once a range image has been converted to a triangle mesh with a weight 
at each vertex, we can update the voxel grid. The signed distance contribution is computed by casting 
a ray from the sensor through each voxel near the range surface and then intersecting it with the triangle 
mesh, as shown in .gure 5. The weight is computed by linearly interpolating the weights stored at the 
intersection triangle s vertices. Having determined the signed distance and weight we can apply the update 
formulae described in equations 3 and 4. At any point during the merging of the range images, we can 
extract the zero-crossing isosurface from the volumetric grid. We restrict this extraction procedure 
to skip samples with zero weight, generating tri­angles only in the regions of observed data. We will 
relax this restric­tion in the next section. 4 Hole .lling The algorithm described in the previous section 
is designed to recon­struct the observed portions of the surface. Unseen portions of the surface will 
appear as holes in the reconstruction. While this result is an accurate representation of the known surface, 
the holes are es­thetically unsatisfying and can present a stumbling block to follow­on algorithms that 
expect continuous meshes. In [17], for example, the authors describe a method for parameterizing patches 
that entails Sensor Figure 5. Sampling the range surface to update the volume. We compute the weight, 
w, and signed distance, d, needed to update the voxel by cast­ing a ray from the sensor, through the 
voxel onto the range surface. We obtain the weight, w, by linearly interpolating the weights (wa, wb,and 
wc) stored at neighboring range vertices. Note that for a translating sensor (like our Cyberware scanner), 
the sensor point is different for each column of range points. generating evenly spaced grid lines by 
walking across the edges of a mesh. Gaps in the mesh prevent the algorithm from creating a fair pa­rameterization. 
As another example, rapid prototyping technologies such as stereolithography typically require a watertight 
model in or­der to construct a solid replica [7]. One option for .lling holes is to operate on the reconstructed 
mesh. If the regions of the mesh near each hole are very nearly planar, then this approach works well. 
However, holes in the meshes can be (and frequently are) highly non-planar and may even require connections 
betweenunconnectedcomponents. Instead,weofferahole.llingap­proach that operates on our volume, which 
contains more information than the reconstructed mesh. The key to our algorithm lies in classifying all 
points in the vol­ume as being in one of three states: unseen, empty, or near the surface. Holes in the 
surface are indicated by frontiers between unseen regions and empty regions (see Figure 6). Surfaces 
placed at these frontiers offer a plausible way to plug these holes (dotted in Figure 6). Ob­taining 
this classi.cation and generating these hole .llers leads to a straightforward extension of the algorithm 
described in the previous section: 1. Initialize the voxel space to the unseen state. 2. Update the 
voxels near the surface as described in the previous section. As before, these voxels take on continuous 
signed dis­tance and weight values. 3. Follow the lines of sight back from the observed surface and 
mark the corresponding voxels as empty . We refer to this step as space carving. 4. Perform an isosurface 
extraction at the zero-crossing of the signed distance function. Additionally, extract a surface be­tween 
regions seen to be empty and regions that remain unseen.  In practice, we represent the unseen and 
empty states using the functionandweight.eldsstoredonthevoxellattice. Werepresentthe unseen state with 
the function values D(x)Dmax, W(x)0and the empty state with the function values D(x)Dmin, W(x)0, as shown 
in Figure 6b. The key advantage of this representation is that we can use the same isosurface extraction 
algorithm we used in the previous section without the restriction on interpolating voxels of zero weight. 
This extraction .nds both the signed distance and hole .ll isosurfaces and connects them naturally where 
they meet, i.e., at the corners in Figure 6a where the dotted red line meets the dashed green line. Note 
that the triangles that arise from interpolations across voxels of zero weight are distinct from the 
others: they are hole .llers. D(x) = D max W(x) = 0 Unseen  Empty Near surface D(x) = Dmin D < D(x) 
< D min maxSensor W(x) = 0 W(x) > 0 (a) (b) Figure 6. Volumetric grid with space carving and hole .lling. 
(a) The re­gions in front of the surface are seen as empty, regions in the vicinity of the surface ramp 
through the zero-crossing, while regions behind remain unseen. The green (dashed) segments are the isosurfaces 
generated near the observed surface, while the red (dotted) segments are hole .llers, gen­erated by tessellating 
over the transition from empty to unseen. In (b), we identify the three extremal voxel states with their 
corresponding function values. We take advantage of this distinction when smoothing surfaces as de­scribed 
below. Figure 6 illustrates the method for a single range image, and pro­vides a diagram for the three-state 
classi.cation scheme. The hole .ller isosurfaces are false in that they are not representative of the 
observed surface, but they do derive from observed data. In particular, they correspond to a boundary 
that con.nes where the surface could plausibly exist. In practice, we .nd that many of these hole .ller 
sur­faces are generated in crevices that are hard for the sensor to reach. Because the transition between 
unseen and empty is discontinuous and hole .ll triangles are generated as an isosurface between these 
bi­nary states, with no smooth transition, we generally observe aliasing artifacts in these areas. These 
artifacts can be eliminated by pre.lter­ing the transition region before sampling on the voxel lattice 
using straightforward methods such as analytic .ltering or super-sampling and averaging down. In practice, 
we have obtained satisfactory re­sults by applying another technique: post-.ltering the mesh after re­construction 
using weighted averages of nearest vertex neighbors as described in [29]. The effect of this .ltering 
step is to blur the hole .ll surface. Since we know which triangles correspond to hole .llers, we need 
only concentrate the surface .ltering on the these portions of the mesh. This localized .ltering preserves 
the detail in the observed surface reconstruction. To achieve a smooth blend between .ltered hole .ll 
vertices and the neighboring real surface, we allow the .l­ter weights to extend beyond and taper off 
into the vicinity of the hole .ll boundaries. We have just seen how space carving is a useful operation: 
it tells us much about the structure of free space, allowing us to .ll holes in an intelligent way. However, 
our algorithm only carves back from ob­served surfaces. There are numerous situations where more carving 
would be useful. For example, the interior walls of a hollow cylinder may elude digitization, but by 
seeing through the hollow portion of the cylinder to a surface placed behind it, we can better approximate 
its geometry. We can extend the carving paradigm to cover these situ­ations by placing such a backdrop 
behind the surfaces being scanned. By placing the backdrop outside of the voxel grid, we utilize it purely 
for carving space without introducing its geometry into the model.  5 Implementation 5.1 Hardware The 
examples in this paper were acquired using a Cyberware 3030 MS laser stripe optical triangulation scanner. 
Figure 1b illustrates the scanning geometry: an object translates through a plane of laser light while 
the re.ections are triangulated into depth pro.les through a CCD camera positioned off axis. To improve 
the quality of the data, we apply the method of spacetime analysis as described in [6]. The bene.ts of 
this analysis include reduced range noise, greater immu­nity to re.ectance changes, and less artifacts 
near range discontinu­ities. When using traditional triangulation analysis implemented in hard­ware in 
our Cyberware scanner, the uncertainty in triangulation for our system follows the lines of sight of 
the expanding laser beam. When using the spacetime analysis, however, the uncertainty follows the lines 
of sight of the camera. The results described in section 6 of this paper were obtained with one or the 
other triangulation method. In each case, we adhere to the appropriate lines of sight when laying down 
signed distance and weight functions. 5.2 Software The creation of detailed, complex models requires 
a large amount of input data to be merged into high resolution voxel grids. The exam­ples in the next 
section include models generated from as many as 70 scans containing up to 12 million input vertices 
with volumetric grids ranging in size up to 160 million voxels. Clearly, time and space opti­mizations 
are critical for merging this data and managing these grids. 5.2.1 Run-length encoding The core data 
structure is a run-length encoded (RLE) volume with three run types: empty, unseen, and varying. The 
varying .elds are stored as a stream of varying data, rather than runs of constant value. Typical memory 
savings vary from 10:1 to 20:1. In fact, the space required to represent one of these voxel grids is 
usually less than the memory required to represent the .nal mesh as a list of vertices and triangle indices. 
 5.2.2 Fast volume traversal Updating the volume from a range image may be likened to inverse volume 
rendering: instead of reading from a volume and writing to an image, we read from a range image and write 
to a volume. As a re­sult, we leverage off of a successful idea from the volume rendering community: 
for best memory system performance, stream through the volume and the image simultaneously in scanline 
order [18]. In general, however, the scanlines of a range image are not aligned with the scanlines of 
the voxel grid, as shown in Figure 7a. By suitably resampling the range image, we obtain the desired 
alignment (Fig­ure 7b). The resampling process consists of a depth rendering of the range surface using 
the viewing transformation speci.c to the lines of sight of the range sensor and using an image plane 
oriented to align with the voxel grid. We assign the weights as vertex colors to be linearly interpolated 
during the rendering step, an approach equiva­lent to Gouraud shading of triangle colors. To merge the 
range data into the voxel grid, we stream through the voxel scanlines in order while stepping through 
the corresponding scanlines in the resampled range image. We map each voxel scanline to the correct portion 
of the range scanline as depicted in Figure 7d, and we resample the range data to yield a distance from 
the range sur­face. Using the combination rules given by equations 3 and 4, we up­date the run-length 
encoded structure. To preserve the linear mem­ory structure of the RLE volume (and thus avoid using linked 
lists of runs scattered through the memory space), we read the voxel scanlines from the current volume 
and write the updated scanlines to a second RLE volume; i.e., we double-buffer the voxel grid. Note that 
depend­ing on the scanner geometry, the mapping from voxels to range image pixels may not be linear, 
in which case care must be taken to resample appropriately [5]. For the case of merging range data only 
in the vicinity of the sur­face, we try to avoid processing voxels distant from the surface. To that 
end, we construct a binary tree of minimum and maximum depths for every adjacent pair of resampled range 
image scanlines. Before processing each voxel scanline, we query the binary tree to decide Voxel VoxelVolumeVolume 
slices slices  (a) (d) Figure 7. Range image resampling and scanline order voxel updates. (a) Range 
image scanlines are not in general oriented to allow for coherently streaming through voxel and range 
scanlines. (b) By resampling the range image, we can obtain the desired range scanline orientation. (c) 
Casting rays from the pixels on the range image means cutting across scanlines of the voxel grid, resulting 
in poor memory performance. (d) Instead, we run along scanlines of voxels, mapping  them to the correct 
positions on the resampled range image. which voxels, if any, are near the range surface. In this way, 
only rel­evant pieces of the scanline are processed. In a similar fashion, the space carving steps can 
be designed to avoid processing voxels that are not seen to be empty for a given range image. The resulting 
speed­ups from the binary tree are typically a factor of 15 without carving, and a factor of 5 with carving. 
We did not implement a brute-force volume update method, however we would expect the overall algo­rithm 
described here would be much faster by comparison. 5.2.3 Fast surface extraction To generate our .nal 
surfaces, we employ a Marching Cubes algo­rithm [20] with a lookup table that resolves ambiguous cases 
[22]. To reduce computational costs, we only process voxels that have varying data or are at the boundary 
between empty and unseen.  6Results We show results for a number of objects designed to explore the 
ro­bustness of our algorithm, its ability to .ll gaps in the reconstruction, and its attainable level 
of detail. To explore robustness, we scanned a thin drill bit using the traditional method of optical 
triangulation. Due to the false edge extensions inherent in data from triangulation scan­ners [6], this 
particular object poses a formidable challenge, yet the volumetric method behavesrobustlywherethe zipperingmethod[30] 
failscatastrophically. ThedragonsequenceinFigure11demonstrates the effectiveness of carving space for 
hole .lling. The use of a back­drop here is particularly effective in .lling the gaps in the model. Note 
that we do not use the backdrop at all times, in part because the range images are much denser and more 
expensive to process, and also be­cause the backdrop tends to obstruct the path of the object when auto­matically 
repositioning it with our motion control platform. Finally, the Happy Buddha sequence in Figure 12 shows 
that our method can be used to generate very detailed, hole-free models suitable for rendering and rapid 
manufacturing. Statistics for the reconstruction of the dragon and Buddha models appear in Figure 8. 
With the optimizations described in the previous section, we were able to reconstruct the observed portions 
of the sur­faces in under an hour on a 250 MHz MIPS R4400 processor. The space carving and hole .lling 
algorithm is not completely optimized, but the execution times are still in the range of 3-5 hours, less 
than the time spent acquiring and registering the range images. For both mod­els, the RMS distance between 
points in the original range images and points on the reconstructed surfaces is approximately 0.1 mm. 
This .gure is roughly the same as the accuracy of the scanning technology, indicating a nearly optimal 
surface reconstruction. 7 Discussion and future work We have described a new algorithm for volumetric 
integration of range images, leading to a surface reconstruction without holes. The algorithm has a number 
of desirable properties, including the repre­sentation of directional sensor uncertainty, incremental 
and order in­dependent updating, robustness in the presence of sensor errors, and the ability to .ll 
gaps in the reconstruction by carving space. Our use of a run-length encoded representation of the voxel 
grid and synchro­nized processing of voxel and resampled range image scanlines make the algorithm ef.cient. 
This in turn allows us to acquire and integrate a large number of range images. In particular, we demonstrate 
the ability to integrate up to 70 scans into a high resolution voxel grid to generate million polygon 
models in a few hours. These models are free of holes, making them suitable for surface .tting, rapid 
prototyp­ing, and rendering. There are a number of limitations that prevent us from generating models 
from an arbitrary object. Some of these limitations arise from the algorithm while others arise from 
the limitations of the scanning technology. Among the algorithmic limitations, our method has dif­.culty 
bridging sharp corners if no scan spans both surfaces meeting at the corner. This is less of a problem 
when applying our hole-.lling algorithm, but we are also exploring methods that will work without hole 
.lling. Thin surfaces are also problematic. As described in sec­tion 3, the in.uences of observed surfaces 
extend behind their esti­mated positions for each range image and can interfere with distance functions 
originating from scans of the opposite side of a thin surface. In this respect, the apexes of sharp corners 
also behave like thin sur­faces. While we have limited this in.uence as much as possible, it still places 
a lower limit on the thickness of surface that we can reli­ably reconstruct without causing artifacts 
such as thickening of sur­faces or rounding of sharp corners. We are currently working to lift this restriction 
by considering the estimated normals of surfaces. Other limitations arise from the scanning technologies 
themselves. Optical methods such as the one we use in this paper can only pro­vide data for external 
surfaces; internal cavities are not seen. Further, very complicated objects may require an enormous amount 
of scan­ning to cover the surface. Optical triangulation scanning has the ad­ditional problem that both 
the laser and the sensor must observe each point on the surface, further restricting the class of objects 
that can be scanned completely. The re.ectance properties of objects are also a factor. Optical methods 
generally operate by casting light onto an ob­ject, but shiny surfaces can de.ect this illumination, 
dark objects can absorb it, and bright surfaces can lead to interre.ections. To minimize these effects, 
we often paint our objects with a .at, gray paint. Straightforward extensions to our algorithm include 
improving the execution time of the space carving portion of the algorithm and demonstrating parallelization 
of the whole algorithm. In addition, more aggressive space carving may be possible by making inferences 
about sensor lines of sight that return no range data. In the future, we hope to apply our methods to 
other scanning technologies and to large scale objects such as terrain and architectural scenes. Model 
Scans Input triangles Voxel size (mm) Volume dimensions Exec. time (min) Output triangles Holes Dragon 
61 15 M 0.35 712x501x322 56 1.7 M 324 Dragon + fill 71 24 M 0.35 712x501x322 257 1.8 M 0 Buddha 48 5 
M 0.25 407x957x407 47 2.4 M 670 Buddha + fill 58 9 M 0.25 407x957x407 197 2.6 M 0 Figure 8. Statistics 
for the reconstruction of the dragon and Buddha mod­els, with and without space carving. Acknowledgments 
We would like to thank Phil Lacroute for his many helpful sugges­tions in designing the volumetric algorithms. 
Afra Zomorodian wrote the scripting interface for scanning automation. Homan Igehy wrote the fast scan 
conversion code, which we used for range image resam­pling. Thanks to Bill Lorensen for his marching 
cubes tables and mesh decimation software, and for getting the 3D hardcopy made. Matt Pharr did the accessibility 
shading used to render the color Bud­dha, and Pat Hanrahan and Julie Dorsey made helpful suggestions 
for RenderMan tricks and lighting models. Thanks also to David Addle­man and George Dabrowski of Cyberware 
for their help and for the use of their scanner. This work was supported by the National Sci­ence Foundation 
under contract CCR-9157767 and Interval Research Corporation.  References [1] C.L. Bajaj, F. Bernardini, 
and G. Xu. Automatic reconstruction of surfaces and scalar .elds from 3D scans. In Proceedings of SIGGRAPH 
95 (Los Angeles, CA, Aug. 6-11, 1995), pages 109 118. ACM Press, August 1995. [2] J.-D.Boissonnat.Geometricstructuresforthree-dimensionalshaperepresentation. 
ACM Transactions on Graphics, 3(4):266 286, October 1984. [3] C.H. Chien, Y.B. Sim, and J.K. Aggarwal. 
Generation of volume/surface octree from range data. In The Computer Society Conference on Computer Vision 
and Pattern Recognition, pages 254 60, June 1988. [4] C. I. Connolly. Cumulative generation of octree 
models from range data. In Pro­ceedings, Intl. Conf. Robotics, pages 25 32, March 1984. [5] B. Curless. 
Better optical triangulation and volumetric reconstruction of complex models from range images. PhD thesis, 
Stanford University, 1996. [6] B. Curless and M. Levoy. Better optical triangulation through spacetime 
analysis. In Proceedings of IEEE International Conference on Computer Vision, pages 987 994, June 1995. 
[7] A. Dolenc. Software tools for rapid prototyping technologies in manufactur­ing. Acta Polytechnica 
Scandinavica: Mathematics and Computer Science Series, Ma62:1 111, 1993. [8] D.Eberly,R.Gardner,B.Morse,S.Pizer,andC.Scharlach.Ridgesforimageanal­ysis. 
Journal of Mathematical Imaging and Vision, 4(4):353 373, Dec 1994. [9] H. Edelsbrunnerand E.P. M¨ucke. 
Three-dimensionalalpha shapes. In Workshop on Volume Visualization, pages 75 105, October 1992. [10] 
A.ElfesandL.Matthies.Sensorintegrationforrobotnavigation:combiningsonar and range data in a grid-based 
representation. In Proceedings of the 26th IEEE Conference on Decision and Control, pages 1802 1807, 
December 1987. [11] H. Gagnon, M. Soucy, R. Bergevin, and D. Laurendeau. Registration of multiple range 
views for automatic 3-D model building. In Proceedings 1994 IEEE Com­puter Society Conference on Computer 
Vision and Pattern Recognition,pages 581 586, June 1994. [12] E. Grosso, G. Sandini, and C. Frigato. 
Extraction of 3D information and volumet­ric uncertainty from multiple stereo images. In Proceedings 
of the 8th European Conference on Arti.cial Intelligence, pages 683 688, August 1988. [13] P. Hebert, 
D. Laurendeau, and D. Poussart. Scene reconstruction and description: geometricprimitiveextractionfrommultipleviewedscattereddata. 
In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 286 292, June 1993. 
[14] A. Hilton, A.J. Toddart, J. Illingworth, and T. Windeatt. Reliable surface recon­struction from 
multiple range images. In Fourth European Conference on Com­ (g) Figure 9. Merging range images of a 
drill bit. We scanned a 1.6 mm drill bit from 12 orientations at a 30 degree spacing using traditional 
optical tri­angulation methods. Illustrations (a) -(d) each show a plan (top) view of a slice taken through 
the range data and two reconstructions. (a) The range data shown as unorganized points: algorithms that 
operate on this form of data would likely have dif.culty deriving the correct surface. (b) The range 
data shown as a set of wire frame tessellations of the range data: the false edge extensions pose a challenge 
to both polygon and volumetric meth­ods. (c) A slice through the reconstructed surface generated by a 
polygon method: the zippering algorithm of Turk [31]. (d) A slice through the re­constructed surface 
generated by the volumetric method described in this paper. (e) A rendering of the zippered surface. 
(f) A rendering of the volu­metrically generated surface. Note the catastrophic failure of the zippering 
algorithm. Thevolumetricmethod,however,producesawatertightmodel. (g) A photograph of the original drill 
bit. The drill bit was painted white for scanning. puter Vision, volume I, pages 117 126, April 1996. 
 [15] Tsai-Hong Hong and M. O. Shneier. Describing a robot s workspace using a se­quence of views from 
a moving camera. IEEE Transactions on Pattern Analysis and Machine Intelligence, 7(6):721 726, November 
1985. [16] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Surface recon­struction from 
unorganized points. In Computer Graphics (SIGGRAPH 92 Pro­ceedings), volume 26, pages 71 78, July 1992. 
[17] V.KrishnamurthyandM.Levoy.Fittingsmoothsurfacestodensepolygonmeshes. In these proceedings. [18] 
P. Lacroute and M. Levoy. Fast volume rendering using a shear-warp factorization of the viewing transformation. 
In Proceedings of SIGGRAPH 94 (Orlando, FL, July 24-29, 1994), pages 451 458. ACM Press, July 1994. [19] 
A. Li and G. Crebbin. Octree encoding of objects from range images. Pattern Recognition, 27(5):727 739, 
May 1994. [20] W.E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3D surface constructionalgorithm. 
In ComputerGraphics(SIGGRAPH 87 Proceedings),vol­ume 21, pages 163 169, July 1987. [21] W.N. Martin and 
J.K. Aggarwal. Volumetric descriptions of objects from multiple views. IEEE Transactions on Pattern Analysis 
and Machine Intelligence, 5(2):150 158, March 1983. [22] C. Montani, R. Scateni, and R. Scopigno. A modi.ed 
look-up table for implicit disambiguation of marching cubes. Visual Computer, 10(6):353 355, 1994. [23] 
M. Potmesil. Generating octree models of 3D objects from their silhouettes in a sequence of images. Computer 
Vision, Graphics, and Image Processing, 40(1):1 29, October 1987. [24] M. Rutishauser, M. Stricker, and 
M. Trobina. Merging range images of arbitrar­ily shaped objects. In Proceedings 1994 IEEE Computer Society 
Conference on Computer Vision and Pattern Recognition, pages 573 580, June 1994. [25] M.SoucyandD.Laurendeau.Ageneralsurfaceapproachtotheintegrationofaset 
of range views. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(4):344 358, April 
1995. [26] G. Succi, G. Sandini, E Grosso, and M. Tistarelli. 3D feature extraction from se­quences of 
range data. In Robotics Research. Fifth InternationalSymposium, pages 117 127, August 1990. [27] R. Szeliski. 
Rapid octree construction from image sequences. CVGIP: Image Un­derstanding, 58(1):23 32, July 1993. 
[28] G.H Tarbox and S.N. Gottschlich. IVIS: An integrated volumetric inspection sys­tem. In Proceedings 
of the 1994 Second CAD-Based Vision Workshop, pages 220 227, February 1994. [29] G. Taubin. A signal 
processing approach to fair surface design. In Proceedings of SIGGRAPH 95 (Los Angeles, CA, Aug. 6-11, 
1995), pages 351 358. ACM Press, August 1995. [30] G. Turk and M. Levoy. Zippered polygon meshes from 
range images. In Proceed­ings of SIGGRAPH 94 (Orlando, FL, July 24-29, 1994), pages 311 318. ACM Press, 
July 1994. [31] Robert Weinstock. The Calculus of Variations, with Applications to Physics and Engineering. 
Dover Publications, 1974. A Isosurface as least squares minimizer It is possible to show that the isosurface 
of the weighted signed dis­tance function is equivalent to a least squares minimization of squared distances 
between points on the range surfaces and points on the de­sired reconstruction. The key assumptions are 
that the range sensor is orthographic and that the range errors are independently distributed along sensor 
lines of sight. A full proof is beyond the scope of this paper, but we provide a sketch here. See [5] 
for details. Consider a region, R, on the desired surface, f, which is observed by nrange images. We 
de.ne the error between an observed range surface and a possible reconstructed surface as the integral 
of the weighted squared distances between points on the range surface and the reconstructed surface. 
These distances are taken along the lines of sight of the sensor, commensurate with the predominant directions 
of uncertainty (see Figure 10). The total error is the sum of the integrals for the nrange images: z 
 z  v2 v1 y x Figure 10. Two range surfaces, f1and f2, are tessellated range images acquired from 
directions v1and v2. The possible range surface, z= f(x;y), is evaluated in terms of the weighted squared 
distances to points on the range surfaces taken along the lines of sight to the sensor. A point, (x;y;z), 
is shown here being evaluated to .nd its corresponding signed distances, d1and d2, and weights, w1and 
w2. ZZ n X E(f) wi(s;t;f)di(s;t;f)2dsdt(6) A i i=1 where each (s;t)corresponds to a particular sensor 
line of sight for each range image, Aiis the domain of integration for the i th range image, and wi(s;t;f)and 
di(s;t;f)are the weights and signed dis­tances taken along the i th range image s lines of sight. Now, 
consider a canonical domain, A, on a parameter plane, (x;y), over which Ris a function zf(x;y). The total 
error can be re­written as an integration over the canonical domain: ZZn [] X [] @z @z E(z) wi(x;y;z)di(x;y;z)2vi.(;;.1)dxdy 
A @x@y i=1 (7) where viis the sensing direction of the i th range image, and the weights and distances 
are evaluated at each point, (x;y;z),by .rst mapping them to the lines of sight of the corresponding 
range image. The dot product represents a correction term that relates differential areas in Ato differential 
areas in Ai. Applying the calculus of vari­ations [31], we can construct a partial differential equation 
for the z that minimizes this integral. Solving this equation we arrive at the fol­lowing relation: n 
X @vi [wi(x;y;z)di(x;y;z)2]0 (8) i=1 where @vi is the directional derivative along vi. Since the weight 
as­sociated with a line of sight does not vary along that line of sight, and the signed distance has 
a derivative of unity along the line of sight, we can simplify this equation to: n X wi(x;y;z)di(x;y;z)0 
(9) i=1 This weightedsumofsigneddistancesis thesameaswhatwe com­pute in equations 1 and 2, without the 
division by the sum of the weights. Since the this divisor is always positive, the isosurface we extract 
in section 3 is exactly the least squares minimizing surface de­scribed here. (i) (j) (k) Figure 11. 
Reconstruction of a dragon. Illustrations (a) -(d) are full views of the dragon. Illustrations (e) -(h) 
are magni.ed views of the section highlighted by the green box in (a). Regions shown in red correspond 
to hole .ll triangles. Illustrations (i) -(k) are slices through the corresponding volumetric grids at 
the level indicated by the green line in (e). (a)(e)(i) Reconstruction from 61 range images without space 
carving and hole .lling. The magni.ed rendering highlights the holes in the belly. The slice through 
the volumetric grid shows how the signed distance ramps are maintained close to the surface. The gap 
in the ramps leads to a hole in the reconstruction. (b)(f)(j) Reconstruction with space carving and hole 
.lling using the same data as in (a). While some holes are .lled in a reasonable manner, some large regions 
of space are left untouched and create extraneous tessellations. The slice through the volumetric grid 
reveals that the isosurface between the unseen (brown) and empty (black) regions will be connected to 
the isosurface extracted from the distance ramps, making it part of the connected component of the dragon 
body and leaving us with a substantial number of false surfaces. (c)(g)(k) Reconstruction with 10 additional 
range images using backdrop surfaces to effect more carving. Notice how the extraneous hole .ll triangles 
nearly vanish. The volumetric slice shows how we have managed to empty out the space near the belly. 
The bumpiness along the hole .ll regions of the belly in (g) corresponds to aliasing artifacts from tessellating 
over the discontinuous transition between unseen and empty regions. (d)(h) Reconstruction as in (c)(g) 
with .ltering of the hole .ll portions of the mesh. The .ltering operation blurs out the aliasing artifacts 
in the hole .ll regions while preserving the detail in the rest of the model. Careful examination of 
(h) reveals a faint ridge in the vicinity of the smoothed hole .ll. This ridge is actual geometry present 
in all of the renderings, (e)-(h). The .nal model contains 1.8 million polygons and is watertight. (a) 
(b) (c) (d) (e) Figure 12. Reconstruction and 3D hardcopy of the Happy Buddha . The original is a plastic 
and rosewood statuette that stands 20 cm tall. Note that the camera parameters for each of these images 
is different, creating a slightly different perspective in each case. (a) Photograph of the original 
after spray painting it matte gray to simplify scanning. (b) Gouraud-shaded rendering of one range image 
of the statuette. Scans were acquired using a Cyberware scanner, modi.ed to permit spacetime triangulation 
[6]. This .gure illustrates the limited and fragmentary nature of the information available from a single 
range image. (c) Gouraud-shaded rendering of the 2.4 million polygon mesh after merging 48 scans, but 
before hole-.lling. Notice that the reconstructed mesh has at least as much detail as the single range 
image, but is less noisy; this is most apparent around the belly. The hole in the base of the model corresponds 
to regions that were not observed directly by the range sensor. (d) RenderMan rendering of an 800,000 
polygon decimated version of the hole-.lled and .ltered mesh built from 58 scans. By placing a backdrop 
behind the model and taking 10 additional scans, we were able to see through the space between the base 
and the Buddha s garments, allowing us to carve space and .ll the holes in the base. (e) Photograph of 
a hardcopy of the 3D model, manufactured by 3D Systems, Inc., using stereolithography. The computer model 
was sliced into 500 layers, 150 microns apart, and the hardcopy was built up layer by layer by selectively 
hardening a liquid resin. The process took about 10 hours. Afterwards, the model was sanded and bead-blasted 
to remove the stair-step artifacts that arise during layered manufacturing.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237270</article_id>
		<sort_key>313</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Fitting smooth surfaces to dense polygon meshes]]></title>
		<page_from>313</page_from>
		<page_to>324</page_to>
		<doi_number>10.1145/237170.237270</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237270</url>
		<keywords>
			<kw><![CDATA[B-spline surfaces]]></kw>
			<kw><![CDATA[dense polygon meshes]]></kw>
			<kw><![CDATA[displacement maps]]></kw>
			<kw><![CDATA[parameterization]]></kw>
			<kw><![CDATA[surface fitting]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Spline and piecewise polynomial approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40026565</person_id>
				<author_profile_id><![CDATA[81100373145]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Venkat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krishnamurthy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>577958</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A.V.Aho and J.D.Ullman. Data structures and algorithms. Addison-Wesley, 1979.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[L Bardis and M Vafiadou. Ship-hull geometry representation with b-spline surface patches. Computer Aided Design, 24(4):217-222,1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Brian A. Barsky. End conditions and boundary conditions for uniform b-spline curve and surface representations. Computers In Industry, 3, 1982.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35072</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Richard Bartels, John Beatty, and Brian Barsky. An Int~vduction to Splinesfor Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann Publishers, Palo Alto, CA, 1987.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn. Simulation of wrinkled surfaces. In Computer Graphics (SIG- GRAPH '78 P1vceedings), volume 12, pages 286-292,1978.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. Vezien Chakib Bennis and G. Iglesias. Piecewise surface flattening for nondistorted texture mapping. In Computer Graphics (SIGGRAPH' 91 P~vceedings), volume 25, pages 237-246, July 1991.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>98601</ref_obj_id>
				<ref_obj_pid>98524</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Jindong Chen and Yijie Han. Shortest paths on a polyhedron. In P1vc. 6th Annual ACM Symposium on ComputationaI Geometry, pages 360-369, June 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook. Shade trees. In Computer Graphics (SIGGRAPH '84 P1vceedings), volume 18, pages 223-231, July 1984.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Brian Curless and Marc Levoy. A volumetric method for building complex models from range images. In Computer Graphics (SIGGRAPH '96 P1vceedings), August 1996.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>151103</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Paul Dierckx. Curve and Sulface Fitting with Splines. Oxford Science Publications, New York, 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Matthias Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. Multiresolution analysis of arbitrary meshes. In Computer Graphics (P1vceedings of SIGGRAPH '95), pages 173-182, August 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237271</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Matthias Eck and Hugues Hoppe. Automatic reconstruction of b-spline surfaces of arbitrary topologicaltype. In Computer Graphics (P~vceedings of SIGGRAPH ' 96), August 1996.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hugues Hoppe et al. Piecewise smooth surface reconstruction. In Computer Graphics (P1vceedings of SIGGRAPH '94), pages 295-302, July 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83600</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Gerald Farin. Curves and Sulfaces for Computer Aided Geometric Design. Academic Press, 1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378512</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[David R. Forsey and Richard H. Bartels. Hierarchical B-spline refinement. In Computer Graphics (SIGGRAPH '88 P1vceedings), volume 22, pages 205-212, August 1988.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Mark Halstead, Michael Kass, and Tony DeRose. Efficient, fair interpolation using Catmull-Clark surfaces. In Computer Graphics (SIGGRAPH '93 P~vceedings), volume 27, pages 35-44, August 1993.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[David Heeger and James R. Bergen. Pyramid-basedtexture analysis/synthesis. In Computer Graphics (SIGGRAPH '95 P1vceedings), pages 229-237, July 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>174506</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J Hoschek and D Lasser. Fundamentals of Computer Aided Geometric Design. AK Peters, Wellesley, 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Charles L. Lawson and Richard J. Hanson. Solving Least Square P1vblems. Prentice-Hall, Englewood Cliffs, New Jersey, 1974.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[W. Ma and J P Kruth. Parameterization of randomly measured points for least squares fitting of b-spline curves and surfaces. Computer Aided Design, 27(9):663-675,1995.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Jerome Maillot. Interactive texture mapping. In Computer Graphics (SIGGRAPH '93 P1vceedings), volume 27, pages 27-34, July 1993.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M.J. Milroy, C. Bradley, G. W. Vickers, and D. J. Weir. G1 continuity ofb-spline surface patches in reverse engineering. Computer-Aided Design, 27:471-478, 1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>33372</ref_obj_id>
				<ref_obj_pid>33367</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J.S.B. Mitchell, D. M. Mount, and C. H. Papadimitriou. The discrete geodesic problem. SIAM J. Comput., 16:647-668,1987.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319138</ref_obj_id>
				<ref_obj_pid>319120</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Eben Ostby. Describing free-form 3d surfaces for animation. In Workshop on Interactive 3D Graphics, pages 251-258,1986.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Hans K. Pedersen. Decorating implicit surfaces. In Computer Graphics (P~vceedings of SIGGRAPH '95), pages 291-300,August 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237268</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Hans K. Pedersen. A framework for interactive texturing on curved surfaces. In Computer Graphics (P1vceedings of SIGGRAPH '96), August 1996.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>917458</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Jorg Peters. Fitting smooth parametric surfaces to 3D data. Ph.d. thesis, Univ. of Wisconsin-Madison, 1990.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves. Simple and complex facial animation: Case studies. In State Of The Art In FaciaI Animation, SIGGRAPH course 26, pages 90-106.1990.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[David R.Forsey and Richard H. Bartels. Surface fitting with hierarchical splines. In Topics in the Construction, Manipulation, and Assessment of Spline Sulfaces, SIGGRAPH course 25, pages 7-0-7-14.1991.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>71804</ref_obj_id>
				<ref_obj_pid>71799</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[D.F. Rogers and N. G. Fog. Constrained b-spline curve and surface fitting. Computer Aided Geometric Design, 21:641-648, December 1989.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15906</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Francis J. M. Schmitt, Brian A. Barsky, and Wen hui Du. An adaptive subdivision method for surface-fitting from sampled data. In Computer Graphics (SIG- GRAPH '86 P1vceedings), volume 20, pages 179-188, August 1986.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122745</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Stan Sclaroff and Alex Pentland. Generalized implicit functions for computer graphics. In Computer Graphics (SIGGRAPH '91 P1vceedings), volume 25, pages 247-250, July 1991.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Sarvajit S. Sinha and Pradeep Seneviratne. Single valuedness, parameterization and approximating 3d surfaces using b-splines. Geometric Methods in Computer Vision 2, pages 193-204,1993.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[J.F. Thompson. The Eagle Papers. Mississippi State University, P.O. Drawer 6176, Mississippi State, MS 39762.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Re-tiling polygonal surfaces. In Computer Graphics (SIGGRAPH '92 P1vceedings), volume 26, pages 55-64, July 1992.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Greg Turk and Marc Levoy. Zippered polygon meshes from range images. In Computer Graphics (SIGGRAPH ' 94 P1vceedings), pages 311-318, July 1994.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[William Welch and Andrew Witkin. Free-Form shape design using triangulated surfaces. In Computer Graphics (P1vceedings of SIGGRAPH '94), pages 247- 256, July 1994.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[William Welch and Andrew Witkin. Free-form shape design using triangulated surfaces. In Computer Graphics (SIGGRAPH '94 P1vceedings), volume 28, pages 237-246, July 1994.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fitting Smooth Surfaces to Dense Polygon Meshes Venkat Krishnamurthy Marc Levoy Computer Science Department 
Stanford University Abstract Recent progress in acquiring shape from range data permits the ac­quisition 
of seamless million-polygon meshes from physical mod­els. In this paper, we present an algorithm and 
system for convert­ing dense irregular polygon meshes of arbitrary topology into ten­sor product B-spline 
surface patches with accompanying displace­ment maps. This choice of representation yields a coarse but 
ef.­cient model suitable for animation and a .ne but more expensive model suitable for rendering. The 
.rst step in our process consists of interactively painting patch boundaries over a rendering of the 
mesh. In many applica­tions, interactive placement of patch boundaries is considered part of the creative 
process and is not amenable to automation. The next stepisgriddedresamplingofeachboundedsectionofthemesh. 
Our resamplingalgorithmlaysagrid ofspringsacrossthepolygonmesh, then iterates between relaxing this grid 
and subdividing it. This grid provides a parameterization for the mesh section, which is initially unparameterized. 
Finally, we .t a tensor product B-spline surface to the grid. We also output a displacement map for each 
mesh section, which represents the error between our .tted surface and the spring grid. These displacement 
maps are images; hence this representa­tion facilitates the use of image processing operators for manipulat­ing 
the geometric detail of an object. They are also compatible with modern photo-realistic rendering systems. 
Our resampling and .tting steps are fast enough to surface a mil­lion polygon mesh in under 10 minutes 
-important for an interactive system. CR Categories: I.3.5 [Computer Graphics]: Computational Geom­etry 
and Object Modeling curve, surface and object representa­tions; I.3.7[Computer Graphics]:Three-Dimensional 
Graphics and Realism texture; J.6[Computer-Aided Engineering]:Computer-Aided Design (CAD); G.1.2[Approximation]:Spline 
Approxima­tion Additional Key Words: Surface .tting, Parameterization, Dense polygon meshes, B-spline 
surfaces, Displacement maps Authors Address: Department of Computer Science, Stanford University, Stanford, 
CA 94305 E-mail: venkat,levoy@cs.stanford.edu World Wide Web: http://www-graphics.stanford.edu/.venkat, 
.levoy Permission to make digital or hard copies of part or all of this work or personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 1 Introduction Advances in range image acquisition 
and integration allow us to compute geometrical models from complex physicalmodels [9, 36]. The output 
of these technologies is a dense, seamless (i.e. mani­fold) irregular polygon mesh of arbitrary topology. 
For example, the model in .gure 12, generated from 75 scans of an action .gure using a Cyberware laser 
range scanner, contains 350,000 polygons. Models like this offer new opportunities to modelers and animators 
in the CAD and entertainment industries. Dense polygon meshes are an adequate representation for some 
applications. Several commercial animation houses employ poly­gon meshes almost exclusively. However, 
for reasons of compact­ness, control, manufacturability, or appearance, many users prefer smooth surface 
representations. To satisfy these users, techniques are needed for .tting surfaces to dense meshes of 
arbitrary topology. A notable property of these new acquisition techniques is their ability to capture 
.ne surface detail. Whatever .tting technique we employ should strive to retain this .ne detail. Surprisingly, 
a uni.ed surfacerepresentationmaynotbethebestapproach. First,theheavy machinery of most smooth surface 
representations (for example B­splines) makes them an inef.cient way to represent .ne geometric detail. 
Second and perhaps more important, although geometric de­tail is useful at the rendering stage of an 
animation pipeline, it may not be of interest to either the modeler or the animator. Moreover, its presence 
may degrade the time or memory performance of the modeling system. For these reasons, we believe it is 
advantageous to separate the representations of coarse geometry and .ne surface detail. Within this framework, 
we may choose from among many rep­resentations for these two components. For representing coarse geometry, 
modelers in the entertainment and CAD industry have long used NURBS [14] and in particular uniform tensor 
product B­splines. Such models typically consist of control meshes stitched together to the level of 
continuity desired for an application. In or­der to address their needs we have chosen uniform tensor 
product B-splines as our surface representation. For representing surface detail, we propose using displacement 
maps. Each pixel in such a map gives an offset from a point on a .tted surface to a point on a gridded 
resampling of the original polygonmesh. Theprincipaladvantageofthisrepresentationisthat displacement 
maps are essentially images. As such, they can be processed, retouched, compressed, and otherwise manipulated 
us­ing simple image processing tools. Some of the effects shown in .gures 11 and 13 were achieved using 
Adobe Photoshop, a com­mercial photo retouching program. 1.1 System overview Figure 1 shows the pipeline 
for our system. We start with a con­nected polygon mesh. The additional connectivity information of­fered 
by a polygonal representation is used to advantage at every stage of our pipeline. Our steps are as follows: 
Set of stitched B-Spline control meshes. Per patch Fit Surface spring meshes. Paint Automatically Scan 
and   boundary resample into integrate curves regular grid Dense, Physical irregular model. polygon 
mesh.  Set of associated displacement maps. Figure 1. Our surface .tting pipeline: the input to our 
system is a dense irregular polygon mesh. First, boundary curves for the desired spline patches are painted 
on the surface of the unparameterized model. The output of this step is a set of bounded mesh regions. 
We call each such region a polygonal patch. We next perform an automated resampling of this polygonal 
patch to form a regular grid that lies on the surface of the polygonal patch. We call this regular grid 
a spring mesh. Finally, we .t surfaces to the spring mesh and output both a B-spline surface representation 
and a set of associated displacement maps to capture the .ne detail. 1. Our .rst step is an interactive 
boundary curve painting phase wherein a modeler de.nes the boundaries of a number of patches. This is 
accomplished with tools that allow the paint­ing of curves directly on the surface of the unparameterized 
polygonal model. Here, the connectivity of the polygon mesh allows the use of local graph search algorithms 
to make curve painting operations rapid. This property is useful when a modeler wishes to experiment 
with different boundary curve con.gurations for the same model. Each region of the mesh that a B-spline 
surface must be .t to is called a polygonal patch. Since patch boundaries have been placed for artistic 
reasons, polygonal patches are not constrained to be height .elds. Our only assumptions about them are 
that each is a rectangularly parameterizable piece of the surface that has no holes. 2. In the next 
step we generate a gridded resampling for each polygonal patch. This is accomplished by an automatic 
coarse-to-.ne resampling of the patch, producing a regular grid that is constrained to lie on the polygonal 
surface. We call this grid the spring mesh. Its purpose is to establish a parameterization for the unparameterized 
surface. Our resampling algorithm is a combination of relaxation and subdivision steps that iteratively 
re.ne the spring mesh at a given resolution to obtain a better sampling of the underlying polygonal patch. 
This re.nement is explicitly directed by distortion metrics relevant to the spline .t. The output of 
this step is a .ne gridding of each polygonal patch in the model. 3. We now use standard gridded data 
.tting techniques to .t a B­spline surface to the spring mesh corresponding to each polyg­onal patch. 
The output of this step is a set of B-spline patches that represent the coarse geometry of the polygonal 
model. To represent .ne detail, we also compute a displacement map for each patch as a gridded resampling 
of the difference between the spring mesh and the B-spline surface. This regular sam­pling can conveniently 
be represented as a vector (rgb) image which stores a 3-valued displacement at each sample location. 
Each of these displacements represents a perturbation in the local coordinate frame of the spline surface. 
This image repre­sentation lends itself to a variety of interesting image process­ing operations such 
as compositing, painting, edge detection and compression. An issue in our technique, or in any tech­nique 
for .tting multiple patches to data is ensuring continu­ity between the patches. We use a combination 
of knot line matching and a stitching post-process which together give us G1continuity everywhere. This 
solution is widely used in the entertainment industry.  The remainder of this paper is organized as 
follows. Section 2 reviews relevant previous work. Section 3 describes our tech­niques for painting boundary 
curves over polygonal meshes. Sec­tion 4 presents our coarse-to-.ne, polygonal patch resampling algo­rithm 
and the surface .tting process. Section 5 describes our strat­egy for extracting displacement maps and 
some interesting applica­tions thereof. Section 6, discusses techniques for dealing with con­tinuity 
across patch boundaries. Finally, section 7 concludes by dis­cussing future work. Throughout this paper 
we will draw on examples from the enter­tainment industry. However, our techniques are generally applica­ble. 
  2 Previous work There is a large literature on surface .tting techniques in the CAD, computer vision 
and approximation theory .elds. We focus here on only those techniques from these areas that use dense 
(scanned) data of arbitrary topology to produce smooth surfaces. We can classify such surface .tting 
techniques as manual, semi-automated and au­tomated. 2.1 Manual techniques Manualapproachescanbedividedintotwocategories. 
The.rstcat­egory includes all methods for digitizing a physical model directly. For example, using a 
touch probe, one can acquire only data that is relevant to the .nal surface model. Catalogues of computer 
mod­els published by ViewPoint Data Labs and the work of Pixar s ani­mation group [24, 28] exemplify 
these methods. These methods in­volve human intervention throughout the data acquisition process and 
are hence time-consuming, especially if the model is complex or the data set required is large. In contrast, 
our pipeline employs automatic data acquisition methods [9]. The second category uses scanned data as 
a template to assist in the model construction process. Point cloud or triangulated data is typically 
imported into a conventional modeling system. A user then manually projects isolated points to this data 
as a means of de­termining the locations of control points (or edit points [15]) for smooth parametric 
surfaces. These methods require less human in­tervention than those in the .rst category but complex 
models may still require a lot of labour. 2.2 Semi-automated techniques The approaches in this category 
take point cloud data sets as input. Examples include commercial systems such as Imageware s Sur­facer 
[33], Delcam s CopyCAD, and some research systems [20, 22]. These approaches begin by identifying a subset 
of points that are to be approximated. Parameterization of data points is usu­ally accomplished by a 
user-guided process such as projection of the points to a manually constructed base plane or surface. 
A con­strained, non-linear least squares problem is then solved on this sub­set of the point cloud to 
obtain a B-spline surface for the speci.ed region. While point cloud techniques are widely applicable, 
they fail to exploit topological information already present in the input data. As demonstrated by Curless 
et al [9] and Turk et al [36], us­ing this additional information can signi.cantly improve quality of 
reconstruction. In the context of our surface .tting algorithm, work­ingwith connectedpolygonalrepresentationshasalsofacilitated 
the development of an automatic parameterization scheme. 2.3 Automated surface .tting techniques Eck 
et al [12] describe a method for .tting irregular meshes with a number of automatically placed bicubic 
Bezier patches. For the pa­rameterization step, a piecewise linear approximation to harmonic maps [11] 
is used, and the number of patches is adjusted to achieve .tting tolerances. While this method produces 
high quality surfaces, it includes a number of expensive optimization steps, making it too slow for an 
interactive system. Further, their technique does not sep­arate .ne geometric detail from coarse geometry. 
Particularly for very dense meshes, we .nd this separation both useful and prefer­able, as already explained. 
We compare some other aspects of the parameterization scheme of Eck et al [11] with ours in section 4.10. 
We brie.y mention some techniques [29, 31] that use hierarchi­cal algorithms to .t parametric surfaces 
to scanned data sets. While these approaches work well for regular data, they do not address the problem 
of unparameterized, irregular polygon meshes. Finally, Sclaroff et al [32] demonstrate the use of displacement 
maps in the context of interpolating data with generalized implicit surfaces. However, this method also 
works only on regular data sets. 2.4 Relevant work in texture mapping A key aspect of our method is 
an automatic parameterization scheme for irregular polygon meshes. As such, there are techniques in the 
texture mapping literature that address similar problems, no­tably the work of Bennis et al [6] and that 
of Maillot et al [21]. Both of these papers present schemes to re-parameterize surfaces for tex­ture 
mapping. These algorithms work well with regular data sets, such as discretized splines. However, they 
can exhibit objection­able parametric distortions in general [11]. Pedersen [25] describes a method for 
texture mapping (and hence parameterizing) implicit surfaces. While the methods work well with implicit 
surfaces, they rely on smoothness properties of the surface and require the evalua­tion of global surface 
derivatives. Irregular polygon meshes in gen­eral, are neither smooth nor conducive to the evaluation 
of global surface derivatives, as discussed by Welch et al [38].  3 Boundary curve speci.cation Our 
surface .tting pipeline starts with the user interactively seg­menting the polygonal model into a number 
of regions that are to be approximated by spline patches. A patch is speci.ed by inter­actively painting 
its boundary curves. This operation should be fast and provide intuitive feedback to the user. We have 
found that curves that lie on the surface of the model are easier to specify and manipulatethanunconstrainedspacecurves. 
Apolygonal(discrete) geodesic [23] is one possible representation. Unfortunately, this is expensive both 
to compute and to maintain. We instead represent patch boundaries as sampled geodesics. We call these 
face-point curves. The steps for painting a boundary curve are shown and de­scribed in .gure 3. This 
painting process yields a piecewise linear face-point curve on the surface through a sequence of picked 
vertices. We now smooth this face-point curve on the surface using a .tted B-spline  Ar S (a) (b) S: 
Space curve F: Sampled face point curve Figure 4. Sliding a face-point curve along a polygon mesh to 
smooth it. (a) shows an attractor Ar on the space curve and an attractee Ae on the polygon mesh. (b) 
shows a 1-D version and the rest position of Ae.  4 Fitting B-spline patches 4.1 B-spline .tting theory: 
overview In general, parametric curve and surface .tting to irregular data can be formulated as a non-linear 
least squares problem [10, 30]. The following discussion assumes uniform cubic (order 4) tensor product 
B-spline surfaces but holds for other kinds of parametric surfaces as well. The equation for a B-spline 
surface P(u;v)can ~ be written as: PP nm ~ P(u;v) Xi;jBi(u)Bj(v) (1) i.0 j.0 ~two parametric directions 
of the surface, Xi;j s are control points of the B-spline surface and the Biare fourth order B-spline 
basis functions [4] . Given some set of points fp~l(x, y, z), l=1 ...Mgto which a B­spline surface must 
be .t, we must .rst make an association of pa­rameter values u and v to each of these data points. Given 
these associations, an over-constrained system of linear equations can be generated from (1), where the 
Xi;jare unknowns. Each linear equa­tion in this system corresponds to a point p~lsatisfying (1). There­fore, 
a least squares solution [19] can be performed to obtain the Xi;j. In our application, we are not given 
parameter associations for our data points. Because the B-spline basis functions are non-linear in the 
parameter values, the problem of parametric surface .tting re­quires a non-linear optimization process. 
This is usually solved by starting with an initial guess for each pl s u and v values and subse­quently 
iterating between re.ning these values and re-.tting the sur­face with the improved parameter associations 
until some tolerance of.tisachieved[30]. Thisprocessisexpensivesinceitrequirescal­culation of spline 
surface partial derivatives at each of the original data points at every step of the iteration. Furthermore, 
the conver­gence of this iteration (and hence the quality of the .tted spline sur­face) is strongly dependant 
on the initial parameter values. If these are not good, convergence can be slow and in general is not 
guaran­teed [18, 20]. where Pis a point in 3-space, u and v are parameter values in the  4.2 Our surface 
.tting strategy To avoid the complexity and cost of the non-linear optimization pro­cess described above, 
we .rst resample each irregular polygonal patch into a regular grid of points (the spring mesh). We can 
then ap­ply gridded data .tting techniques [29] to this spring mesh to obtain a spline approximation. 
The advantage of these techniques is that they avoid the parameter re-estimation step described earlier 
and are hence signi.cantly faster. It is worth pointing out that in our appli­cation there is nothing 
sacrosanct about the original mesh vertices. In particular, the vertices produced by our range image 
integration method[9]areatascalethatapproachesthenoise-limited resolution of our sensor. As long as the 
grid is a reasonably careful sampling of the polygon mesh, surface quality is not compromised. We use 
a piecewise linear reconstruction for this sampling, which we .nd to be satisfactory. If in other applications 
it is required to .t the origi­nal mesh vertices, this can be accomplished by .rst parameterizing the 
mesh vertices using our regular spring grid and then running the standard non-linear optimization process 
described above. The following subsections describe how to perform a gridded re­sampling of each polygonal 
patch and discuss some of its advan­tages and drawbacks. For the discussion, patches are assumed to be 
four sided; cylindrical, toroidal and triangular patches are all mod­eled as special cases of four-sided 
patches. 4.3 Gridded resampling of each polygonal patch Each polygonal patch can be an arbitrary four-sided 
region of the polygon mesh. The only constraints are that it must be rectangu­larly parameterizable and 
must not have holes. These are reasonable assumptions since the models input to our system are seamless 
or can easily be made so by acquiring and integrating more scans and by recent hole-.lling techniques 
[9]. Our goal is to generate a uni­form grid of points over the polygonal surface that samples the sur­face 
well. Finite element literature [34] describes a number of tech­niques for generating grids over smooth 
surfaces. Unfortunately, these techniques rely on the existence of higher order global deriva­tives (i.e. 
a smooth surface de.nition already exists). While it is possible to make local approximations to surface 
curvature for irreg­ular polygonal surfaces [37], there is no scheme to evaluate global derivatives at 
arbitrary surface positions. 4.4 What is a good gridding of a surface? Although we cannot utilize the 
.nite element literature directly, it offers useful insight on objective functions one might minimize 
to produce different surface parameterizations. Since each polygonal patch is resampled into a regular 
grid in or­der to .t a smooth surface, it is important that the grid not lose any geometric detail present 
in the original data. We have chosen three criteria for our surface grids: (In the following, a grid 
line along ei­ther direction is referred to as an iso-curve; the two directions are called u and v.) 
1) Arc length uniformity: the grid spacing along a particular iso­ curve should be uniform. 2) Aspect 
ratio uniformity: the grid spacing along a u iso-curve should be the same as the grid spacing along a 
v iso-curve. 3) Parametric fairness: Every u and v iso-curve should be of the minimum possible length 
given the .rst two criteria. An obvious criterion we have omitted above is that iso-curves should always 
lie within the polygonal patch they are supposed to sample. Our intuitions for the above criteria are 
based on sampling the­ory. Since our triangulations come from real models that have been sampled uniformly 
over their surfaces, our triangle meshes tend to be uniformly dense across different parts of the polygonal 
model. For a resampling of such a mesh to be faithful, it should give equal importance to equal areas 
of the surface. With this intuition in mind, the arc-length criterion accounts for the fact that geometrically 
interesting detail is equally likely along any given section of an iso-curve on the surface. The aspect-ratio 
criterion captures the fact that detail is equally likely in either di­rection of a gridding. Finally, 
the parametric fairness term, mini­mizes wiggles along the spring iso-curves. This is important since 
 Figure 5. This .gure explores the three sampling criteria on part of the right leg of the model in .gure 
12a. Each of the above images represents a triangulated and smooth shaded spring mesh at a very low resolution. 
In each case, the number of spring points sampling the polygon mesh was kept the same. The differences 
arise from their redistribution over the surface. The spring edges are shown in red. (a) shows what happens 
when the aspect ratio criterion is left out. Notice how a lot of detail is captured in the vertical direction, 
but not in the horizontal. (b) shows the effect of leaving out the arc length criterion. Notice how the 
kneecap looks slightly bloated and that detail above and around the knee region is missed. This is because 
few samples were distributed over the knee resulting in a bad sampling of this region. (c) shows a missing 
fairness criterion. The iso-curves exhibit many wiggles . Finally (d) shows the result when all three 
criteria are met. See .gure 8a for the original model and 8e for a full resampling of the leg. the spline 
surface follows the spring mesh closely. The fairness cri­terion thus indirectly minimizes unnecessary 
wiggles in the spline iso-curves. Note that this term bears some similarities to the idea of fairness 
in the parametric interpolation literature [16].  4.5 A fast gridding algorithm Our algorithm is a coarse-to-.ne 
procedure that for each polygonal patch, incrementally builds a successively re.ned sampling of the patch 
into a spring mesh. At each subdivision level, the spring mesh points included in the procedure are a 
subset of face points of the polygonal patch. Here are the steps of the gridding algorithm: 1) Perform 
a seed .ll of the patch. This restricts graph searches to vertices of this patch only. 2) Compute an 
initial guess for the iso-curves using Dijkstra s shortest-path algorithm, with an appropriate aspect 
ratio for this patch. 3) Re.ne the spring mesh using the arc length and fairness crite­ ria. 4) Subdivide 
the spring mesh. 5) Iterate between steps 3 and 4 until the number of spring mesh points reaches a density 
close to that of the number of vertices of the polygonal patch. In our system, the user can stop the 
resampling at any stage to view incremental results and .t a spline surface to the spring mesh points 
at the current resolution. We consider some of these steps in detail in the following subsections.  
4.6 Initialization of iso-curves To obtain an initial guess for each u and v iso-curve, we use Dijk­stra 
s single-source, single-destination, shortest-path algorithm [1] to compute a path between corresponding 
pairs of points along op­posing boundary curves. The initial number of iso-curves in each direction are 
chosen to be proportional to the aspect ratio of the patch. This is computed as the ratio of the longer 
of the two bound­ary curves in either direction. The starting spring mesh points are computed as intersections 
of these initial iso-curves; the curves must intersect if the patch is rect­angularly parameterizable. 
Dijkstra s algorithm is O(nlog(n))in the number of vertices to be searched. However, since the vertex 
set is restricted to that of a single patch and we search for only a small set of initial iso-curves, 
this procedure is rapid. Starting with a large number of iso-curves is both slower and not guaranteed 
to produce as good a .nal spring mesh as starting with a small number of iso­curves and using a coarse-to-.ne 
re.nement. We return to this point in section 4.10.  4.7 Re.ning the spring mesh: relaxation The initial 
guess for the spring mesh, as obtained above, can be quite poor. The next step in the algorithm is to 
re.ne the position of the spring mesh points using a relaxation procedure. In our choice of the number 
of spring mesh points to place along each boundary curve, we have implemented criteria 2 (section 4.4): 
aspect ratio unifor­mity. Subsequent subdivisions are all uniform. During our relax­ation procedure, 
we apply the remaining two criteria of arc length and fairness. The relaxation procedure works as follows. 
Let Pup, Pdown, Pleftand Prightrepresent the positions of the 4 neighboring spring points in the u and 
v directions of the spring point P. The algorithm computes a resultant force on each of these points 
and slides it along the surface to a new position. P up P left P right P P down Figure 6. shows the 
neighbors of a face point Pof the spring mesh. The resultant force in a relaxation step is some linear 
combination of these forces. See the text for details. Minimizing arc length distortion along one of 
P s iso-curves is achieved by moving Ptowards whichever neighbor (on the same iso-curve) is farther away 
from it. Consider the two forces Force(Pup, P)and Force(Pdown, P). We make the direction of the larger 
of these two the direction of a new force Fud. The magnitude of Fudis set to be the difference of the 
two magnitudes. We perform a similar computation in the other direction (left-right) as well to get aforce 
Flr. Let us denote the resultant of Flrand Fudby Farc.This resultant becomes one of the two terms in 
equation (2) below. Fairness distortion is minimized by moving the point Pto a po­sition that minimizes 
the energy corresponding to the set of springs consisting of Pand P s immediate neighbors along both 
iso-curves. This corresponds to computing a force Ffairequal to the resultant of the forces acting on 
Pby its four neighbors: Force(Pup, P), Force(Pdown, P), Force(Pleft, P)and Force(Pright, P). The point 
Pis moved according to a force given by a weighted sum of Ffairand Farc: FresultC*Ffair+f*Farc (2) The 
relaxation iteration starts with C0and f1and ends with C1, f0. This strategy has proved to produce satisfactory 
results. Note that we have used Euclidean forces in the previous step, i.e. forces that represent the 
vector joining two spring points. A relax­ation step based on Euclidean forces alone is fast but not 
guaranteed to generate good results in all cases. Figure 7a shows an example where Euclidean forces alone 
fail to produce the desired effect. In contrast to Euclidean forces, geodesic forces are forces along 
the surface of the mesh. These would produce the correct motion for the spring points in the above case. 
One approach to solving the problem exempli.ed by Figure 7a, would be to use geodesic forces, or approximations 
thereof, as substitutes for Euclidean forces in the relaxation step. However this is an expensive proposition 
since the fastest algorithm for point to point geodesics is O(n 2)in the size of the patch [7]. Even 
approximations to geodesics such as local graph searches are O(n)and would be too expensive to perform 
at every relaxation step. A solution to the problem is motivated by .gure 7b; create a new spring point 
Pmid.pointthat lies on the surface halfway between P1and P2. This point generates new Euclidean forces 
acting on the original points, moving them towards each other on the surface. We call this process spring 
mesh subdivision.  4.8 Subdividing the spring mesh Spring mesh subdivision is based on a graph search 
and re.nement algorithm. Given two spring points P1and P2our algorithm com­putes a new face point Pthat 
is the mid-point of the two spring points and that lies on the graph represented by the patch. The pro­cedure 
is: 1.) Find the two closest vertices v1 and v2 on P1and P2 s faces. 2.) Compute a breadth .rst graph 
path from v1 to v2. The mid­ point of this path serves as a .rst approximation to P s loca­ tion. 3.) 
Re.ne this location by letting the forces given by Force(P1, P)and Force(P2, P) act on P, moving it to 
a new position on the surface. This process is distinct from the relaxation pro­ cess. It is used only 
to obtain a better approximation for P. Subdivision along boundary curves is based on a static resam­pling 
of the face point curve representation; these points are never moved during the relaxation and subdivision 
steps. We terminate subdivision when the number of spring points increases to within a certain range 
of the polygonal patch s vertices.  4.9 B-spline .tting to gridded data The techniques described above 
minimize parametric distortion in the spring mesh. In particular, they enforce minimal distortion with 
respect to aspect ratio and edge lengths while ensuring parametric V P2 P1  mid-pointV (a) (b) Figure 
7. shows a case where relaxation alone fails to move a spring mesh point in the desired direction. In 
each case F represents the force on P1from its right neighbor and V represents the resulting di­rection 
of motion. The desired motion of the point P1is into the cav­ity. In (a) just the opposite occurs; the 
points move apart. (b) shows how this case is handled by subdividing the spring mesh along the surface. 
See the text for details. fairness. The resulting spring meshes have low area distortion as well, as 
evidenced by the example shown in .gure 5. The .nal step in our algorithm is to perform an unconstrained, 
gridded data .t of a B-spline surface to each spring mesh. As pointed out earlier, .tting to a good resampling 
of the data does not compromise surface quality. We refer the reader to [29] for an ex­cellent tutorial 
on the subject of gridded data .tting. Figure 8 sum­marizes the sampling and .tting processes on a cylindrical 
patch of the model from .gure 12. 4.10 Discussion Our two-step approach of gridding and then .tting 
has several de­sirable characteristics. First, it is fast. This can be understood as follows. At each 
level of subdivision, each spring mesh point must traverse some fraction of the polygons as it relaxes. 
The cost of this relaxation thus depends linearly on size of the polygon mesh. It ob­viously also depends 
on the size of the spring mesh. If these two were equal, as would occur if we immediately subdivided 
the spring meshto the.nestlevel,thenthecostofrunningtherelaxationwould be O(n 2). If, however, we employ 
the coarse-to-.ne strategy de­scribed in the foregoing sections, then at each subdivision level, four 
times as many spring mesh points move as on the previous (coarser) level, but they move on average half 
as far. Thus, the cost of relax­ation at each subdivision level is linear in the number of spring mesh 
points, and the total cost due to relaxation is O(nlogn). This argu­ment breaks down if we start with 
a large initial set of iso-curves. Similar arguments apply to the cost of subdivision. A second advantage 
of our overall strategy is that it allows a user to pause the iteration at an intermediate stage and 
still obtain good quality previews of the model. This is a useful property for an inter­active system, 
specially when dealing with large meshes. In partic­ular, subdivision to higher levels can be postponed 
until the model designer is satis.ed with a patch con.guration. A third advantage of our approach is 
that once the resampling is done, the spline resolution can be changed interactively, since no further 
parameter re-estimation is necessary. We have found this to be a useful interactive tool for a modeler, 
specially when making the tradeoff between explicitly represented geometry and displacement mapped detail 
as explained in section 5. As mentioned earlier, there are other schemes that may be used to parameterize 
irregular polygon meshes. In particular, the harmonic maps of Eck et al[11] produce parameterizations 
with low edge and aspect ratio distortions. However, the scheme has two main draw­backs for our purposes. 
First, it can cause excessive area distortions in parameter space. Second, the algorithm employs an O(n 
2)itera­tion to generate .nal parameter values for vertices of the mesh and no usable intermediate parameterizations 
are produced. As pointed Figure 8. The above represents a summary of our strategy for resampling a polygonal 
patch into a regular grid. (a) shows the original polygonal patch (the right leg from the model in .gure 
12a. This particular patch is cylindrical and has over 25000 vertices. (b), (c), (d) and (e)show a triangulated 
and smooth shaded reconstruction of the spring mesh at various stages of our re-sampling algorithm. We 
omit the lines from (e) to prevent clutter. (b) shows the initial guess for u and v iso-curves (under 
4 seconds). Notice that the guess is of a poor quality. (c) shows the mesh after the .rst relaxation 
step (under 1 second). (d) shows the spring mesh at an intermediate stage, after a few relaxation and 
subdivision steps (under 3 seconds). (e) shows the .nal spring mesh without the spring iso-curves. Notice 
how the .ne detail on the leg was accurately captured by the resampled grid. This resampling took 23 
seconds. All times are on a 250 Mhz Mips R4400 processor. (f) shows a spline .t that captures the coarse 
geometry of the patch. This surface has 27x36 control points. It took under 1 second to perform a gridded 
data .t to the spring mesh of (e). out in the discussion above, we have found intermediate parameter­izations 
useful in an interactive system. Our .tting technique is capable of capturing both .ne and coarse geometry. 
However, we typically use it only to capture the coarse geometry. Considerforexamplethepolygonalpatchfrom.gure8a. 
We .nd that most of its coarse geometry has been captured by the spline surface of .gure 8e. Although 
the remaining surface detail might be of little use to an animator, it is desirable to retain and use 
this information as well, if only for rendering. While there are a variety of multi-resolution techniques 
that can be applied to capture these details in a uni.ed surface representa­tion [13, 15], for reasons 
discussed earlier, we represent the .ne de­tail in our models as displacement maps. In the next section 
we .rst describe how to extract displacement maps from a polygonal patch and then demonstrate some of 
the operations that are enabled by this representation.  5 Capturing .ne detail with displacement maps 
A displacement map perturbs the position of a surface based on a displacement function de.ned over the 
surface [8]. Displacement maps are usually applied during rendering and are available in a number of 
commercial renderers. A typical formulation for a bivari­ate parametric surface, such as a B-spline is: 
given a point P(u;v) on the surface, a displacement map is a function d(u;v)giving a perturbation of 
the point Pin space. In general dcan be a vector or a scalar. In the .rst case, the new position of the 
point is P+d.In ~ the second case, the new position of the point is usually interpreted ^^ as P+Nd,where 
Nrepresents the surface normal at (u, v). 5.1 Vector displacement maps In the context of our .tting 
system, the obvious displacement func­tion relates points on the spline surface to points on triangles 
of the  : spring mesh point : iso-curve of spline surface : piecewise linear reconstruction of spring 
iso-curve^ N : normal to spline Figure 9. A vector displacement map over a curve. Displacement vectors 
are shown from an iso-curve S of the spline surface to an iso­curve P of the spring mesh. This map recreates 
the spring mesh (with a bilinear reconstruction). original polygon mesh. However, computing such a function 
re­quires projecting perpendicularly from the spline surface to the orig­inal unparameterized mesh -an 
expensive operation. Furthermore, our .tting procedure is premised on the assumption that the spring 
mesh is a faithful representation of the original mesh. Therefore, we de.ne a displacement function that 
relates points on the spline sur­face to points on the parameterized spring mesh surface. Even given 
this simpli.cation, computing a displacement func­tion using perpendicular projection is dif.cult. In 
particular, the method may fail if the spring mesh curves sharply away from the spline surface. We can 
avoid this dif.culty by de.ning displace­ments as offsets to vertices of the spring mesh from corresponding 
points on the spline surface. Recall that there is a natural associ­ation of the spring mesh points to 
parameter values: these are the same parameters that were used for the surface .tting step. We thus obtain 
a regular grid of displacement vectors at the resolution of the spring mesh. These are represented in 
the local coordinate frame of the spline surface. For applications that modify the underlying sur­face 
(such as animation), this choice of coordinate frame allows the displacements to move and deform relative 
to the surface. The dis­placement function d(u;v)is then given by a reconstruction from this grid of 
samples. We have used a bilinear .lter for the images shown in this paper. Note that the displacement 
map is essentially a resampled error function since it represents the difference of the positions of 
the spring points from the spline surface. Since the displacement map, as computed, is a regular grid 
of 3­vectors, it can conveniently be represented as an rgb image. This representation permits the use 
of a variety of image-processing op­erations such as painting, compression, scaling and compositing to 
manipulate .ne surface detail. Figure 11 and Figure 13 explore these and other games one can play with 
displacement maps.  5.2 Scalar displacement maps While vector displacement maps are useful for a variety 
of effects, some operations such as (displacement image) painting are more in­tuitive on grayscale images. 
There are several methods of arriving at a scalar displacement image. One method is to compute a nor­mal 
offset from the spline surface to the spring mesh. However, as discussed earlier, this method is both 
expensive and prone to non­robustness in the presence of high curvature in the spring mesh. Instead we 
have used two alternative formulations. The .rst computes and stores at each sample location (or pixel) 
the magni­tude of the corresponding vector displacement. In this case, mod­ifying the scalar image scales 
the associated vector displacements along their existing directions. A second alternative, stores at 
each sample location the component of the displacement vector normal to the spline surface. Modifying 
the scalar image therfore changes only the normal component of the vector displacement. Both of these 
last two options offer different interactions with the displacement map. Figure 11 employs the third 
option -normal component editing. 5.3 Bump maps A bump map is de.ned as a function that performs perturbations 
on the direction of the surface normal before using it in lighting calcu­lations [5]. In general, a bump 
map is less expensive to render than a displacement map since it does not change the geometry (and oc­clusion 
properties) within a scene but instead changes only the shad­ing. Bump maps can achieve visual effects 
similar to displacement maps except in situations where the cues provided by displaced ge­ometry become 
evident such as along silhouette edges. We compute and store bump maps using techniques very similar 
to those used for displacement maps; at each sample location instead of storing the displacement we store 
the normal of the corresponding spring mesh point. Figures 13d and e show a comparison of a displace­ment 
mapped spline and a bump mapped spline, both of which are based on the same underlying spring mesh. Notice 
how, differences are visible at the silhouette edges.  6 Continuity across patch boundaries Thus far 
we have addressed the sampling and .tting issues con­nected with a single polygonal patch. In the presence 
of multiple patches, we are faced with the problem of keeping patches continu­ous across shared boundaries 
and corners. If displacement maps are used, it is essential to keep the displacement mapped spline surface 
continuous. The extent of inter-patch continuity desired in a multi-patch B­spline (or more generally, 
NURBS) model depends on the domain of application. For example, in the construction of the exterior of 
a car body, curvature plots and re.ection lines [14] are frequently used to verify the quality of surfaces. 
In this context, even curvature con­tinuous (C2) surfaces might be inadequate. Furthermore, workers in 
the automotive industry often use trimmed NURBS and do not nec­essarily match knot lines at shared patch 
boundaries during model construction. Therefore it is not always possible to enforce math­ematical continuity 
across patch boundaries. Instead, statistical or visual continuity is enforced based on user speci.ed 
tolerances for position and normal deviation. These are either enforced as linear constraints to the 
.tting process [2, 33], or they are achieved through an iterative optimization process [22]. In the 
animation industry, by contrast, curvature continuity is seldom required and tangent continuity (G1) 
usually suf.ces. To achieve this the number of knots are usually forced to be the same for patches sharing 
a boundary. This has several advantages. In the .rst place, control point deformations are easily propagated 
across patch boundaries. Secondly, there is minimal distortion at bound­aries in the process of texture 
mapping. Finally, the process of maintaining patch continuity during an animation becomes easier; continuity 
is either made part of the model de.nition [24] or is re­established on a frame by frame basis using 
a stitching post-process. Our system can be adapted to either of the above paradigms (i.e. either statistical 
or geometric continuity). Since our focus in this pa­per is on the entertainment industry, we enforce 
geometrical conti­nuity, and for this purpose we use a stitching post-process. Specif­ically, we allow 
an animator to specify the level of continuity re­quired for each boundary curve (C0or G1). Unconstrained, 
gridded data .tting to each patch leaves us with C .1spline boundaries. We use end-point interpolating, 
uniform, cubic B-splines. To maintain mathematical continuity we constrain adjacent patches to have the 
same number of control points along a shared boundary. Following boundary conditions for these surfaces 
as de.ned by Barsky [3], C0 continuity across a shared boundary curve is obtained by averaging end control 
points between adjacent patches. Alternatively, G1con­tinuity can be obtained by modifying the end control 
points such that the tangent control points (last two rows) line up in a .xed ratio over the length of 
the boundary. Patch corners pose a harder problem. We refer the reader to [27] for a detailed account 
of this problem. For the kinds of basis func­tions we use, projecting the 4 corner control points of 
each of the patches meeting at that corner to an average plane guarantees G1 continuity. For the case 
of displacement mapped splines, continuity may be de.ned on the basis of the reconstruction .lter used 
for the displace­ment maps. Recall that we generate these from the spring meshes and that we use bilinear 
reconstruction. Displacement mapped splines will therefore exactly recreate the spring mesh. Also adja­cent 
patches can at most be position continuous with a bilinear re­construction .lter. Therefore, if the spring 
resolutions are the same at a shared boundary of two patches, they will be continuous by virtue of the 
reconstruction. However, spring mesh resolutions can differacrosssharedboundaries. ThiscanresultinoccasionalT-joint 
discontinuities. The problem is solved by averaging bordering rows of displacement maps in adjacent patches. 
This ensures that there is no cracking at patch boundaries. Figure 12 shows a case study of the use of 
our system to .t spline surfaces, with associated displacement maps, to a large and detailed polygonal 
mesh of an action .gure. 7 Conclusions and future work In conclusion, we have presented fast techniques 
for .tting smooth parametric surfaces, in particular tensor product B-splines, to dense, irregular polygon 
meshes. A useful feature of our system is that it allows incremental previewing of large patch con.gurations. 
This feature is enabled by our coarse-to-.ne gridded resampling scheme and proves invaluable when modelers 
wish to experiment with dif­ferent patch con.gurations of the same model. We also provide a useful method 
for storing and manipulating .ne surface detail in the form of displacement map images. We have found 
that this repre­sentation empowers users to manipulate geometry using tools out­side our modeling system. 
Oursystemhasseverallimitations. First,becauseitreliesonsur­face walking strategies and mesh connectivity 
to resample polygo­nal patches, it breaks down in the presence of holes in the polygon mesh. However, 
new range image integration techniques include methods for .lling holes. Another limitation is that B-spline 
surface patches, our choice to represent coarse geometry, perform poorly for very complex sur­faces such 
as draped cloth. B-splines have other disadvantages as well, such as the inability to model triangular 
patches without ex­cessive parametric distortion. Despite these limitations, B-splines (and NURBS in 
general) are widely used in the modeling industry. This has been our motivation for choosing this over 
other represen­tations. There are a number of fruitful directions for future research. Straightforward 
extensions include developing tools to assist in boundary curve painting and editing, improving robustness 
in the presence of holes, adding further constraints to the the parameter­ization process, allowing variable 
knot density at the .tting stage, implementing other continuity solutions, and using adaptive spring 
grids for sampling decimated meshes. An example of a boundary painting tool is a geometry-snapping brush 
that attaches curves to features as the user draws on the object. Examples of constraints to the parameterization 
process include interactively placed curve and point attractors within a patch. An interesting application 
of the parameterization portion of our system is the interactive texture mapping and texture placement 
[26] for complex polygonal models. Related to this is the possibility of applying procedural texture 
analysis/synthesis techniques [17] to create synthetic displacement maps from real ones. Using our tech­niques 
such maps can be applied to objects of arbitrary topology.  8 Acknowledgements We thank Brian Curless 
for his range image integration software, David Addleman and George Dabrowski of Cyberware for the use 
of a scanner, Lincoln Hu and Christian Rouet of Industrial Light and Magic for advice on the needs of 
animators, and Pat Hanrahan,Hans Pederson, Bill Lorensen for numerous useful discussions. We also thank 
the anonymous reviewers for several useful comments. This work was supported by the National Science 
Foundation under con­tract CCR-9157767 and Interval Research Corporation.  References [1] A.V.Aho and 
J.D.Ullman. Data structures and algorithms. Addison-Wesley, 1979. [2] L Bardis and M Va.adou. Ship-hull 
geometry representation with b-spline sur­face patches. Computer Aided Design, 24(4):217 222,1992. [3] 
Brian A. Barsky. End conditions and boundary conditions for uniform b-spline curve and surface representations. 
Computers In Industry, 3, 1982. [4] Richard Bartels, John Beatty, and Brian Barsky. An Introduction to 
Splines for Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann Pub­lishers, Palo Alto, 
CA, 1987. [5] James F. Blinn. Simulation of wrinkled surfaces. In Computer Graphics (SIG-GRAPH 78 Proceedings), 
volume 12, pages 286 292, 1978. [6] J. Vezien Chakib Bennis and G. Iglesias. Piecewise surface .attening 
for non­distorted texture mapping.In ComputerGraphics (SIGGRAPH 91 Proceedings), volume 25, pages 237 
246, July 1991. [7] Jindong Chen and Yijie Han. Shortest paths on a polyhedron. In Proc. 6th Annual ACM 
Symposium on Computational Geometry, pages 360 369, June 1990. [8] Robert L. Cook. Shade trees. In Computer 
Graphics (SIGGRAPH 84 Proceed­ings), volume 18, pages 223 231, July 1984. [9] Brian Curless and Marc 
Levoy. A volumetric method for building complex mod­els from range images. In Computer Graphics (SIGGRAPH 
96 Proceedings), August 1996. [10] Paul Dierckx. Curve and Surface Fitting with Splines. Oxford Science 
Publica­tions, New York, 1993. [11] MatthiasEck,TonyDeRose,TomDuchamp,HuguesHoppe,MichaelLounsbery, 
and Werner Stuetzle. Multiresolution analysis of arbitrary meshes. In Computer Graphics (Proceedings 
of SIGGRAPH 95), pages 173 182, August 1995. [12] Matthias Eck and Hugues Hoppe. Automatic reconstruction 
of b-spline surfaces ofarbitrarytopologicaltype. In Computer Graphics (Proceedingsof SIGGRAPH 96), August 
1996. [13] Hugues Hoppe et al. Piecewise smooth surface reconstruction. In Computer Graphics (Proceedings 
of SIGGRAPH 94), pages 295 302, July 1994. [14] Gerald Farin. Curves and Surfaces for Computer Aided 
Geometric Design. Aca­demic Press, 1990. [15] David R. Forsey and Richard H. Bartels. Hierarchical B-spline 
re.nement. In Computer Graphics (SIGGRAPH 88 Proceedings), volume 22, pages 205 212, August 1988. [16] 
Mark Halstead, Michael Kass, and Tony DeRose. Ef.cient, fair interpolation us­ing Catmull-Clark surfaces. 
In Computer Graphics (SIGGRAPH 93 Proceed­ings), volume 27, pages 35 44, August 1993. [17] DavidHeegerandJamesR.Bergen.Pyramid-basedtextureanalysis/synthesis.In 
Computer Graphics (SIGGRAPH 95 Proceedings), pages 229 237, July 1995. [18] J Hoschek and D Lasser. Fundamentals 
of Computer Aided Geometric Design. AK Peters, Wellesley, 1993. [19] Charles L. Lawson and Richard J. 
Hanson. Solving Least Square Problems. Prentice-Hall, Englewood Cliffs, New Jersey, 1974. [20] W. Ma 
and J P Kruth. Parameterization of randomly measured points for least squares .tting of b-spline curves 
and surfaces. Computer Aided Design, 27(9):663 675,1995. [21] JeromeMaillot.Interactivetexturemapping.In 
ComputerGraphics (SIGGRAPH 93 Proceedings), volume 27, pages 27 34, July 1993. [22] M. J. Milroy, C. 
Bradley, G. W. Vickers, and D. J. Weir. G1 continuity of b-spline surface patches in reverse engineering. 
Computer-Aided Design, 27:471 478, 1995. [23] J. S. B. Mitchell, D. M. Mount, and C. H. Papadimitriou. 
The discrete geodesic problem. SIAM J. Comput., 16:647 668, 1987. [24] Eben Ostby. Describing free-form 
3d surfaces for animation. In Workshop on Interactive 3D Graphics, pages 251 258, 1986. [25] Hans K. 
Pedersen. Decorating implicit surfaces. In Computer Graphics (Pro­ceedings of SIGGRAPH 95), pages 291 
300, August 1995. [26] Hans K. Pedersen. A framework for interactive texturing on curved surfaces. In 
Computer Graphics (Proceedings of SIGGRAPH 96), August 1996. [27] Jorg Peters. Fitting smooth parametric 
surfaces to 3D data. Ph.d. thesis, Univ. of Wisconsin-Madison, 1990. [28] William T. Reeves. Simple and 
complex facial animation: Case studies. In State Of The Art In Facial Animation, SIGGRAPH course 26, 
pages 90 106. 1990. [29] David R.Forsey and Richard H. Bartels. Surface .tting with hierarchical splines. 
In Topics in the Construction, Manipulation, and Assessment of Spline Surfaces, SIGGRAPH course 25, pages 
7 0 7 14. 1991. [30] D. F. Rogers and N. G. Fog. Constrained b-spline curve and surface .tting. Com­puter 
Aided Geometric Design, 21:641 648, December 1989. [31] Francis J. M. Schmitt, Brian A. Barsky, and Wen 
hui Du. An adaptive subdivi­sion method for surface-.tting from sampled data. In Computer Graphics (SIG-GRAPH 
86 Proceedings), volume 20, pages 179 188, August 1986. [32] Stan Sclaroff and Alex Pentland. Generalized 
implicit functions for computer graphics. In Computer Graphics (SIGGRAPH 91 Proceedings), volume 25, 
pages 247 250, July 1991. [33] Sarvajit S. Sinha and Pradeep Seneviratne. Single valuedness, parameterization 
and approximating3d surfaces using b-splines. Geometric Methods in Computer Vision 2, pages 193 204, 
1993. [34] J. F. Thompson. The Eagle Papers. Mississippi State University, P.O. Drawer 6176, Mississippi 
State, MS 39762. [35] Greg Turk. Re-tiling polygonal surfaces. In Computer Graphics (SIGGRAPH 92 Proceedings), 
volume 26, pages 55 64, July 1992. [36] Greg Turk and Marc Levoy. Zippered polygon meshes from range 
images. In Computer Graphics (SIGGRAPH 94 Proceedings),pages311 318,July 1994. [37] William Welch and 
Andrew Witkin. Free Form shape design using triangulated surfaces. In Computer Graphics (Proceedings 
of SIGGRAPH 94), pages 247 256, July 1994. [38] William Welch and Andrew Witkin. Free-form shape design 
using triangulated surfaces. In Computer Graphics (SIGGRAPH 94 Proceedings), volume 28, pages 237 246, 
July 1994. Appendix A - v -- P v = vt tangent (a.) T2 T2 T1 T1 (b.) (c.) Figure 10. Fig a.) shows 
a side view of a face point being pulled over the surface. (b) and (c) show a top view of the two cases 
that arise when P moves: it either intersects an edge or it intersects a vertex. An operation we use 
often on face points is sliding them on the surface. We call this procedure MovePointOnSurface.There 
are a number of ways of implementing this on polygonal surfaces. Turk [35] describesa scheme where points 
.rst leave the surface and then are re-projected back. We use instead a scheme where points never leave 
the surface but instead just slide along it. Our algo­rithm projects the force on a face-point P to P 
s plane. The point P is moved along the surface, till it either meets an edge or a vertex. In either 
case we determine the appropriate next triangle to move in to using our adjacency structure (eg: a winged-edge 
representation).  Figure 11. This .gure explores the possibility of multi-resolution editing of geometry 
using multiple displacement map images. All grayscale displacement images in this .gure represent the 
normal componentoftheircorrespondingdisplacementmaps. Displacement values are scaled such that a white 
pixel represents the maximum displacement and black, the minimum displacement. (a) shows a B­spline surface 
with 24x30 control points that has been .t to the patch from .gure 8a. (c) is its corresponding displacement 
image. (b) shows a B-spline surface with 12x14 control points that was also .t to the same patch. Its 
displacement image is shown in (d). The combi­nation of spline and displacement map in both cases reconstructs 
the same surface (i.e. the original spring mesh of .gure 8e). This surface is shown in (f). We observe 
that (c) and (d) encode different frequen­cies in the original mesh. For example (d) encodes a lot of 
the coarse geometry of the leg as part of the displacement image (for example the knee), while (c) encodes 
only the .ne geometric detail, such as bumps and creases. As such, the two images allow editing of geom­etryatdifferentscales. 
Forexample,onecaneditthegeometryofthe knee using a simple paint program on (d). In this case, the resulting 
edited displacement map is shown in (e) and the result of applying this image to the spline of (b) gives 
us an armour plated knee that is shown in (g). Operations such as these lead us to the issue of whether 
multiple levels of displacement map can essentially provide a image .lter bank for geometry i.e. an alternative 
multi-resolution surface representation based on images. Note however that the images from (c) and (d) 
are offsets from different surfaces and the displacements are in different directions, so they cannot 
be combined using simple arithmetic operations.  Figure 12. Data.ttingtoascannedmodel. (a)isthepolygonalmodel(over350,000polygons,75scans). 
(b)and(c)showtwodifferentsetsofboundary curves painted on the model. Each was speci.ed interactively 
in under 2 hours. The patch boundaries for (d), (e), (f) and (g) are taken from (b). (d) is a close up 
of the results of our gridded resampling algorithm at an intermediate stage. The spring mesh is reconstructed 
and rendered as triangles and the spring edges are shown as red lines. The right half of the .gure is 
the original polygon mesh. (e) shows u and v iso-curves for all the .tted and stitched spline patches. 
(The control mesh resolution was chosen to be 8x8 for all the patches.) (f.) shows a split view of the 
B-spline surfaces smooth shaded on the left with the polygon mesh on the right. A few interesting displacement 
maps are shown alongside their corresponding patches. (g) shows a split view of the displacement mapped 
spline patches on the left with the polygon mesh on the right. Note that the .ngers and toes of the model 
were not patched. This is because insuf.cient data was acquired in the crevices of those regions. This 
can be easily remedied by using extra scans or hole .lling techniques [9]. The total number of patches 
for (b and d through g) were 104 (only the left half have been shown here). The gridding stage took 8 
minutes and the gridded .tting with 8x8 control meshes per patch, took under 10 seconds for the entire 
set of 104 patches. All timings are on a 250 Mhz MIPS R4400 processor. Figure 13. Games one can play 
with displacement maps: (a) shows a patch from the back of the model in 12a. The patch has over 25,000 
vertices. We obtained a spline .t (in 30 seconds) with a 15x20 control mesh, shown in (b) and a corresponding 
vector displacement map. The normal component of the vector displacement map, is displayed as a grayscale 
image in (c). (d) and (e) show the corresponding displacement and bump mapped spline surface. The differences 
between (d) and (e) are evident at the silhouette edges. The second row of images show a selection of 
image processing games on the displacement map. (f) shows jpeg compression of the displacement image 
to a factor of 10 and (g) shows compression to a factor of 20. (h) represents a scaling of the displacement 
image, to enhance bumps. (i) demonstrates a compositing operation, where an image with some words was 
alpha composited with the displacement map. The result is an embossed effect for the lettering. Finally, 
the third row of images (j -l) show transferring of displacement maps between different objects. (j) 
is a relatively small polygonal model of a wolf s head (under 60,000 polygons). It was .t with 54 spline 
patches in under 4 minutes. The splined model is shown in (k). (l) shows a close up view of a partially 
splined result, where we have mapped the displacement map from (c) onto each of 4 spline patches around 
the eyes of the model.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237271</article_id>
		<sort_key>325</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[Automatic reconstruction of B-spline surfaces of arbitrary topological type]]></title>
		<page_from>325</page_from>
		<page_to>334</page_to>
		<doi_number>10.1145/237170.237271</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237271</url>
		<keywords>
			<kw><![CDATA[range data analysis]]></kw>
			<kw><![CDATA[shape recovery]]></kw>
			<kw><![CDATA[surface fitting]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Spline and piecewise polynomial approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31076977</person_id>
				<author_profile_id><![CDATA[81100357649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Darmstadt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P112851</person_id>
				<author_profile_id><![CDATA[81100397561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hugues]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoppe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>48486</ref_obj_id>
				<ref_obj_pid>48483</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ANDERSSON, E., ANDERSSON, R., BOMAN, M., ELM- ROTH, T., DAHLBERG, B., AND JOHANSSON, B. Automatic construction of surfaces with prescribed shape. CAD 20 (1988), 317-324.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218424</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., BERNARDINI, F., AND XU, G. Automatic reconstruction of surfaces and scalar fields from 3D scans. Computer Graphics (SIGGRAPH '95 Proceedings) (1995), 109-118.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B., AND LEVOY, M. A volumetric method for building complex models from range images. Computer Graphics (SIGGRAPH '96 Proceedings) (1996).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DIETZ, U. Erzeugung glatter Fl~ichen aus Mef3punkten. Technical Report 1717, Department of Mathematics, University of Darmstadt, Germany, February 1995.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DOO, D., AND SABIN, M. Behaviour of recursive division surfaces near extraordinary points. CAD 10, 6 (September 1978), 356-360.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218440</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY~ M.~ AND STUETZLE~ W. Multiresolution analysis of arbitrary meshes. Computer Graphics (SIGGRAPH '95 Proceedings) (1995), 173-182.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FANG~ L.~ AND GOSSARD~ D. Reconstruction of smooth parametric surfaces from unorganized data points. In Curves and Swfaces in Computer Vision and Graphics 3 (1992), J. Warren, Ed., vol. 1830, SPIE, pp. 226-236.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>151048</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FARIN, G. Curves and Surfaces for Computer Aided Geometric Design, 3rd ed. Academic Press, 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>221665</ref_obj_id>
				<ref_obj_pid>221659</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FORSEY~ D.~ AND BARTELS~ R. Surface fitting with hierarchical splines. ACM Transactions on Graphics 14, 2 (1995), 134-161.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GREINER~ G. Variational design and fairing of spline surfaces. Computer Graphics Forum 13, 3 (1994), 143-154.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., HALSTEAD~ M., JIN, H., MCDONALD, J., SCHWEITZER, J., AND STUETZLE, W. Piecewise smooth surface reconstruction. Compuwr Graphics (SIGGRAPH '94 Proceedings) (1994), 295-302.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134011</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, ,J., AND STUETZLE, W. Surface reconstruction from unorganized points. Computer Graphics (SIGGRAPH '92 Proceedings) 26, 2 (1992), 71-78.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, ,J., AND STUETZLE, W. Mesh optimization. Computer Graphics (SIGGRAPH '93 Proceedings) (1993), 19-26.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>174506</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HOSCHEK, J., AND LASSER, D. Fundamentals of Computer Aided Geometric Design. AK Peters, Wellesley, 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>80734</ref_obj_id>
				<ref_obj_pid>80732</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HOSCHEK, J., SCHNEIDER, F.-J., AND WASSUM, P. Optimal approximate conversion of spline surfaces. CAGD 6 (1989), 293-306.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[IMAGEWARE~ CORP. Surfacer product information, http:- //www'iware'c~m/htmls/surfacer'html"]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237270</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KRISHNAMURTHY~ V.~ AND LEVOY~ M. Fitting smooth surfaces to dense polygon meshes. Computer Graphics (DIG- GRAPH '96 Proceedings) (1996).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LAWLER~ E. L. Combinatorial optimization: networks and matroids. Holt, Rinehart, and Winston, 1976.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LAWSON, C., AND HANSON, R. Solving Least Squares Problems. Prentice-Hall, Englewood Cliffs, NJ, 1974.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LEE, S., TAN, H., AND MAJID, A. Smooth piecewise biquartic surfaces from quadrilateral control polyhedra with isolated n-sided faces. CAD 27 (1995), 741-758.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192238</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Loop, C. Smooth spline surfaces over irregular meshes. Computer Graphics (SIGGRAPH '94 Proceedings) (1994), 303-310.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MA, W., AND KRUTH, J. Parametrization of randomly measured points for least squares fitting of B-spline curves and surfaces. CAD 27 (1995), 663-675.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MILROY, M., BRADLEY, C., VICKERS, G., AND WEIR, D. G1 continuity of B-spline surface patches in reverse engineering. CAD 27 (1995), 471-478.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[MOORE, D., AND WARREN, ,J. Approximation of dense scattered data using algebraic surfaces. In Proc. of the 24th Annual Hawaii Intnl. Conf. on System Sciences, Kauai, HI, USA (1991), V. Milutinovic and D. Shriver, Eds., IEEE Comp. Soc. Press, pp. 681-690.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134035</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[MORETON, H., AND SI~QUIN, C. Functional optimization for fair surface design. Computer Graphics 26, 3 (July 1992), 167-176.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>124971</ref_obj_id>
				<ref_obj_pid>124966</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[NASRI, A. H. Boundary-corner control in recursivesubdivision surfaces. CAD 23, 6 (1991), 405-410.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PETERS, J. Constructing C~ surfaces of arbitrary topology using biquadratic and bicubic splines. In Designing Fair Curves and Swfaces, N. Sapidis, Ed. SIAM, 1994, pp. 277- 293.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[PETERS, J. Biquartic C~-surface splines over irregular meshes. CAD (1995). submitted.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>211168</ref_obj_id>
				<ref_obj_pid>211163</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[REIF, U. Biquadratic G-spline surfaces. CAGD 12 (1995), 193-205.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>71804</ref_obj_id>
				<ref_obj_pid>71799</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[ROGERS, D., AND FOG, N. Constrained B-spline curve and surface fitting. CAD 21 (1989), 641-648.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>124934</ref_obj_id>
				<ref_obj_pid>124930</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[SARKAR, B., AND MENQ, C.-H. Parameter optimization in approximating curves and surfaces to measurement data. CAGD 8 (1991), 267-290.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15906</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[SCHMITT, F., BARSKY, B., AND DU, W. H. An adaptive subdivision method for surface fitting from sampled data. Compuwr Graphics (SIGGRAPH '86 Proceedings) 20 (1986), 179-188.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>3485</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[TARJAN, R. Data structures and network algorithms. CBMS- NSF Regional Conference Series in Applied Mathematics 44 (1983).]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[TURK, G., AND LEVOY, M. Zippered polygon meshes from range images. Computer Graphics (SIGGRAPH '94 Proceedings) 28, 3 (1994), 311-318.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automatic Reconstruction of B-Spline Surfaces of Arbitrary Topological Type Matthias Eck University 
of Darmstadt  ABSTRACT Creating freeform surfaces is a challenging task even with advanced geometric 
modeling systems. Laser range scanners offer a promis­ing alternative for model acquisition the 3D scanning 
of existing objects or clay maquettes. The problem of converting the dense point sets produced by laser 
scanners into useful geometric models is referred to as surface reconstruction. In this paper, we present 
a procedure for reconstructing a tensor product B-spline surface from a set of scanned 3D points. Unlike 
previous work which considers primarily the problem of .tting a single B-spline patch, our goal is to 
directly reconstruct a surface of arbitrary topological type. We must therefore de.ne the surface as 
a network of B-spline patches. A key ingredient in our solution is a scheme for automatically constructing 
both a network of patches and a parametrization of the data points over these patches. In addi­tion, 
we de.ne the B-spline surface using a surface spline construc­tion, and demonstrate that such an approach 
leads to an ef.cient procedure for .tting the surface while maintaining tangent plane continuity. We 
explore adaptive re.nement of the patch network in order to satisfy user-speci.ed error tolerances, and 
demonstrate our method on both synthetic and real data. CR Categories and Subject Descriptors: I.3.5 
[Computer Graphics]: Computational Geometry and Object Modeling -surfaces and object repre­sentations; 
J.6 [Computer-Aided Engineering]: Computer-Aided Design. Additional Keywords: surface .tting, shape recovery, 
range data analysis. 1 INTRODUCTION In the .elds of computer graphics and computer-aided design (CAD), 
advanced modeling systems such as SOFTIMAGE 3D, ALIAS/WAVEFRONT, CATIA, and ICEM SURF have made possi­ble 
the design of highly detailed models. Even so, it is still dif.cult with these systems to directly create 
organic shapes such as human faces and freeform surfaces such as car-body panels. Email: eck@mathematik.th-darmstadt.de 
Email: hhoppe@microsoft.com Permission to make digital or hard copies of part or all of this work or 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage and that copies bear this notice and the full citation on the first page. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 Hugues Hoppe Microsoft Research 
 The advent of laser range scanners offers an alternative means of acquiring geometric models: the 3D 
scanning of existing objects. With 3D scanning, modeling systems can import organic or sculp­tured shapes 
that would otherwise be dif.cult to create. For instance, in the automobile industries, many artists 
prefer to initially sculpt car bodies in clay, as they .nd that CAD systems lack the tactile and visual 
advantages of the traditional medium. Similarly, many mod­els used in computer graphics are .rst created 
in clay or wood and subsequently scanned into digital forms. In addition, 3D scanning permits reverse 
engineering, allowing existing manufactured parts to be incorporated or modi.ed into new CAD designs. 
Laser range scanners produce large collections of points on sur­faces of objects. The problem of converting 
these data points into useful geometric models is referred to as surface reconstruction. There is a large 
body of literature on the reconstruction of sur­faces of simple topological type, such as deformed planar 
regions and spheres (see Section 2). Methods have been developed to re­construct meshes of arbitrary 
topological type [3, 13, 34], but the resulting representations are often verbose since many planar faces 
are required to accurately model curved surfaces (e.g. Figure 9l). For this reason, it is desirable to 
use a representation with smooth surface primitives. Some recent work addresses the problem of reconstructing 
smooth surfaces of arbitrary topological type using subdivision surfaces [11] and algebraic surfaces 
[2, 24]. How­ever, these two smooth surface representations are not commonly supported within current 
modeling systems. Indeed, for better or worse, the ubiquitous smooth surface primitive is the tensor 
product B-spline patch. The general class of non-uniform rational B-splines (NURBS) is considered by 
many the de facto CAD standard. In this paper we present a procedure for automatically recon­structing 
a B-spline surface S of arbitrary topological type from an unorganized set of points P = fp1;:::;pN g. 
To our knowledge, this reconstruction problem has not been addressed previously. The problem presents 
two major dif.culties: Since a single B-spline patch can only represent surfaces of simple topological 
type (deformed planar regions, cylinders, and tori), a surface of arbitrary topological type must be 
de.ned as a network of B-spline patches. Automatically constructing both a network of patches and a parametrization 
of the data points over these patches is a dif.cult task. The reconstructed B-spline patch network is 
often expected to be smooth (by which we mean tangent plane continuous or G1). Enforcing G1 continuity 
between adjacent patches while at the same time .tting the patch network to the points is a challenging 
problem. Our B-spline reconstruction procedure adapts the previous sur­face reconstruction work of Hoppe 
et al. [12, 13], the parametriza­tion work of Eck et al. [6], and the B-spline construction scheme of 
Peters [27], as summarized in Sections 3.1, 3.2, and 3.4 respectively. The main contributions of this 
paper are: It presents a combinatorial optimization method for building a quadrilateral domain from 
a triangular one (Section 3.3), a crucial step in constructing the B-spline patch network. The optimization 
method makes use of harmonic maps to minimize distortion in the resulting reparametrization.  It presents 
an ef.cient method for .tting a G1 B-spline surface of arbitrary topological type to unorganized points. 
The .tting method makes use of a surface spline construction to maintain G1 continuity between patches. 
As a consequence, .tting the surface to the data involves only a sparse linear least squares problem 
with a few linear constraints.  It introduces a scheme for adaptive re.nement of the quadri­lateral 
patch network, and demonstrates the use of this re.ne­ment strategy in attempting to .t B-spline surfaces 
within user­speci.ed error tolerances.  Most importantly, it brings together all these techniques into 
an effective procedure addressing an important problem in com­puter graphics and geometric modeling: 
automatic reconstruc­tion of B-spline surfaces of arbitrary topological type.  In addition to surface 
reconstruction, our procedure can also be applied to the problem of surface approximation. That is, it 
can be used to approximate an arbitrary initial surface S0 with a B-spline surface (e.g. Figures 10j 
10l) as shown in Section 4. 2 RELATED WORK Reconstruction of B-spline surfaces There has been con­siderable 
work on .tting B-spline surfaces to 3D points. However, most methods either assume that the surface has 
simple topological type, or require user intervention in setting up the patch network. For instance, 
Dietz [4], Hoschek and Schneider [15], Rogers and Fog [30], and Sarkar and Menq [31] assume that the 
surface is a single open B-spline patch (a deformed quadrilateral region), possibly with trimmed boundaries. 
Forsey and Bartels [9] consider .tting a single hierarchical B-spline patch to gridded data. Schmitt 
et al. [32] assume that the surface is a deformed cylinder and explore adaptive re.nement of the B-spline 
surface in .tting cylindrical range data. Andersson et al. [1], Fang and Gossard [7], Krishnamurthy and 
Levoy [17], and Milroy et al. [23] .t B-spline surfaces of arbi­trary topological type, but require the 
user to manually delineate the patch boundaries either by labeling boundary points or by draw­ing boundary 
curves on an approximating surface. The same is true of current commercial systems such as Imageware 
s Surfacer [16]. Furthermore, the initial parametrizations of the data points is critical in the .tting 
process, as demonstrated by Ma and Kruth [22], and these schemes may require additional user intervention 
to obtain good initial parameter distributions. Krishnamurthy and Levoy [17] develop a hierarchical relaxation 
procedure for automatically com­puting these parameter values. In contrast, our method is able to reconstruct 
a B-spline surface of arbitrary topological type without user assistance. To our knowledge this has not 
been done before. Moreover, the surface consists of a network of low-degree, tensor-product B-spline 
patches that meet with G1 continuity. Reconstruction of other smooth surface representations Hoppe et 
al. [11] reconstruct piecewise smooth surfaces of arbitrary topological type using a subdivision surface 
representation. Both Bajaj et al. [2] and Moore and Warren [24] reconstruct G1 piecewise algebraic surfaces 
of arbitrary topological type. Their surfaces are de.ned as algebraic patches within 3D (tetrahedral) 
triangulations of R3. They consider adaptive re.nement of the 3D triangulation based on the quality of 
.t. 3 ALGORITHM Our B-spline surface reconstruction algorithm consists of 5 suc­cessive steps. We .rst 
present a brief overview of these steps and illustrate them with the example in Figure 9. Sections 3.1 
3.5 describe the details of the 5 steps. 1. Constructing an initial parametrization over a dense approxi­matingmeshM0: 
Using the previous surface reconstruction work of Hoppe et al. [12, 13], Step 1 constructs from an unorganized 
set of points P = fp1;:::;pN g(Figure 9a) an initial surface approximation in the form of a dense triangular 
mesh M0 (Figure 9b). The points P are projected onto M0 to obtain their initial parametrizations. Our 
purpose in constructing M0 is to .nd a parametric domain of the correct topological type. Of course, 
this particular domain is unwieldy since it may consist of thousands of faces. 2. Reparametrizing over 
a simple triangular base complex K4: Using the parametrization work of Eck et al. [6], Step 2 au­tomatically 
constructs from the initial mesh M0 both a simple base complex K4(Figure 9e) and a continuous parametrization 
p4: K4!M0. As the construction exploits the mathemati­cal framework of harmonic maps, the parametrization 
p4tends to have low metric distortion. The parametrization of P from Step 1 are mapped through p 1 to 
obtain new parametrizations 4 over K4.  3. Reparametrizing over a quadrilateral domain complex K2: By 
merging faces of K4pairwise, Step 3 constructs a new base complex K2whose faces consist solely of quadrilaterals 
(Fig­ure 9f). The merging process is cast as a combinatorial graph optimization problem, whose goals 
are both to .nd a maximum pairing and to minimize parametric distortion. We again make use of harmonic 
maps to .nd a good reparametrization of the points P from K4to K2. 4. B-spline .tting: Step 4 de.nes 
over each face f of K2a tensor product B-spline patch sf using the surface splines scheme of Peters [27] 
such that the patches sf collectively form a G1 B-spline surface S.More precisely, this construction 
consists of two steps. First, a control mesh Mx is de.ned by topologically subdividing K2. Second, the 
control points dfr;s of sf are de.ned as af.ne combinations of the vertices Vx of Mx. Fitting S to the 
points P is cast as an opti­mization problem over Vx, and is solved by alternating between a linear least 
squares .tting step and a parameter correction step. The result of this .tting process is shown in Figures 
9g 9i. 5. Adaptive re.nement: In order for P and S to differ by no more than a user-speci.ed error tolerance 
I, Step 5 adaptively subdivides the faces of K2into smaller quadrilateral subfaces based on the .t errors. 
After each step of subdivision, Step 4 is reinvoked to .t the re.ned surface. Further subdivisions are 
performed until the error tolerance Iis satis.ed. The result is a re.ned domain complex K2 + (Figure 
9j) and a new control mesh (Figure 9k) de.ning a new B-spline surface S (Figure 9l) within Iof P. 3.1 
Constructing an initial parametrization over a dense approximating mesh M0 From an unorganized set of 
points P, Step 1 constructs an initial surface approximation M0 and parametrizes the points over this 
ini­tial domain. This step is performed using the surface reconstruction method of Hoppe et al., which 
we brie.y summarize now. Figure 1: Example of the two-phase surface reconstruction method of Hoppe et 
al.. (Refer also to Figures 9a and 9b.) Summary of surface reconstruction method of Hoppe et al. Hoppe 
et al. [12, 13] develop a two-phase procedure for reconstructing a mesh (Figure 1b) approximating an 
unknown sur­face Su from a set of unorganized points P (Figure 9a) sampled on or near Su. The goal of 
phase one [12] is to determine the topological type of Su and to obtain a crude estimate of its geometry, 
in the form of a dense mesh (Figure 1a). Using P, phase one de.nes a function f : R3 !Rthat estimates 
the signed geometric distance to Su,and then uses a contouring algorithm to extract a mesh approximating 
its zero set, Z(f )= fq2R3: f (q)=0g. The goal of phase two [13] is to reduce the number of faces in 
the mesh and to improve its .t to the data (Figure 1b). Phase two optimizes over both the connectivity 
and geometry of the mesh in order to minimize an energy function that explicitly models the trade-offs 
of conciseness and accuracy. Our use of the surface reconstruction method For our purpose, we .rst run 
phase one to obtain a crude mesh (Figure 1a). We then use the initial .tting procedure of phase two to 
improve the geometry of this mesh while keeping its connectivity constant, to obtain the mesh M0 (Figure 
9b). The optimization over connectivity performed later in phase two is unnecessary for our use, since 
Step 2 (described in the next section) provides a faster algorithm for creating a simpler domain and 
at the same time constructs a low­distortion parametrization of P over that domain. To obtain an initial 
parametrization of P, we project the points onto the mesh M0. For each point pi, we store the closest 
face of M0 and the barycentric coordinates of the projection of pi onto that face. 3.2 Reparametrizing 
over a simple triangular base complex K4 From the initial mesh M0, Step 2 constructs a simple base complex 
K4(Figure 9e) and a map p4: K4!M0 allowing the points P to be reparametrized over K4. This step is achieved 
using the parametrization method of Eck et al. [6], which we brie.y sum­marize. Next we present a minor 
modi.cation to the method that facilitates the construction of K2in Step 3. Summary of parametrization 
method of Eck et al. Eck et al. .rst introduce a method for mapping a (topological) disk D CM of a mesh 
M CR3 to a convex polygonal region R CR2. As an example, the mesh region in Figure 2a is parametrized 
over the planar polygon in Figure 2b. Their solution, based on the theory of harmonic maps, has the property 
of minimizing metric distortion. They .nd that the metric distortion energy Eharm[h] associated with 
Figure 2: An example of a harmonic map. (The vertices indicated by small balls are mapped to the vertices 
of the polygon.) a piecewise linear map h : D !R can be interpreted as the energy of a con.guration of 
springs placed on the edges of D: X Eharm[h]=1/2 Ii;jkh(i) -h(j)k2 ; fi;jg2Edges(D) where each spring 
constant Ii;j is a simple function of the lengths of nearby edges in the original mesh D. Thus the (piecewise-linear) 
harmonic map h on D can be computed by solving a sparse linear least-squares problem. Since the initial 
mesh M0 may have arbitrary topological type, it must .rst be partitioned into a set of disks in order 
to apply the har­monic map framework. Eck et al. describe a method for partitioning M0 into well-shaped 
triangular regions. This partitioning method is based on generalizing the concepts of Voronoi diagrams 
and De­launay triangulations to surfaces of arbitrary topological type. The algorithm automatically selects 
a set of site faces in M0 and parti­tions M0 into a set of Voronoi tiles, such that each tile comprises 
those faces closest to a given site (Figure 9c). The Voronoi tiles are grown incrementally from their 
site faces using a multi-source shortest path algorithm. In order for the Voronoi-like partition to be 
dual to a triangulation, the sites are chosen to satisfy a set of 4 conditions (see [6]). Next, the method 
makes use of harmonic maps to construct a Delaunay-like triangular partition T1;:::;Tr (Figure 9d) that 
is dual to the Voronoi-like partition. Finally, Eck et al. construct a base complex K4of r faces (Fig­ure 
9e) using the connectivity of the Delaunay-like partition, and parametrize M0 over this domain by computing 
the harmonic map from each Delaunay triangle Ti to the corresponding face of K4. The result is a global, 
continuous parametrization p4: K4!M0 of the initial mesh over a simple base complex. Modi.cation to the 
parametrization method In Step 3 de­scribed in the next section, we construct from K4a new domain K2 
with quadrilateral faces by matching adjacent pairs of faces in K4. To form a complete matching, the 
face merging process requires K4to have an even number of faces. This requirement is met by giving the 
Voronoi partitioning algorithm an additional condition to satisfy: The dual to the Voronoi partition 
must have an even number of faces. When this condition is not satis.ed, an additional site is added at 
the face farthest from any current site, and the Voronoi region growing algorithm restarts. Reparametrization 
After constructing K4and p4,we map the parametrizations of P obtained in Step 1 through p41 to obtain 
parametrizations of P over K4. The new parametrization is illus­trated in Figure 9e, where a line segment 
is drawn between each data point pi and its parametric location on K4. Note that we do not de.ne a geometric 
embedding of K4into R3 but have created one in Figure 9e for illustration purposes only. 3.3 Reparametrizing 
over a quadrilateral do­main complex K 2 After Step 2, the points P are parametrized over a base complex 
K4made up of an even number of triangular faces. Since the B-spline construction scheme in Step 4 expects 
a domain made up of quadrilateral faces, the goal of Step 3 is to map K4onto a quadrilateral domain complex 
K2(Figure 9f). A simple strategy would be to subdivide each triangular face of K4into 3 quadrilateral 
faces by introducing vertices at the edge midpoints and the face centroids. Instead, our method is based 
on merging triangle faces of K4pairwise. This merging strategy is advantageous because it results in 
a domain K2with one sixth the number of quadrilaterals as would be obtained from subdivision. We cast 
face merging as a graph matching problem. We construct the graph G =(VG;EG) that is the dual to K4: each 
vertex in VG corresponds to a face of K4, and each edge in EG corresponds to a pair of faces sharing 
an edge in K4. Finding a maximum pair­ing of faces in K4then amounts to .nding a maximum cardinality 
set Em CEG of vertex-disjoint edges an instance of the MAXI-MUM MATCHING graph problem on G [18], which 
can be solved ef.ciently in O(jVGjjEGj) time [33]. We would like to obtain a complete matching: one in 
which all faces of K4are paired. Since we have constructed K4to have an even number of regions, a complete 
matching is likely to exist. Although counter-examples may be possible, we have not seen them occur in 
practice. (It would be interesting to prove if such counter­examples can or cannot exist.) If G was to 
lack a complete matching, we would resort to global subdivision as described above. The graph G typically 
has many possible complete matchings. Of those, we would prefer one that minimizes the distortion of 
the resulting reparametrization. In order to achieve this, we de.ne a heuristic for the distortion associated 
with the pairing of two adja­cent faces Fi and Fj of K4as follows. We construct the harmonic map hi;j 
of the region Ti UTj of M0 onto a unit square, and use the re­sulting harmonic energy term Eharm[hi;j] 
as our heuristic measure of distortion. We encode these distortion measures into G by assigning to each 
edge e = fi;jg2EG the weight w(e)= -Eharm[hi;j]. The face merging problem is now cast as an instance 
of the MAX-MIN MATCHING problem .nding a maximum cardinality matching for which the minimum weight of 
the edges is maximum [18]. A solution to this combinatorial problem corresponds to a complete pairing 
of faces of K4for which the maximal distortion of the face pairs is minimized. The MAX-MIN MATCHING problem 
can be solved in O(jVGj3) time [18]. Since our graphs G typically have on the order of a hundred vertices, 
computing the matching requires only a few seconds. Once the matching is computed, the parametrizations 
of the points P are mapped from K4to K2using the same harmonic maps con­structed for the graph optimization 
problem. Speci.cally, for each edge fi;jg2Em of the matching solution, we map the points whose parametrizations 
lie on faces Fi and Fj of K4through hi;j onto the unit square, and use the resulting coordinates as (bilinear) 
parametrizations on the new face Fi;j in K2. The parametrizations are illustrated by the line segments 
in Figure 9f. Again, we have created an embedding for K2in R3 in the .gure for illustration purposes 
only. There is one .nal complication. The resulting K2may have interior vertices of degree 2, and such 
vertices are best avoided for Step 4. When such vertices are present, we merge the two quadrilateral 
faces adjacent to them into larger quadrilateral faces. 3.4 B-spline .tting General framework In the 
most general setting, a B-spline sur­face S(K2;d) is de.ned as a network of tensor product B-spline surface 
patches nf mf XX f sf (u;v)= dr;s Nr;kf (u) Ns;lf (v) r=0 s=0 over a domain complex K2, with local coordinates 
(u;v) 2[0;1]2 on each face f 2K2.Here drf ;s 2R3 denote the control points, Nr;kf (u) are the univariate 
B-spline basis functions of or­der kf in the u-direction, de.ned over the knot sequences Uf = (u0;u1;:::;unf 
+kf ), and Ns;lf (v) are de.ned analogously over the knot vectors Vf in the v-direction. De.nitions of 
the B-spline basis functions and related evaluation algorithms can be found in text­books on geometric 
modeling (e.g. [8, 14]). Surface reconstruction In surface reconstruction we seek to .nd the control 
points drf ;s of all patches sf such that the distance of the data points P to the surface S(K2;d) is 
minimized. More precisely, we minimize the distance functional N X Edist(S)= d2(pi;S) : i=1 Note that 
the distance of each point pi to the surface S is itself the solution of a minimization problem: d(pi;S)= 
min kpi -s(ti)k2 = min kpi -sfi (ui;vi)k2 ti fi 2K2(ui; vi )2[0;1]2 ; in which ti =(fi;ui;vi) is the 
parametrization of the projection of pi onto S. Iterative methods have been developed to solve this type 
of nested minimization problem in the context of B-spline surface .tting [15, 30]. In these methods, 
each iteration consists of two steps: 1. Fitting step: For .xed parametrizations ti, the optimal control 
points dare found by solving a linear least-squares problem. 2. Parameter correction step: For .xed 
control points d, optimal parametrizations ti are found by projecting the points onto S.  Usually the 
.t accuracy is improved considerably after only a few iterations (we typically use 4). (An alternative 
solution method to this nonlinear problem is the Levenberg Marquardt optimization method,whichhasfasterconvergencerate[31]; 
however, oursimple iterative scheme is suf.cient for obtaining reasonable .ts.) Fairness functional One 
problem with surface .tting is that the resulting surface may have unwanted wiggles . It is there­fore 
common to augment the energy functional with an additional fairness term [4, 7]: E(d)= Edist(d)+ ) Efair(d) 
;)2R0+ :(1) The fairness term is often de.ned to be the thin plate energy func­tional Z Z 1 1 2 22 X 
@@@ Efair(d)= ( sf )2 +2( sf )2 +( sf )2du dv 00@u2 @u @v @v2 f 2K2 (Greiner [10] discusses alternative 
functionals involving higher­order derivatives.) Note that E(d) can still be minimized with the iterative 
scheme described previously since Efair(d) is independent of the parameter values ti and its minimization 
still gives rise to a linear system. There remains the problem of .nding a reasonable choice for the 
fairness weight ). Dietz [4] suggests starting with a relatively large initial weight )and reducing )by 
a factor of 2 after each iteration of parameter correction. In our case, the initial parametrizations 
obtained in Step 3 are quite good, and we have obtained satisfactory results using simply a small, constant 
). Continuity Obviously, constraints must be established between adjacent B-spline patches so that they 
join up seamlessly. To sim­plify these constraints, most schemes (e.g. [23]) set all patches to have 
the same knot vectors (i.e. n = nf = mf and U = Uf = Vf ) and the same order k = kf = lf . Then, simple 
(G0) continuity is achieved trivially by sharing control points along the boundaries of adjacent patches. 
In contrast, tangent plane (G1) continuity is more dif.cult since it involves nonlinear constraints on 
the control points of adjacent patches. There are two main approaches to satisfying these G1 continuity 
constraints. In the .rst approach [23, 25], the nonlinear G1 constraints are approximated by introducing 
an additional penalty term EG1(d)to minimize. Unfortunately, minimizing Edist(d)+ Efair(d)+ EG1(d) requires 
more expensive nonlinear optimization. Moreover, the resulting surface is only approximately tangent-plane 
smooth, or I-G1, and the lack of smoothness is often visible in the resulting surfaces (e.g. [23]). In 
the second approach, often referred to as surface splines or G-splines [20, 21, 27, 28, 29], the idea 
is to construct a network of triangular and/or tensor product B´ ezier patches from a global control 
mesh Mx. The control points of these B´ ezier patches are computed using local combinations of vertices 
in Mx , and are de.ned in such a way that the B´ ezier patches automatically meet with G1 continuity. 
Using this approach, the surface is exactly G1,and the .tting process again involves solving a simple 
linear system, in which the unknowns are the vertex positions of Mx. We have opted for the second approach, 
and have adapted a surface spline scheme of Peters [27]. As described in the next section, we construct 
over each face of K2a single tensor product B-spline patch sf with k =4 and n = 11. To overcome the problem 
of .xed n and k, we present in Section 3.5 a re.nement scheme that adaptively subdivides K2to locally 
introduce additional degrees of freedom. bi-cubic bi-quadr. G 1 join C 1 join (a) (b) (c) Figure 3: 
Schematic of the B-spline construction scheme of Peters. (a) one quadrilateral face f of the input mesh 
Mc;(b) the 4 .4 vertices of the re.ned control mesh Mx associated with f ;(c) the 4 .4B´ ezier patches 
created from Mx associated with f . Review of Peters scheme Fromaclosed mesh Mc of arbitrary topological 
type, the construction scheme of Peters [27] creates a G1 B-spline surface S. This construction proceeds 
in two steps, as illustrated in Figure 3. First, a re.ned control mesh Mx is created In the second step, 
a tensor product B´ ezier patch is constructed centered on each vertex of Mx as shown in Figure 3c. 
The B´ ezier patch is de.ned to be bicubic if the vertex is adjacent to an extraor­dinary face, otherwise 
it is de.ned to be biquadratic. The af.ne combinations for setting the B´ ezier control points of these 
patches as functions of the vertices Vx of Mx are given in the Appendix. Peters [27] proves that the 
resulting collection of B´ ezier patches form a G1 surface, subject to a few linear constraints on Vx 
near those extraordinary faces for which m is even and greater than 4 (see Appendix). We denote this 
G1 surface as S(Vx). Over each quadrilateral face of Mc, the collection of 4 .4B´ ezier patches (in 
general 12 biquadratic and 4 bicubic) can be combined into a single tensor product bicubic B-spline patch 
(with k =4 and n = 11). To satisfy the G1 and C1 joins indicated in Fig­ure 3c, the knot sequences in 
both parameter directions are set to 11111333 Uf = Vf =(0;0;0;0;4 ;4 ;4 ;2 ;2 ;4 ;4 ;4 ;1;1;1;1) . The 
B-spline representation requires 15% less storage than storing each B´ ezier patch separately. Modi.ed 
.tting step To apply Peters scheme to the problem of B-spline .tting, we modify the .tting step in the 
iterative procedure described earlier. We use the quadrilateral domain complex K2as the input mesh Mc 
to Peters scheme. Since K2does not possess a geometric embedding, only the topological structure Kx of 
the control mesh Mx can be constructed initially. The vertices Vx of Mx are computed by .tting the B-spline 
surface S(Vx) to the data points. Speci.cally, we compute Vx by minimizing the energy functional E(Vx)= 
Edist(Vx)+ )Efair(Vx) for .xed parametrizations ti =(fi;ui;vi) of the data points pi. Since Peters construction 
is af.ne, every point s(t) on the surface S can be written as an af.ne combination of Vx. Treating Vx 
as a matrix whose rows are (x;y;z) coordinates, we can express this af.ne combination as s(t)= yVx where 
the entries of the row vector yare obtained by appropriately composing Bernstein polynomials and the 
formulas given in the Appendix. We can therefore rewrite Edist as N Edist(Vx)= X kpi -yiVxk2 i=1 which 
is quadratic on Vx.The term Efair can similarly be expressed as a quadratic function over Vx by summing 
up the thin-plate energies of all B´ ezier patches and using the formulas given in the Appendix. Thus, 
E(Vx) is a quadratic functional on Vx, and therefore its minimization is a linear least squares problem. 
Moreover, the linear system is sparse because of the locality of the surface construction. As mentioned 
earlier, some linear constraints on Vx must be satis.ed near extraordinary faces for the surface to be 
G1. These constraints are introduced into the optimization through the use of Lagrange multipliers, making 
the problem only slightly more dif.cult (see [19] for details). Extensions to the basic .tting method 
We generalize the construction of Mx to allow surface boundaries in K2. In a con­struction similar to 
[26], we add for each boundary edge of K2an additional layer of vertices to Mx. To each valence m boundary 
vertex of K2we associate in Mx a(2m-2)-sided face if m by subdividing Mc using two Doo-Sabin subdivisions 
[5]. In our =2anda 6 4-sided face if m = 2. This process is illustrated in Figure 4. As a re­ sult, the 
boundaries of S are smooth everywhere except at valence 2 application Mc has only quadrilateral faces, 
and therefore a 4 .4 grid of vertices in Mx is associated with each face of Mc as shown boundary vertices 
of K2where surface corners are introduced. in Figure 3b. Note that all vertices of Mx have valence 4 
(i.e. 4 The two Doo-Sabin subdivisions in the .rst step of Peters con­adjacent edges) and that Mx consists 
mainly of 4-sided faces, except struction serve to isolate the extraordinary faces. With two subdi­ for 
a small number of extraordinary m-sided faces (m =6 note that the extraordinary faces are isolated, in 
the sense that each as shown in Figure 3b. More generally, a construction with s .s vertex of Mx is adjacent 
to at most one extraordinary face. vertices on each face of K2still results in a G1 surface for any 4). 
Also visions, a 4 .4 grid of vertices is introduced on each face of K2  Figure 4: Example of construction 
of Mx from a domain K2con­taining a boundary.  template 1 template 2 template 3 template 4 Figure 5: 
The four face re.nement templates. s ?2. We have experimented with different values of s in the .tting 
procedure, but have obtained best visual results with the original setting of s =4. 3.5 Adaptive re.nement 
The surface .tting algorithm described in Section 3.4 minimizes P the total squared distances id(pi;S)2 
of the data points pi to the B-spline surface S. It is often desirable to specify a maximum error tolerance 
for the .t. Step 5 attempts to .nd a surface S such that maxid(pi;S)Ifor a user-speci.ed error tolerance 
I. To achieve a given tolerance within our least squares optimization framework, it may be necessary 
to introduce new degrees of freedom into the surface representation. One could achieve this by globally 
subdividing the domain K2(e.g. using template 4 in Figure 5). However, this would introduce degrees of 
freedom uniformly over the whole surface, even if data points exceed the error tolerance only in isolated 
neighborhoods. We instead develop an adaptive re.nement scheme. The goal of this re.nement scheme is 
to subdivide any face of K2onto which any point pi projects with d(pi;S) >I, while at the same time ensuring 
that the resulting subdivided faces still form a valid patch network K2 +. We specify the re.nement of 
K2by selecting a subset E0CE of edges in K2. For each edge in E0, a new vertex is introduced at its midpoint. 
(The selection of E0will be discussed shortly.) We then subdivide each face of K2using one of the 4 face 
re.nement templates shown in Figure 5, depending on which of its edges are in E0 . Note that constraints 
exist on valid choices of E0, since the face re.nement templates can only be applied to faces with 0, 
2, or 4 re.ned edges. To satisfy these constraints, any chosen set E0 is augmented with additional edges 
so that all faces have an even number of re.ned edges. Our algorithm for achieving this closure is as 
follows. We place all faces of K2onto a stack. In each iteration, we remove the face at the front of 
the stack. If it has three re.ned edges, we add the fourth edge to E0and push the neighboring face on 
the stack. If instead it has one re.ned edge, we add to E0the next clockwise edge on the face and push 
the neighboring face on the stack. The algorithm is guaranteed to terminate, since, in the Figure 6: 
Example of closure of E0:on the left E0initially contains only one edge; on the right its computed closure 
contains 5 edges, resulting in the face re.nement indicated by the dashed edges.  worst case, E0will 
contain all edges of K2(which leads to global re.nement). Figure 6 demonstrates a re.nement obtained 
when a single edge is initially placed in E0 . We now address the problem of selecting the set E0that 
deter­mines the re.nement. Our algorithm considers all data points with d(pi;S) >Iin order of decreasing 
d(pi;S). For each of these data points, if the face onto which it projects is not set to be subdivided 
(i.e. none of its edges are in E0), then all its edges are added to E0 , and the closure of the resulting 
E0is computed. Having constructed the locally re.ned domain K2 +, we update the parametrizations of the 
points P. The new vertices intro­duced in K21), + lie either at the midpoints of edges (coordinates (0; 
2 (1; 12), ( 21 ;0), ( 12 ;1)), or at the centroid of faces (coordinates ( 21 ; 12 )). Reparametrization 
on faces created by face re.nement templates 1, 2, and 4 proceeds in the obvious way, since there exists 
a unique piecewise bilinear map between the original face and the quadrilat­eral subfaces. For a face 
subdivided by template 3, however, such a bilinear map does not exist on the two trapezoid pieces, so 
we approximate it by assuming that the original quadrilateral has the geometry of a square. After adaptive 
re.nement, the .tting method of Step 4 is rein­voked. The resulting surface may still not be within Iof 
all the points, indicating that further re.nement is necessary. We repeat the process of re.nement and 
re.tting until the error tolerance Iis satis.ed. Figures 9j 9l show the resulting surfaces.  4 RESULTS 
Figure 9 shows the reconstruction of a B-spline surface from a set of 4000 points; this synthetic data 
set was obtained by randomly sampling an existing surface. Figures 10a 10c, 10d 10f, and 10g 10i show 
reconstructions using real data obtained from a laser range scanner (courtesy of Technical Arts Co.). 
Figures 10j 10l show the B-spline approximation of a mesh S0 of 69,473 faces. To approximate S0,aset 
P of 30,000 points is sampled randomly from its surface. Step 1 of the procedure is skipped, and S0 is 
used directly as the initial mesh M0. Table 1: Parameter settings and execution times. Object #points 
Tolerance Fairness Execution times (minutes) N £ , Step 1 Step 2 Step 3 Step 4 Step 5 holes3 4,000 0.6% 
0.1 1 1 1 12 134 club 16,585 0.7% 0.1 6 1 1 11 599 foot 20,021 0.3% 0.05 7 13 1 12 228 skidoo 37,974 
0.7% 0.1 12 2 1 14 132 bunny 30,000 1.5% 0.1  16 1 45 200 As Table 1 indicates, the user-speci.ed parameters 
are the max­imum error tolerance Iand the fairness weight ).(To make these values unitless, we uniformly 
scale the data points P to .t within a unit cube.) The table also compares the execution times of the 
5 Table 2: Surface complexities and B-spline .t errors. Object M0 K4 Initial S Re.ned S #faces #faces 
#patches .t error #patches .t error rms max rms max holes3 2,080 98 49 0.14% 0.75% 178 0.07% 0.59% club 
5,152 72 35 0.22% 1.36% 285 0.06% 0.41% foot 10,972 62 29 0.20% 1.20% 156 0.03% 0.27% skidoo 3,661 30 
15 0.23% 3.00% 94 0.03% 0.69% bunny 69,473 162 72 0.43% 4.64% 153 0.19% 1.44% successive steps, as obtained 
on a 105 MHz HP 735 workstation. Table 2 lists for each example the complexities of the initial mesh 
M0 and the base complex K4. It also shows the .t errors of both the initial B-spline surface (Step 4) 
and the adaptively re.ned B­spline surface (Step 5), giving both rms and maximum errors as percentages 
of the object diameter. 5 SUMMARY AND FUTURE WORK We have developed a procedure for constructing a G1 
tensor prod­uct B-spline surface of arbitrary topological type from a set of 3D points without user assistance. 
The procedure makes use of a surface spline construction to obtain G1 continuity; we show that such an 
approach leads to an ef.cient B-spline .tting method. We have in­troduced an adaptive re.nement algorithm. 
Finally, we have applied our procedure to reconstruct B-spline surfaces within user-speci.ed maximum 
error tolerances on a number of real data sets. There exist a number of areas for future research. The 
pro­cedure should be extended to allow reconstruction of piecewise smooth surfaces that contain discontinuities 
such as creases and corners [11]. Currently our algorithm has dif.culty with such fea­tures, as it approximates 
them by adaptively re.ning the smooth surface numerous times (e.g. the club data set). Identifying these 
discontinuities as well as other characteristic lines on the surface may require some user intervention. 
Hopefully semi-automated segmentation methods can be developed that do not require com­plete speci.cation 
of patch boundaries. Such methods could replace Steps 2 and 3 of our procedure. In the context of surface 
approximation, the current procedure provides error bounds d(pi;S) between a set of sampled points and 
the approximating surface; instead a stronger error bound would be the distance d(S0;S) between the original 
surface and its approxi­mation. Some surfaces such as the mesh in Figure 10j contain .ne geo­metric detail 
that is dif.cult to approximate with a smooth surface representation. As demonstrated by [17], this detail 
can be stored conveniently in the form of a displacement map from the underlying smooth surface.  ACKNOWLEDGMENTS 
We wish to thank Ken Birdwell of Technical Arts Co. for the laser range data, and Greg Turk and Marc 
Levoy for the bunny mesh. REFERENCES [1] Andersson,E.,Andersson,R.,Boman,M.,Elm-roth,T.,Dahlberg,B.,andJohansson,B.Auto­matic 
construction of surfaces with prescribed shape. CAD 20 (1988), 317 324. [2] Bajaj,C.,Bernardini,F.,andXu,G.Automatic 
reconstruction of surfaces and scalar .elds from 3D scans. Computer Graphics (SIGGRAPH 95 Proceedings) 
(1995), 109 118. [3] Curless,B.,andLevoy,M.A volumetric method for building complex models from range 
images. Computer Graphics (SIGGRAPH 96 Proceedings) (1996). [4] Dietz,U.Erzeugung glatter Fl¨achen aus 
Meßpunkten. Tech­ nical Report 1717, Department of Mathematics, University of Darmstadt, Germany, February 
1995. [5] Doo,D.,andSabin,M.Behaviour of recursive division surfaces near extraordinary points. CAD 10, 
6 (September 1978), 356 360. [6] Eck,M.,DeRose,T.,Duchamp,T.,Hoppe,H., Lounsbery,M.,andStuetzle,W.Multiresolution 
analysis of arbitrary meshes. Computer Graphics (SIGGRAPH 95 Proceedings) (1995), 173 182. [7] Fang,L.,andGossard,D.Reconstruction 
of smooth parametric surfaces from unorganized data points. In Curves and Surfaces in Computer Vision 
and Graphics 3 (1992), J. Warren, Ed., vol. 1830, SPIE, pp. 226 236. [8] Farin,G.Curves and Surfaces 
for Computer Aided Geo­metric Design, 3rd ed. Academic Press, 1992. [9] Forsey,D.,andBartels,R.Surface 
.tting with hier­archical splines. ACM Transactions on Graphics 14, 2 (1995), 134 161. [10] Greiner,G.Variational 
design and fairing of spline sur­faces. Computer Graphics Forum 13, 3 (1994), 143 154. [11] Hoppe,H.,DeRose,T.,Duchamp,T.,Halstead, 
M.,Jin,H.,McDonald,J.,Schweitzer,J.,and Stuetzle,W.Piecewise smooth surface reconstruction. Computer 
Graphics (SIGGRAPH 94 Proceedings) (1994), 295 302. [12] Hoppe,H.,DeRose,T.,Duchamp,T.,McDonald, J.,andStuetzle,W.Surface 
reconstruction from un­organized points. Computer Graphics (SIGGRAPH 92 Pro­ceedings) 26, 2 (1992), 71 
78. [13] Hoppe,H.,DeRose,T.,Duchamp,T.,McDonald, J.,andStuetzle,W.Mesh optimization. Computer Graphics 
(SIGGRAPH 93 Proceedings) (1993), 19 26. [14] Hoschek,J.,andLasser,D.Fundamentals of Com­puter Aided 
Geometric Design. AK Peters, Wellesley, 1993. [15] Hoschek,J.,Schneider,F.-J.,andWassum,P.Op­timal approximate 
conversion of spline surfaces. CAGD 6 (1989), 293 306. [16] Imageware,Corp.Surfacer product information. 
http:­//www.iware.com/htmls/surfacer.html. [17] Krishnamurthy,V.,andLevoy,M.Fitting smooth surfaces to 
dense polygon meshes. Computer Graphics (SIG-GRAPH 96 Proceedings) (1996). [18] Lawler,E.L.Combinatorial 
optimization: networks and matroids. Holt, Rinehart, and Winston, 1976. [19] Lawson,C.,andHanson,R.Solving 
Least Squares Problems. Prentice-Hall, Englewood Cliffs, NJ, 1974. [20] Lee,S.,Tan,H.,andMajid,A.Smooth 
piecewise biquartic surfaces from quadrilateral control polyhedra with isolated n-sided faces. CAD 27 
(1995), 741 758. [21] Loop,C.Smooth spline surfaces over irregular meshes. Computer Graphics (SIGGRAPH 
94 Proceedings) (1994), 303 310. [22] Ma,W.,andKruth,J.Parametrization of randomly measured points for 
least squares .tting of B-spline curves and surfaces. CAD 27 (1995), 663 675. [23] Milroy,M.,Bradley,C.,Vickers,G.,andWeir, 
D.G1 continuity of B-spline surface patches in reverse engi­neering. CAD 27 (1995), 471 478. [24] Moore,D.,andWarren,J.Approximation 
of dense scattered data using algebraic surfaces. In Proc. of the 24th Annual Hawaii Intnl. Conf. on 
System Sciences, Kauai, HI, USA (1991), V. Milutinovic and D. Shriver, Eds., IEEE Comp. Soc. Press, pp. 
681 690. [25] Moreton,H.,SEC.Functional optimization andequin, for fair surface design. Computer Graphics 
26, 3 (July 1992), 167 176. [26] Nasri,A.H.Boundary-corner control in recursive­subdivision surfaces. 
CAD 23, 6 (1991), 405 410. [27] Peters,J.Constructing C1 surfaces of arbitrary topol­ogy using biquadratic 
and bicubic splines. In Designing Fair Curves and Surfaces, N. Sapidis, Ed. SIAM, 1994, pp. 277 293. 
 [28] Peters,J.Biquartic C1-surface splines over irregular meshes. CAD (1995). submitted. [29] Reif,U.Biquadratic 
G-spline surfaces. CAGD 12 (1995), 193 205. [30] Rogers,D.,andFog,N.Constrained B-spline curve and surface 
.tting. CAD 21 (1989), 641 648. [31] Sarkar,B.,andMenq,C.-H.Parameter optimization in approximating curves 
and surfaces to measurement data. CAGD 8 (1991), 267 290. [32] Schmitt,F.,Barsky,B.,andDu,W.H.An adap­tive 
subdivision method for surface .tting from sampled data. Computer Graphics (SIGGRAPH 86 Proceedings) 
20 (1986), 179 188. [33] Tarjan,R.Data structures and network algorithms. CBMS-NSF Regional Conference 
Series in Applied Mathematics 44 (1983). [34] Turk,G.,andLevoy,M.Zippered polygon meshes from range 
images. Computer Graphics (SIGGRAPH 94 Proceedings) 28, 3 (1994), 311 318. CCC 2221 20  b22 b21 b20 
C b10 10 b bb 02 0100 CCC 02 0100 Figure 7: Regular case: neighborhood of vertex C11 2Vx giving rise 
to a biquadratic patch. B C 3  C C 4 2B1,2 B 2,1 C m ABB 2,m 1,1 Figure 8: Extraordinary case: neighborhood 
of vertex C1 2Vx giving rise to a bicubic patch APPENDIX The purpose of this appendix is to present 
the formulas expressing the control points of the B´ ezier patches of S (Figure 3c) as af.ne combinations 
of the control mesh vertices Vx (Figure 3b) in Peters surface spline construction [27].1 Recall that 
a B´ ezier patch is associated with each vertex of Mx. Regular case Since Peters surface scheme generalizes 
bi­quadratic B-splines, in the regular case of a vertex C11 adjacent to four 4-sided faces (Figure 7), 
a biquadratic B´ ezier patch is created. The formulas for its B´ ezier points are obtained trivially: 
b00 =(C00 + C10 + C01 + C11)/4 b10 =(C11 + C10)/2 b01 =(C11 + C01)/2 b11 = C11 (The remaining B´ ezier 
points follow by symmetry.) Near extraordinary face At a vertex C1 near an m-sided ex­traordinary face 
(Figure 8), a bicubic B´ ezier patch is created. The formulas for its B´ ezier points are quite dif.cult 
and are derived in [27]: b00 =(B2;1+ B1;1+ C1+ A)/4 b10 =(5B2;1+ B1;1+5C1+ A)/12 b20 =(5B2;1+ B1;2+5C1+ 
C2)/12 b30 =(B2;1+ B1;2+ C1+ C2)/4 b11 =(5B2;1+5B1;1 +(25+4a)C1 +(1 -4a)A)/36 b21 =((5-10a)B2;1 +(1+2a)B1;2 
+(25+6a)C1 +(5+2a)C2)/36 b31 = h1;1 { P -im =1(-1)ih3;i if m is odd, b22= 2 m P -(-1)i (m -i) h3;i if 
m is even, mi=1b32 = h2;1 m 1 X b33 = Ci m i=1 where the following abbreviations are used: c =cos 
(27/m) a = c/(1 -c) h1;i = ((1-2a)B2;1 +(1-2a)B1;2 +(5+2a)C1 +(5+2a)C2)/12 m 1 X2a h2;i = Cl + cos (27l/m)(Ci+l 
+ Ci+l+1) m 3c l=1 22 h3;i =(1 -c) h2;i + ch1;i 33 (The remaining B´ ezier points again follow by symmetry.) 
Finally, in the case that the number of sides m of the extraordinary face is even and greater than 4 
the following linear condition must hold for G1 continuity: m 2 XX (-1)i+jBi;j =0 i=1 j=1 1Some minor 
mistakes in the original manuscript have been corrected. (a) Input: 4000 unorganized points P (b) Step 
1: Reconstructed mesh M0 (c) Step 2a: Voronoi partition of M0  (d) Step 2b: Delaunay triangulation 
of M0 (e) Step 2c: Triangular base complex K4(f) Step 3: Quadrilateral base complex K2 (j) Step 5: Adaptively 
re.ned K2(k) Step 5: Optimized control mesh Mx (l) Step 5: Final B-spline surface S  + Figure 9: Illustration 
of the B-spline surface reconstruction procedure. From the points P in (a), the procedure automatically 
creates the G1 B-spline surface in (l) which deviates from P by no more than 0.59% of the object s diameter. 
 (a) P (16,585 points) (b) 35 patches, 1.36% max error (c) 285 patches, 0.41% max error (d) P (20,021 
points) (e) 29 patches, 1.20% max error (f) 156 patches, 0.27% max error (g) P (37,974 points) (h) 15 
patches, 3.00% max error (i) 94 patches, 0.69% max error (j) S0 (69,473 faces) (k) 72 patches, 4.64% 
max error (l) 153 patches, 1.44% max error   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237272</article_id>
		<sort_key>335</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Reconstructing curved surfaces from specular reflection patterns using spline surface fitting of normals]]></title>
		<page_from>335</page_from>
		<page_to>342</page_to>
		<doi_number>10.1145/237170.237272</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237272</url>
		<keywords>
			<kw><![CDATA[corneal modeling]]></kw>
			<kw><![CDATA[normal fitting]]></kw>
			<kw><![CDATA[photogrammetry]]></kw>
			<kw><![CDATA[surface reconstruction]]></kw>
			<kw><![CDATA[videokeratography]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Spline and piecewise polynomial approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P190353</person_id>
				<author_profile_id><![CDATA[81100290289]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Halstead]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apple Computer, 1 Infinite Loop M/S 301-3J, Cupertino, CA and Computer Science Division, University of California, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P32082</person_id>
				<author_profile_id><![CDATA[81100387067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brain]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Barsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Division, University of California, Berkeley, CA and School of Optometry, University of California, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078856</person_id>
				<author_profile_id><![CDATA[81332508823]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stanley]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Klein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[School of Optometry, University of California, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P245685</person_id>
				<author_profile_id><![CDATA[81100599958]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Mandell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[School of Optometry, University of California, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALTSCHULER, M., ALTSCHULER, B., AND TABOADA, J. Laser electrooptic system for rapid three-dimensional (3-D) topographic mapping of surfaces. Optical Engineering 20, 6 (1981), 953-961.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARSKY, B. A., MANDELL, R. B., AND KLEIN, S. A. Corneal shape illusion in keratoconus. Invest Opthalmol Vis Sci 36 Suppl.:5308 (1995).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35072</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BARTELS, R. H., BEATTY, J. C., AND BARSKY, B. A. An Introduction to @lines for Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann, 1987.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BELIN, M. W., LITOFF, D., AND STRODS, S. J. The PAR technology corneal topography system. Refract Corneal Surg 8 (1992), 88- 96.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>30394</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BLAKE, A., AND ZISSERMAN, A. Visual Reconstruction. MIT Press, 1987.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BOLLE, R., AND VEMURI, B. On three-dimensional surface reconstruction methods. IEEE Trans. PAMI 11, 8, 840-858.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>2052624</ref_obj_id>
				<ref_obj_pid>2052538</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BRINKLEY, J. Knowledge-driven ultrasonic three-dimensional organ modeling. IEEE Trans. PAMI 7, 4, 431-441.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CHENG, F., AND BARSKY, B. A. Interproximation: Interpolation and approximation using cubic spline curves. Computer-Aided Design 23, 10 (1991), 700-706.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[COHEN, E., LYCHE, T., AND RIESENFELD, R. Discrete B-splines and subdivision techniques in computer aided geometric design and computer graphics. Computer Graphics and Image Processing 14 (1980), 87-111.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Doss, J. D., HUTSON, R. L., ROWSEY, J. J., AND BROWN, D. R. Method for calculation of corneal profile and power distribution. Arch Ophthalmol 99 (1981), 1261-5.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FAVARDIN, C. Determination automatique de structures geometriques destinees a la reconstruction de courbes et de surfaces a partir de donnees ponctuelles. PhD thesis, Universite Paul Sabatier, Toulouse, France, 1993.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GOSHTASBY, A. Surface reconstruction from scattered measuremeAts. SPIN 1830 (1992), 247-256.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., HALSTEAD, M., JIN, H., MCDONALD, J., SCHWEITZER, J., AND W., S. Piecewise smooth surface reconstruction. Computer Graphics (SIGGRAPH '94 Proceedings) (July 1994), 295-302.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[JARVIS. A perspective on range finding techniques for computer vision. IEEE Trans. PAMI 5, 2 (1983), 122-139.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KLYCE, S.D. Computer-assisted corneal topography, highresolution graphic presentation and analysis of keratoscopy. Invest Ophthalmol Vis Sci 25 (1984), 1426-35.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[KOCH, D. D., FOULKS, G. N., AND MORAN, T. The corneal eyesys system: accuracy, analysis and reproducibility of first generation prototype. Refract Corneal Surg 5 (1989), 424-9.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[KRACHMER, J. H., FEDER, R. S., AND BELIN, M. W. Keratoconus and related noninflammatory corneal thinning disorders. Surv. Ophthalmol 28, 4 (1984), 293-322.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MAGUIRE, L. J., AND BOURNE, W. D. Corneal topography of early keratoconus. Am J Ophthalmol 108 (1989), 107-12.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MAMMONE, R. J., GERSTEN, M., GORMLEY, D. J., KOPLIN, R. S., AND LUBKIN, V. L. 3-D corneal modeling system. IEEE Trans Biomedical Eng 37 (1990), 66-73.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MOORE, D., AND WARREN, J. Approximation of dense scattered data using algebraic surfaces. Tech. rep., TR 90-135, Rice University, 1990.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37420</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PRATT, V. Direct least-squares fitting of algebraic surfaces. SIC- GRAPH '87 Conference Proceedings (1987), 145-152.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[SATO, Y., KITAGAWA, H., AND FUJITA, H. Shape measurement of curved objects using multiple slit-ray projections. IEEE Trans. PAMI g, 6 (1982), 641-649.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15906</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SCHMITT, F., BARSKY, B. n., AND DU, W.-H. An adaptive subdivision method for surface fitting from sampled data. SIGGRAPH '86 Conference Proceedings (1986), 179-188.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[TAUBIN, G. An improved algorithm for algebraic curve and surface fitting. In Proc. gth International Conf. on Comp. Vision, Berlin (1993), pp. 658-665.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5994</ref_obj_id>
				<ref_obj_pid>5979</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[TERZOPOLOUS, D. Regularization of inverse visual problems involving discontinuities. IEEE Trans. PAMI 8 (1986), 413-424.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>16527</ref_obj_id>
				<ref_obj_pid>16520</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[TOPA, L., AND SCHALKOFF, R. An analytical approach to the determination of planar surface orientation using active-passive image pairs. Computer Vision, Graphics, and Image Processing 35 (1994), 404-418.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[TURK, G., AND LEVOY, M. Zippered polygon meshes from range images. Computer Graphics (SIGGRAPH '9g Proceedings) (1994), 311-318.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[VaN SaaRLOOS, P. P., aND CONSTABLE, I. Improved method for calculation of corneal topography for any photokeratoscope geometry. Optom Vis Sci 68 (1991), 960-6.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[WANG, J., RICE, D. A., AND KLYCE, S. D. A new reconstruction algorithm for improvement of corneal topographical analysis. Refract. Corneal Surg 5 (1989), 379-387.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WARNICKI, J. W., REHKOPF, P. G., AND CURTIN, S. n. Corneal topography using computer analyzed rasterographic images. Am. J. Opt 27 (1988), 1125-1140.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[WILSON, S. E., AND KLYCE, S. D. Advances in the analysis of corneal topography. Surv. Ophthalmol. 35 (1991), 269-277.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Reconstructing Curved Surfaces From Specular Re.ection Patterns Using Spline Surface Fitting of Normals 
 § § § Mark A. Halstead Brian A. Barsky Stanley A. Klein Robert B. Mandell University of California at 
Berkeley ABSTRACT We present an algorithm that reconstructs a three­dimensional surface model from an 
image. The image is generated by illuminating a specularly re.ective surface with a pattern of light. 
We discuss the application of this algo­rithm to an important problem in biomedicine, namely the measurement 
of the human cornea, although the algorithm is also applicable elsewhere. The distinction between this 
re­construction technique and more traditional techniques that use light patterns is that the image is 
formed by specular re­.ection. Therefore, the reconstruction algorithm .ts a sur­face to a set of normals 
rather than to a set of positions. Furthermore, the normals do not have prescribed surface positions. 
We show that small surface details can be recov­ered more accurately using this approach. The results 
of the algorithm are used in an interactive visualization of the cornea. CR Categories: I.3.5 [Computer 
Graphics]: Computa­tional Geometry and Object Modeling; J.6 [Computer-Aided Engineering]: Computer-Aided 
Design. Keywords: Surface reconstruction, videokeratography, corneal modeling, photogrammetry, normal 
.tting. 1 INTRODUCTION A problem of particular interest to the computer graphics community is how to 
construct accurate computer mod­els of existing objects. One use of these reverse-engineered models is 
to populate rendered scenes with realistic objects. Another important use is for computer-aided visualization, 
where the models are displayed in ways that expose informa­tion for analysis. An example is medical imaging, 
in which models of a patient s anatomy are .rst extracted from mea­surements made by a variety of scanners 
(for example MRI or ultrasound [7]). The models are then displayed in vari­ Apple Computer, 1 In.nite 
Loop M/S 301-3J, Cupertino, CA 95014. Computer Science Division, University of California, Berke­ley, 
CA 94720-1776. www.cs.berkeley.edu/projects/optical/ § School of Optometry, University of California, 
Berkeley, CA 94720-2020. Permission to make digital or hard copies of part or all of this work or personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage and that copies bear this notice and the full citation on the first page. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 ous forms for diagnosis. The success of 
this approach has made medical imaging one of the more important applica­tions of computer graphics and 
modeling. In addition to displaying the reconstructed models, they can be used to provide a valuable 
starting point for other operations. For example, in the area of computer-aided design (CAD), an architect 
can input a model of an existing building, then design and visualize modi.cations by editing the existing 
structure. Reverse-engineering is also studied in disciplines other than computer graphics and modeling. 
For example, in the machine vision community, it appears under the guise of automatic object recognition. 
This paper presents a solution to an important reverse­engineering problem in biomedicine: constructing 
a com­puter model of the human cornea. The cornea is the outer layer of the eye, and plays the primary 
role in focusing im­ages on the retina. The algorithm that we have devised to construct a model of the 
cornea is of interest to the graphics community for the following reasons: We derive the surface model 
by applying backward ray­tracing to simulate re.ection at a specular surface and the resulting virtual 
image.  We use a variation on the standard B-spline surface representation to increase the e.ciency 
of the backward ray-tracing by at least an order of magnitude.  We solve a problem of .tting a surface 
to a set of nor­mals at unprescribed locations.  The algorithm we present here has signi.cantly ad­vanced 
the frontier of corneal modeling and visualiza­tion.  Building a model of a physical object usually 
proceeds in two stages: data capture, and construction of the model from the data. Data capture takes 
many forms. For a survey of techniques, the reader is referred to [6, 14]. A common technique uses correspondences 
between two or more images of an object taken from di.erent positions [5]. Stereopsis or depth disparity 
allows the recovery of an approximate depth map for the object. Depth is also recovered by a di.erent 
class of techniques that use structured light. In this ap­proach, the object is illuminated with a pattern 
to form an image. The geometric relationship between the light source, the object, and the image recording 
device is su.cient to determine depth. In some cases, estimates of surface orien­tation are also provided. 
Examples include laser range.nd­ers, slit-ray or grid projectors, and Moir´e pattern generators [1, 22, 
26]. In most cases, the data is returned as samples of surface position. A model is built by .tting the 
captured positional data with a surface [8, 11, 12, 20, 21, 23]. Problems faced at this stage include 
surface discontinuities, and noisy or missing data. Terzopoulos [25] presents a robust algorithm Cornea 
 Figure 1: Cross-section of the eye. which handles these problems using a multigrid approach. In [24], 
the model is built from implicit algebraic surfaces. The problem of joining positional data taken from 
di.er­ent measurements is addressed in [27]. Hoppe [13] discusses an algorithm for recovering surfaces 
with complex topology from range data. The important distinction between these .tting problems and the 
one presented in this paper is that we measure sur­face orientation rather than surface position. This 
presents a number of interesting challenges and leads to a new ap­proach to surface .tting. The problem 
di.ers from those faced by the vision community (such as shape-from-shading [5]) in that we rely on the 
specular re.ection of illumina­tion at the surface in order to reconstruct very small surface features. 
As shown in Figure 1, the cornea is the transparent tissue that forms the foremost surface of the eye. 
Refraction at the air/cornea interface accounts for approximately three­quarters of the focusing power 
of the eye. Consequently, the exact shape of the cornea is critical to visual acuity. Even subtle deviations 
of corneal shape can have a signi.cant im­pact on vision. Measurements must be at the level of submi­crons 
to be meaningful. It is the high precision needed for the measurements that leads us to measure corneal 
orientation rather than position, and that makes surface reconstruction a challenging problem. Vision 
problems arise when the cornea is asymmetric or is too peaked or .at to focus light uniformly on the 
retina. Eyeglasses and contact lenses address the problem by pre­refracting the light so that the overall 
e.ect of the correc­tive lens plus cornea is more uniform. Recent surgical tech­niques change the degree 
of refraction by adjusting the shape of the cornea directly. Radial keratotomy (RK) and astig­matic keratotomy 
(AK) reduce the curvature of the cornea by weakening the structure with radial cuts; photorefractive 
keratectomy (PRK) and laser in-situ keratomileusis (LASIK) achieve a similar result by ablating corneal 
material with an excimer laser. All these procedures bene.t from having accurate models of corneal shape. 
Until recently, however, models used in the optometric community have assumed that the cornea is spherical, 
ellipsoidal, toric (like a section cut from a torus), or has radial symmetry. On a gross scale, most 
corneas do have these smooth shapes. However, we wish to measure, and model, deviations from these simple 
shapes that are on the order of microns and that contribute signi.cantly to the refractive power of the 
surface. Ideally, we would like to have an algorithm that, in clinical practice, can scan a patient s 
cornea and display usable re­sults almost immediately. We have developed an interactive program that 
presents a good approximation to the shape of a patient s cornea within seconds, and then continues to 
re.ne the display until micron accuracy is achieved. 2 MEASURING SURFACE SHAPE Depth from binocular disparity 
does not work well for mea­suring corneal shape, since the surface has no distinctive variation in texture. 
This makes it di.cult to identify corre­spondences between multiple images. Therefore, we consider measurement 
techniques in which the surface is illuminated by a pattern of light. The resulting image is recorded 
by a scanner such as a CCD array. There are two ways to ap­ply this technique, which we will refer to 
as approach A and approach B. Applications of approach A are more typical. Often re­ferred to as raster 
photogrammetry , they require a pattern of one or more lines to be projected along a known direction 
onto a di.use surface [4, 27, 30]. The projected pattern acts as a secondary di.use source, which is 
viewed by the scan­ner along a direction o.-axis to the direction of projection. The observed distortion 
of the lines indicates the variation in surface distance from the projector, and fully determines the 
position of a set of sample points. This approach is used, for example, by the Cyberware laser scanner 
[27]. Lately, in­terest has focused on .tting the data points with surface models, and combining the 
results from multiple scans [27]. The approach taken by applications of type Bis for a spec­ular surface, 
rather than di.use, near a di.usely emitting source pattern. The scanner captures the virtual image of 
the pattern caused by specular re.ection at the surface. As with approach A, the shape of the surface 
a.ects the scanned image. However, the relationship between the shape and the image is harder to de.ne, 
as it now depends on surface ori­entation as well as position. One advantage of approach B is that very 
small devia­tions in surface orientation cause large changes in the im­age. There is no such magni.cation 
possible in approach A a change in orientation has no e.ect on the scanned image, and a change in position 
causes changes only of the same magnitude. The accuracy is therefore limited by the resolution of the 
scanning device. We believe that, with a good reconstruction algorithm, approach B allows submi­cron 
level detail to be recovered. This paper describes such a reconstruction algorithm. 2.1 Videokeratography 
Fortunately, the cornea in its normal state is covered by a thin layer of tears and presents a specular 
surface. This means that we can conveniently image the surface us­ing approach B. Over the last few years, 
simple observa­tion devices based on this approach and used in the op­tometric community have evolved 
into the videokeratograph [15, 16, 19, 29, 31]. This instrument contains a video camera to capture a 
digital image which is analyzed by an on-board computer. Although there are variations among systems, 
a standard arrangement is to have the source pattern painted on the inner surface of a cone. The cone 
has a hole in its apex through which a system of lenses and a CCD array capture the re.ected image (Figure 
2). The most common source pattern is alternating black and white concentric rings. Re­cently, however, 
we have been experimenting with a proto­type source pattern, not commercially available, that resem­bles 
a dartboard. We show later how such a pattern allows a more accurate reconstruction. In preparation for 
measurement, the patient s line of sight is aligned with the axis of the cone and the image is brought 
into focus. After capturing an image (Figure 3), the video­keratograph performs a processing step to 
locate prominent  Source pattern Simulated cornea Figure 2: Videokeratograph. Figure 4: Simulation 
using ray-tracing. respond to features in the source. For example, a sample point taken from an edge 
in the image is the re.ection of some point on an edge in the source. If we trace a ray from the sample 
point fi on the image to the surface (through the nodal point) we can identify a ray that goes from the 
surface to the correct point on the source (that is, some point on the corresponding source feature). 
In Figure 4, this second ray is shown as a dashed line. These two rays the inci­dent ray ii and the 
modi.ed re.ected ray r * i , where the hat indicates normalization de.ne a modi.ed surface normal Figure 
3: Captured videokeratograph image. vector: r * i - ii r * image features. The nature of these features 
varies between n = i . * - ii source patterns. For both the concentric ring and the dart­board sources, 
the feature set includes the edges between regions of black and white in the image. These edges are located 
with standard image processing techniques and are discretely sampled. For the dartboard source, the crossings 
created by the junction of four black and white patches form i The modi.ed re.ected ray and the modi.ed 
normal vector may di.er from the actual re.ected vector ri and the actual normal vector n i. In this 
case, we attempt to adjust the surface so that it interpolates the modi.ed normal. an additional feature 
set. The aim of our reconstruction al­gorithm is to generate a model of the corneal surface from the 
image feature positions and the geometry of the video­keratograph. Previous algorithms have failed to 
recover con­tinuous, accurate surface models [4, 10, 15, 16, 28, 30]. Our algorithm satis.es these goals. 
3 RECONSTRUCTION ALGORITHM The reconstruction algorithm inputs a set of feature samples from a videokeratograph 
image. The goal is to .nd a surface that, if placed in the videokeratograph, would create the same image. 
With certain assumptions and constraints, we claim that this surface is a good .t to the original cornea. 
In order to .nd the surface, we use a simulation of the videokeratograph. As explained in Section 2.1, 
light from the source pattern is re.ected at the corneal surface and gathered by a system of lenses to 
form the videokeratograph image. We simulate this process by backward ray-tracing, as illustrated in 
Figure 4. In the simulation, the system 1 of lenses is replaced by its equivalent nodal point , which 
becomes the center of projection. The video CCD array becomes the image plane, with which we associate 
the coor­dinate system ( u,v . ) In Section 2.1, we noted that image processing techniques are used to 
extract features such as edges and crossings from the videokeratograph image. The positions of these 
features in ( u,v ) are sampled to form the set {fi : i =1 ...k }. Our algorithm relies on the fact that 
features in the image cor­ 1For our purposes, the action of a camera with one or more lenses is adequately 
modeled by a pinhole camera positioned at the nodal point We determine a modi.ed normal vector for each 
image sample and .t the set of normals with a new surface. Be­cause the backward rays intersect the new 
surface at new positions, the set of modi.ed normals we just .t is now in­correct, so we must recompute 
them and repeat the .tting step. This leads to an iterative process which we initialize by taking a guess 
at the shape of the cornea. Each iter­ation consists of a normal .tting phase and a re.nement phase. 
The normal .tting phase determines the set of mod­i.ed normals using the current surface and .ts them 
with a new surface. The re.nement phase adds degrees of freedom to the surface model as needed for a 
more accurate .t. 3.1 The Normal Fitting Phase For each sample fi, we determine the modi.ed normal n 
* i using the current surface. We de.ne the range of fi to be the set of points in the source pattern 
that could be imaged to fi. This set may form a curve, such as the entire source edge in the above example, 
or it may contain a single point. The latter case arises when the dartboard pattern is used, because 
crossings can be located exactly in the source. Using backward ray-tracing from fi and the current sur­face, 
we .nd where the re.ected ray ri intersects the source pattern. We then determine which point in the 
range lies nearest the intersection. The ray from the surface to this point is r * i . The set {n * : 
i =1 ...k} is viewed as a set of normal vec­ i tor constraints which, together with a positional constraint 
discussed in the next section, is .t with a new surface S * . The details are given in Section 3.4 after 
we have described the surface representation scheme. 3.2 Convergence and Uniqueness of Solution The 
search process clearly must be iterative, since each nor­mal .tting phase changes not only the normal 
vectors of the surface but the position, which causes the rays from the im­age plane to intersect at 
new locations. We have no formal proof that our search algorithm will always converge, but in all trial 
runs we have observed rapid and stable convergence to a good solution. For a given set of features, there 
may be zero, one, or many surfaces that generate matching images. The result depends on the distribution 
of image features and the representation chosen for the surface model. Fortunately, we are working in 
a restricted problem domain, in which we can assume that the cornea has a smooth, regular shape. This 
assumption is based on the fact that the cornea is a pliable tissue subject to internal pressure. Based 
on this assumption, we formu­late the search process to favor smoothly varying surfaces. With the further 
assumption that the number of degrees of freedom in the surface model is related to its smoothness, we 
begin the search with a surface of few degrees of freedom, and incrementally add degrees of freedom until 
a satisfactory solution is found. Although this algorithm may not be a rigorous statement of our goal, 
it has given more than satisfactory results in practice. Note that we use far more features than degrees 
of freedom, so that the search for a match is overconstrained. Section 3.5 will discuss e.cient methods 
for adding de­grees of freedom to the model using re.nement. To further reduce the number of possible 
solutions, we im­pose one or more interpolation constraints. The constraint that we use most often is 
to .x the position of the apex of the cornea. Some videokeratograph systems have attach­ments that can 
directly measure this position. If the infor­mation is not directly available, it can be estimated from 
the image and the known focal length of the camera lens. The algorithm then generates an initial surface 
that interpolates this point, and maintains the interpolation constraint for the remainder of the search. 
 3.3 Surface Representation The representation of S is carefully chosen to allow the e.­cient execution 
of the normal .tting phase. At .rst glance, conventional CAGD wisdom would suggest the use of a para­metric 
polynomial patch scheme such as tensor product B-splines with control points in IR 3 to de.ne the posi­tion 
of S in ( x,y,z ). One drawback of this scheme is that numerically expensive algorithms such as root 
.nding are re­quired to compute the ray/surface intersections during the backward ray-tracing stage. 
In contrast, we use a representation scheme that allows the point of intersection of a ray to be determined 
simply by taking a linear combination of scalar control points. Further­more, the coe.cients of the linear 
combination remain con­stant throughout the search, and are computed only once, at initialization. Figure 
5 illustrates how the scheme works. By suitably scaling the feature positions fi, we can assume that 
the image plane lies at z = -1 and that u=x,v =y. We de.ne D u,v ) ( to be the z-coordinate of the point 
of intersection between the surface and the ray that originates at ( u,v ) and passes through the nodal 
point. The coordinates of this point are x -uD u,v ) (1) =( y -vD u,v ) (2) =( z = D u,v . ( ) (3) Figure 
5: Surface representation scheme. These equations de.ne the surface S in ( x,y,z ), parameter­ized by 
image plane coordinates ( u,v ). Furthermore, the representation is ideal for the backward ray-tracing 
task, since, by de.nition, the equations directly give the inter­section of each ray with S. In the current 
implementation, Du,v ( ) is represented by a set of biquintic tensor product B-spline patches, with uni­form 
knot spacing and no boundary conditions [3]. A surface consisting of ( -×- ( 5) patches is de.ned by 
an m×n m 5) n array of control points {P11, ,...,Pm,n}in IR 1 . From the de.nition of ( x,y,z ), we .nd 
that the x-coordinate patches of S are degree 6 polynomials in u and degree 5 in v; the y-coordinate 
patches are degree 5 in u and degree 6 in v; and the z-coordinate patches are degree 5 in both u and 
v. The surface S can be rendered using standard techniques for polynomial patch surfaces. We have chosen 
to use biquintic patches rather than a lower degree representation because of the high degree of smoothness 
exhibited by the cornea. Furthermore, one of our scienti.c visualization tasks is to display curvature 
maps of the surface. Since the formula for curvature involves sec­ond partial derivatives of surface 
position, a bicubic repre­sentation allows cusps in curvature, whereas a biquintic for­mulation provides 
for smoother joins at patch boundaries. Since the feature positions {fi : i =1 ...k}are de.ned by the 
input image, their ( u,v ) coordinates, which are used as ray origins for backward ray-tracing, remain 
.xed through­out the search. For .xed ( u,v ), the functions x,y,z given by (1) (3) are linear combinations 
of the control points {P11,...,P m,n}. This is a consequence of the B-spline func­ , ( ) being linear 
in {P11, m,n}for .xed ( u,v tion D u,v ,...,P ). For each of the k features, we can evaluate the basis 
func­tions of Du,v ( ) at the feature s coordinate and multiply by -u (for x) or -v (for y) to .nd the 
coe.cients that multi­ply each of the control points. Collecting these coe.cients together in matrix 
form, we can write x = Mx[P11, ,...,P m,n]T y = My[P11, ,...,P m,n]T z = Mz[P11, ,...,P m,n]T , where 
M,M y x and Mz are k ×mn matrices that are pre­computed and stored using a sparse matrix representation 
for e.cient evaluation. Similarly, we derive matrices Mxu and Mxv such that .x/.u = Mxu[P11, ,...,P m,n]T 
and .x/.v = v T uvu P11,...,P m,n] . M,M,M ,Mx [ , Together with matrices yyz and Mzv for y and z, these 
are used to .nd expressions for the surface tangent vectors in the u and v directions: tu = .u,v /.u 
S() tv = .u,v /.v S() S u,v xu,v,yu,v,zu,v . where ( )=[( )( )( )] The surface normal required for computing 
the re.ected ray during ray-tracing is given by tu ×tv . 3.4 Solving the Normal Fitting Problem In Section 
3.1, the normal .tting problem was de.ned as .nding the surface S * that .ts the prescribed surface nor­mals 
{n * : i =1 ...k } subject to one or more interpolation i constraints. Let ( x,y,z ) be a point interpolated 
by the surface. Us­ing (1) (3), we .nd that the constraint is satis.ed if ccc D(-x /z , -y /z )= z. cccc 
c Using the same technique employed to construct the ma­trix Mz in the previous section, we determine 
coe.cients {c , ,...,c } such that 11 m,n (-x /z , c -y /z cc)=[ , ,...,cm,n P11 T = c D c c11 ][ , ,...,P 
m,n] z. This constraint equation is linear in the control points, and some number, say s, of interpolation 
constraints are ex­pressed in matrix form as CP 11,...,P m,n]T =[ 1 ]T , [ , z ,...,z s where C is an 
s× mn matrix. Let N be the mn × t matrix whose columns span the null space of C. Given an input surface 
S with control points {P11,...,P m,n} that satis.es the constraints, all other sur­faces , S * that satisfy 
the constraints, and hence are possible solutions to the normal .tting problem, have control points given 
by ** T TT [P11, ,...,Pm,n] =[P11,...,P m,n]+[ ] . N Q ,...,Q (4) , 1 t Now consider a feature fi with 
image plane coordinates ii) The modi.ed surface normal computed for this fea­ (u,v . * *** * ture is 
n =( n,n ,n ). We can require S to have a scalar multiple of this normal by using the pair of constraints 
i i,x i,y i,z * nt. |=0, uu,v i ii and nt. | ,v * vu =0. i ii Using the matrices from the previous section, 
and de.ning matrix with 1 j on diag a ,...,a ( 1 j ) to be the j × j a ,...,a the diagonal and zeroes 
elsewhere, we write the pairs of con­straints for all features as ** u ** u diag n 1,x,...,n k,x )Mx 
+ diag n 1,y ,...,nk,y )My + (( ** u ** T diag n 1,z,...,n k,z )Mz P11 ,...,P ] =0 ([ , m,n ** v ** v 
diag n 1,x,...,nk,x )Mx + diag n 1,y ,...,nk,y )My + (( ** v ** T diag n 1,z,...,nk,z )Mz P11 ,...,P 
] =0 ([ , m,n These normal constraint equations are combined to form the simpler expression ** T MP [ 
11,...,Pm,n] =0. (5) , Substituting (4) into (5), we arrive at a system of linear equa­1 t]: tions which 
is solved in a least squares sense for [ Q ,...,Q MN Q ,...,Q T [ 11 T [ 1 t]= -MP , ,...,Pm,n] . (6) 
** The .nal control points [ P11, ,...,Pm,n] of the new surface S * are determined by substitution of 
[ Q ,...,Q 1 t] into (4). The kt× matrix MN in (6) has the same number of rows as there are image features, 
and the number of columns is no more than the number of surface control points minus one. To ensure a 
sensible .t to the data, we overconstrain the system by using as many as 20 to 30 times as many features 
as control points. This is an attempt to smooth out errors in the feature location process. The least 
squares solution to the resulting rectangular system is found with standard numerical algorithms. In 
the current implementation, the sparsity of MN is not used to advantage.  3.5 Re.nement The goal of 
our interactive system is to rapidly display an initial, approximate solution which is then improved 
incre­mentally. Therefore, the initial search iterations should exe­cute quickly at the expense of accuracy. 
Since each iteration is dominated by the solution of (6), this goal is satis.ed by performing the initial 
iterations with a small number of control points and features. Accuracy is improved at the ex­pense of 
iteration time by increasing the number of control points and features, which is consistent with the 
algorithm discussed in Section 3.2 in regard to uniqueness of solution. In our current implementation, 
which uses biquintic B­spline patches with uniform knot spacing, the number of control points is increased 
by subdividing each patch into four subpatches. A new knot is inserted at the midpoint of each knot interval 
in both u and v. The new control points are derived using simple linear combinations of the old, with 
coe.cients given by an application of the Oslo algorithm [3, 9]. The search begins with a single patch 
model of the surface. Re.nement is performed when the mean change in angle between normals at successive 
iterations falls below a given threshold. The model moves through representations using ,, ) patches 
until the maximum change in normal (4 16 64 ,... falls below a predetermined threshold at a predetermined 
level of re.nement. A typical feature set contains over 5000 samples. For low subdivision levels, this 
is more than we need, so a subset of features is used. This subset is chosen so that the image is uniformly 
sampled. 4 RESULTS To test the algorithm, we need to run it on surfaces whose shape is accurately known. 
Unfortunately, it is di.cult to manufacture interesting test cases. For this reason, we have tested the 
algorithm on both data collected from real ob­jects and data generated synthetically. The synthetic data 
is generated automatically from various surface de.nitions by a software simulation of the videokeratograph. 
Figures 9a-10d show some frames illustrating the progress of the algorithm. The results of the search 
process are dis­played to the user after each iteration. The algorithm is formulated so that a good approximation 
to the .nal answer is reached in a few seconds, so the user can start analyzing the results immediately. 
A more accurate picture evolves over the next few minutes. In Figure 9, there are four frames showing 
the patches converging to a solution. The input data is a simple ellip­soid with axis radii of 8mm, 9mm, 
and 10mm. The im­age data for this example was synthetically generated and is shown in Figure 6. For 
illustration purposes, we show the exact ellipsoid (lower surface) plus the current solution o.set above. 
However, since the di.erence in shape is so small, we magnify this di.erence by a factor of 20 in order 
to better visualize the progress of the algorithm. The sur­face colors indicate the logarithm (base 10) 
of the distance in millimeters between the current solution and the exact ellip­soid. In the .nal frame, 
we can see that good convergence has been achieved. We measure the error as the distance in z between 
the two surfaces, computed at a large number of sample points in the x, y plane. The RMS error for this 
example was 9 2 -6mm, which is 0.0092 microns. This . × 10 extremely high accuracy is typical of all 
synthetic data sets we have tried. Figure 10 shows frames from another run of the algorithm. The input 
data in this example is again synthetically gener­ated, and is shown in Figure 7. The aim here is to 
simulate keratoconus , which is a condition in which the cornea has a local region of high curvature 
[2, 17, 18]. The surface is gen­erated from a sphere with a rotationally symmetric bump grafted onto 
it. The bump and the sphere meet with curva­ture continuity. The curvature at the peak of the bump is 
signi.cantly greater than the curvature of the sphere. This situation has not been handled very well 
by existing algo­rithms. Our algorithm, however, has no di.culty in .nding an accurate solution. Note 
that the bump rises only approx­imately 20 microns above the sphere. This is a deviation of about 0.2 
percent of the radius of the sphere. However, the bump causes large deviations in the image rings (Figure 
7), demonstrating that we can record smaller deviations using an image formed by specular re.ection than 
one formed by di.use re.ection. Rather than color encode the distance between the current solution and 
the actual surface, in Figure 10 we have color encoded the separation between the current surface and 
a sphere whose radius is the same as that of the input test surface. This form of rendering illustrates 
one of our visual­ization techniques, which is to display the surface separation from a best-.tting ellipsoid. 
This enhances the deviations so that the bump, which is positionally very close to the sphere, becomes 
noticeable. In this example, we get extremely high positional accuracy of 0.013 microns. Figure 8 illustrates 
the results of the algorithm run on real data taken from a cornea. In this case, we cannot report accuracy 
information because the true shape is unknown. Nonetheless, we can render it with our in-house scienti.c 
visualization software package. Figure 8 shows the surface with pseudo-color representing Gaussian curvature 
(and the height information in the image is simply the true height of the 3-D surface). The red area 
on the left depicts a local area of high Gaussian curvature. The vectors correspond to the direction 
of minimum curvature at each point on the surface. This image demonstrates how e.ective the use of curvature 
can be in conveying subtle changes in shape. We have run the algorithm on real data measured from physical 
ellipsoids of known radii. In these runs, the .nal accuracy lies in a range of 0.9-1.5 microns of mean 
error in z. This is still extremely accurate, but it is signi.cantly larger than the error in the synthetic 
runs. We conclude that the error is introduced, not by the algorithm, but by the feature extraction algorithm 
and in the measurements we have made of the physical videokeratograph geometry (such as distance between 
rings, etc.). We are currently addressing these issues. In all these runs, the .nal surface consists 
of 8 ×8 patches. This gives adequate accuracy, although there is no reason why we cannot go to the next 
level of 16 ×16 patches. Beyond that, we reach the limits of the feature sampling process. In the source 
patterns we currently use, the features are not uniformly spread across the image but are concentrated 
along boundaries between areas of black and white rings. This limits how small the patches can be, because 
if a patch falls between feature clusters it will be unconstrained (except by continuity between adjacent 
patches). 5 CONCLUSION We have presented an algorithm that reconstructs the shape of the cornea from 
a single videokeratograph image. The al­gorithm is interesting because it .ts a surface to a set of nor­mals 
rather than to a set of positions. Furthermore, the nor­mals are not associated with spatial positions 
as in standard normal .tting problems. This distinguishes it from more typical surface reconstruction 
problems. The normal .tting is necessary because the surface imaging technique uses re­.ection from a 
specular surface. This improves its ability to detect small variations in surface position because surface 
orientation is a more sensitive indicator of shape variations than is surface position. This technique 
can be applied to objects other than the human cornea, and any applications that require high accuracy 
would be candidates. However, we have made some assumptions about the surface that al­low us to proceed 
with little direct information about the corneal position. For example, we only require a single posi­tional 
constraint. These assumptions are valid in the case of corneas. For other more general objects where 
these assump­tions could not be made, more positional measurements may be needed to provide additional 
constraints. REFERENCES [1] Altschuler, M., Altschuler, B., and Taboada, J. Laser electro­optic system 
for rapid three-dimensional (3-D) topographic map­ping of surfaces. Optical Engineering 20 , 6 (1981), 
953 961. [2] Barsky, B. A., Mandell, R. B., and Klein, S. A. Corneal shape il­lusion in keratoconus. 
Invest Opthalmol Vis Sci 36 Suppl.:5308 (1995). [3] Bartels, R. H., Beatty, J. C., and Barsky, B. A. 
An Introduc­tion to Splines for Use in Computer Graphics and Geometric Modeling . Morgan Kaufmann, 1987. 
[4] Belin, M. W., Litoff, D., and Strods, S. J. The PAR technology corneal topography system. Refract 
Corneal Surg 8 (1992), 88 96. [5] Blake, A., and Zisserman, A. Visual Reconstruction . MIT Press, 1987. 
[6] Bolle, R., and Vemuri, B. On three-dimensional surface recon­struction methods. IEEE Trans. PAMI 
11 , 8, 840 858. [7] Brinkley, J. Knowledge-driven ultrasonic three-dimensional or­gan modeling. IEEE 
Trans. PAMI 7 , 4, 431 441. [8] Cheng, F., and Barsky, B. A. Interproximation: Interpolation and approximation 
using cubic spline curves. Computer-Aided Design 23 , 10 (1991), 700 706. [9] Cohen, E., Lyche, T., and 
Riesenfeld, R. Discrete B-splines and subdivision techniques in computer aided geometric design and computer 
graphics. Computer Graphics and Image Processing 14 (1980), 87 111. [10] Doss, J. D., Hutson, R. L., 
Rowsey, J. J., and Brown, D. R. Method for calculation of corneal pro.le and power distribution. Arch 
Ophthalmol 99 (1981), 1261 5. [11] Favardin, C. Determination automatique de structures ge­ometriques 
destinees a la reconstruction de courbes et de sur­faces a partir de donnees ponctuelles . PhD thesis, 
Universite Paul Sabatier, Toulouse, France, 1993. [12] Goshtasby, A. Surface reconstruction from scattered 
measure­ments. SPIE 1830 (1992), 247 256. [13] Hoppe, H., DeRose, T., Duchamp, T., Halstead, M., Jin, 
H., McDonald, J., Schweitzer, J., and W., S. Piecewise smooth surface reconstruction. Computer Graphics 
(SIGGRAPH 94 Proceedings) (July 1994), 295 302. [14] Jarvis . A perspective on range .nding techniques 
for computer vision. IEEE Trans. PAMI 5 , 2 (1983), 122 139. [15] Klyce, S. D. Computer-assisted corneal 
topography, high­resolution graphic presentation and analysis of keratoscopy. In­vest Ophthalmol Vis 
Sci 25 (1984), 1426 35. [16] Koch, D. D., Foulks, G. N., and Moran, T. The corneal eyesys system: accuracy, 
analysis and reproducibility of .rst generation prototype. Refract Corneal Surg 5 (1989), 424 9. [17] 
Krachmer, J. H., Feder, R. S., and Belin, M. W. Keratoconus and related nonin.ammatory corneal thinning 
disorders. Surv. Ophthalmol 28 , 4 (1984), 293 322. [18] Maguire, L. J., and Bourne, W. D. Corneal topography 
of early keratoconus. Am J Ophthalmol 108 (1989), 107 12. [19] Mammone, R. J., Gersten, M., Gormley, 
D. J., Koplin, R. S., and Lubkin, V. L. 3-D corneal modeling system. IEEE Trans Biomedical Eng 37 (1990), 
66 73. [20] Moore, D., and Warren, J. Approximation of dense scattered data using algebraic surfaces. 
Tech. rep., TR 90-135, Rice Uni­versity, 1990. [21] Pratt, V. Direct least-squares .tting of algebraic 
surfaces. SIG-GRAPH 87 Conference Proceedings (1987), 145 152. [22] Sato, Y., Kitagawa, H., and Fujita, 
H. Shape measurement of curved objects using multiple slit-ray projections. IEEE Trans. PAMI 4 , 6 (1982), 
641 649. [23] Schmitt, F., Barsky, B. A., and Du, W.-H. An adaptive subdivi­sion method for surface .tting 
from sampled data. SIGGRAPH 86 Conference Proceedings (1986), 179 188. [24] Taubin, G. An improved algorithm 
for algebraic curve and sur­face .tting. In Proc. 4th International Conf. on Comp. Vision, Berlin (1993), 
pp. 658 665. [25] Terzopolous, D. Regularization of inverse visual problems in­volving discontinuities. 
IEEE Trans. PAMI 8 (1986), 413 424. [26] Topa, L., and Schalkoff, R. An analytical approach to the deter­mination 
of planar surface orientation using active-passive image pairs. Computer Vision, Graphics, and Image 
Processing 35 (1994), 404 418. [27] Turk, G., and Levoy, M. Zippered polygon meshes from range images. 
Computer Graphics (SIGGRAPH 94 Proceedings) (1994), 311 318. [28] van Saarloos, P. P., and Constable, 
I. Improved method for calculation of corneal topography for any photokeratoscope ge­ometry. Optom Vis 
Sci 68 (1991), 960 6. [29] Wang, J., Rice, D. A., and Klyce, S. D. A new reconstruction algorithm for 
improvement of corneal topographical analysis. Re­fract. Corneal Surg 5 (1989), 379 387. [30] Warnicki, 
J. W., Rehkopf, P. G., and Curtin, S. A. Corneal topography using computer analyzed rasterographic images. 
Am. J. Opt 27 (1988), 1125 1140. [31] Wilson, S. E., and Klyce, S. D. Advances in the analysis of corneal 
topography. Surv. Ophthalmol. 35 (1991), 269 277. Figure 6: Synthetic ellipsoid Figure 7: Synthetic bump 
image. on sphere image. Figure 8: Visualization in 3D of surface recovered from real data.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237273</article_id>
		<sort_key>343</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[Coarse-grained parallelism for hierarchical radiosity using group iterative methods]]></title>
		<page_from>343</page_from>
		<page_to>352</page_to>
		<doi_number>10.1145/237170.237273</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237273</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Sorting and searching</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.3</cat_node>
				<descriptor>Linear systems (direct and iterative methods)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.3</cat_node>
				<descriptor>Distributed programming</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010148.10010149.10010158</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms->Linear algebra algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003719</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on matrices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10010031.10010033</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Data structures design and analysis->Sorting and searching</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010177</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Verification</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P280650</person_id>
				<author_profile_id><![CDATA[81100182132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Funkhouser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories, Murray Hill, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>91420</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baum, D., and Winget, J. Real Time Radiosity Through Parallel Processing and Hardware Acceleration. Computer Graphics (1990 Symposium on Interactive 3D Graphics), 24, 2, 67-75.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bouatouch, K., and Priol, T. Data Management Scheme for Parallel Radiosity. Computer-Aided Design, 26, 12, December, 1994, 876-883.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chalmers, A, and Paddon, D. Parallel Processing of Progressive Refinement Radiosity Methods. Second Eurographics Workshop on Rendering, Barcelona, Spain, May, 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chen, S.E. A Progressive Radiosity Method and its Implementation in a Distributed Processing Environment. Master's Thesis, Cornell University, 1989.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cohen, M., Greenberg, D., Immel, D., and Brock, E An Efficient Radiosity Approach for Realistic Image Synthesis.IEEE Computer Graphics and Applications, 6, 3 (March, 1986), 25-35.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cohen, M., Chen, S., Wallace, J., and Greenberg, D. A Progressive Refinement Approach to Fast Radiosity Image Generation. Computer Graphics (Proc. SIGGRAPH '88), 22, 4, 75-84.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Drettakis, G., Fiume, E., and Fournier, A. Tightly-Coupled Multi-Processing for a Global Illumination Algorithm. EU- ROGRAPHICS '90, Montreux, Switzerland, 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Drucker, S., and Schroder, E Fast Radiosity Using a Data Parallel Architecture. Third Eurographics Workshop on Rendering, 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Feda, M., and Purgathofer, W. Progressive Refinement Radiosity on a Transputer Network. Second Eurographics Workshop on Rendering, 1991, 139-148.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Feda, M., and Purgathofer, W. Progressive Ray Refinement for Monte Carlo Radiosity. Fourth Eurographics Workshop on Rendering, 1993, 15-25.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578533</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Garey, M., and Johnson, D. Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. Freeman and Company, New York, 1979.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>313702</ref_obj_id>
				<ref_obj_pid>313651</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Guattery, S., and Miller, G. On the Performance of Spectral Graph Partitioning Methods. 1995 ACM-SIAM Symposium on Discrete Algorithms (SODA), 1995.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Golub, G., and Van Loan, C. Matrix Computations. John Hopkins University Press, Baltimore, MD, 2nd Edition, 1989.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Goral, C., Torrance, K., Greenberg, D., and Battaile, B. Modeling the Interaction of Light Between Diffuse Surfaces. Computer Graphics (Proc. SIGGRAPH '84), 18, 3,213-222.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166146</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Gortler, S., Schroder, R, Cohen, M., and Hanrahan, R Wavelet Radiosity. Computer Graphics (Proc. SIGGRAPH '93), 221- 230.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218334</ref_obj_id>
				<ref_obj_pid>218327</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Guitton, R, Roman, J., and Subrenat, G. Implementation Results and Analysis of a Parallel Progressive Radiosity. In 1995 Parallel Rendering Symposium, Atlanta, Georgia, October, 1995,31-37.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, R, and Salzman, D. A Rapid Hierarchical Radiosity Algorithm. Computer Graphics (Proc. SIGGRAPH '91), 25, 4, 197-206.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Naylor, B. Constructing Good Partitioning Trees. Graphics Interface '93. Toronto, CA, May, 1993, 181-191.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Paddon, D., and Chalmers, A. Parallel Processing of the Radiosity Method. Computer-Aided Design, 26, 12, December, 1994, 917-927.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91419</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Recker, R., George, D., and Greenberg, D. Acceleration Techniques for Progressive Refinement Radiosity. Computer Graphics (1990 Symposium on Interactive 3D Graphics), 24, 2, 59-66.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Rushmeier, H., Patterson, C., and Veerasamy, A. Geometric Simplification for Indirect Illumination Calculations. Graphics Interface '93, May, 1993,227-236.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614311</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Sillion, F. A Unified Hierarchical Algorithm for Global Illumination with Scattering Volumes and Object Clusters. IEEE Transactions on Visualization and Computer Graphics, I, 3, September, 1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>182461</ref_obj_id>
				<ref_obj_pid>182452</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Singh, J.R, Gupta, A. and Levoy, M. Parallel Visualization Algorithms: Performance and Architectural Implications. IEEE Compuwr, 27, 7 (July 1994), 45-55.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192277</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Smits, B., Arvo, J., and Greenberg, D. A Clustering Algorithm for Radiosity in Complex Environments. Computer Graphics (Proc. SIGGRAPH '94), 435-442.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171029</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Teller, S., Visibility Computations in Densely Occluded Polyhedral Environments. Ph.D. thesis, Computer Science Division (EECS), University of California, Berkeley, 1992. Also available as UC Berkeley technical report UCB/CSD-92-708.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166148</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Teller, S., and Hanrahan, R Global Visibility Algorithms for Illumination Computations. Computer Graphics (Proc. SIG- GRAPH '93), 239-246.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192279</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Teller, S., Fowler, C., Funkhouser, T., and Hanrahan, R Partitioning and Ordering Large Radiosity Computations. Computer Graphics (Proc. SIGGRAPH '94), 443-450.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74366</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Wallace, J., Elmquist, K., Haines, E. A Ray Tracing Algorithm for Progressive Radiosity. Computer Graphics (Proc. SIG- GRAPH '89), 23, 3, 315-324.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Young, D.M. Iterative Solution of Large Linear Systems. Computer Science and Applied Mathematics. Academic Press, New York, 1971.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218336</ref_obj_id>
				<ref_obj_pid>218327</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Zareski, D., Wade, B., Hubbard, R and Shirley, R Efficient Parallel Global Illumination using Density Estimation. 1995 Parallel Rendering Symposium. Atlanta, Georgia, October, 1995, 47-54.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Zareski, D. Parallel Decomposition of View-Independent Global Illumination Algorithms. Master's thesis, Cornell University, 1996.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Coarse-Grained Parallelism for Hierarchical Radiosity Using Group Iterative Methods Thomas A. Funkhouser 
Bell Laboratories . Abstract This paper describes algorithms that allow multiple hierarchical ra­diosity 
solvers to work on the same radiosity solution in parallel. We have developed a system based on a group 
iterative approach that repeatedly: 1) partitions patches into groups, 2) distributes a copy of each 
group to a slave processor which updates radiosities for all patches in that group, and 3) merges the 
updates back into a master solution. The primary advantage of this approach is that separate instantiations 
of a hierarchical radiosity solver can gather radiosity to patches in separate groups in parallel with 
very little contention or communication overhead. This feature, along with automatic partitioning and 
dynamic load balancing algorithms, en­ables our implemented system to achieve signi.cant speedups run­ning 
on moderate numbers of workstations connected by a local area network. This system has been used to compute 
the radiosity solution for a very large model representing a .ve .oor building with furniture. CR Categories 
and Subject Descriptors: D.1.3 [Programming Techniques]: Concurrent Programming -Distributed Programming; 
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -Radiosity.  1 Introduction An important 
application of computer graphics is lighting simula­tion for architectural design. Although radiosity 
methods are often used for simulating illumination of building interiors, current ra­diosity algorithms 
generally are not fast enough or robust enough to handle large architectural models complete with furniture 
due to their large computational and memory requirements. A plausible approach for accelerating such 
a large computation is to partition the problem among multiple concurrent processors each of which solves 
a separate subcomputation. This approach is particularly attractive using a network of loosely connected 
workstations since this type of parallel computing resource is common today in many industrial and research 
laboratories. In this paper, we describe a new approach to executing large radiosity computations in 
parallel. The key innovation is a group iterative algorithm that partitions the patches into groups, 
and it­eratively solves radiosities for patches in each group separately on different processors in parallel, 
while dynamically merging updated radiosities into a single solution. The primary advantage of this .Murray 
Hill, NJ 07974, funk@research.att.com Permission to make digital or hard copies of part or all of this 
work or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 approach is that different 
group subcomputations update separate subsets of the form factors and radiosities, and therefore they 
can execute hierarchical radiosity solvers concurrently with little or no contention. This feature, along 
with dynamic load balancing al­gorithms, enables our implemented system to achieve signi.cant speedups 
with moderate numbers of workstations distributed on a local area network. In our implementation, no 
single process ac­cesses the entire scene database, and thus we are able to compute accurate radiosity 
solutions for very large models. The paper is organized as follows. The next section reviews the radiosity 
method and describes previous work on parallel radiosity systems. Section 3 describes the classical group 
iterative method and discusses how it can be applied to radiosity problems. An overview of our system 
organization appears in Section 4, while de­tailed descriptions of the partitioning and load balancing 
algorithms are included in Section 5. Section 6 contains results of experiments with our system. Finally, 
Section 7 contains a brief summary and conclusion. 2 Previous work Radiosity methods [14] simulate diffuse 
global illumination by com­puting the amount of light arriving at each patch by emission or diffuse re.ection 
from other patches. If each patch is composed of elements (i.e., substructured [5]), the method must 
solve the following linear system of equations: n X Bi=Ei+PiBjFij (1) j=1 where Biis the radiosity of 
element i, Eiis the emission of element i, Piis the diffuse re.ectivity of element i, Fijis the fraction 
of the energy leaving element ithat arrives element j,and nis the number of elements in the scene. The 
primary challenges in implementing the radiosity method are ef.cient computation and storage of the form 
factors. For each form factor, Fij, a visibility calculation must be performed to deter­mine a visibility 
percentage for elements iand j. This calculation must consider other patches in the scene as potential 
blockers, and thus accounts for the majority of the computation time in most radiosity systems. There 
has been considerable prior work on parallel implemen­tations of the radiosity method. Most of this work 
has been applied to the progressive radiosity algorithm [6] which has O(n 2)compu­tational complexity 
when solved to full convergence. Implementa­tions for this algorithm have been described for MIMD computers 
[2, 3, 16, 19], SIMD computers [8], transputers [9], shared memory multi-processors [1, 7], and networks 
of workstations [4, 20]. Re­cently, a few papers have appeared describing work on parallelizing the Monte 
Carlo Radiosity algorithm [10, 30]. Most current imple­mentations require a complete description of the 
scene s geometry to be resident in memory on all processors, thus limiting the size of models for which 
they can be applied. There has been relatively little work on parallel implementa­tions of the hierarchical 
radiosity method, which is surprising at .rst glance since its asymptotic complexity is O(n)[17]. Singh 
[23] im­plemented a parallel hierarchical radiosity solver for a shared mem­ory multiprocessor system 
in which each processor was initially assigned a queue of element-element interactions to process. When 
a processor subdivided an element, it added new interactions for the element s children to the head of 
its own queue. Load balancing was achieved by task stealing idle processors removed and pro­cessed interactions 
from the tail of other processors queues. Due its communication intensive nature, this approach is not 
practical for a network of distributed workstations. Zareski [31] implemented a parallel version of the 
hierarchical radiosity algorithm on a network of workstations using a master­slave architecture in which 
each slave performed patch-ray intersec­tion calculations for a separate subset of patches in the scene. 
For each element-element interaction, the master process constructed a set of rays and distributed them 
to every slave for parallel calcula­ tion of intersections with each slave s subset of the patches. After 
the slaves returned their hits for each ray, the master computed form factors and updated radiosities. 
Speedup with this .ne-grained ap­ proach was thwarted by both master processing bottlenecks and the 
overhead of inter-process communication, resulting in longer execution times with more processors. There 
are several aspects of the hierarchical radiosity algorithm that make parallelization dif.cult. First, 
since two patches can inter­act at any level of their hierarchies, subcomputation times are highly variable 
making load balancing dif.cult. Second, since both shooter and receiver patches can be subdivided dynamically, 
reader/writer locks must be used to enforce concurrency control during updates, anddeadlockavoidance/resolutionmustbeconsidered. 
Thecompu­tation required to manage concurrency control and deadlock during access to the element radiosity 
and mesh hierarchies can signi.­cantly reduce speedup results. In summary, none of the previous parallel 
systems is fast or robust enough to compute an accurate radiosity solution for a very large building 
model because they suffer from at least one of the following shortcomings: 1) greater than O(n)computational 
com­plexity, 2) replication of the entire model for each processor, 3) inaccuracies due to energy transfer 
or form factor approximations (e.g., hemi-cube artifacts), 4) limited speedup due to contention during 
access to shared data, and 5) communication overhead for process control. In order to scale to very large 
models, a radiosity system must use an ef.cient matrix solution method, such as hierarchical ra­diosity. 
The algorithm must be partitioned into separate concurrent subcomputationsthateachaccessasmallsubsetofthemodel. 
Inor­der to scale to many processors, the separate subcomputations must read/write separate regions or 
copies of the model to avoid slow­downs due to contention. Finally, the granularity of parallelism must 
be coarse enough to allow execution with minimal overhead for communication between participating processors. 
The design and implementation of a parallel radiosity system meeting these criteria is the topic of this 
paper. 3 Group Iterative Methods The radiosity method must solve a linear system of equations rep­resented 
by the row-diagonally dominant interaction matrix shown in Figure 1. Group iterative methods partition 
the Bivariables into groups, and rather than just relaxing one variable at a time, they relax an entire 
group during a single step [13, 29]. Gauss-Seidel group iteration relaxes each group using current estimates 
for Bjs from other groups, while Jacobi group iteration uses Bjs from other groups updated at the end 
of the previous complete iteration through all groups. Figure 2: One relaxation step for group ABC . 
There are several advantages to the group iterative approach for large radiosity problems, particularly 
with regards to parallel pro­cessing. First, each group gathering step updates radiosities only for the 
elements in its group, which is advantageous for concur­rency control when compared to shooting algorithms 
that update radiosities for all elements in each step [3]. Second, with Jacobi methods, updates to the 
radiosity values of elements in each group depend only upon radiosity values copied at the end of the 
previous iteration, and do not require access to current radiosity values for el­ements in all groups. 
This property allows multiple radiosity solvers to execute concurrently on different groups, with each 
solver updat­ing a separate copy of the radiosity values without readers/writers contention. It then 
becomes practical to use ef.cient, yet compli­cated, radiosity algorithms, such as Hierarchical Radiosity 
[17], to solve each group subproblem. Finally, group methods exhibit better cache coherence than element-by-element 
methods [13] since links between patches in the same group can be reused several times as the group is 
solved to convergence. This feature is particularly im­portant for radiosity problems whose form factor 
matrices do not .t in memory all at once. In this paper, we describe the design and implementation of 
a radiosity system based on group iterative techniques that uses multiple concurrent hierarchical radiosity 
solvers. For each iter­ation, the system automatically partitions the patches describing a scene into 
groups and executes hierarchical radiosity solvers to compute converged radiosity solutions for separate 
groups on sep­arate processors using separate versions of the model in parallel. Throughout the computation, 
updated versions of the element ra­diosities are copied into a master scene database for later use by 
other processors. Since hierarchical substructuring and form factor calculations are performed for different 
groups in parallel on the separate processors accessing separate copies of the model, we can accelerate 
overall computation times due to parallelism with little or no contention. Since coordination of processes 
is performed at a coarse-grained level (groups), relatively little communication is required between 
processes. As a result, signi.cant speedups are possible for moderate numbers of processors. Furthermore, 
since each processor must store only the working set for computations for one group at a time, the approach 
scales to support very large models. 4 System Organization Our system is organized in a Master-Slave 
con.guration with one master and Pslaves running concurrently on separate processors. The slaves are 
used to execute radiosity computations, while the master performs dynamic load balancing and data distribution. 
All processes maintain independent (partial) copies of the scene database, and slaves communicate with 
the master only via TCP messages. This organization allows distribution across loosely­coupled workstations 
without shared memory, or even shared disks. 4.1 Flow of Control The.owofcontrolbetweenthemasterandslaveprocessesis 
shown in Figure 3. The master iteratively relaxes groups until convergence. For each master iteration, 
the master partitions patches of the scene database into groups, and then dynamically distributes the 
groups to slaves one at a time for group relaxation computations. These automatic partitioning and scheduling 
algorithms are the focus of this paper, and are described in detail in the following section. This section 
describes the organization of the system in which these algorithms execute. Master Slave Si Figure 
3: Master-slave .ow of execution. The master starts by spawning Pslave processes (usually on remote computers) 
and opens a TCP socket connection to each of them. It uses the select UNIX system call to detect messages 
from multiple slaves. Whenever a slave, Si, indicates it is ready, the master selects a group, G, from 
a list of candidate groups waiting to beprocessedduringthecurrentmasteriteration. Next,itdownloads to 
Siall patches potentially visible to any patch in group G(i.e., the working set forG). After the downloads 
have completed, the master sends slave Sia message indicating that it can begin its radiosity computations 
for group G. While Sirelaxes group G to convergence, the master continues servicing other slaves. After 
slave Si.nishes its computation, it sends updated radiosity values back to the master for use in the 
current or future iterations. Each slave runs asynchronously on a separate processor under the guidance 
of the master process as shown on the right side of Figure 3. When a slave receives a download message 
from the master, it updates its local copy of the patches it receives, waits for a compute message from 
the master, and then invokes a hierarchical radiosity solver to gather radiosity to all patches in group 
Guntil convergence. The radiosity solver is based on the hierarchical (wavelet) ra­diosity system described 
in [15, 17, 27]. Although its details are not the focus of this paper, it is important to note that it 
stores its evolv­ing solution in a disk-resident database and loads into memory only the data required 
for the current subcomputation. It manages a .xed size, memory-resident, LRU cache to store the most 
recently used elements and links (form factors) in hopes that they will be used again before they are 
discarded. As computation of the form factors is the most costly componentof the system execution, effective 
man­agement of this cache is critical to avoiding costly recomputation or re-loading from disk. This 
feature of the slave solver is advanta­geous for the group iterative approach. Since gathers are performed 
multiple times to the elements of the same group in succession, the group algorithms exhibit far greater 
cache coherency, and in our case, we are often able to re-use links computed for two elements multiple 
times before they are discarded from the cache. In contrast, effective cache management would be very 
dif.cult with classical gather algorithms that make successive sweeps over all patches in the entire 
database. After the radiosity computation for group Ghas been com­pleted, the slave writes into its local 
cache updated versions of all patches in G, including the re.ned hierarchical element meshes and radiosities 
for each patch, and sends to the master an upload message containing a packed representation of these 
patches. This updated version of Gis merged into the master scene database and can potentially be downloaded 
to other slaves for later computations. Note that communication between master and slave occurs only 
three times for each group iteration: 1) to download patches to the slave at the beginning of an iteration, 
2) to invoke a radiosity com­putation, and 3) to upload patches from the slave at the end of an iteration. 
This coarse-grained approach to parallelism is important. Other efforts to parallelize the radiosity 
method with a master-slave organization have found master processing to be a bottleneck, and communication 
overhead has diminished speedup results signi.­cantly using relatively few slave processors. Our strategy 
is to design a system in which a master coordinates execution of the slaves, but at a very coarse granularity, 
with very infrequent com­munication. 4.2 Data Distribution The scene description is initially available 
only to the master. It is stored in a database containing patches represented by quad-trees of elements 
with diffuse re.ectivity, radiosity, and emission attributes. The patches are stored in the scene database 
arranged in clusters speci.ed by the modeler at scene creation time. The scene database also contains 
precomputed cluster-to-cluster visibility information. The cluster visibility calculation is performed 
off-line using the algorithms described in [26] and generates a list for each cluster indicating which 
other clusters are potentially visible to it i.e., not occluded by a wall, ceiling, or .oor. Although 
clustering and visibility techniques are an important research area, and essential to the ef.cient execution 
of our radiosity system, these topics are not addressed in this paper. See [21, 22, 24, 26] for further 
information. Since only the master has access to the complete scene database, it must download portions 
of the database (i.e., potential working sets) to slaves during execution. We de.ne the potential working 
set for a group Gto be all the patches, including their element meshes and radiosities, that are visible 
to any patch in G. This de.nition of the working set is a conservative over-estimate of the set of data 
the slave may need access to during radiosity gather operations for any group. Since patches in occluded 
clusters cannot exchange energy directly, we can use the precomputed cluster-to-cluster visibility information 
of the scene database to compute the potential working set ef.ciently. During execution, the master keeps 
an index of which clusters havealreadybeendownloadedtoeachslave. Ittraversesthecluster­to-cluster visibility 
lists for all clusters containing patches in group G, checking whether the potentially visible cluster, 
C, is already up-to-date on the slave, Si. If not, it reads from the disk-resident scene database all 
the data describing patches in C, including the hierarchical mesh of elements with radiosities. It packs 
this data into a buffer and performs a write to the TCP socket for slave Si. Finally, it marks slave 
Siup-to-date for cluster Cin its index, and continues checking for other potentially visible clusters 
to download. Note that all patches of a cluster are required to be in the same group, making this download 
processing somewhat more ef.cient. After all clusters in the working set of Gare up-to-date on slave 
Si,the master sends a short message indicating which clusters belong to group G, and directing it to 
gather radiosity to all patches in those clusters to convergence. After the slave has updated the radiosities 
for all patches in group Gto convergence, it sends an upload message to the master with complete updates 
for every cluster Cin group G. The master writes these updates back to a new version of cluster Cin its 
disk-resident database. It then marks clusters in Gout-of-date for all slaves except Si, causing them 
to be freshly downloaded for subsequent gather operations for clusters visible to G. With this concurrent 
copy­update-replace methodology, our system does not truly execute either the Jacobi group or Gauss-Seidel 
group iterative method, as it is indeterminate whether the old or updated copies of a group s variables 
will be used during each group relaxation step. Proof of convergence with this optimization is shown 
in the following section. The data distribution features of our system are important for scaling to support 
computations with very large models. The master stores only the scene database header information in 
main memory (generally less than 20MB), while the clusters, patches, and ele­ments reside on disk. Each 
slave receives and stores only the subset of the scene database required for its computation, avoiding 
full replication of the entire database on any processor as is required by most other parallel radiosity 
systems. 4.3 Convergence Proof Proof of convergence of our parallel group iterative method can be shown 
by comparison to the standard sequential group Jacobi method, which is known to converge [29]. Consider 
splitting the matrix A(as in Ax=b)into A=D-L-Uwhere Dhas blocks along the diagonal, Lhas the opposites 
of element below D,and U has opposites of elements above D. For the radiosity equation, A is monotone 
(i.e., it has non-negative elements along the diagonal and non-positive elements elsewhere), Dis monotone, 
L>0, and U>0. The standard group Jacobi method iterates according to the following equation: D-1 xk+1 
=IJxk+b D-1 whereIJ=(L+U) whereas our modi.ed group method iterates using some variables updated in the 
current iteration and some updated in the previous iteration: k-1 xk+1 =IMxk+(D-L1 )b k-1 k whereIM=(D-L1 
)(L2 +U); kkk k L=L1 +L2 ;L1 >0;andL2 >0 We can show that our modi.ed group method converges if the error 
is reduced during each iteration. Since jjxk+1 -xjj P(IM) jjxk-xjj where jjxk+1 -xjjis a suitable vector 
norm, and P(IM)is the spectral jjxk-xjj radius of IM, convergence is guaranteed if P(IM) P(IJ)<1. We 
prove P(IM) P(IJ)using corollary 5.6 on page 125 of Young [29]: Let Abe a monotone matrix and let A=Q1 
-R1 and A=Q2 -R2 be two regular splittings of A.If R2 R1,then P(Q-21R2) P(Q-11R1). A regular splitting 
of Ais one in which A=Q-Rwhere Q-1 >0and R>0. For the standard group Jacobi iteration method, let A=Q1 
-R1 where Q1 =Dand R1 =L+U.Note that Q-11 >0since Dis monotone and R1 >0since L>0and U>0. For our modi.ed 
group iteration method, let A=Q2 -R2 where Q2 =D-L1 kand R2 =Lk 2 +U. Note that Q-21 >0since kk D-L1 
is monotone. Also note that R2 >0since L2 >0and U>0, and R2 R1 since L>L2 k . Applying corollary 5.6 
and convergence of the standard group Jacobi method, we see that P(IM)<1 and the modi.ed group iterative 
method must converge: -1 -1 P(IM)=P(Q2 R2) P(Q1 R1)=P(IJ)<1  5 Parallel Programming A general strategy 
for parallel programming is to decompose a computation into a set of independent subcomputations, and 
then to distribute the subcomputations for execution in parallel on avail­able processors. The important 
issues are to .nd an appropriate decomposition (i.e., partition patches into groups), and to schedule 
execution of the subcomputations effectively (i.e., load balancing). These issues are addressed in detail 
in this section. 5.1 Group Partitioning Goals and Strategies Based on intuition derived from experimentation 
with our system, we have developed the following set of guidelines that constrain our automatic partitioning 
algorithms: 1) the number of groups, N, should be bounded from below so that there are guaranteed to 
be enough groups to schedule effectively on Pslave processors (e.g., N8P); 2) each group should be large 
enough that the time required to distribute its computation to a slave is not more than it would have 
been to perform it locally on the master; and 3) each group should be small enough that the links for 
radiosity updates to all elements in the group .t in a slave s memory-resident cache so that they may 
be re-used over and over again without recomputation as the group is solved to convergence. We combine 
these constraints with the goals of maximizing intra-group form factors while minimizing inter-group 
form factors to form the basis of our partitioning algorithms. For practical purposes, we consider only 
partitionings in which all patches of a cluster are assigned to the same group. This re­striction simpli.es 
the partitioning algorithms, and aids execution of the data distribution algorithms during execution 
of our radiosity system, as described in the previous section. Conceptually, we address the cluster partitioning 
problem as a computation on a form factor graph in which each node in the graph represents a cluster, 
and each edge represents an estimate of the form factor between its nodes clusters (a simple form factor 
graph is shown in Figure 4). With this formulation, we state the cluster partitioning problem as follows: 
assign nodes of the form factor graph to groups such that the cumulative weight of edges between nodes 
in the same group divided by the cumulative weight of all edges is maximal. Unfortunately, this problem 
is equivalent to the Graph Bisec­tion Problem [12], which is known to be NP-complete. However, we have 
developed two automatic algorithms that .nd approximate and useful solutions in polynomial time. The 
.rst algorithm, called the Merge Algorithm, starts by assigning each cluster to a separate group and 
then iteratively merges groups. Conversely, the second algorithm, called the Split Algorithm, starts 
by assigning all clus­ters to the same group and then recursively splits groups. Either algorithm can 
be used to construct groups, or the algorithms can be applied successively to iteratively re.ne groups. 
 Cluster Geometry Form Factor Graph Figure 4: Simple scene (left) with its form factor graph (right). 
Edge thickness represents form factor magnitude. Merge Algorithm The Merge Algorithm operates on an 
augmented version of the form factor graph in which nodes represent groups rather than clusters. In this 
augmented graph, the edge between two nodes representing groups Aand Bhas weight equal to the sum of 
the form factors between all combinations of clusters in groups Aand B. Initially, a graph is created 
with one node for each cluster. For the purposes of constructing this graph, an edge weight is set to 
zero (or the edge is not created at all) if two clusters are known to be occluded from one another (as 
determined by a lookup in the precomputed cluster­to-cluster visibility information stored in the scene 
database). Oth­erwise, the form factor, FAB, from one cluster Ato another cluster Bis estimated as the 
solid angle subtended by a disk representing cluster B [28]: FAB=r 2.(d2 +r 2 ) (2) where d is the distance 
between A and B,and r is the radius of a sphere bounding B. This approximation is an over-estimate that 
does not consider individual patch orientations and assumes that A is entirely visible to B. Once the 
form factor graph has been constructed, the Merge Algorithm iteratively merges groups (nodes of the graph) 
until no further combinations are possible within the following constraints: 1) the number of groups 
is greater than a user speci.ed minimum, MinGroups, and 2) the estimated number of links for any group 
with more than one cluster is below a user speci.ed maximum, MaxLinks. By default, MaxLinks is arbitrarily 
set to be 1:25 . TotalLinks.MinGroups,where TotalLinks is the sum of the link estimates for all groups. 
The key challenge for implementation of the Merge Algorithm is selecting two appropriate groups to merge 
during each step of the algorithm. We take a greedy approach. The pair of groups, A and B, is chosen 
whose merger causes the greatest increase in the ratio of intra-group edge weights divided by the total 
of all edge weights. If the merger of these groups meets all constraints, they are combined into one. 
During the merge operation, edges from A and Bto other nodes are replaced by ones to the new merged node. 
The weight of this new edge is the sum of the weights of the edges it replaces (see Figure 5). The algorithm 
repeatedly merges groups until it can no longer .nd any pair of groups to merge legally, or the solution 
cannot be improved. In the worst case, when all clusters are visible to one another, the algorithm is 
bounded by O(N2logN). However, in situations such as building interiors, where visibility sets are usually 
of constant size, the average execution time for the merge algorithm is O(NlogN). .5 .5 .1 .1.3 Figure 
5: Merge operation for nodes D and E . Split Algorithm The Split Algorithm uses a strategy that is the 
converse of the Merge Algorithm. It starts with all clusters assigned to a single group and then recursively 
splits groups until further splits do not improve the solution. This algorithm can be interpreted as 
a recursive binary partitioning of the form factor graph. During each step of the algorithm, our goal 
is to choose an appropriate partition of one group into two new ones that meet all size constraints and 
have minimal inter-group form factors. We use geometric split heuristics originally developed for construction 
of spatial subdivisions for use in visibility determination (e.g., BSP trees [18]). Speci.cally, we partition 
the model along planes aligned with major occluding polygons of the model (see [25] for details). As 
the model is split recursively by these planes, clusters are as­signed to groups depending on whether 
their centroid lies above or below the splitting plane (see Figure 6). This process is applied recursively 
until no groups can be split within minimum group size constraints, or until no further major occluder 
polygons can be found. The algorithm runs in O(NlogN). If split planes are chosen appropriately (i.e., 
such that the cumulative form factors between clusters on separate sides of the plane are small), it 
generates a partitioning with little exchange of energy between groups during a radiosity simulation. 
 Figure 6: Split operation creating groups A and B . Figure 7 shows two sets of 16 groups constructed 
using the Merge and Split algorithms, respectively, for a one .oor building model comprising 1667 clusters. 
(clusters are shaded based upon which group they were assigned). Using the Merge algorithm, groups tend 
to be formed from clusters that are visible to each other (e.g., of.ces across hallways), whereas groups 
tend to be formed from clusters that are nearby each other using the Split algorithm (e.g., neighboring 
of.ces). Merge Algorithm Split Algorithm Figure 7: Groups formed by the merge and split algorithms. 
 5.2 Scheduling Load balancing is a primary concern in any parallel system. Our goal is to schedule 
group radiosity subcomputations on slaves in a manner that maximizes the rate of convergence to an overall 
solution. Unfortunately, this Multi-Processor Scheduling Problem is NP-Complete since each subcomputation 
is non-preemptable, task execution times are highly variable, and workstations may have different performance 
capabilities [11]. In this section, we describe our approximation algorithms for scheduling and load 
balancing. First-Fit Decreasing Algorithm A common scheduling strategy for minimizing the total completion 
time for a set of tasks run on multiple processors is to select tasks in order of their expected execution 
times, largest to smallest. This strategy is called the First Fit Decreasing (FFD) algorithm [11]. The 
idea is to schedule the large tasks .rst so that there is less chance that their execution times will 
extend beyond the last execution time of any other task. We have applied this principal in our radiosity 
system. The dif.cult challenge is to predict in advance how long a radiosity computation for a group 
will take. We estimate the relative compute time for a transfer of radiosity from one cluster Ato another 
cluster Bby FAB. This estimate is based on the observation that slave compute times are dictated by the 
number link evaluations (ray­patch intersections), which is determined by the errors in computed element-element 
form factor estimates, which in turn are roughly correlated to form-factors. In order to estimate the 
computation time for gathers to a group of clusters, G, we sum estimated computation times for all cluster 
pairs in which at least one of the two clusters is in G, and the clusters are known to be at least partially 
visible to one another via the form factor graph. To execute the FFD algorithm,the master sorts groups 
according to computation time estimates as they are constructed. Then, groups are simply assigned in 
FFD order as slaves become available during execution. Working Set Algorithm The general principal of 
minimizing total completion time for a set of independent subcomputations is not enough to guarantee 
a fast convergence rate for our radiosity system. We must also consider factors affecting data download 
performance, duplicate calculation, and energy distribution. These issues are particularly important 
be­cause each slave maintains a local cache of data containing element radiosities and links previously 
computed. The history of which groups a slave has previously processed affects the download time and 
the energy distribution rate for the current computation. These issues are likely even more important 
for a system utilizing bi­directional links (our system creates uni-directional links) in which case 
re-use of inter-group links could be a signi.cant scheduling consideration. We have developed a dynamic 
scheduling algorithm that con­siders data download factors when scheduling group computations on slaves. 
The Working Set (WS) algorithm uses a heuristic that is designed to assign groups to slaves for which 
their working set has already been downloaded. Each time a slave Sibecomes available, it considers groups 
remaining to be processed during this iteration. For each candidate group, G, it computes the percentage 
of the clusters visible to any cluster in Gthat are already resident on Si. It then subtracts from this 
value the percentage of clusters visible to Gthat are not resident on Si, but are resident on some other 
slave. This latter factor helps to keep the visibility sets of groups assigned to different slaves separated. 
The difference between these two per­centages forms the heuristic that the Working Set algorithm uses 
to choose the best group for each slave dynamically as the system executes. Combined Scheduling Algorithm 
The methodologiesof the FFD and WS algorithms can be combined. We generally use a combined scheduling 
algorithm (FFD-WS) that dynamically chooses a group as each slave becomes available ac­cording to the 
WS heuristic subject to the constraint that every group must be scheduled within delta slots of its position 
in FFD order. This algorithm is equivalent to the FFD algorithm if delta=1, and it is equivalent to the 
WS algorithm if delta=1.Otherwise, if 1 <delta<N, we hope to realize the advantages of both the FFD and 
WS approaches.   6 Results and Discussion In order to test the effectiveness of the group iterative 
approach for solving large radiosity problems in parallel, we executed a series of experiments with our 
system using different group partitioning and load balancing algorithms. During these experiments, we 
used up to eight Silicon Graphics slave workstations, each with a 150MHz R4400 processor and at least 
80MB of available memory, 32MB of which was available for caching links. The workstations were spread 
over two separate local area networks and did not share disk .les. Unless stated otherwise, the Merge 
Algorithm was used to construct 256 groups, and the FFD-WS algorithm was used with delta=16 for dynamic 
load balancing. In all experiments except the one described in Section 6.1, the master process performed 
two complete iterations in which a slave gathered to every patch in each group twice with a moderately 
.ne error tolerance. During the initial slave iteration, patches gathered radiosity only from the lights. 
Our test model in every experiment was the computation of a ra­diosity solution for one unfurnished .oor 
of the Soda Hall building model. This test model contained 6,418 patches in 1,667 clusters, 242 of which 
contained only emissive patches. The total area of all surfaces was 10,425,645 square inches. Although 
this test model was not particularly complex, it was useful for experimentation. With a larger model, 
it would have been impractical for us to inves­tigate algorithmic trade-offs by performing many executions 
of the radiosity solver with different parameters. 6.1 Group Iteration Results We .rst compared the 
performance of the group iterative method to traditional iterative methods (independent of parallel processing) 
by computing the radiosity solution for our test model using a sin­gle processor both with and without 
grouping of patches. During the .rst test, patches were not grouped, and 4 traditional Gauss-Seidel iterations 
were made over all patches. During the second test, patches were partitioned into 256 groups by the Merge 
algo­rithm. Then, three Gauss-Seidel group iterations were made over all groups, during which every patch 
in a group gathered radios-iterative approach was better using partitions with larger intra-group ity 
twice (groups were not solved to full convergence during each form factors. step). During the test without 
grouping, every patch gathered ra­diosity from every other patch four times. In contrast, during the 
testwith grouping,patchesgatheredradiositysixtimes frompatches within the same group, but only three 
times from patches in other groups. Plots of transfer rates measured during these tests are shown in 
Figure 8. Circles on the plots indicate the end of a compete sweep through all variables in each test. 
 0 Elapsed Time (m) 300 Figure 8: Transfer rates for grouped/ungrouped iteration. Even without parallel 
processing, the group iterative method out-performed the traditional approach during this experiment. 
The performance difference was mostly due to the fact that the group method more effectively made use 
of links and patches cached in memory by the solver. As described in Section 4, the solver main­tained 
LRU memory resident caches of links and patches. Patches that did not .t in the cache had to be .ushed 
to disk, while links that did not .t in the cache were discarded and later recomputed. During this experiment, 
although the total amount of storage required for links exceeded the cache limit (32MB), the maximum 
working set for any group did not. As a result, since the group method cycled over patches in each group 
multiple times in succession, it was often able to re-use previously computed links (45% of the time). 
In con­trast, the traditional method executed a worst-case access pattern for the LRU cache, making complete 
sweeps through all patches in succession, and thus was not able to re-use any links.  6.2 Partitioning 
Results We next studied the effects of different group partitioning algorithms by executing a sequence 
of tests with 8 slaves using the following methods to partition clusters into 256 groups: .Merge: Groups 
were constructed using the Merge Algorithm with MinGroups=256. .Split: Groups were constructed using 
the Split Algorithm partitioning on .oors, ceilings, and walls of the building model with MaxGroups=256. 
.Region: Clusters were assigned to groups based on the (x,y) coordinates of their centroids in a 16x16 
grid. .Random: Clusters were assigned to groups randomly. Figure 9 contains plots of transfer rates measured 
during these tests. The system converged fastest using partitions generated au­tomatically with the Merge 
and Split Algorithms. This is due to the fact that these algorithms combined clusters into the same group 
based on estimated form factor and proximity relationships. During every test, each patch gathered radiosity 
a total of four times two iterations in a slave for each of two master iterations. This means that energy 
was distributed with four re.ections between clusters in the same group, while only two re.ections occurred 
between clus­ters in different groups. As expected, the performance of the group 0 Elapsed Time (m) 
30 Figure 9: Transfer rates for different partitioning algorithms. 6.3 Granularity Results We studied 
the effects of group granularity by measuring system performance using 8 slaves for tests with groups 
of different sizes. Using the Merge Algorithm, we executed tests with the clusters partitioned into 32, 
128, 256, and 1,425 groups. The test with 1425 groups represents construction of a separate group for 
each cluster containing at least one re.ective patch. Plots of transfer rates measured in each test appear 
in Figure 10. We found that the advantage of the group iterative method is reduced if groups are very 
small since there is little opportunity to re-use links computed for intra-group radiosity transfers. 
On the other hand, if we used just a few large groups, the data required for all intra-group links exceeded 
a slave s cache capacity for some groups, reducing the effectiveness of the cache. Also, it was more 
dif.cult to schedule a relatively few, large subcomputations across available slave processors in order 
to achieve the best possible com­pletion times. During our experiments, tests performed best with 256 
groups that roughly corresponded to the small, convex regions of the model. This result depends on a 
variety of factors, including the size of link caches in slaves and the variability of group sizes. 
0 Elapsed Time (m) 30  6.4 Scheduling Results We investigated load balancing and scheduling effects 
by executing a series of tests using 8 slaves with different scheduling algorithms: .FFD: Groups were 
assigned to slaves in FFD order. .WS: Groups were assigned to available slaves dynamically to minimize 
the WS heuristic. .FFD-WS: Groups were assignedto slaves dynamically using the FFD-WS algorithm with 
delta=16. From statistics measured during these tests, we found that the scheduling factors impacting 
convergence rates most were: 1) 30 master-slave download times, and 2) slave idle times (particularly 
at the end of each iteration). As expected, the master spent the least amount of time downloading data 
to slaves during tests using the WS algorithm (98 seconds). The advantages of the WS approach can be 
seen in Figure 11, in which all 256 groups are shaded ac­cording to which of the 8 slaves they were assigned 
during tests using the FFD and WS algorithms. The coherence of the working sets assigned to slaves using 
the WS algorithm allows the system to minimize data downloads and maximize energy transfers. The trade-offs 
between scheduling to minimize downloads and 4 Elapsed Time (m) 0 FFD Slaves WS Slaves FFD-WS Slaves 
Figure 12: Slave compute time (solid) vs. wait time (blank). at a time. As a result, if two slaves .nish 
a subcomputation and become ready for further processing at the same time, one must wait while the master 
exchanges data with the other. The impact of this effect is determined by the likelihood that a slave 
will .nish a subcomputation while the master is processing data for another slave. Although this likelihood 
grows with the number of slaves, it is also affected by the relationship of time required for master 
processing of downloads/uploads versus the time required for slave processing of a group radiosity subcomputation. 
For solutions in which the slave radiosity subcomputations are longer relative to the data distribution 
times, speedup results are better. 9 8 7 Speedup 6 5 FFD Algorithm WS Algorithm Figure 11: Visualization 
with groups shaded by slave. Unfortunately, the test using the WS algorithm also spent the most amount 
of time waiting for the last slave to .nish at the end of each iteration (547 seconds). In particular, 
one very large group computation was postponed until the very end of the second itera­tion, causing the 
master and seven of the slaves to sit idling while the eighth slave .nished its computation for that 
group. In contrast, the FFD algorithm spent a small amount of time waiting for the last slave at the 
end of each iteration (13 seconds), but it spent the most time downloading data to slaves (248 seconds). 
scheduling to minimize time waiting for the last slave can be seen in Figure 12, which shows a vertical 
elapsed time-line for each of the 8 slaves during tests with the FFD, WS, and FFD-WS algorithms. Every 
distinct vertical bar segment represents radiosity computation for one group on one slave. Using the 
FFD algorithm, the execution time predictor does fairly well, and longer tasks are generally sched­uled 
earlier in each iteration (the .rst of the two master iterations ends approximately 1/3rd of the way 
up the time-line). However, because the master spends more time downloading data to slaves (synchronously), 
there are more frequent and longer periods during which a slave is waiting for the master (blank spaces 
between verti­cal bars). Using the WS algorithm, although download times are far less (intra-bar gaps 
are smaller), the computation for one very large group was scheduled near the end of iteration 2 (on 
Slave 2) causing the master and all other slaves to wait for it to complete. The com­bined FFD-WS algorithm 
seemed to achieve a good combination of download times (191 seconds) and wait times (8 seconds), and 
thus converged most rapidly.  6.5 Speedup Results Finally, we executed an experiment to determine how 
much speedup is possible with our system via parallel processing. During this experiment, we solved the 
one .oor test model four times using 1, 2, 4, and 8 slaves, respectively. A plot of speedup for increasing 
numbers of slave processes is shown in Figure 13. For up to 8 slave processors, the system maintains 
a 65-75% speedup. The speedup is less than 100% due to the synchronous master-slave communication model 
of our system. Although the group iterative approach provides a relatively coarse granularity of parallelism, 
the master communicates with slaves synchronously in our current implementation i.e., it can only talk 
to one slave 3 2 1 12345678 Number of Slaves Figure 13: Transfer rate speedup for 1, 2, 4, and 8 slaves. 
The speedup bottleneck resulting from our current synchronous communication model with a centralized 
master can be mitigated somewhat by enhancing the master to use asynchronous I/O proto­cols or by switching 
to communication protocols in which slaves transfer data among themselves under master coordination. 
Based on our preliminary results, we are optimistic that the group iterative approach provides coarse 
enough granularity that our master-slave system can scale to large numbers of slave processors with the 
addi­tion of enhanced communication methods. Unfortunately, we have not yet implemented these improvements, 
and do not currently have access to enough workstations to determine the absolute limits of our current 
system. The speedup experiments point out an interesting trade-off of our parallel group iterative approach. 
On one hand, when more slaves compute concurrently, we are able to evaluate more element interactions 
in less time. On the other hand, since updated radiosity values are distributed from the master to a 
slave workstations only afterthey havebeenuploadedfromotherslaves,Gauss-Seidelgroup iteration is achieved 
only during tests with one slave. In contrast, if all groups were scheduled simultaneously on different 
slaves, the system would perform a true Jacobi group iteration. As more slaves are added to the system, 
the system more closely resembles Jacobi iteration since more and more computations are performed with 
copies of radiosity values last updated at the end of the previous iteration. Further research is required 
to investigate the impact of this effect. 6.6 Practical Results As a .nal test, we computed a radiosity 
solution for a very large model using the system described in this paper. The model repre­sents .ve .oors 
of a large building with approximately 250 rooms containing furniture. It was constructed with 14,234 
clusters com­prising 280,836 patches, 8,542 of which were emitters and served as the only light sources. 
The total area of all surfaces was 75,946,664 square inches. Three complete iterations were made through 
all patches using an average of 4.96 slave processors in 168 hours. The entire computation generated 
7,649,958 mesh elements and evalu­ated 374,845,618 element-to-element links. During this execution, the 
master spent 0.8% of its time con­structing and scheduling groups, 4.4% downloading data to slaves, 2.6% 
uploading results from slaves, and 89.0% waiting for slaves. The slaves spent 0.1% of its time downloading 
data from the master, 0.1% uploading results to the master, 79.1% updating radiosities, and 5.0% waiting 
for the master. Although it was not practical for us to solve this model using a single processor for 
comparison, we estimatethe speedupdueto parallelismasthetime spentperforming radiosity computations in 
slaves divided by the elapsed time, which was 3.9 in this case, or 79% of linear speedup. Figure 14 shows 
renderings of this large radiosity solution from various viewpoints captured during an interactive walkthrough. 
Outlines around mesh elements are included in the bottom-right image for detailed inspection. Note the 
adaptive re.nement of el­ements in areas of partial visibility (e.g., on the .oor near the legs of tables 
and chairs) due to hierarchical radiosity meshing. To the author s knowledge, this model is the most 
complex for which a radiosity solution has ever been computed.  7 Conclusion This paper describes a 
system for computing radiosity solutions for very large polygonal models using multiple concurrent processes. 
A master process automatically partitions the input model into groups of patches and dynamically schedulesslave 
processeswhich execute independent hierarchical radiosity solvers to update the radiosities of patches 
in separate groups. During experiments with this system, uniprocessor group methods out-performed traditional 
methods due to improved cache coherence, while multi-processor group methods achieved further speedups 
of 65-75% using up to 8 slave worksta­tions. We have found that the implementation and analysis of a 
dis­tributed approach to the radiosity problem requires careful con­sideration of group partitioning, 
data distribution, and load bal­ancing issues. Coarse-grained parallel execution using multiple separate 
copies of a shared database allows multiple processors to execute concurrently with little contention 
or synchronization over­head. However, since updates to the shared database are executed with coarse 
granularity, many of the subcomputations may be per­formed using out-of-date database values, potentially 
reducing the convergence rate. The con.icting goals between computing in parallel versus computing with 
the most up-to-date data results in an interesting trade-off whose resolution is affected by a multitude 
of factors, in­cluding the size of workstation memories, the size of working sets, the speed of the network, 
and so on. We believe that examining issues in parallel execution for large computations distributed 
over a network is an interesting research area that will become more and more important as networked 
computing resources become more and more prevalent. Acknowledgements The author thanks Roland Freund 
and Wim Sweldens for developing the convergence proof appearing in this paper. I am also grateful to 
Pat Hanrahan, Peter Schroder, and Stephen Gortler for their helpful insights and discussion, and to Seth 
Teller and Celeste Fowler for their efforts building the original radiosity system at Princeton. Fi­nally, 
special thanks to Carlo S´equin and the UC Berkeley Building Walkthrough Group for building the model 
of Soda Hall and for getting me started on this research project.  References [1] Baum, D., and Winget, 
J. Real Time Radiosity Through Paral­lel Processing and Hardware Acceleration. Computer Graph­ics (1990 
Symposium on Interactive 3D Graphics), 24, 2, 67-75. [2] Bouatouch, K., and Priol, T. Data Management 
Scheme for Parallel Radiosity. Computer-Aided Design, 26, 12, Decem­ber, 1994, 876-883. [3] Chalmers, 
A, and Paddon, D. Parallel Processing of Pro­gressive Re.nement Radiosity Methods. Second Eurographics 
Workshop on Rendering, Barcelona, Spain, May, 1991. [4] Chen, S.E. A Progressive Radiosity Method and 
its Imple­mentation in a Distributed Processing Environment. Master s Thesis, Cornell University, 1989. 
[5] Cohen, M., Greenberg, D., Immel, D., and Brock, P. An Ef.­cient Radiosity Approach for Realistic 
Image Synthesis. IEEE Computer Graphics and Applications, 6, 3 (March, 1986), 25-35. [6] Cohen, M., Chen, 
S., Wallace, J., and Greenberg, D. A Pro­gressive Re.nement Approach to Fast Radiosity Image Gen­eration. 
Computer Graphics (Proc. SIGGRAPH 88), 22, 4, 75-84. [7] Drettakis, G., Fiume, E., and Fournier, A. Tightly-Coupled 
Multi-Processing for a Global Illumination Algorithm. EU-ROGRAPHICS 90, Montreux, Switzerland, 1990. 
[8] Drucker, S., and Schroder, P. Fast Radiosity Using a Data Parallel Architecture. Third Eurographics 
Workshop on Ren­dering, 1992. [9] Feda,M.,andPurgathofer,W.ProgressiveRe.nementRadios­ity on a Transputer 
Network. Second Eurographics Workshop on Rendering, 1991, 139-148. [10] Feda, M., and Purgathofer, W. 
Progressive Ray Re.nement for Monte Carlo Radiosity. Fourth Eurographics Workshop on Rendering, 1993, 
15-25. [11] Garey, M., and Johnson, D. Computers and Intractability: A Guide to the Theory of NP-Completeness. 
W.H. Freeman and Company, New York, 1979. [12] Guattery, S., and Miller, G. On the Performance of Spectral 
Graph Partitioning Methods. 1995 ACM-SIAM Symposium on Discrete Algorithms (SODA), 1995. [13] Golub, 
G., and Van Loan, C. Matrix Computations. John Hop­kins University Press, Baltimore, MD, 2nd Edition, 
1989. [14] Goral, C., Torrance, K., Greenberg, D., and Battaile, B. Mod­eling the Interaction of Light 
Between Diffuse Surfaces. Com­puter Graphics (Proc. SIGGRAPH 84), 18, 3, 213-222.  [15] Gortler, S., 
Schroder, P., Cohen, M., and Hanrahan, P. Wavelet Radiosity. Computer Graphics (Proc. SIGGRAPH 93), 221­ 
230. [16] Guitton, P., Roman, J., and Subrenat, G. Implementation Re­sults and Analysis of a Parallel 
Progressive Radiosity. In 1995 Parallel Rendering Symposium, Atlanta, Georgia, October, 1995, 31-37. 
[17] Hanrahan, P., and Salzman, D. A Rapid Hierarchical Radiosity Algorithm. Computer Graphics (Proc. 
SIGGRAPH 91), 25, 4, 197-206. [18] Naylor, B. Constructing Good Partitioning Trees. Graphics Interface 
93. Toronto, CA, May, 1993, 181-191. [19] Paddon, D., and Chalmers, A. Parallel Processing of the Ra­diosity 
Method. Computer-Aided Design, 26, 12, December, 1994, 917-927. [20] Recker, R., George, D., and Greenberg, 
D. Acceleration Techniques for Progressive Re.nement Radiosity. Computer Graphics (1990 Symposium on 
Interactive 3D Graphics), 24, 2, 59-66. [21] Rushmeier, H., Patterson, C., and Veerasamy, A. Geometric 
Simpli.cation for Indirect Illumination Calculations. Graph­ics Interface 93, May, 1993, 227-236. [22] 
Sillion, F. A Uni.ed Hierarchical Algorithm for Global Illu­mination with Scattering Volumes and Object 
Clusters. IEEE Transactions on Visualization and Computer Graphics,I, 3, September, 1995. [23] Singh, 
J.P., Gupta, A. and Levoy, M. Parallel Visualization Al­gorithms: Performance and Architectural Implications. 
IEEE Computer, 27, 7 (July 1994), 45-55. [24] Smits, B., Arvo, J., and Greenberg, D. A Clustering Algorithm 
for Radiosity in Complex Environments. Computer Graphics (Proc. SIGGRAPH 94), 435-442. [25] Teller, S., 
Visibility Computations in Densely Occluded Poly­hedral Environments. Ph.D. thesis, Computer Science 
Divi­sion (EECS), University of California, Berkeley, 1992. Also available as UC Berkeley technical report 
UCB/CSD-92-708. [26] Teller, S., and Hanrahan, P. Global Visibility Algorithms for Illumination Computations. 
Computer Graphics (Proc. SIG-GRAPH 93), 239-246. [27] Teller, S., Fowler, C., Funkhouser, T., and Hanrahan, 
P. Par­titioning and Ordering Large Radiosity Computations. Com­puter Graphics (Proc. SIGGRAPH 94), 443-450. 
[28] Wallace, J., Elmquist, K., Haines, E. A Ray Tracing Algorithm for Progressive Radiosity. Computer 
Graphics (Proc. SIG-GRAPH 89), 23, 3, 315-324. [29] Young, D.M. Iterative Solution of Large Linear Systems.Com­puter 
Science and Applied Mathematics. Academic Press, New York, 1971. [30] Zareski, D., Wade, B., Hubbard, 
P. and Shirley, P. Ef.cient Parallel Global Illumination using Density Estimation. 1995 Parallel Rendering 
Symposium. Atlanta, Georgia, October, 1995, 47-54. [31] Zareski, D. Parallel Decomposition of View-Independent 
Global Illumination Algorithms. Master s thesis, Cornell Uni­versity, 1996.     
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237274</article_id>
		<sort_key>353</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Talisman]]></title>
		<subtitle><![CDATA[commodity realtime 3D graphics for the PC]]></subtitle>
		<page_from>353</page_from>
		<page_to>363</page_to>
		<doi_number>10.1145/237170.237274</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237274</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.1.2</cat_node>
				<descriptor>Parallel processors**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.1.2</cat_node>
				<descriptor>Pipeline processors**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.2.1</cat_node>
				<descriptor>Parallel</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.2.1</cat_node>
				<descriptor>Pipeline</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010615</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Logic circuits</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010615</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Logic circuits</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010522.10010526</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Serial architectures->Pipeline computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P135885</person_id>
				<author_profile_id><![CDATA[81100089136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Torborg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311368300</person_id>
				<author_profile_id><![CDATA[81546339256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Kajiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378516</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akeley, K. and T. Jermoluk, "High Performance Polygon Rendering", Proceedings of SIGGRAPH 1988 (July 1988), p239-246.]]></ref_text>
				<ref_id>Ake88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166131</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Akeley, Kurt, "Reality Engine Graphics", Proceedings of SIGGRAPH 1993 (July 1993), p 109-116.]]></ref_text>
				<ref_id>Ake93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808585</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Carpenter, L. "The A-Buffer, an Anti-Aliased Hidden Surface Method", Proceedings of SIGGRAPH 1984, (July 1984), p103-108.]]></ref_text>
				<ref_id>Car84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen, Lance Williams, View interpolation for image synthesis, Proceedings of SIGGRAPH 93, (August 1993), pp. 279-288.]]></ref_text>
				<ref_id>Che94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Shenchang Eric Chen, QuickTime VR--an image based approach to virtual environment navigation, Proceedings of SIGGRAPH 95, (August 1995), pp. 29-38.]]></ref_text>
				<ref_id>Che95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cook, R, "Shade Trees", Proceedings of SIGGRAPH 1984, July 1984, p223-231.]]></ref_text>
				<ref_id>Coo84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cook, R., L. Carpenter, E. Catmull, "The REYES Image Rendering Architecture, Proceedings of SIGGRAPH 1987 (July 1987). p95-102]]></ref_text>
				<ref_id>Coo87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74341</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., J. Poulton, J. Eyles, T. Greer, J. Goldfeather, D. Ellsworth, S. Molnar, G. Turk, B. Tebbs, L. Isreal, "Pixel Planes 5: A Heterogenous Multiprocessor Graphics System Using Processor-Enhanced Memories", Proceedings of SIGGRAPH 89, p79-88.]]></ref_text>
				<ref_id>Fuc89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, P. and J. Lawson, "A Language for Shading and Lighting Calculations", Proceedings of SIGGRAPH 1990, August 1990, p289-298.]]></ref_text>
				<ref_id>Han90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808581</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Levinthal, A., T. Porter, "Chap - a SIMD Graphics Processor", Proceedings of SIGGRAPH 84, p77-82.]]></ref_text>
				<ref_id>Lev84</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Microsoft, "DirectDraw API Specification" and "Direct3D API Specification", Microsoft Corporation, Redmond WA, 1995.]]></ref_text>
				<ref_id>Mic95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Leonard McMillan, Gary Bishop, Plenoptic modeling: an image-based rendering system, Proceedings of SIGGRAPH 95, (August 1995), pp. 39-46.]]></ref_text>
				<ref_id>McM95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>145453</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Molnar, S., "Image Composition Architectures for Real-Time Image Generation", PhD Dissertation, University of North Carolina, 1991.]]></ref_text>
				<ref_id>Mol91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Molnar, S., J. Eyles, J. Poulton, "PixelFlow: High Speed Rendering Using Image Composition", Proceedings of SIGGRAPH 1992 (July 92), p231-240]]></ref_text>
				<ref_id>Mol92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801678</ref_obj_id>
				<ref_obj_pid>800046</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[H. Nishimura, H. Ohno, T. Kawata, LINKS-1: a parallel pieplined multimicrocomputer system for image creation, Proceedings of the 10th Symposium on computer architecture (1983), pp.387-394]]></ref_text>
				<ref_id>Nis83</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Nvidia, various press releases on the Nvidia NV 1 Multimedia Accelerator, Nvidia Corporation, Sunnyvale CA, 1995.]]></ref_text>
				<ref_id>Nvi95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74340</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Michael Potmesil and Eric Hoffert, The PixelMachine: a parallel image computer, Proceedings of SIGGRAPH 89, (July 1989), pp. 69-78.]]></ref_text>
				<ref_id>Pot89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192192</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Regan, M. and R. Pose, "Priority Rendering with a Virtual Reality Address Recalculation Pipeline", Proceedings of SIGGRAPH 1994 (July 94), p. 155-162.]]></ref_text>
				<ref_id>Reg94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Reeves, W., D. Salesin, R. Cooke, "Rendering Antialiased Shadows with Depth Maps", Proceedings of SIGGRAPH 87, p283-291.]]></ref_text>
				<ref_id>Ree87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122733</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Schilling, A. "A New Simple and Efficient Antialiasing with Subpixel Masks", Proceedings of SIGGRAPH 1991 (July 1991), p133-141.]]></ref_text>
				<ref_id>Sch91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Mark Segal, Carl Korobkin, Rolf van Widenfelt, Jim Foran, Paul Haeberli, Fast shadows and lighting effects using texture mapping, Proceedings of SIGGRAPH 92, (July 1992), pp. 249-252.]]></ref_text>
				<ref_id>Seg92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Williams, L., "Casting Curved Shadows on Curved Surfaces", Proceedings of SIGGRAPH 78, p270-274.]]></ref_text>
				<ref_id>Wil78</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130802</ref_obj_id>
				<ref_obj_pid>130745</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Woo, A. in "The Shadow Depth Map Revisited", in Graphics Gems, edited by D. Kirk, Academic Press, Boston, 1992, p338-442.]]></ref_text>
				<ref_id>Woo92</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Talisman: Commodity Realtime 3D Graphics for the PC Jay Torborg James T. Kajiya Microsoft Corporation 
 ABSTRACT A new 3D graphics and multimedia hardware architecture, code­named Talisman, is described which 
exploits both spatial and temporal coherence to reduce the cost of high quality animation. Individually 
animated objects are rendered into independent image layers which are composited together at video refresh 
rates to create the final display. During the compositing process, a full affine transformation is applied 
to the layers to allow translation, rotation, scaling and skew to be used to simulate 3D motion of objects, 
thus providing a multiplier on 3D rendering performance and exploiting temporal image coherence. Image 
compression is broadly exploited for textures and image layers to reduce image capacity and bandwidth 
requirements. Performance rivaling high­end 3D graphics workstations can be achieved at a cost point 
of two to three hundred dollars. CR Categories and Subject Descriptors: B.2.1 [Arithmetic and Logic Structures]: 
Design Styles - Parallel, Pipelined; C.1.2 [Processor Architectures]: Multiprocessors - Parallel processors, 
Pipelined processors; I.3.1 [Computer Graphics]: Hardware Architecture - Raster display devices; I.3.3 
[Computer Graphics]: Picture/Image Generation - Display algorithms.  INTRODUCTION The central problem 
we are seeking to solve is that of attaining ubiquity for 3D graphics. Why ubiquity? Traditionally, the 
purpose of computer graphics has been as a tool. For example, mechanical CAD enhances the designerÕs 
ability to imagine complex three dimensional shapes and how they fit together. Scientific visualization 
seeks to translate complex abstract relationships into perspicuous spatial relationships. Graphics in 
film-making is as a tool that realizes the vision of a creative imagination. Today, computer graphics 
has thrived on being the tool of choice for augmenting the human imagination. However, the effect of 
ubiquity is to promote 3D graphics from a tool to a medium. Without ubiquity, graphics will remain as 
it does today, a tool for those select few whose work justifies investment in exotic and expensive hardware. 
With ubiquity, jaytor@microsoft.com kajiya@microsoft.com Permission to make digital or hard copies of 
part or all of this work or personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
graphics can be used as a true medium. As such, graphics can be used to record ideas and experiences, 
to transmit them across space, and to serve as a technological substrate for people to communicate within 
and communally experience virtual worlds. But before it can become a successful medium, 3D graphics must 
be universally available: the breadth and depth of the potential audience must be large enough to sustain 
interesting and varied content. How can we achieve ubiquity? There are a few criteria: 1) hardware must 
be so inexpensive that anyone who wants it can afford it, 2) there must be a minimum level of capability 
and quality to carry a wide range of applications, and 3) the offering must carry compelling content. 
This paper will treat the first two problems and a novel hardware approach to solving them. There are 
two approaches to making inexpensive graphics hardware. One approach is to make an attenuated version 
of conventional hardware. In the next section we make an analysis of the forces driving the cost of conventional 
graphics architectures. By mitigating some of these costs, one may obtain cheaper implementations with 
more modest performance. Over a dozen manufacturers are currently exploring this approach by cutting 
down on one or another cost factor. The risk of this approach, of course, is that each time one cuts 
cost, one also cuts performance or quality. An alternative approach is to look to new architectures that 
have a fundamentally different character than the conventional graphics pipeline. This is an approach 
pioneered at the high end by the Pixel Planes project [Fuc89], PixelFlow [Mol92], and various parallel 
ray tracing machines [Nis83, Pot89]. At the low end, Nvidia [Nvi95] is offering such a different architecture. 
We present an architecture that very much is in the spirit of this latter path, delivering a high performance, 
high quality graphics system for a parts cost of $200 to $300. The second criterion, quality, must be 
evaluated in terms of the applications and content to be executed by the machine. Here we make a fundamentally 
different assumption from that underlying the conventional graphics pipeline. We believe that the requirements 
and metric of performance for a ubiquitous graphics system to be much different than that for a system 
designed primarily for mechanical CAD. In MCAD the ability to accurately and faithfully display the shape 
of the part is a strict requirement. The metric of performance is often polygons per second, but ultimate 
result is frame rateÑa low-cost system will display at a much slower rate than a high-performance system, 
but both will be able to display the shape accurately with the exactly the same image. One of our central 
assumptions is that in applications and content for ubiquitous graphics this situation is reversed. In 
a system to be used as a medium, rather than as a tool, the ability to smoothly convey motion, to be 
synchronized with sound and video, and to achieve low-latency interaction are critical requirements. 
We believe the fidelity of the shapes, the precise nature of their geometric relationships, and image 
quality are performance metrics. In our architecture we have striven to make it possible for one to always 
be able to interact in real-time, at video frame rates (e.g. 72-85 Hz). The difference between high-cost 
and low-cost systems will be in the fidelity and quality of the images. FUNDAMENTAL FORCES A graphics 
system designer struggles with two fundamental forces: memory bandwidth, and system latency. To achieve 
low­cost, a third force looms large: memory cost. Space considerations do not allow us to detail all 
the bandwidth requirements for a conventional graphics pipeline. The considerations are straightforward: 
for example, simple multiplication shows display refresh bandwidth for a 75 Hz, 640x480x8 frame buffer 
requires 23MB per second, while that for 1024x768x24 requires 169 MB per second. If we add the requirements 
for z-buffering (average depth complexity of 3 with random z-order), texture map reads with various antialiasing 
schemes (point sample, bilinear, trilinear, anisotropic), and additional factors imposed by anti-aliasing, 
we obtain the following chart: 52 48 44 40 36 Memory 32 Capacity 28 Requirement 24 Mbytes 20 16 12 
8 4  640x480x16 bit, 16 bit Z, 2 texels/pixel 8 bit palletized point sampled texture 640x480x16 bit, 
24 bit Z, 3 texels/pixel 16 bit bilinear filtered texture 800x600x16 bit, 24 bit Z, 3 texels/pixel 16 
bit trilinear filtered texture 800x600x24 bit, 24 bit Z, 3 texels/pixel 16 bit trilinear filtered texture 
1024x768x24 bit, 24 bit Z, 3 texels/pixel 16 bit trilinear filtered texture 1024x768x24 bit, 24 bit 
Z, 3 texels/pixel 32 bit trilinear filtered texture 1024x768x32 bit, 24 bit Z, 3 texels/pixel 32 bit 
anisotropic filtered texture 1024x768x32 bit, 24 bit Z, 3 texels/pixel 32 bit anisotropic filtered texture, 
anti-aliased polygon edges Memory Capacity Requirements for Conventional Graphics Pipeline for various 
3D Graphics Performance, Quality, and Resolutions Note that although capacity has improved tremendously, 
latency and bandwidth have not made similar improvements. There is every indication that these trends 
will continue to hold. These charts suggest that achieving high-quality imagery using the conventional 
graphics pipeline is an inherently expensive enterprise. Those who maintain that improvements in CPU 
and VLSI technology are sufficient to produce low-cost hardware or even software systems that we would 
consider high-performance today, have not carefully analyzed the nature of the fundamental forces at 
work. DRAM Technology Improvements 1976 1995 Change Change per Year Access Time 350 ns 50 ns 7X 10% Bandwidth 
(per data pin) 2 Mb/sec 22 Mb/sec 11X 12% Capacity 4 Kbit 16 Mbit 4096X 50% Cost per MByte $ 16,500 $ 
23 720X 40% Memory bandwidth is a key indicator of system cost. The left hand two columns indicate where 
current 3D accelerators for the PC are falling. A full up SGI RE2 a truly impressive machine boasts a 
memory bandwidth of well over 10,000 MB per second. Its quite clear that SGI has nothing to fear from 
evolving PC 3D accelerators, which utilize traditional 3D pipelines, for some time to come. The second 
force, system latency, is handled mainly through careful design of the basic algorithms of the architecture, 
as well as careful pipelining to mask memory latencies. The third force, memory cost, traditionally has 
not been of great concern to high-end systems because achieving the aggregate bandwidth has required 
large amounts of memory. The next chart shows the results of calculating memory requirements for a conventional 
graphics pipeline with different levels of performance. Over the last two decades, the drop in price 
per bit of semiconductor memory has been phenomenal. A look at an early DRAM vs. today s reveals interesting 
trends.  IMAGE PROCESSING AND 3D GRAPHICS Although the conventional graphics pipeline uses massive amounts 
of memory bandwidth to do its job, it is equally clear that much of this bandwidth is creating unused, 
if not unusable, capacity. For example, the conventional pipeline is fully capable of making every frame 
a display of a completely different geometric model at full performance. The viewpoint may skip about 
completely at random with no path coherence at all. Every possible pixel pattern may serve as a texture 
map, even though the vast majority of them are perceptually indistinguishable from random noise. A frame 
may be completed in any pixel order even though polygons tend to occupy adjacent pixels. In our architecture 
we have sought to employ temporal coherence of models, of motion, of viewpoint, and spatial coherence 
of texture and display. We have found that this approach greatly mitigates the need for large memory 
bandwidths and capacities for high-quality systems. A fundamental technique we have used repeatedly is 
to replace image synthesis with image processing. That image processing and 3D graphics have always had 
an intimate theoretical relationship, is evident to anyone perusing the contents of a typical SIGGRAPH 
proceedings. Even in high-quality off-line rendering, image processing and composition has served essential 
functions for many years. But, with a few exceptions like the Pixar Image Computer [Lev84], Regan s image 
remapping system [Reg94], and the PixelFlow architecture [Mol92] this relationship has not extended into 
the physical embodiment of hardware. In a sense, one can view texture mapping as an example of marrying 
images and 3d graphics early in the pipeline. Segal, et. al. [Seg92] have shown that texture mapping, 
especially when considered in context with multiple renderings can simulate many lighting effects. We 
have adopted this idea for the real-time context, calling it multi-pass rendering. Image compositing 
and image morphing have been long used in the utilization of temporal coherence at least in software 
systems, [Coo87, Che94, Che95, McM95]. Our architecture extends these ideas into the real-time hardware 
domain, for the case of affine image transformations.  HARDWARE ARCHITECTURE There are four major concepts 
utilized in Talisman, these are: Composited image layers with full affine transformations.  Image compression. 
 Chunking.  Multi-pass rendering.  Composited Image Layers The Talisman hardware does not incorporate 
a frame buffer in the traditional sense. Instead, multiple independent image layers are composited together 
at video rates to create the output video signal. These image layers can be rendered into and manipulated 
independently. The graphics system will generally use an independent image layer for each non-interpenetrating 
object in the scene. This allows each object to be updated independently so that object update rates 
can be optimized based on scene priorities. For example, an object that is moving in the distant background 
may not need to be updated as often, or with as much accuracy, as a foreground object. Image layers can 
be of arbitrary size and shape, although the first implementation of the system software uses only rectangular 
shaped layers. Each pixel in a layer has color and alpha (opacity) information associated with it so 
that multiple layers can be composited together to create the overall scene. Several different operations 
can be performed on these image layers at video rates, including scaling, rotation, subpixel positioning, 
and skews (i.e., full affine transformations). So, while image layer update rates are variable, image 
layer transformations (motion, etc.) occur at full video rates (e.g. 72 to 85 Hz), resulting in much 
more fluid dynamics than can be achieved by a conventional 3D graphics system that has no update rate 
guarantees. Many 3D transformations can be simulated by 2D imaging operations. For example, a receding 
object can be simulated by scaling the size of the image. By utilizing 2D transformations on previously 
rendered images for intermediate frames, overall processing requirements are significantly reduced, and 
3D rendering power can be applied where it is needed to yield the highest quality results. Thus, the 
system software can employ temporal level of detail management and utilize frame-to-frame temporal coherence. 
 By using image layer scaling, the level of spatial detail can also be adjusted to match scene priorities. 
For example, background objects (e.g., cloudy sky) can be rendered into a small image layer (low resolution) 
which is then scaled to the appropriate size for display. By utilizing high quality filtering, the typical 
low resolution artifacts are reduced. A typical 3D graphics application (particularly an interactive 
game) trades off geometric level of detail to achieve higher animation rates. The use of composited image 
layers allow the Talisman system to utilize two additional scene parameters temporal level of detail 
and spatial level of detail to optimize the effective performance as seen by the user. Further, the Talisman 
system software can manage these trade-offs automatically without requiring application support. Image 
Compression Talisman broadly applies image compression technology to solve these problems. Image compression 
has traditionally not been used in graphics systems because of the computational complexity required 
for high quality, and because it does not easily fit into a conventional graphics architecture. By using 
a concept we call chunking (described below), we are able to effectively apply compression to images 
and textures, achieving a significant improvement in price-performance. In one respect, graphics systems 
have employed compression to frame buffer memory. High end systems utilize eight bits (or more) for each 
of three color components, and often also include an eight bit alpha value. Low end systems compress 
these 32 bits per pixel to as few as four bits by discarding information and/or using a color palette 
to reduce the number of simultaneously displayable colors. This compression results in very noticeable 
artifacts, does not achieve a significant reduction in data requirements, and forces applications and/or 
drivers to deal with a broad range of pixel formats. The compression used in Talisman is much more sophisticated, 
using an algorithm similar to JPEG which we refer to as TREC to achieve very high image quality yet still 
provide compression ratios of 10:1 or better. Another benefit of this approach is that a single high 
quality image format (32 bit true color) can be used for all applications. Chunking A traditional 3D 
graphics system, or any frame buffer for that matter, can be, and usually is, accessed randomly. Arbitrary 
pixels on the screen can be accessed in random order. Since compression algorithms rely on having access 
to a fairly large number of neighboring pixels in order to take advantage of spatial coherence, and only 
after all pixel updates have been made, the random access patterns utilized by conventional graphics 
algorithms make the application of compression technology to display buffers impractical. This random 
access pattern also means that per-pixel hidden surface removal and anti-aliasing algorithms must maintain 
additional information for every pixel on the screen. This dramatically increases the memory size requirements, 
and adds another performance bottleneck. Talisman takes a different approach. Each image layer is divided 
into pixel regions (32 x 32 pixels in our reference implementation) called chunks. The geometry is presorted 
into bins based on which chunk (or chunks) the geometry will be rendered into. This process is referred 
to as chunking. Geometry that overlaps a chunk boundary is referenced in each chunk it is visible in. 
As the scene is animated, the data structure is modified to adjust for geometry that moves from one chunk 
to another. While chunking adds some upstream overhead, it provides several significant advantages. Since 
all the geometry in one chunk is rendered before proceeding to the next, the depth buffer need only be 
as large as a single chunk. With a chunk size of 32 x 32, the depth buffer is implemented directly on 
the graphics rendering chip. This eliminates a considerable amount of memory, and also allows the depth 
buffer to be implemented using a specialized memory architecture which can be accessed with very high 
bandwidth and cleared instantly from one chunk to the next, eliminating the overhead between frames. 
Anti-aliasing is also considerably easier since each 32 x 32 chunk can be dealt with independently. Most 
high-end graphics systems which implement anti-aliasing utilize a great deal of additional memory, and 
still perform relatively simplistic filtering. By using chunking, the amount of data required is considerably 
reduced (by a factor of 1000), allowing practical implementation of a much more sophisticated anti-aliasing 
algorithm. The final advantage is that chunking enables block oriented image compression. Once each 32 
x 32 chunk has been rendered (and anti-aliased), it can then be compressed with the TREC block transform 
compression algorithm.  Multi-pass Rendering One of the major attractions of the Talisman architecture 
is the opportunity for 3D interactive applications to break out of the late 1970 s look of CAD graphics 
systems: boring lambertian Gouraud-shaded polygons with Phong highlights. Texture mapping of color improves 
this look but imposes another characteristic appearance on applications. In the 1980 s, the idea of programmable 
shaders and procedural texture maps[Coo84, Han90] opened a new versatility to the rendering process. 
These ideas swept the off-line rendering world to create the high-quality images that we see today in 
film special effects. By reducing the bandwidth requirements using the techniques outlined above, Talisman 
can use a single shared memory system for all memory requirements including compressed texture storage 
and compressed image layer storage. This architecture allows data created by the rendering process to 
be fed back through the texture processor to be used as data in the rendering of a new image layer. This 
feedback allows rendering algorithms which require multiple passes to be implemented. By coupling multi-pass 
rendering with a variety of compositing modes, texture mapping techniques [Seg92], and a flexible shading 
language, Talisman provides a variety of rendering effects that have previously been the domain of off-line 
software renderers. This includes support of functions such as shadows (including shadows from multiple 
light sources), environment mapped reflective objects, spot lights, fog, ground fog, lens flare, underwater 
simulation, waves, clouds, etc.  REFERENCE HARDWARE IMPLEMENTATION The Talisman architecture supports 
a broad range of implementations which provide different performance, features, rendering quality, etc. 
The reference implementation is targeted at the high-end of the consumer PC market and is designed to 
plug into personal computers using the PCI expansion bus. This board replaces functionality that is typically 
provided by a Windows accelerator board, a 3D accelerator board, an MPEG playback board, a video conferencing 
board, a sound board, and a modem. The reference hardware consists of a combination of proprietary VLSI 
devices and commercially available components. The VLSI components have been developed using a top-down 
modular design approach allowing various aspects of the reference implementation to be readily used to 
create derivative designs. The reference implementation uses 4 Mbytes of shared memory implemented using 
two 8-bit Rambus channels. The Rambus memory provides higher bandwidth than traditional DRAM at near 
commodity DRAM pricing. This shared memory is used to store image layers and texture data in compressed 
form, DSP code and data, and various buffers used to transfer data between processing subsystems. A 2MB 
configuration is also possible, although such a configuration would have lower display resolution and 
would have other resource limitations. The Media DSP Processor is responsible for video codecs, audio 
processing, and front-end graphics processing (transformations, lighting, etc.). The reference HW implementation 
uses the Samsung MSP to perform these functions. The DSP combines a RISC processor with a specialized 
SIMD processor capable of providing high performance floating point and integer processing (>1000 MFLOPS/MOPS). 
A real-time kernel and resource manager deals with allocating the DSP to the various graphics and multimedia 
tasks which are performed by this system. The Polygon Object Processor is a proprietary VLSI chip which 
performs scan-conversion, shading, texturing, hidden-surface removal, and anti-aliasing. The resulting 
rendered image layer chunks are stored in compressed form in the shared memory. The Image Layer Compositor 
operates at video rates to access the image layer chunk information from the shared memory, decompress 
the chunks, and process the images to perform general affine transformations (which include scaling, 
translation with subpixel accuracy, rotation, and skew). The resulting pixels (with alpha) are sent to 
Compositing Buffer. Memory Use -T ypical Scenario Net Memory R equirements Image Layer DataS torage 
Display R esolution Average Image Layer S ize Average Image Layer Depth Complexity Image Layer Data CompressionF 
actor ImageLayer Memory Management Overhead Memory Allocation Overhead T otal Image Layer Data S torage 
R equirements 1024 128 1.7 5 51 bytes 4 bytes x x per per 768 128 32x32 chunk 128 bytes 1,171,637 bytes 
Dis play Memory Management 64 bytes per imagelayer 5,222 bytes T extureData S torage Number of T exels 
Percent T exels with Alpha Avg. Number of T extureLODs T exture Data Compression F actor T otal T exture 
Data S torage R equirements 4,000,000 30% 6 15 texels 1,415,149 bytes Command B uffers 53,248 bytes Audio 
Output B uffer 2,450 bytes Audio S ynthes is Data 32,768 bytes Wav T able B uffer 524,800 bytes Media 
DS P P rogram and S cratch Mem 524,288 bytes T otal 3,729,563 bytes Image layer chunk data is processed 
32 scan lines at a time for display. The Compositing Buffer contains two 32 scan line buffers which are 
toggled between display and compositing activities. Each chip also contains a 32 scan line alpha buffer 
which is used to accumulate alpha for each pixel. The Video DAC includes a USB serial channel (for joysticks, 
etc.), and an IEEE1394 media channel (up to 400 Mbits/sec. for connection to an optional break-out box 
and external A/V equipment), as well as standard palette DAC features. A separate chip is used to handle 
audio digital to analog and analog to digital conversion. The table above indicates the total memory 
usage for a typical 3D application scenario. For the same scenario, the memory bandwidth requirements 
are shown in the following table. Memory Bandwidth - Typical Scenario Pixel Rendering (avg. depth complexity 
2.5) 32.4 Mbytes/sec Display Bandwidth 130.0 Mbytes/sec Texture Reads Texels per Pixel (anisotropic filtering) 
16 Texture Cache Multiplier (avg. texel reuse) 2.5 Texture Read Bandwidth 58 Mbytes/sec Polygon Command 
(30,000 polygons/scene) 61.0 Mbytes/sec Total 3D Pipeline Bandwidth 281.4 Mbytes/sec  POLYGON OBJECT 
PROCESSOR The Polygon Object Processor is one of the two primary VLSI chips that are being developed 
for the reference HW implementation. Unique Functional Blocks Many of the functional blocks in the Polygon 
Object Processor will be recognized as being common in traditional 3D graphics pipelines. Some of the 
unique blocks are described here. The operation of this chip is provided later in the paper. Initial 
Evaluation - Since polygons are processed in 32 x 32 chunks, triangle processing will typically not start 
at a triangle vertex. This block computes the intersection of the chunk with the triangle and computes 
the values for color, transparency, depth, and texture coordinates for the starting point of the triangle 
within the chunk. Pixel Engine - performs pixel level calculations including compositing, depth buffering, 
and fragment generation for pixels which are only partially covered. The pixel engine also handles z-comparison 
operations required for shadows.  Fragment Resolve - performs the final anti-aliasing step by resolving 
depth sorted pixel fragments with partial coverage or transparency.  Coping with Latency One of the 
most challenging aspects of this design was coping with the long latency to memory for fetching texture 
data. Not only do we need to cope with a decompression step which takes well over 100 12.5 ns. cycles, 
but we are also using Rambus memory devices which need to be accessed using large blocks to achieve adequate 
bandwidth. This results in a total latency of several hundred cycles. Maintaining the full pixel rendering 
rate was a high priority in the design, so a mechanism that could ensure that texels were available for 
the texture filter engine when needed was required. The basic solution to this problem is to have two 
rasterizers ­one calculating texel addresses and making sure that they are available in time, and the 
other performing color, depth, and pixel address interpolation for rendering. While these rasterizers 
both calculate information for the same pixels, they are separated by up to several hundred cycles. Two 
solutions were considered for this mechanism - one was to duplicate the address calculations in both 
rasterizers; the other was to pass the texture addresses from the first rasterizer (called the Pre-Rasterizer 
in the block diagram) to the second rasterizer using a FIFO. In this case, texture address calculation 
logic in the rasterizers is fairly complex to deal with perspective divides and anisotropic texture filtering 
(discussed later). To duplicate this logic in both rasterizers required more silicon area than using 
the pixel queue, so the latter approach was chosen. Die Area and Packaging The total die area of the 
Polygon Object Processor is shown in the following table. The die area figures shown here are estimates 
since the layout of this part was not complete at the POP Area Calculation 0.35 Micron Functional Block 
Gates RAM bits Total Area RAC Cell 5.17 Memory Interface 4,500 12,288 1.77 Input Logic 10,044 0 1.09 
Setup Logic 30,920 0 3.92 Scan Convert 125,510 57,760 18.38 Texture Lookup 83,450 0 8.87 Pixel Logic 
86,090 137,216 20.03 Cache Logic 42,000 71,680 10.91 Compression Logic 33,120 32,896 14.62 Decompression 
Logic 47,000 16,000 6.02 90.77 Testability Gates 50,000 6.55 Interblock Routing Area 9.73 Core Area 
107.05 I/O Cells Area 21.69 Total Area 128.75 time of paper submission. The Polygon Object Processor 
is implemented using an advanced 0.35 micron four layer metal 3.3 volt CMOS process. The die is mounted 
in a 304 pin thermally-enhanced plastic package.   IMAGE LAYER COMPOSITOR The Image Layer Compositor 
is the other custom VLSI chip that is being developed for the reference HW implementation. This part 
is responsible for generating the graphics output from a collection of depth sorted image layers.  Comparison 
with Polygon Object Processor You will notice that this block diagram is similar in many ways to the 
Polygon Object Processo. In fact, many of the blocks are identical to reduce design time. In many ways, 
the Image Layer Compositor performs the same operations as triangle rasterization with texture mapping. 
In addition to the obvious differences (no depth buffering, anti­aliasing, image compression, etc.) there 
are a couple of key differences which significantly affect the design: Rendering Rate - the Image Layer 
Compositor must composite the images of multiple objects at full video rates with multiple objects overlapping 
each other. To support this, the rendering rate of the Image Layer Compositor is eight times higher than 
the Polygon Object Processor. Texture/Image Processing - the sophistication of the image processing used 
by the Image Layer Compositor is significantly reduced in order to keep silicon area to a reasonable 
level. Instead of performing perspective correct anisotropic filtering, this chip performs simple bi-linear 
filtering and requires only linear address calculations (since perspective transforms are not supported). 
These differences significantly affect the approach used to deal with memory latency. The rasterizer 
in the Image Layer Compositor is significantly simpler due to the simplified image processing, and the 
higher pixel rate requires the pre-rasterizer to be much further ahead of the rasterizer. As a result, 
the Image Layer Compositor eliminates the Pixel Queue and simply recalculates all the parameters in the 
second rasterizer. Die Area and Packaging The total die area of the Image Layer Compositor is shown 
in the following table. The die area figures shown here are estimates since the layout of this part was 
not complete at the time of paper submission. ILC Area Estimate 0.35 Micron Functional Block Gates RAM 
bits Interface Controller 2,290 2,048 1.40 Initial Evaluation 10,514 0 1.35 Header Registers 7,550 2,048 
1.25 Pre-Rasterizer 21,018 0 2.65 Rasterizer 19,019 0 2.35 Cache Logic 41,350 71,636 18.30 Decompressor 
86,000 25,856 21.00 Filter Engine 21,608 0 4.10 Compositing Buffer Controller 1,200 0 0.25 Testability 
Gates 50,000 6.60 Interblock Routing Area 4.65 Core Area 63.90 I/O Cells Area 16.99 Total Area 80.89 
 The Image Layer Compositor is implemented using an advanced 0.35 micron four layer metal 3.3 volt CMOS 
process. The die is mounted in a 304 pin thermally-enhanced plastic package.  OPERATION This section 
describes the overall operation of the system and discusses some of the key features. Objects and Image 
Layers As in a traditional 3D graphics system, objects are placed in the virtual environment by the application 
specifying their position, orientation, and scale relative to the coordinate system of the virtual environment. 
The transform engine uses this information, in conjunction with the viewpoint specification to construct 
the synthetic scene. In this system, however, independent objects are rendered into separate image layers 
which are composited together at video rates to create the displayed image. Independent objects can be 
described directly by the APIs (in the case of the Windows PC, this is done using DirectDraw and Direct3D 
[Mic95]), or can be calculated automatically based on object hierarchy and bounding boxes. The latter 
approach will likely be used for virtual reality environments described using behavior languages where 
the relative motion of objects can be predicted. In our implementation, the host processor maintains 
control of real-time display operation. The object database is maintained in host memory, and primitive 
descriptions are passed to the graphics system as needed for rendering. Unlike a traditional 3D graphics 
system, the entire display is not updated at the same time. Each image layer can be updated based on 
scene priorities. As previously discussed, an affine image transformation can be applied to each image 
layer at video rates as it is composited. This affine transformation is used to simulate a 3D transformation 
of the object. The appropriate affine transform and the geometric and photometric error that results 
is computed based on a least square error of selected points within the object. The host software maintains 
a priority list of image layers to be updated based on perceptible error and object priorities. More 
error is allowed for objects that are not considered primary, allowing higher quality to be maintained 
for the important elements in the scene. For those objects that are not updated, the system will always 
try to produce the best possible result using affine transforms on previously rendered image layers. 
Image layer transforms can be processed considerably faster than re-rendering the geometry - typically 
10 to 20 times faster. Since the frame to frame changes of animated objects are typically small, an affine 
transformed image of a 3D object can be used for several frames before incurring enough distortion to 
require re-rendering. This gives a tremendous performance advantage to the front end of the graphics 
pipeline; and since the polygon rendering requires significantly more processing resources, the net result 
is significantly higher price­performance. Image Layer Chunking As previously noted, all image layers 
are processed as 32 x 32 pixel regions known as chunks. Prior to sending graphics rendering primitives 
to the Talisman hardware, the host processor must sort the geometry for each image layer into these independent 
regions. After some experimentation, we determined that it was more efficient to sort into 64 x 64 regions 
and allow the hardware to process this geometry for each of the four 32 x 32 chunks in the region. Sorting 
into these 64 x 64 regions can be accomplished using a variety of algorithms. In the simplest case, a 
binary sort is performed based on bounding volumes using an algorithm similar to polygon scanline techniques 
(active lists, etc.). More advanced algorithms utilize incremental algorithms based on the temporal behavior 
of the environment. We have found that we can achieve full performance using static algorithms on a mid­range 
personal computer. Primitive Rendering The Talisman software provides the capability to render independent 
triangles, meshed triangles (strips and fans), lines, and points. All of these primitives are converted 
to triangles for rendering by the Polygon Object Processor. Triangle rendering provides numerous simplifications 
in the hardware since it is always planar and convex. All coordinate transformations, clipping, lighting, 
and initial triangle set-up is handled by the Media DSP using 32 bit IEEE floating point. During scan 
conversion, the Polygon Object Processor uses the linear equation parameters generated by the Media DSP 
to determine if the triangle is visible in the current chunk. The edge equations are also stored in the 
Primitive Registers until required by the Pre-Rasterizer and Rasterizer. As previously discussed, rasterization 
is split into two sections which are separated by several hundred clock cycles. This separation allows 
the first section (the Pre-Rasterizer) to determine which texture blocks will be required to complete 
rendering of the triangle. This information is sent to the Texture Cache Controller so that it can fetch 
the necessary data from the common memory system, decompress it, and move it into the specialized high-speed 
on-chip memory system used by the texture filtering engine, as described below. The second section, the 
Rasterizer, calculates the color, translucency, depth, and coverage information, and passes this to the 
Pixel Engine where it can be combined with the texture information to determine the output pixel color. 
 Texture Mapping Texture data is stored in the common memory system in the TREC compressed image format. 
8x8 blocks are grouped together in 32x32 chunks for memory management purposes, although each 8x8 block 
is individually addressable. The 32x32 chunks are identical in format to the image layer chunks, allowing 
textures and image layers to be used interchangeably (although textures are generally mip-mapped). Data 
is fetched from the common memory in blocks called Memory Allocation Units. Each MAU is 128 bytes, allowing 
high bandwidth utilization from the Rambus DRAMs to be achieved. As texture blocks are needed, they are 
fetched into a compressed texture cache in the Polygon Object Processor. The compressed texture cache 
holds sixteen MAUs. Holding texture data in compressed form increases the effective size of the compressed 
texture cache by the compression factor (typically 15 times). Texture blocks are decompressed as required 
by the texturing engine and placed in an on-chip decompressed texture cache. A total of sixteen 8x8 texture 
blocks are cached in RGB. format. The texture cache allows each texel to be used for an average of 2.5 
pixel calculations. A texture filter kernel is generated on the fly for each pixel, depending on the 
texture resolution, offset, orientation, and Z­slope. The texture processor supports anisotropic filtering 
with up to 2:1 anisotropy at full pixel rendering rates. Higher anisotropy is supported at lower rendering 
rates. The resulting filter quality is considerably better than the tri-linear filtering that is commonly 
used in high-end graphics workstations, and will result in sharper looking images. Tri-linear filtering 
is also available at full pixel rendering rates. Hidden Surface Removal The Polygon Object Processor 
has an on-chip 32 x 32 pixel depth buffer (with 26 bits per pixel) used to store the depth of the closest 
primitive to the eye point at each pixel location. The 26 bit depth value uses between 20 and 24 bits 
for depth with the remaining 2 to 6 bits used for priority and/or stencil. Priority is used to eliminate 
depth buffering artifacts due to coplanar objects. Priority is tagged per surface, and indicates which 
surface should be considered as closer to the viewpoint, if the depth of the two surfaces at the specific 
pixel location are within a certain depth tolerance of each other. The depth tolerance is a fixed value 
which is based on the overall accuracy of the graphics pipeline with regards to computing pixel depth 
values. The Polygon Object Processor also supports translucent triangles, translucent textures, and triangle 
edge anti-aliasing, all of which fall outside of normal depth buffer operations. To properly compose 
pixels which are only partially covered, or have an alpha value less than 1.0, the Talisman system has 
special anti-aliasing hardware, which is described below. Anti-Aliasing One of the significant advantages 
of the chunking architecture used by Talisman is that high quality anti-aliasing can be implemented cost 
effectively, without the need for large memory systems. The algorithm we have implemented is compatible 
with depth buffering and translucent surfaces. The color buffer always stores the pixel value for the 
front most, fully opaque, fully covered pixel at each pixel location in the 32x32 chunk. Pixels with 
partial geometric coverage or which are translucent are contained in a fragment buffer (the basic anti­aliasing 
algorithm is loosely based on the A-Buffer algorithm described by L. Carpenter [Car84]). Each entry in 
the fragment buffer provides the color, alpha, depth, and geometric coverage information associated with 
these pixels. Multiple fragment buffer entries can be associated with a single pixel location within 
the 32x32 chunk. Memory is managed within the fragment buffer using a linked list structure. To represent 
the geometric coverage of a pixel by a polygon edge, a coverage mask is used. In the Talisman system, 
a 4x4 mask is used, which effectively divides each pixel into 16 sub­pixels. The Talisman hardware does 
not store an individual entry for each sub-pixel, as do many high end graphics systems of this quality, 
but instead stores a simple coverage mask which tags which virtual sub-pixels the fragment entry would 
cover. The 4x4 coverage mask is generated using an algorithm similar to the algorithm described by A. 
Schilling [Sch91] in order to provide improved dynamic results for near horizontal and near vertical 
edges. All fragments generated by the scan converter are tested against the 32x32 pixel depth buffer 
prior to being sent to the fragment buffer. This eliminates storage and subsequent processing of any 
fragments which are obscured by a nearer fully covered, fully opaque pixel. To further reduce the number 
of fragments that are stored and subsequently processed, the Pixel Engine implements a merging operation 
for complimentary fragments. In many cases, at a single pixel location, sequential fragments are from 
the mating edges of adjacent polygons which represent a common surface within an object. In these cases, 
the pixel depth and color are often virtually identical, with the only difference between the two fragments 
being the coverage masks. The Pixel Engine compares the incoming fragment to the most recently received 
fragment at that pixel location, and if the depth and color are within predefined limits, combines the 
fragments by ORing the incoming fragment coverage mask with the stored fragment coverage mask, and stores 
the result in place of the stored coverage mask. The rest of the incoming fragment data is discarded, 
eliminating the use of another fragment entry. When this operation occurs, the resulting mask is tested 
to see if full coverage has been reached. If full coverage is achieved, the pixel is moved out of the 
fragment buffer and placed in the 32x32 pixel depth buffer, freeing up additional fragment memory. Once 
all the polygons for the chunk are rendered, the anti­aliasing engine resolves the remaining pixel coverage 
values performing front-to-back compositing of each pixel which has one or more fragments associated 
with it. As a chunk is being resolved, the Rasterizer, Pixel Engine, etc. are rendering the next chunk. 
As fragments are freed up by the resolve process, they are added to the free fragment list where they 
can be used by the next chunk being rendered. Shadows The Pixel Engine supports a full range of compositing 
functions which are useful for multipass rendering. One of the most common uses is for the generation 
of real-time shadows. The Polygon Object Processor implements a shadow buffer algorithm that determines 
which parts of the object are in shadow from a given light source. This is done in three passes, assuming 
a single light source. In the first pass, a shadow buffer is generated by rendering the scene from the 
point of view of the light source [Wil78, Ree87]. The shadow buffer generated from this step is a depth 
buffer, where each pixel depth location is set to the average depth of the two closest surfaces to the 
light source. This is done to eliminate the possibility of the front surface casting a shadow on itself 
due to accuracy errors in the algorithm, and was first described by A. Woo [Woo92]. This shadow buffer 
is then saved in the common memory system. In the second pass, the scene is rendered from the eyepoint, 
generating a normally lit scene from the single light source. In the third pass, the scene is rendered 
from the eye point, to generate the shadow contribution to the image. As the scene is rendered, the shadow 
buffer is accessed using the texture map addressing hardware, and the shadow buffer depth is compared 
with a projection of the surface depth in the shadow buffer coordinate system. If the surface depth is 
behind the shadow buffer depth, it is in shadow. For each pixel in the eyepoint scene, a 4x4 or 8x8 set 
of nearest shadow buffer locations are visited, each producing an in-front or behind result. A trapezoid 
weighted filter is multiplied against the results of the shadow depth compares, giving a resulting shadow 
factor, ranging from 0 to 1.0. The trapezoid filter gives a soft, anti-aliased edge to the shadow in 
the resulting scene. The shadow factor is used to attenuate the pixel intensity from the scene rendered 
in pass two. Display Image Generation A data structure defining the image layer display list (sorted 
in depth order) is stored in shared memory and is traversed every frame time to determine how the image 
layers are displayed. This data structure can be modified by the host at any time to control the temporal 
behavior of each image layer (and hence each independently rendered object). Image layer compositing 
is performed 32 scanlines at a time. The Image Layer Compositor traverses the image layer data structure 
(sorted in front to back order), performing image transforms on those chunks that are visible in each 
32 scanline band. The Compositing Buffer is a specialty memory device developed for the Talisman architecture. 
This part has two 32 scanline color buffers (24 bit RGB) and a 32 scanline alpha buffer. One of the color 
buffers is used for compositing into while the other is being used for streaming the video data out to 
the monitor. The two buffers ping-pong back and forth so that as one scanline region is being displayed, 
the next is being composited. The single 32 scanline alpha buffer is used only for compositing and is 
cleared at the start of each new 32 scanline region. The Image Layer Compositor (which does the image 
layer chunk addressing, decompressing, and image processing), passes the color data and alpha data to 
the Compositing Buffer for each pixel to be composited. Since the sprites are processed in front to back 
order, the alpha buffer can be used to accumulate opacity for each pixel, allowing proper anti-aliasing 
and transparency.  FUNCTIONALITY AND PERFORMANCE The features and major performance goals for the reference 
implementation are: Single PCI board implementation with audio, video, 2D and 3D graphics.  High resolution 
display capability - 1344 x 1024 @ 75 Hz.  24 bit true-color pixel data at all resolutions for maximum 
image fidelity.  Optimized for 3D animation at full refresh rates (75 Hz) using a combination of image 
layer animation and 3D rendering. Scene complexity of 20,000 to 30,000 rendered polygons or higher can 
be supported. This is comparable to a 3D graphics workstation capable of 1.5 to 2 million polygons per 
second.  Polygon Object Processor pixel rendering rate of 40 Mpixels per second with anisotropic texturing 
and anti­aliasing. Image Layer Compositor pixel compositing rate of 320 Mpixels/sec.  Very high quality 
image generation incorporating anisotropic texture filtering, subpixel-filtered anti-aliasing, translucent 
surfaces, shadows, blur, fog, and custom shading algorithms.  Front-end geometry processor to off-load 
transformations, clipping, lighting, etc.  Full resolution (720 x 486) MPEG-2 decode, as well as other 
video codecs. Video can be used as surface textures, and can be combined with graphics animations.  
Base system has two-channel 16-bit audio inputs and outputs with DSP based MIDI synthesis (wave table 
and other mechanisms supported), 3D spatialization, and digital audio mixing. Other audio processing 
is also supported.  CONCLUSIONS The Talisman architecture demonstrates how a fresh look at 3D animation 
hardware can result in dramatic improvements in price-performance. The system described in this paper 
has a bill of materials of $200 to $300, yet can achieve performance and quality comparable or superior 
to high-end image generators and 3D graphics workstations. The first reference implementation is a high-end 
PC board level system with adequate functionality to meet a broad range of applications. The goal of 
the Talisman architecture is to significantly improve the quality, performance, and integration of media 
technologies on the PC. This first implementation is intended to be a realization of this goal at a price 
point that is viable for consumer applications, and to be a reference from which derivative designs can 
be created. Although the reference implementation or other products based on this technology are not 
yet on the market, we believe that products based on the architecture described in this paper will have 
retail street prices of $200 to $500. The Talisman architecture has been fully simulated and Verilog 
models are complete. We expect prototype implementations of this hardware by late this year. ACKNOWLEDGMENTS 
The authors would like to thank the entire Talisman research and develop team for their contributions 
to this program. We would specifically like to thank Jim Blinn, Joe Chauvin, Steve Gabriel, Howard Good, 
Andrew Glassner, Kent Griffin, Bruce Johnson, Mark Kenworthy, On Lee, Jed Lengyel, Nathan Myhrvold, Larry 
Ockene, Bill Powell, Rob Scott, John Snyder, Mike Toelle, Jim Veres, and Turner Whitted for their contributions 
to the HW architecture, algorithms, and demos, although many others also contributed. REFERENCES [Ake88] 
Akeley, K. and T. Jermoluk, High Performance Polygon Rendering , Proceedings of SIGGRAPH 1988 (July 1988), 
p239-246. [Ake93] Akeley, Kurt, Reality Engine Graphics , Proceedings of SIGGRAPH 1993 (July 1993), p109-116. 
[Car84] Carpenter, L. The A-Buffer, an Anti-Aliased Hidden Surface Method , Proceedings of SIGGRAPH 1984, 
(July 1984), p103-108. [Che94] Shenchang Eric Chen, Lance Williams, View interpolation for image synthesis, 
Proceedings of SIGGRAPH 93, (August 1993), pp. 279-288. [Che95] Shenchang Eric Chen, QuickTime VR an 
image based approach to virtual environment navigation, Proceedings of SIGGRAPH 95, (August 1995), pp. 
29-38. [Coo84] Cook, R, Shade Trees , Proceedings of SIGGRAPH 1984, July 1984, p223-231. [Coo87] Cook, 
R., L. Carpenter, E. Catmull, The REYES Image Rendering Architecture, Proceedings of SIGGRAPH 1987 (July 
1987). p95-102 [Fuc89] Fuchs, H., J. Poulton, J. Eyles, T. Greer, J. Goldfeather, D. Ellsworth, S. Molnar, 
G. Turk, B. Tebbs, L. Isreal, Pixel Planes 5: A Heterogenous Multiprocessor Graphics System Using Processor-Enhanced 
Memories , Proceedings of SIGGRAPH 89, p79-88. [Han90] Hanrahan, P. and J. Lawson, A Language for Shading 
and Lighting Calculations , Proceedings of SIGGRAPH 1990, August 1990, p289-298. [Lev84] Levinthal, A., 
T. Porter, Chap - a SIMD Graphics Processor , Proceedings of SIGGRAPH 84, p77-82. [Mic95] Microsoft, 
DirectDraw API Specification and Direct3D API Specification , Microsoft Corporation, Redmond WA, 1995. 
[McM95] Leonard McMillan, Gary Bishop, Plenoptic modeling: an image-based rendering system, Proceedings 
of SIGGRAPH 95, (August 1995), pp. 39-46. [Mol91] Molnar, S., Image Composition Architectures for Real-Time 
Image Generation , PhD Dissertation, University of North Carolina, 1991. [Mol92] Molnar, S., J. Eyles, 
J. Poulton, PixelFlow: High Speed Rendering Using Image Composition , Proceedings of SIGGRAPH 1992 (July 
92), p231-240 [Nis83] H. Nishimura, H. Ohno, T. Kawata, LINKS-1: a parallel pieplined multimicrocomputer 
system for image creation, Proceedings of the 10th Symposium on computer architecture (1983), pp.387-394 
[Nvi95] Nvidia, various press releases on the Nvidia NV1 Multimedia Accelerator, Nvidia Corporation, 
Sunnyvale CA, 1995. [Pot89] Michael Potmesil and Eric Hoffert, The PixelMachine: a parallel image computer, 
Proceedings of SIGGRAPH 89, (July 1989), pp. 69-78. [Reg94] Regan, M. and R. Pose, Priority Rendering 
with a Virtual Reality Address Recalculation Pipeline , Proceedings of SIGGRAPH 1994 (July 94), p. 155-162. 
[Ree87] Reeves, W. , D. Salesin, R. Cooke, Rendering Anti­aliased Shadows with Depth Maps , Proceedings 
of SIGGRAPH 87, p283-291. [Sch91] Schilling, A. A New Simple and Efficient Anti­aliasing with Subpixel 
Masks , Proceedings of SIGGRAPH 1991 (July 1991), p133-141. [Seg92] Mark Segal, Carl Korobkin, Rolf van 
Widenfelt, Jim Foran, Paul Haeberli, Fast shadows and lighting effects using texture mapping, Proceedings 
of SIGGRAPH 92, (July 1992), pp. 249-252. [Wil78] Williams, L., Casting Curved Shadows on Curved Surfaces 
, Proceedings of SIGGRAPH 78, p270-274. [Woo92] Woo, A. in The Shadow Depth Map Revisited , in Graphics 
Gems, edited by D. Kirk, Academic Press, Boston, 1992, p338-442. SAMPLE This sample image, and the one 
shown on the frontispiece of the 1996 SIGGRAPH Proceedings were generated using a bit and cycle accurate 
simulator of the Talisman reference hardware. Both of these images are single frames from an animation 
that will be rendered in realtime on the Talisman hardware.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237275</article_id>
		<sort_key>365</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[VC-1]]></title>
		<subtitle><![CDATA[a scalable graphics computer with virtual local frame buffers]]></subtitle>
		<page_from>365</page_from>
		<page_to>372</page_to>
		<doi_number>10.1145/237170.237275</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237275</url>
		<keywords>
			<kw><![CDATA[demand paging]]></kw>
			<kw><![CDATA[frame buffers]]></kw>
			<kw><![CDATA[parallel polygon rendering]]></kw>
			<kw><![CDATA[scalable]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.1.2</cat_node>
				<descriptor>Multiple-instruction-stream, multiple-data-stream processors (MIMD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.1.2</cat_node>
				<descriptor>Parallel processors**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.3.2</cat_node>
				<descriptor>Virtual memory</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528.10010531</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures->Multiple instruction, multiple data</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010607.10010608</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Semiconductor memory->Dynamic memory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P260486</person_id>
				<author_profile_id><![CDATA[81540723456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Satoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Aizu, Tsuruga, Ikki-machi, Aizu-Wakamatsu City, Fukushima, 965-80, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14153096</person_id>
				<author_profile_id><![CDATA[81100435002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tosiyasu]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Kunii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Aizu, Tsuruga, Ikki-machi, Aizu-Wakamatsu City, Fukushima, 965-80, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166131</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AKELEY, K. RealityEngine Graphics. Proceedings of SIG- GRAPH '93 (August 1993), 109-116.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[COHEN, D., AND DEMETRESCU, S. A VLSI approach to Computer Image Generation. Tech. rep., Information Sciences Institute, University of Southern California, 1979.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>220568</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cox, M. Algorithms for Parallel Rendering. PhD thesis, Dept. of Computer Science, Princeton University, 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378468</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DEERING, M., WINNER, S., SCHEDIWY, B., DUFFY, C., AND HUNT, N. The Triangle Processor and Normal Vector Shader: A VLSI System for High Performance Graphics. ACM Computer Graphics 22, 4 (August 1988), 21-30.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91437</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[ELLSWORTH, D., GOOD, H., AND TEBBS, B. Distributing Display Lists on a Multicomputer. ACM Computer Graphics 24, 2 (March 1990), 147-154.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[HAEBERLI, P., AND AKELEY, K. The Accumulation Buffer: Hardware Support for High-Quality Rendering. ACM Computer Graphics 24, 4 (August 1990), 309-318.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[KUBOTA COMPUTER INC. TITAN2 Technical Overview, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108357</ref_obj_id>
				<ref_obj_pid>108345</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, S. Combining Z-buffer Engines for Higher-Speed Rendering. In Advances in Computer Graphics Hardware III (1988), Springer Verlag, pp. 171-182.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>182469</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, S., Cox, M., ELLSWORTH, D., AND FUCHS, H. A sorting classification of parallel rendering. IEEE Computer Graphics and Applications 14, 4 (July 1994), 23-32.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134067</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[MOLNAR, S., EYLES, J., AND POULTON, J. PixelFlow: High-Speed Rendering Using Image Composition. ACM Computer Graphics 26, 2 (July 1992), 231-240.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[NISHIMURA, S. A Parallel Architecture for Computer Graphics Based on the Conflict-Free Multiport Frame Buffer. Doctoral dissertation, Dept. of Information Science, Faculty of Science, The University of Tokyo, 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[PALOVUORI, K. An Implementation of an Linearly Expandable Graphics Processing Architecture. Tech. Rep. 8-94, Dept. of Electrical Engineering, Tampere University of Technology, Finland, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[ROMAN, G. C., AND KIMURA, T. VLSI perspective of real-time hidden-surface elimination. Computer-Aided Design 13, 2 (March 1981), 99-107.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>58990</ref_obj_id>
				<ref_obj_pid>58985</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[SCHNEIDER, B.-O. A Processor for an Object-Oriented Rendering System. Computer Graphics Forum 7, 4 (1988), 301-310.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108358</ref_obj_id>
				<ref_obj_pid>108345</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[SHAW, C. D., GREEN, M., AND SCHAEFFER, J. A VLSI Architecture for Image Composition. In Advances in Computer Graphics Hardware III (1988), Springer Verlag, pp. 183-199.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806789</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[WEINBERG, R. Parallel Processing Image Synthesis and Anti-Aliasing. ACM Computer Graphics 15, 3 (August 1981), 55-62.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 VC-1: A Scalable Graphics Computer with Virtual Local Frame Buffers Satoshi Nishimura Tosiyasu L. Kunii 
The University of Aizut Abstract The VC-1 is a parallel graphics machine for polygon rendering based 
on image composition. This paper describes the architec­ture of the VC-1 along with a parallel polygon 
rendering algorithm for it. The structure of the VC-1 is a loosely-coupled array of 16 general-purpose 
processors, each of which is equipped with a local frame buffer. The contents of the local frame buffers 
are merged in real time for generating the .nal image. The local frame buffers are virtualized with a 
demand-paging technique, by which the im­age memory capacity for each local frame buffer is reduced to 
one eighth of full-screen capacity. Polygons are rendered in either pixel parallel or polygon parallel 
depending on the on-screen area of each polygon. The real performance of the VC-1 as well as estimated 
performance for systems with up to 256 processors is shown. CR Categories and Subject Descriptors: C.1.2 
[Processor Ar­chitectures]: Multiprocessors -MIMD, Parallel processors; B.3.2 [Memory Structures]: Design 
Styles -Virtual memory; I.3.1 [Com­puter Graphics]: Hardware Architecture -Parallel processing, Raster 
display devices; I.3.3 [Computer Graphics]: Picture/Image Generation -Display algorithms; I.3.7 [Computer 
Graphics]: Three-Dimensional Graphics and Realism -Visible line/surface al­gorithms. Additional Key Words 
and Phrases: demand paging, frame buffers, parallel polygon rendering, scalable. 1 Introduction The 
increasing demand for real-time generation of high-quality im­ages puts more and more emphasis on the 
importance of scalability in high-end graphics machines. In mechanical CAD or virtual re­ality applications, 
solid models composed of more than one million polygons are often used for generating high-quality images. 
To dis­play such models at the rate of 30 frames per second, more than 30 million polygons need to be 
processed in one second, which requires hundreds of processors with current technology. Thus, scalability 
is the most important issue in order to satisfy such a demand. The image composition architecture proposed 
by Molnar [8] is the most promising candidate for future polygon-rendering ma­chines because of its linear 
scalability. According to Molnar et al. [9], graphics machine architectures can be categorized into three 
types. The .rst category, called sort-.rst, performs both geometry tTsuruga, Ikki-machi, Aizu-Wakamatsu 
City, Fukushima, 965-80, Japan. Email: nisim@u-aizu.ac.jp, kunii@u-aizu.ac.jp Permission to make digital 
or hard copies of part or all of this work or personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
calculation and rasterization in pixel parallel, while the second cat­egory, named sort-middle, executes 
geometry calculation in object parallel and rasterization in pixel parallel. The sort-middle cate­gory, 
to which most of today s commercial machines including Sil­icon Graphics RealityEngine [1] belong, has 
limits on the number of processors since its communication network becomes bottlenecked on a large-scale 
system when the results of geometry calculation are sent to every rasterization processor. The sort-.rst 
category has a similar problem. The last category, called sort-last, performs both geometry calculation 
and rasterization in object parallel, and is pos­sibly scalable since the required bandwidth of its communication 
network is almost constant against the number of polygons. The sort-last category can be further divided 
into two classes based on which set of data is transmitted via the communication network. In the .rst 
class, each rasterization processor sends only the pix­els generated [7]. This class is hard to scale 
because implementing a scalable communication network for this class is dif.cult. The second class of 
the sort-last category is the image composition ar­chitecture, in which case each processor outputs all 
the pixels on the screen. With image composition architecture, a linearly-structured communication network 
can be used, and therefore, there is no dif­.culty with increasing the number of processors. The history 
of image composition architecture descends from Cohen and Demetrescu s proposal [2]. Various modi.cations 
or ad­ditions are later applied by many researchers; for example, the inte­gration of a geometry calculation 
unit in each processor [13], anti­aliasing by generating a depth-sorting list of pixel values [14, 16], 
anti-aliasing by alpha-blending [15], and the implementation of Phong shading on this architecture [4]. 
Molnar s architecture dif­fers from these architectures in that each processor can handle plu­ral polygons 
while the architectures listed above can handle just one polygon per processor. Figure 1 depicts Molnar 
s image composition architecture. Each processor has its own full-screen frame buffer, called a local 
frame buffer, which holds a subimage (including Z-values) generated by the processor. This subimage possibly 
overlaps with the subimages generated by other processors. The contents of all the local frame buffers 
are merged periodically by the image merger at the speed of the CRT scan. During merging, depth values 
are taken into account in order to accomplish hidden surface removal. Palovuori [12] has implemented 
a system based on this architecture. One of the problems with this architecture lies in the memory cost 
for local frame buffers. Since frame buffer bandwidth is one of the critical factors of graphics machines, 
frame buffer memory should be fast as possible. However, enabling fast memory of full­screen capacity 
for each processor is impractical if the number of processors is large. Therefore, the use of low-speed 
memories is imposed, which will degrade the system s performance. One possible method for overcoming 
this problem is the region­based approach utilized in the PixelFlow system [10]. In this ap­proach, the 
screen is divided into several regions, and each local frame buffer holds the pixel information only 
for one region. To hold pixel information for the entire screen, a global frame buffer is placed between 
the image merger and the CRT. Prior to rasteriza­tion, each processor performs geometry calculations 
and classi.es polygons according to regions using xy-buckets. Then, the poly­gons are rasterized in multiple 
passes: for each pass, images for one region are generated on the local frame buffers, and then, the 
images are merged and stored into the appropriate section on the Rendering Processors Local Frame Buffers 
CRT Figure 1: Image composition architecture global frame buffer. With this method, the memory requirement 
for the local frame buffer is reduced in inverse proportion to the number of regions. However, it suffers 
from the following disadvantages: 1. The load balancing problem becomes more dif.cult. In the full-screen 
approach, it is enough to execute global syn­chronization once per frame. In the region-based approach, 
global synchronization is necessary for each pass. Therefore, processor idle time increases since a processor 
can not start the processing in the next region until all the other processors .nish processing the current 
region. 2. The clipping time increases. Polygon clipping occurs not only with the screen boundary but 
also with region boundaries.  In order to solve these problems, we propose an alternative ap­proach 
called a virtual local frame buffer for reducing the frame buffer memory requirement. Section 2 describes 
the principle of the virtual local frame buffer. Section 3 presents a prototype machine called the VC-1 
developed for justifying the virtual local frame buffer. Section 4 discusses a parallel polygon rendering 
algorithm suitable for the VC-1 architecture. Section 5 shows the results of performance measurement 
and compares the virtual frame buffer approach with the region-based approach. Section 6 discusses the 
scalability of the VC-1 architecture. Section 7 concludes this paper with some suggestions for future 
research. 2 Virtual Local Frame Buffer Access to the local frame buffer (LFB) tends to have space locality. 
In other words, a processor seldom generates an image .lling the entire screen. In most cases, only a 
part of the screen is accessed. The virtual local frame buffer (VLFB) utilizes this characteristic to 
reduce the memory requirement. We use a demand-paging tech­nique so that the LFB can virtually hold the 
pixel information of the entire screen. The screen is divided into small equal-sized rectan­gular regions 
called patches, and memory units are allocated only to such patches that the processor has accessed (Figure 
2). To im­plement this, two types of memories are used: image memory and a patch table. Image memory 
holds pixel information and its capac­ity is less than full-screen capacity. The patch table maintains 
the access existence and image memory address of every patch. In most cases, generated images can be 
completely stored in im­age memory. However, when access locality is weak, it is possible that image 
memory is exhausted. This situation is called a local frame buffer over.ow (LFB over.ow). To handle this, 
the global frame buffer (GFB) in Figure 1, which is a full-screen frame buffer holding both color and 
depth information, is necessary. The GFB is cumulative: it stores pixel values only when a new pixel 
value is closer to the viewer than the one previously stored in the GFB. Image Memory Figure 2: Virtual 
local frame buffer When an LFB over.ow occurs in a processor, the processor sus­pends rendering until 
a certain number of patches in the LFB are transferred to the GFB, and then it continues rendering. Since 
the GFB is cumulative, the patches can be reused to store a different part of the screen once its contents 
are transferred to the GFB. There are two concerns with the VLFB. One is that LFB access time may increase 
because two memories, the patch table and image memory, are accessed during each access operation. However, 
by organizing a two-stage pipeline consisting of patch table access and image memory access, the access 
time increase will be mitigated. The other concern is processor idle time due to LFB over.ows. In our 
implementation, true real-time image generation no longer be­comes possible if an LFB over.ow occurs 
in some processor. How­ever, LFB over.ows are nearly avoidable with the careful consider­ation of the 
image memory capacity as well as a software technique called adaptive parallel rasterization described 
in Section 4.2.  3 The VC-1 The VC-1 (Figures 3, 10 and 11) is a loosely-coupled multiprocessor with 
a frame buffer subsystem containing the VLFBs. The primary purposes of the VC-1 are to evaluate our architecture 
and also to provide an environment for developing parallel software for the ar­chitecture. Rendering 
speed is not a primary design goal; instead, .exibility is maximized. For this reason, the VC-1 has no 
special hardware accelerators for rasterization. 3.1 Processing Elements The VC-1 has 16 processing elements, 
each of which contains the Intel i860 processor operating at 40MHz together with an 8MB local memory. 
The peak .oating-point operation performance of the VC­1 is 1.3 GFLOPS. This high performance makes it 
possible to apply the VC-1 not only to polygon rendering but also to ray tracing or other numerical applications. 
The processing elements are connected by both point-to-point communication links and a broadcast bus. 
The point-to-point com­munication links are organized in a modi.ed 2D-torus topology and able to exchange 
data between any pair of processors or between a processor and the host computer at the speed of 20Mbits/sec. 
The broadcast bus can transmit data from either the host computer or one of the processors to all the 
processors. The peak bandwidth of the broadcast bus is 27Mbytes/sec. With polygon rendering, only the 
broadcast bus is utilized; communication links are reserved for other algorithms. The sync line is a 
1-bit line realizing global synchronization across all the processors. The host computer can optionally 
par­ticipate in the global synchronization.  3.2 The Host Computer The host computer is an IBM-compatible 
personal computer equipped with communication interfaces for the VC-1. All the data Figure 3: The structure 
of the VC-1  Display  necessary for calculation, including i860 program codes or polygon data, initially 
reside in the hard disk of the host computer and are loaded to the local memory of the processors via 
the broadcast bus. The host computer has a direct access path to the global frame buffer. This is useful 
for dumping created images or displaying a cursor on the screen, for example. 3.3 Frame Buffer Subsystem 
 Local Frame Buffer The LFB is double-buffered so that the processor can generate the image of the next 
frame in parallel with the image composition of the previous frame. One buffer is called the front buffer, 
which is accessed by the processor, and the other is called the back buffer, of which the contents are 
transmitted to the image merger. These buffers are swapped at the end of each frame generation as well 
as at the LFB over.ow. Figure 4 shows a block diagram of the LFB unit. The color and depth values are 
stored in image memory consisting of high-speed static RAMs. The image memory capacity for each buffer 
is 1/4.7 of full-screen capacity. The address space of the image memory is divided into pages, each of 
which has the capacity for storing a patch. Each page is assigned a unique sequential number called a 
page identi.er. The patch table is a high-speed static RAM that maintains the sta­tus of every patch. 
The status consists of a 1-bit .eld, called a page existence .ag, and a 12-bit .eld, called a page ID 
.eld. The page existence .ag indicates whether an image memory page is allocated or not. The page ID 
.eld contains the identi.er of the associated im­age memory page. Patch size options include 424, 828, 
and 16216 pixels. Usually, the 424-pixel con.guration yields the best perfor­mance. Other patch sizes 
are designed for evaluation purposes. The scan counter is a free-running counter that controls the trans­mission 
from the LFB to the image merger. The counter value ad­vances according to the raster scan order; it 
starts from the upper­left corner of the screen and ends at the lower-right corner. When it reaches the 
end, it restarts immediately at the beginning. The clock and reset signals of this counter come from 
the GFB unit. The page ID generator (PIG) is a counter used for the automatic allocation of a new image 
memory page. Its value indicates the identi.er of the next free page. The LFB access operations are performed 
through a two-stage CPU Data Bus Figure 4: Local frame buffer pipeline. The .rst stage reads the patch 
table and examines the page existence .ag. If the .ag is false, the .rst stage also updates the page 
table entry with the PIG value, sets the page existence .ag to true, and increments the PIG. The second 
stage accesses the image mem­ory. In the .rst stage, it takes 4 CPU clocks if the page existence .ag 
is false; otherwise, it takes 3 clocks. The second stage always takes 3 clocks. Therefore, LFB access 
operations can be executed at intervals of 3 or 4 clocks. For more detailed descriptions about LFBs, 
see [11].  Pipelined Image Merger The pipelined image merger (PIM) periodically merges the images stored 
in the LFBs according to the raster scan order. The PIM con­sists of a linearly-connected array of per-processor 
elements called merging units (MUs). Each MU receives two pixel values and out­puts whichever one has 
the smallest screen depth. When a corre­sponding image memory page is not resident on an LFB, the LFB 
sends a possible maximum depth value to its MU, which makes the MU forward the pixel value from the neighboring 
unit. Each MU performs a depth comparison and data selection through a 4-stage pipeline. Therefore, the 
PIM, as a whole, con­structs a 4 -stage pipeline, where represents the number of pro­cessors. To realize 
pipeline processing, the scan counter in each Host computer bus  Figure 5: Global frame buffer unit 
 LFB is advanced by 4 pixels in comparison with the succeeding LFB. The time needed for merging one full-screen 
image (denoted by T.) is equal to (M+()1N1t+41 1t, where Mand Nare respectively horizontal/vertical screen 
sizes, tis the PIM clock pe­riod, and (is the number of PIM clocks required for per-scanline overhead 
processing in the GFB unit. The .rst term represents the time for scanning the entire screen, and the 
second term represents the pipeline delay time. Obviously, T.should be less than the target frame interval 
(33.3ms in our case). In the VC-1, those parameters are M=640N=480=6(=0 and t=80nsec., and therefore, 
T.= msec. Global Frame Buffer There are two purposes for inserting the GFB between the PIM and the 
CRT display: to deal with the LFB over.ow, and  to make the PIM raster scan asynchronous with the 
CRT raster scan. This is preferable because the scanning frequency can be set independently to a value 
convenient for each unit, and also because the image composition can be continued even during the blanking 
periods of the CRT scan.  Figure 5 shows the structure of the GFB unit. The GFB is double­buffered: 
one buffer is for CRT refresh, and the other buffer receives images from the PIM. Double-buffering prevents 
incomplete im­ages from appearing on the CRT. Both of the buffers have full­screen capacity and hold 
both color and depth information. The roles of these buffers are alternated at the end of image generation. 
In addition to the GFBs, there is another full-screen buffer called an accumulation buffer, which implements 
anti-aliasing with multi­pass accumulation [6]. The merging unit for the accumulation buffer (MUACC) 
is used to accumulate the image stored in one of the GFBs into the accumulation buffer. The GFBs and 
accumulation buffer are implemented with triple­port DRAMs. The two serial access ports of the triple-port 
DRAMs are used for data transmissions with the PIM or to refresh the CRT, while the random access port 
is utilized for direct access from the host computer. Fast Screen Clearing In the VC-1, the LFBs are 
cleared as follows. The image mem­ory is automatically cleared by hardware while it functions as the 
back buffer. When the back buffer is read out according to the scan CRT V-Blank CRTNode 1 Node 2 Node 
3 ... GFB display(60Hz) Frame No. 1  Drawing No. 2 LFB swap wait Frame  LFB swap and patch table clear 
LFB overflow Frame No. 1 GFB write Frame No. 3 GFB swap Tm: Merging time (25.3ms) Frame No. 2 Frame No. 
4 Frame Frame No. 5 No. 3 Figure 6: A timing diagram for buffer swapping counter, it is cleared immediately 
after each read operation. As for the patch table, the processor must clear all the patch table entries 
with software after each LFB swap. Explicit clearance of the GFB is avoided in the following way. During 
the .rst round of the image merging scan after the GFB is swapped, blank pixel values are sent to the 
PIM input (i.e., to the most left MU in Figure 3) so that the old contents of the GFB are replaced. After 
the .rst round is completed, the GFB becomes cu­mulative by sending the old contents of the GFB to the 
PIM input.  Buffer Swap Control A timing diagram for buffer swapping is illustrated in Figure 6. When 
each processor completes image generation, it waits for all of the other processors to complete their 
tasks using global synchro­nization, and then it swaps its LFB (@ 1 ). After this swapping, the processor 
immediately clears the patch table and starts generating the next frame (@ 2 ). Parallel with this, the 
contents of back buffers are merged into the GFB (@ 3 ). When the merge is completed (i.e., T.has passed 
since the last LFB swap), the host computer swaps the GFB after synchronization with the CRT vertical 
blanking pe­riod (@ 4 ). When image generation is completed earlier than the GFB swap for the previous 
frame, the LFB swap is postponed until the comple­tion of the GFB swap (@ 5 ). Without this control, 
the current frame s image will be mixed with the previous frame s image. An LFB swap on a processor is 
also blocked if T.has not passed since the last LFB swap on that processor; otherwise, the unsent contents 
in the back buffer will be lost. When an LFB over.ow occurs in a particular processor (@ 6 ), an extra 
LFB swap is executed on that processor (@ 7 ). Like a normal LFB swap, this LFB swap is also postponed 
until both of the condi­tions mentioned above are met. Then, the processor clears its patch table and 
resumes drawing (@ 8 ). A processor s worst-case idle time upon an LFB over.ow is 33.3ms. We could reduce 
this time by utilizing the back buffer s image memory pages immediately after they are sent to the GFB 
rather than waiting until all the contents in the back buffer are transferred; however, this greatly 
complicates LFB hardware.  4 Parallel Polygon Rendering on the VC-1 An outline of the rendering routine 
on the VC-1 is as follows. First, the scene database is transmitted from the host computer to each processor 
s local memory via the broadcast bus. Polygon data are partitioned into subsets, each of which is loaded 
to a different pro­cessor s local memory. Other database components, such as the eye position or light 
source information, are duplicated to all of the pro­cessors local memory. Next, the host computer broadcasts 
a com­mand packet to all the processors, by which each processor starts ge­ometric calculations and rasterization 
for assigned polygons. When the image generation is completed, frame buffers are swapped as described 
previously. In generating animated images, the scene database on each pro­cessor s local memory is incrementally 
modi.ed, i.e., only the inter­frame difference of the database is transmitted between frames rather than 
retransmitting the whole database. For example, if the eye position has changed between frames, only 
the viewing matrix is transmitted. 4.1 Polygon Assignment The mapping of polygons to processors is one 
of the key factors in.uencing system performance. Three points have to be consid­ered in relation to 
this problem: (1) load balancing, (2) the reduc­tion of duplicated polygon vertices among processors, 
and (3) the minimization of LFB over.ows. As for (1), it is not enough for rea­sonable load balancing 
only to keep the number of polygons even, since the processing time of a polygon depends on its size 
and clip­ping conditions. Problems (1) and (2) have been discussed by some researchers [3, 5]; however, 
(3) is a new problem speci.c to our architecture. The simplest way to achieve load balancing is to allocate 
inde­pendent polygons one by one to the most lightly loaded processor at each moment. If there is more 
than one choice, the target processor is selected randomly among them. This method is advantageous in 
(1); however, it is unacceptable with respect to (2) because common vertices shared by neighboring polygons 
are likely scattered to dif­ferent processors. Moreover, this method is disadvantageous in (3) because 
each processor generates a highly discrete image consum­ing a large number of patches. A more practical 
method is to allocate clusters of polygons to the most lightly loaded processor rather than allocating 
independent ones [5]. The number of polygons in each cluster is called the cluster size (denoted by .). 
Each cluster contains up to .polygons which are geometrically close to each other. The sequence of polygons 
speci.ed implicitly by the user through a graphics library tends to have space coherence, i.e., two polygons 
consecutively indicated by the user have a high probability of being geometrically close. Therefore, 
it is suf.cient when constructing clusters to let the .rst . polygons in the sequence be the .rst cluster, 
let the next .polygons be the second cluster, and so on. With this method, the best balance of the aforementioned 
three points can be achieved by tuning the value of .. 4.2 Adaptive Parallel Rasterization The clustering 
method is not suf.cient for reasonable load balanc­ing if some polygons occupy a large area on the screen. 
For ex­ample, the scene shown in Figure 13(c) has one large-sized poly­gon constructing the table. Such 
a polygon not only causes load concentration but also increases the LFB over.ow. To avoid these problems, 
we developed a technique called adaptive parallel raster­ization (APR). The APR method rasterizes a polygon 
in pixel parallel when its estimated size is larger than a predetermined value called the APR threshold; 
otherwise, the polygon is rasterized in polygon parallel as usual. After geometry calculation, the processors 
calculate the on-screen area of each polygon. If the area is larger than the APR threshold, the screen 
coordinates and colors of the vertices of the polygon are broadcast to all the processors using the broadcast 
bus. When a processor receives the broadcast data, it rasterizes the poly­gon only for the pixels that 
are assigned to the processor. Figure 12 illustrates a processor assignment based on APR. One of the 
concerns of APR is the overhead due to the area cal­culation of the polygons. However, on the VC-1 which 
calculates the area as the vector product of the two edges of a triangle, it is known from experiments 
that the overhead is less than 3% of the total computation time.  5 Evaluation Results In this section, 
the VC-1 architecture and our parallel polygon ren­dering methods are evaluated using the sample scenes 
shown in Fig­ure 13. For systems with 16 or less processors, real performance is shown. For systems with 
17 to 256 processors, estimated perfor­mance is displayed. Using the VC-1, the rendering time for over 
16 processors is esti­mated. When APR is not performed, the rendering time is estimated using one of 
the processors and the host computer as follows. Ini­tially, the rendering time for each logical processor 
of the estimation target system is measured on the physical processor, and then, the system rendering 
time is calculated as the maximum of the mea­sured rendering times. If APR is active, the rendering time 
is split into polygon-parallel and pixel-parallel parts, and the estimation is performed in two passes; 
the .rst pass determines the polygon­parallel part of the rendering time, and the second pass .gures 
out the complete rendering time. These methods yield exactly the same performance as real performance 
(See [11] for justi.cation). For a 16-processor system or smaller, it is con.rmed from experiments that 
the performance estimated by this method coincides with real performance. 5.1 Rendering Time Figure 7 
indicates the rendering time for each sample scene varying the number of processors. The rendering time 
decreases in inverse proportion to the number of processors as long as they do not reach 33.3ms, which 
is the lower limit determined by the speed of the im­age merger and the rate of CRT refresh. For scenes 
composed of many polygons, such as the ter250 scene (containing 203522 poly­gons), the excellent scalability 
of the VC-1 architecture is observed. In fact, a speed 190 times faster is achieved with 256 processors 
for the ter250 scene. The maximum rendering performance of the VC­1 calculated from Figure 7 is 514K 
polygons per second for a 16­processor system and 6.1M polygons per second for a 256-processor system. 
Figure 4.2 shows the rendering time as a function of image mem­ory capacity. As the image memory capacity 
decreases, the render­ing time increases because of LFB over.ows. In a 16-processor system, it is observed 
that 1/8 of full-screen capacity is enough to render all the sample scenes without signi.cant performance 
degra­dation. Similarly, in a 256-processor system, image memory capac­ity can be reduced to 1/32 of 
full-screen capacity.  5.2 The Effect of Adaptive Parallel Rasterization Figure 9 displays the relationship 
between the APR threshold and rendering time. As the APR threshold becomes larger (i.e., as the ratio 
of polygon-parallel rasterization increases), load balancing be­comes worse and also LFB over.ows increase. 
Conversely, as the APR threshold becomes smaller, total rasterization time as well as broadcast communication 
time increase. Therefore, we have a min­imum rendering time at some APR threshold. When the table scene 
is rendered with 16 processors, the most appropriate APR threshold is 500, where the rendering time is 
reduced to 75% of the rendering time without APR (i.e., the rendering time when the APR threshold is 
in.nity). It becomes clear from further experiments that the main advan­tageous effect of APR is the 
reduction of LFB over.ows rather than the improvement of load balancing. Without APR, a polygon whose 
 t eapot16  teapot  tate ble r1k   te r250  Rendering time (ms) Rendering Time (ms) Rendering 
Time (ms) 400 ter250 teapo t16 table ter1k teapot  150 300 200 100 150 80 100 60 70 50 50 40 40 
 30 1/4 1/8 1/16 1/32 1/64 1/4 1/8 1/16 1/32 1/64 1/128 Image memory capacity Image memory capacity 1 
2 4 8 16 32 64128256 No. of processors  (a) No. of processors = 16 (b) No. of processors = 256 Figure 
7: Rendering time versus the number Figure 8: Rendering time versus image memory capacity      
   of processors Rendering Time (ms) 1000 700 500 300 200 150 100 70 50 30 10 100 1000 10000 infinity 
APR threshold (pixels2 ) Figure 9: Rendering time versus the APR threshold image is larger than the 
image memory capacity would inevitably cause an LFB over.ow, which often leads to signi.cant perfor­mance 
degradation. APR avoids LFB over.ows by dividing such a large polygon into pieces and effectively making 
image memory consumption even among processors. 5.3 Comparison with the Region-Based Approach As described 
in Section 1, the PixelFlow system reduces image memory capacity by dividing the screen into regions 
and rasteriz­ing polygons in multiple passes. Each renderer in the PixelFlow system actually has several 
buffers to improve load balancing. We simulated this memory-saving method on the VC-1 for comparison. 
Table 1 contrasts the performances for the two different memory­saving methods when the ter1k scene is 
rendered with 16 processors. We averaged these values over 48 different view angles. The num­ber of buffers 
indicates the number of independently accessible LFB banks in a processor. For the VLFB, this number 
is 2 because we use double-buffering. For the region-based approach, this number cor­responds to the 
maximum number of regions which each renderer Table 1: Comparison with the region-based approach Memory-saving 
method VLFB RB 222t RB 424t RB 424t Number of buffers 2 2 2 4 Image memory capacity for each buffer 1/8 
1/4 1/16 1/16 Total image memory capacity 1/4 1/2 1/8 1/4 Rendering time 33.4ms 51.0ms 73.5ms 66.9ms 
Geometry calculation time 12.3ms 14.1ms 14.1ms 14.1ms Rasterization time 14.7ms 29.1ms 41.5ms 41.5ms 
Global synchronization time 1.4ms 7.3ms 17.3ms 10.7ms t RB .2. means the region-based approach in which 
the screen is divided into .2.regions. can hold. The geometry calculation time refers to the total time 
for transformation, lighting, and region classi.cation. The rasteri­zation time represents the total 
time for clipping and rasterization. The global synchronization time indicates processor idle time due 
to load imbalance. With the VLFB, the ter1k scene can be rendered in 33.4ms using an image memory whose 
total capacity is 1/4 of full-screen capacity. On the other hand, if the region-based approach is taken, 
the render­ing time increases considerably even if the image memory capacity is 1/2. There are two main 
reasons for this: the increase of raster­ization time due to additional clipping tasks for region boundaries 
and the increase of global synchronization time resulting from the aggravation of load balancing. It 
is possible to introduce special hardware for clipping in order to improve the performance for the region-based 
approach. Never­theless, this will only lighten the increase in rasterization time. The increase in global 
synchronization time is essential except for that due to the load imbalance caused by the clipping. The 
superiority of the VLFB becomes more remarkable in large­scale systems. In the VLFB, the image memory 
capacity decreases as the number of processors increases. However, the region-based approach has no such 
characteristic.  This board contains a CPU, a local memory, communication interfaces, an LFB unit and 
a merging unit. It is a 6-layer printed circuit board whose size is 356 x 400 mm. Figure 12: Processor 
assignment with APR The color of each polygon identifies the processor to which the polygon is assigned. 
The large polygons constructing the table are rasterized in pixel parallel with line interleaving. 
High-resolution TIFF versions of these images can be found on the CD-ROM in S96PR/papers/nishimur  (a) 
teapot (No. of polygons = 4096) (b) teapot16 (No. of polygons = 65536) (c) table (No. of polygons = 10224) 
(d) ter1k (No. of polygons = 12482) (e) ter250 (No. of polygons = 203522) High-resolution TIFF versions 
of these images can be found on the CD-ROM in S96PR/papers/nishimur  6 Discussions As the number of 
processors increases, the APR threshold should be lowered, since the image memory capacity for each processor 
decreases. One might think that this limits the system scalability because the number of polygons transmitted 
via the broadcast bus increases. However, the average size of these polygons tends to be­come smaller 
as the number of processors increases, and therefore, we believe that this is not a serious limitation. 
To perform anti-aliasing with true real-time image generation or to increase screen resolution, the bandwidth 
of the image merger must be increased in proportion to the number of samples per pixel or the number 
of pixels on the screen. As a result, an extremely high bandwidth will be required. This is a major weakness 
of image com­position architecture, although we think that future advancement in device technology will 
take care of this. As for scalability, this still remains regardless of these extensions. If special 
hardware for rasterization is added to our architecture, the number of polygons per processor will increase. 
This does not necessarily mean an increase in image memory capacity. The im­age memory capacity is strongly 
related to the number of pixels ac­cessed in each processor, which is represented as a1pi, where ais 
the average size of polygons, pis the total number of polygons, and is the number of processors. Since 
a1p(i.e., the number of pix­els totaled over all the polygons) usually remains constant against changes 
in the number of polygons, a1piis invariable no matter how piincreases. As for scalability, adding special 
hardware does not in.uence this. VC-1 is designed mainly for retained-mode graphics APIs. It is possible 
to support immediate-mode APIs if the host computer broadcasts the whole scene database for each frame; 
however, the system will no longer be scalable because the broadcast bus will be bottlenecked. It seems 
that there is no way to make a scal­able machine with immediate-mode APIs since they enforce the se­rial 
traversal of the database in the host computer. Even with the retained-mode APIs, the broadcast bus may 
be bottlenecked when the coordinates of the polygon vertices are constantly changing. However, this problem 
can be solved by allowing the node proces­sors to determine the coordinates by solving physical equations 
or dividing curved surfaces, for example. 7 Conclusions In this paper, we proposed a novel frame buffer 
architecture called the virtual local frame buffer to reduce the memory requirement in parallel graphics 
machines based on image composition. To eval­uate the architecture, we developed a prototype machine 
called the VC-1. From experiments on the VC-1, it was observed that the virtual local frame buffer technique 
reduces the image memory ca­pacity to 1/8 of full-screen capacity in a 16-processor system and to 1/32 
in a 256-processor system without sacri.cing the system s per­formance. This reduction enables us to 
use fast static RAMs for the local frame buffer, by which rendering performance is improved. The adaptive 
parallel rasterization method, which selects an ap­propriate parallelization approach based on a threshold 
value, is es­sential to the virtual local frame buffer. Without this technique, the system performance 
would be degraded considerably due to the ex­haustion of the image memory. However, the problem of how 
to .nd the optimal threshold, which guarantees the avoidance of this exhaustion, is still open. In the 
future, we would like to extend our architecture to support both anti-aliasing and texture mapping. We 
are also planning to add special hardware for rasterization to this architecture to improve polygon rendering 
performance.  Acknowledgments We greatly appreciate the .nancial support of Kubota Computer, Inc. We 
are also grateful to the members of the past VC-1 project in the University of Tokyo, Ryo Mukai, Reiji 
Suda and Yukio Saka­gawa for their help in hardware design, and Jun Naito for his assis­tance in software 
development. Special thanks go to Hiroyuki Nitta for his help in gathering equipment and materials and 
providing use­ful technical information. We would also like to thank Thomas Orr for his thoughtful comments. 
 References [1] AKELEY,K.RealityEngine Graphics. Proceedings of SIG-GRAPH 93 (August 1993), 109 116. 
[2] COHEN,D.,ANDDEMETRESCU,S.A VLSI approach to Computer Image Generation. Tech. rep., Information Sciences 
Institute, University of Southern California, 1979. [3] COx,M.Algorithms for Parallel Rendering. PhD 
thesis, Dept. of Computer Science, Princeton University, 1995. [4] DEERING,M.,WINNER,S.,SCHEDIWY,B.,DUFFY, 
C.,ANDHUNT,N.The Triangle Processor and Normal Vector Shader: A VLSI System for High Performance Graph­ics. 
ACM Computer Graphics 22, 4 (August 1988), 21 30. [5] ELLSWORTH,D.,GOOD,H.,ANDTEBBS,B.Distribut­ing Display 
Lists on a Multicomputer. ACM Computer Graph­ics 24, 2 (March 1990), 147 154. [6] HAEBERLI,P.,ANDAKELEY,K.The 
Accumulation Buffer: Hardware Support for High-Quality Rendering. ACM Computer Graphics 24, 4 (August 
1990), 309 318. [7] KUBOTACOMPUTERINC.TITAN2 Technical Overview, 1993. [8] MOLNAR,S.Combining Z-buffer 
Engines for Higher-Speed Rendering. In Advances in Computer Graphics Hardware III (1988), Springer Verlag, 
pp. 171 182. [9] MOLNAR,S.,COx,M.,ELLSWORTH,D.,ANDFUCHS, H.A sorting classi.cation of parallel rendering. 
IEEE Com­puter Graphics and Applications 14, 4 (July 1994), 23 32. [10] MOLNAR,S.,EYLES,J.,ANDPOULTON,J.Pix­elFlow: 
High-Speed Rendering Using Image Composition. ACM Computer Graphics 26, 2 (July 1992), 231 240. [11] 
NISHIMURA,S.A Parallel Architecture for Computer Graphics Based on the Con.ict-Free Multiport Frame Buffer. 
Doctoral dissertation, Dept. of Information Science, Faculty of Science, The University of Tokyo, 1995. 
[12] PALOVUORI,K.An Implementation of an Linearly Expand­able Graphics Processing Architecture. Tech. 
Rep. 8-94, Dept. of Electrical Engineering, Tampere University of Technology, Finland, 1994. [13] ROMAN,G.C.,ANDKIMURA,T.VLSI 
perspective of real-time hidden-surface elimination. Computer-Aided Design 13, 2 (March 1981), 99 107. 
[14] SCHNEIDER,B.-O.A Processor for an Object-Oriented Rendering System. Computer Graphics Forum 7, 4 
(1988), 301 310. [15] SHAW,C.D.,GREEN,M.,ANDSCHAEFFER,J.A VLSI Architecture for Image Composition. In 
Advances in Computer Graphics Hardware III (1988), Springer Verlag, pp. 183 199. [16] WEINBERG,R.Parallel 
Processing Image Synthesis and Anti-Aliasing. ACM Computer Graphics 15, 3 (August 1981), 55 62.   
   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237276</article_id>
		<sort_key>373</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Rendering from compressed textures]]></title>
		<page_from>373</page_from>
		<page_to>378</page_to>
		<doi_number>10.1145/237170.237276</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237276</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor>Approximate methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor>Exact coding**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P17710</person_id>
				<author_profile_id><![CDATA[81100069132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Beers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University, Gates Hall, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40026305</person_id>
				<author_profile_id><![CDATA[81100346089]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maneesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agrawala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University, Gates Hall, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P207711</person_id>
				<author_profile_id><![CDATA[81100278002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Navin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chaddha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Laboratory, Stanford University, Gates Hall, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166131</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kurt Akeley. RealityEngine graphics. In Computer Graphics (SIGGRAPH '93 Proceedings), volume 27, pages 109-116, August 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>841287</ref_obj_id>
				<ref_obj_pid>839284</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[N. Chaddha, P. Chou, and T. Meng. Scalable compressionbased on tree structured vector quantization of perceptually weighted generic block, lapped and wavelet transforms. IEEE International Conference on Image Processing, October 1995.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lawrence French. Toy story. Cinefantastique, 27(2):36-37, 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>128857</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Gersho and R. M. Gray. VectorQuantizationand Signal Compression.Kluwer Academic Publishers, 1991.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H.-M. Hang and B. Haskell. Interpolativevector quantizationof color images. In TCOM, pages 465-470, 1987.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801294</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Paul S. Heckbert. Color image quantizationfor framebuffer display. In Computer Graphics (SIGGRAPH'82 Proceedings), volume 16, pages 297-307, July 1982.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>16601</ref_obj_id>
				<ref_obj_pid>16564</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Paul S. Heckbert. Survey of texture mapping. In M. Green, editor, Proceedings of Graphics Interface '86, pages 207-212,May 1986.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949853</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[P. Ning and L. Hesselink. Fast volume renderingof compressed data. In G. Nielson and D. Bergeron, editors, Proc. Visualization'93, pages 11-18,October 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Darwyn Peachey. Texture on demand. Technical report, Pixar, 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>573326</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[W.B. Pennebaker and J.L. Mitchell. JPEG Still Image Data Compression Standard. Van Nostrand Reinhold, 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Pyramidal parametrics. In Computer Graphics (SIGGRAPH'83 Proceedings), volume 17, pages 1-11, July 1983.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[L. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Trans. Inform.Theory, Vol.IT-23, (3), May 1977.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rendering from Compressed Textures *Andrew C. Beers, *Maneesh Agrawala, and yNavin Chaddha *Computer 
Science Department yComputer Systems Laboratory Stanford University Stanford University Abstract We present 
a simple method for rendering directly from compressed textures in hardware and software rendering systems. 
Textures are compressed using a vector quantization (VQ) method. The advan­tage of VQ over other compression 
techniques is that textures can be decompressed quickly during rendering. The drawback of us­ing lossy 
compression schemes such as VQ for textures is that such methods introduce errors into the textures. 
We discuss techniques for controlling these losses. We also describe an extension to the basic VQ technique 
for compressing mipmaps. We have observed compression rates of up to 35:1, with minimal loss in visual 
qual­ity and a small impact on rendering time. The simplicity of our tech­nique lends itself to an ef.cient 
hardware implementation. CR categories: I.3.7 [Computer Graphics]: 3D Graphics and Real­ism -Texture; 
I.4.2 [Image Processing]: Compression -Coding 1 Introduction Texture mapping is employed on high-end 
graphics workstations and rendering systems to increase the visual complexity of a scene without increasing 
its geometric complexity[7]. Texture mapping allows a rendering system to map an image onto simple scene 
ge­ometry to make objects look much more complex or realistic than the underlying geometry. Recently, 
texture mapping hardware has become available on lower-end workstations, personal computers, and home 
game systems. One of the costs of texture mapping is that the texture images of­ten require a large amount 
of memory. For a particular scene, the memory required by the textures is dependent on the number of 
tex­tures and the size of each texture. In some cases, the size of the tex­tures may exceed the size 
of the scene geometry[3]. In hardware systems supporting real time texture mapping, tex­tures are generally 
placed in dedicated memory that can be accessed quickly as pixels are generated. In some hardware systems, 
tex­tures are replicated in memory to facilitate fast parallel access [1]. Because texture memory is 
a limited resource in these systems, it can be consumed quickly. Although memory concerns are less se­vere 
for software rendering systems since textures are stored in main memory,thereareadvantagestoconservingtexturememory. 
Inpar­ticular, using less memory for textures may yield caching bene.ts, especially in cases where the 
textures do not .t in main memory and cause the machine to swap. Permission to make digital/hard copy 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. .Gates Hall, Stanford University, Stanford, CA 94305 maneesh@cs.stanford.edu http://www-graphics.stanford.edu/ 
 One way to alleviate these memory limitations is to store com­pressed representations of textures in 
memory. A modi.ed renderer could then render directly from this compressed representation. In this paper 
we examine the issues involved in rendering from com­pressed texture maps and propose a scheme for compressing 
tex­tures using vector quantization (VQ)[4]. We also describe an ex­tension of our basic VQ technique 
for compressing mipmaps. We show that by using VQ compression, we can achieve compression rates of up 
to 35:1with little loss in the visual quality of the ren­dered scene. We observe a 2 to 20 percent impact 
on rendering time using a software renderer that renders from the compressed format. However, the technique 
is so simple that incorporating it into hard­ware should have very little impact on rendering performance. 
 2 Choosing a Compression Scheme There are many compression techniques for images, most of which are 
geared towards compression for storage or transmission. In choosing a compression scheme for texture 
mapping there are sev­eral issues to consider. In this section we discuss these issues and we show how 
VQ compression addresses each of them. Decoding Speed. In order to render directly from the com­pressed 
representation, an essential feature of the compression scheme is fast decompression so that the time 
necessary to access a single texture pixel is not severely impacted. With VQ, decompres­sion is performed 
through table lookups and is very fast. A trans­form coding scheme such as JPEG[10] is more expensive 
because extracting the value of a texture pixel would require an expensive inverse Discrete Cosine Transform 
(DCT) computation. Random Access. It is dif.cult to know in advance how a ren­dererwillaccessatexture. 
Thus,texturecompressionschemesmust provide fast random access to pixels in the texture. For compres­sion 
schemes like JPEG or run length coding which produce vari­able rate codes, extracting a texture pixel 
might require decompress­ing a large portion of the texture. Unlike variable rate codes, .xed­rate VQ 
represents each block of texture pixels with a .xed number of bits. Since the number of bits is known 
in advance, indexing any particular pixel is fast and easy. Compression Rate and Visual Quality. While 
lossless com­pression schemes, such as Lempel Ziv compression[12], will per­fectly preserve a texture, 
they achieve much lower compression rates than lossy schemes. However, using a lossy compression scheme 
introduces errors into the textures. With VQ, there are many parameters that can be used to control these 
errors. A major differ­ence between images and textures is that images are viewed on their own, while 
textures are viewed as part of a scene with orientation and size dependent on the mapping from scene 
surface to texture. Thus, for image compression algorithms, the visual quality of the compressed image 
is most important, while for texture compression algorithms, the visual quality of the rendered scene, 
not the texture map, is most important. Encoding Speed. Experimenting with the compression rate ver­sus 
visual quality tradeoff can be dif.cult if encoding is slow. Al­though optimal VQ encoding can be a time-consuming 
process, fast sub-optimal encoding algorithms exist. Texture compression, how­ever, is an asymmetric 
application of compression, since decoding speed is essential while encoding speed is useful but not 
necessary. Texture Coordinate Compressed Texture (i,j) (Blocki,Blockj) (Offseti,Offsetj)  Figure 1: 
Accessing a pixel from a compressed texture. 3 Rendering We have chosen VQ as our texture compression 
algorithm because it addresses all of the issues presented in the previous section, and in particular 
it supports fast decompression. When using VQ, we con­sider a texture as a set of pixel blocks. VQ attempts 
to characterize this set of blocks by a smaller set of representative blocks called a codebook. Alossy 
compressedversionofthe originalimage is rep­resented as a set of indices into this codebook, with one 
index per block of pixels. This set of indices is called the index map. The tex­ture can be decompressed 
by looking up each block of pixels in the codebook via its index. In [9], such an indexed-lookup technique 
is used to page in uncompressed textures from disk on demand. Color quantization algorithms such as the 
Median Cut Algorithm[6] use this same indexed-lookup technique for representing 24 bit images with 8 
bits per pixel. A method for compressing 3D volumes us­ing VQ and a fast technique for volume rendering 
directly from this compressed format is presented in [8]. While we will describe ex­actly how to encode 
textures using VQ in section 4, we .rst show how to render directly from VQ compressed textures. The 
rendering algorithm is outlined in the following algorithm. Assuming that the (s;t)texture coordinate 
has already been con­verted to an integral (i;j)location within the texture, we: 1. Determine in which 
block Bpixel (i;j)lies, and the offset of pixel (i;j)within that block. 2. Lookup the index associated 
with block Bto determine the corresponding codeword in the codebook. 3. Lookup the pixel (i;j) within 
this codeword block.  The three step process is shown pictorially in .gure 1. Step 1 is easily implemented 
with fast bit shift and logical operators when texture and block sizes are a power of two. To achieve 
the highest compression rate we store the index map in a packed representation. Therefore, in step 2, 
two memory accesses and a few simple bit op­erations may be required if the size of each index is such 
that the in­dices do not fall on word boundaries. However, for word aligned in­dices, the appropriate 
index can be determined with a single lookup. 4 Encoding The most critical part of encoding a texture 
using VQ is designing the codebook. The Generalized Lloyd Algorithm (GLA)[4] is one technique for generating 
a codebook. It is an iterative clustering al­gorithm which yields a locally optimal codebook for a given 
set of pixel blocks, called the training set of vectors.We generally use all the blocks in the original 
texture as our training set. The algorithm begins by selecting a set of potential codewords from the 
training set and then iterates on the following steps. Each training vector is grouped with the nearest 
codeword, based on some distortion mea­sure such as Euclidean distance. The centroids of the groups are 
chosen as the new set of codewords, and the process repeats until the set of codewords converges. Although 
this Full Search approach is locally optimal, gener­ating the codebook is computationally expensive. 
A faster tech­nique for producing the codebook is Tree Structured VQ[4]. This approach designs the codebook 
recursively, organizing the code­book as a binary tree. The .rst step is to .nd the centroid of the set 
of training vectors, which becomes the root level codeword. To .nd the children of this root, the centroid 
and a perturbed centroid are chosenasinitial childcodewords. TheGLAisthenusedto converge on the locally 
optimal codewords for this .rst level in the tree. The training vectors are split into two groups based 
on these locally opti­mal codewords and the algorithm recurses on each of these subtrees. Note that once 
the child codewords have been chosen, the training vectors are permanently grouped with the nearest codeword. 
Since codewords cannot jump across subtrees, this approach is not guar­anteed to produce the locally 
optimal codebook, but is substantially faster than Full Search VQ [4]. Once the codebook has been generated 
we encode a texture by mapping each block of pixels to the nearest codeword. With Full Search VQ we exhaustively 
search for the nearest codeword. With Tree Structured VQ, we can traverse the codebook tree always tak­ing 
the path with the closest codeword. Thus, both codebook gener­ation and texture encoding are faster with 
Tree Structured VQ than with Full Search VQ. We use Tree Structured VQ for quick experi­mentation, and 
Full Search VQ for generating .nal codebooks.  5 Texture Encoding Tradeoffs In generating the VQ encoding 
for a texture we have control over several parameters that can be used to tradeoff compression rate for 
the quality of the compressed texture. This tradeoff must be con­sidered carefully when encoding textures 
for a given scene. In this section we describe some of the parameters in VQ encoding that af­fect this 
tradeoff. We have many choices on how to partition the image data into training vectors when designing 
a VQ codebook. The size of a vec­tor is dictated by the dimensions of the block of pixels being coded 
and the number of color channels used to de.ne the color of each pixel. We can either design a codebook 
for each color channel sep­arately, or treat components of a color as a single value and code them together. 
We use the latter approach, which results in a higher compression rate since only one codebook and index 
map is used, instead of one codebook and index map per color channel. The size of the codebook in.uences 
the compression rate in two ways. A larger codebook will lower the compression rate by in­creasing the 
size of the compressed representation. A larger code­bookalsomeansthattheindicesinto thecodebookwill 
requiremore bits, increasing the size of the index map. However, a larger code­book will contain more 
of the representative blocks giving us better quality compressed textures. The size of the pixel block 
used in tex­ture encoding also has a large effect on the overall compression rate. For example, if we 
use 4.4blocks when encoding a texture, each RGB pixel block contains 4.4.348bytes. With a 256 en­try 
codebook, we use a 1 byte index to represent each pixel block, yielding a base compression rate of 48:1. 
The overall compression rate is reduced from the base rate by the storage requirements of the codebook. 
For example, when encoding a 512.512image using a 256 entry codebook, the overall compression rate falls 
to 27:4:1. With 2.2blocks the base compression rate is only 12:1.How­ever, there are fewer possible 2.2blocks 
than 4.4blocks, and therefore, 2.2codebooks produce better quality compressed tex­tures than 4.4codebooks. 
Similarly we could use larger blocksizes to gain higher compression rates for worse quality. To achieve 
additional compression, we can encode three channel RGB textures in the 4:1:1 YUV format, commonly used 
in video standards. This format stores the color channels U and V decimated by a factor 2 both horizontally 
and vertically, and the luminance channel Y at full resolution. Converting from this YUV representa­Texture 
Mipmap  Trial Overall Rate MSE RGB Files: All: 4096, 4X4,RGB 11.8:1 12.12 RGB Files: All: 4096, 4X4, 
YUV 15.3:1 16.17 RGB Files: Signs, Roads: 256, 2X2, YUV Other: 4096, 4X4, YUV 12:7:1 11.27 RGB Files: 
Signs: 256, 2X2, YUV Roads, Other: 256, 4X4, YUV 24.5:1 16.19 Codebook Figure 2: The encoding of three 
consecutive levels of a mipmap. tion back to the RGB representation used by most renderers requires only 
multiplication by a constant 3x3 matrix, which is easily imple­mented in hardware. We can also increase 
the compression rate by amortizing the cost of a larger codebook over several textures. We train a codebook 
on multiple textures to determine one codebook for all of them. A sin­gle codebook may be enough to characterize 
all of the textures in a scene. If one codebook results in too many compression artifacts, we can group 
the textures and design one codebook per group.  6 Mipmapping An effective way to resample textures 
is by using a mipmap[11]. A mipmap stores a texture as an image pyramid, and is designed to al­low ef.cient 
.ltering of a texture. Each mipmap level stores a .l­tered version of the texture corresponding to a 
particular image pixel to texture pixel ratio. To apply a mipmapped texture, we compute texture coordinates 
(s;t)and an approximation to d, the image pixel to texture pixel ratio. Because these values are not 
integral in most cases, trilinear interpolation is used to determine the texture value. One way to compress 
mipmaps would be to compress each mipmap level individually. However, we can take advantage of the correlation 
betweensuccessivelayersofamipmapbyencodingsev­eral levels at once, generating one codebook as well as 
one index map for the group of levels. This allows us to gain higher compres­sion rates in compressing 
mipmaps. To compress a mipmap, we begin by compressing the original texture using 4.4blocks. Given the 
codebook for this .rst mipmap level, we can form a codebook for the next level by averaging each 4.4codeword 
down to a 2.2codeword. Similarly, a codebook for the third level of the mipmap can be formed by averaging 
these 2.2codewords down to single pixel codewords. Instead of stor­ing three codebooks, we can combine 
them into one codebook con­taining extended codewords. Each extended codeword is formed by concatenating 
the 4.4codeword with the corresponding subsam­pled 2.2and 1.1codewords. Note that the index maps for 
the three levels are identical, and therefore can be shared. The encod­ing of three mipmap levels is 
shown pictorially in .gure 2. The full mipmap can be formed by repeating this process for every group 
of three levels, creating a separate index map and codebook for every group of three. In practice, once 
the size of a mipmap level is less than 32.32pixels, we store it uncompressed. For the trilinear interpolation, 
each (s;t;d)mipmap texture co­ordinate is converted to the eight nearest integral (i;j;L)mipmap locations 
in two adjacent levels of the mipmap. To access a pixel in a given level Lwe .rst determine the group 
of levels to which L Table 3: For each of these trials, one channel textures were com­pressed using 256, 
2.2codewords, and RGBA textures were com­pressed using 256, 2.2codewords. belongs. The texture coordinates 
are then used to look up the index into the codebook from the index map for that group of levels. The 
levelnumberdeterminesin whichpartoftheextendedcodewordthe desired pixel is stored. Finally, the block 
offsets of the pixel are used to address the desired pixel. The drawback of our mipmap encoding approach 
is that the qual­ity of the compressed level 1 and level 2 mipmaps can not be any better than the quality 
of the compressed level 0 mipmap, even though smaller sized codewords are used for levels 1 and 2. Inter­polative 
VQ[5] [4] is an alternative technique for compressing im­age pyramids such as mipmaps. The general approach 
is to perform VQ on a subsampled image, decompress the image, interpolate it up to the next larger image 
size and VQ the difference between the in­terpolated image and the original. The bene.ts of this approach 
are that it takes advantage of the correlation between successive levels in the mipmap during encoding, 
and unlike the scheme we propose, the quality of each compressed mipmap level is somewhat indepen­dent 
of the quality of other mipmap levels. However, Interpolative VQ also requires a separate codebook per 
mipmap level, and re­trieving a pixel from a particular mipmap level requires building up the the pixel 
value from the most subsampled mipmap level up to the desired level, accessing a codeword from each mipmap 
level in between. Although we have not performed direct comparisons to our encoding scheme, these two 
drawbacks make Interpolative VQ unattractive for fast texture mapping applications and we do not to use 
it in encoding mipmaps. 7Results We have evaluated our proposed VQ texture compression method by rendering 
texture mapped scenes using two different renderers: IRIS Performer and a custom software scan converter. 
We present the results for two scenes in this section. Since we cannot directly load our compressed format 
into Performer, we compress and de­compress each texture in a preprocessing step that introduces com­pression 
errors into the textures. While we cannot directly compare rendering speed we can compare the visual 
quality of the rendered images. We also use a software scan converter to render textures di­rectly from 
our compressed format. This allows us to compare both rendering times for, and the visual quality of, 
images rendered with and without compressed textures. We experiment with several of the VQ encoding parameters 
discussed in section 5 and report the compression rates for each of the scenes. The Performer Town is 
a fully texture mapped virtual environ­ment, containing 85 textures which require a total of 5 MB when 
un­compressed. Although most of the textures are three channel RGB textures,there are 14one channelintensity 
texturesand 7 fourchan­nel RGBA textures. We compress the intensity textures with a sin­gle codebook 
containing 256 codewords and a blocksize of 2.2. Wegenerateaseparatecodebookforthe RGBAtexturesagainusing 
256 2.2codewords. We vary several parameters in compressing the RGB textures and the overall compression 
results are given in table 3. The mean squared error (MSE) for a color channel is com­puted as the sum 
of squared differences between original frame pix­els andcorrespondingpixelsfrom the compressedtexture 
frame, di­Table 4: Compression rates and average MSE for 50 frames of the Topspin animation. As shown 
in plate 2, the scene contains three textures: Wood, Marble and Top. Texturing used Filter Rate Avg. 
MSE Wood,Marble: 128, 4X4YUV Top: 128, 2X2YUV Point 34.7:1 34.7 Wood,Marble: 256, 4X4YUV Top: 256, 2X2YUV 
Point 28.3:1 19.1 all: 256, 2X2, YUV Point 11.5:1 37.9 separate: 256, RGB Mipmap 17.6:1 14.2 separate: 
1024, RGB Mipmap 8.0:1 13.3 Sampling Textures used Rendering Time Increase Point Uncompressed 21.0 sec 
VQ w/8 bit index 21.4 sec 1.7% VQ w/12 bit index 23.0 sec 9.6% Mipmap Uncompressed 65.4 sec VQ w/8 bit 
index 69.5 sec 6.3% VQ w/12 bit index 78.3 sec 19.7% Table 5: Rendering time for the Topspin animation 
for uncom­pressed and compressed textures using 8 and 12 bit indices. Twenty frames were rendered at 
400.400on a 132MHz MIPS R4600. vided by the number of pixels in the frame. The MSE results we present 
are calculated for the frame shown in plate 1 and are aver­aged across the three RGB color channels. 
The .rst two rows of table 3 present the results of using a sin­gle codebook across all the RGB textures. 
Using 4.4blocks, cer­tain textures such as signs, billboards and roads contain some no­ticeable artifacts. 
Based on this observation we separated the RGB textures into three groups: signs and billboards, roads, 
and all oth­ers, using a separate codebook for each group. This allows us to use smaller 2.2blocks for 
the signs and roads, while using 4.4blocks for the other textures. As shown in table 3, we can achieve 
slightly higher compression rates with a smaller average MSE using sepa­rate codebooks (see rows 1 and 
3). Using separate codebooks, even at a compression rate of 24.5:1, the rendered scenes must be exam­ined 
closely to see the artifacts in the textures, as shown in plate 1. The Topspin animation contains three 
texture maps requiring 1.4 MB when uncompressed. We render this animation using a software scan converter. 
Some compression rates for this animation using point mapped and mipmapped textures are given in table 
4. Note thattheMSEnumberspresentedin this tableare averagedacrossthe 50 frames in the Topspin animation. 
We do not vary the codebook blocksize for mipmaps, since this is .xed by our three level code­book architecture. 
A separate compressed mipmap is generated for each of the three textures in the Topspin animation. A 
frame from this animation rendered with point sampled tex­tures is shown in plate 2, while the textures 
are shown in plate 3. The frame is almost free of artifacts, although there is some blockiness in the 
foreground Marble texture. The Wood and Marble textures in plate 3 have been cropped and enlarged to 
make the compression artifacts more visible. The timing results for the Topspin animation are presented 
in ta­ble 5. Index maps with 12 bit indices take more time to decode than those with 8 bit indices because 
two lookups and bit manipulations are required to extract each index instead of a single lookup. Al­though 
these timing results indicate the computational overhead of rendering from VQ compressed textures in 
a software renderer, they do not represent a situation in which texture compression would be used. In 
a software renderer textures would only be compressed if theuncompressedtexturessurpassthe mainmemorylimit 
andcause the machine to swap. In such a situation, compression would alle­viate the swapping, thereby 
drastically improving rendering times. In a hardware implementation of this scheme, specialized address­ing 
logic could be built to reduce the the penalty caused by irregular index sizes. 8 Conclusions We have 
presented a method for rendering directly from VQ com­pressed texture maps. The advantage of using VQ 
over other com­pression schemes is that it addresses many of the issues involved in choosing a compression 
scheme for texture mapping. In partic­ular, decompression is inexpensive. Even though VQ compression 
is lossy, we have been able to achieve compression rates of up to 35:1with few visible artifacts in the 
rendered images. There are several directions in which this work may be extended. Designing codebooks 
currently requires some experimentation with the various VQ encoding parameters. While an automatic method 
for designing optimal codebooks would be useful, designing a measure of optimality is dif.cult. There 
has been some work on de­signing perceptual distortion measures to minimize such distortions in compressed 
images [2]. For texture mapping however, distortion in the rendered scene, not the compressed textures, 
must be mini­mized, so the distortion measure must use information about how the textures will be mapped 
into the scene. It may be possible to use a hint driven approach that allows the application designer 
to provide hints about characteristics like which textures are similar to one another, or which textures 
are more or less important to pre­serve perfectly. The VQ compression approach naturally extends to other 
classes of texture maps such as bump maps, displacement maps and environment maps. Each of these classes 
of textures has some unique .ltering or access issues and although our preliminary results indicate that 
VQ compression works well for them, we are studying them in more detail. Using VQ compressed textures 
in a rendering system is a vi­able method for reducing the memory overhead of texture mapping. Such compression 
is ideal for rendering systems that use specialized texture memory and aim for real-time performance. 
It will allow lower-end systems such as PCs and home game systems to achieve greater graphical realism 
through the use of more complex textures.  Acknowledgments Homan Igehy and Gordon Stoll provided the 
scan converter used to generate timing results. The Performer Town database appears courtesy of Silicon 
Graphics and Paradigm Simulation. References [1] Kurt Akeley. RealityEngine graphics. In Computer Graphics 
(SIGGRAPH 93 Proceedings), volume 27, pages 109 116, August 1993. [2] N. Chaddha, P. Chou, and T. Meng. 
Scalable compression based on tree structured vector quantization of perceptually weighted generic block, 
lapped and wavelet transforms. IEEE International Conference on Image Processing, October 1995. [3] Lawrence 
French. Toy story. Cinefantastique, 27(2):36 37, 1995. [4] A. Gersho and R. M. Gray. VectorQuantizationand 
Signal Compression.Kluwer Academic Publishers, 1991. [5] H.-M.HangandB.Haskell.Interpolativevectorquantizationofcolorimages.In 
TCOM, pages 465 470, 1987. [6] PaulS.Heckbert.Colorimagequantizationforframebufferdisplay.In Computer 
Graphics (SIGGRAPH 82 Proceedings), volume 16, pages 297 307, July 1982. [7] Paul S. Heckbert. Survey 
of texture mapping. In M. Green, editor, Proceedings of Graphics Interface 86, pages 207 212, May 1986. 
[8] P.NingandL.Hesselink.Fastvolumerenderingofcompresseddata.InG.Niel­son and D. Bergeron, editors, Proc. 
Visualization 93, pages11 18,October 1993. [9] Darwyn Peachey. Texture on demand. Technical report, Pixar, 
1990. [10] W.B. Pennebaker and J.L. Mitchell. JPEG Still Image Data Compression Stan­dard. Van Nostrand 
Reinhold, 1993. [11] Lance Williams. Pyramidal parametrics. In Computer Graphics (SIGGRAPH 83 Proceedings), 
volume 17, pages 1 11, July 1983. [12] L. Ziv and A. Lempel. A universal algorithm for sequential data 
compression. IEEE Trans. Inform.Theory, Vol.IT-23, (3), May 1977.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237277</article_id>
		<sort_key>379</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Display of clouds taking into account multiple anisotropic scattering and sky light]]></title>
		<page_from>379</page_from>
		<page_to>386</page_to>
		<doi_number>10.1145/237170.237277</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237277</url>
		<keywords>
			<kw><![CDATA[clouds]]></kw>
			<kw><![CDATA[multiple scattering]]></kw>
			<kw><![CDATA[optical length]]></kw>
			<kw><![CDATA[participating media]]></kw>
			<kw><![CDATA[photo-realism]]></kw>
			<kw><![CDATA[radiative transfer]]></kw>
			<kw><![CDATA[sky light]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP36036594</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fukuyama University, Sanzo, Higashimura-cho, Fukuyama, 729-02 Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P306681</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima University, 1-4-1, kagamiyama, Higashi-hiroshima, 739 Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P75905</person_id>
				<author_profile_id><![CDATA[81100145250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eihachiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima Prefectural University, Nanatsuka-cho, Shoubara City, 727 Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E Blasi, B.L. Saec, C. Schlics, "A Rendering Algorithm for Discrete Volume Density Objects, "Proc. of EUROGRAPH- ICS'93, Vo1.12, No.3 (1993) pp.201-210.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J.F. Blinn, "Light Reflection Functions for Simulation of Clouds and Dusty Surfaces, " Computer Graphics, Vol. 16, No. 3 (1982) pp. 21-29.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357310</ref_obj_id>
				<ref_obj_pid>357306</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J.F. Blinn, "A Generalization of Algebraic Surface Drawing," ACM Tog, Vol.2, No.3 (1980) pp.235-256.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C.F. Bohren, "Multiple scattering of light and some of its observable consequences, " Am. J. Phys. Vol.55, No.6 (1987) pp.524-533.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M.F. Cohen, D.E Greenberg, "The Hemicube, A Radiosity Solution for Computer Environment,", Computer Graphics, Vol. 19, No.3 (1985) pp.31-40.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[W.M. Cornette, J.G. Shanks, "Physical reasonable analytic expression for the single-scattering phase function," Applied Optics, Vol.31, No.16 (1992) pp.3152-3160.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325248</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[G.Y. Gardener, "Visual Simulation of Clouds," Computer Graphics, Vo1.19, No.3 (1985) pp.297-303.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[E Hanrahan, W. Krueger, "Reflection from Layered Surfaces due to Subsurface Scattering," Proc. of SIGGRAPH' 93 (1994) pp.165-174.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>113028</ref_obj_id>
				<ref_obj_pid>113023</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Inakage,"Volume Tracing of Atmospheric Environments," The Visual Computer, 7 ( 1991 ) pp. 104-113.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J.T. Kajiya, B.V. Herzen, "Ray tracing Volume Densities," Computer Graphics, Vo1.18, No.3 (1984) pp.165-174.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[K. Kaneda, G. Yuan, E. Nakamae, T. Nishita, "Photorealistic Visual Simulation of Water Surfaces Taking into account Radiative Transfer," Proc. of CG &amp; CAD' 91, (China) (1991) pp.25-30.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[K. Kaneda, T. Okamoto, E. Nakamae, T. Nishita, "Photorealistic Image Synsesis for Outdoor scenery Under Various Atmospheric Conditions," The Visual Computel, Vol.7 (1991) pp.247-258.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[W.E. Lorensen, H.E. Cline, "Marching Cubes: a High Resolution 3D Surface Construction Algorithm," Computer Graphics, Vol.21, No.4 (1987) pp.163-169.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35071</ref_obj_id>
				<ref_obj_pid>35068</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[R.V. Klassen, "Modeling the Effect of the Atmosphere on Light, "ACM Transaction on Graphics, Vol. 6, No. 3 (1987) pp. 215-237.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5515</ref_obj_id>
				<ref_obj_pid>5513</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[N. Max, "Light Diffusion through Clouds and Haze," Graphics and Image Processing, Vol.33, No.3 (1986) pp.280-292.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[N. Max, "Efficient Light Propagation for Multiple Anisotropic Volume Scattering," Proc. of the Fifth Eurographics Work- Shop on Rendering (1994) pp. 87-104.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15900</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[T. Nishita and E. Nakamae, continuous tone Representation of Three-Dimensional Objects Illuminated by Sky Light," Computer Graphics, Vol. 20, No. 4 (1986) pp. 125-132.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37437</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, Y. Miyawaki, E. Nakamae, "A Shading Model for Atmospheric Scattering Considering Distribution of Light Sources," Computer Graphics, Vol. 21, No. 4 (1987) pp. 303- 310.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97916</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, T.W. Sederberg, M. Kakimoto, "Ray Tracing Rational Trimmed Surface Patches," Computer Graphics, Vol.24, No.4 (1990) pp.337-345.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166140</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, T. Shirai, K. Tadamura, E. Nakamae, "Display of The Earth Taking into Account Atmospheric Scattering, " Proc. of SIGGRAPH'93 (1993) pp.175-182.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, E. Nakamae, "A Method for Displaying Metaballs by using B6zier Clipping," Proc. of EUROGRAPHICS'94, Vo1.13, No.3 (1994) c271-280.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192261</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, E. Nakamae, "Method of Displaying Optical Effects within Water using Accumulation Buffer," Proc. of SIG- GRAPH'94 (1994) pp.373-379.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[H.E. Rushmeier, K.E. Torrance, "The Zonal Method for Calculating Light Intensities in The Presence of a Participating Medium," Computer Graphics, Vol.21, No.4 (1987) pp.293- 302.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[G. Sakas, M. Gerth, "Sampling and Anti-Aliasing of Discrete 3-D Volume Density Textures," Proc. of EUROGRAPH- ICS'91 (1991) pp.87-102.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[S. Sekine, "Corrected Color Temperature of Daylight(2) : Characteristics on Clear Sky and Overcast Sky," J. Illumination Engineering Inst. Japan, Vol.79, No.ll (1995) pp.621- 627.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218430</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[J. Stare, E. Fiume, "Depicting Fire and Other Gaseous Phenomena Using Diffusion Processes," Proc. of SIGGRAPH'95 (1995) pp.129-136.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Display of Clouds Taking into Account Multiple Anisotropic Scattering and Sky Light Tomoyuki Nishita 
Yoshinori Dobashi Eihachiro Nakamae Fukuyama University Hiroshima University Hiroshima Prefectural University 
Sanzo, Higashimura-cho, Fukuyama, 1-4-1, kagamiyama, Higashi-hiroshima, Nanatsuka-cho, Shoubara City, 
729-02 Japan 739 Japan 727 Japan nis@eml.hiroshima-u.ac.jp doba@eml.hiroshima-u.ac.jp naka@bus.hiroshima-pu.ac.jp 
 Abstract Methods to display realistic clouds are proposed. To display realis­tic images, a precise shading 
model is required: two components should be considered. One is multiple scattering due to particles in 
clouds, and the other factor to be considered is sky light. For the former, the calculation of cloud 
intensities has been assumed to be complex due to strong forward scattering. However, this pa­per proposes 
an ef.cient calculation method using these scattering characteristics in a positive way. The latter is 
a very signi.cant fac­tor when sky light is rather stronger than direct sunlight, such as at sunset/sunrise, 
even though sky light has been ignored in previous methods. This paper describes an ef.cient calculation 
method for light scattering due to clouds taking into account both multiple scatter­ing and sky light, 
and the modeling of clouds. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image 
Generation I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Key Words: Clouds, Multiple 
scattering, Sky light, Participating Media, Optical Length, Photo-realism, Radiative Transfer   1 INTRODUCTION 
Display of natural scenes such as mountains, trees, the earth, the sea, and the waves have been attempted. 
This paper discusses the display of clouds. The display of clouds is indispensable for the background 
images of buildings and .ight simulators. For display­ing clouds, mapping of fractal textures onto ellipsoids 
is often used. However, we discuss a display method taking account of light scat­tering due to cloud 
particles illuminated by sky light. The color of clouds varies according to the relationship between 
the viewing di­rection and the position of the sun. The intensity of clouds is depen­dent on absorption/scattering 
effects due to clouds particles. The albedos of clouds are very high: It is well known that for objects 
with such a high albedo multiple scattering can not be ignored[4]. Clouds are illuminated by both direct 
sunlight and sky light affected by atmospheric scattering. Their re.ected light from the ground (or the 
sea) also can not be ignored, and we take these ef­fects into account. That is, this paper discusses 
not only a local illu­mination model of clouds, but also a global illumination model tak­ing into account 
the color variation of incident light on them passing through the atmosphere and sky light. A brief description 
of our proposed method is as follows. Clouds are de.ned by density .elds, which are modeled by the metaball 
technique. Shapes of clouds are modeled by applying the fractal technique to metaballs. The particles 
they consist of have strong Permission to make digital or hard copies of part or all of this work or 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage and that copies bear this notice and the full citation on the first page. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 forward scattering characteristics. 
This was considered as one of the dif.culties due to the intensity calculation in the previous work. 
We use this characteristic in a positive way. That is, the space, which should be calculated, is restricted 
because the scattering direction is very narrow. For the calculation of multiple scattering, the space 
containing the clouds is subdivided into a number of volume ele­ments (voxels). As a preprocess, a sample 
space is prepared, which is de.ned as a parallelepiped consisting of a set of some voxels with the average 
density of the clouds, the high order of scattering at a speci.ed voxel from the other voxels in the 
space is calculated and stored, before the calculation of scattering due to every voxel in the total 
space. By using this pattern which is the contribution ratio at each voxel in the sample space to the 
speci.ed voxel, the calcula­tion cost for the total space can be reduced. At least the 3rd order of scattering 
is calculated in our paper. The spectrum and spatial distribution of sky light are precalculated by taking 
into account Rayleigh scattering and Mie scattering by assuming negligible at­tenuation due to cloud 
particles. The intensity of the .rst order of scattering at each voxel due to sky light (including re.ected 
light from the ground) can be easily calculated by using the optical depths from the cloud surface, stored 
in a look-up table. Finally, several examples are demonstrated in order to show the effectiveness of 
the method proposed here.  2 Previous Work Density volume display methods such as those of clouds in 
previ­ous work are divided into two categories, mapping technique and physical model taking into account 
scattering/absorption due to par­ticles. For the former, Gardner[7] used a mapping technique of frac­tal 
textures onto ellipsoids. For the latter, the displaying of the at­mosphere(sky color), water color, 
and particles such as ice has been developed: a) for light scattering from particles in the air, the 
shafts of light caused by spot lights[18][9], light beams passing through gaps in clouds or through leaves[15], 
the sky color taking account of atmospheric scattering[14][12], scattered light due to nonuni­form density 
particles such as clouds and smoke[18][24][10][26], the color of the atmosphere viewed from space[20], 
the effect of the radiosity of a participating medium[23], and multiple anisotropic volume scattering[16][10][1]. 
b) the display of the color of water affected by particles in the water, such as ponds[11], the color 
of the sea as viewed from outer space[20] and optical effects such as shafts of light within water[22]. 
and c) the display of Saturn s rings (re­.ective ice particles)[2], and subsurface scattering such as 
skin[8]. In this paper we focus our discussion on multiple scattering. For high albedo particles, multiple 
scattering should be calculated. On this, Kajiya[10] was .rst to offer a solution. For multiple scattering 
the two-pass method[10][16][23] is usually used. The .rst pass de­posits .ux from the light source and 
the light scattered at each cloud Figure 1: Light paths through clouds. voxel. The second pass gathers 
the scattered light along each view­ing ray. Kajiya used spherical harmonics to express the intensity 
distribution of scattered light at each voxel as a function of direc­tion. As cloud particles have strong 
forward scattering character­istics (caused by a narrow phase function), a relatively high order of spherical 
harmonics are required for representation of the dis­tribution of light scattered. In the case of the 
phase function de­scribed in section 3.3, the 20th order of harmonics and more than 400 coef.cients at 
each voxel may be required. Max s approach[16] is to allocate the radiosity leaving each volume element 
into a col­lection of Mdirection bins of constant intensity. This discrete or­dinate method also requires 
many direction bins to express such a narrow scattering beam. Even though the distribution of scattering 
becomes slightly isotropic after multiple scattering, some error ap­pears in the .rst order of scattering. 
Blasi[1] used the Monte Carlo method for determining the scattering direction of photons, but he did 
not calculate the scattered component in the viewing ray. There­fore, this method can not be considered 
as one which takes into ac­count anisotropic phase function. Hanrahan[8] proposed a method for subsurface 
scattering, but this method is limited to layered sur­faces such as skin and leaves; this method is not 
applicable to com­plex shapes such as clouds. Stam [26] took into account multiple scattering to display 
gaseous phenomena such as .re. He employed LU-decomposition to solve a matrix equation. He ignored the 
phase function. Here, we propose a method which takes into account multiple scattering, sky light effects, 
and re.ection from the ground.  3 Shading Model for Clouds 3.1 Basic Ideas In order to render the particles 
in clouds, the following elements should be taken into account: (i) Phase functions should be taken into 
account; scattering by small particles such as air molecules is called Rayleigh scattering, and scattering 
by aerosols such as dust is called Mie scattering. The sizes of particles in clouds are relatively large 
(i.e., 2-40 fm), so the particles have strong forward scattering. (ii) The multiple scattering of light 
among particles in clouds can not be neglected because their albedos are very high[4][25]: 0.7 ­ 0.9 
for cumulus and stratus. (iii) The clouds are illuminated by both direct sunlight and sky light. And 
they are also illuminated by the re.ected light of direct sunlight and sky light from the ground (i.e., 
re.ected groundlight). (iv) The effects of the atmosphere should be considered. The scattered light at 
the clouds is attenuated by par­ticles in the atmosphere and reaches the viewpoint. Sunlight is ab­sorbed 
when light passes through the atmosphere. (v) The density distributions of clouds are not uniform. As 
shown in Fig.1, the following optical paths or optical effects also should be considered. (1) Particle 
Pin a cloud is illuminated by direct sunlight (Isun), scattered light from the other particles (path 
 reflection ground scattering absorption scattering absorption  scattering light from the ground sky 
light light from PP  av Ic I sun  IvIs absorptionsunlight viewpointIa  scattering Figure 2: Block 
diagram for the intensity reaching the viewpoint passing through clouds. P1P2P), sky light (or sky radiance), 
and re.ected light from the earth. (2) The light reaching viewpoint is determined by every par­ticle 
on the viewing ray; the incident light of sky light to the cloud at Pb(Is), particles in clouds (PbPa), 
and particles in the atmosphere (PaPv). The intensity is obtained by integrating all the scattered light 
due to these particles. The attenuation of light due to these particles is also considered. (3) The sky 
light consists of light scat­tered by particles in the air. The spatial and spectrum distribution of 
sky light depends on the sun position. The atmosphere consists of air molecules and aerosols; their scattering 
characteristics obey Rayleigh scattering and Mie scattering, respectively. The density distributions 
of air molecules and aerosols vary exponentially with altitude. (4) The re.ected light from the earth 
consists of the direct sunlight and sky light. The direct sunlight is attenuated by the path PsPeand 
the re.ected light is also attenuated by the path PeP.The scattered light due to particles on these paths 
are added. As described above, the optical paths are complicated, we can express these optical effects 
as the block diagram, as shown in Fig.2. This paper discusses a rendering algorithm for clouds taking 
into account at least the 3rd order of multiple scattering.  3.2 Calculation of Light Scattering for 
Clouds Let s discuss .rst the calculation method for single scattering due to cloud particles. As shown 
in Fig.1, let s denote the intensity of incident sunlight to a cloud as Icand the intensity of that behind 
the cloud (i.e., sky light) as Is, the intensity of light from the cloud at Pa, Ia,which is the summation 
of the attenuated light of scattering on PaPbof the viewing ray and the attenuated sky light Isdue to 
the cloud. Iacan be obtained by the following equation: Ia()) Is())exp(.T(PaPb; ) )) Z Pb + Ip())j p( 
l) F ( 0 ) exp(.T(P P a ; ) ))dl; (1) Pa where the .rst term means the incident light at Pbis attenuated 
by particles on the path of PaPb, and the second the scattered light due to particles on the path. )is 
the wavelength of the light, Fthe scattering phase function indicating the directional characteristics 
of scattering, 0the scattering angle (see Fig. 1), pthe density, Is the intensity of the sky light in 
the viewing direction, Tthe optical length obtained by integrating the attenuation coef.cient jalong 
R S the path, i.e., given by T(S;))jp(s)ds(Sis path length). 0 As the incident light at point Phas been 
attenuated due to move­ment through the cloud (PcP), the incident intensity at Pis ob­tained by, Ip())Ic())exp(.T(PPc;)));(2) 
where Icis the attenuated light of Isunwhich is the solar radiation at the top of the atmosphere. Equation 
(1) is rewritten by using gains light  Figure 3: Calculation of multiple scattering and reference 
pattern. Gaand Gs; Ia())Is())Ga())+Ic())Gs()):(3) Therefore, the intensity reaching the viewpoint is 
represented by the block diagram as shown in Fig. 2. As shown in Figs. 1 and 2, the types of the incident 
light onto the cloud are the intensity of sky light in the viewing direction, Is, the attenuated intensity 
of Isun,sky light Isky, and the re.ected light from the ground. See reference [20] for the calculation 
method for atmospheric scattering. The light reaching viewpoint Pvcan be obtained as the remainder after 
scat­tering and absorption due to air molecules along the path between Paand Pv. The calculation methods 
for multiple scattering and sky light (including re.ection from the ground) are described in section 
3.4 and 3.5, respectively.  3.3 Phase Function Scattering of light due to particles (water droplets) 
in clouds obeys Mie scattering. The characteristics of scattering depend on the size of the particles, 
and have strong forward scattering. That is, when the scattering angle 0(see Fig. 1) is greater than 
10., the scattering intensity becomes less than 10%, compared to that at angle 0..As various sizes of 
particles exist, the phase function is expressed by the linear combination of several phase functions; 
K X F(0)wiFi(0);(4) i=0 where Kis the number of types of functions, and wiis weight for phase function 
i. The Henyey-Greenstein function is well known as a phase function. Recently, Cornette[6] improved it, 
which gives more reasonable physical expression: 22 3(1.g)(1+cos0) F(0;g) ;(5) 2 2(2+g2)(1+g.2gcos0)3/2 
where gis an asymmetry factor and is a function (see [20]) which is determined by the cloud condition 
and the wave length (see references[6] for the parameters of clouds). If g0, this function is equivalent 
to Rayleigh scattering. In this paper, two functions, Rayleigh (e.g., 5%) and Mie scattering (the remainder) 
for cloud particles, are combined. For scattering due to clouds the spectrum of scattering is not much 
in.uenced compared with scattering due to air molecules. Therefore, the color of clouds depends on the 
spectrum of incident light. 3.4 Multiple Scattering For solving multiple scattering phenomena, two methods 
have been developed; solving integral equations and using the Monte Calro method. This paper combines 
both of these. The space including some clouds is subdivided into a number of volume elements, and shooting/receiving 
energy among them is calculated. For calculat­ing its exchanging energy among the elements, a form factor 
gener­ally used in radiosity methods is useful. Rushmeier[23] introduced the volume/volume form factor: 
form factor Fkjrepresents the ratio of the total scattered and emitted energy leaving element Vkwhich 
is absorbed or scattered by volume element Vj, but her method is limited to isotropic scattering. The 
phase function is an important factor for energy transport. In our method the form factors are cal­culated 
by taking into account the phase function. That is, in the nu­merical integral for form factors the phase 
functions having angles between the viewpoint and sub-voxels are multiplied. Let s con­sider the beam 
spread which is an angle including most of scattered energy (see 0bin Fig. 3). Every volume element within 
the beam spread of the phase function is subdivided into smaller sub-voxels compared with the regular 
voxels. Let s denote the intensity at point Pin direction !as I(P;!), the extinction coef.cient per unit 
length as j, the length of cloud in viewing ray S, the path length from Pas s(s0at P, PS; S from P). 
Then I(P;!)is expressed by I(P;!)I(PS;!)exp(.T(PPS))+ ZZ S 1 0 0jp(s)exp(.T(P(s)P))F(0)I(P;!)d!0lds;(6) 
47 s=04. where 0is phase angle between !and !0: !0is angular variable for integration. The problem is 
that Iexists in both sides of the equa­tion. To solve this, this space is subdivided into a number of 
volume elements. If we denote the number of voxels as N, and the number of the discrete directions as 
M,then MNmatrix equations should be solved. For a narrow phase function (strong forward scattering), 
the ma­trix becomes sparse because even if some volume elements existing outside of the beam spread shoot 
their energy to the volume element Pto be calculated, the energy hardly contribute to the intensity at 
Pin the viewing direction. The form factors for distant pairs of ele­ments are very small. Therefore, 
we can predict which voxel affects a speci.ed volume element. That is, a sample space is prepared, light 
scattering in the space is calculated before the calculation of scattering due to every voxel in the 
whole space. By using the con­tribution pattern of voxels in the sample space, the calculation cost can 
be reduced. Fig.3 shows a sample space, the ellipsoid means the scaled phase function when the viewpoint 
is assumed as a light source. This region tells us which voxels contribute to scattering at point P. 
The contribution ratio of voxel at P1to Pis high because it exists within the beam spread. For a given 
viewpoint and a light direction, assume the uniform density (average density), and then calculate the 
voxels which af­fect a speci.ed voxel. The scattered light in the viewing direction at point Pthrough 
every path of the 2nd and 3rd orders of scattering are calculated (e.g., P1P; 2nd order path,P3P2P; 3rd 
order path in Fig.3). These results, the 2nd and 3rd order components, are stored at each voxel and the 
total scattered intensity at Pdue to every voxel is calculated. By using these results, each contribution 
ratio to the total intensity can be obtained. We refer to this contribution-ratio pattern in a look-up 
table as a reference pattern (or a template).For simplifying, the z-axis of the voxel is assumed to be 
in line with the viewing ray. That is, the edges of the voxel coincide with the prin­cipal axes of the 
eye coordinates system. For Nvoxels, the number of paths for the 2nd order of scattering is (N.1), and 
the number of paths for the 3rd order of scattering is (N.1)2. In our experiments, when 8.8.16voxels 
are used as a reference pattern, 1,046,529 paths were required for the 3rd order of the scattering, but 
only 400 paths effectively contribute 90% of the total intensity. The above size of voxels is just an 
example, we used more large sizes of the reference patterns as described later. For the 3rd order of 
scattering, it is equivalent to that the intensity is deter­mined by multiplying two form factors and 
three phase functions of angles of 00 ;00and 00, as shown in Fig.3. 123 In our examples, the percentages 
of the 2nd and 3rd order of scat­tering are roughly 10-40% and 1-3% for small phase angles (less than 
10.), 40-70% and 2-10% for large phase angles: these data de­pend on the conditions such as cloud density. 
For the large phase angles, the higher order of scattering may be required. But to save the computation 
time, we truncated more than 4th order of scatter­ing. In general, as geometric factors, such as form 
factors and phase angles, are much more effective compared with the density distri­bution, we assumed 
that the contribution pattern of voxels taking into account non-uniform density is close to the reference 
pattern of uniform density. The voxels in the whole space are scanned by the reference pattern; this 
process is similar to .ltering in image pro­cessing, in our case the reference pattern being equivalent 
to a 3-D .lter. The proposed algorithm includes the following steps: step 1) : The center of the reference 
pattern is set to voxel Pto be calculated, and the intensities of light scattered due to other voxels 
affecting voxel Pare calculated and are accumulated on the strength of the reference pattern; the voxels 
whose contribution ratios are higher than a given threshold are selected. The form factors be­tween each 
pair of voxels are stored over the look-up table along with the reference pattern. In our method, the 
modi.ed form factors are stored as described before: they are obtained by multiplying the values of phase 
functions. step 2) : The attenuation ratio (or optical length) for the sunlight at each voxel is stored. 
Moving the reference pattern, voxel by voxel, over the whole space, the light scattered (the 2nd and 
3rd order of scattering) in the viewing direction at each voxel can be stored. The density distribution 
of the sample space for the reference pattern and the whole space to be calculated are different. So, 
the attenua­tion between the voxels should be calculated, even though the stored data in the reference 
pattern for the form factors and the values of phase functions can be used. step 3) : For each pixel, 
the intensity is obtained by line integral: the intensity of the 1st order of scattering at a sampling 
point on the viewing ray is calculated by using the attenuation ratio stored at each voxel, and the intensities 
of the 2nd and 3rd order of scattering at the sampling point are interpolated from them stored at voxles. 
As examples, Fig.4 shows the distribution of voxels with high contribution ratios in the cases of the 
phase angle of 10.and 160. , respectively. In the .gure, the viewpoint is located on the left side, the 
black lines show the paths having a high contribution to the 2nd order of scattering (i.e., scattered 
twice like path P1Pin Fig.3), the green lines show the 1st path with a high contribution to the 3rd or­der 
of scattering (i.e., scattered three times), and the pink lines show the 2nd path with the maximum contribution 
to the voxels with a high contribution to the 3rd order of scattering. Let s consider the 2nd order scattering. 
As shown in Fig.5, the distribution of voxels which have high contribution ratios (e.g., more than 80%) 
are cate­gorized into the following two cases. Case A): light source and viewpoint are located in the 
opposite sides (Fig.5(a)). Case B): light source and viewpoint are located in the same side (Fig.5(b)). 
In both of these cases, three sub-spaces, Rf, Rb,and Rc, contribute to the light scattered at voxel Pin 
the viewing direction. The 1st order of scattering due to particles in Rfis strong because of the small 
phase angle, even though the 2nd order of scattering at Pis weak because of the large phase angle. Even 
though the 1st order of scattering in Rbis weak because of a large phase angle, the 2nd  (b) Phase angle 
160° Figure 4: Examples of reference patterns. order of scattering at Pis strong because of the small 
phase angle. In Rcdistances of voxels from Pare very close, so the form fac­tors are large even though 
the phase functions are small. As shown in these .gures, the sub-spaces with high contribution ratios 
depend on the light and viewing directions, and they also depend on the size of voxel, density, and extinction 
coef.cients. Thus the size of the reference pattern is adaptively determined. First, we prepare a large 
sample space (e.g., 30.30.30voxels). And we can get the rea­sonable size of the reference pattern by 
the following method. To get the reference pattern, every possible path in a sample space is examined, 
and the bounding box of voxels having high ratios is se­lected as the reference pattern. Even though 
the computational cost is reduced by this method, it is not suf.cient. Therefore, we employ the following 
additional stochastic method. The voxels to be calculated are selected by us­ing random numbers within 
the paths with a high contribution ratio. The summed intensity of these paths is corrected by using the 
ratio of the selected paths: assuming the number of the calculated paths to be n1, the number of paths 
with a high contribution rate to be n2, the summed intensity as I, and the total contribution ratio due 
to n2 paths as r, then the intensity can be estimated by Irn2.n1. In our experiments, only 10% of the 
voxels in the reference pattern con­tribute 50% of the total intensity in a case. To use these voxels 
with high possibility we can improve the accuracy. We uses the constant density assumption for the sample 
space. If the sub-space to be cal­culated has wide range of density, it is solved by increasing ratio 
r. We set r0:8in our examples. In the preprocessing stage (i.e., step 1: obtaining the reference pattern), 
the paths, whose contribu­tion ratio are higher than r, are selected. Though we have discussed multiple 
scattering up to the 3rd order, the idea using the reference pattern can be expanded to higher orders. 
Applying the method described above, the intensity at each voxel can be obtained and be stored. In the 
rendering step, the intensities at each sampling point on the viewing ray are integrated. In order to 
obtain high accuracy, the intensity of the .rst order scattering is calculated at each sampling point 
even though tri-linear interpola­tion is used for the intensity of the higher order of scattering and 
the attenuation (or optical depths) stored at each voxel. 3.5 Atmospheric Effects (Sky Light) Clouds 
are visible even if the sun is hidden behind other clouds, af­ter sunset or before sunrise; in other 
words, the intensity of the sun­ (a) R Rb c Rf light viewpoint (b) Figure 5: Sub-spaces with high 
contribution ratios. light is rather feeble. This implies that sky light can not be ignored. The intensity 
of sky light is determined by the scattering and/or absorption of air molecules and aerosols. The former 
obeys Rayleigh scattering (proportional to the fourth power of the wave length) while the latter obeys 
Mie scattering. The color of sky light changes depending on the altitude of the sun and the observer 
s po­sition. Note that the intensity of the sky around the sun is stronger than elsewhere. See reference[20] 
for the calculation method for sky light. Direct sunlight attenuates when passing through the at­mosphere. 
The incident light to a cloud (see Figs. 1 and 2), Is,is attenuated sunlight and the light scattered 
due to particles in the at­mosphere. In meteorology it is well known that the brightness of clouds is 
affected by the re.ected light from the ground. The re.ected light from the ground is assumed as upward 
sky light because of the scattered light due to particles between the cloud and the ground. The re.ected 
light has two components; direct sunlight and sky light. See reference[20] for the calculation method 
for the re.ected light from sky light. Even though the albedo of the ground de­pends on materials such 
as soil, trees, sand, we used average albedos (spectrum re.ectivity in this case) used in reference[25] 
which are weighted averages of that of each material. Single scattering for sky light (and re.ected light 
from the ground) is calculated as follows. Ignoring attenuation by cloud par­ticles, a particle (i.e., 
voxel) is illuminated by the sky light from ev­ery direction: The light scattered in the viewing direction 
arrives at the viewpoint. Since the sky dome can be considered a hemisphere with a large radius[17], 
the particle can be considered at the center of the hemisphere. Thus, it is possible to take it for granted 
that the radiance distribution incident onto each particle in a cloud is identi­cal (assuming the difference 
in the altitude is small). In the case of the calculation of sky light on the ground, the hemisphere 
is enough. But clouds exist at a high altitude, so the sky light from the bottom should also be considered: 
the top hemisphere is due to pure sky light and the bottom hemisphere consists of the light scattered 
due to particles in the bottom of clouds and the attenuated light of re.ected light from the ground. 
The sky dome is divided into several sky el­ements, and the intensity of the light scattered in the view 
direction Ivis obtained by calculating the solid angle of the sky elements and their intensities. In 
practice, the incident light onto a particle is attenuated by the other particles in the cloud. Therefore, 
the attenuation factor (trans­mittance) toward each sky element which is caused by them and which is 
obtained from its optical length, should be taken into ac­ sky element sky dome z (b) Figure 6: Calculation 
of skylight. count. Let s denote the number of the sky elements as K, Ivis ob­ tained by K Z K X X Iv()) 
tl F(0)Isky(! ; ) ) d! tlIl()); (7) l=0 !l l=0 where tlis the attenuation factor for the direction of 
sky element l due to cloud particles, wlis a solid angle of sky element l, Fthe phase function, and Iskyis 
the intensity of sky element l,and Ilis the integrated intensity for the sky element. In the rendering 
pro­cess, the attenuation factors tlfrom each particle on the viewing ray to all sky elements must be 
calculated. To calculate these ef.ciently, we use a look-up table. We make use of the fact that it is 
easy to cal­culate the optical length in both the principal and diagonal axes of the voxel. Let s consider 
the six faces comprising of the voxel as a sim­ple example (see Fig. 6(a)). The incident light of sky 
light passing through each face can be calculated by setting the eye at the cen­ter of the cube. Each 
face of the cube is divided into small meshes, which is similar to the hemi-cube method[5]. The attenuation 
fac­tors (or optical depths) in x, y,and z-directions can be used for six sky elements. Thus, if the 
accumulated attenuations for each axis are calculated once and stored at each voxel, it is suf.cient 
to get the intensity at the viewpoint by multiplying the attenuation coef.cient tlby the intensity at 
the voxel and make a summation of them. Note that, in the case of the .rst order scattering, the intensity 
of the sky in the viewing direction is the most important as the forward scattering is strong. For example, 
let s consider the attenuation in ydirection at voxel P(see Fig. 6(b)). At voxel P, the attenuation, 
t, between Pand Puwas stored. If the attenuation at Pdis td, the attenua­tion between Pdand Pis easily 
calculated as t.td. Even though we only describe for six directions, it is possible to increase the ac­curacy 
by preparing the attenuations in other directions, such as di­agonal. As described above, sky elements 
around the sun are very signi.cant since the intensity surrounding the sun is much greater. To take this 
into account, the attenuation in the sun s direction stored at each voxel can be utilized. In addition 
to the density, the attenuation in the sun s direction, the attenuation factors in K.2directions, and 
the intensity of light due to the higher order scattering are stored for each voxel.  4 MODELING OF 
CLOUDS For modeling clouds, controlling its shape and density distribution is necessary. In this paper, 
the density distribution of the cloud is de.ned by using the meta-ball technique[21] (or blobs[3]). Each 
meta-ball is de.ned by its center, radius, and the density at the center of the ball. The .eld value 
at any point is de.ned by distances from the speci.ed points in space. The density distribution of a 
meta-ball is given by a polynomial function in degree 6 of the distance from its center([21]). The surface 
of a cloud is de.ned by the isosurfaces of potential .elds de.ned by the meta-balls. In the rendering 
process, the intersection of the isosurface (i.e., cloud surface) with a viewing ray is calculated by 
ray tracing, which effectively expresses the density distribution on a ray by using B´ezier function 
of degree 6 (see [21]). B´ezier Clipping[19] is em­ployed for the calculation of intersections. First, 
several meta-balls are arranged to form the basic shape of the cloud. Then, small meta-balls on the surface 
of the cloud are generated recursively by using the following fractal method to form the subtle fringe 
of the cloud. The method of generating new balls on a curved surface is as fol­lows. The isosurface is 
extracted by the marching-cubes algorithm [13] and triangulated. New balls are then generated within 
each tri­angle (their positions and radii are determined randomly). Generat­ing new balls produces a 
new surface, which is again triangulated. And again new balls are generated. This procedure is repeated. 
By making the radius smaller the further from the center of the cloud, the smaller balls appear around 
the fringe. The bottom of a cloud is sometimes relatively .at, and the top of it is bumpy. To realize 
this kind of shape, the normal vectors of the generated triangles on the iso-surface can be used. If 
the normal vector is not downward new balls are created, otherwise the gener­ation of balls is limited. 
By using this technique we can control the cloud shape, and make possible clouds which grow upwards. 
 5 EXAMPLES Fig. 7 shows an example of clouds: the altitude of the sun in Figs.(a), (b), and (c) is 65. 
, 10.and 5.(sunset), respectively. These examples depict beautiful variations in the color of the clouds 
and sky. One can see the bright edges of cloud in Fig.(b) because the sun is behind the cloud. Fig.8 
shows examples of cumulonimbus. The atmospheric effect between the clouds and the viewpoint is calculated, 
so the color of the clouds is bluish (Fig.(a)) or reddish (Fig.(b)). The number of meta-balls for these 
clouds for Figs.7 and 8 are 338 and 358, respec­tively. This calculation was done on an IRIS Indigo2 
(R4400). The computation times for Fig.7 (a), and Fig.8 (a) were 20 minutes, and 31.3 minutes, respectively 
(image width=500). In these examples, the size of voxels is 1003in average, but if they are sparse, we 
could save the memory by using the list­structure. 6 CONCLUSION We have proposed an algorithm for a 
physical based image synthe­sis of clouds. As shown in the examples, the proposed method gives us photo-realistic 
images taking into account anisotropic multiple scattering and sky light. The advantages of the proposed 
method are as follows: (1) For anisotropic multiple scattering, the optical paths of the light scattered 
in the viewing direction are limited because of strong forward scattering (a narrow phase function). 
Employ­ing the pattern expressing the contribution ratio at each voxel to the speci.ed voxel in the sample 
space, the calculation cost for the total space can be reduced. (2) For sky light, the spectrum of the 
sky light is calculated by tak­ing into account scattering/absorption due to particles in the atmosphere, 
and the intensity of light scattered at one voxel illuminated by each sky element is stored. The intensity 
of the .rst order scattering at every voxel due to sky light can be easily calculated by using the optical 
depths from the cloud surface stored in a look-up table. The re.ected light from the ground can be calculated 
by treating it as upward sky light. (3) The clouds can be modeled by using metaballs. The compli­cated 
cloud surfaces are generated by a fractal technique ap­plying to metaballs.  Shading models for clouds 
and snow are basically the same be­cause the intensity from clouds or snow reaching the viewpoint is 
determined by light scattered and absorbed due to particles in clouds/snow. Even though this paper discussed 
a display method of clouds, the proposed method can be applied to snow. Acknowledgment The authors wish 
to thank Prof. Yamashita and Kaneda in Hi­roshima University for many valuable discussions. The original 
ti­tle of our paper included snow . But it and the part of the paper dealing with snow are removed by 
following the reviewers advice. We are going to submit the paper on snow as a separate paper. We would 
like to acknowledge to the reviewers for their helpful com­ments.  References [1] P. Blasi, B.L. Saec, 
C. Schlics, A Rendering Algorithm for Discrete Volume Density Objects, Proc. of EUROGRAPH­ICS 93, Vol.12, 
No.3 (1993) pp.201-210. [2] J.F. Blinn, Light Re.ection Functions for Simulation of Clouds and Dusty 
Surfaces, Computer Graphics, Vol. 16, No. 3 (1982) pp. 21-29. [3] J.F. Blinn, A Generalization of Algebraic 
Surface Drawing, ACM Tog, Vol.2, No.3 (1980) pp.235-256. [4] C.F. Bohren, Multiple scattering of light 
and some of its ob­servable consequences, Am.J.Phys. Vol.55, No.6 (1987) pp.524-533. [5] M.F. Cohen, 
D.P. Greenberg, The Hemicube, A Radiosity Solution for Computer Environment, ,Computer Graphics, Vol.19, 
No.3 (1985) pp.31-40. [6] W.M. Cornette, J.G. Shanks, Physical reasonable analytic expression for the 
single-scattering phase function, Applied Optics, Vol.31, No.16 (1992) pp.3152-3160. [7] G.Y. Gardener, 
Visual Simulation of Clouds, Computer Graphics, Vol.19, No.3 (1985) pp.297-303. [8] P. Hanrahan, W. Krueger, 
Re.ection from Layered Surfaces due to Subsurface Scattering, Proc. of SIGGRAPH 93 (1994) pp.165-174. 
 [9] M. Inakage, Volume Tracing of Atmospheric Environments, The Visual Computer, 7 (1991) pp.104-113. 
[10] J.T. Kajiya, B.V. Herzen, Ray tracing Volume Densities, Computer Graphics, Vol.18, No.3 (1984) 
pp.165-174. [11] K. Kaneda, G. Yuan, E. Nakamae, T. Nishita, Photorealis­tic Visual Simulation of Water 
Surfaces Taking into account Radiative Transfer, Proc. of CG &#38; CAD 91, (China) (1991) pp.25-30. [12] 
K. Kaneda, T. Okamoto, E. Nakamae, T. Nishita, Photoreal­istic Image Synsesis for Outdoor scenery Under 
Various At­mospheric Conditions, The Visual Computer, Vol.7 (1991) pp.247-258. [13] W.E. Lorensen, H.E. 
Cline, Marching Cubes: a High Resolu­tion 3D Surface Construction Algorithm, Computer Graph­ics, Vol.21, 
No.4 (1987) pp.163-169. [14] R.V. Klassen, Modeling the Effect of the Atmosphere on Light, ACM Transaction 
on Graphics, Vol. 6, No. 3 (1987) pp. 215-237. [15] N. Max, Light Diffusion through Clouds and Haze, 
Graph­ics and Image Processing, Vol.33, No.3 (1986) pp.280-292. [16] N. Max, Ef.cient Light Propagation 
for Multiple Anisotropic Volume Scattering, Proc. of the Fifth Eurographics Work­shop on Rendering (1994) 
pp.87-104. [17] T. Nishita, and E. Nakamae, Continuous tone Representa­tion of Three-Dimensional Objects 
Illuminated by Sky Light, Computer Graphics, Vol. 20, No. 4 (1986) pp. 125-132. [18] T. Nishita, Y. Miyawaki, 
E. Nakamae, A Shading Model for Atmospheric Scattering Considering Distribution of Light Sources, Computer 
Graphics, Vol. 21, No. 4 (1987) pp. 303­ 310. [19] T. Nishita, T.W. Sederberg, M. Kakimoto, Ray Tracing 
Rational Trimmed Surface Patches, Computer Graphics, Vol.24, No.4 (1990) pp.337-345. [20] T. Nishita, 
T. Shirai, K. Tadamura, E. Nakamae, Display of The Earth Taking into Account Atmospheric Scattering, 
 Proc. of SIGGRAPH 93 (1993) pp.175-182. [21] T. Nishita, E. Nakamae, A Method for Displaying Metaballs 
by using B´ezier Clipping, Proc. of EUROGRAPHICS 94, Vol.13, No.3 (1994) c271-280. [22] T. Nishita, E. 
Nakamae, Method of Displaying Optical Ef­fects within Water using Accumulation Buffer, Proc. of SIG-GRAPH 
94 (1994) pp.373-379. [23] H.E. Rushmeier, K.E. Torrance, The Zonal Method for Cal­culating Light Intensities 
in The Presence of a Participating Medium, Computer Graphics, Vol.21, No.4 (1987) pp.293­ 302. [24] G. 
Sakas, M. Gerth, Sampling and Anti-Aliasing of Dis­crete 3-D Volume Density Textures, Proc. of EUROGRAPH­ICS 
91 (1991) pp.87-102. [25] S. Sekine, Corrected Color Temperature of Daylight(2) : Characteristics on 
Clear Sky and Overcast Sky, J. Illumina­tion Engineering Inst. Japan , Vol.79, No.11 (1995) pp.621­ 627. 
[26] J. Stam, E. Fiume, Depicting Fire and Other Gaseous Phe­nomena Using Diffusion Processes, Proc. 
of SIGGRAPH 95 (1995) pp.129-136.  Figure 7. Examples of clouds. Figure 8. Examples of cumulonimbus. 
High-resolution TIFF versions of these images can be found on the CD-ROM in: S96PR/papers/nishita  
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237278</article_id>
		<sort_key>387</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Modeling and rendering of metallic patinas]]></title>
		<page_from>387</page_from>
		<page_to>396</page_to>
		<doi_number>10.1145/237170.237278</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237278</url>
		<keywords>
			<kw><![CDATA[material models]]></kw>
			<kw><![CDATA[reflection models]]></kw>
			<kw><![CDATA[time-dependent phenomena]]></kw>
			<kw><![CDATA[weathering and appearance]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P150906</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology, Room NE43-213, 545 Technology Square, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, 370 Gates Computer Science Building 3B, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BARABASI, A. L., AND STANLEY, H. E. Fractal Concepts in Smface Growth. Cambridge University Press, Cambridge, 1995.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BECKET, W., AND BADLER, N. I. Imperfection for realistic image synthesis. Journal of Visualization and Computer Animation 1, 1 (Aug. 1990), 26-32.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLINN, J. F. Light reflection functions for simulation of clouds and dusty surfaces. Computer Graphics 16, 3 (July 1982), 21-29.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COOK, R. L. Shade trees. Computer Graphics 18, 3 (July 1984), 223-231.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237280</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DORSEY, J., PEDERSEN, H. K., AND HANRAHAN, P. Flow and changes in appearance. In Computer Graphics P~vceedings (1996), Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[EBERT, D. S., Ed. Texturing and Modeling. Academic Press, New York, 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FLEMING, S. J. Dating in Archaeology. St. Martin's Press, New York, 1977.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FRANEY, J. P., AND DAVIS, M. E. Metallographic studies of the copper patina formed in the atmosphere. Corlvsion Science 27, 7 (1987), 659-688.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FRENCH, L. Toy story. Cinefantastique 27, 2 (1995), 36-37.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GRAEDEL, T. E. Copper patinas formed in the atmosphere - a qualitative assessment of mechanisms. Corlvsion Science 27, 7 (1987), 721-740.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GRAEDEL, T. E., NASSAU, K., AND FRANEY, J. P. Copper patinas formed in the atmosphere. Corlvsion Science 27, 7 (1987), 639-652.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>146452</ref_obj_id>
				<ref_obj_pid>146443</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HAASE, C. S., AND MEYER, G. W. Modeling pigmented materials for realistic image synthesis. ACM Tran. Graphics 11, 4 (Oct. 1992), 305-335.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND KRUEGER, W. Reflection from layered surfaces due to subsurface scattering. In Computer Graphics P~vceedings (1993), Annual Conterence Series, ACM SIGGRAPH, pp. 165-174.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND LAWSON, J. A language for shading and lighting calculations. Computer Graphics 24, 4 (Aug. 1990), 289-298.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617946</ref_obj_id>
				<ref_obj_pid>616034</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HSU, S., AND WONG, Y. Simulating dust accumulation. IEEE Computer Graphics and Applications 15, 1 (Jan. 1995), 18-22.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HUGHES, R., AND ROWE, M. The Colouring, B~vnzing and Patination of Metals. Watson-Guptill Publications, New York, 1991.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[JUDD, D. B., AND WYSZECKI, G. Color in Business, Science, and Industry. John Wiley &amp; Sons, New York, 1975.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[KORTUM, G. Reflectance Spect~vscopy. Springer-Verlag, New York, 1969.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[KUBELKA, P. New contributions to the optics of intensely light-scattering material, part I. J. Opt. Soc. Am. 38 (1948), 448.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[KUBELKA, P. New contributions to the optics of intensely light-scattering material, part II: Non-homogeneous layers. J. Opt. Soc. Am. 44 (1954), 330.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[KUBELKA, P., AND MUNK, F. Ein beitrag zur optik der farbanstriche. Z. tech. Physik. 12 (1931), 593.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MATTSSON, E. Basic Cor~vsion Technology for Scientists and Engineers. Ellis Horwood Limited, New York, 1989.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192244</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[MILLER, G. Efficient algorithms for local and global accessibility shading. In Computer Graphics P1vceedings (1994), Annual Conference Series, ACM SIG- GRAPH, pp. 319-326.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[MOSTAFAVI, M., AND LEATHERBARROW, D. On Weathering: The Life of Buildings in Time. MIT Press, Cambridge, MA, 1993.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[NEWMAN, R. C., AND SIERADZKI, K. Metallic corrosion. Science 263 (1994), 1708-1709.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. An image synthesizer. Computer Graphics 19, 4 (July 1985), 287- 296.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[PORTER, T., AND DUFF, T. Compositing digital images. Computer Graphics 18, 3 (July 1984), 253-259.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SIMPSON, J. W., AND HORROBIN, P. J. The Weathering and Pelformance of Building Materials. MTP Publishing Co, London, 1970.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[THOMAS, T. R., Ed. Rough Smfaces. Longman, New York, 1982.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Generating textures for arbitrary surfaces using reaction-diffusion. Computer Graphics 25, 4 (July 1991), 289-298.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[UPSTILL, S. The Renderman Companion. Addison-Wesley, New York, 1990.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[VERNON, W. H. J., AND WHITBY, L. The open air corrosion of copper, a chemical surface patina. Journal Instit. of Metals 42, 6 (1932), 181-195.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122750</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND KASS, M. Reaction-diffusion textures. Computer Graphics 25, 4 (July 1991), 299-308.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Modeling and Rendering of Metallic Patinas Julie Dorsey* Pat Hanrahany Massachusetts Institute of Technology 
Stanford University Abstract An important component that has been missing from image synthe­ sis is the 
effect of weathering. In this paper, we present an approach for the modeling and rendering of one type 
of weathering metal­ lic patinas. A patina is a .lm or incrustation on a surface that is produced by 
the removal of material, the addition of material, or the chemical alteration of a surface. Oxidation, 
sulphidization, and painting are examples of phenomena that produce patinas. We represent a surface as 
a series of layers. Patinas are simulated with a collection of operators, such as coat, erode, and 
pol­ ish, which are applied to the layered structure. The development of patinas is modulated according 
to an object s geometry and lo­ cal environmental factors. We introduce a technique to model the re.ectance 
and transmission of light through the layered structure using the Kubelka-Munk model. This representation 
yields a model that can simulate many aspects of the time-dependent appearance of metals as they are 
exposed to the atmosphere or treated chemically. We demonstrate the approach with a collection of copper 
models. CR Categories and Subject Descriptors: I.3.7 [Computer Graph­ics]: Three-Dimensional Graphics 
and Realism I.3.6 [Computer Graphics]: Methodology and Techniques. Additional Key Words and Phrases: 
weathering and appearance, material models, time-dependent phenomena, re.ection models.  1 Introduction 
All materials have an inherent tendency to change in appearance or composition when exposed to the physical 
and chemical condi­ tions of the surrounding environment. The rate of change is depen­ dent on the material 
s characteristics and the degree of its exposure. The deterioration, decay, and change in appearance 
of materials due to the effects of the surrounding environment are generally termed weathering. Speci.c 
examples of weathering include the corrosion of metals, ef.orescence on stone and brick, fungal attack 
on organic materials [28], and the wear and tear of everyday life. Techniques for realistic image synthesis 
have advanced dramati­ cally in recent years. However, a common criticism of such images is that they 
look too ideal, and therefore animators and modelers go to great lengths to create a more natural look. 
An outstanding ex­ ample of this approach is the techniques to simulate wear and tear in the recent movie 
Toy Story produced at Pixar [9]. Many texture maps per surface were used to model scuffs, dirt, gouges 
and so on. Unfortunately, this use of texture maps is labor intensive and some­ what ad hoc. Each texture 
map must be hand-painted and combined using special shaders. It is also very dif.cult to properly account 
for *Room NE43-213, 545 Technology Square, Cambridge, MA 02139. http://graphics.lcs.mit.edu/.dorsey y370 
Gates Computer Science Building 3B, Stanford, CA 94305-4070. http://www-graphics.stanford.edu/.hanrahan 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 Figure 1: Example of a real patina. certain effects, such as a 
spill that crosses the boundary between two patches. There is clearly a need for modeling and rendering 
tools that make it is easier to create naturally worn surfaces. In addition to its importance in computer 
graphics, visualization of weathering effects has broad applicability to a variety of addi­tional .elds. 
Moreover, practitioners in other .elds have studied weathering effects for many years, and their theories 
and studies are good sources from which to draw. For many applications it is useful to predict how a 
material will look in the future. For example, in architecture and preserva­tion, it is important to 
understand how the surfaces of buildings change over time. Since buildings may stand for hundreds of 
years, much of their .nal appearance is dominated by weath­ering effects [22, 24].  Conversely, there 
are applications where modeling weather­ing interactions is applicable to the inverse problem of under­standing 
the history of an object from its current appearance. Restoration fundamentally involves returning a 
weathered ob­ject to its true initial appearance [7].  Many materials are created pre-weathered. These 
materials are treated in various ways to simulate the process of weath­ering. Since such treatments can 
be highly desirable (from prewashed jeans to fake antiques), modeling them is impor­tant [24].  Our 
long range goal is to develop easily controllable models of weathered materials for computer graphics. 
This task involves the identi.cation of the basic physical processes underlying changes in appearance 
and the development of appropriate computer models. Since weathering involves the action of many environmental 
forces over time, this will require simulating these processes. Such simu­lations will also give us the 
ability to visualize changes over time. We expect that different materials may require different modeling 
effects. For example, stone, wood and metals weather quite differ­ently because their chemistry and material 
structures are very dif­ferent. In this paper we consider only the modeling and rendering of metallic 
patinas as a starting point. Metals are particularly susceptible to weathering interactions and often 
develop a characteristic patina. The term patination is gen­erally reserved for effects involving the 
chemical alteration of the surface resulting in a change in color. It may describe the results of either 
deliberately applied craft processes or natural corrosion [16]. Metals in general and the .nished metal 
surface in particu­lar begin to change under the in.uence of the atmosphere, or the local chemical environment, 
as soon as they are exposed. Copper and its alloys are particularly interesting, as they have broad aes­thetic 
as well as practical applications. 1.1 Previous Work To date, the simulation of weathering effects has 
been given little explicit attention in the computer graphics literature. Related work exists in three 
areas: procedural textures and fractal surface growth models, speci.c weathering models, and layered 
surface represen­tations. Procedural textures [6] can be used to build up complex patterns that often 
resemble natural effects. For example, the shade tree concept of Cook [4] allowed arbitrary procedures 
to de.ne a differ­ent shading model for each surface, as well as lighting and atmo­spheric optics. Perlin 
[26] described an entire procedural language to de.ne textures and laid the foundation for the stochastically­generated 
textures that permeate rendering today. Finally, Turk [30] and Witkin and Kass [33] introduced synthetic 
texture models in­spired by biochemical processes. In this paper, we use procedural textures in a new 
way: to vary parameters of a physically-inspired model of material properties over time. We draw on fractal 
surface growth models, used mainly in physics and various branches of en­gineering. These models are 
concerned with the morphology of var­ious pre-formed interfaces and with the dynamics of how the mor­phology 
develops over time [1]. Starting with procedural texture models, several researchers have attempted to 
simulate related weathering effects. Becket and Badler [2] modeled surface imperfections through texture 
speci.­cation and generation techniques, which are based on fractal subdi­vision and simple distribution 
models. Blinn modeled the appear­ance of dusty surfaces, given the thickness of the dust layer [3]. More 
recently, Hsu and Wong [15] introduced functions for simu­lating dust accumulation that attempt to mimic 
the dust adherence process. In addition, Miller [23] proposed a set of algorithms for lo­cal and global 
accessibility shading; this type of shading yields vi­sual effects that resemble tarnish on surfaces. 
We take a different approach focusing in considerable detail on one particular mate­rial and its changes 
in appearance due to weathering. A particularly promising method for modeling the appearance of materials 
is to treat a surface as a set of layers. For example, a com­mon application of RenderMan is to develop 
surface descriptions as a series of layers [31]. Another area of interest is the modeling of re.ection 
and transmission of light through layered surfaces. Han­rahan and Krueger [13] present a model for subsurface 
scattering in layered surfaces in terms of one-dimensional linear transport theory. This model is useful 
for rendering common layered materials such as skin, snow, and dust. In this work, we build on such layered 
rep­resentations by developing a set of tools for modeling and rendering surface and subsurface structures 
as a function of time. 1.2 Overview We begin by brie.y reviewing the physical basis of metallic pati­nas. 
We then present a representation of a metal surface as a stack of layers and propose a collection of 
operators that can be applied to the layered structure to produce a taxonomy of patination effects. We 
also discuss how patinas can be modulated according to environ­mental factors and the surface geometry. 
Next, we offer a method of rendering the layered structure. Last, we demonstrate the approach on several 
complex models.  2 Physical Basis of Patina Formation Natural patinas develop primarily as the result 
of the process of at­mospheric corrosion [22, 25]. The atmosphere alters the surface of a metal, causing 
gradual changes that quickly tarnish it with a thin, but uneven, dark coloring, and may eventually convert 
the whole surface into crumbly mineral products. In this section, we brie.y de­scribe the principal causes 
of natural patination. Similarly, arti.cial patinas are deliberately induced through various surface 
treatments designed to mimic and exaggerate the natural processes. The corrosion process forms a complicated 
system of reacting layers consisting of the metal, corrosion products, surface elec­trolyte, and the 
atmosphere. The exact nature of the surface chem­istry underlying the development of surface coatings 
and the fac­tors that in.uence how they form and break down are major foci of current corrosion research. 
Unfortunately, the experimental data to support a physical simulation that would predict the appearance 
of a metal is not yet available. Therefore, in this paper we present a phenomenological model for the 
development of patinas, based on observed physical behavior. 2.1 Composition and Formation of Copper 
Patinas In order to study the development of metallic patinas, we chose cop­per as a representative metal. 
The patinas of copper and copper al­loys are classic examples of layered structures. When viewed in cross-section 
(see Figure 3 [8]) multiple layers are distinctly visi­ble to the eye. Vernon and Whitby identi.ed the 
principal chemical constituents of such natural copper patinas in the 1930s [32]. More recently, Franey 
and Davis [8] and Graedel et al. [11] have carried out a series of investigations, which have provided 
a detailed pic­ture of the basic composition and formation mechanisms of natural patinas on copper. Clean 
copper surfaces exposed to the atmosphere quickly form a thin layer of dull brown tarnish that gradually 
changes with time to a reddish brown color, which is indicative of copper oxide, or mineral cuprite [8, 
22]. Once this layer is in place, subsequent lay­ers grow much more slowly. The primary chemical constituents 
of the patina on copper and copper alloys include copper oxides, sul­phides, and inorganic and organic 
copper salts. Copper sulphide is very dark brown, and the sulphide coating forms integrally with the 
underlying metallic crystal structure. Thus, the sulphide layer of­ten appears relatively shiny. The 
copper salts, consisting mostly of sulphates, chlorates, and nitrates, come in a wide variety of colors. 
Copper sulphate causes the characteristic green color of aged cop­per. Figure 2 contains a key showing 
the approximate color of some of the most important copper minerals. 2.2 Dependence on Physical Environment 
Figure 2 is a schematic diagram of the patination process as it can be outlined for three environments 
based on Graedel s observa­tions [10]. This diagram indicates the varying composition of the patina in 
the environments and average lengths of time for develop­ment. The left column represents four primary 
stages of patina growth in a marine environment. The .rst and second stages, which are common to the 
other two environments, involve the formation of a layer of tarnish and a layer of cuprite at the copper 
surface. The third stage is characterized by the formation of several copper minerals, with atacamite 
and other chlorine-containing substances expected to predominate because of the high chloride concentration 
near the sea. Small amounts of organic matter are also present within the struc­ture. The .nal stage 
of the process augments the patina with similar minerals and binding materials. On land, a patina of 
a different composition is produced. Both rural and urban air have moderate to high concentrations of 
sulfur. Thus, the transition from the second to the third stage involves sul­phurization atop the cuprite, 
with some atacamite being formed as well. The initial form of the sulphur compound is primarily the min­eral 
posnjakite; an organic binder is also present in the patina. The .nal stage differs in the two environments. 
In the rural case, where there are only moderate concentrations of acid and oxidized sulfur in the atmosphere, 
the formation of sulfates proceeds slowly, and a mixed patina of atacamite and brochantite is typical. 
In urban areas, there are higher concentrations of sulfur, favoring the forma­tion of the basic sulfate 
minerals antlerite and brochantite. Organic compounds are also incorporated into the patina as it forms. 
In a marine environment, patinas grow in thickness at an aver­age rate of 1tm per year. In a rural environment, 
the rate is less, about :5tm per year; in an urban environment the rate is slightly greater, about 1-2tm. 
These rates are highest during the .rst few years of exposure and approach a lower, almost negligible 
rate after about 20 years [22]. While the preceding section gives a descrip­ Figure 2: A schematic diagram 
of the processes involved in the growth of copper patinas in marine, rural, and urban atmospheres. tion 
of the development of patinas in three different environments, including the constituents of the patina 
and general growth rates, the development can be further in.uenced by a speci.c surrounding en­vironment. 
Wetness is perhaps the most important factor in the pati­nation process. Patination occurs more rapidly 
in areas of the sur­face that retain stagnant water. Thus, horizontal or inclined surfaces patinate more 
rapidly than vertical surfaces [11]. In addition, expo­sure to daylight decreases the rate of patination 
due to evaporation of surface water. There are many other factors that affect the corrosion of metals, 
such as the composition of the electrolyte on the surface and the tem­perature; variations in surface 
thickness due to abrasions, polishing, and pitting are also relevant. However, the in.uence of these 
factors is not well understood [22], and we do not consider them further in this paper.  3 Modeling 
As we observed in the previous section, the development of a metal­lic patina is a process that proceeds 
in a complex system of layers. In this section we describe a representation of the structure of a layered 
 (a) (b) Figure 3: A surface patina as a stack of layers. (a) Micrograph of a copper surface showing 
the layers. (b) Our abstraction of the lay­ered structure. material and a set of operators that can be 
applied to this representa­tion to simulate a variety of effects. By writing a script in terms of these 
operators, the weathering of the material as a function of time may be simulated. 3.1 Layers and Materials 
In our representation, a surface contains a stack of nlayers (see Fig­ure 3). The 0 -th layer is assumed 
to be the base material with an essentially in.nite thickness. The total thickness of the stack of lay­ers 
is assumed to be small relative to the area covered by the layer. Each individual layer also has a thickness, 
although it may be zero at certain points. We have varied only the thickness as a function of position, 
but other properties could be similarly controlled. The remainder of the material properties do not depend 
on po­sition. Each layer consists of a homogeneous material. The sur­face of each material has the standard 
set of surface shading param­eters such as diffuse and specular colors and an overall roughness or shininess. 
In addition each material has two volumetric properties that control how light is transmitted and re.ected 
due to subsurface scattering in the layer. The light re.ection and transmission proper­ties of layers 
and the rendering of the layer structure are discussed in Section 4. 3.2 Operators The development of 
a patina is implemented as a series of opera­tors on layers. By controlling the sequencing of these operators, 
a wide variety of effects may be created. Our current implementation includes the following operators: 
Layers Base . Coat Erode Figure 4: The coat and erode operators. coat material thickness thickness-map 
The coat operator adds a new layer of material to a surface. The new layer has a speci.ed maximum thickness 
that is mod­ulated by the thickness map (see Figure 4). erode thickness thickness-map The erode operator 
removes material from a layered surface. The depth of the erosion may be controlled by a thickness map. 
The erosion operator proceeds through the stack of layers de­creasing the thickness of each layer until 
the desired amount of material has been removed. This provides a means to cut into a material and expose 
underlying layers. In principle, the amount of an erosion may depend on the hardness of the ma­terial 
in each layer, although our current system does not store hardness as a material attribute (see Figure 
4). .ll material height height-map The .ll operator deposits material up to a given absolute height above 
the base material. This is somewhat akin to .lling all the valleys with water until it reaches a given 
height. This is a simple way to simulate the deposition of material in cracks and crevices. polish height 
height-map The polish operator removes material until a given absolute height above the base material 
is reached. The result is a smoothing effect across a surface. A variation of the opera­tor removes material 
until a given area of surface is exposed. This version of the operator was motivated by observing pro­.les 
and statistics of rough surfaces after they have been pol­ished [29]. offset radius The offset operator 
applies a material to a surface by .rst ap­plying a thick coat and then removing the part accessible 
to a sphere of a given radius. The offset surface is computed using techniques described by Miller [23]. 
(We have implemented both versions of his operator: one computes the largest tangent sphere, the other 
forms a positive and then a negative offset surface and compares that to the height of the original surface). 
We have implemented a simple scripting language to describe the sequential application of different operators. 
This can be used sim­ply to assemble a layered surface from a set of materials and texture maps. In this 
case each material and thickness map represents the current characteristics of the layered surface. This 
makes it possi­ble to give the user control over the ordering of various operators and special effects 
such as polishing. Alternatively, the operators can be repeatedly applied to simulate the action of the 
environment on the surface. When used in this way the scripts may be generated by a program that attempts 
to cycle through various operations oc­curring through time. In Section 5 we will show several example 
scripts. 3.3 Texture/Thickness Maps The operators we have described are able to model a multi-layered 
patina, but one that is completely uniform and devoid of the varia­tions and richness of detail that 
any natural process generates. Next, we will explain a few simple, physically plausible methods to mod­ulate 
these operators across the surface, simulating the detail present in the real aging process. In our system 
we have used two represen­tations for spatially-varying thickness maps: 1. Rectangular texture maps. 
In this implementation the thick­ness is controlled by a standard 2D texture map. 2. Triangulations 
as texture maps. This implementation repre­sents thicknesses as values attached to the vertices of a 
trian­gulation.  In order to simulate variations in thickness over time, we have implemented a series 
of fractal surface growth models for use with the layer structure. Many of these approaches are variations 
of models from the book by Barab´asi and Stanley [1]. We chose a set of models that deals with the deposition 
of material and the lateral growth of patches on surfaces. These models are particularly appropriate 
for modeling the types of patterns that emerge in the corrosion process, as corrosion typically begins 
with the deposition of water and particulate matter from the atmosphere onto a surface. Patches develop 
and spread based on the amount of water and other substances on the surface. Our implementation of these 
models includes several growth rates, such as linear, parabolic, and logarithmic. We provide the following 
models: Steady thickening (ST). This model creates a very simple, rela­tively uniform pattern, which 
increases in thickness with time. We sample a surface evenly with a small number of points. An initial 
thickness is assigned to each of these points, and the thickness at intermediate points is interpolated. 
Over time, the thickness at each sample point is increased by a user-speci.ed growth rate. A small amount 
of noise is added to the pattern to keep the appearance natural. (a) (b) Figure 5: (a) Random deposition; 
the bent arrows indicate RD with surface relaxation. (b) Ballistic deposition. Random deposition (RD). 
Random deposition is the simplest growth model that we utilize. From a randomly chosen position over 
the surface, a particle falls vertically until it reaches the top of the surface under it, whereupon 
it is deposited. We implement this model by randomly choosing a position iand increasing its height h(i;t)by 
one, where tdenotes the time step. A variation of this model, random deposition with surface relaxation, 
allows the deposited particle to diffuse along the surface up to a .nite dis­tance, settling when it 
.nds the position with the lowest height (see Figure 5a). Due to the relaxation process, the .nal interface 
 or surface of the layer will be smooth, compared to the model without relaxation, which is extremely 
rough. Ballistic deposition (BD). In the Ballistic deposition model, a particle is also released from 
a randomly chosen position above a surface and follows a straight vertical trajectory un­til it reaches 
the surface, whereupon it sticks (see Figure 5b). The height of the interface at that point iis increased 
to max[h(neighboringpoints;t);h(i;t)+1]. Growth is de­.ned quantitatively through a simple function that 
calculates the mean height of the surface. If the deposition rate (number of par­ticles arriving at a 
position) is constant, the mean height increases linearly with time. In addition, the interface width, 
which describes the roughness of the surface, is de.ned by the rms .uctuation in height. The key difference 
between the RD and BD models is that the RD interface is uncorrelated i.e. the thickness at each point 
on the surface grows independently, since there is no mechanism that can generate correlations along 
the interface. In BD, the fact that particles are capable of sticking to the edge of neighboring points 
leads to lateral growth. Figure 6: A lattice of blocked and unblocked cells (left), early stage of pattern 
formation (middle), late stage of pattern formation (right). Directed percolation depinning (DPD). Starting 
with a collection of initial patches on a surface, we develop an interface that grows in all directions 
in two-dimensions and increases in thickness. We begin with a simple 2D lattice and mark a percentage 
of cells as blocked and others as unblocked. Over time, patches are much more likely to advance onto 
an unblocked cell than a blocked cell (the rules for it are much less stringent). In our version of the 
model, unblocked cells imply concentrations of moisture on the surface and thus stimulate growth. Figure 
6 shows a representative, initial lattice (where black in­dicates blocked and white indicates unblocked) 
and a collection of patches at early and mature stages. In this case, the seed patches are 10 x10 squares. 
The patches grow in the direction of their prin­cipal neighbors (cardinal directions) based on a simple 
probability function and according to whether the given cell is blocked. Note that the pattern develops 
much more readily in the bottom half of the image, re.ecting the lower percentage of blocked cells. Over 
time as the initial patches spread across the surface, the system in­serts additional seed patches at 
unoccupied positions according to a given probability; these patches grow according to the same rules. 
The above process creates the bottom level or overall pattern. The upper levels (accounting for variations 
in thickness) are .lled in as follows: the probability of a cell appearing in level l+1is propor­tional 
to the number of cells in level lthat support it. Variations in thickness are denoted by gray levels; 
white is solid and black is void. Other initial patterns and lattices could be used, and such compo­nents 
could be varied as a function of time. In our current system the development of the layer structure through 
time is controlled in two ways. In addition to scripting dif­ferent operations, speci.c growth models, 
as described above, can be used to generate thickness maps through time. These growth models can be controlled 
by environmental factors and the geometry of the surface. Finally, the layer structure is output as a 
series of ma­terial properties and texture maps, which are passed to the rendering system. This allows 
additional control over the .nal appearance of the surface. The use of development models represents 
something of a black box in that it is possible to use other procedural models or scanned patterns to 
generate the thickness maps. In addition to the ap­proaches described above, we also make use of standard 
procedu­ral texturing approaches, such as noise and turbulence functions [6]. We have found the above 
models to be especially useful. However, as additional information is learned about the development of 
pati­nas, more exact models could be used, without affecting the overall patina modeling framework. 
 4 Rendering The appearance of a layered structure is a result of light interact­ing with the surface 
and subsurface. Surfaces arise at the interfaces between layers. Light is re.ected from the surface depending 
on the surface roughness and the specular color. The surface re.ection gives the layer a glossy appearance. 
Light is also re.ected from and transmitted through the interior depending on the absorption and scattering 
properties of the material. The subsurface re.ection gives the layer a matte or diffuse appearance. The 
fact that light is also transmitted through the stack of layers causes the colors of differ­ent layers 
to be mixed; it also allows underlying materials to remain partially visible. 4.1 Single Layer To model 
re.ection and transmission through a single layer we use the Kubelka-Munk (KM) model [21]. The KM model 
is widely used in the paint, printing, and textile industries to compute diffuse colors due to subsurface 
scattering [17]. The book by Kortum is an excel­lent source of information about the KM model [18]. The 
KM model was recently introduced to computer graphics by Haase and Meyer [12]. In that work the color 
of a thick layer of paint consisting of several pigments was quantitatively modeled. Given the relative 
concentrations of several pigments, they were able to predict the .nal appearance of the paint. The KM 
model was com­pared to the standard additive or subtractive color models used in computer graphics and 
was found to be more accurate. They also used the KM model to estimate the pigment concentrations needed 
to match a given color. In this work we use the KM model to predict the color effects due to variations 
in layer thickness and to predict the color of a stack of layers of different pigments. dz Figure 7: 
Positive and negative .ux density within a layer. The KM model corresponds roughly to 1-dimensional volume 
ra­diosity. Figure 7 shows the transport of light within a layer. The KM model assumes that the light 
distribution is directionally isotropic, but varies as a function of depth. The light distribution therefore 
is described by the .ux density (energy per unit area), or volume radiosity, in the inward or positive, 
and outward or negative direc­tions, denoted by B+and B_. As light propagates through the volume it may 
be absorbed or scattered, according to the following coupled differential equations @B+ -(K+S)B++SB_ 
@z @B_ SB+-(K+S)B_ @z where Kis the absorption per unit length, and Sis the backscatter­ing per unit 
length. Backscattering decreases the energy .ow in one direction, thereby causing an increase in the 
opposite direction. These equations have an analytical solution for a homogeneous layer of thickness 
d[19]. Given these solutions it is possible to com­pute the re.ectance Rand transmittance Tthrough a 
layer: B_(0)sinhbSd R B+(0)asinhbSd+bcoshbSd B+(d) b T B+(0)asinhbSd+bcoshbSd p 2 Here a(S+K)ISand b 
a-1. Note that in general Kand Sare functions of wavelength, hence the calculations involved in the KM 
model must be performed separately for each color sample. The major problem in using the KM theory is 
the determination of the layer parameters Kand S. One method is based on the for­mula for the re.ectance 
of an in.nitely thick layer: r KK2K R11+++2: SSS This formula can be inverted and the ratio KIScomputed 
from R1. Thus, measuring the re.ectance of an optically thick sample allows one to compute KIS. Another 
common method for determining the KM parameters is to measure the re.ectance of a thick layer and then 
the re.ectance of a layer of known thickness over a background material of known re.ectance. A good discussion 
of different meth­ods for determining the KM parameters in the context of computer graphics is contained 
in Haase and Meyer [12]. In our work we have estimated the KM parameters by match­ing synthesized color 
samples to physical copper and mineral spec­imens, as well as photographs, such as those found in Hughes 
and Rowe [16]. We generally match R1to the sample, determine KIS from the above formula, and assume a 
value for S. This approach has enough accuracy to capture the range of colors and color effects that 
we are attempting to model. A more careful study involving de­tailed comparisons with physical samples 
under controlled weath­ering conditions would require more accurate radiometric and col­orimetric measurements. 
However, this is beyond the scope of this study. 4.2 Multiple Layers Kubelka also extended his model 
to account for the re.ectance and transmittance, including all scattering events, due to a stack of lay­ers 
[20]. The re.ectance and transmittance of two layers may be merged to yield an equivalent re.ectance 
and transmittance: T2 1R2 RR1+ 1-R1R2 T1T2 T 1-R1R2 This compositing process can be repeated, sequentially 
combining two layers into a single layer, to account for the re.ection and trans­mission through a stack 
of nlayers. We normally do this front to back, although it can be done in any order. Thus it is essentially 
no more dif.cult to compute the diffuse re.ectance and transmittance, including all possible transport 
paths, for a stack of nlayers than for a single layer. These formulas may also be used to combine a stack 
of layers with an opaque base layer. (Note that most formulations of the KM model explicitly include 
composition with a background layer Rg.) We call this process subsurface compositing to differentiate 
it from the normal compositing operators widely used in computer graphics [27]. Subsurface compositing 
differs from normal com­positing in that visible light is always assumed to be transmitted through each 
layer twice, which gives rise to a T2factor in front of each re.ectance. This lowers the visibility of 
underlying lay­ers. Subsurface compositing also accounts for the effect of multiple scattering which 
leads to a 1I(1-R1R2)factor which increases the contribution from each layer. These are important effects 
when viewing a stack of adjoining layers; it differs from the typical case in computer graphics where 
a set of disjoint partially transparent re­.ecting surfaces are being viewed. In this case it is incorrect 
to only consider one-dimensional interre.ection, and the light source is not necessarily emanating from 
the eye. 4.3 BRDF The above techniques are used during rendering to compute an ap­proximation to the 
BRDF of each point on the layered surface. We assume the .nal BRDF consists of a diffuse term and a mixture 
of glossy terms. The diffuse re.ectance of a stack of layers is com­puted exactly as described above, 
and needs no further explanation. Accounting correctly for glossy re.ection is more complicated, so we 
make several assumptions to simplify rendering. Glossiness is caused by light re.ection at the interface 
between layers. Tra­ditionally in computer graphics there is only a single interface be­tween air and 
the material the surface itself and this inter­face causes the glossy appearance; in a layered structure 
there are multiple interfaces. In our system we perform the following two steps. First, glossy re.ection 
at each interface is modeled using the standard computer graphics model for shiny surfaces; that is, 
with a specular color and a microfacet distribution function parameter­ ized by the surface roughness, 
Cs(N.H)1.r. Each interface inher­its properties from the material below it, hence, each interface may 
have a different roughness and color. Second, to account for atten­uation due to absorption and scattering 
in the layers above the inter­face, the specular color is multiplied twice by diffuse transmittance of 
the intervening layers calculated using the KM equations. The output glossy BRDF consists of a set of 
surface roughnesses and at­tenuated specular colors. The renderer needs to be modi.ed to sum over this 
set of microfacet distributions when computing the .nal glossy re.ection component. If the .nal specular 
color of one of the glossy terms is 0, that usually implies that it has been covered by an opaque material. 
This method of computing glossy re.ection is an approximation for several reasons. First, it is not strictly 
correct to use the diffuse transmittance to attenuate the incoming and outgoing radiance be­cause even 
with a homogeneous layer the transmittance will be a function of direction. Second, the above model does 
not account for scattering events in the intervening layers before or after the glossy re.ection. In 
general, accounting for this would require the full so­lution of the one-dimensional transport equation, 
as was done in Hanrahan and Krueger [13]. The precise nature of the errors intro­duced by these approximations 
needs further study. But our approx­imations capture quite well all the major visual effects that we 
hoped to achieve. Finally, we output the .nal thickness of the stack of layers by summing the thicknesses 
of each layer. This .nal thickness may be used to either perturb the normals or displace the surface 
during .nal rendering.  5 Results To demonstrate the modeling and rendering approaches described in 
the paper, we show results from several complex simulations. 5.1 Copper Strips Figure 8 depicts the 
weathering of copper strips exposed to marine, rural, and urban environments respectively over the course 
of a thirty year period (in six year increments). This example shows the possibilities for the development 
of natural patinas in distinct environments. In all cases, the basic development of the patina was derived 
from the background material presented in Section 2. The patterns used in the layer maps were created 
though the fractal surface growth models described in Section 3.3; in the scripts below, the models are 
identi.ed with the algorithm name, growth rate, step number, and total number of steps. Names such as 
marine patina 3signify mixtures of the substances outlined in Figure 2 and are treated as materials with 
the various operators; the integer indicates the speci.c stage in this chart. The development of the 
patina was simulated using the layer structure and combinations of operators. The leftmost strips represent 
the appearance of pure copper the starting point for the simulations. Marine environment. The top row 
is a simulation of changes due to a typical marine environment. Here, we used the BD and DPD models to 
vary the thickness of the layers as a function of time. The BD model yields spotty patterns that are 
characteristic of marine environments, which often leave uneven coatings of moisture and salts on surfaces. 
The DPD model provides a patchy but more uniform coating of patina in the latter stages. In the last 
stage, we erode away a small portion of the top layer. The script for the last strip in the series is 
the following: new copper; coat tarnish 1 0.35 texture(BD linear 1 20); coat cuprite 2 1.2 texture(DPD 
linear 5 40); coat marine patina 3 3.0 texture(BD linear 10 20) coat marine patina 4 1.8 texture(DPD 
linear 20 40); erode 0.5 texture(BD linear 5 20); render maps; Rural environment. The middle row is 
a simulation of ef­fects common in a typical rural environment. Note that it takes longer for the green 
patina to begin to appear here. Patinas in rural environments tend to be quite spatially homogeneous. 
To capture this quality, we used the ST pattern generator to vary the thickness of the layers in the 
sequence. The script for the last strip in this series is the following: new copper; coat tarnish 1 0.25 
texture(ST cubic 1 20); coat cuprite 2 1.0 texture(ST cubic 5 20); coat rural patina 3 1.0 texture(ST 
cubic 10 20); coat rural patina 4 1.1 texture(ST cubic 15 20); render maps;  Figure 8: Copper time 
lines: marine environment (top); rural environment (middle); urban environment (bottom). Urban environment. 
The bottom row is representative of changes due to an urban environment. Here, the .lms are very thick. 
Note that in this case, the green copper salts begin to appear much earlier in the sequence. Half way 
into the sequence, the surface is covered with a thick, almost continuous, coating. In this sequence, 
we used the DPD model to generate the variations in thickness. This model gives rise to fairly continuous 
patches that grow laterally on the surface. It is common for urban patinas to be very soluble and to 
develop coatings of soot/dirt. To capture this quality, we erode away some of the patina and add a layer 
of dirt on the surface using the RD model on the last strip in the sequence. Here is the script for this 
strip: new copper; coat tarnish 1 0.55 texture(RD linear 1 40); coat cuprite urban 2 1.0 texture(RD cubic 
10 40); coat patina urban 3 4.0 texture(DPD cubic 5 40); coat patina urban 4 3.0 texture(DPD cubic 10 
40); erode 0.8 texture(DPD linear 30 40); coat dirt 1.9 texture(BD linear 5 20); render maps; Figure 
9 illustrates the layers and the .nal BRDF parameters for the urban series. Note that the layer structure 
records the evolution of the strip from bottom to top. The initial development (bottom row) features 
output from the RD algorithm without surface relaxation. The remaining rows show patterns created with 
the DPD model. In the top row, the dirt pattern created with the BD model is visible. It is interesting 
to observe that each layer has constant surface properties; all the variations in color and texture arise 
from varying the thickness of the different layers.  5.2 Buddha Figure 11 shows the development of the 
patina on a small statue of a buddha. The buddha model was created from a Cyberware scan and consists 
of approximately 60,000 small evenly sized tri­angles. In these pictures the table and wall are rendered 
using con­ventional texturing techniques, and the buddha is rendered using the techniques described in 
this paper. In this experiment the various stages of the development of the patina are simulated in the 
RenderMan shading language. A shader  30 years erosion, dirt final surface 24 years oxides, sulphates 
18 years oxides, sulphates chlorides 6 years tarnish, cuprite Figure 9: Layer properties and the .nal 
BRDF parameters for the urban series. was written that modeled a three-layered surface: base copper, 
a tar­nished layer, and a green patina. Parameters related to thickness are shown as a function of time 
in Figure 10. The thickness of different layers also depends on position and other factors as follows: 
 The thickness of the tarnish layer was computed using two functions. The .rst, labeled tarnish in Figure 
10, models tar­nishing due to atmospheric processes. This parameter does not vary spatially. The second 
parameter, labeled polish, shows the decrease in tarnish thickness due to polishing the buddha. This 
parameter does vary spatially as determined by the acces­sibility map (the accessibility map is computed 
as a prepro­cess, and therefore there is a single accessibility value per tri­angle) [23]. Thus, tarnish 
appears in cracks and crevices, and shiny copper appears in exposed areas.  The green patina consists 
of a steadily thickening layer and a set of steadily growing random patches. The steadily thick­ening 
layer depends on the local wetness of the surface; the wetter the surface the thicker the patina. The 
wetness is con­trolled by a precomputed exposure map, that gives the aver­age irradiance due to the sun 
and sky received by that part of  Thickness 4 3 2 1 0 base body tarnish polish bump  0 1 2 3 4 5 6 
7 8 910 Time Figure 10: Development of patina on the buddha statue. the surface, and a simple function 
that decreases the average wetness of inclined surfaces. Another simple procedure in­creases the wetness 
along the base of the statue where water is likely to accumulate; this effect changes through time un­der 
control of the base parameter. The patches of the patina are controlled by a random fractal surface growth 
process. To show the .exibility of the system, the surface growth pattern is computed using the standard 
noise function in the shading language. The thickness of these various components of the patina change 
through time as shown in Figure 10. The curve labeled body controls the thickness of the steadily thickening 
layer, the curve labeled base controls the thickness along the bottom, and the curve labeled bump controls 
the thickness of the random growth process. 5.3 Towers Figure 12 shows two copper-covered towers of 
different ages in an urban environment. The tower on the left displays a dark cuprite coating due to 
a few years of exposure; the tower to the right has a green patina typical of several decades of exposure. 
For this simulation, we modulated the appearance of the patina based on several functions. As in the 
previous example, the wetness is controlled by a precomputed exposure map and a simple function that 
decreases the average wetness on inclined surfaces. For exam­ple, inclined areas that face south have 
thinner layers of patina, re­.ecting the fact that they would be dryer. We also used an accessi­bility 
map to vary the thickness of the patinas across the surfaces. The new tower to the left is covered with 
a thin layer of cuprite. In addition, some simple texture maps were created with a noise func­tion and 
combined with the layer structure to simulate the staining due to the .ow of water over the top of the 
tower. Modeling such washing and staining effects is the subject of a separate work [5]. In the tower 
on the right, there is a thin layer of cuprite on the base copper. This is augmented with a fairly uniform 
layer of patina. We also added a thin layer of dirt, which is common to urban patinas. All layers are 
varied by simple noise functions.  6 Summary and Discussion We have presented an approach for the modeling 
and rendering of one type of weathering metallic patinas. A surface is represented as a set of layers. 
This representation resembles the underlying structure of the physical model of a patina. A set of operators 
was introduced that can simulate a wide variety of weathering effects. We also presented an approach 
to modeling the re.ection and trans­mission through the layer structure using the Kubelka-Munk model. 
The modeling and rendering approach is capable of simulating a va­riety of metallic patinas. Although 
we attempted to model the development of patinas on surfaces based on the available physical evidence, 
our model is still a phenomenological one. We believe that an exact model is not pos­sible at this time, 
as the more general problem of the atmospheric corrosion of metals is not yet fully understood. However, 
the new layered model would appear to have great usefulness in computer graphics due to the ease with 
which it is possible to give the designer  Figure 12: Two copper-covered towers. (Left) After a few 
years of exposure. (Right) After several decades of exposure. control over a wide variety of processes 
important in determining the .nal appearance of an object. This representation of a material is signi.cant, 
as it can accom­modate the time-varying nature of a surface and its exposure to a complex class of weathering 
effects. We believe that this general model will be applicable to other materials as well. The problem 
of creating physically-based models of real materials, which can in­corporate variations over time, is 
an important topic for computer graphics. This is a dif.cult problem, as the factors that determine the 
changes in appearance of materials operate simultaneously and are not completely understood. These dif.culties 
notwithstanding, many interesting research directions remain. One area is in the de­sign of models for 
other materials, such as wood or plastics. An­other topic is the development of more complex operators, 
which can simulate dynamic processes, such as erosion. Last, we would like to add interactive operators 
to the system. These would be es­pecially useful for creating patterns due to various brushes used in 
arti.cial patination techniques.  Acknowledgements We would like to thank Jeff Feldgoise for modeling 
the tower scene and Matt Pharr for rendering it, Brian Curless for scanning the buddha model, Hans Pedersen 
for providing a geometry transla­tor, Steve Westin for reviewing an early draft of the paper, Craig Kolb 
for video assistance, Tom Graedel for the copper micrograph, and the anonymous reviewers for helpful 
suggestions. This re­search was supported by research grants from the National Science Foundation (CCR-9207966 
and CCR-9624172) and the MIT Cabot and NEC Funds, and by equipment grants from Apple and Silicon Graphics 
Inc. References [1] BARAB ´ ASI,A. L., AND STANLEY,H. E. Fractal Concepts in Surface Growth. Cambridge 
University Press, Cambridge, 1995. [2] BECKET,W., AND BADLER, N. I. Imperfection for realistic image 
synthesis. Journal of Visualization and Computer Animation 1, 1 (Aug. 1990), 26 32. [3] BLINN, J. F. 
Light re.ection functions for simulation of clouds and dusty sur­faces. Computer Graphics 16, 3 (July 
1982), 21 29. [4] COOK, R. L. Shade trees. Computer Graphics 18, 3 (July 1984), 223 231. [5] DORSEY,J., 
PEDERSEN,H. K., AND HANRAHAN, P. Flow and changes in ap­pearance. In Computer Graphics Proceedings (1996), 
Annual Conference Series, ACM SIGGRAPH. [6] EBERT,D. S., Ed. Texturing and Modeling. Academic Press, 
New York, 1994. [7] FLEMING,S. J. Dating in Archaeology. St. Martin s Press, New York, 1977. [8] FRANEY,J. 
P., AND DAVIS, M. E. Metallographic studies of the copper patina formed in the atmosphere. Corrosion 
Science 27, 7 (1987), 659 688. [9] FRENCH, L. Toy story. Cinefantastique 27, 2 (1995), 36 37. [10] GRAEDEL, 
T. E. Copper patinas formed in the atmosphere a qualitative assess­ment of mechanisms. Corrosion Science 
27, 7 (1987), 721 740. [11] GRAEDEL,T. E., NASSAU,K., AND FRANEY, J. P. Copper patinas formed in the 
atmosphere. Corrosion Science 27, 7 (1987), 639 652. [12] HAASE,C. S., AND MEYER, G. W. Modeling pigmented 
materials for realistic image synthesis. ACM Tran. Graphics 11, 4 (Oct. 1992), 305 335. [13] HANRAHAN,P., 
AND KRUEGER, W. Re.ection from layered surfaces due to subsurface scattering. In Computer Graphics Proceedings 
(1993), Annual Con­ference Series, ACM SIGGRAPH, pp. 165 174. [14] HANRAHAN,P., AND LAWSON, J. A language 
for shading and lighting calcu­lations. Computer Graphics 24, 4 (Aug. 1990), 289 298. [15] HSU,S., AND 
WONG, T. Simulating dust accumulation. IEEE Computer Graph­ics and Applications 15, 1 (Jan. 1995), 18 
22. [16] HUGHES,R., AND ROWE,M. The Colouring, Bronzing and Patination of Met­als. Watson-Guptill Publications, 
New York, 1991. [17] JUDD,D. B., AND WYSZECKI,G. Color in Business, Science, and Industry. John Wiley 
&#38; Sons, New York, 1975. [18] KORTUM,G. Re.ectance Spectroscopy. Springer-Verlag, New York, 1969. 
[19] KUBELKA, P. New contributions to the optics of intensely light-scattering mate­rial, part I. J. 
Opt. Soc. Am. 38 (1948), 448. [20] KUBELKA, P. New contributions to the optics of intensely light-scattering 
mate­rial, part II: Non-homogeneous layers. J. Opt. Soc. Am. 44 (1954), 330. [21] KUBELKA,P., AND MUNK, 
F. Ein beitrag zur optik der farbanstriche. Z. tech. Physik. 12 (1931), 593. [22] MATTSSON,E. Basic Corrosion 
Technology for Scientists and Engineers. Ellis Horwood Limited, New York, 1989. [23] MILLER, G. Ef.cient 
algorithms for local and global accessibility shading. In Computer Graphics Proceedings (1994), Annual 
Conference Series, ACM SIG-GRAPH, pp. 319 326. [24] MOSTAFAVI,M., AND LEATHERBARROW,D. On Weathering: 
The Life of Buildings in Time. MIT Press, Cambridge, MA, 1993. [25] NEWMAN,R. C., AND SIERADZKI, K. Metallic 
corrosion. Science 263 (1994), 1708 1709. [26] PERLIN, K. An image synthesizer. Computer Graphics 19, 
4 (July 1985), 287 296. [27] PORTER,T., AND DUFF, T. Compositing digital images. Computer Graphics 18, 
3 (July 1984), 253 259. [28] SIMPSON,J. W., AND HORROBIN,P. J. The Weathering and Performance of Building 
Materials. MTP Publishing Co, London, 1970. [29] THOMAS,T. R., Ed. Rough Surfaces. Longman, New York, 
1982. [30] TURK, G. Generating textures for arbitrary surfaces using reaction-diffusion. Computer Graphics 
25, 4 (July 1991), 289 298. [31] UPSTILL,S. The Renderman Companion. Addison-Wesley, New York, 1990. 
[32] VERNON,W. H. J., AND WHITBY, L. The open air corrosion of copper, a chem­ical surface patina. Journal 
Instit. of Metals 42, 6 (1932), 181 195. [33] WITKIN,A., AND KASS, M. Reaction-diffusion textures. Computer 
Graphics 25, 4 (July 1991), 299 308.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237279</article_id>
		<sort_key>397</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Visual models of plants interacting with their environment]]></title>
		<page_from>397</page_from>
		<page_to>410</page_to>
		<doi_number>10.1145/237170.237279</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237279</url>
		<keywords>
			<kw><![CDATA[L-system]]></kw>
			<kw><![CDATA[clonal plant]]></kw>
			<kw><![CDATA[ecosystem]]></kw>
			<kw><![CDATA[modeling]]></kw>
			<kw><![CDATA[plant development]]></kw>
			<kw><![CDATA[realistic image synthesis]]></kw>
			<kw><![CDATA[root]]></kw>
			<kw><![CDATA[scientific visualization]]></kw>
			<kw><![CDATA[simulation]]></kw>
			<kw><![CDATA[software design]]></kw>
			<kw><![CDATA[tree]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.4.2</cat_node>
				<descriptor>Parallel rewriting systems (e.g., developmental systems, L-systems)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003766.10003767.10003769</concept_id>
				<concept_desc>CCS->Theory of computation->Formal languages and automata theory->Formalisms->Rewrite systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39075806</person_id>
				<author_profile_id><![CDATA[81100163820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Radom&#237;r]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#283;ch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Calgary, Calgary, Alberta, Canada T2N 1N4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14162862</person_id>
				<author_profile_id><![CDATA[81100465812]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Przemyslaw]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Prusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Calgary, Calgary, Alberta, Canada T2N 1N4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>211667</ref_obj_id>
				<ref_obj_pid>211662</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AGRAWAL, P. The cell programming language. Artificial Life 2,1 (1995), 37-77.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ARVO,J.,AND KIRK, D. Modeling plants with environment-sensitive automata. In Proceedings of Ausgraph'88 (1988), pp. 27 - 33.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BELL,A.Plant form: An illustrated guide to flowering plants. Oxford University Press, Oxford, 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BELL, A. D. The simulation of branching patterns in modular organisms. Philos. Trans. Royal Society London, Ser. B 313 (1986), 143-169.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BELL,A.D.,ROBERTS,D.,AND SMITH, A. Branching patterns: the simulation of plant architecture. Journal of Theoretical Biology 81 (1979), 351-375.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325249</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BLOOMENTHAL, J. Modeling the Mighty Maple. Proceedings of SIG- GRAPH '85 (San Francisco, California, July 22-26, 1985), in Computer Graphics, 19, 3 (July 1985), pages 305-311, ACMSIGGRAPH, New York, 1985.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BORCHERT,R.,AND HONDA, H. Control of development in the bifurcating branch system of Tabebuia rosea: A computer simulation. Botanical Gazette 145, 2 (1984), 184-195.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[BORCHERT,R.,AND SLADE, N. Bifurcation ratios and the adaptive geometry of trees. Botanical Gazette 142, 3 (1981), 394-401.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[CHEN,S.G.,CEULEMANS,R.,AND IMPENS, I. A fractal based Populus canopy structure model for the calculation of light interception. Fores t Ecology and Management (1993).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[CHIBA,N.,OHKAWA,S.,MURAOKA,K.,AND MIURA, M. Visual simulation of botanical trees based on virtual heliotropism and dormancy break. The Journal of Visualization and Computer Animation 5 (1994), 3-15.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[CHIBA,N.,OHSHIDA,K.,MURAOKA,K.,MIURA,M.,AND SAITO,N.A growth model having the abilities of growth-regulations for simulating visual nature of botanical trees. Computers and Graphics 18, 4 (1994), 469-479.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[CLAUSNITZER,V.,AND HOPMANS, J. Simultaneous modeling of transient three-dimensional root growth and soil water flow. Plant and Soil 164 (1994), 299-314.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[COHEN, D. Computer simulation of biological pattern generation processes. Nature 216 (October 1967), 246-248.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>154731</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[COHEN,M.,AND WALLACE,J.Radiosity and realistic image synthesis. Academic Press Professional, Boston, 1993. With a chapter by P. Hanrahan and a foreword by D. Greenberg.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[DE REFFYE,P.,HOULLIER,F.,BLAISE,F.,BARTHELEMY,D.,DAUZAT, J., AND AUCLAIR, D. A model simulating above- and below-ground tree architecture with agroforestry applications. Agroforestry Systems 30 (1995), 175-197.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[DIGGLE, A. J. ROOTMAP - a model in three-dimensional coordinates of the structure and growth of fibrous root systems. Plant and Soil 105 (1988), 169-178.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[DONG,M.Foraging through morphological response in clonal herbs. PhD thesis, Univeristy of Utrecht, October 1994.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[FISHER, J. B. How predictive are computer simulations of tree architecture. International Journal of Plant Sciences 153 (Suppl.) (1992), 137-146.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[FISHER,J.B.,AND HONDA, H. Computer simulation of branching pattern and geometry in Terminalia (Combretaceae), a tropical tree. Botanical Gazette 138, 4 (1977), 377-384.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[FLEISCHER,K.W.,AND BARR, A. H. A simulation testbed for the study of multicellular development: The multiple mechanisms of morphogenesis. In Artificial Life III, C. G. Langton, Ed. Addison-Wesley, Redwood City, 1994, pp. 389-416.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[FORD,E.D.,AVERY,A.,AND FORD, R. Simulation of branch growth in the Pinaceae: Interactions of morphology, phenology, foliage productivity, and the requirement for structural support, on the export of carbon. Journal of Theoretical Biology 146 (1990), 15-36.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[FORD, H. Investigating the ecological and evolutionary significance of plant growth form using stochastic simulation. Annals of Botany 59 (1987), 487-494.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>697933</ref_obj_id>
				<ref_obj_pid>646593</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[FRIJTERS,D.,AND LINDENMAYER, A. A model for the growth and flowering of Aster novae-angliae on the basis of table (1,0)L-systems. In L Systems, G. Rozenberg and A. Salomaa, Eds., Lecture Notes in Computer Science 15. Springer-Verlag, Berlin, 1974, pp. 24-52.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[GARDNER, W. R. Dynamic aspects of water availability to plants. Soil Science 89, 2 (1960), 63-73.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74351</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[GREENE, N. Voxel space automata: Modeling with stochastic growth processes in voxel space. Proceedings of SIGGRAPH '89 (Boston, Mass., July 31-August 4, 1989), in Computer Graphics 23, 4 (August 1989), pages 175-184, ACM SIGGRAPH, New York, 1989.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[GREENE, N. Detailing tree skeletons with voxel automata. SIG- GRAPH '91 Course Notes on Photorealistic Volume Modeling and Rendering Techniques, 1991.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[GUZY, M. R. A morphological-mechanistic plant model formalized in an object-oriented parametric L-system. Manuscript, USDA-ARS Salinity Laboratory, Riverside, 1995.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[HANAN, J. Virtual plants -Integrating architectural and physiological plant models. In Proceedings of ModSim 95 (Perth, 1995), P. Binning, H. Bridgman, and B. Williams, Eds., vol. 1, The Modelling and Simulation Society of Australia, pp. 44-50.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>920650</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[HANAN,J.S.Parametric L-systems and their application to the modelling and visualization of plants. PhD thesis, University of Regina, June 1992.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[HART,J.W. Plant tropisms and other growth movements.Unwin Hyman, London, 1990.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578742</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[HERMAN,G.T.,AND ROZENBERG,G. Developmental systems and languages. North-Holland, Amsterdam, 1975.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[HONDA, H. Description of the form of trees by the parameters of the tree-like body: Effects of the branching angle and the branch length on the shape of the tree-like body. Journal of Theoretical Biology 31 (1971), 331-338.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[HONDA,H.,TOMLINSON,P.B.,AND FISHER, J. B. Computer simulation of branch interaction and regulation by unequal flow rates in botanical trees. American Journal of Botany 68 (1981), 569-585.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>175192</ref_obj_id>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[KAANDORP,J. Fractal modelling: Growth and form in biology. Springer-Verlag, Berlin, 1994.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[KANAMARU,N.,CHIBA,N.,TAKAHASHI,K.,AND SAITO,N.CG simulation of natural shapes of botanical trees based on heliotropism. The Transactions of the Institute of Electronics, Information, and Communication Engineers J75-D-II, 1 (1992), 76-85. In Japanese.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[KURTH,W. Growth grammar interpreter GROGRA 2.4: A software tool for the 3-dimensional interpretation of stochastic, sensitive growth grammars in the context of plant modeling. Introduction and reference manual. Forschungszentrum Wald~kosysteme der Universit~t G~ttingen, G~ttingen, 1994.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[LIDDELL,C.M.,AND HANSEN, D. Visualizing complex biological interactions in the soil ecosystem. The Journal of Visualization and Computer Animation 4 (1993), 3-12.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[LINDENMAYER, A. Mathematical models for cellular interaction in development, Parts I and II. Journal of Theoretical Biology 18 (1968), 280-315.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[LINDENMAYER, A. Developmental systems without cellular interaction, their languages and grammars. Journal of Theoretical Biology 30 (1971), 455-484.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[MACDONALD,N.Trees and networks in biological models. J. Wiley &amp; Sons, New York, 1983.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[PRESS,W.H.,TEUKOLSKY,S.A.,VETTERLING,W.T.,AND FLANNERY, B. P. Numerical recipes in C: The art of scientific computing. Second edition. Cambridge University Press, Cambridge, 1992.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1668019</ref_obj_id>
				<ref_obj_pid>1668014</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[PRUSINKIEWICZ, P. Visual models of morphogenesis. Artificial Life 1, 1/2 (1994), 61-74.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
			<ref>
				<ref_obj_id>267880</ref_obj_id>
				<ref_obj_pid>267871</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[PRUSINKIEWICZ,P.,HAMMEL,M.,HANAN,J.,AND M. ECH,R. Visual models of plant development. In Handbook of formal languages, G. Rozenberg and A. Salomaa, Eds. Springer-Verlag, Berlin, 1996. To appear.]]></ref_text>
				<ref_id>43</ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[PRUSINKIEWICZ,P.,AND HANAN, J. L-systems: From formalism to programming languages. In Lindenmayer systems: Impacts on theoretical computer science, computer graphics, and developmental biology, G. Rozenberg and A. Salomaa, Eds. Springer-Verlag, Berlin, 1992, pp. 193-211.]]></ref_text>
				<ref_id>44</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192254</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[PRUSINKIEWICZ,P.,JAMES,M.,AND M. ECH, R. Synthetic topiary. Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), pages 351-358, ACM SIGGRAPH, New York, 1994.]]></ref_text>
				<ref_id>45</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83596</ref_obj_id>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[PRUSINKIEWICZ,P.,AND LINDENMAYER,A.The algorithmic beauty of plants. Springer-Verlag, New York, 1990. With J. S. Hanan, F. D. Fracchia, D. R. Fowler, M. J. M. de Boer, and L. Mercer.]]></ref_text>
				<ref_id>46</ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[ROOM, P. M. 'Falling apart' as a lifestyle: the rhizome architecture and population growth of Salvinia molesta. Journal of Ecology 71 (1983), 349-365.]]></ref_text>
				<ref_id>47</ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[ROOM,P.M.,MAILLETTE,L.,AND HANAN, J. Module and metamer dynamics and virtual plants. Advances in Ecological Research 25 (1994), 105-157.]]></ref_text>
				<ref_id>48</ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[ROZENBERG, G. T0L systems and languages. Information and Control 23 (1973), 357-381.]]></ref_text>
				<ref_id>49</ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[SACHS,T.,AND NOVOPLANSKY, A. Tree from: Architectural models do not suffice. Israel Journal of Plant Sciences 43 (1995), 203-212.]]></ref_text>
				<ref_id>50</ref_id>
			</ref>
			<ref>
				<ref_obj_id>211664</ref_obj_id>
				<ref_obj_pid>211662</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[SIPPER, M. Studying artificial life using a simple, general cellular model. Artificial Life 2, 1 (1995), 1-35.]]></ref_text>
				<ref_id>51</ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[TAKENAKA, A. A simulation model of tree architecture development based on growth response to local light environment. Journal of Plant Research 107 (1994), 321-330.]]></ref_text>
				<ref_id>52</ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[ULAM, S. On some mathematical properties connected with patterns of growth of figures. In Proceedings of Symposia on Applied Mathematics (1962), vol. 14, American Mathematical Society, pp. 215-224.]]></ref_text>
				<ref_id>53</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218427</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[WEBER,J.,AND PENN, J. Creation and rendering of realistic trees. Proceedings of SIGGRAPH '95 (Los Angeles, California, August 6- 11, 1995), pages 119-128, ACM SIGGRAPH, New York, 1995.]]></ref_text>
				<ref_id>54</ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[WYVILL,G.,MCPHEETERS,C.,AND WYVILL, B. Data structure for soft objects. The Visual Computer 2, 4 (February 1986), 227-234.]]></ref_text>
				<ref_id>55</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visual Models of Plants Interacting with Their Environment Radom´ir M.ech and Przemyslaw Prusinkiewicz1 
 University of Calgary ABSTRACT Interaction with the environment is a key factor affecting the devel­opment 
of plants and plant ecosystems. In this paper we introduce a modeling framework that makes it possible 
to simulate and visualize a wide range of interactions at the level of plant architecture. This framework 
extends the formalism of Lindenmayer systems with constructs needed to model bi-directional information 
exchange be­tween plants and their environment. We illustrate the proposed framework with models and 
simulations that capture the develop­ment of tree branches limited by collisions, the colonizing growth 
of clonal plants competing for space in favorable areas, the interaction between roots competing for 
water in the soil, and the competition within and between trees for access to light. Computer animation 
and visualization techniques make it possible to better understand the modeled processes and lead to 
realistic images of plants within their environmental context. CR categories: F.4.2 [Mathematical Logic 
and Formal Lan­guages]: Grammars and Other Rewriting Systems: Parallel rewrit­ing systems, I.3.7 [Computer 
Graphics]: Three-Dimensional Graphics and Realism, I.6.3 [Simulation and Modeling]: Appli­cations, J.3 
[Life and Medical Sciences]: Biology. Keywords: scienti.c visualization, realistic image synthesis, soft­ware 
design, L-system, modeling, simulation, ecosystem, plant de­velopment, clonal plant, root, tree.  INTRODUCTION 
Computer modeling and visualization of plant development can be traced back to 1962, when Ulam applied 
cellular automata to sim­ulate the development of branching patterns, thought of as an ab­stract representation 
of plants [53]. Subsequently, Cohen presented a more realistic model operating in continuous space [13], 
Linden­ 1Department of Computer Science, University of Calgary, Cal­gary, Alberta, Canada T2N 1N4 (mechjpwp@cpsc.ucalgary.ca) 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 mayer proposed the formalism of L-systems as a general framework 
for plant modeling [38, 39], and Honda introduced the .rst computer model of tree structures [32]. From 
these origins, plant modeling emerged as a vibrant area of interdisciplinary research, attracting the 
efforts of biologists, applied plant scientists, mathematicians, and computer scientists. Computer graphics, 
in particular, contributed a wide range of models and methods for synthesizing images of plants. See 
[18, 48, 54] for recent reviews of the main results. One aspect of plant structure and behavior neglected 
by most models is the interaction between plants and their environment (including other plants). Indeed, 
the incorporation of interactions has been identi.ed as one of the main outstanding problems in the domain 
of plant modeling [48] (see also [15, 18, 50]). Its solution is needed to construct predictive models 
suitable for applications ranging from computer-assisted landscape and garden design to the determination 
of crop and lumber yields in agriculture and forestry. Using the information .ow between a plant and 
its environment as the classi.cation key, we can distinguish three forms of interaction and the associated 
models of plant-environment systems devised to date: 1. The plant is affected by global properties of 
the environment, such as day length controlling the initiation of .owering [23] and daily minimum and 
maximum temperatures modulating the growth rate [28]. 2. The plant is affected by local properties of 
the environment, such as the presence of obstacles controlling the spread of grass [2] and directing 
the growth of tree roots [26], geometry of support for climbing plants [2, 25], soil resistance and temperature 
in various soil layers [16], and prede.ned geometry of surfaces to which plant branches are pruned [45]. 
 3. The plant interacts with the environment in an information feed­back loop, where the environment 
affects the plant and the plant reciprocally affects the environment. This type of interaction is related 
to sighted [4] or exogenous [42] mechanisms controlling plant development, in which parts of a plant 
in.uence the devel­opment of other parts of the same or a different plant through the space in which 
they grow. Speci.c models capture:  -competition for space (including collision detection and ac­cess 
to light) between segments of essentially two-dimensional schematic branching structures [4, 13, 21, 
22, 33, 34, 36]; -competition between root tips for nutrients and water trans­ported in soil [12, 37] 
(this mechanism is related to competition between growing branches of corals and sponges for nutrients 
 diffusing in water [34]); -competition for light between three-dimensional shoots of herbaceous plants 
[25] and branches of trees [9, 10, 11, 15, 33, 35, 52]. Models of exogenous phenomena require a comprehensive 
repre­sentation of both the developing plant and the environment. Con­sequently, they are the most dif.cult 
to formulate, implement, and document. Programs addressed to the biological audience are often limited 
to narrow groups of plants (for example, poplars [9] or trees in the pine family [21]), and present the 
results in a rudimentary graphical form. On the other hand, models addressed to the com­puter graphics 
audience use more advanced techniques for realistic image synthesis, but put little emphasis on the faithful 
reproduction of physiological mechanisms characteristic to speci.c plants. In this paper we propose a 
general framework (de.ned as a mod­eling methodology supported by appropriate software) for mod­eling, 
simulating, and visualizing the development of plants that bi-directionally interact with their environment. 
The usefulness of modeling frameworks for simulation studies of models with com­plex (emergent) behavior 
is manifested by previous work in the­oretical biology, arti.cial life, and computer graphics. Examples 
include cellular automata [51], systems for simulating behavior of cellular structures in discrete [1] 
and continuous [20] spaces, and L-system-based frameworks for modeling plants [36, 46]. Frame­works may 
have the form of a general-purpose simulation program that accepts models described in a suitable mini-language 
as in­put, e.g. [36, 46], or a set of library programs [27]. Compared to special-purpose programs, they 
offer the following bene.ts: At the conceptual level, they facilitate the design, speci.cation, documentation, 
and comparison of models.  At the level of model implementation, they make it possible to de­velop software 
that can be reused in various models. Speci.cally, graphical capabilities needed to visualize the models 
become a part of the modeling framework, and do not have to be reimple­mented.  Finally, .exible conceptual 
and software frameworks facilitate interactive experimentation with the models [46, Appendix A].  Our 
framework is intended both for purpose of image synthesis and as a research and visualization tool for 
model studies in plant mor­phogenesis and ecology. These goals are addressed at the levels of the simulation 
system and the modeling language design. The un­derlying paradigm of plant-environment interaction is 
described in Section 2. The resulting design of the simulation software is outlined in Section 3. The 
language for specifying plant models is presented in Section 4. It extends the concept of environmentally-sensitive 
L­systems [45] with constructs for bi-directional communication with the environment. The following sections 
illustrate the proposed framework with concrete models of plants interacting with their environment. 
The examples include: the development of planar branching systems controlled by the crowding of apices 
(Section 5), the development of clonal plants controlled by both the crowding of ramets and the quality 
of terrain (Section 6), the development of roots controlled by the concentration of water transported 
in the soil (Section 7), and the development of tree crowns affected by the local distribution of light 
(Section 8) The paper concludes with an evaluation of the results and a list of open problems (Section 
9). Plant Environment Reception Response Internal processes Internal processes Response Reception Figure 
1: Conceptual model of plant and environment treated as communicating concurrent processes 2 CONCEPTUAL 
MODEL As described by Hart [30], every environmentally controlled phe­nomenon can be considered as a 
chain of causally linked events. After a stimulus is perceived by the plant, information in some form 
is transported through the plant body (unless the site of stimulus perception coincides with the site 
of response), and the plant re­acts. This reaction reciprocally affects the environment, causing its 
modi.cation that in turn affects the plant. For example, roots growing in the soil can absorb or extract 
water (depending on the water concentration in their vicinity). This initiates a .ow of water in the 
soil towards the depleted areas, which in turn affects further growth of the roots [12, 24]. According 
to this description, the interaction of a plant with the environment can be conceptualized as two concurrent 
processes that communicate with each other, thus forming a feedback loop of information .ow (Figure 1). 
The plant process performs the following functions: reception of information about the environment in 
the form of scalar or vector values representing the stimuli perceived by spe­ci.c organs;  transport 
and processing of information inside the plant;  generation of the response in the form of growth changes 
(e.g. development of new branches) and direct output of information to the environment (e.g. uptake and 
excretion of substances by a root tip). Similarly, the environmental process includes mechanisms for 
the:  perception of the plant s actions;  simulation of internal processes in the environment (e.g. 
the diffusion of substances or propagation of light);  presentation of the modi.ed environment in a 
form perceivable by the plant.  The design of a simulation system based on this conceptual model is 
presented next. 3 SYSTEM DESIGN The goal is to create a framework, in which a wide range of plant structures 
and environments can be easily created, modi.ed, and used for experimentation. This requirement led us 
to the following design decisions: The plant and the environment should be modeled by separate programs 
and run as two communicating processes. This design is: -compatible with the assumed conceptual model 
of plant-envi­ronment interaction (Figure 1); -consistent with the principles of structured design (modules 
with clearly speci.ed functions jointly contribute to the solu­tion of a problem by communicating through 
a well de.ned interface; information local to each module is hidden from other modules); -appropriate 
for interactive experimentation with the models; in particular, changes in the plant program can be implemented 
without affecting the environmental program, and vice versa; -extensible to distributed computing environments, 
where dif­ferent components of a large ecosystem may be simulated using separate computers. The user 
should have control over the type and amount of infor­mation exchanged between the processes representing 
the plant and the environment, so that all the needed but no super.uous information is transferred. 
 Plant models should be speci.ed in a language based on L­systems, equipped with constructs for bi-directional 
communi­cation between the plant and the environment. This decision has the following rationale:  -A 
succinct description of the models in an interpreted lan­guage facilitates experimentation involving 
modi.cations to the models; -L-systems capture two fundamental mechanisms that control development, namely 
.ow of information from a mother mod­ule to its offspring (cellular descent) and .ow of information between 
coexisting modules (endogenous interaction) [38]. The latter mechanism plays an essential role in transmitting 
information from the site of stimulus perception to the site of the response. Moreover, L-systems have 
been extended to allow for input of information from the environment (see Section 4); -Modeling of plants 
using L-systems has reached a relatively advanced state, manifested by models ranging from algae to herbaceous 
plants and trees [43, 46]. Given the variety of processes that may take place in the environ­ment, they 
should be modeled using special-purpose programs.  Generic aspects of modeling, not speci.c to particular 
models, should be supported by the modeling system. This includes:  -an L-system-based plant modeling 
program, which interprets L-systems supplied as its input and visualizes the results, and -the support 
for communication and synchronization of pro­cesses simulating the modeled plant and the environment. 
A system architecture stemming from this design is shown in Fig­ure 2. We will describe it from the perspective 
of extensions to the formalism of L-systems. Plant model (L-system) Interface plant- environment Plant 
simulator Environ- mental data Figure 2: Organization of the software for modeling plants interact­ing 
with their environment. Shaded rectangles indicate components of the modeling framework, clear rectangles 
indicate programs and data that must be created by a user specifying a new model of a plant or environment. 
Shaded arrows indicate information exchanged in a standardized format. 4 OPEN L-SYSTEMS Historically, 
L-systems were conceived as closed cybernetic sys­tems, incapable of simulating any form of communication 
between the modeled plant and its environment. In the .rst step towards the inclusion of environmental 
factors, Rozenberg de.ned table L­systems, which allow for a change in the set of developmental rules 
(the production set of the L-system) in response to a change in the environment [31, 49]. Table L-systems 
were applied, for ex­ample, to capture the switch from the production of leaves to the production of 
.owers by the apex of a plant due to a change in day length [23]. Parametric L-systems [29, 46], introduced 
later, made it possible to implement a variant of this technique, with the envi­ronment affecting the 
model in a quantitative rather than qualitative manner. In a case study illustrating this possibility, 
weather data containing daily minimum and maximum temperatures were used to control the rate of growth 
in a bean model [28]. Environmentally­sensitive L-systems [45] represented the next step in the inclusion 
of environmental factors, in which local rather than global properties of the environment affected the 
model. The new concept was the introduction of query symbols, returning current position or ori­entation 
of the turtle in the underlying coordinate system. These parameters could be passed as arguments to user-de.ned 
functions, returning local properties of the environment at the queried location. Environmentally-sensitive 
L-systems were illustrated by models of topiary scenes. The environmental functions de.ned geometric 
shapes, to which trees were pruned. Open L-systems, introduced in this paper, augment the functionality 
of environmentally-sensitive L-systems using a reserved symbol for bilateral communication with the environment. 
In short, parameters associated with an occurrence of the communication symbol can be set by the environment 
and transferred to the plant model, or set by the plant model and transferred to the environment. The 
environment is no longer represented by a simple function, but becomes an active process that may react 
to the information from the plant. Thus, plants are modeled as open cybernetic systems, sending information 
to and receiving information from the environment. In order to describe open L-systems in more detail, 
we need to recall the rudiments of L-systems with turtle interpretation. Our presentation is reproduced 
from [45]. An L-system is a parallel rewriting system operating on branching structures represented as 
bracketed strings of symbols with asso­ciated numerical parameters, called modules. Matching pairs of 
square brackets enclose branches. Simulation begins with an ini­tial string called the axiom, and proceeds 
in a sequence of discrete derivation steps. In each step, rewriting rules or productions replace all 
modules in the predecessor string by successor modules. The applicability of a production depends on 
a predecessor s context (in context-sensitive L-systems), values of parameters (in produc­tions guarded 
by conditions), and on random factors (in stochastic L-systems). Typically, a production has the format: 
id: lc<pred rc: cond!succ: prob where idis the production identi.er (label), lc, pred,and rcare the left 
context, the strict predecessor, and the right context, condis the condition, succis the successor, and 
probis the probability of production application. The strict predecessor and the successor are the only 
mandatory .elds. For example, the L-system given below consists of axiom !and three productions p1, p2,and 
p3. !: A(1)B(3)A(5) p1:A(x) !A(x+1) : 0.4 p2:A(x) !B(x 1) : 0.6 p3: A(x)<B(y)>A(z): y<4 !B(x+z)[A(y)] 
The stochastic productions p1 and p2 replace module A(x)by either A(x+1)or B(x-1), with probabilities 
equal to 0.4 and 0.6, respectively. The context-sensitive production p3 replaces a module B(y)with left 
context A(x)and right context A(z)by module B(x+z)supporting branch A(y). The application of this production 
is guarded by condition y<4. Consequently, the .rst derivation step may have the form: A(1)B(3)A(5) =A(2)B(6)[A(3)]B(4) 
It was assumed that, as a result of random choice, production p1 was applied to the module A(1), and 
production p2 to the module A(5). Production p3 was applied to the module B(3), because it occurred with 
the required left and right context, and the condition 3 <4was true. In the L-systems presented as examples 
we also use several addi­tional constructs (cf. [29, 44]): Productions may include statements assigning 
values to local variables. These statements are enclosed in curly braces and separated by semicolons. 
 The L-systems may also include arrays. References to array ele­ments follow the syntax of C; for example, 
MaxLen[order].  The list of productions is ordered. In the deterministic case, the .rst matching production 
applies. In the stochastic case, the set of all matching productions is established, and one of them 
is chosen according to the speci.ed probabilities.  For details of the L-system syntax see [29, 43, 
46]. . In contrast to the parallel applica- U tion of productions in each deriva-H \ . tion step, the 
interpretation of the + - resulting strings proceeds sequen­/ tially, with reserved modules act­^ ing 
as commands to a LOGO-style turtle [46]. At any point of the . &#38; Figure 3: Controlling the string, 
the turtle state is charac­ turtle in three dimensions ~ terized by a position vector Pand ... A(a1,...,ak) 
?E(x1,...,x) B(b1,...,b) ...mn ... A(a1,...,ak) ?E(y1,...,y) B(b1,...,b) ...mn Figure 4: Information 
.ow during the simulation of a plant inter­acting with the environment, implemented using an open L-system 
~~ three mutually perpendicular orientation vectors H, U,and L~, indi­cating the turtle s heading, the 
up direction, and the direction to the left (Figure 3). Module Fcauses the turtle to draw a line in the 
cur­rent direction. Modules +, -,&#38;, ^, Iand nrotate the turtle around ~~~ one of the vectors H;U,or 
L, as shown in Figure 3. The length of the line and the magnitude of the rotation angle can be given 
globally or speci.ed as parameters of individual modules. During the interpretation of branches, the 
opening square bracket pushes the current position and orientation of the turtle on a stack, and the 
closing bracket restores the turtle to the position and orientation popped from the stack. A special 
interpretation is reserved for the module %, which cuts a branch by erasing all symbols in the string 
from the point of its occurrence to the end of the branch [29]. The meaning of many symbols depends on 
the context in which they occur; for example, +and -denote arithmetic operators as well as modules that 
rotate the turtle. The turtle interpretation of L-systems described above was de­signed to visualize 
models in a postprocessing step, with no effect on the L-system operation. Position and orientation of 
the turtle are important, however, while considering environmental phenom­ena, such as collisions with 
obstacles and exposure to light. The environmentally-sensitive extension of L-systems makes these at­tributes 
accessible during the rewriting process [45]. The generated string is interpreted after each derivation 
step, and turtle attributes found during the interpretation are returned as parameters to re­served query 
modules. Syntactically, the query modules have the form ?X(x;y;z),where X=P;H;U;or L. Depending on the 
actual symbol X, the values of parameters x, y,and zrepresent a position or an orientation vector. Open 
L-systems are a generalization of this concept. Communica­tion modules of the form ?E(x1;:::;xm)areusedbothtosend 
and receive environmental information represented by the values of parameters x1;:::;xm(Figure 4). To 
this end, the string resulting from a derivation step is scanned from left to right to determine the 
state of the turtle associated with each symbol. This phase is similar to the graphical interpretation 
of the string, except that the results need not be visualized. Upon encountering a communica­tion symbol, 
the plant process creates and sends a message to the environment including all or a part of the following 
information: the address (position in the string) of the communication module (mandatory .eld needed 
to identify this module when a reply comes from the environment),  values of parameters xi,  the state 
of the turtle (coordinates of the position and orientation  vector, as well as some other attributes, 
such as current line width), the type and parameters of the module following the communi­cation module 
in the string (not used in the examples discussed in this paper). The exact message format is de.ned 
in a communication speci.­cation .le, shared between the programs modeling the plant and the environment 
(Figure 2). Consequently, it is possible to include only the information needed in a particular model 
in the messages sent to the environment. Transfer of the last message corresponding to the current scan 
of the string is signaled by a reserved end-of­transmission message, which may be used by the environmental 
process to start its operation. The messages output by the plant modeling program are transferred to 
the process that simulates the environment using an interprocess communication mechanism provided by 
the underlying operating system (a pair of UNIX pipes or shared memory with access syn­chronized using 
semaphores, for example). The environment pro­cesses that information and returns the results to the 
plant model using messages in the following format: the address of the target communication module, 
 values of parameters yicarrying the output from the environment.  The plant process uses the received 
information to set parameter val­ues in the communication modules (Figure 4). The use of addresses makes 
it possible to send replies only to selected communication modules. Details of the mapping of messages 
received by the plant process to the parameters of the communication modules are de.ned in the communication 
speci.cation .le. After all replies generated by the environment have been received (a fact indicated 
by an end-of-transmission message sent by the environment), the resulting string may be interpreted and 
visualized, and the next derivation step may be performed, initiating another cycle of the simulation. 
Note that, by preceding every symbol in the string with a communi­cation module it is possible to pass 
complete information about the model to the environment. Usually, however, only partial informa­tion 
about the state of a plant is needed as input to the environment. Proper placement of communication modules 
in the model, com­bined with careful selection of the information to be exchanged, provide a means for 
keeping the amount of transferred information at a manageable level. We will illustrate the operation 
of open L-systems within the con­text of complete models of plant-environment interactions, using examples 
motivated by actual biological problems.  A MODEL OF BRANCH TIERS Background. Apical meristems, located 
at the endpoints of branches, are engines of plant development. The apices grow, con­tributing to the 
elongation of branch segments, and from time to time divide, spawning the development of new branches. 
If all apices divided periodically, the number of apices and branch segments would increase exponentially. 
Observations of real branching struc­tures show, however, that the increase in the number of segments 
is less than exponential [8]. Honda and his collaborators mod­eled several hypothetical mechanisms that 
may control the extent of branching in order to prevent overcrowding [7, 33] (see also [4]). One of the 
models [33], supported by measurements and earlier simulations of the tropical tree Terminalia catappa 
[19], assumes an exogenous interaction mechanism. Terminalia branches form horizontal tiers, and the 
model is limited to a single tier, treated as a two-dimensional structure. In this case, the competition 
for light effectively amounts to collision detection between the apices and leaf clusters. We reproduce 
this model as the simplest example illustrating the methodology proposed in this paper. Communication 
speci.cation. The plant communicates with the environment using communication modules of the form ?E(x). 
Messages sent to the environment include the turtle position and the value of parameter x, interpreted 
as the vigor of the corresponding apex. On this basis, the environmental process determines the fate 
of each apex. A parameter value of x=0 returned to the plant indicates that the development of the corresponding 
branch will be terminated. A value of x=1 allows for further branching. The model of the environment. 
The environmental process con­siders each apex or non-terminal node of the developing tier as the center 
of a circular leaf cluster, and maintains a list of all clusters present. New clusters are added in response 
to messages received from the plant. All clusters have the same radius ., speci.ed in the environmental 
data .le (cf. Figure 2). In order to determine the fate of the apices, the environment compares apex 
positions with leaf cluster positions, and authorizes an apex to grow if it does not fall into an existing 
leaf cluster, or if it falls into a cluster surrounding an apex with a smaller vigor value. The plant 
model. The plant model is expressed as an open L­system. The values of constants are taken from [33]. 
#de.ne r1 0.94 /* contraction ratio and vigor 1 */ #de.ne r2 0.87 /* contraction ratio and vigor 2 */ 
#de.ne a1 24.4 /* branching angle 1 */ #de.ne a2 36.9 /* branching angle 2 */ #de.ne '138.5 /* divergence 
angle */ !: (90)[F(1)?E(1)A(1)]+(')[F(1)/?E(1)A(1)] +(')[F(1)?E(1)A(1)]+(')[F(1)/?E(1)A(1)] +(')[F(1)?E(1)A(1)] 
p1: ?E(x)<A(v): x==1 ! [+(a2)F(v*r2)?E(r2)A(v*r2)] (a1)F(v*r1)/?E(r1)A(v*r1) p2:?E(x) !" The axiom !speci.es 
the initial structure as a whorl of .ve branch segments F. The divergence angle 'between consecutive 
segments is equal to 138:5.. Each segment is terminated by a communication symbol ?Efollowedbyan apex 
A. In addition, two branches include module I, which changes the directions at which subsequent branches 
will be issued (left vs. right) by rotating the apex 180. around the segment axis. Production p1 describes 
the operation of the apices. If the value of parameter xreturned by a communication module ?Eis not 1, 
the associated apex will remain inactive (do nothing). Otherwise the apex will produce a pair of new 
branch segments at angles a1 and a2 with respect to the mother segment. Constants r1 and r2 determine 
the lengths of the daughter segments as fractions of the length of their mother segment. The values r1 
and r2 are also passed to the process simulating the environment using communication modules ?E. Communication 
modules created in the previous derivation step are no longer needed and are removed by production p2. 
 Figure 5: Competition for space between two tiers of branches simulated using the Honda model Simulation. 
Figure 5 illustrates the competition for space between two tiers developing next to each other. The extent 
of branching in each tier is limited by collisions between its apices and its own or the neighbor s leaf 
clusters. The limited growth of each struc­ture in the direction of its neighbor illustrates the phenomenon 
of morphological plasticity, or adaptation of the form of plants to their environment [17].  A MODEL 
OF FORAGING IN CLONAL PLANTS Background. Foraging (propagation) patterns in clonal plants pro­vide another 
excellent example of response to crowding. A clonal plant spreads by means of horizontal stem segments 
(spacers), which form a branching structure that grows along the ground and connects individual plants 
(ramets) [3]. Each ramet consists of a leaf supported by an upright stem and one or more buds, which 
may give rise to further spacers and ramets. Their gradual death, after a certain amount of time, causes 
gradual separation of the whole structure (the clone) into independent parts. Following the surface of 
the soil, clonal plants can be captured using models operating in two dimensions [5], and in that respect 
resem­ble Terminalia tiers. We propose a model of a hypothetical plant that responds to favorable environmental 
conditions (high local in­tensity of light) by more extensive branching and reduced size of leaves (allowing 
for more dense packing of ramets). It has been inspired by a computer model of clover outlined by Bell 
[4], the analysis of responses of clonal plants to the environment presented by Dong [17], and the computer 
models and descriptions of veg­etative multiplication of plants involving the death of intervening connections 
by Room [47]. Communication speci.cation. The plant sends messages to the en­vironment that include turtle 
position and two parameters associated with the communications symbol, ?E(type;x). The .rst param­eter 
is equal to 0, 1, or 2, and determines the type of exchanged information as follows: The message ?E(0;x)represents 
a request for the light intensity (irradiance [14]) at the position of the communication module. The 
environment responds by setting xto the intensity of incom­ing light, ranging from 0 (no light) to 1 
(full light). The message ?E(1;x)noti.es the environment about the cre­ation of a ramet with a leaf 
of radius xat the position of the communication module. No output is generated by the environ­ment in 
response to this message.  The message ?E(2;x)noti.es the environment about the death of a ramet with 
a leaf of radius xat the position of the communi­cation module. Again, no output is generated by the 
environment.  The model of the environment. The purpose of the environment process is to determine light 
intensity at the locations requested by the plant. The ground is divided into patches (speci.ed as a 
raster image using a paint program), with different light intensities assigned to each patch. In the 
absence of shading, these intensities are returned by the environmental process in response to messages 
of type 0. To consider shading, the environment keeps track of the set of ramets, adding new ramets in 
response to a messages of type 1, and deleting dead ramets in response to messages of type 2. If a sampling 
point falls in an area occupied by a ramet, the returned light intensity is equal to 0 (leaves are assumed 
to be opaque, and located above the sampling points). The plant model. The essential features of the 
plant model are speci.ed by the following open L-system. #de.ne a45 /* branching angle */ #de.ne MinLight 
0.1 /* light intensity threshold */ #de.ne MaxAge 20 /* lifetime of ramets and spacers */ #de.ne Len 
2.0 /* length of spacers */ #de.ne ProbB(x) (0.12+x*0.42) #de.ne ProbR(x) (0.03+x*0.54) #de.ne Radius(x) 
(sqrt(15 x*5)/h) !: A(1)?E(0,0) p1: A(dir) > ?E(0,x) : x >= MinLight !R(x)B(x,dir)F(Len,0)A( dir)?E(0,0) 
p2: A(dir) > ?E(0,x) : x < MinLight !" p3:B(x,dir) ![+(a*dir)F(Len,0)A( dir)?E(0,0)] : ProbB(x) p4:B(x,dir) 
!": 1 ProbB(x) p5:R(x) ![@o(Radius(x),0)?E(1,Radius(x))] : ProbR(x) p6:R(x) !": 1 ProbR(x) p7: @o(radius,age): 
age < MaxAge !@o(radius,age+1) p8: @o(radius,age): age == MaxAge !?E(2,radius) p9: F(len,age): age < 
MaxAge !F(len,age+1) p10: F(len,age): age == MaxAge !f(len) p11: ?E(type,x) !" The initial structure 
speci.ed by the axiom !consists of an apex A followed by the communication module ?E. If the intensity 
of light xreaching an apex is insuf.cient (below the threshold MinLight), the apex dies (production p2). 
Otherwise, the apex creates a ramet initial R(i.e., a module that will yield a ramet), a branch initial 
B, a spacer F,and a new apex Aterminated by communication module ?E(production p1). The parameter dir, 
valued either 1 or -1, controls the direction of branching. Parameters of the spacer module specify its 
length and age. A branch initial Bmay create a lateral branch with its own apex Aand communication module 
?E(production p3), or it may die and disappear from the system (production p4). The probability of survival 
is an increasing linear function ProbBof the light intensity xthat has reached the mother apex Ain the 
previous derivation step. A similar stochastic mechanism describes the production of a ramet by the ramet 
initial R(productions p5 and p6), with the probability of ramet formation controlled by an increasing 
linear function ProbR. The ramet is represented as a circle @o; its radius is a decreasing function Radiusof 
the light intensity x.As in the case of spacers, the second parameter of a ramet indicates its age, initially 
set to 0. The environment is noti.ed about the creation of the ramet using a communication module ?E. 
The subsequent productions describe the aging of spacers (p7)and ramets (p9). Upon reaching the maximum 
age MaxAge, a ramet is removed from the system and a message notifying the environment about this fact 
is sent by the plant (p8). The death of the spacers is simulated by replacing spacer modules Fwith invisible 
line seg­ments fof the same length. This replacement maintains the relative position of the remaining 
elements of the structure. Finally, produc­tion p11 removes communication modules after they have performed 
their tasks. Simulations. Division of the ground into patches used in the sim­ 0.2 0.6 ulations is shown 
in Figure 6. Ara­ bic numerals indicate the intensity III II of incoming light, and Roman nu­merals identify 
each patch. The de­ 1.0 velopment of a clonal plant assum­ 0.2 ing this division is illustrated in Fig- 
I IV ure 7. As an extension of the basic Figure 6: Division of model discussed above, the length the 
ground into patches of the spacers and the magnitude of the branching angle have been var­ied using random 
functions with a normal distribution. Ramets have been represented as trifoliate leaves. The development 
begins with a single ramet located in relatively good (light intensity 0.6) patch II at the top right 
corner of the growth area (Figure 7, step 9 of the simulation). The plant propagates through the unfavorable 
patch III without producing many branches and leaves (step 26), and reaches the best patch I at the bottom 
left corner (step 39). After quickly spreading over this patch (step 51), the plant searches for further 
favorable areas (step 62). The .rst attempt to reach patch II fails (step 82). The plant tries again, 
and this time succeeds (steps 101 and 116). Light conditions in patch II are not suf.cient, however, 
to sustain the continuous presence of the plant (step 134). The colony disappears (step 153) until the 
patch is reached again by a new wave of propagation (steps 161 and 182). The sustained occupation of 
patch I and the repetitive invasion of patch II represent an emerging behavior of the model, dif.cult 
to predict without running simulations. Variants of this model, includ­ing other branching architectures, 
responses to the environment, and layouts of patches in the environment, would make it possible to analyze 
different foraging strategies of clonal plants. A further extension could replace the empirical assumptions 
regarding plant responses with a more detailed simulation of plant physiology (for example, including 
production of photosynthates and their trans­port and partition between ramets). Such physiological models 
could provide insight into the extent to which the foraging patterns optimize plants access to resources 
[17]. Figure 7: Development of a hypothetical clonal plant simulated using an extension of L-system 
3. The individual images represent structures generated in 9, 26, 39, 51, 62, and 82 derivation steps 
(top), followed by structures generated in 101, 116, 134, 153, 161, and 182 steps (bottom). 7 A MODEL 
OF ROOT DEVELOPMENT Background. The development of roots provides many examples of complex interactions 
with the environment, which involve me­chanical properties, chemical reactions, and transport mechanisms 
in the soil. In particular, the main root and the rootlets absorb water from the soil, locally changing 
its concentration (volume of water per unit volume of soil) and causing water motion from water-rich 
to depleted regions [24]. The tips of the roots, in turn, follow the gra­dient of water concentration 
[12], thus adapting to the environment modi.ed by their own activities. Below we present a simpli.ed 
implementation of the model of root development originally proposed by Clausnitzer and Hopmans [12]. 
We assume a more rudimentary mechanism of water transport, namely diffusion in a uniform medium, as suggested 
by Liddell and Hansen [37]. The underlying model of root architecture is sim­ilar to that proposed by 
Diggle [16]. For simplicity, we focus on model operation in two-dimensions. Communication speci.cation. 
The plant interacts with the en­vironment using communication modules ?E(c;.)located at the apices of 
the root system. A message sent to the environment in­ ~~ cludes the turtle position P, the heading vector 
H,the value of parameter crepresenting the requested (optimal) water uptake, and the value of parameter 
.representing the tendency of the apex to follow the gradient of water concentration. A message returned 
to the plant speci.es the amount of water actually received by the apex as the value of parameter c, 
and the angle biasing direction of further growth as the value of .. The model of the environment. .in.C 
The environment maintains a .eld Cof water concentrations, repre­sented as an array of the amounts .C 
of water in square sampling areas. Water is transported by diffusion, simulated numerically using .nite 
differencing [41]. The environ-Figure 8: De.nition of ment responds to a request for wa­the biasing angle 
.outter from an apex located in an area (i;j)by granting the lesser of the values requested and available 
at that location. The amount of water in the sampled area is then decreased by the amount received by 
the ~ apex. The environment also calculates a linear combination Tof ~ the turtle heading vector Hand 
the gradient of water concentration rC(estimated numerically from the water concentrations in the sampled 
area and its neighbors), and returns an angle .between the ~~ vectors Tand H(Figure 8). This angle is 
used by the plant model to bias turtle heading in the direction of high water concentration. The root 
model. The open L-system representing the root model makes use of arrays that specify parameters for 
each branching order (main axis, its daughter axes, etc.). The parameter values are loosely based on 
those reported by Clausnitzer and Hopmans [12]. #de.ne N 3 /* max. branching order + 1 */ De.ne: farray 
Req[N] = f0.1, 0.4, 0.05g, /* requested nutrient intake */ MinReq[N] = f0.01, 0.06, 0.01g, /* minimum 
nutrient intake */ ElRate[N] = f0.55, 0.25, 0.55g, /* maximum elongation rate */ MaxLen[N] = f200, 5, 
0.8g, /* maximum branch length */ Sens[N] = f10, 0, 0g, /* sensitivity to gradient */ Dev[N] = f30, 75, 
75g, /* deviation in heading */ Del[N 1] = f30, 60g, /* delay in branch growth */ BrAngle[N 1] = f90, 
90g, /* branching angle */ BrSpace[N 1] = f1, 0.5g/* distance between branches */ g !: A(0,0,0)?E(Req[0],Sens[0]) 
p1: A(n,s,b) > ?E(c,.) : (s > MaxLen[n]) || (c < MinReq[n]) !" p2: A(n,s,b) > ?E(c,.): (n >= N 1) || 
(b < BrSpace[n]) fh=c/Req[n]*ElRate[n];g !+(nran(.,Dev[n]))F(h) A(n,s+h,b+h)?E(Req[n],Sens[n]) p3: A(n,s,b) 
> ?E(c,.): (n < N 1) &#38;&#38; (b >= BrSpace[n]) fh=c/Req[n]*ElRate[n];g !+(nran(.,Dev[n]))B(n,0)F(h) 
 /(180)A(n,s+h,h)?E(Req[n],Sens[n]) p4: B(n,t):t<Del[n] !B(n,t+1) p5: B(n,t) : t >= Del[n] ![+(BrAngle[n])A(n+1,0,0)?E(Req[n+1],Sens[n+1])] 
p6:?E(c,.) !" The development starts with an apex Afollowed by a communica­tion module ?E. The parameters 
of the apex represent the branch order (0 for the main axis, 1 for its daughter axes, etc.), current 
axis length, and distance (along the axis) to the nearest branching point. Figure 9: A two-dimensional 
model of a root interacting with water in soil. Background colors represent concentrations of water diffus­ing 
in soil (blue: high, black: low). The initial and boundary values have been set using a paint program. 
 Figure 10: A three-dimensional extension of the root model. Water concentration is visualized by semi-transparent 
iso-surfaces [55] surrounding the roots. As a result of competition for water, the roots grow away from 
each other. The divergence between their main axes depends on the spread of the rootlets, which grow 
faster on the left then on the right. Productions p1 to p3 describe possible fates of the apex as described 
below. If the length sof a branch axis exceeds a prede.ned maximum value MaxLen[n]characteristic to the 
branch order n, or the amount of water creceived by the apex is below the required minimum MinReq[n], 
the apex dies, terminating the growth of the axis (production p1). If the branch order nis equal to the 
maximum value assumed in the model (N-1), or the distance bto the closest branching point on the axis 
is less than the threshold value BrSpace[n],the apex adds a new segment Fto the axis (production p2). 
The length h of Fis the product of the nominal growth increment ElRate[n] and the ratio of the amount 
of water received by the apex cto the amount requested Req[n]. The new segment is rotated with re­spect 
to its predecessor by an angle nran(.;Dev[n]),where nranis a random function with a normal distribution. 
The mean value ., returned by the environment, biases the direction of growth towards regions of higher 
water concentration. The standard devia­tion Dev[n]characterizes the tendency of the root apex to change 
direction due to various factors not included explicitly in the model. If the branch order nis less than 
the maximum value assumed in the model (N-1), and the distance bto the closest branching point on the 
axis is equal to or exceeds the threshold value BrSpace[n],the apex creates a new branch initial B(production 
p3). Other aspects of apex behavior are the same as those described by production p2. After the delay 
of Del[n]steps (production p4), the branch initial B is transformed into an apex Afollowed by the communication 
mod­ule ?E(production p5), giving rise to a new root branch. Production p6 removes communication modules 
that are no longer needed. Simulations. A sample two-dimensional structure obtained using the described 
model is shown in Figure 9. The apex of the main axis follows the gradient of water concentration, with 
small deviations due to random factors. The apices of higher-order axes are not sensitive to the gradient 
and change direction at random, with a larger standard deviation. The absorption of water by the root 
and the rootlets decreases water concentration in their neighborhood; an effect that is not fully compensated 
by water diffusion from the water-rich areas. Low water concentration stops the development of some rootlets 
before they have reached their potential full length. Figure 10 presents a three-dimensional extension 
of the previous model. As a result of competition for water, the main axes of the roots diverge from 
each other (left). If their rootlets grow more slowly, the area of in.uence of each root system is smaller 
and the main axes are closer to each other (right). This behavior is an emergent property of interactions 
between the root modules, mediated by the environment.  8 MODELS OF TREES CONTROLLED BY LIGHT Background. 
Light is one of the most important factors affect­ing the development of plants. In the essentially two-dimensional 
structures discussed in Section 5, competition for light could be considered in a manner similar to collision 
detection between leaves and apices. In contrast, competition for light in three-dimensional structures 
must be viewed as long-range interaction. Speci.cally, shadows cast by one branch may affect other branches 
at signi.cant distances. The .rst simulations of plant development that take the local light environment 
into account are due to Greene [25]. He considered the entire sky hemisphere as a source of illumination 
and computed the amount of light reaching speci.c points of the structure by casting rays towards a number 
of points on the hemisphere. Another approach was implemented by Kanamaru et al. [35], who computed the 
amount of light reaching a given sampling point by considering it a center of projection, from which 
all leaf clusters in a tree were projected on a surrounding hemisphere. The degree to which the hemisphere 
was covered by the projected clusters indicated the amount of light received by the sampling point. In 
both cases, the models of plants responded to the amount and the direction of light by simulating heliotropism, 
which biased the direction of growth towards the vector of the highest intensity of incoming light. Subsequently, 
Chiba et al. extended the models by Kanamaru et al. using more involved tree models that included a mechanism 
simulating the .ow of hypothetical endogenous information within the tree [10, 11]. A biologically better 
justi.ed model, formulated in terms of production and use of photosynthates by a tree, was proposed by 
Takenaka [52]. The amount of light reaching leaf clusters was calculated by sampling a sky hemisphere, 
as in the work by Greene. Below we reproduce the main features of the Takenaka s model using the formalism 
of open L-systems. Depending on the underlying tree architecture, it can be applied to synthesize images 
of deciduous and coniferous trees. We focus on a deciduous tree, which requires a slightly smaller number 
of productions. Communication speci.cation. The plant interacts with the envi­ronment using communication 
modules ?E(r). A message sent by ~ the plant includes turtle position P, which represents the center 
of a spherical leaf cluster, and the value of parameter r, which represents the cluster s radius. The 
environment responds by setting rto the .ux [14] of light from the sky hemisphere, reaching the cluster. 
The model of the environment. Once all messages describing the current distribution of leaves on a tree 
have been received, the environmental process computes the extent of the tree in the x, y, and zdirections, 
encompasses the tree in a tight grid (32 .32 . 32 voxels in our simulations), and allocates leaf clusters 
to voxels to speed up further computations. The environmental process then estimates the light .ux F 
from the sky hemisphere reaching each cluster (shadows cast by the branches are ignored). To this end, 
the hemisphere is represented by a set of directional light sources S(9 in the simulations). The .ux 
densities (radiosities) Bof the sources approximate the non-uniform distribution of light from the sky 
(cf. [52]). For each leaf cluster Liand each light source S,the environment determines the set of leaf 
clusters Ljthat may shade Li. This is achieved by casting a ray from the center of Liin the direction 
of Sand testing for intersections with other clusters (the grid accelerates this process). In order not 
to miss any clusters that may partially occlude Li, the radius of each cluster Ljis increased by the 
maximum value of cluster radius rmax. To calculate the .ux reaching cluster Li, this cluster and all 
clusters Ljthat may shade it according to the described tests are projected on a plane Pperpendicular 
to the direction of light from the source S. The impact of a cluster Ljon the .ux F reaching cluster 
Liis then computed according to the formula: F=(Ai -Aij)B+Aij.B where Aiis the area of the projection 
of Lion P, Aijis the area of the intersection between projections of Liand Lj,and .is the light transmittance 
through leaf cluster Lj(equal to 0.25 in the simulations). If several clusters Ljshade Li, their in.uences 
are multiplied. The total .ux reaching cluster Liis calculated as the sum of the .uxes received from 
each light source S. The plant model. In addition to the communication module ?E, the plant model includes 
the following types of modules: Apex A(vig;del). Parameter vigrepresents vigor, which deter­mines the 
length of branch segments (internodes) and the diam­eter of leaf clusters produced by the apex. Parameter 
delis used to introduce a delay, needed for propagating products of photo­synthesis through the tree 
structure between consecutive stages of development (years).  Leaf L(vig;p;age;del). Parameters denote 
the leaf radius vig, the amount of photosynthates produced in unit time according to the leaf s exposure 
to light p, the number of years for which a leaf has appeared at a given location age, and the delay 
del, which plays the same role as in the apices.  Internode F(vig). Consistent with the turtle interpretation, 
the  parameter vigindicates the internode length. Branch width symbol !(w;p;n), also used to carry the 
endoge­nous information .ow. The parameters determine: the width of the following internode w, the amount 
of photosynthates reach­ing the symbol s location p, and the number of terminal branch segments above 
this location n. The corresponding L-system is given below. #de.ne '137.5 /* divergence angle */ #de.ne 
a0 5 /* direction change -no branching */ #de.ne a1 20 /* branching angle -main axis */ #de.ne a2 32 
/* branching angle -lateral axis */ #de.ne W 0.02 /* initial branch width */ #de.ne VD 0.95 /* apex vigor 
decrement */ #de.ne Del 30 /* delay */ #de.ne LS 5 /* how long a leaf stays */ #de.ne LP 8 /* full photosynthate 
production */ #de.ne LM 2 /* leaf maintenance */ #de.ne PB 0.8 /* photosynthates needed for branching 
*/ #de.ne PG 0.4 /* photosynthates needed for growth */ #de.ne BM 0.32 /* branch maintenance coef.cient 
*/ #de.ne BE 1.5 /* branch maintenance exponent */ #de.ne Nmin25 /* threshold for shedding */ Consider: 
?E[]!L /* for context matching */ !: !(W,1,1)F(2)L(1,LP,0,0)A(1,0)[!(0,0,0)]!(W,0,1) p1: A(vig,del) 
: del<Del !A(vig,del+1) p2: L(vig,p,age,del) : (age<LS)&#38;&#38;(del<Del 1) !L(vig,p,age,del+1) p3: 
L(vig,p,age,del) : (age<LS)&#38;&#38;(del==Del 1) !L(vig,p,age,del+1)?E(vig*0.5) p4: L(vig,p,age,del) 
> ?E(r) : (age<LS) &#38;&#38; (r*LP>=LM) &#38;&#38; (del == Del) !L(vig,LP*r LM,age+1,0) p5: L(vig,p,age,del) 
> ?E(r) : ((age == LS)||(r*LP<=LM)) &#38;&#38; (del == Del) !L(0,0,LS,0) p6: ?E(r) < A(vig,del) : r*LP 
LM>PB fvig=vig*VD;g !/(')[+(a2)!(W, PB,1)F(vig)L(vig,LP,0,0)A(vig,0) [!(0,0,0)]!(W,0,1)] (a1)!(W,0,1)F(vig)L(vig,LP,0,0)/A(vig,0) 
p7: ?E(r) < A(vig,del) : r*LP LM > PG fvig=vig*VD;g !/(') (a0)[!(0,0,0)] !(W, PG,1)F(vig)L(vig,LP,0,0)A(vig,0) 
p8: ?E(r) < A(vig,del) : r*LP LM <= PG !A(vig,0) p9: ?E(r) !" p10: !(w0,p0,n0) > L(vig,pL,age,del) [!(w1,p1,n1)]!(w2,p2,n2): 
fw=(w1 2+w2 2) 0.5; p=p1+p2+pL BM*(w/W) BE;g (p>0) || (n1+n2 =Nmin) !!(w,p,n1+n2) p11: !(w0,p0,n0) > 
L(vig,pL,age,del) [!(w1,p1,n1)]!(w2,p2,n2) !!(w0,0,0)L% The simulation starts with a structure consisting 
of a branch segment F, supporting a leaf Land an apex A(axiom !). The .rst branch width symbol ! de.nes 
the segment width. Two additional symbols ! following the apex create virtual branches," needed to provide 
proper context for productions p10 and p11. The tree grows in stages, with the delay of Del+1 derivation 
steps between consecutive stages introduced by production p1 for the apices and p2 for the leaves. Immediately 
before each new growth stage, communication symbols are introduced to inform the environment about the 
location and size of the leaf clusters (p3). If the .ux rreturned by the environment results in the production 
of photosynthates r.LP 32768 8192 2048 512 128 32 8 2 0 year Figure 11: The number of terminal branch 
segments resulting from unrestricted bifurcation of apices (continuous line), compared to the number 
of segments generated in a simulation (isolated points) exceeding the amount LMneeded to maintain a cluster, 
it remains in the structure (p4). Otherwise it becomes a liability to the tree and dies (p5). Another 
condition to production p5 prevents a leaf from occupying the same location for more than LSyears. The 
.ux ralso determines the fate of the apex, captured by pro­ductions p6 to p8. If the amount of photosynthates 
r.LP-LM transported from the nearby leaf exceeds a threshold value PB,the apex produces two new branches 
(p6). The second parameter in the .rst branch symbol ! is set to -PB, to subtract the amount of photosynthates 
used for branching from the amount that will be transported further down. The length of branch segments 
vigis reduced with respect to the mother segment by a prede.ned factor VD, re.ecting a gradual decrease 
in the vigor of apices with age. The branch width modules ! following the .rst apex Aare intro­duced 
to provide context required by productions p10 and p11,as in the axiom. If the amount of photosynthates 
r.LP-LMtransported from the leaf is insuf.cient to produce new branches, but above the threshold PG, 
the apex adds a new segment Fto the current branch axis without creating a lateral branch (p7). Again, 
a virtual branch containing the branch width symbol ! is being added to provide context for productions 
p10 and p11. If the amount of photosynthates is below PG, the apex remains dor­mant (p8). Communication 
modules no longer needed are removed from the structure (p9). Production p10 captures the endogenous 
information .ow from leaves and terminal branch segments to the base of the tree. First, it determines 
the radius wof the mother branch segment as a function of the radii w1 and w2 of the supported branches: 
p w=w12 +w22: Thus, a cross section of the mother segment has an area equal to the sum of cross sections 
of the supported segments, as postulated in the literature [40, 46]. Next, production p10 calculates 
the .ow pof photosynthates into the mother segment. It is de.ned as the sum of the .ows pL, p1 and p2 
received from the associated leaf Land from BE both daughter branches, decreased by the amount BM.(wIW)representing 
the cost of maintaining the mother segment. Finally, production p9 calculates the number of terminal 
branch segments nsupported by the mother segment as the sum of the numbers of terminal segments supported 
by the daughter branches, n1 and n2. Production p10 takes effect if the .ow pis positive (the branch 
is not a liability to the tree), or if the number nof supported terminals number of terminals  Figure 
12: A tree model with branches competing for access to light, shown without the leaves is above a threshold 
Nmin. If these conditions are not satis.ed, production p11 removes (sheds) the branch from the tree using 
the cut symbol %. Simulations. The competition for light between tree branches is manifested by two phenomena: 
reduced branching or dormancy Figure 14: A model of deciduous trees competing for light. The trees are 
shown in the position of growth (top) and moved apart (bottom) to reveal the adaptation of crown geometry 
to the presence of the neighbor tree. of apices in unfavorable local light conditions, and shedding of 
branches which do not receive enough light to contribute to the whole tree. Both phenomena limit the 
extent of branching, thus controlling the density of the crown. This property of the model is supported 
by the simulation results shown in Figure 11. If the growth was unlimited (production p6 was always chosen 
over p7 and p8), the number of terminal branch segments would double every year. Due to the competition 
for light, however, the number of terminal segments observed in an actual simulation increases more slowly. 
For related statistics using a different tree architecture see [52]. A tree image synthesized using an 
extension of the presented model is shown in Figure 12. The key additional feature is a gradual reduction 
of the branching angle of a young branch whose sister branch has been shed. As the result, the remaining 
branch assumes the role of the leading shoot, following the general growth direction of its supporting 
segment. Branch segments are represented as texture-mapped generalized cylinders, smoothly connected 
at the branching points (cf. [6]). The bark texture was created using a paint program. As an illustration 
of the .exibility of the modeling framework pre­sented in this paper, Figure 13 shows the effect of seeding 
a hypo­thetical climbing plant near the same tree. The plant follows the surface of the tree trunk and 
branches, and avoids excessively dense colonization of any particular area. Thus, the model integrates 
sev­eral environmentally-controlled phenomena: the competition of tree branches for light, the following 
of surfaces by a climbing plant, and the prevention of crowding as discussed in Section 6. Leaves were 
modeled using cubic patches (cf. [46]).  In the simulations shown in Figure 14 two trees described 
by the same set of rules (younger specimens of the tree from Figure 12) compete for light from the sky 
hemisphere. Moving the trees apart after they have grown reveals the adaptation of their crowns to the 
presence of the neighbor tree. This simulation illustrates both the necessity and the possibility of 
incorporating the adaptive behavior into tree models used for landscape design purposes. The same phenomenon 
applies to coniferous trees, as illustrated in Figure 15. The tree model is similar to the original model 
by Takenaka [52] and can be viewed as consisting of approximately horizontal tiers (as discussed in Section 
5) produced in sequence by the apex of the tree stem. The lower tiers are created .rst and therefore 
potentially can spread more widely then the younger tiers higher up (the phase effect [46]). This pattern 
of development is affected by the presence of the neighboring tree: the competition for light prevents 
the crowns from growing into each other. The trees in Figure 15 retain branches that do not receive enough 
light. In contrast, the trees in the stand presented in Figure 16 shed branches that do not contribute 
photosynthates to the entire tree, using the same mechanism as described for the deciduous trees. The 
resulting simulation reveals essential differences between the shape of the tree crown in the middle 
of a stand, at the edge, or at the corner. In particular, the tree in the middle retains only the upper 
part of its crown. In lumber industry, the loss of lower branches is usually a desirable phenomenon, 
as it reduces knots in the wood and the amount of cleaning that trees require before transport. Simulations 
may assist in choosing an optimal distance for planting trees, where self-pruning is maximized, yet there 
is suf.cient space between trees too allow for unimpeded growth of trunks in height and diameter.  9 
CONCLUSIONS In this paper, we introduced a framework for the modeling and visu­alization of plants interacting 
with their environment. The essential elements of this framework are: a system design, in which the 
plant and the environment are treated as two separate processes, communicating using a stan­dard interface, 
and  the language of open L-systems, used to specify plant models that can exchange information with 
the environment.  We demonstrated the operation of this framework by implementing models that capture 
collisions between branches, the propagation of clonal plants, the development of roots in soil, and 
the development of tree crowns competing for light. We found that the proposed framework makes it possible 
to easily create and modify models spanning a wide range of plant structures and environmental pro­cesses. 
Simulations of the presented phenomena were fast enough to allow interactive experimentation with the 
models (Table 1). There are many research topics that may be addressed using the simulation and visualization 
capabilities of the proposed framework. They include, for instance: Fundamental analysis of the role 
of different forms of informa­tion .ow in plant morphogenesis (in particular, the relationship Fig. Number 
of Derivation Timea branch segments leaf clusters steps yrs sim. render. 5 138 140 5 5 1s 1s 7 786 229 
182 NA 50 s 2s 9 4194 34b 186 NA 67 s 3s 10 37228 448b 301 NA 15 min 70 s 12 22462 19195 744 24 22 min 
13 sc 15 13502 3448 194 15 4min 8sd aSimulation and rendering using OpenGL on a 200MHz/64MB Indigo2 
Extreme bactive apices cwithout generalized cylinders and texture mapping dbranching structure without 
needles Table 1: Numbers of primitives and simulation/rendering times for generating and visualizing 
selected models between endogenous and exogenous .ow). This is a continuation of the research pioneered 
by Bell [4] and Honda et al. [7, 33]. Development of a comprehensive plant model describing the cycling 
of nutrients from the soil through the roots and branches to the leaves, then back to the soil in the 
form of substances released by fallen leaves.  Development of models of speci.c plants for research, 
crop and forest management, and for landscape design purposes. The models may include environmental phenomena 
not discussed in this paper, such as the global distribution of radiative energy in the tree crowns, 
which affects the amount of light reaching the leaves and the local temperature of plant organs.  The 
presented framework itself is also open to further research. To begin, the precise functional speci.cation 
of the environment, im­plied by the design of the modeling framework, is suitable for a formal analysis 
of algorithms that capture various environmental processes. This analysis may highlight tradeoffs between 
time, memory, and communication complexity, and lead to programs matching the needs of the model to available 
system resources in an optimal manner. A deeper understanding of the spectrum of processes taking place 
in the environment may lead to the design of a mini-language for envi­ronment speci.cation. Analogous 
to the language of L-systems for plant speci.cation, this mini-language would simplify the modeling of 
various environments, relieving the modeler from the burden of low-level programming in a general-purpose 
language. Fleischer and Barr s work on the speci.cation of environments supporting collisions and reaction-diffusion 
processes [20] is an inspiring step in this direction. Complexity issues are not limited to the environment, 
but also arise in plant models. They become particularly relevant as the scope of modeling increases 
from individual plants to groups of plants and, eventually, entire plant communities. This raises the 
problem of selecting the proper level of abstraction for designing plant models, including careful selection 
of physiological processes incorporated into the model and the spatial resolution of the resulting structures. 
The complexity of the modeling task can be also addressed at the level of system design, by assigning 
various components of the model (individual plants and aspects of the environment) to different components 
of a distributed computing system. The communication structure should then be redesigned to accommodate 
information transfers between numerous processes within the system. In summary, we believe that the proposed 
modeling methodology and its extensions will prove useful in many applications of plant modeling, from 
research in plant development and ecology to land­scape design and realistic image synthesis.  Acknowledgements 
We would like to thank Johannes Battjes, Campbell Davidson, Art Diggle, Heinjo During, Michael Guzy, 
Naoyoshi Kanamaru, Bruno Moulia, Zbigniew Prusinkiewicz, Bill Remphrey, David Reid, and Peter Room for 
discussions and pointers to the literature rele­vant to this paper. We would also like to thank Bruno 
Andrieu, Mark Hammel, Jim Hanan, Lynn Mercer, Chris Prusinkiewicz, Pe­ter Room, and the anonymous referees 
for helpful comments on the manuscript. Most images were rendered using the ray tracer rayshadeby Craig 
Kolb. This research was sponsored by grants from the Natural Sciences and Engineering Research Council 
of Canada.  REFERENCES [1] AGRAWAL, P. The cell programming language. Arti.cial Life 2,1 (1995), 37 
77. [2] ARVO,J., AND KIRK, D. Modeling plants with environment-sensitive automata. In Proceedings of 
Ausgraph 88 (1988), pp. 27 33. [3] BELL,A. Plant form: An illustrated guide to .owering plants. Oxford 
University Press, Oxford, 1991. [4] BELL, A. D. The simulation of branching patterns in modular or­ganisms. 
Philos. Trans. Royal Society London, Ser. B 313 (1986), 143 169. [5] BELL,A. D., ROBERTS,D., AND SMITH, 
A. Branching patterns: the simulation of plant architecture. Journal of Theoretical Biology 81 (1979), 
351 375. [6] BLOOMENTHAL, J. Modeling the Mighty Maple. Proceedings of SIG-GRAPH 85 (San Francisco, California, 
July 22-26, 1985), in Com­puter Graphics, 19, 3 (July 1985), pages 305 311, ACM SIGGRAPH, New York, 1985. 
[7] BORCHERT,R., AND HONDA, H. Control of development in the bi­furcating branch system of Tabebuia rosea: 
A computer simulation. Botanical Gazette 145, 2 (1984), 184 195. [8] BORCHERT,R., AND SLADE, N. Bifurcation 
ratios and the adaptive geometry of trees. Botanical Gazette 142, 3 (1981), 394 401. [9] CHEN,S. G., 
CEULEMANS,R., AND IMPENS, I. A fractal based Populus canopy structure model for the calculation of light 
interception. Forest Ecology and Management (1993). [10] CHIBA,N., OHKAWA,S., MURAOKA,K., AND MIURA, 
M. Visual sim­ulation of botanical trees based on virtual heliotropism and dormancy break. The Journal 
of Visualization and Computer Animation 5 (1994), 3 15. [11] CHIBA,N., OHSHIDA,K., MURAOKA,K., MIURA,M., 
AND SAITO,N. A growth model having the abilities of growth-regulations for simulating visual nature of 
botanical trees. Computers and Graphics 18, 4 (1994), 469 479. [12] CLAUSNITZER,V., AND HOPMANS, J. Simultaneous 
modeling of tran­sient three-dimensional root growth and soil water .ow. Plant and Soil 164 (1994), 299 
314. [13] COHEN, D. Computer simulation of biological pattern generation pro­cesses. Nature 216 (October 
1967), 246 248. [14] COHEN,M., AND WALLACE,J. Radiosity and realistic image synthesis. Academic Press 
Professional, Boston, 1993. With a chapter by P. Hanrahan and a foreword by D. Greenberg. [15] DE REFFYE,P., 
HOULLIER,F., BLAISE,F., BARTHELEMY,D., DAUZAT, J., AND AUCLAIR, D. A model simulating above-and below-ground 
tree architecture with agroforestry applications. Agroforestry Systems 30 (1995), 175 197. [16] DIGGLE, 
A. J. ROOTMAP -a model in three-dimensional coordinates of the structure and growth of .brous root systems. 
Plant and Soil 105 (1988), 169 178. [17] DONG,M. Foraging through morphological response in clonal herbs. 
PhD thesis, Univeristy of Utrecht, October 1994. [18] FISHER, J. B. How predictive are computer simulations 
of tree archi­tecture. International Journal of Plant Sciences 153 (Suppl.) (1992), 137 146. [19] FISHER,J. 
B., AND HONDA, H. Computer simulation of branching pattern and geometry in Terminalia (Combretaceae), 
a tropical tree. Botanical Gazette 138, 4 (1977), 377 384. [20] FLEISCHER,K. W., AND BARR, A. H. A simulation 
testbed for the study of multicellular development: The multiple mechanisms of morpho­genesis. In Arti.cial 
Life III, C. G. Langton, Ed. Addison-Wesley, Redwood City, 1994, pp. 389 416. [21] FORD,E. D., AVERY,A., 
AND FORD, R. Simulation of branch growth in the Pinaceae: Interactions of morphology, phenology, foliage 
pro­ductivity, and the requirement for structural support, on the export of carbon. Journal of Theoretical 
Biology 146 (1990), 15 36. [22] FORD, H. Investigating the ecological and evolutionary signi.cance of 
plant growth form using stochastic simulation. Annals of Botany 59 (1987), 487 494. [23] FRIJTERS,D., 
AND LINDENMAYER, A. A model for the growth and .owering of Aster novae-angliae on the basis of table 
(1,0)L-systems. In LSystems, G. Rozenberg and A. Salomaa, Eds., Lecture Notes in Computer Science 15. 
Springer-Verlag, Berlin, 1974, pp. 24 52. [24] GARDNER, W. R. Dynamic aspects of water availability to 
plants. Soil Science 89, 2 (1960), 63 73. [25] GREENE, N. Voxel space automata: Modeling with stochastic 
growth processes in voxel space. Proceedings of SIGGRAPH 89 (Boston, Mass., July 31 August 4, 1989), 
in Computer Graphics 23, 4 (August 1989), pages 175 184, ACM SIGGRAPH, New York, 1989. [26] GREENE, N. 
Detailing tree skeletons with voxel automata. SIG-GRAPH 91 Course Notes on Photorealistic Volume Modeling 
and Rendering Techniques, 1991. [27] GUZY, M. R. A morphological-mechanistic plant model formalized in 
an object-oriented parametric L-system. Manuscript, USDA-ARS Salinity Laboratory, Riverside, 1995. [28] 
HANAN, J. Virtual plants Integrating architectural and physiological plant models. In Proceedings of 
ModSim 95 (Perth, 1995), P. Binning, H. Bridgman, and B. Williams, Eds., vol. 1, The Modelling and Sim­ulation 
Society of Australia, pp. 44 50. [29] HANAN,J. S. Parametric L-systems and their application to the mod­elling 
and visualization of plants. PhD thesis, University of Regina, June 1992. [30] HART,J. W. Plant tropisms 
and other growth movements.Unwin Hyman, London, 1990. [31] HERMAN,G. T., AND ROZENBERG,G. Developmental 
systems and languages. North-Holland, Amsterdam, 1975. [32] HONDA, H. Description of the form of trees 
by the parameters of the tree-like body: Effects of the branching angle and the branch length on the 
shape of the tree-like body. Journal of Theoretical Biology 31 (1971), 331 338. [33] HONDA,H., TOMLINSON,P. 
B., AND FISHER, J. B. Computer simulation of branch interaction and regulation by unequal .ow rates in 
botanical trees. American Journal of Botany 68 (1981), 569 585. [34] KAANDORP,J. Fractal modelling: Growth 
and form in biology. Springer-Verlag, Berlin, 1994. [35] KANAMARU,N., CHIBA,N., TAKAHASHI,K., AND SAITO,N. 
CG sim­ulation of natural shapes of botanical trees based on heliotropism. The Transactions of the Institute 
of Electronics, Information, and Commu­nication Engineers J75-D-II, 1 (1992), 76 85. In Japanese. [36] 
KURTH,W. Growth grammar interpreter GROGRA 2.4: A software tool for the 3-dimensional interpretation 
of stochastic, sensitive growth grammars in the context of plant modeling. Introduction and refer­ence 
manual. Forschungszentrum Wald¨at okosysteme der Universit¨G¨ottingen, 1994. ottingen, G¨ [37] LIDDELL,C. 
M., AND HANSEN, D. Visualizing complex biological interactions in the soil ecosystem. The Journal of 
Visualization and Computer Animation 4 (1993), 3 12. [38] LINDENMAYER, A. Mathematical models for cellular 
interaction in development, Parts I and II. Journal of Theoretical Biology 18 (1968), 280 315. [39] LINDENMAYER, 
A. Developmental systems without cellular interac­tion, their languages and grammars. Journal of Theoretical 
Biology 30 (1971), 455 484. [40] MACDONALD,N. Trees and networks in biological models. J. Wiley &#38; 
Sons, New York, 1983. [41] PRESS,W. H., TEUKOLSKY,S. A., VETTERLING,W. T., AND FLANNERY, B. P. Numerical 
recipes in C: The art of scienti.c computing. Second edition. Cambridge University Press, Cambridge, 
1992. [42] PRUSINKIEWICZ, P. Visual models of morphogenesis. Arti.cial Life 1, 1/2 (1994), 61 74. [43] 
PRUSINKIEWICZ,P., HAMMEL,M., HANAN,J., AND M.ECH,R. Visual models of plant development. In Handbook of 
formal languages, G. Rozenberg and A. Salomaa, Eds. Springer-Verlag, Berlin, 1996. To appear. [44] PRUSINKIEWICZ,P., 
AND HANAN, J. L-systems: From formalism to programming languages. In Lindenmayer systems: Impacts on 
theoretical computer science, computer graphics, and developmental biology, G. Rozenberg and A. Salomaa, 
Eds. Springer-Verlag, Berlin, 1992, pp. 193 211. [45] PRUSINKIEWICZ,P., JAMES,M., AND M.Synthetic topiary. 
ECH, R. Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994), pages 351 358, ACM SIGGRAPH, 
New York, 1994. [46] PRUSINKIEWICZ,P., AND LINDENMAYER,A. The algorithmic beauty of plants. Springer-Verlag, 
New York, 1990. With J. S. Hanan, F. D. Fracchia, D. R. Fowler, M. J. M. de Boer, and L. Mercer. [47] 
ROOM, P. M. Falling apart as a lifestyle: the rhizome architecture and population growth of Salvinia 
molesta. Journal of Ecology 71 (1983), 349 365. [48] ROOM,P. M., MAILLETTE,L., AND HANAN, J. Module and 
metamer dynamics and virtual plants. Advances in Ecological Research 25 (1994), 105 157. [49] ROZENBERG, 
G. T0L systems and languages. Information and Control 23 (1973), 357 381. [50] SACHS,T., AND NOVOPLANSKY, 
A. Tree from: Architectural models do not suf.ce. Israel Journal of Plant Sciences 43 (1995), 203 212. 
[51] SIPPER, M. Studying arti.cial life using a simple, general cellular model. Arti.cial Life 2, 1 (1995), 
1 35. [52] TAKENAKA, A. A simulation model of tree architecture development based on growth response 
to local light environment. Journal of Plant Research 107 (1994), 321 330. [53] ULAM, S. On some mathematical 
properties connected with patterns of growth of .gures. In Proceedings of Symposia on Applied Mathematics 
(1962), vol. 14, American Mathematical Society, pp. 215 224. [54] WEBER,J., AND PENN, J. Creation and 
rendering of realistic trees. Proceedings of SIGGRAPH 95 (Los Angeles, California, August 6 11, 1995), 
pages 119 128, ACM SIGGRAPH, New York, 1995. [55] WYVILL,G., MCPHEETERS,C., AND WYVILL, B. Data structure 
for soft objects. The Visual Computer 2, 4 (February 1986), 227 234.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237280</article_id>
		<sort_key>411</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Flow and changes in appearance]]></title>
		<page_from>411</page_from>
		<page_to>420</page_to>
		<doi_number>10.1145/237170.237280</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237280</url>
		<keywords>
			<kw><![CDATA[light reflection models]]></kw>
			<kw><![CDATA[material models]]></kw>
			<kw><![CDATA[particle systems]]></kw>
			<kw><![CDATA[physically-inspired texturing]]></kw>
			<kw><![CDATA[weathering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Chemistry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010436</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Chemistry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P150906</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology, NE43-218, 545 Technology Square, Cambridge MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31089767</person_id>
				<author_profile_id><![CDATA[81332520351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[K&#248;hling]]></middle_name>
				<last_name><![CDATA[Pedersen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, NE43-218, 545 Technology Square, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, 370 Gates Computer Science Building 3B, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ACHESON, D. J. Elementary Fluid Dynamics. Oxford Univerity Press, New York, NY, 1992.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ADDLESON, L., AND RICE, C. Pelformance of Materials in Buildings. Butterworth Heinemann, Boston, MA, 1991.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>237278</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DORSEY, J., AND HANRAHAN, P. Modeling and rendering of metallic patinas. In Computer Graphics Proceedings (1996), Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DULLIEN, F. A. L. Porous Media: Fluid Transport and Pore Structure, second ed. Academic Press, New York, NY, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15894</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[FOURNIER, A., AND REEVES, W.T. A simple model of ocean waves. Computer Graphics 20, 4 (Aug. 1986), 75-84.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FRENCH, L. Toy story. Cinefantastique 27, 2 (1995), 36-37.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[JOHNSON, J. B., HANEEF, S. J., AND HEPBURN, B. J. Laboratory exposure systems to simulate atmospheric degradation of building stone under dry and wet deposition. Atmospheric Environment 24A, 10 (Oct 1990), 2785-2792.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97884</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[KASS, M., AND MILLER, G. Rapid, stable fluid dynamics for computer graphics. Computer Graphics 24, 4 (Aug. 1990), 49-57.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[LIPPERT, H. G. Systeme zur dachentwasserung bei gotischen kirchenbauten. Architecture: Zeitschrift fur Geschichte der Baukunst 24, 1 (1994), 111-128.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[MASO, J. C., Ed. Pore Structure and Moisture Characteristics. Chapman and Hall, New York, 1987.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192244</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[MILLER, G. Efficient algorithms for local and global accessibility shading. In Computer Graphics Proceedings (1994), Annual Conference Series, ACM SIGGRAPH, pp. 319-326.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[MILLER, G., AND PEARCE, A. Globular dynamics: A connected particle system for animating viscous fluids. Computers and Graphics 13, 3 (1989), 305-309.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[MOSTAFAVI, M., AND LEATHERBARROW, D. On Weathering: The Life of Buildings in Time. MIT Press, Cambridge, MA, 1993.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74337</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MUSGRAVE, F. K., KOLB, C. E., AND MACE, R. S. The synthesis and rendering of eroded fractal terrains. Computer Graphics 23 (July 1989), 41-50.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PAZ, 0. A Draft of Shadows and Other Poems. New Directions, New York, MY, 1979.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15893</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PEACHEY, D. R. Modeling waves and surf. Computer Graphics 20, 4 (Aug. 1986), 65-74.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357320</ref_obj_id>
				<ref_obj_pid>357318</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[REEVES, W. T. Particle systems - a technique for modeling a class of fuzzy objects. ACM Trans. Graphics 2 (Apr. 1983), 91-108.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325250</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[REEVES, W. T., AND BLAU, R. Approximate and probabilistic algorithms for shading and rendering structured particle systems. Computer Graphics 19, 4 (July 1985), 313-322.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SMALL, D. Simulating watercolor by modeling diffusion, pigment, and paper fibers. In Proceedings of SPIE '91 (Feb. 1991), pp. 70-76.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Generating textures for arbitrary surfaces using reaction-diffusion. Computer Graphics 25, 4 (July 1991), 289-298.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[WINKLER, E.M. Stone in Architecture: Properties and Durability. Springer-Verlag, New York, MY, 1994.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192227</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A. P., AND HECKBERT, P. S. Using particles to sample and control implicit surfaces. In Computer Graphics Proceedings (1994), Annual Conference Series, ACM SIG- GRAPH, pp. 269-278.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15895</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[YAEGER, L., UPSON, C., AND MYERS, R. Combining physical and visual simulation i creation of the planet Jupiter for the film "2010". Computer Graphics 20, 4 (Aug. 1986), 85- 93.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[YALIN, M. S. Mechanics of sediment transport, second ed. Oxford, New York, MY, 1977.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Flow and Changes in Appearance y Julie Dorsey * Hans Køhling PedersenPat Hanrahanz Massachusetts Institute 
of Technology Stanford University Stanford University Abstract An important, largely unexplored area 
of computer image genera­ tion is the simulation of weathering and its effects on appearance. Weathering 
results from the interaction of the environment with the materials in the world. The .ow of water is 
one of the most pervasive and important natural forces involved in the weathering of materials, producing 
a distinctive set of patterns of washes and stains. This paper presents an intuitive phenomenological 
model for the .ow of water over surfaces that is capable of generating such changes in appearance. We 
model the .ow as a particle system, each particle represent­ ing a drop of water. The motion of the 
water particles is controlled by parameters such as gravity, friction, wind, roughness, and con­ straints 
that force the particles to maintain contact with the surface. The chemical interaction of the water 
with the surface materials is governed by a set of coupled differential equations describing both the 
rate of absorption of water by the surface and the rate of solu­ bility and sedimentation of deposits 
on the surface. To illustrate the power of this simple model, we show examples of .ows over com­ plex 
geometries made from different materials; the resulting pat­ terns are striking and very dif.cult to 
achieve using traditional tex­ turing techniques. CR Categories and Subject Descriptors: I.3.7 [Computer 
Graph­ics]: Three-Dimensional Graphics and Realism I.3.6 [Computer Graphics]: Methodology and Techniques. 
Additional Key Words and Phrases: weathering, material mod­els, physically-inspired texturing, particle 
systems, light re.ection models.  1 Introduction Over time, the natural environment acts upon materials 
and changes their appearance. These processes and changes are termed weath­ ering. Through weathering, 
objects become tarnished, bleached, stained, eroded, and otherwise modi.ed. Weathering is unavoidable 
and therefore must be simulated to make realistic pictures of natural environments. For many years, computer 
graphics researchers and practitioners have been grappling with the problem of creating surfaces that 
have *NE43-213, 545 Technology Square, Cambridge, MA 02139. http://graphics.lcs.mit.edu/.dorsey yCurrent 
address: NE43-218, 545 Technology Square, Cambridge, MA 02139. http://graphics.lcs.mit.edu/.hkp z370 
Gates Computer Science Building 3B, Stanford, CA 94305-4070. http://www-graphics.stanford.edu/.hanrahan 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 a worn appearance. In the production of Pixar s recent computer­generated 
.lm Toy Story, for example, weathering effects such as scuffs and dirt were added to the surfaces by 
painting textures and compositing them within a programmable shader [6]. However, this process is very 
time consuming. Current tools for texturing geomet­ric models made from many surfaces are primitive; 
for example, it is very dif.cult to match textures across surface boundaries or account properly for 
area distortions caused by the parameterization. More fundamentally, the texture mapping approach does 
not take into ac­count the structure or properties of a given material or the processes in the surrounding 
environment that account for weathering. Our long range goal is to simulate the effect of the environment 
on com­plex material surfaces so that their appearance may be controlled semi-automatically. The focus 
of this paper is the modeling of changes in appearance caused by the .ow of water over complex surfaces. 
Figure 1 con­tains several photographs we have collected showing the weathering of various buildings. 
Inspection of these photographs reveals that many of the complex patterns on the surfaces are due to 
the .ow of water. Water may wash dirt from some areas and clean them; in other areas dirt and other materials 
are deposited, creating stains. The patterns produced by this process are visually rich and very dif­.cult 
to model with current texturing techniques. Exactly how water travels over surfaces is very complicated 
and is dependent on a large number of variables. Unfortunately, the workings of complex .ows are still 
not fully understood based on .rst principles [1]. However, for the purposes of this study, we are concerned 
with the patterns created by the .ow of water on surfaces, not the detailed appearance of the .ow itself. 
We therefore present a phenomenological model, based on particle systems and rate equa­tions governing 
the absorption of water by the surface and the sed­imentation of deposits. This model is simple and robust, 
and it is capable of generating a wide range of .ow effects. 1.1 Related Work Relevant previous work 
can be conveniently divided into two major categories: techniques for simulating .uid .ow and particle 
systems. In this section, we discuss our approach in light of these categories. Flow models. Kass and 
Miller presented an approach for an­imating water using a set of partial differential equations [8]. 
They used a wetness map to simulate the wetting and drying of sand as water passes over it. Miller and 
Pearce described a model for animating viscous .uids by simulating the forces of particles interacting 
with one another [12]. The focus of both of these works was on the realistic appearance of the .ow itself, 
both in terms of its motion and rendering. In addition, Musgrave et al. proposed a simple, heuristic 
erosion model that simulates hydraulic and thermal erosion processes on fractal terrains [14]. Particle 
systems. Particle systems represent an effective ap­proach to modeling objects whose behavior over time 
cannot be well-represented in terms of the surface of the objects [17, 18]. In such a system, a collection 
of particles evolves over time. The  (b) (a)  (c) (d) (e) (f) (g) (h) Figure 1. A collection of 
representative flow effects.     evolution is determined by applying certain rules to the particles. 
Particle motion may follow either deterministic or stochastic laws of motion. Particle systems have been 
used to model trees, grass, fog, waterfalls, and .re [5, 16, 23]. In a related work, Small described 
a parallel approach to the prob­lem of modeling watercolor by predicting the actions of pigment and water 
when applied to paper .bers [19]. This model was cast as a complex cellular automata on the Connection 
Machine II. Small s model produces nice results, but it is limited to two dimensions and requires excessive 
processing power. The speci.c technical contribution of this paper is to combine particle systems with 
processes similar to those used to model water colors. This allows us to simulate the effects of material 
transport on complex surfaces. 1.2 Overview The remainder of this paper is structured as follows. We 
.rst in­troduce the basic mechanisms of .ow and staining and give exam­ples of typical effects that inspired 
our work. We then describe our .ow model in detail, both the dynamics of particles and the coupled differential 
equations controlling absorption and sedimentation. Fi­nally, we illustrate the model with examples of 
.ows over complex geometries: a building facade, two statues, and a portion of a Gothic cathedral.  
 2 Background Figure 1 features photographs of real scenes that show many effects of the .ow of water 
over surfaces. In this paper, we will demonstrate that a number of these effects can be captured with 
a simple under­lying model. In this section we describe the motivation behind the development of our 
model with a discussion of the mechanisms of .ow and staining including a review of the major factors 
involved. We then discuss a series of speci.c representative effects that are associated with this process. 
2.1 Exposure and Runoff The basic factors affecting the rate of .ow over a surface are the quantities 
of incident water, the height, inclination and geometry of the surface, and the absorption of water by 
the material comprising the surface [1, 4]. One of the major features of the pattern of .ows on a building 
is the arrangement of water sources. From these sources, water .ows downward under the in.uence of gravity. 
This is often termed pri­mary .ow. This depends on which parts of the structure receive the most incident 
water and the effect of geometry on directing the .ow. Typically the .ow separates into streams, and, 
in a way similar to rivers, produces patterns that often become self-reinforcing. Fig­ure 1a shows a 
typical example of .ows on a wall of moderate expo­sure. In particular, the window sills and lintels 
above the windows serve to concentrate the .ow on both sides of the windows. The absorption of water 
by the surface is controlled by the ab­sorptivity, or rate of water uptake, and the absorption, or capac­ity 
to absorb water. Runoff occurs when the surface is fully satu­rated (i.e. has no more capacity to absorb 
water) or non-absorbent (i.e. has a low absorptivity). Figure 2 shows the relationship be­tween incident 
rain, runoff, and absorption for several materials [2]. Curve A represents a spongy material with high 
absorptivity and ab­sorption. This material has the capacity to absorb all the incident water over time, 
hence there is no runoff. Curve B describes a mod­erately absorbent concrete with high absorptivity but 
a limited ab­sorption capacity. Initially, from t0to t2, all incident water is ab­sorbed and there is 
no runoff. However, from t2to t3, because of Fraction absorbed A 100% B A: sponge tissue. B: concrete. 
C: concrete, higher absorptivity. D: window glass t0t1t2 t3 Time Figure 2: Simpli.ed diagram showing 
absorption over time for sev­eral materials assuming a constant rain. saturation, the amount absorbed 
is less than the incident amount; hence, some rain is absorbed but an increasing quantity runs off. In 
Curve C, which gives the behavior of concrete with a higher ab­sorptivity, runoff starts sooner, because 
the material becomes sat­urated more quickly. Finally in Curve D, which describes a very non-absorbent 
material such as glass, the amount of water that is ab­sorbed is very small and drops off slowly during 
the time span; thus there is signi.cant runoff. The exposed parts of a structure become saturated .rst, 
so the .ow starts there and proceeds onto dryer areas below. These lower surfaces absorb a proportion 
of the water until they too have a rate of incidence that exceeds the rate of absorption. Splashback 
is a phenomenon that occurs where a wall meets the ground. Here, water hits the ground adjacent to the 
building and causes dirt from the ground to be propelled up and deposited a short distance up the wall. 
This effect also occurs when water hits hori­zontal ledges higher up on a building. Figure 1b shows an 
excellent example of splashback at the base of a building. 2.2 Staining The washing and staining of 
surfaces are strongly in.uenced by ex­ternal, directional sources of dirt. These sources include a variety 
of airborne pollutants, such as exhaust from traf.c or smoke emissions from industrial plants, loose 
material on the ground carried against the base of the building by splashback, or dirt of a biological 
source ranging from bird droppings to plant growth. saturation. staining (a) Even deposit. (b) Water 
from. (c) ...redepositing. . of dirt . above picks. . it at the limit. . up some dirt... . of flow Figure 
3: The basic mechanisms of staining. In Figure 1h, the pattern of dirt on the spindles in the balustrade 
is due to a directional dirt source. The effect of an external source can also be observed in Figure 
1a. The right side of the building curves away from the street, which leads to a reduction in the exposure 
to dirt caused by street traf.c. Without the effects of rainfall, dirt would be distributed fairly evenly 
over vertical surfaces, with higher concentrations on hori­zontal regions such as window sills. Although 
large quantities of dirt are dis.guring (as is the case with buildings completely black­ened by soot), 
generally an even distribution of dirt due to exposure is not considered staining. However, a different 
picture emerges when water .ows over a surface. Dirt is picked up by water move­ment and redeposited 
elsewhere, so that its distribution becomes un­even. The areas that are exposed to a rapid .ow of water 
may be washed clean, whereas those where the dirt is redeposited have a greater accumulation. This basic 
mechanism is illustrated in Fig­ure 3. An interesting effect occurs at the locus of points on the surface 
where the runoff stops; this interface is critical with respect to stain­ing, since this is where any 
dirt picked up by the runoff water is redeposited [10]. A very similar stain occurs when water evapo­rates, 
leaving behind any soluble material. Another interesting ef­fect occurs when a non-absorbent surface 
is adjacent to a porous sur­face. In this case, the staining is limited to the porous material. In Figure 
1g, the top portion of the bridge is made of a non-absorbent metal, which causes a large portion of the 
incident water to run onto the porous stone work below, causing signi.cant staining. Another important 
effect in staining is differential .ow, which oc­curs when water running over one material dissolves 
small quanti­ties of the material and deposits them as stains elsewhere. A com­mon example, known as 
the spilt-milk effect, can be readily ob­served in places where runoff from concrete leads to white streaking 
on brickwork below. Other examples include green stains generated by runoff from copper [3] or rust stains 
from other corroded metals such as iron. If there are several materials present, even more com­plex patterns 
of stains may occur depending on the relative solubil­ities of the different materials. Figures 1c and 
1d depict examples of differential .ow. In Figure 1c the circled region shows the spilt­milk effect caused 
by the deposits from a limestone window sill. In Figure 1d the circled region shows an example where 
rust from an iron chain has been washed onto the street below. In areas that are exposed to water for 
long periods of time, sat­uration staining often occurs. Different materials have a different color response 
to saturation; most porous materials become darker due to a decrease in the average scattering angle 
caused by the inter­action of the water and the substrate. Figure 1f illustrates saturation staining. 
Here, water collects on the ridge at the base of the build­ing.  3 Flow Model In this section, we describe 
a model that qualitatively captures many of the .ow effects described in the previous section. The model 
has three basic inputs: surface geometry to create structures, materials for the structures and loose 
deposits, and the environment. Our .ow simulator is based on particle systems and rate equations. Particles 
are used to model water droplets both on the surface and in the air. The environment description speci.es 
the initial distribution of water droplets. Equations of motion describe the movement of each water particle 
over the surface accounting for gravity, friction, contact forces, and the in.uence of obstacles. The 
quantity of wa­ter in a particle decreases through absorption into the base material of the surface or 
evaporation. Water droplets may dissolve mate­rial, carry it to a different location, and subsequently 
deposit it there. This model is simple conceptually, is easy to implement, and only requires modest computation. 
Surface geometry is represented as a collection of parametric patches. The current system supports two 
types of patches: poly­gons and cubic spline patches. The geometric information is aug­mented with topological 
information that describes adjacency rela­tionships between all the patches; this is essential because 
the .ow must be continuous across a patch boundary. Each parametric sur­face also has a set of two-dimensional 
texture coordinates which is used to index a set of texture maps attached to each surface. For ex­ample, 
the amount of water absorbed by the surface is stored in a saturation map. In addition to water, the 
system also models other materials. Each patch represents a surface of a solid object made from a base 
mate­rial, which is coated with a mixture of loose deposits. The concen­tration of each loose deposit 
is stored in a texture map attached to the surface. Each type of material has an associated set of render­ing 
properties, e.g. diffuse and specular colors, shininess, and a set of physical properties, e.g. roughness, 
absorptivity, and other rate constants. These properties are summarized in Table 1. Material Properties 
Properties Notation Material Diffuse color Specular color Shininess Roughness Absorption Absorptivity 
Cd Cs s r a ka Deposits Diffuse color Adhesion rate constant Solubility rate constant Cd kS kD Table 
1: Attributes of the two major classes of materials: base mate­rials and loose deposits. Rate constants 
(properties beginning with k) are used in the differential equations controlling absorption of water 
and sedimentation of loose deposits (see Table 3). 3.1 Water Particle Model In our model water is represented 
as a collection of water particles. The attributes of each water particle are shown in Table 2; they 
in­clude the mass or volume of the particle, as well as positional at­tributes. Particle systems have 
been used widely in computer graph­ics and the techniques we use for modeling their motion are well de­scribed 
in the literature [20, 22]. Water Particle Properties Attribute Notation Mass Position Velocity Soluble 
material i m x v Si Table 2: Particle attributes. Water particles are created on the geometric model 
according to a distribution function for incident rain. This function depends on exposure to the prevailing 
rain direction. These distribution func­tions will be discussed in more detail in Section 3.4. The .ow 
of water particles along the surface depends on a set of forces: grav­ity, friction, self-repulsion, 
and diffusion. Gravity and friction cause the particles to .ow downward; self-repulsion prevents the 
particles from clumping and causes the motion to be more .uid-like (see Fig­ure 4). Normally the .ow 
of particles is constrained to lie on the sur­face; this is done by projecting the resulting force vector 
onto the Figure 5: The effects on the .ow due to surface roughness. The three .gures are generated with 
different surface roughnesses: the surface on the left is smoothest; the surface on the right is roughest. 
 In order to create more interesting .ows on surfaces that are ge­ometrically smooth, we have experimented 
with two simple rough surface models. In the .rst model, a scalar roughness controls a dif­fusion process. 
To simulate diffusion, each particle is subjected to a random force in the tangent plane to the surface; 
the magnitude of the displacement force is proportional to the roughness parameter. Figure 5 shows the 
effect of roughness on the .ow. As can be seen, increasing the roughness causes the particles to disperse, 
whereas decreasing the roughness causes the .ow to be streaky. In the sec­ond model, a displacement map 
is added to a surface. The displace­ment map is used to perturb the surface normal which in turn de.nes 
a perturbed tangent plane. When a displacement map is present, the resulting force is .rst projected 
onto the true tangent plane and then reprojected onto the perturbed tangent plane. This simple technique 
causes the particles to conform to the displacement map .owing more slowly across a bumpy surface and 
hence collecting in cavities and cracks, and streaming along cracks and valleys. To model the effects 
of secondary .ow, particles are allowed to fall off a surface. A particle leaves the surface if the angle 
between its velocity vector and the surface tangent exceeds a prespeci.ed critical angle. When a particle 
loses contact with the surface, it falls vertically under the in.uence of gravity until it hits another 
surface. Since computing such intersections can be computationally inten­sive, we pre-compute a table 
of positions where particles will land when they fall off the surface. This is an important feature, 
as our models consist of large numbers of patches. Together, these steps form a model capable of reproducing 
a suf­.ciently wide variety of effects to generate interesting weathered appearances. Complex .ow patterns 
arise naturally by constrain­ing particles to remain on the surface, thereby forcing the parti­cles to 
conform to the geometry. Collisions naturally divert parti­cles around obstacles and allow for non-local 
interactions between different parts of the model. Roughness and displacement maps change the look of 
.ows on different surfaces. A snapshot of parti­cles .owing over a complex surface is shown in Figure 
4. A  Figure 6: Absorption and deposition model. 3.2 Absorption Model The absorption of water by a 
surface depends largely on the proper­ties of void space, or pores, in the material. However, for practical 
purposes the following model is often adopted [2]: Three param­eters control the absorption: absorption, 
absorptivity,and satura­tion. Absorption is the maximum amount of water that the surface may hold, whereas 
absorptivity is the rate that the surface absorbs water. Saturation is the ratio of the actual water 
absorbed to the ca­pacity of the surface. The amount of water absorbed depends on the absorptivity and 
the duration of exposure, but it is limited by the ab­sorption. This effect can be modeled by adjusting 
the absorptivity as a function of the saturation. The absorption process is shown diagrammatically in 
Figure 6, and the equations that govern the process are contained in Table 3. As a water particle moves 
across a porous surface, its mass will de­crease due to absorption and evaporation. At each time step, 
after the position of the particle is updated, its mass is updated by numer­ically intergrating a differential 
equation controlling absorption and evaporation. When the mass of the particle falls below some thresh­old, 
it dies and is removed from the simulation. Absorption and Deposition Process Absorption @m @t .ka a=w 
a A m @w @t ka a=w a m A .Iwsun Sedimentation @Si @t .kSi Si+kDi Di m A @Di @t kSi Si A m .kDi Di+i ID 
 Table 3: Sedimentation equations. The top two equations control the absorption of water by the surface; 
the bottom two equations control the sedimentation of loose deposits. In this last set of equa­tions, 
the subscript iis used to signify different types of deposits. Si is the concentration of dissolved material 
in a water particle, and Di is the concentration of material deposited on the surface. All other parameters 
and functions are described in Tables 1 and 2. Figure 7: The effects on the .ow due to absorption of 
a porous sur­face. The two .gures show the amount of absorbed water on two surfaces with different absorptions. 
Because the surface on the left has a lower absorption than the one on the right, it becomes saturated 
sooner and the water streak is longer. In contrast, the surface on the left absorbs more water limiting 
the length of the streak. Since more water is inside the streak on the right, the streak is brighter. 
Figure 7 illustrates the .ow pattern from a pipe across a surface with two different absorptions. To 
make these images, the quantity of water absorbed at each point by the surface is stored in a satura­tion 
map. This saturation map may be used by the rendering sys­tem to modulate the appearance of the surface. 
Unlike this .gure, a saturation stain makes the surface appear darker; this point will be discussed in 
Section 3.5. 3.3 Deposition Model The last main category of surface attributes controls transport and 
deposition of various substances such as dirt. In our model, these sedimentation characteristics are 
captured by two parameters: solu­bility, describing the rate at which water picks up surface deposits, 
and adhesion, specifying the rate of redeposition from water parti­cles on to the surface. These parameters 
may vary for different ma­terials. The sedimentation process is shown diagrammatically in Fig­ure 6, 
and the equations governing the process are shown in Table 3. These coupled differential equations depend 
on the relative concen­trations of the materials in the water particle and on the surface. In the case 
of a water particle, the concentration is de.ned to be ma­terial per unit mass; in the case of the surface, 
the concentration is the material per unit area. Note that as a water particle s mass de­creases due 
to evaporation, the concentration of dissolved materials decreases. Thus, the rate of deposition naturally 
increases during evaporation, causing the dissolved material to be deposited on the underlying surface. 
The relative solubility and adhesion of deposits on a surface play a major role in the generation of 
stains. When water dissolves ma­terial from a surface, it has the effect of washing the surface. When 
water deposits material, it has the effect of staining the surface. The combination of these two effects, 
when coupled with the .ow, can generate complex stain patterns. Even more complex patterns arise if multiple 
materials with different solubilities are present. More ex­amples of this will be shown in the results. 
 3.4 Environment Model Environment attributes Attribute Notation Rain Sunlight Deposits Iwrain Iwsun 
IDi Table 4: Environment The external factors affecting the .ow are shown in Table 4. In this section 
we describe the models we used to generate the images in this paper. Although it should be emphasized 
that it is very easy to add more complex environmental models to the system. As described in Section 
3.1, the rate and direction of incident rain seeds the particle .ow process. We model this by creating 
rain sources that emit rain drops. In our model, the direction of rain is controlled by the direction 
of wind. This in turn is given by ran­domly perturbing the prevailing wind direction. These drops are 
traced until they intersect the model and are deposited in an expo­sure map. For ef.ciency reasons we 
precompute these exposure maps. During the .ow simulation the exposure determines the prob­ability that 
a water particle will be created at different points on the surface; however, once a particle is created, 
water sources have no effect on the particle s mass. The rate of evaporation is very sensitive to the 
orientation of the surface and whether it is shadowed. This effect may be modeled by computing the total 
solar irradiance, both due to the sky and the sun itself. In some of the experiments performed in this 
paper, the evap­oration rate is set to be constant. Finally, the initial distribution of deposits of 
various types may also be controlled with directional sources, or prestored in a texture map. 3.5 Rendering 
To create .nal renderings, we use simple methods to approximate the diffuse color of the loose deposits 
and to account for wet sur­faces and saturation stains. The color of the deposit layer is com­puted simply 
by summing the color of each deposit, weighted by the concentration of that deposit from the appropriate 
texture map. An alpha value is computed using a similar technique, and this is used to composite the 
deposit color over the base material. The color of a wet surface is modi.ed to make it look darker. To 
approximate this effect, we simply modulate the diffuse re.ectivity depending upon the saturation of 
the surface.  4 Results In this section we show results for four complex models: a building facade, 
two statues, and a section of a cathedral.  Figure 8: Building facade. Rendering without .ow patterns 
(left). Rendering with .ow patterns (right). 4.1 Building Model Figure 8 shows side-by-side renderings 
of the building, without (on the left) and with (on the right) the .ow effects. Figure 9 shows the changes 
in appearance due to the .ows in more detail. The building was modeled using AutoCAD and consists of 
approximately 450 polygons. All the .ow effects in this series were created in approxi­mately three hours 
on a Silicon Graphics Reality Engine2 with a 250 Mhz R4400 processor. In the center section of the building 
alongside the main window, there are several examples of primary .ows that continue until the water reaches 
the stone work comprising the base of the building. The .ow has the effect of washing dirt and soot from 
the brick and depositing it on the lower parts of the building. Note the difference between the patterns 
generated by the .ows on the yellow brick and grey stone below. This is caused by the greater roughness 
of the brick surface relative to the smoother stone. On the stone at the base of the building, there 
are subtle splashback effects making the lower part even dirtier. Notice the underside of the vertical 
panels that curve outward to the left of the yellow brick section: here there is evidence of saturation 
staining. There is also staining due to par­tial .ows on the sign below the lamp, in this case simulating 
a rust stain where the lamp meets the building. Similar staining and accu­mulation of a patina [3] are 
shown on the copper rain pipe on the left side of the building. Finally, on the sides of the upper window 
there is a pattern due to the differential .ow of the copper patina. To compute these images, the simulation 
results are stored in standard texture maps, and these are input into the rendering sys­tem. Each surface 
has nine texture maps, thus the .nal rendering uses over 4,000 texture maps (although a fair number of 
these are very small). In our system, we combine the texture maps using a shading language similar to 
the one used in RenderMan. In these images, displacement maps are used to vary the height of the sur­face 
of bricks and stones; ray tracing is used so that the displacement maps self-shadow the surface, which 
adds to the realism of the pic­tures.  4.2 Venus de Milo Figure 10 shows the development of washing 
and staining patterns due to .ows over a statue of Venus de Milo, a classic work of art. The model was 
created from a Cyberware scan and consists of approx­imately 260,000 small, evenly sized triangles. This 
data was then used to create 63 bicubic patches, which are input to the .ow simu­lation system. All the 
.ow effects in this sequence were created in a session of approximately twenty minutes in length. The 
left image depicts the original, white marble statue prior to the .ow simulation. It is pure white, with 
no imperfections, and is rendered with only diffuse re.ection and a single light source. The right image 
shows the results of applying a uniform coating of reddish brown dirt across the statue, followed by 
a .ow simu­lation to wash the surface. There are noticeable streaks in the dirt patterns due to the .ow, 
and a randomness due to the individual na­ture of each particle. Dirt accumulates in various parts of 
the statue where the surface is protected from the path of the .ow, such as un­der the left arm. The 
dirt texture conforms to the folds in the fab­  Figure 9: Closeups of .ow patterns on the building facade. 
ric below. For example, the upper surfaces of the convex parts of the folds are clean; the lower surfaces 
are dirty. The pattern is more uniform on the base of the statue and areas closest to the ground, since 
less water reached that part of the statue. It is not possible to achieve these effects with simple accessibility 
[11]. The image was rendered using a diffuse model for the marble, and the dirt layer was composited 
using the same technique as for the building.  4.3 Gargoyle Figure 11 shows a scanned model of a gargoyle 
before and after the application of a simple .ow pattern. The model was created from a Cyberware scan 
and consists of approximately 310,000 triangles. This data was then used to create 30 bicubic patches, 
which were input to the .ow simulation system. The re.ection function of the metallic surface was modi.ed 
to make the surface appear as if it were covered by a thin layer of soot that was partially washed away. 
 4.4 Cathedral Figure 12 shows a portion of a Gothic cathedral, which was simu­lated in our system. The 
cathedral and its statues and gargoyles were modeled using approximately 100,000 polygons which then 
had dis­placement maps applied to them, leading to a geometric complexity of over 6,000,000 polygons. 
The gutters and drainage system on the cathedral were carefully designed to be similar to those on actual 
cathedrals: water .ows down the upper roof to a gutter below the railing which directs the water down 
the main columns, and down along the top of the .ying buttresses [9]. Water exits the system be­low the 
statues of the saints. The gutter above the middle section of the facade was blocked, so that the gutter 
backed up and the particles spilled over the front of the building. As a result, there was much less 
.ow beneath the statue of the saint on the left than the statue on the right. All .ow simulations were 
performed using the displace­ment maps, so that the .ow conformed to the actual displaced ge­ometry; 
this is evident in the cracks between the stones.  5 Summary and Discussion We have described a system 
for simulating the .ow of water over complex surfaces. The .ow conforms to the geometry of the shapes, 
and the water interacts with the surface materials. Speci.cally, the system is able to simulate the absorption 
of water by the material and the transport of deposits by dissolving and carrying surface material, and 
later redepositing it. As a result, a wide range of .ow effects may be simulated, yielding complex patterns 
showing washing and staining. The system is simple, robust, and practical. By using particle sys­tems 
a wide range of phenomena are easily programmed. We be­lieve the system can be extended to include additional 
factors, such as windblown dust and biological growth. Also, by embedding the .ow model in an interactive 
system, the user can control the .ows to produce the desired images, which is required if a physical 
model is to be used for artistic purposes. Finally, the methods we use to de­scribe textures on complex 
surfaces are fairly general; they could be used as a basis for the creation and design of other complex 
patterns on these surfaces.  Water .ows are a major cause of the weathering of outdoor struc­tures and 
objects and must be simulated to create convincing pic­tures of such environments. The key to modeling 
weathering is to simulate the effects of the environment on the materials. Although our pictures have 
many effects new to computer graphics, it takes only a few minutes studying and comparing the real to 
the virtual examples to realize that there is still much research to be done. This is a challenging new 
direction for computer graphics. Acknowledgements Thanks to Jeff Feldgoise for modeling the cathedral 
and building facade, Matt Pharr for building much of the rendering system and for help with the .nal 
renderings, Craig Kolb for help assembling and printing the .nal images, Tamara Munzner for video assistance, 
and the anonymous reviewers for their suggestions. Thanks also to Brian Curless, Venkat Krishnamurthy 
and Marc Levoy for pro­viding access to the Cyberware scanner and their software for cre­ating complex 
models; their system was used to scan the statue of the Venus and the Angels in the cathedral niches. 
This research was supported by research grants from the National Science Foundation (CCR-9207966 and 
CCR-9624172) and the MIT Cabot and NEC Research Funds, and by equipment grants from Apple and Silicon 
Graphics Inc. References [1] ACHESON,D. J. Elementary Fluid Dynamics. Oxford Uni­verity Press, New York, 
NY, 1992. [2] ADDLESON,L., AND RICE,C. Performance of Materials in Buildings. Butterworth Heinemann, 
Boston, MA, 1991. [3] DORSEY,J., ANDHANRAHAN,P.Modelingandrenderingof metallic patinas. In Computer Graphics 
Proceedings (1996), Annual Conference Series, ACM SIGGRAPH. [4] DULLIEN, F.A.L. Porous Media: Fluid Transport 
and Pore Structure, second ed. Academic Press, New York, NY, 1992. [5] FOURNIER,A., AND REEVES, W. T. 
A simple model of ocean waves. Computer Graphics 20, 4 (Aug. 1986), 75 84. [6] FRENCH, L. Toy story. 
Cinefantastique 27, 2 (1995), 36 37. [7] JOHNSON,J.B., HANEEF,S. J., AND HEPBURN,B. J. Lab­oratory exposure 
systems to simulate atmospheric degradation of building stone under dry and wet deposition. Atmospheric 
Environment 24A, 10 (Oct 1990), 2785 2792. [8] KASS,M., AND MILLER, G. Rapid, stable .uid dynamics for 
computer graphics. Computer Graphics 24, 4 (Aug. 1990), 49 57. [9] LIPPERT, H. G. Systeme zur dachentwasserung 
bei gotischen kirchenbauten. Architecture: Zeitschrift fur Geschichte der Baukunst 24, 1 (1994), 111 
128.  [10] MASO,J.C., Ed. Pore Structure and Moisture Characteris­tics. Chapman and Hall, New York, 
1987. [11] MILLER, G. Ef.cient algorithms for local and global acces­sibility shading. In Computer Graphics 
Proceedings (1994), Annual Conference Series, ACM SIGGRAPH, pp. 319 326. [12] MILLER,G., AND PEARCE, 
A. Globular dynamics: A con­nected particle system for animating viscous .uids. Comput­ers and Graphics 
13, 3 (1989), 305 309. [13] MOSTAFAVI,M., AND LEATHERBARROW,D. On Weather­ing: The Life of Buildings 
in Time. MIT Press, Cambridge, MA, 1993. [14] MUSGRAVE,F. K., KOLB,C.E., AND MACE,R.S. The synthesis 
and rendering of eroded fractal terrains. Computer Graphics 23 (July 1989), 41 50. [15] PAZ,O. A Draft 
of Shadows and Other Poems. New Direc­tions, New York, NY, 1979. [16] PEACHEY,D.R.Modelingwavesandsurf. 
Computer Graph­ics 20, 4 (Aug. 1986), 65 74. [17] REEVES, W. T. Particle systems a technique for modeling 
a class of fuzzy objects. ACM Trans. Graphics 2 (Apr. 1983), 91 108. Figure 12: Simulated .ows on a Gothic 
cathedral. [18] REEVES,W. T., AND BLAU, R. Approximate and proba­bilistic algorithms for shading and 
rendering structured parti­cle systems. Computer Graphics 19, 4 (July 1985), 313 322. [19] SMALL, D. 
Simulating watercolor by modeling diffusion, pigment, and paper .bers. In Proceedings of SPIE 91 (Feb. 
1991), pp. 70 76. [20] TURK, G. Generating textures for arbitrary surfaces using reaction-diffusion. 
Computer Graphics 25, 4 (July 1991), 289 298. [21] WINKLER,E.M. Stone in Architecture: Properties and 
Durability. Springer-Verlag, New York, NY, 1994. [22] WITKIN,A. P., AND HECKBERT, P. S. Using particles 
to sample and control implicit surfaces. In Computer Graphics Proceedings (1994), Annual Conference Series, 
ACM SIG-GRAPH, pp. 269 278. [23] YAEGER,L., UPSON,C., AND MYERS, R. Combining phys­ical and visual simulation 
 creation of the planet Jupiter for the .lm 2010 .Computer Graphics 20, 4 (Aug. 1986), 85 93. [24] YALIN,M. 
S. Mechanics of sediment transport, second ed. Oxford, New York, NY, 1977.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237281</article_id>
		<sort_key>421</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[Simulating facial surgery using finite element models]]></title>
		<page_from>421</page_from>
		<page_to>428</page_to>
		<doi_number>10.1145/237170.237281</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237281</url>
		<keywords>
			<kw><![CDATA[data reconstruction]]></kw>
			<kw><![CDATA[facial modeling]]></kw>
			<kw><![CDATA[facial surgery simulation]]></kw>
			<kw><![CDATA[finite element method]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.8</cat_node>
				<descriptor>Finite element methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Pixel classification</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003718</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations in finite fields</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P248404</person_id>
				<author_profile_id><![CDATA[81332509242]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rolf]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Koch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swiss Federal Institute of Technology, Institute for Information Systems, Department of Computer Science, ETH Zentrum, CH-8092 Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40035544</person_id>
				<author_profile_id><![CDATA[81100260276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swiss Federal Institute of Technology, Institute for Information Systems, Department of Computer Science, ETH Zentrum, CH-8092 Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P87098</person_id>
				<author_profile_id><![CDATA[81100611730]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Friedrich]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Carls]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dept. of Maxillofacial Surgery, University Hospital, Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P59566</person_id>
				<author_profile_id><![CDATA[81100371169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[von B&#252;ren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swiss Federal Institute of Technology, Institute for Information Systems, Department of Computer Science, ETH Zentrum, CH-8092 Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P95442</person_id>
				<author_profile_id><![CDATA[81100351718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fankhauser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swiss Federal Institute of Technology, Institute for Information Systems, Department of Computer Science, ETH Zentrum, CH-8092 Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P305997</person_id>
				<author_profile_id><![CDATA[81100124860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yoav]]></first_name>
				<middle_name><![CDATA[I. H.]]></middle_name>
				<last_name><![CDATA[Parish]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swiss Federal Institute of Technology, Institute for Information Systems, Department of Computer Science, ETH Zentrum, CH-8092 Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>11275</ref_obj_id>
				<ref_obj_pid>11274</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CANNY, J. F. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 8, 6 (1986), pp. 679-697.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122746</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CELNIKER, G., AND GOSSARD, D. Deformable curve and surface finite elements for free-form shape design. In Compuwr Graphics (SIGGRAPH '91 Proceedings) (July 1991), ACM SIGGRAPH, T. W. Sederberg, Ed., vol. 25, pp. 257- 266.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378484</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DREBIN, R. A., CARPENTER, L., AND HANRAHAN, P. Volume rendering. In Computer Graphics (SIGGRAPH '88 Proceedings) (Aug. 1988), ACM SIGGRAPH, J. Dill, Ed., vol. 22, pp. 65-74.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[FARKAS, L. G. Anth~vpometry of the Head and Face, second ed. Raven Press, 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>833866</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[GROSS, M. U., GATTI, e., AND STAADT, O. Fast multiresolution surface meshing. In Proceedings oflEEE Visualization '95 (1995), IEEE Computer Society Press, pp. 135-142.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614294</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[GROSS, M. U., AND KOCH, e. Visualization of multidimensional shape and texture features in laser range data using complex-valued Gabor wavelets. IEEE Transactions on Visualization and Computer Graphics 1, 1 (Mar. 1995), pp. 44-59. ISSN 1077-2626.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND KRUEGER, W. Reflection from layered surfaces due to subsurface scattering. In Computer Graphics (SIGGRAPH '93 Proceedings) (Aug. 1993), ACM SIC- GRAPH, J. T. Kajiya, Ed., vol. 27, pp. 165-174.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>573190</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[HARALICK, R. M., AND SHAPIRO, L. G. Computer and Robot Vision, vol. 1-3. Addison-Wesley, 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[KUNDERT, K. S., AND SANGIOVANNI-VINCENTELLI, A. A Sparse Linear Equation Solver. Dept. of Electrical Engineering and Computer Sciences, University of California, Berkeley, 1988.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[LEE, Y., TERZOPOULOS, D., AND WATERS, K. Realistic face modeling for animation. In Computer Graphics (SIC- GRAPH '95 Proceedings) (Aug. 1995), ACM SIGGRAPH, R. Cook, Ed., vol. 29, pp. 55-62.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[NATIONAL LIBRARY OF MEDICINE. The Visible Human P~vject. http://www.nlm.nih.gov/extramural_research.dir/visible_human.html, 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[PARKE, F. I. Parameterized models for facial animation. IEEE Computer Graphics and Applications 2 (Nov. 1982), pp. 61-68.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[SCHROEDER, W. J., ZARGE, J. A., AND LORENSEN, W. E. Decimation of triangle meshes. In Computer Graphics (SIC- GRAPH '92 Proceedings) (Aug.1992), ACM SIGGRAPH, E. E. Catmull, Ed., pp. 65-70.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>902680</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[VAN GELDER, A., AND WILHELMS, J. Topological considerations in isosurface generation. Tech. Rep. UCSC-CRL-94- 31, Baskin Center for Computer Engineering and Information Sciences, University of California, Santa Cruz, 1994.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617709</ref_obj_id>
				<ref_obj_pid>616020</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[VANNIER, M. W., PILGRAM, T., BHATIA, G., BRUNSDEN, B., AND COMMEAN, P. Facial surface scanner. IEEE Computer Graphics and Applications 11, 6 (Nov. 1991), pp. 72- 80.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37405</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[WATERS, K. A muscle model for animating three-dimensional facial expression. In Computer Graphics (SIGGRAPH '87 Proceedings) (July 1987), ACM SIGGRAPH, M. C. Stone, Ed., vol. 21, pp. 17-24.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[ZIENKIEWICZ, O. C. The Finite Element Method, fourth ed., vol. 1-2. McGraw-Hill, 1994.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simulating Facial Surgery Using Finite Element Models R. M. Koch:,M. H. Gross:, F.R.Carlsy, D. F. von 
B¨uren:, G. Fankhauser:, Y.I. H.Parish: :Swiss Federal Institute of Technology (ETH), Z¨urich yDept. 
of Maxillofacial Surgery, University Hospital, Z¨urich ABSTRACT This paper describes a prototype system 
for surgical planning and prediction of human facial shape after craniofacial and maxillofa­ cial surgery 
for patients with facial deformities. For this purpose it combines, uni.es, and extends various methods 
from geometric modeling, .nite element analysis, and image processing to render highly realistic 3D images 
of the post surgical situation. The ba­ sic concept of the system is to join advanced geometric modeling 
and animation systems such as Alias with a special purpose .nite element model of the human face developed 
under AVS. In contrast to existing facial models we acquire facial surface and soft tissue data both 
from photogrammetric and CT scans of the individual. After initial data preprocessing, reconstruction, 
and registration, a .nite element model of the facial surface and soft tissue is provided which is based 
on triangular .nite elements. Stiffness parameters of the soft tissue are computed using segmentations 
of the underly­ ing CT data. All interactive procedures such as bone and soft tissue repositioning are 
performed under the guidance of the modeling system which feeds the processed geometry into the FEM solver. 
The resulting shape is generated from minimizing the global en­ ergy of the surface under the presence 
of external forces. Photore­ alistic pictures are obtained from rendering the facial surface with the 
advanced animation system on which this prototype is built. Although we do not claim any of the presented 
algorithms them­ selves to be new, the synthesis of several methods offers a new fa­ cial model quality. 
Our concept is a signi.cant extension to exist­ ing ones and, due to its versatility, can be employed 
in different applications such as facial animation, facial reconstruction, or the simulation of aging. 
We illustrate features of our system with some examples from the Visible Human Data Set.TM CR Descriptors: 
I.3.5 [Computational Geometry and Object Modeling]: Physically Based Modeling; I.3.7 [Three-Dimensio­nal 
Graphics and Realism]; I.4.6 [Segmentation]: Edge and Fea­ture Detection -Pixel Classi.cation; I.6.3 
[Applications]; Additional Keywords and Phrases: Finite Element Method, Fa­cial Surgery Simulation, Facial 
Modeling, Data Reconstruction. INTRODUCTION Background and Previous Work Since the human face plays a 
key role in interpersonal relationships, prediction of post-surgical morphology and appearance of human 
faces for patients with facial deformities is a critical issue in facial .Swiss Federal Institute of 
Technology, Institute for Information Sys­tems, Department of Computer Science, ETH Zentrum, CH-8092 
Z¨urich, Switzerland. fgrossm, kochg@inf.ethz.ch Permission to make digital or hard copies of part or 
all of this work or personal or classroom use is granted without fee provided that copies are not made 
or distributed for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, 
requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 a)b) 
c) Figure 1: Example of a facial disharmony resulting from a craniofacial disorder and its correction. 
a) Presurgical facial shape contour (pro.le). b) Lateral X-ray image of the skull with actual and planned 
new soft tissue pro.les (2D). c) Postsurgical appearance after maxillofacial procedures. (Data source: 
Courtesy Prof. H. F. Sailer, Dept. of Maxillofacial Surgery, University Hospital Z¨urich, Switzerland) 
surgery. Even very subtle malformations of facial proportions can strongly affect the appearance of a 
face and determine on aesthetic aspects such as individual beauty [4]. In addition, many procedures in 
maxillofacial surgery lead to a change in the facial morphology of the patient, e.g. cutting of the jaw 
bones (osteotomies) and bring­ing them into a desired position. Maxillofacial surgery also consists of 
the resection of malformed parts of the face or tumors and the re­construction of missing or malformed 
structures. Furthermore, the treatment of patients with facial fractures in.uences facial shape and surface. 
Therefore, surgeons often face the problem of pre­dicting a fair facial surface before the actual surgery 
is carried out. Figure 1a and 1c illustrate a typical malformation of a young girl s face and its correction 
by surgery. Typically, the maxillofacial sur­geon (sometimes with the help of a medical artist) draws 
the pa­tients predicted pro.le to give at least a 2D appearance of the future face. A lateral X-ray with 
the actual and predicted soft tissue pro­.les is presented in Figure 1b. It is clear that both surgeons 
and their patients have a strong demand for a method which enables them to compute highly realistic 3D 
pictures of the post surgical shape. Any computation should be based on data available, or at least easy 
to obtain from the patient, e.g. CT scans. More specif­ically, with the development of advanced range 
scanners [15] the idea was born to combine both volume and surface data to build a physically based facial 
model. Obviously, the model has to capture the most important anatomical and mechanical parameters of 
the face. Furthermore, it should allow interactive manipulations and the prediction of resulting facial 
shape. It is straightforward to investigate computer graphics literature where facial modeling has a 
long tradition. Early works such as [12] restricted themselves to pure geometric deformations, mostly 
carried out directly on parametric surface models. However, with physically based modeling paradigms, 
more realistic facial mod­els based on mass spring systems feature a linear approximation of the facial 
surface. The particle based approach is also used to gov­ern control vertices of parametric B-spline 
models and thus pro­vides continuous shape descriptions. In particular, the works of Terzopoulos e.a. 
[10, 16] extend the prior approaches in many sig­ni.cant ways: Anatomy is incorporated and an elaborate 
model of facial skin layers and muscles is proposed which is essential to com­pute realistic facial appearance. 
In facial animation, computational complexity is a fundamental constraint, and real-time performance 
is often much more impor­tant than a highly accurate facial shape in terms of a still image. To conclude, 
the methods mentioned above are based on strategies which exclude them from immediate use in facial surgery: 
. In most cases facial models are adapted from a template face instead of generated directly from the 
underlying data set. . The elastomechanics of the surface is only approximated by spring-mass systems 
which can be considered linear and hence C0-continuous .nite elements. . Unsatisfactory approximations 
of the patient s skull surface were used which are essential for surgical applications. . Although [10] 
proposes a very elaborate and elegant model, the underlying facial tissue is still template based and 
does not account for individual variations of the different layers and tissue types. Our Approach In 
the following paper we present a human facial model that enables prediction of facial deformations after 
surgery. In contrast to prior approaches, our goal is to approximate the physics as closely as possible 
and to accept higher computational costs. Therefore, we extend the models described earlier in 3 ways: 
1. We employ a non-linear, globally C1continuous .nite ele­ment model of the facial surface which is 
based on triangular polynomial shape functions similar to the modeling paradigm proposed by [2]. Increasing 
the number of .nite elements re­ sults in a facial surface that tends to be C3continuous. 2. We compute 
the external forces of the model by connecting the surface with nodal springs to the skull. The individual 
spring stiffnesses are computed by means of 3D line integra­tion through the CT data. 3. We build our 
model on top of commercially available mod­eling, visualization, and animation systems which support 
in­teractive geometric manipulation and rendering.  We organize our paper as follows: First, we give 
an overview of the system and describe the general set-up as a combination of com­mercial tools and individual 
components. In section 2 we elaborate on the different preprocessing steps to build the facial model. 
They range from geometry extraction and adaptive surface meshing to the computation of spring stiffnesses. 
Section 3 reviews the .nite ele­ment model we employ. Particular emphasis is given to triangular patches. 
Section 4 illuminates the processing steps necessary to ini­tialize the FEM model and perform computational 
surgery. Finally, we demonstrate the proposed system with experimental results ob­tained from the Visible 
Human Data SetTM(VHD) [11]. 1 System Overview This section expands on the different procedures, data 
processing steps, and general set-up of our simulation system illustrated in the chart of Figure 2. Our 
data sources consist of laser range scans, CT data or, for the VHD, photo slices. Our system combines 
a higher order poly­nomial FEM surface model with a mesh of springs attached to the skull. In order to 
generate a model for an individual patient, the following processing steps are necessary: First of all, 
an initial facial surface is extracted from the data sources and an adaptive surface mesh computed for 
further process­ing. The mesh of springs connecting the surface and the skull re­quires that the skull 
has the same mesh topology as the skin. There­fore, we project the skin vertices down to the skull. One 
of the major advantages of our model is that it derives stiffness from the underlying volume data set, 
which makes a segmentation procedure necessary. All surgical planning is performed interactively on the 
precomputed skull model. Further interaction is required to de.ne boundary conditions and the stretching 
and bending tensors crucial to achieve robust results. The FEM pipeline is comprised of com­putations 
for local and global stiffness matrices, preloading, assem­bling, solving and disassembling to obtain 
the new shape. Finally, the facial surface is rendered using Alias.  procedure holds for the extraction 
of the skull from the CT data set. Bone tissue produces high intensity values in CT data and can be enhanced 
with windowing operations. The .nal skull shape is shown in Figure 4c. a) b)c) Figure 4: Facial surface 
and skull data extraction. a) Edges detected from a CT slice using the Canny edge detector (yellow edges) 
and simulated range scans for skull (red) and skin (skin-colored). b) Facial surface and underlying (u;v)coordinate 
system. c) Skull surface sharing the same coordinate system. The above procedure is vulnerable to some 
minor artifacts due to self occlusion (e.g. nostrils, ears), but it successfully avoids the gen­eration 
of non-manifold shapes as an isosurface tool might produce [14]. The resulting skull is suf.ciently shaped 
to perform facial surgery. 2.3 Adaptive Meshing The surface model is based on a .nite element surface 
de.ned over an arbitrary triangulation of the parameter domain. Therefore, a trade-off has to be found 
between computational costs and preci­sion. Since the computing time depends mostly on the number of 
triangular patches, a good adaptive triangulation of the parameter domain is an essential preprocessing 
step. There are a wide range of meshing and triangulation schemes available in the literature, like Delaunay, 
greedy meshing, or wavelets [5]. We decided to use a slightly modi.ed version of the method of [13] for 
thinning and retiling. The mesh has to follow the 2D parameter domain and additional effort has to be 
spent at its boundaries. In our case, we start with a given trivial mesh directly obtained from the laser 
range scan as presented in Figure 5a. It is thinned out up to a given quality threshold, as visualized 
in Figure 5c. A mesh comparable to Figure 5b is used for all results presented in this paper. The .nal 
bust of the VHD was generated from the original data set by subsampling by 2. The initial mesh consisted 
of 69k triangles and was reduced to 39k.  a)b) c) Figure 5: Adaptive triangular meshes of the facial 
surface of the VHD. a) Initial mesh generated by an equally spaced subsampling by 20:5(21k triangles). 
b) Adaptive mesh with 7k triangles after subsampling by 2. c) Adaptive mesh consisting of 2.5k triangles. 
In order to remove high frequency spikes resulting from gaps in the Canny edges and from wire ropes in 
the VHD, additional Gaussian low pass .ltering was performed locally.  2.4 Registration and Segmentation 
Registration of facial surface and CT data is fundamental to the task of modeling the facial tissue. 
Although many automatic matching methods have been proposed in the computer vision literature [8], fully 
automatic techniques are still prone to errors. In the general case of laser range scans and CTs from 
different sources we carry out the registration manually by setting landmarks [6]. However, in the experimental 
set-up with the VHD, the facial surface is com­puted directly on the volume data set (see section 2.2) 
and registra­tion is accomplished inherently. In order to compute a 3D physically based model of the 
individ­ual s facial tissue, it is necessary to provide detailed information on the different tissue 
types which have different physical parameters. [10] proposes an elaborate 5-layered soft tissue model 
with biphase springs, but individual compositions of the patient s facial tissue are not considered. 
For this reason, we base the computation of the spring stiffness on a segmentation of the underlying 
volume data. In the general case of CT data sets, much work has been done to provide appropriate tissue 
extraction, but intensity-based methods [3] still do a good job. In this case, the intensity range is 
segmented into different tissue types which tend to be Gaussian distributed, as shown in Figure 6a. The 
resulting segmentation is illustrated in Figure 6b. In the special case of photo slices, more elaborate 
color clustering methods can be applied, such as in Figure 6c. a) b)c) Figure 6: a) Intensity distribution 
of different tissue types in CT data. b) Threshold­based segmentation of a CT slice. c) Color-based segmentation 
of a photo slice. For all results presented in this paper we have assigned the stiff­ness values of Table 
1 to the different tissue types. These values are proportional to the corresponding spring stiffness 
values. Only tissue between skin and skull is considered. Table 1: Spring stiffness parameters. tissue 
type color (red, green, blue) stiffness bone (extracted from CT) 1 skin 151, 125, 90 200.0 muscle 80, 
40, 35 100.0 fat dark 165, 140, 100 10.0 fat bright 190, 160, 110 20.0  2.5 Spring Mesh Topology Every 
vertex x iof the facial surface mesh belongs to a vertex x i sb on the skull. This relation enables us 
to construct a mesh of springs between skin and skull. The attachment point on the skull x iis bcomputed 
by tracing the surface normal niof the skin through the tissue in the direction to the skull. niis approximated 
by averaging the normals of all adjacent triangles. The skull intersection point x iis found by thresholding 
the CT data set registered to the facial b surface. We distinguish between two types of springs: Main 
springs connect the skin vertices x iwith the skull vertices sx ion the bone. They are normal to the 
skin surface. b Strut springs connect a skin vertex x iwith the neighbors s neighbor(i;j) i x of the 
related skull vertex x b. Figure 7 illustrates this relationship. We do not use biphase springs b since 
we are interested in the stationary shape, rather than dynamic behavior. Our tissue model assumes the 
skin directly connected to the underlying bone. There is no sliding between tissue and bone surface and 
none between different tissue types. Any extension of the model towards sliding and non-sliding tissue 
parts would re­quire additional segmentations and is a task for future work.  2.6 Computation of the 
Spring Stiffness from CT To compute the spring stiffnesses according to the segmentation presented in 
section 2.4, we have to consider that a spring penetrates different types of facial tissue on its way 
from the skin surface to the skull attachment point. We cope with this problem by decom­posing the spring 
into a set of smaller springs which are connected in series. Let c(x)be the tissue stiffness as a function 
of the spatial pa­rameter x,and let x iand x ibe the parameter values for the skin sb and bone attachment 
points of one individual spring k. The overall stiffness ckof the resulting spring is computed by line 
integration from the surface node to the skull attachment point. 3.1 Variational Surface Modeling The 
mathematical description of deformable surface models is well studied and can be considered a variational 
approach. The result­ing shape w(u;v)is computed by minimizing the overall energy Esurfunder the presence 
of external and internal loading forces. The shape s internal deformation energy Edefdepends on its nat­ural 
resistance against bending and stretching. Cand ;denote the respective tensors de.ning the individual 
material properties. The surface energy has to be computed over the parameter domain f. For convenience 
we follow the notation of [17] and [2]. Z Edef=(Cstretching+;bending)df(3) . "# hi C11C12 ;11 C= ;;=;22 
C12C22 ;12 More speci.cally, let f(u;v)be the external forces applied to a surface w(u;v),and let wu, 
wuu, wv, wvvbe the .rst and second order derivatives in the principal directions. We can reformulate 
(3) to ib Zx 11 =dx (1) Esurf= (4) ckc(x)Z 22 is C11w+C12wuwv+C22w + 2fwdf uv x 222 ;11w+2;12w+;22w 
uuuvvv For a discrete set of Lsprings with individual lengths l1;:::;lL kk 1 L and stiffnesses ck;:::;ckconnected 
in series, as in Figures 8a and . The central idea underlying the .nite element approach is to b, the 
upper integral collapses to L lhL 11XX kh = withlk=lk (2) hcklk ck h=1 h=1  a) b)c) Figure 8: a) Springs 
connecting the facial surface nodes with the skull. b) Decompo­sition into smaller springs connected 
in series as they penetrate different tissue types. c) Distribution of spring stiffnesses as pseudo-colored 
vectors. This summation starts from the skin surface and runs along the surface normal to the skull 
attachment point. If the normal does not intersect the skull, a maximum distance from the surface is 
assumed and the spring stiffness is set to zero. The skull attachment points are also used in section 
4.4 for calculation of the loading forces after bone repositioning. As stated earlier, an adaptively 
triangulated surface mesh is ap­plied. Hence, different springs kare affecting differently sized sur­face 
areas !k. It is recommended to solve this problem by ad­ditional weighting of either the stiffness value 
ckor the resulting force with !k. See also section 4.4. Figure 8c illustrates the distribution of spring 
stiffnesses for the VHD using pseudo-colored vectors.   3 The Finite Element Model This section discusses 
the .nite element surface model we use to represent the facial surface. Finite element modeling is a 
funda­mental engineering method with many different types of elements proposed in the past. We focus 
on the construction of globally C1 continuous shape functions over triangles. Global continuity is re­quired 
for smooth representation and rendering of the facial shape. Furthermore, the triangular approach is 
topologically more .exible and .xes irregular meshes of the parameter domain. C1triangular .nite elements 
have .rst been proposed by [17] and used by [2] to develop physically based modeling paradigms. compute 
an approximate solution of the unknown shape w(u;v) by dividing w(u;v)into patches wp(u;v)and using expansions 
with weighted sums of bases ¢l, which form the .nite elements: X wp(u;v).al¢l (5) l where (u;v)2fpis 
the local parameter space of a surface patch p. These bases are de.ned over a .nite domain fpprovided 
by the initial discretization of the underlying parameter domain f. Thus, the problem of solving the 
upper functional is reduced to .nd ap­propriate weights al. These weights represent the coordinates of 
the shape function in a functional space spanned by the ¢l. Based on the upper relation, the functional 
(4) can be rewritten in matrix form so it collapses to Ka=FK:System global stiffness matrix (6) F:External 
load vector This method allows the global stiffness matrix to be composed by assembling all local stiffness 
matrices of the different .nite el­ements. Kis usually a large, sparse matrix and sophisticated li­braries 
for ef.cient handling are available [9]. This approach is fundamentally different from the .nite differ­ence 
method where the solution is only provided at discrete shape points. Many different types of shape functions 
have been proposed in the past, depending on the job they have been tailored for. We face two essential 
requirements for our application: 1. Topologically .exible discretization of the parameter domain in 
terms of irregular triangular meshes. 2. Global C1continuity to ensure a smooth shape for further rendering. 
 This leads directly to C1shape functions over barycentric coordi­nates. Although these types of .nite 
elements can be found in [17], there is no ready-to-use recipe for how to construct them. For this reason, 
we explain our construction scheme in detail below. 3.2 Globally C1Continuous Shape Functions The globally 
C1continuous shape functions can be derived from the very popular N9shape functions. Let r, s, tde.ne 
a barycen­tric coordinate system with r+s+t=1. The following 9 DOF polynomial elements N1;:::;N9independently 
.x the nodal dis­placements and derivatives along the triangle edges, according to Figure 9. N1;:::;N3are 
given by Hermite type polynomials (7). All other N4;:::;N9can be computed by cyclic shifting of r, s, 
and t. u 5 2 11 4 displacements6[us vs] [ut vt] 9 3 7 derivatives in u8 10 12 derivatives in v 1 2 cross 
boundary derivatives [ur vr] 3 v 1 Figure 9: Indices of shape functions which control the nodal displacements 
and all derivatives. 2222 N1=r+rs+rt-rs-rt N2=r 2 s+1 rst 221 (7) N3=rt+rst 2 r 2 s 2t(1+t) N10=(r+t)(s+t) 
In order to achieve global C1-continuity, we have to control the cross boundary derivative of two adjacent 
triangular patches with­out in.uencing the other DOFs. This can be accomplished by ex­tending the initial 
set of shape functions with the polynomials N10, N11,and N12. They only in.uence the cross boundary derivative 
of the respective edge. Figure 10 shows how N1, N2, N3,and N10 affect the displacement and boundary derivatives 
of node r. N1 N2 N3 N10 r r r r s s s s t ttt Figure 10: Illustration of the fundamental shape functions 
N1, N2, N3,and Nto 10 control the displacements and derivatives. Unfortunately, the cross boundary derivatives 
of our initial set of C0bases do not vanish. Therefore, it is necessary to correct these contributions. 
This correction is performed by evaluating the derivatives of the N1;:::;N9shape functions normal to 
all triangle edges rs, st,and trin the underlying global (u;v)parameter space. Let mirsbe the derivative 
of the shape function Ninormal to edge rs. We can de.ne a modi.ed set of shape functions N iwhose cross 
boundary derivatives vanish. This is done by correcting them with appropriate weighted sums of the functions 
N10;:::;N12. . . rs sttr N10miN11miN12miN i=Ni-8L++ (8)lenrs lenst lentr The upper correction term is 
in.uenced by twice the triangular area L, by the individual lengths of the edges (lenrs, lenst, lentr), 
and by a factor of 4 which is the inverse of the derivative values of N10, N11,and N12at the edge midpoints. 
Additional effort has to be spent to take into account the actual orientation of the triangular patch 
in the global surface coordinate system. The detailed description of the computation of miis be­yond 
the scope of this paper but can be found in [2]. Now the de.nition of the .nal set of globally C1continuous 
shape functions qiis straightforward: q1=N 1 q7=N 7 q2=(c3NN 2-c2NN3)2Lq8=(c2NN8-c1NN9)2L NN NN q3=(-b3N2+b2N3)2Lq9=(-b2N8+b1N9)2L 
rst(1+t)2 q4=NN4 q10=sigrs 22(9) (r+t)(s+t)lenrs NN s 2t2 r(1+r)2 q5=(c1N 5-c3N6)2Lq11=sigst (s+r)(t+r)lenst 
NN t2 r 2 s(1+s)2 q6=(-b1N5-b3N6)2Lq12=sigtr (t+s)(r+s)lentr We require a1=usvt-utvs b1=vs-vt (10) c1=ut-us 
and for the sigoperator which is incorporated in q10;:::;q12 if (ur-us=0) f if (vr-vs>0) sigrs:=1;else 
sigrs:=-1; g else f if (ur-us>0) sigrs:=1;else sigrs:=-1; g and .nally get all other ai, bi, ci, sigst,and 
sigtrby cyclic shift of the indices. For each edge shared by two triangular patches we have to .nd a 
unique de.nition of the sign of their normal direction. This is necessary for the assembling step in 
the .nite element system and is taken into account during the transformation of the shape func­tion from 
the local barycentric coordinate system into the global cartesian coordinates. Now, a global convention 
for the cross boundary normal ncan be de.ned via the dot product: . n for n·v>0 n:= u for n·v= 0 (11) 
- else n where vis the v-axis of the surface coordinate system at the edge midpoint. Figure 11 shows 
two adjacent patches e1and e2. In order to keep the C1continuity across the common edge, the corresponding 
cross boundary normals njand nkhave to point in the same direction. v nj nk e2 e1 u Figure 11: Orientation 
of cross boundary normals on two adjacent .nite elements. The set of shape functions q1;:::;q12enables 
us to construct globally C1contiuous shapes over irregular triangular meshes. This is illustrated for 
a 3-patch surface with its center node displaced in the z-direction in Figures 12a-d for an increasing 
number of subdi­visions. As the number of polygons increases, the approximation converges to a continuous 
shape. a) b)c) d) Figure 12: 3 patch element whose center node is displaced. a) Underlying triangular 
mesh. b), c), d) Approximations using 4, 8, and 16 edge subdivisions. The .nal shape is determined by 
the weighted sum of the q1;:::;q12, as in (5). Due to the parametric approach, an individ­ual qihas vector-related 
weights and the resulting weighting vector ahas 36 entries: a=w ;w ;:::;w ;w ;w ;; T(12) 128910w 11w 
12 3.3 Computation of Local and Global Stiffness As explained earlier, the central idea of the .nite 
element approach is to approximate the global solution of the energy functional by piecewise shapes which 
are represented by linear combinations of the shape functions. Let aebe the weight vector of a single 
.nite element eand <e=(q1;:::;q12)Tbe the corresponding vector of the shape functions. We obtain the 
fractional energy of an indi­vidual patch Eeby Z . . T T <T Ee=aee;stra<e;str+<e;benj<e;benaed.e .e (13) 
R T -2f.eaed.e .e Since aeis constant over e, we are able to reformulate (13) as T Ee=aeKeae .2Feae (14) 
where Keund Ferepresent the local patch stiffness matrix and the external load vector KeFe = Z .e (. 
T e;stra.e;str+. T e;benj.e;ben)d.e(15) = Z f T .ed.e (16) .e In order to compute Ke, it is neccessary 
to evaluate the .rst and second order derivatives .e;strand .e;benof the .erelated to the barycentric 
coordinate system. This is accomplished by a set of operators which are explained in [2]. Although it 
is possible to provide analytic descriptions of these derivatives they end up in very complex expressions. 
Hence, we use .nite differences to approximate them in our current imple­mentation. The outer integral 
of the stiffness matrix is calculated by Gaussian quadrature. The last step of the .nite element approach 
is to assemble the local stiffness matrices and force vectors into a global system of equations, according 
to (6). Therefore, all local matrices and vec­tors are superimposed in accordance to their topological 
relations.  4 Computational Surgery This section explains additional computations, constraints, param­eters, 
and boundary conditions neccessary to accomplish facial surgery simulations. A fundamental precondition 
is the de.nition of the boundary conditions of the .nite element shape in terms of rigid and non-rigid 
nodes. Furthermore, we have to assign differ­ent stretching and bending tensors for each local .nite 
element to compute the local stiffness matrices. The computation of the ini­tial shape vector itself 
has to be performed in advance because it can be considered an initial equilibrium state of the shape. 
Nodal displacement forces applied onto the .nite elements are due to the tissue springs attached to the 
skull. 4.1 Boundary Conditions For all .nite element approximations of differential equations, the boundary 
conditions are fundamental. Boundary conditions de.ne a unique solution and diminish the overall degree 
of freedom. Fig­ure 13 illustrates the in.uence of different boundary conditions on the .nal shape for 
a simple 32 patch surface. Although the same external force vector is applied, due to the different boundary 
con­ditions at the four boundaries (depicted in red), the resulting shapes are not equal. Note that the 
boundary conditions are set both for nodal displacements and for all derivatives. a) b)c) The same situation 
holds for the facial skin surface. A careful de.nition of the boundary conditions is critical to a satisfying 
solu­tion and helps to enhance computational performance by lowering the dimensionality of the global 
stiffness matrix. Since in facial surgery most procedures affect only parts of the facial shape, we explicitly 
de.ne displacement conditions. A distinction between rigid and non-rigid nodes is proposed, i.e. only 
non-rigid nodes can be displaced during the FEM computation. This is very effective because even in very 
complicated cases, backside, neck, and hair do not change dramatically in shape. For users convenience 
we de.ne the whole set of boundary con­ditions in terms of binary texture maps on the facial shape, as 
shown in Figure 14a. The red regions are non-rigid, the blue ones are rigid. The texture map itself can 
be generated with an interactive paint program, for instance Alias Paint. In Figure 14c a typical de­formation 
of the facial shape is presented in accordance with the boundary conditions set in Figure 14a and the 
nodal displacement forces indicated as blue lines in Figure 14b. a)b) c) Figure 14: a) Boundary conditions 
represented as a texture map onto the range data (blue: rigid nodes, red: non-rigid nodes). b) Interactive 
de.nition of nodal forces. c) Deformation of the facial shape in accordance with the boundary conditions 
on the left.  4.2 Stretching and Bending Parameters Similar to the spring stiffness, the stretching 
and bending tensors may change their values for different regions of the face. More speci.cally, facial 
skin stiffness is a function of age, sex, weight, and other parameters [4]. Higher values lead to higher 
internal en­ergy of a deformed surface patch, thus giving rigid parts of the face high aand jvalues. 
Changing the stretching and bending param­eters in.uences the size of the deformed facial region and 
the dis­placement of individual surface nodes if a loading force is applied. An interactive paint procedure 
is also used to assign stretching and bending parameters of different regions of the face. A well­suited 
facial segmentation algorithm for the automatic setting of fa­cial stretching and bending parameters 
can be found in [6]. For all computations in this paper, the parameters were set constant over the surface 
of one triangular patch. Typical settings are a=0:01 for stretching, and j=0:02for the bending factor. 
Higher values guarantee appropriate deformations of facial regions with underly­ing cartilage (ears, 
nose). Figure 15 illustrates the variation of the surface parameters color-coded for the VHD. Darker 
colors indicate positive deviations and brighter colors negative deviations from the values above. a0 
-10%, / 0 -10% a0 . 0.01, / 0 . 0.02 a0 + 10%, / 0 + 10% Figure 15: Variation of the stretching and bending 
tensors as a texture map. 4.3 Initialization A fundamental task during a FEM modeling step of the kind 
we propose here is to compute the initial shape of the face from the in­put data. More speci.cally, we 
have to determine the initial weight vector ainitwhich controls the contribution of each shape function 
to the overall facial shape based on adaptive triangular meshes like the one in Figure 5. Conversely, 
once the initial weight vector is derived from the underlying data set, the FEM approach provides us 
with a set of virtual nodal preloading vectors Finit,as given in (17). Although this initial preloading 
is inherent to the approach, it can be considered an external energy which forces the deformable surface 
into the initial presurgical facial shape. Finit= (17) Kainit In other words, the initialization of our 
model is a two pass pro­cess: 1. Estimation of the ainitweight vector from the initial facial surface. 
 2. Computation of the preload vector Finit.  The estimation of ainitcan be carried out as follows: 
The dis­placement vectors w¢1, w¢4,and w¢7are given immediately from the vertex positions of the triangular 
mesh. Any nodal derivative vector in (u;v)direction w¢2, w¢3, w¢5, w¢6, w¢8,and w¢9 as well as any cross 
boundary derivative vector w¢10,and , w¢11wat the midpoints of the triangle edges are computed using 
the ¢12approximated surface normals nuvat the coordinates (u;v). nuv is computed by averaging the normals 
of adjacent triangles. For a given normal vector nuv, the slope vectors in uand vdirection can be approximated 
as follows: @w(u;v) acylXnuv @u kacylXnuvk (18) @w u;v Xn @w(u;v) @u uv  @v @w u;v Xn @u uv where 
acylis the rotation axis of the underlying cylindrical coordi­nate system. The cross boundary slopes 
at the edge midpoints are derived as (19). ersuv sigXn w¢10Xn kersuvk estXnuv w¢11 sig (19) kestXnuvk 
etrXnuv sig w¢12 ketrXnuvk where sigare the respective signs to guarantee condition (11), and ers, est,and 
etrare vectors pointing in the edge direction. Figure 16a illustrates the estimated derivatives encoded 
as small lines of different colors around the nose and mouth. The resulting FEM shape is presented in 
Figure 16b, where the shape functions are subdivided by 2. a) b)  4.4 Repositioning of Bones: Computation 
of No­dal Loading Forces We perform any surgical procedure on the skull as a geometric mod­eling step 
with Alias. The skull used for surgery is shown in Figure 17a. As stated earlier, the initial facial 
mesh is projected along the normals of every surface vertex onto the skull. This provides a mesh on the 
skull surface which is topologically identical to the skin mesh, as in Figure 17b. This skull has many 
artifacts arising from the underlying generation method. The jaw bone is marked in a different color 
and will be repositioned subsequently. ineighbor(i) After surgery on the skull the translations tand 
t of the spring-related skull attachment points at position xbiand neighbor(i) xb have to be computed. 
This is performed by matching a)b) c) d) the affected regions of the skull in Figure 17b with the model 
in Figure 17c. The result of this procedure is illustrated in Figure 17d. Any deformation of the skull 
generates a force fdef;ion the sur­face node xsiwhich in.uences the energy equilibrium (13) between the 
internal energy of the surface and the preloading energy de­scribed in the former section. fdef;iis computed 
as follows: Xj.. sij-t fdef;i=kcki cijksijk-ksij-t j k(20) j ksij-tk j2Si Vertex indices are equal for 
related skin and skull nodes. fdef;iDeformation force at skin vertex ias a result of a deformation of 
the skull. kcForce scaling constant. ki Mesh dependent scaling factor. SiSet of skull nodes for all springs 
attached at vertex ion the skin. sijSpring connecting skin vertex iat position xsiwith jij skull vertex 
jat position xb,i.e. xs-xb. t j Translation vector of the skull vertex jas a result of a surgery on the 
skull. cijStiffness of spring sij. The mesh dependent scaling factor kiis de.ned by X ki=ij (21) j2Pi 
where Piis the set of all triangles containing node i,and ijthe area of the mesh triangle j. Figure 18 
illustrates the upper relations for a particular node i.  facial surface node  initial node on the 
surface of the skull  repositioned skull node  The deformation energy of the facial surface is derived 
from the nodal displacement forces as follows: Z T Fdef=f.d. (22) def n 4.5 Solving the Global System 
The external energy vector Fextis computed by superimposing the nodal preloading energy Finitwith the 
deformation energy Fdef. The global energy equilibrium can be reformulated for the re­sulting shape vector 
aresultby Karesult=Finit+Fdef (23) It is recommended to carry out the simulation in small steps, including 
the recalculation of the patch stiffnesses of all patches affected in the last step.  5 Results The 
goal was to predict the facial shape after standard procedures in craniomaxillo facial surgery. Figures 
19a-c show the shapes of the skull and face before and after an osteotomy and advancement of the lower 
jaw bone. These pictures represent the corresponding deformations of skin in pro.le and frontal view. 
Obviously, the sur­gical procedure strongly affects the appearance of the face. This is even more striking 
in Figures 19d-f where an osteotomy and ad­vancement of the upper jaw bone is presented. All computations 
are based on the methods described above. The pictures are rendered with Alias. a)b) c) d)e) f) The 
affected facial region of the .rst surgical procedure com­prises about 3.1k triangles. The computation 
of the global stiffness matrix took approximately 17 minutes, the solving of the global equation system 
11 minutes. The calculations were carried out on a SGI Indigo2, R4400, 200MHz, 192MB. 6 Discussion and 
Future Work We present a system which enables us to predict deformations of the facial shape after surgical 
procedures. Our prototype system uses commercial tools for geometric modeling and rendering. In contrast 
to existing approaches in physically based modeling of fa­cial shapes, our approach is based on non-linear 
.nite elements, and therefore provides a continuous approximation of the facial geom­etry. We consider 
our approach to be a signi.cant extension in this .eld because we build the model on both laser range 
scans and CT volume data. This enables us to compute individual physical pa­rameters. Some results achieved 
with our system are demonstrated using the VHD. We are aware that some processing steps presented here 
are spe­ci.c to the underlying data. Usually neither high resolution CT scans comparable to the VHD, 
nor photo slices are available. With future generations of MR and CT scanners, however, we expect that 
both facial surface and skull of the patient will be available at suf.­cient resolutions to make range 
scans obsolete. Our future research is directed towards true 3D volume FEM sys­tems based on tetrahedralizations. 
They would integrate both the current surface and volume spring model and allow incorporation of more 
anatomic features. In this case, the global C1constraint could be relaxed and replaced by local continuity 
properties. Fur­ther effort has to be spent on our current geometric model of skull and skin, which is 
based on polygons. It is clear that polysets per­form worse than NURB representations and do not guarantee 
loss­less data conversions between the FEM system and Alias.Other important topics are incompressible 
.nite elements which can cope with the problem of wrinkles. The current rendering quality has to be enhanced 
with using texture maps and better re.ection models for facial skin. Sophisticated models, such as in 
[7], have to be investigated for suitability. Finally, our goal is to carry out a case study with individual 
patients of different ages, genders and ethnic origins and to compare the computed results with those 
achieved by actual surgery. We expect also that an error quanti.cation would help us to improve the model 
towards better performance. 7 Acknowledgement The authors would like to thank Caroline Westort for her 
native­ english review of the text. References [1] CANNY, J. F. A computational approach to edge detection. 
IEEE Transactions on Pattern Analysis and Machine Intelli­gence 8, 6 (1986), pp. 679 697. [2] CELNIKER,G., 
AND GOSSARD, D. Deformable curve and surface .nite elements for free-form shape design. In Com­puter 
Graphics (SIGGRAPH 91 Proceedings) (July 1991), ACM SIGGRAPH, T. W. Sederberg, Ed., vol. 25, pp. 257 
266. [3] DREBIN,R. A., CARPENTER,L., AND HANRAHAN,P. Volume rendering. In Computer Graphics (SIGGRAPH 
88 Proceedings) (Aug. 1988), ACM SIGGRAPH, J. Dill, Ed., vol. 22, pp. 65 74. [4] FARKAS,L.G. Anthropometry 
of the Head and Face, sec­ond ed. Raven Press, 1994. [5] GROSS,M.H., GATTI,R., AND STAADT, O. Fast multireso­lution 
surface meshing. In Proceedings of IEEE Visualization 95 (1995), IEEE Computer Society Press, pp. 135 
142. [6] GROSS,M. H., AND KOCH, R. Visualization of multidi­mensional shape and texture features in laser 
range data using complex-valued Gabor wavelets. IEEE Transactions on Visu­alization and Computer Graphics 
1, 1 (Mar. 1995), pp. 44 59. ISSN 1077-2626. [7] HANRAHAN,P., ANDKRUEGER,W.Re.ectionfromlayered surfaces 
due to subsurface scattering. In Computer Graph­ics (SIGGRAPH 93 Proceedings) (Aug. 1993), ACM SIG-GRAPH, 
J. T. Kajiya, Ed., vol. 27, pp. 165 174. [8] HARALICK,R. M., AND SHAPIRO,L.G. Computer and Robot Vision, 
vol. 1 3. Addison-Wesley, 1992. [9] KUNDERT, K. S., AND SANGIOVANNI-VINCENTELLI,A. A Sparse Linear Equation 
Solver. Dept. of Electrical Engineer­ing and Computer Sciences, University of California, Berke­ley, 
1988. [10] LEE,Y., TERZOPOULOS,D., AND WATERS, K. Realistic face modeling for animation. In Computer 
Graphics (SIG-GRAPH 95 Proceedings) (Aug. 1995), ACM SIGGRAPH, R. Cook, Ed., vol. 29, pp. 55 62. [11] 
NATIONAL LIBRARY OF MEDICINE. The Visible Human Project. http://www.nlm.nih.gov/extramural research.dir/vi­sible 
human.html, 1995. [12] PARKE, F. I. Parameterized models for facial animation. IEEE Computer Graphics 
and Applications 2 (Nov. 1982), pp. 61 68. [13] SCHROEDER,W. J., ZARGE,J. A., AND LORENSEN,W. E. Decimation 
of triangle meshes. In Computer Graphics (SIG-GRAPH 92 Proceedings) (Aug.1992), ACM SIGGRAPH, E. E. Catmull, 
Ed., pp. 65 70. [14] VAN GELDER,A., AND WILHELMS, J. Topological consid­erations in isosurface generation. 
Tech. Rep. UCSC-CRL-94­31, Baskin Center for Computer Engineering and Information Sciences, University 
of California, Santa Cruz, 1994. [15] VANNIER,M. W., PILGRAM,T., BHATIA,G., BRUNSDEN, B., AND COMMEAN, 
P. Facial surface scanner. IEEE Com­puter Graphics and Applications 11, 6 (Nov. 1991), pp. 72 80. [16] 
WATERS, K. A muscle model for animating three-dimensio­nal facial expression. In Computer Graphics (SIGGRAPH 
87 Proceedings) (July 1987), ACM SIGGRAPH, M. C. Stone, Ed., vol. 21, pp. 17 24. [17] ZIENKIEWICZ,O. 
C. The Finite Element Method, fourth ed., vol. 1 2. McGraw-Hill, 1994.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237282</article_id>
		<sort_key>429</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[Superior augmented reality registration by integrating landmark tracking and magnetic tracking]]></title>
		<page_from>429</page_from>
		<page_to>438</page_to>
		<doi_number>10.1145/237170.237282</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237282</url>
		<keywords>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[calibration]]></kw>
			<kw><![CDATA[frame buffer techniques]]></kw>
			<kw><![CDATA[registration]]></kw>
			<kw><![CDATA[stereo video see-though head-mounted display]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Three-dimensional displays**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14103562</person_id>
				<author_profile_id><![CDATA[81100275873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[State]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42052320</person_id>
				<author_profile_id><![CDATA[81339504645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirota]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77037409</person_id>
				<author_profile_id><![CDATA[81414596128]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43119139</person_id>
				<author_profile_id><![CDATA[81100340428]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Garrett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15030395</person_id>
				<author_profile_id><![CDATA[81100365560]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Livingston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Azuma, R. A Survey of Augmented Reality. SIGGRAPH 1995 Course Notes #9 (Developing Advanced Virtual Reality Applications).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192199</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AZUMA, R., BISHOP, G. Improved Static and Dynamic Registration in an Optical See-through HMD. Proceedings of SIGGRAPH 94 (Orlando, FL, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp.197-203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618306</ref_obj_id>
				<ref_obj_pid>616038</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BAJURA, M., NEUMANN, U. Dynamic Registration Correction in Video-Based Augmented Reality Systems. IEEE Computer Graphics and Applications (September 1995), pp.52-60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DIXON, A.L. The Elimination of Three Quantics in Two Independent Variables. Proceedings of the London Mathematical Society, 6 (1908), 49-69, pp.209-236.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>169506</ref_obj_id>
				<ref_obj_pid>169059</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[DRASCIC, D. ARGOS: A Display System for Augmenting Reality. ACM SIGGRAPH Technical Video Review, Volume 88: InterCHI 1993 Conference on Human Factors in Computing Systems (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>9358</ref_obj_id>
				<ref_obj_pid>9356</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[FAUGERAS, O.D., HEBERT, M. The Representation, Recognition and Locating of 3-D Objects. Int. J. Robotics Res., 5:3 (1986), pp.27-52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358692</ref_obj_id>
				<ref_obj_pid>358669</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[FISCHLER, M.A., BOLLES, R.C. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Communications of the ACM, 24:6 (1981), pp.381-395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>39857</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FLETCHER, R. Practical Methods of Optimization. John Wiley and Sons, Inc., New York (1987).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>102900</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[GRIMSON, W.E.L. Object Recognition by Computer: The Role of Geometric Constraints. MIT Press, Cambridge (1990).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>573190</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[HARALICK, R.M., SHAPIRO, L.G. Computer and Robot Vision, Volume I, Addison-Wesley (1993), p.48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>239875</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[HOLLOWAY, R. Registration Errors in Augmented Reality Systems. Ph.D. dissertation, University of North Carolina at Chapel Hill (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[JANIN, A., ZIKAN, K., MIZELL, D., BANNER, M., SOWIZRAL, H. A videometric tracker for augmented reality applications. Proceedings of SPIE, November 1994 (Boston).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>704779</ref_obj_id>
				<ref_obj_pid>646769</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[KANCHERLA, A.R., ROLLAND, J.P., WRIGHT, D.L., BURDEA, G. A Novel Virtual Reality Tool for Teaching Dynamic 3D Anatomy. Proceedings of CVRMed '95 (Nice, France, April 3-5, 1995) pp.163-169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[LIVINGSTON, M., STATE, A. Improved Registration for Augmented Reality Systems via Magnetic Tracker Calibration. University of North Carolina at Chapel Hill Technical Report TR95-037 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>25366</ref_obj_id>
				<ref_obj_pid>25363</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LOWE, D.G. Three-Dimensional Object Recognition from Single Two-Dimensional Images. Artificial Intelligence, 3 1 (1987), pp.355-395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>144381</ref_obj_id>
				<ref_obj_pid>144379</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LOWE, D.G. Robust Model-based Motion Tracking Through the Integration of Search and Estimation. International Journal o f Computer Vision, 8:2 (1992), pp.113-122.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617895</ref_obj_id>
				<ref_obj_pid>616031</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[MANOCHA, D. Solving Systems of Polynomial Equations. IEEE Computer Graphics and Applications (March 1994), pp.46-55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>704768</ref_obj_id>
				<ref_obj_pid>646769</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MELLOR, J.P. Realtime Camera Calibration for Enhanced Reality Visualization. Proceedings of CVRMed '95 (Nice, France April 3-5, 1995), pp.471-475.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897856</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MINE, M.R. Characterization of End-to-End Delays in Head- Mounted Display Systems. University of North Carolina at Chapel Hill Technical Report TR93-001 (1993a).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897859</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MINE, M.R., BISHOP, G. Just-In-Time Pixels. University of North Carolina at Chapel Hill Technical Report TR93-005 (1993b).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MORE, J.J., GARBOW, B.S. HILLSTROM, K.E. User Guide for MINPACK-1. Argonne National Laboratory Report ANL-80-74 (1980).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134071</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[SEGAL, M., KOROBKIN, C., VAN WIDENFELT, R., FORAN, J., HAEBERLI, P. Fast Shadows and Lighting Effects Using Texture Mapping. Proceedings of SIGGRAPH '92 (Chicago, IL, July 26-31, 1992). In Computer Graphics, 26, 2 (July 1992), ACM SIGGRAPH, New York, 1992, pp.249-252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SILICON GRAPHICS, INC. Sirius Video Technical Report. Silicon Graphics, Inc., Mountain View, CA (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237283</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[STATE, A., LIVINGSTON, M., GARRETT, W.F., HIROTA, G., WHITTON, M.C., PISANO, E.D.(MD), FUCHS, H. Technologies for Augmented-Reality Systems: Realizing Ultrasound-Guided Needle Biopsies. Proceedings of SIGGRAPH '96 (New Orleans, LA, August 4-9, 1996). In Computer Graphics Proceedings, Annual Conference Series, 1996, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SUTHERLAND, I.E. A Head-Mounted Three Dimensional Display. Fall Joint Computer Conference (1968), pp.757- 764.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[TSAI, R. Y. A Versatile Camera Calibration Technique for High- Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses. IEEE Journal of Robotics and Automation, RA-3:4 (August 1987), pp.323-344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614312</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[TUCERYAN, M., GREER, D.S., WHITAKER, R.T., BREEN, D.E., CRAMPTON, C., ROSE, E., AHLERS, K.H. Calibration Requirements and Procedures for a Monitor-Based Augmented Reality System. IEEE Transactions on Visualizations and Computer Graphics, 1:3 (September 1995), pp.255-273.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>704601</ref_obj_id>
				<ref_obj_pid>646769</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[UENOHARA, M., KANADE, T. Vision-Based Object Registration for Real-Time Image Overlay. 1995 Conference on Computer Vision, Virtual Reality and Robotics in Medicine (Nice, France, April 1995), pp.13-22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>82445</ref_obj_id>
				<ref_obj_pid>82444</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[WANG, L.-L., TSAI, W.-H. Computing Camera Parameters using Vanishing-Line Information from a Rectangular Parallelepiped. Machine Vision and Applications, 3 (1990), pp.129-141.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>147162</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WARD, M., AZUMA, R., BENNETT, R., GOTTSCALK, S., FUCHS, H. A Demonstrated Optical Tracker with Scalable Work Area for Head-Mounted Display Systems. Proceedings of the 1992 Symposium on Interactive 3D Graphics (Boston, MA, March 1- April 1, 1992), pp.43-52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897865</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[YOO, T.S., OLANO, T.M. Instant Hole (Windows into Reality). University of North Carolina at Chapel Hill Technical Report TR93-027 (1993).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Superior Augmented Reality Registration by Integrating Landmark Tracking and Magnetic Tracking Andrei 
State, Gentaro Hirota, David T. Chen, William F. Garrett, Mark A. Livingston Department of Computer 
Science University of North Carolina at Chapel Hill http://www.cs.unc.edu/~us/hybrid.html ABSTRACT 
Accurate registration between real and virtual objects is crucial for augmented reality applications. 
Existing tracking methods are individually inadequate: magnetic trackers are inaccurate, mechanical trackers 
are cumbersome, and vision-based trackers are computationally problematic. We present a hybrid tracking 
method that combines the accuracy of vision-based tracking with the robustness of magnetic tracking without 
compromising real-time performance or usability. We demonstrate excellent registration in three sample 
applications. CR Categories and Subject Descriptors: I.3.7 [Three-Dimensional Graphics and Realism]: 
Virtual Reality, I.3.1: [Hardware Architecture]: Three-dimensional displays, I.3.6 [Methodology and Techniques]: 
Interaction techniques. Additional Keywords and Phrases: Augmented reality, stereo video see-through 
head-mounted display, frame buffer techniques, registration, calibration.  1 MOTIVATION While the advent 
of Head-Mounted Displays (HMDs) and affordable real-time computer graphics engines has given rise to 
much research in the field of Virtual Reality (VR), comparatively little work has been done in the closely-related 
field of Augmented Reality (AR). A VR system immerses the user in a totally synthetic, computer-generated 
environment. An AR system, on the other hand, merges computer­synthesized objects with the user s space 
in the real world. Synthetic objects enhance the user s interaction with, or his perception of, the real 
world [Azuma95]. The following are typical requirements for an AR system: (1) Accurate registration between 
synthetic and real objects: a virtual object should appear at its proper place in the real world, otherwise 
the user cannot correctly determine spatial relationships. Dynamic registration is particularly important 
when the user moves around the CB #3175 Sitterson Hall, Chapel Hill, NC 27599-3175. Tel: +1-919-962-1700. 
E-mail: {state, hirota, chen, garrett, livingst}@cs.unc.edu Permission to make digital or hard copies 
of part or all of this work or personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that copies bear this notice and the 
full citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
environment. The relative position between real and synthetic objects should be constant. (2) Reasonable 
image generation rate (10 Hz) and stereopsis: these are important for good depth perception. The lack 
of kinetic or stereoscopic depth cues greatly reduces the believability of an augmented environment. 
 (3) Simple initial set up procedure: users of AR applications should not have to be familiar with the 
specific techniques used in AR systems. (4) Minimal constraint on user motion: in most applications, 
the user wants to move without restriction. (5) Low latency: minimal delay between the user's movement 
and the display update is required for smooth and effective interaction.  Among these requirements, 
the accurate registration turns out to be a very difficult problem. Current AR systems cannot convincingly 
meet this requirement. Typically a virtual object appears to swim about as the user moves, and often 
does not appear to rest at the same spot when viewed from several different positions. In current AR 
systems, most of these registration errors are due to the limitations of the tracking systems [Holloway95]. 
No conventional tracker satisfies all of the above requirements.  2 PREVIOUS WORK There has been much 
research in the field of tracking and registration. Most tracking systems used today in fully immersive 
VR systems have been magnetic. In the field of computer vision there is a wealth of research on motion 
tracking. Today s magnetic trackers are subject to large amounts of error and jitter. An uncalibrated 
system can exhibit errors of 10 cm or more, particularly in the presence of magnetic field disturbances 
such as metal and electric equipment. Carefully calibrating a magnetic system can reduce position errors 
to within 2 cm [Livingston95]. Despite their lack of accuracy, magnetic trackers are popular because 
they are robust and place minimal constraints on user motion. Other AR systems have used mechanical [Sutherland68] 
or optical [Ward92, Azuma94] tracking systems. Both of these systems generally have better accuracy than 
magnetic trackers, but are burdensome. Mechanical systems tether the user and have a limited working 
volume, and the optical tracker in [Ward92] requires four dedicated tracking cameras mounted on the user 
s HMD. In a video see-through AR system [Azuma95], video images of the user s view are always available. 
Using those images to track the camera s position and orientation should be a reasonable approach, and 
camera tracking has been extensively investigated in the field of computer vision or photogrammetry. 
Nevertheless, recovering 3D information from 2D images is not an easy task. An intrinsic problem of computer 
vision is that an almost infinite number of possibilities must be considered until the images can be 
interpreted correctly. Model-based vision assumes a priori knowledge of the 3D geometry of visible objects, 
reducing the problem from shape recovery to mere camera motion tracking [Lowe87, Lowe92]. Even by simplifying 
the problem this way, model-based vision methods must still extract object features from images. This 
typically requires special-purpose image processing hardware to achieve real-time updates. Further acceleration 
can be achieved through the use of fiducials or landmarks. These artificial features of objects simplify 
image analysis. The advantage of vision-based tracking when applied to video-see-through AR is that it 
uses the very same image on which synthetic objects are overlaid. Therefore nearly perfect registration 
can be achieved under certain conditions [Mellor95, Uenohara95]. The problem of vision-based methods 
is their instability; to save computation cost, they make numerous assumptions about the working environment 
and the user s movements, but those assumptions are often impractical. For example, they usually assume 
temporal coherence of camera movement in order to avoid frequent use of costly search algorithms [Faugeras86, 
Grimson90] that establish the correspondence between image features and model features. Thus, they usually 
cannot keep up with quick, abrupt user movements. No vision­based tracker reliably deals with the occlusion 
of features caused by deformable objects (e.g. hands). Once a vision tracker's assumptions fail, the 
results can be catastrophic. Computationally, most vision-based methods use iterative minimization techniques 
that rely on frame-to-frame coherence. Linearization reduces the problem to a single global solution 
but requires the vision-based tracker to extract a relatively large amount of information from features 
or landmarks [Mellor95]. Since image analysis and correspondence finding are costly and error-prone, 
and because landmarks can be occluded, obscured, or may disappear from the camera s view at any time, 
it is impractical to attempt to continuously track a large number of features in real time. 3 CONTRIBUTION 
We have developed a hybrid tracking scheme which has the registration accuracy of vision-based tracking 
systems and the robustness of magnetic tracking systems. We use video tracking of landmarks as the primary 
method for determining camera position and orientation. This tracking method inherits the accuracy of 
some vision-based methods, but avoids unnecessary computational cost and reduces the demands on the image 
analyzer. Color-coding the landmarks helps the system to quickly identify and distinguish between landmarks. 
This not only eases system setup and improves performance but also lets the system handle abrupt user 
movement. A global non-linear equation solver and a local least square minimizer are used to reduce the 
burden on the image analyzer. Typically 3 landmarks suffice to determine camera position and orientation. 
Our formulation gives a universal solution for single and stereo camera cases. The result of the vision-based 
tracker is also used for on­the-fly calibration of the magnetic tracker, which assists the rest of the 
system in four different ways: Image analysis acceleration: The magnetic tracker helps narrow the landmark 
search area on images, speeding up the landmark search process. Selection from multiple solutions: Information 
from the magnetic tracker is often used to select one of several solutions of a non-linear equation. 
Backup tracking: the magnetic tracker acts as the primary tracker if the image analyzer cannot locate 
enough landmarks. Since the magnetic tracker is locally calibrated on-the-fly, we avoid complete loss 
of registration. If 1 or 2 landmarks (not enough for a unique solution) are detected, several heuristic 
methods are used to minimize registration loss. Sanity check of the vision-based tracker: As mentioned 
above, vision-based tracking is sometimes unstable. We avoid catastrophic failure by monitoring the difference 
between results from the magnetic tracker and the vision-based tracker and discarding corrections that 
exceed a certain magnitude. 4 SYSTEM HARDWARE All principal components of our system are commercial, 
off­the-shelf devices. Our system consists of: a Virtual Research VR-4 HMD.  two Panasonic GP-KS102 
CCD video cameras with Cosmicar F1.8 12.5 mm lenses (28° field of view, selected for minimal optical 
distortion), attached to the HMD.  an Ascension Flock of Birds magnetic tracker with Extended Range 
Transmitter; the magnetic tracking sensor is attached to the HMD.  a Silicon Graphics Onyx RealityEngine2 
graphics workstation equipped with a Sirius Video real-time video capture device (Sirius), and a Multi-Channel 
Option .  The HMD-mounted cameras are 64 mm apart a typical interpupillary distance for humans and are 
oriented with a convergence angle of 4° for sufficient stereo overlap in a tabletop working environment. 
This angle was chosen for one of our driving applications [State96], which involves manipulation directly 
in front of the user. The Sirius captures stereo video images from the head­mounted cameras in real-time 
and transfers the images to the graphics frame buffer of the RealityEngine2. 5 SYSTEM OVERVIEW The hybrid 
tracker analyzes sensor data from two input streams: real-time video images from the stereo cameras, 
and tracking reports from the magnetic tracking sensor. The system assumes that the two cameras and the 
tracking sensor are rigidly interconnected and are rigidly attached to the HMD and the user s head. Head 
pose will refer to the position and orientation of this rigid HMD-cameras-sensor assembly. We assume 
that the geometry of this assembly is known and that the transformations between the various coordinate 
systems (cameras, sensor) have been determined via calibration procedures. We also assume that the world 
space positions of the landmarks used in the vision-based tracking algorithm are precisely calibrated. 
All calibration procedures are described in Section 8. 5.1 Operation For each stereo image pair (i.e. 
frame), the hybrid tracker attempts to determine the head pose from the landmarks positions in the images. 
If this attempt is successful, it determines an error-correcting transformation between the magnetic 
tracker reading and the head pose computed by the vision-based tracker. We will refer to this transformation 
as the magnetic tracker error. The magnetic tracker error computed in one frame is used to predict the 
head pose in the next frame (temporal coherence). This prediction is subsequently used to compute the 
expected positions of the landmarks in image space. Figure 1 shows the data flow within the hybrid tracker. 
At startup, the magnetic tracker error is initialized to zero. The head pose predictor therefore passes 
the readings from the magnetic tracker unchanged to the landmark predictor, which computes the expected 
image-space search areas for the landmarks. Using this data as a starting point, the image analyzer searches 
for landmarks in the video images. As soon as the first landmark is detected, the head pose is adjusted 
via a simple heuristic to line up the detected landmark in image space [Bajura95]. The resulting adjusted 
head pose in the case of a single landmark only head orientation is adjusted is fed back to the landmark 
predictor for re-prediction of landmark search areas. The system uses these improved values to find additional 
landmarks, thus iteratively refining its knowledge about the head pose. Each time a new landmark is found, 
an appropriate head pose adjuster or solver is invoked, depending on the total number of landmarks detected. 
There are two distinct cases: (1) If the number of detected landmarks is not sufficient to completely 
determine the head pose (under-determined cases), the methods used are local, heuristic position and/or 
orientation adjusters (Section 7.1) such as the single-landmark method mentioned above. (2) In well-determined 
and over-determined cases, a global, analytical solver is invoked (Section 7.2). This solver may compute 
multiple solutions, in which case a solution selector is invoked. The selector attempts to pick a solution 
by verifying the consistency of all detected landmarks but is not always able to determine a single best 
solution. In particular, we often encounter situations in which only 3 different landmarks are visible 
in both cameras. In such cases we use the sensor reading from the magnetic tracker to determine which 
solution is correct.  In all cases, under-, well- and over-determined, the computed or adjusted head 
poses are first subjected to sanity checks. Then they are fed back to the landmark predictor to iteratively 
detect additional landmarks. This continues until a maximum preset number have been found or until all 
landmarks in the two stereo images have been found. The solutions resulting from well-or over-determined 
cases are stabilized by a local least-square optimizer. If the head pose remains under-determined even 
after exhaustive search for additional landmarks, the partial correction derived by the most recently 
invoked heuristic adjuster(s) is retained. The magnetic tracker error (whether computed and optimized 
or merely partially corrected) is preserved for head pose prediction in the next frame. This constant, 
0th order prediction for the magnetic tracker error is adequate given that our system s frame rates rarely 
exceed 15 Hz in stereo. We use higher-order prediction (linear, combining the magnetic tracker errors 
from the 2 most recent frames) only if the application and the tracking environment allow higher frame 
rates (e.g. non-stereo operation). [Azuma94] showed that higher-order prediction works best at high frame 
rates. The corrected head pose delivered by the hybrid tracker yields excellent AR registration between 
real and virtual objects. Figure 2 shows a view within a video-see-through HMD. A tabletop model with 
wooden cuboids and landmarks is accurately registered with a computer model of the cuboids (white wireframe 
lines). digitized stereo images approximate head pose from from head-mounted cameras magnetic tracker 
 5.2 Vision-only tracking Vision-only tracking (i.e., without assistance from the magnetic tracker) 
requires only minor modifications. The predicted head pose delivered to the landmark predictor and to 
the heuristic adjusters is estimated directly from the head pose in the previous frame(s).  6 LANDMARK 
TRACKING The image analyzer detects and tracks landmarks in the video images. Since this is the most 
time-consuming task in our system, its performance is a primary design concern. 6.1 Landmark shape and 
color The landmarks used by the hybrid tracker are two-color concentric circular dots. 11 such landmarks 
are visible in Figure 2. Each landmark consists of an inner dot and a surrounding outer ring with a diameter 
that is 3 times larger than the diameter of the inner dot. We use four different colors (mixed from commercial 
fluorescent fabric paints), which we label as red, green, blue, and yellow; thus we can create 12 unique 
combinations which can be recognized and identified by the landmark finder. Color landmarks are useful 
in several ways. Multiple colors simplify and accelerate low-level pixel inspection, resulting in quick 
detection. The concentric layout makes our method very robust. While the search algorithm might be easily 
fooled if it were simply looking for a uniform spot of a single color (as in earlier versions of our 
system), the more complex structure of two-color landmarks makes spurious detection much more unlikely 
(Figure 3).  6.2 Operation The landmark finding subsystem consists of two main components: the landmark 
predictor, which predicts where the Figure 2. View inside the HMD while the user's head is stationary. 
The axis-aligned search areas accelerate landmark search. 11 out of the 12 different landmarks created 
with two-color concentric rings are visible. Note accurately registered computer-generated cuboid outlines 
(white). Figure 4. Maintaining registration while the user's head is in motion. Some of the landmarks 
were not contained within their initial search areas, so the search areas were progressively expanded. 
Note motion blur.  Figure 3. Maintaining registration in the presence of spurious color spots. During 
landmark search, only specific color and shape signatures are recognized as valid landmarks. Other color 
areas are inspected but rejected. Figure 5. Correct head pose despite landmark occlusion. The landmark 
tracker is robust enough to handle occlusion. The design of the landmarks makes it possible to detect 
partial occlusion. landmarks should be in the video camera image, and the image analyzer, which locates 
the landmarks in the image.  6.2.1 Landmark predictor The main task of the landmark predictor is to 
compute the expected positions and extents of landmarks in image space. For each landmark, a search area 
is determined based upon the predicted extent. Since the image analyzer operates by exhaustive pixel 
searches inside search areas, it is important to keep the extents small, i.e., to tightly track the landmarks 
in image space with bounding boxes (Figure 2). As described above, the hybrid tracker incrementally improves 
head pose after each newly-found landmark, increasing the accuracy of the predicted positions and predicted 
extents of the remaining undetected landmarks. As shown in [Bajura95], lining up a single landmark often 
results in dramatically improved registration. Therefore lining up the first landmark detected often 
yields accurate search areas for the remaining landmarks, accelerating the subsequent searches. Similar 
ideas can be found in computer vision literature [Lowe87, Lowe92]. When searching for the first landmark, 
there are no landmark-derived head pose corrections available, so it is important that the first landmark 
in each frame be easy to detect. This means the first landmark should have a relatively small search 
area, and there should be a high probability of actually finding it within that area. To this end, the 
landmark predictor keeps track of potentially detectable landmarks and sorts them in order of decreasing 
expected ease of detection. The landmark predictor uses predicted and iteratively improved head poses 
to compute the expected positions of the landmarks in image space. In addition to this 3D prediction, 
the landmark predictor performs an internal 2D image space prediction which is not based on input from 
the magnetic tracker, but only on detected landmarks. For each landmark, the 3D and 2D predictions are 
compared; if the distance between the two predicted positions is below a preset threshold or if the expected 
position is far enough from the edge of the image, then the landmark is assigned a high score for ease 
of detection. 6.2.2 Image analyzer The second component of the landmark finder is the image analyzer, 
which starts its search for a landmark by inspecting the search area defined by the landmark predictor. 
The first step is pixel marking. Every pixel is classified as belonging to one of the landmark colors 
or as belonging to no landmark based on the ratios of RGB component values. For our specific camera and 
frame grabber hardware, and under the lighting conditions in our lab, such a simple algorithm can reliably 
distinguish between only a small number of different colors. We use the four colors mentioned in Section 6.1. 
The algorithm looks first for areas whose color matches the color of the outer ring of a concentric landmark 
and then attempts to locate the inner color dot within the identified area. The marked regions are segmented 
by horizontal and vertical signature [Haralick93] to determine their centers of mass. If a marked region 
does not fit inside the bounding box of the search area, the search area is enlarged (Figure 4). For 
large search areas, a lower sampling density of as little as 1 in 64 (8x8) pixels is used initially; 
the sampling density is then successively increased as the algorithm reduces the search areas while refining 
its estimate of the landmark s location. For all candidate detections consisting of an outer color ring 
and an inner color dot, two additional tests are performed: (1) The number of marked pixels in both the 
inner dot and the outer ring are determined and their ratio is computed. In our case the diameter of 
the outer ring is 3 times the diameter of the inner dot, so the ratio of marked pixels must be close 
to 3x3-1=8. If not, the candidate is rejected. (2) If the centers of mass of the outer and inner regions 
are not close enough, the landmark may be partially occluded or clipped (explained below). The candidate 
is rejected.  For accepted candidates, the center of mass of the inner dot is taken as the center of 
the landmark. Using the center of only the inner dot instead of the average of the centers of the inner 
and outer areas is advantageous when a landmark becomes partially occluded. In such a case the outer 
dot will become occluded first, but as long as the landmark passes test (2), the center will be computed 
correctly. When the occluding object starts approaching the center dot, the center of mass of the outer 
ring shifts noticeably, and the candidate fails test (2) and is rejected (Figure 5). If we did not reject 
these landmarks, then the center would drift before the landmark disappears, corrupting the head pose 
solutions.  7 HEAD POSE DETERMINATION Three cases arise when determining the head pose from landmarks. 
The landmarks represent a set of constraints that is under-determined, well-determined, or over-determined. 
7.1 Under-determined case Until the image analyzer detects at least three different landmarks, the head 
pose cannot be completely determined from landmarks alone. In these cases, the magnetic tracker is the 
primary source of information about head pose. A static position calibration lookup table and on-the-fly 
calibration for the magnetic tracker enable us to use an arsenal of heuristic correctors. These rely 
on the initial head position being reasonably accurate. After a first rough correction via the predicted 
magnetic tracker error, a local, heuristic adjustment is applied to the head pose. Different heuristic 
adjustment methods are used depending on the number of landmarks available. The heuristic adjusters are 
designed to ensure highest possible head pose and registration accuracy even when very few landmarks 
have been detected. They bridge the gap between magnetic-only and vision-based operation of our system. 
The adjusters are designed to improve head pose as smoothly as possible while more and more landmarks 
are detected. As a result of this, the hybrid tracker is characterized by reluctant degradation in accuracy 
when landmarks are lost. When landmarks are re-acquired, the system quickly recovers. A total of six 
different under-determined cases exist for our stereoscopic system. The following list describes the 
basic ideas behind the heuristic adjusters in each case: (1) Camera 1 sees landmark A, camera 2 sees 
no landmarks. This is the simple case described and used in [Bajura95]. The method does not adjust head 
position; it corrects only head orientation by lining up landmark A in the view of camera 1. Only two 
orientation degrees of freedom can be corrected. The remaining, uncorrected orientation degree of freedom 
is best described as rotation about A. (2) Camera 1 sees two landmarks, A and B, camera 2 sees no landmarks. 
The method lines up both A and B in the view of camera 1 by reorienting the head. This orientation correction 
is preceded by a small position correction which is computed to minimize the rotation angle of the following 
orientation correction. In other words, the head is moved to a position from which the landmarks can 
be lined up head  by only minimally changing images and adjusts head position by moving the head to 
the nearest point on a sphere of radius a centered at landmark A. In addition to this position adjustment, 
two out of the three orientation degrees of freedom can be corrected as in (1). (5) Camera 1 sees landmarks 
A and B, camera 2 sees landmark A but not landmark B. This is a hybrid of the methods from (3) and (4). 
The method triangulates landmark A as in (4), thereby determining its distance a from the head. Then 
a position adjustment to minimize orientation change is applied as in (3), but with the additional constraint 
that the position be adjusted towards a point on the sphere of radius a, centered at landmark A s world­space 
position. In addition to this slight position adjustment, all three orientation degrees of freedom can 
be corrected as in (3). (6) Camera 1 sees two landmarks, A and B, camera 2 sees the same two landmarks, 
A and B. Here the triangulation technique from (4) can be applied to both landmarks, yielding two spheres 
of diameters a and b, which are centered at their respective landmarks positions in world space. The 
two spheres intersect in a circle. The head position is adjusted by translating the head to a point on 
the circle from which the 2 landmarks can be lined up in the two views by only minimally correcting head 
orientation. In addition to the slight position change, the three orientation degrees of freedom can 
be adjusted with a method similar to (2).  The above list shows all possible configurations of 1 or 
2 landmarks with a binocular system. As soon as a third landmark is detected in one of the camera views, 
the system switches to the well-determined case described in the next section. 7.2 Well-determined case 
In this section we describe the analytical methods used to determine the head pose when necessary and 
sufficient information is available from the image analyzer. These methods are based on global equation 
solvers.  7.2.1 Global solution Let us consider the head as fixed and the world as attached to landmarks 
that are moving. The actual head motion can be obtained as an inverse transformation of the landmarks 
motions. We need at least 3 positions of non-collinear points to determine a rigid three-space motion. 
Therefore 3 non­collinear landmarks are essential. If we find 3 landmarks on the two cameras image planes, 
that gives us 3 X-Y coordinate pairs. It is not difficult to see that 6 independent values are sufficient 
information to determine a 6-degree-of-freedom rigid motion for the head. Figure 6 shows the geometric 
relationships between two v C1 adjustment, all three orientation degrees of freedom are landmarks v 
C2 and v I v L 1, and 2 v Lv L2 3. in the image of and and three landmarks and Theorientation. In addition 
to the slight position cameras v L12 v L , and the landmark v I 1 is detected at are detected at corrected. 
(3) Camera 1 sees landmark A, camera 2 sees landmark B. v I133 v C v L v C 2. The special case in which 
all three landmarks are detected by v C in the image of 2. This case is similar to (2), except that 
the two landmarks v C1 and B in their respective camera views by reorienting the can consider Figure 6 
as the general case. v V2 v Cv C21 vIvI v V2 v Cv C11 vIvI v V1 appear in different camera views. 
The method lines up A one camera can be treated as a case where 11 21 Therefore we = head after the initial 
position correction. All three The unit direction and 32 v C v CvIvI are obtained 32 vectors orientation 
degrees of freedom can be corrected. Head v V- 1, and v V= v V3 3 - - . - position is adjusted slightly, 
similarly to (2). (4) Camera 1 sees landmark A, camera 2 sees the same simply as: = = , - - landmark 
A. The method computes the distance a from the head to landmark A via triangulation in the 2 camera Figure 
6. Geometric v Since M3 is singular, by substituting z'=1 z into (3), relationships L2between three 
 we get: landmarks and the M'(z') =M0 z'3 +M1z'2 +M2 z'+M3 two stereo cameras. We want z' such that det 
M'(z') =0 . We can find solutions for z' as eigenvalues of the companion matrix of M'(z'): '=1 z' is 
plugged into (2), and an Once we have z, z (x, y) solution pair that satisfies the three equations can 
be found.  7.2.2 Selecting one solution There are eight solutions to our system of equations, so we 
have to find the most sound one among them. In general, imaginary solutions are trivially rejected, and 
the physics of the cameras tell us to discard all negative solutions. We typically find two positive 
solutions. Then the problem is how to disambiguate between these two. The triangle If the image analyzer 
has detected additional landmarks (that is, in addition to the ones used to solve the equations), we 
can use these landmarks for disambiguation. Using each v v v L-L-L is undergoing a rigid motion, hence 
we do not know 12 3 v v v where it is. But, since we know the positions of L1, L2 and L3 remaining 
candidate solution of the camera, we project the from landmark calibration (Section 8), we can compute 
the additional landmarks onto the image planes and check how lengths of the 3 edges. They are: closely 
the projections match the detected positions. This v v v v v v L12 = L2 -L1 , L23 = L3 -L2 and L31 
= L1 -L3 . matching error method works most of the time, but, as shown in [Fischler81], there are degenerate 
cases in which two or more Since both cameras are rigidly mounted on the head set, v v v extra landmarks 
project to exactly the same position in the T=C2 -C1 is also a constant measured through static image. 
In addition, errors in landmark detection prevent us calibration (Section 8). from rejecting solutions 
with small matching errors. However, 3 vv L v v v v the most problematic case occurs when we do not 
have any -C1 , L2 - - L1 C1C2 Let x, yand z be and redundant landmarks, i.e. when we have already used 
all three respectively. The result is: available landmarks for equation solving. In such cases we resort 
to the aid of the magnetic tracker. Unless the two solutions are very close to each other, we can disambiguate 
by selecting the solution that best matches the v v L12 = xV1 -yV2 v v v L23 = yV2 -(T+zV3) L31 = ( 
v T+z v v ) -x V (1) V1 3 magnetic tracker s readings.Taking the square of both sides of (1) results 
in: 22 a + b·x·y+ x+ y= 0 7.3 Over-determined case c+ d·y+ e·z+ f·y·z+ y2 + z2 = 0 (2) Since the equation 
solver uses only the minimum necessary 2 2 number of landmarks, it is sensitive to landmark tracking 
error. g+ h·x+ e·z+ j·x·z+ x+ z= 0 Least square error minimization allows us to find an optimum where 
aK jare constants given by: solution using all the detected landmarks. This process 2 2 neutralizes 
fluctuations in landmark tracking and significantly -L31 v v v T a=-L122 d=-2T·Vg= 2 stabilizes the 
final head pose, thereby yielding superior frame­ v v V23 2 -L2 f=-2V·Vj=-2V·V 23 1 v v v v b=-2V· 
=2T·Vh=-2T· V to-frame coherence in registration. e 1 3 1 v v v v c= v T 2 The optimization process 
is local and depends on the 3 availability of a good initial guess. In any case, the optimizer This 
is a system of equations consisting of 3 quadratic equations with 3 variables and a total degree of 2x2x2=8. 
The solutions of this system can be thought of as the intersection of three ellipsoidal cylinders with 
infinite extents in the x, y and z directions respectively. will converge towards a single solution. 
It is therefore not advisable to use the optimizer in underdetermined cases, due t o the infinite number 
of solutions. Similarly, in well-determined cases, the number of solutions is finite, but invoking the 
optimizer would result in convergence towards a single solution. This would preclude inspecting the multiple 
 v If there is only one camera, i.e. T=0 , then d, eand h solutions with the goal of selecting the best 
one. We therefore vanish. In this special case, the following substitution reduces invoke the optimizer 
only when we are confident that a good (2) into a system with 2 quadratic equations: approximate solution 
has been found via the methods described in Section 7.2.x'=xz and y'=yz [Fischler81]. For the general 
case the solution is more complicated. We use a robust global equation solver that utilizes resultants 
and polynomial matrices to reduce the system to an eigenvalue problem [Manocha94]. First we eliminate 
xand y from the system via Dixon s resultant [Dixon08]. The resultant is a determinant of a 6× 6 matrix 
where each element is up to degree 3 in terms of z . The matrix can be written as a matrix polynomial: 
M(z) =M3 z3 +M2 z2 +M1 z+M0 (3) The mathematical relationships between the user s head, the head-mounted 
camera, a landmark and the projected image of the landmark as seen by the camera are: I .I' Iz '.. x. 
x . (4) .I . I' .y..= .Iy ' z. 7.4 Non-stereo operation . . . . . . L . . . . I' 100 010 v 
 x v . . .. . . . ... . . .. . . ... Rh -Rh .. .. The hybrid tracker can also operate with a single 
camera (non­ stereo). In that case, none of the binocular solution methods . .. . .. . . .. . . .. 
 L y L z 1 Th R -R ' T (5) = c c y c 001 f ' . are applied. This means that only heuristic adjusters 
(1) and (2) z 000 1 from Section 7.1 are used, and only the simplified monocular global three-landmark 
solver is used. Local optimization is In the above equations, performed using only landmarks visible 
in one camera. v Th is a 3D vector representing the position of the head in the   8 STATIC CALIBRATION 
 world space. is a 3x3 rotation matrix representing the orientation of The initial calibration of the 
system determines numerous Rh static parameters that are required by the tracking procedures the head 
in world space. described in Sections 5-7. The following list describes the v Tc is a 3D vector representing 
the position of the camera in static calibration procedures.the head coordinate system. Rc is a 3x3 rotation 
matrix representing the orientation of (1) Camera-to-magnetic-sensor transformation: The transformation 
between a camera and the magnetic the camera in the head coordinate system. tracker s sensor is calculated 
using an iterative procedure f is the focal length. proposed in [Bajura95]. (L ,L ,L ) is the position 
of a landmark in world space. (2) Intrinsic camera parameters: The camera lenses were x yz selected for 
their low distortion characteristics well(Ix ,Iy ) is the projected position of the landmark in image 
below 1% barrel distortion in the corners of the image. space. This allows us to keep the mathematical 
camera model in (I',I',I') is the projected position of the landmark in x yz our system very simple: 
it is a pin-hole model (no homogeneous image space. distortion, no skew, 1:1 aspect ratio). This model 
has I', I' and I' of (4) can be eliminated using (5). Then (4) only three intrinsic degrees of freedom, 
which we define x yzas the 3D coordinates of the center of projection with can be written simply as respect 
to the CCD camera s pixel array. Note that the F = I - P (L ,L ,L )= 0 x x xxyz focal length is in fact 
equal to one of the three Fy = Iy - Py (Lx ,Ly ,Lz )= 0 coordinates. We calibrate these coordinates for 
each where Px and Py are a combined transformation function that camera individually using the vision-based 
tracker. First we position each camera to see as many landmarks as maps a world coordinate to a 2D image 
coordinate. All values possible. Then we execute the landmark tracking v except for Th and Rh are given, 
therefore Fx and Fy are v procedure described in previous sections. The residual functions of Th and 
Rh ; error of the least square optimization is an indicator for v v Fx (Th ,Rh )= 0 and Fy (Th ,Rh 
)= 0 (6) v the accuracy of the estimated intrinsic parameters. An optimization method is then applied 
to find values for the Let (t ,t ,t ) be the three components of Th . Rh has 9 x yz elements, but a rotation 
has only 3 real degrees of freedom. This means we can express Rh as simple rational functions of 3 variables, 
u , v and w . In our implementation, these parameters are defined as follows. First the initial orientation 
is converted to a quaternion, then a hyperplane is defined such that it is tangential to the unit hypersphere 
at the point corresponding to this initial quaternion. Finally u , v and w . are defined as a 3D coordinate 
system in the hyperplane. Hence (6) can also be written as: F (t ,t ,t ,u,v,w)= 0. and F (t ,t ,t ,u,v,w)= 
0 (7) x xyz yxyz If we find n landmark-projection pairs, using (7) we can set up a system of 2n equations 
with 6 variables. Since Ix and Iy are measured values, Fx and Fy may not vanish. Instead, they should 
be considered measurement errors in image space. If the total number of distinct landmarks detected by 
the two cameras is at least 3, and the total number of landmark­projection pairs detected is at least 
4, then this system is overdetermined. In this case we must be able to solve the system as a non-linear, 
least-square minimization problem using iterative methods. To this end, we incorporated an implementation 
of the Levenberg-Marquardt algorithm [More80, Fletcher87] into the system. Since a good initial guess 
is provided by the previously described analytical methods, an optimized solution is computed in only 
a few milliseconds. intrinsic parameters that minimize the residual error. We do not dynamically calibrate 
the intrinsic camera parameters, because producing reliable results would require tracking considerably 
more landmarks than our system can identify [Tsai87]. (3) Interocular Transformation: To calculate the 
transformation between the left and right cameras, we first calibrate the intrinsic parameters as described 
above. Then we operate the hybrid tracker in dual-mono mode, i.e., by tracking and correcting each camera 
individually, as described in Section 7.4. In this mode, the transformation between the cameras is not 
used in the tracking algorithms. It can be computed as the transformation between the cameras coordinate 
systems as they are determined by the vision-based tracker. For accurate results, each of the two cameras 
should see at least three, but preferably more landmarks. We average the data acquired over 10 frames 
to reduce the effect of landmark tracking errors. This interocular calibration procedure is fast enough 
for real time execution if desired. (4) Landmark centers: The world space positions of all the landmark 
centers are acquired using a precise mechanical arm (FARO Metrecom IND-1).  The FARO mechanical arm 
is an auxiliary tracker in our system. It is also used to acquire accurate models for real-world objects 
(for example, the computer model of the cuboids in Figures 2­5). The coordinate system of the mechanical 
arm must be calibrated to the coordinate system of the magnetic system. To this end, we measure a reference 
system with both trackers. The reference is a lab-mounted wooden box.  Figure 7. Virtual and real card 
prisms. Accurate registration makes it possible to acquire an object s texture by projecting the video 
image onto a precisely registered polygonal model of the object. Notice accurate interpenetration of 
the virtual card prism and the (real) gray cuboids. The computer-generated white outlines on the cuboids 
in the background also illustrate the precise registration. Note 3D coordinate axes at the tip of the 
mechanical arm (top right) used to move the virtual card prism. Figure 8. Virtual shadow into the real 
environment. A polygonal model of the sculpture is registered to the real sculpture. The virtual knot 
floating beside it casts a (virtual) shadow onto the sculpture and the ground plane. A tracked light 
source moves real and virtual shadows in sync. Figure 9. Another example of accurate interpenetration: 
the virtual knot penetrates into the gray cuboids and also casts virtual shadows into the scene. The 
landmarks that are occluded by the virtual knot are still used for tracking.  RESULTS AND DISCUSSION 
To evaluate the registration performance of our system, we built the tabletop scene shown in Figures 2-5 
and 7-9. We register the real world cuboids to computer models. The registration errors are represented 
by the distances between computer-generated edges and the corresponding real world edges. Typically these 
errors are below 1 pixel. We demonstrate the registration accuracy of our method in three experimental 
AR systems. Figure 7 demonstrates a 3D copy and paste operation in which a virtual copy is made of a 
real object. The user manipulates the virtual copy of the card prism. Notice that the virtual prism intersects 
with the real cuboids in a convincing manner. For the 3D copy operation, the real card prism is measured 
with the mechanical arm. After each face is defined by digitizing its vertices, a texture is extracted 
from the camera image and is applied to the face. Figure 8 demonstrates a virtual object, a knot, casting 
a shadow on a real object, a sculpture. The geometry of the sculpture was digitized with the mechanical 
arm and placed in the scene. The (real) light source is tracked (by the mechanical arm), and the shadow 
map is calculated in real-time [Segal92]. Figure 9 shows a similar scene. The knot intersects the real 
objects, emphasizing the accurate registration of the synthetic imagery (the knot and its shadow) with 
the real cuboids. We have also used hybrid tracking in an experimental system designed ultimately to 
aid physicians in performing ultrasound-guided needle biopsies [State96]. In such a procedure the physician 
may be attempting to pierce a suspicious lump in a patient s breast. Traditionally ultrasound echography 
images are used to locate the lump and aim the needle. Our experimental system creates a virtual display 
of the lump and the needle. The ultrasound image slice which also contains a computer-enhanced display 
of the target lump, appears to lie within the patient, correctly registered in space. Figure 10 shows 
images from this system. A mechanically tracked needle is being inserted into a training phantom of a 
human breast. This system uses an early version of our hybrid tracker that did not use two-color concentric 
landmarks. It is difficult to quantitatively determine the final camera position and orientation error 
in an AR system. It is nearly impossible to evaluate the accuracy of the intrinsic camera parameter calibration 
and of the interocular transformation calibration procedures. This is due to the fact that ground truth 
values are unavailable. We have therefore implemented a simulator for the camera video images. The simulator 
generates synthetic stereo images, complete with landmarks. The intrinsic parameters for the (simulated) 
cameras are user­settable, as are landmark calibration errors, landmark tracking errors, and magnetic 
tracker errors. Using the simulator, we determined that the intrinsic parameter calibration is very sensitive 
to landmark tracking errors and landmark calibration errors. We also determined that intrinsic parameter 
errors affect interocular calibration accuracy. The system s final camera position and orientation errors 
when used in the tabletop cuboids environment are generally below 2 mm and 0.2 degrees (simulator data). 
This assumes very accurate landmark calibration and image analysis. In practice, camera pose errors are 
larger but seldom exceed 1 cm and 1 degree in overdetermined cases. It is important to note that in this 
system as opposed to AR systems in general [Holloway95] the effects of position and orientation errors 
are not cumulative. Instead, they neutralize each other s influence on registration accuracy in the region 
of space containing landmarks. It follows that our system s registration accuracy is in large part due 
to the design decision to track landmarks in the target images. 10 FUTURE WORK Our system is not without 
limitations. The most important of these is suboptimal performance due to the lack of synchronization 
between the magnetic tracker and the vision­based subsystem. The magnetic tracker s readings lag behind 
the camera video images, which makes the magnetic tracker error grow beyond reasonable values if the 
head moves quickly. Since the landmark predictor does not compute useful landmark search areas in such 
cases, this leads to full-screen searches and thus to noticeable glitches. The obvious way to reduce 
the influence of lag is by using a faster head tracker [Mine93a] and sophisticated prediction algorithms 
[Azuma94]. Delaying the video images [Bajura95] is also possible but undesirable since it increases overall 
system latency. Additional though less severe synchronization problems are due to sequential scanout 
in the video cameras [Mine93b]. Our system does not account for the 17-msec time difference between the 
top and the bottom scanlines of the video images. Nor does it compensate for the latency difference between 
the left and right camera video images The effects of such latency differences could be reduced by time-stamping 
detected landmarks and by reformulating the head pose correctors and solvers to exploit the time stamps. 
Under even lighting conditions (Figure 7), the image analyzer can easily recognize our fluorescent landmarks. 
But despite the use of adaptive brightness evaluation for each landmark, harsh or changing lighting conditions 
(Figure 8) noticeably diminish the analyzer s performance. Landmark recognition reliability and tracking 
accuracy could be improved by building constant-intensity landmarks, such as active (for example back-lit) 
fiducials, or by using retro-reflective materials in combination with an HMD-mounted light source. A 
more realistic camera model incorporating optical distortion should make the system usable with wide-angle 
lenses, thus providing a wide field of view and large stereo overlap. To determine the image-space landmark 
centers more accurately in wide-angle views, perspective correction should be performed on the centers 
coordinates. Finally, our wish list also includes: attaching landmarks to moving objects (in order to 
track object motion simultaneously with camera position and orientation), using the system at a different 
scale (for example, in a room-sized environment), and real-time tracking of visually unobtrusive natural 
features.  ACKNOWLEDGMENTS We wish to express our gratitude to Ronald T. Azuma, Michael Bajura, David 
C. Banks, Gary Bishop, Stephen and Clara Chen, D nardo Colucci, Henry Fuchs, Arthur Gregory, Stefan Gottschalk, 
David Harrison, Marco Jacobs, Fred Jordan, Kurtis Keller, Amy Kreiling, Shankar Krishnan, Alan Liu, Dinesh 
Manocha, Mark McCarthy, Michael North, Stephen M. Pizer, Scott Pritchett, Russell M. Taylor II, Bruce 
Scher, Chris Tector, John Thomas, Greg Turk, Peggy Wetzel, Mary C. Whitton, Scott Williams, Steve Work, 
and Silicon Graphics, Inc. We thank the anonymous reviewers for their comments and criticism. This work 
was supported in part by ARPA DABT63-93-C­0048 ( Enabling Technologies and Application Demonstrations 
for Synthetic Environments ). Approved by ARPA for Public Release Distribution Unlimited. Additional 
partial support was provided by the National Science Foundation Science and Technology Center for Computer 
Graphics and Scientific Visualization (NSF prime contract 8920219). REFERENCES Azuma, R. A Survey of 
Augmented Reality. SIGGRAPH 1995 Course Notes #9 (Developing Advanced Virtual Reality Applications). 
AZUMA, R., BISHOP, G. Improved Static and Dynamic Registration in an Optical See-through HMD. Proceedings 
of SIGGRAPH 94 (Orlando, FL, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 
1994, ACM SIGGRAPH, pp. 197-203. BAJURA, M., NEUMANN, U. Dynamic Registration Correction in Video-Based 
Augmented Reality Systems. IEEE Computer Graphics and Applications (September 1995), pp. 52-60. DIXON, 
A.L. The Elimination of Three Quantics in Two Independent Variables. Proceedings of the London Mathematical 
Society, 6 (1908), 49-69, pp. 209-236. DRASCIC, D. ARGOS: A Display System for Augmenting Reality. ACM 
SIGGRAPH Technical Video Review, Volume 88: InterCHI 1993 Conference on Human Factors in Computing Systems 
(1993). FAUGERAS, O.D., HEBERT, M. The Representation, Recognition and Locating of 3-D Objects. Int. 
J. Robotics Res., 5:3 (1986), pp. 27-52. FISCHLER, M.A., BOLLES, R.C. Random Sample Consensus: A Paradigm 
for Model Fitting with Applications to Image Analysis and Automated Cartography. Communications of the 
ACM, 24:6 (1981), pp. 381-395. FLETCHER, R. Practical Methods of Optimization. John Wiley and Sons, Inc., 
New York (1987). GRIMSON, W.E.L. Object Recognition by Computer: The Role of Geometric Constraints. MIT 
Press, Cambridge (1990). HARALICK, R.M., SHAPIRO, L.G. Computer and Robot Vision, Volume I, Addison-Wesley 
(1993), p. 48. HOLLOWAY, R. Registration Errors in Augmented Reality Systems. Ph.D. dissertation, University 
of North Carolina at Chapel Hill (1995). JANIN, A., ZIKAN, K., MIZELL, D., BANNER, M., SOWIZRAL, H. A 
videometric tracker for augmented reality applications. Proceedings of SPIE, November 1994 (Boston). 
KANCHERLA, A.R., ROLLAND, J.P., WRIGHT, D.L., BURDEA, G. A Novel Virtual Reality Tool for Teaching Dynamic 
3D Anatomy. Proceedings of CVRMed 95 (Nice, France, April 3­5, 1995) pp. 163-169. LIVINGSTON, M., STATE, 
A. Improved Registration for Augmented Reality Systems via Magnetic Tracker Calibration. University of 
North Carolina at Chapel Hill Technical Report TR95-037 (1995). LOWE, D.G. Three-Dimensional Object Recognition 
from Single Two-Dimensional Images. Artificial Intelligence, 31 (1987), pp. 355-395. LOWE, D.G. Robust 
Model-based Motion Tracking Through the Integration of Search and Estimation. International Journal of 
Computer Vision, 8:2 (1992), pp. 113-122. MANOCHA, D. Solving Systems of Polynomial Equations. IEEE Computer 
Graphics and Applications (March 1994), pp. 46-55. MELLOR, J.P. Realtime Camera Calibration for Enhanced 
Reality Visualization. Proceedings of CVRMed 95 (Nice, France April 3-5, 1995), pp. 471-475. MINE, M.R. 
Characterization of End-to-End Delays in Head-Mounted Display Systems. University of North Carolina at 
Chapel Hill Technical Report TR93-001 (1993a). MINE, M.R., BISHOP, G. Just-In-Time Pixels. University 
of North Carolina at Chapel Hill Technical Report TR93-005 (1993b). MORE, J.J., GARBOW, B.S. HILLSTROM, 
K.E. User Guide for MINPACK-1. Argonne National Laboratory Report ANL-80-74 (1980). SEGAL, M., KOROBKIN, 
C., VAN WIDENFELT, R., FORAN, J., HAEBERLI, P. Fast Shadows and Lighting Effects Using Texture Mapping. 
Proceedings of SIGGRAPH 92 (Chicago, IL, July 26-31, 1992). In Computer Graphics, 26, 2 (July 1992), 
ACM SIGGRAPH, New York, 1992, pp. 249-252. SILICON GRAPHICS, INC. Sirius Video Technical Report. Silicon 
Graphics, Inc., Mountain View, CA (1994). STATE, A., LIVINGSTON, M., GARRETT, W.F., HIROTA, G., WHITTON, 
M.C., PISANO, E.D.(MD), FUCHS, H.. Technologies for Augmented-Reality Systems: Realizing Ultrasound-Guided 
Needle Biopsies. Proceedings of SIGGRAPH 96 (New Orleans, LA, August 4-9, 1996). In Computer Graphics 
Proceedings, Annual Conference Series, 1996, ACM SIGGRAPH. SUTHERLAND, I.E. A Head-Mounted Three Dimensional 
Display. Fall Joint Computer Conference (1968), pp. 757­ 764. TSAI, R. Y. A Versatile Camera Calibration 
Technique for High-Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses. IEEE 
Journal of Robotics and Automation, RA-3:4 (August 1987), pp. 323-344. TUCERYAN, M., GREER, D.S., WHITAKER, 
R.T., BREEN, D.E., CRAMPTON, C., ROSE, E., AHLERS, K.H. Calibration Requirements and Procedures for a 
Monitor-Based Augmented Reality System. IEEE Transactions on Visualizations and Computer Graphics, 1:3 
(September 1995), pp. 255-273. UENOHARA, M., KANADE, T. Vision-Based Object Registration for Real-Time 
Image Overlay. 1995 Conference on Computer Vision, Virtual Reality and Robotics in Medicine (Nice, France, 
April 1995), pp. 13-22. WANG, L.-L., TSAI, W.-H. Computing Camera Parameters using Vanishing-Line Information 
from a Rectangular Parallelepiped. Machine Vision and Applications, 3 (1990), pp. 129-141. WARD, M., 
AZUMA, R., BENNETT, R., GOTTSCALK, S., FUCHS, H. A Demonstrated Optical Tracker with Scalable Work Area 
for Head-Mounted Display Systems. Proceedings of the 1992 Symposium on Interactive 3D Graphics (Boston, 
MA, March 1-April 1, 1992), pp. 43-52. YOO, T.S., OLANO, T.M. Instant Hole (Windows into Reality). University 
of North Carolina at Chapel Hill Technical Report TR93-027 (1993).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237283</article_id>
		<sort_key>439</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Technologies for augmented reality systems]]></title>
		<subtitle><![CDATA[realizing ultrasound-guided needle biopsies]]></subtitle>
		<page_from>439</page_from>
		<page_to>446</page_to>
		<doi_number>10.1145/237170.237283</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237283</url>
		<keywords>
			<kw><![CDATA[3D medical imaging]]></kw>
			<kw><![CDATA[BSP tree]]></kw>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[calibration]]></kw>
			<kw><![CDATA[registration]]></kw>
			<kw><![CDATA[stereo video see-through head-mounted display]]></kw>
			<kw><![CDATA[ultrasound echography]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Medical information systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Three-dimensional displays**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010447</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health care information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14103562</person_id>
				<author_profile_id><![CDATA[81100275873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[State]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Departments of Computer Science and University of North Carolina at Chapel Hill CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15030395</person_id>
				<author_profile_id><![CDATA[81100365560]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Livingston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Departments of Computer Science and University of North Carolina at Chapel Hill CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43119139</person_id>
				<author_profile_id><![CDATA[81100340428]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Garrett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Departments of Computer Science and University of North Carolina at Chapel Hill CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42052320</person_id>
				<author_profile_id><![CDATA[81339504645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirota]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Departments of Computer Science and University of North Carolina at Chapel Hill CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P193374</person_id>
				<author_profile_id><![CDATA[81100122627]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Whitton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Departments of Computer Science and University of North Carolina at Chapel Hill CB #3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31090535</person_id>
				<author_profile_id><![CDATA[81342507614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Etta]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Pisano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Radiology, 503 Old Infirmary 226, CB #7510, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43136081</person_id>
				<author_profile_id><![CDATA[81339500019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fuchs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Radiology, 503 Old Infirmary 226, CB #7510, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AZUMA, R. A Survey of Augmented Reality. SIGGRAPH 1995 Course Notes #9 (Developing Advanced Virtual Reality Applications), pp. 20-1 through 20-38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192199</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[AZUMA, R., BISHOP, G. Improving Static and Dynamic Registration in an Optical See-through HMD. Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 197-204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134061</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BAJURA, M., FUCHS, H., OHBUCHI, R. Merging Virtual Objects with the Real World. Proceedings of SIGGRAPH '92 (Chicago, Illinois, July 26- 31, 1992). In Computer Graphics 26, 2 (July 1992), ACM SIGGRAPH, New York, 1992, pp. 203-210.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618306</ref_obj_id>
				<ref_obj_pid>616038</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BAJURA, M., NEUMANN, U. Dynamic Registration Correction in Video- Based Augmented Reality Systems. IEEE Computer Graphics and Applications (September, 1995), pp. 52-60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BARBER, C.B., DOBKIN, D.P., HUHDANPAA, H. The Quickhull Algorithm for Convex Hull. Geometry Center Technical Report GCG53, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BRYSON, S. Measurement and Calibration of Static Distortion of Position Data from 3D Trackers. SPIE Stereoscopic Displays and Applications III, 1992, pp. 244-255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>197972</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CABRAL, B., CAM, N., FORAN, J. Accelerated Volume Rendering and Tomographic Reconstruction Using Texture Mapping Hardware. Proceedings of the 1994 Symposium on Volume Visualization (Washington, D. C., October 1994), pp. 91-97, 131, back cover.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[COLUCCI, D., CHI, V. Computer Glasses: A Compact, Lightweight, and Cost-effective Display for Monocular and Tiled Wide Field-of-View Systems. Proceedings ofSPIE, Vol. 2537, pp. 61-70 (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897857</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[CULLIP, T.J., NEUMANN, U. Accelerating Volume Reconstruction With 3D Texture Hardware. University of North Carolina Department of Computer Science Technical Report TR93-027.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801134</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FUCHS, H., ABRAM, G., GRANT, E. Near Real-time Shaded Display of Rigid Objects. Proceedings of SIGGRAPH '83 (July 1983). In Computer Graphics; 17, 3 (July 1983), ACM SIGGRAPH, New York, 1983, pp. 65- 72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FUCHS, H., KEDEM, Z., NAYLOR, B. On Visible Surface Generation by a Priori Tree Structures. Proceedings of SIGGRAPH '80 (July 1980). In Computer Graphics; 14, 3 (July 1980), ACM SIGGRAPH, New York, 1980, pp. 124-133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[FURNESS, T. The super cockpit and its human factors challenges. Proceedings of the Human Factors Society, 30, 1968, pp. 48-52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897879</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GARRETT, W.F., FUCHS, H., STATE, A., WHITTON, M.C. Real-Time Incremental Visualization of Dynamic Ultrasound Volumes Using Parallel BSP Trees. University of North Carolina Department of Computer Science Technical Report TR96-018.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836007</ref_obj_id>
				<ref_obj_pid>527216</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GHAZlSAEDY, M., ADAMCZYK, D., SANDIN, D., KENYON, R., DEFANTI, T. Ultrasonic Calibration of a Magnetic Tracker in a Virtual Reality Space. Proceedings of the Virtual Reality Annual International Symposium '95, IEEE Computer Society, Los Alamitos, CA, 1995, pp. 179-188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[LALOUCHE, R.C., BICKMORE, D., TESSLER, F., MANKOVICH, H.K., KANGARALOO, H. Three-dimensional reconstruction of ultrasound images. SPIE'89, Medical Imaging, SPIE, 1989, pp. 59-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LIVINGSTON, M.A., STATE, A. Magnetic Tracker Calibration for Improved Registration in Augmented Reality Systems. University of North Carolina Department of Computer Science Technical Report TR95- 037.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LORENSEN, W., CLINE, H., NAFIS, C., KIKINIS, R., ALTOBELLI, D., GLEASON, L. Enhancing reality in the operating room. Proceedings of Visualization 1993 (Los Alamitos, CA, October 1993), pp. 410-415.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MELLOR, J.P. Enhanced Reality Visualization in a Surgical Environment. MS Thesis, MIT Department of Electrical Engineering, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97892</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[NAYLOR, B., AMANATIDES, J., THIBAULT, W. Merging BSP Tree Yields Polyhedral Set Operations. Proceedings of SIGGRAPH '90 (Dallas, Texas, August 1990). In Computer Graphics; 24, 4 (August 1990), pp. 115-124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617875</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[NELSON, T., ELVINS, T. Visualization of 3D Ultrasound Data. IEEE Computer Graphics and Applications, (November, 1993), pp. 50-57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897854</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[OHBUCHI, R., CHEN, D.T., FUCHS, H.Incremental Volume Reconstruction and Rendering for 3D Ultrasound Imaging. University of North Carolina Department of Computer Science Technical Report TR92- 037.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>128951</ref_obj_id>
				<ref_obj_pid>128947</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[ROBINETT, W., ROLLAND, J.P. A Computational Model for the Stereoscopic Optics of a Head-Mounted Display. Presence, Vol. 1, No. 1 (Winter 1992), pp. 45-62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>836012</ref_obj_id>
				<ref_obj_pid>527216</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[ROLLAND, J.P., BIOCCA, F., BARLOW, T., KANCHERLA, A.R. Quantification of Adaptation to Virtual-Eye Location in See-Thru Head- Mounted Displays. Proceedings of the Virtual Reality: Annual International Symposium '95, IEEE Computer Society, Los Alamitos, CA, 1995, pp. 56-66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>212226</ref_obj_id>
				<ref_obj_pid>212189</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[ROSE, E., BREEN, D., AHLERS, K., CRAMPTON, C., TUCERYAN, M., WHITAKER, R., GREER, D. Annotating Real-World Objects Using Augmented Reality. Proceedings of Computer Graphics International 1995 (Leeds, UK, June, 25-30, 1995), pp. 357-370.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617900</ref_obj_id>
				<ref_obj_pid>616031</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SIMS, D. New Realities in Aircraft Design and Manufacture. IEEE Computer Graphics and Applications, (March, 1994), p. 91.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>951156</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[STATE, A., CHEN, D.T., TECTOR, C., BRANDT, A., CHEN, H., OHBUCHI, R., BAJURA, M., FUCHS, H. Case Study: Observing a Volume-Rendered Fetus within a Pregnant Patient. Proceedings of IEEE Visualization '94, October 1994, pp. 364-368, CP-41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237282</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[STATE, A., HIROTA, G., CHEN, D.T., GARRETT, W.F., LIVINGSTON, M.A. Superior Augmented Reality Registration by Integrating Landmark Tracking and Magnetic Tracking. Proceedings of SIGGRAPH '96 (New Orleans, LA, August 4-9, 1996). In Computer Graphics Proceedings, Annual Conference Series, 1996, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199416</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[STATE, A., MCALLISTER, J., NEUMANN, U., CHEN, H., CULLIP, T.J., CHEN, D.T., FUCHS, H. Interactive Volume Visualization on a Heterogeneous Message-Passing Multicomputer. Proceedings of the 1995 Symposium on Interactive 3D Graphics (Monterey, CA, April 1995), pp. 69-74, 208, front cover.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SUTHERLAND, I. A head-mounted three dimensional display. Proceedings of the Fall Joint Computer Conference, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199405</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WLOKA, M.M., ANDERSON, B.G. Resolving Occlusion in Augmented Reality. Proceedings of the 1995 Symposium on Interactive 3D Graphics (Monterey, CA, April 1995), pp. 5-12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Technologies for Augmented Reality Systems: Realizing Ultrasound-Guided Needle Biopsies Andrei State, 
Mark A. Livingston, William F. Garrett, Gentaro Hirota, Mary C. Whitton, Etta D. Pisano, MD* and Henry 
Fuchs Departments of Computer Science and *Radiology University of North Carolina at Chapel Hill http://www.cs.unc.edu/~us/ 
 ABSTRACT We present a real-time stereoscopic video-see-through augmented reality (AR) system applied 
to the medical procedure known as ultrasound-guided needle biopsy of the breast. The AR system was used 
by a physician during procedures on breast models and during non-invasive examinations of human subjects. 
The system merges rendered live ultrasound data and geometric elements with stereo images of the patient 
acquired through head-mounted video cameras and presents these merged images to the physician in a head-mounted 
display. The physician sees a volume visualization of the ultrasound data directly under the ultrasound 
probe, properly registered within the patient and with the biopsy needle. Using this system, a physician 
successfully guided a needle into an artificial tumor within a training phantom of a human breast. We 
discuss the construction of the AR system and the issues and decisions which led to the system architecture 
and the design of the video see-through head-mounted display. We designed methods to properly resolve 
occlusion of the real and synthetic image elements. We developed techniques for real­time volume visualization 
of time-and position-varying ultrasound data. We devised a hybrid tracking system which achieves improved 
registration of synthetic and real imagery and we improved on previous techniques for calibration of 
a magnetic tracker. CR Categories and Subject Descriptors: I.3.7 [Three-Dimensional Graphics and Realism]: 
Virtual Reality, I.3.1: [Hardware Architecture]: Three-dimensional displays, I.3.6 [Methodology and Techniques]: 
Interaction techniques, J.3 [Life and Medical Sciences]: Medical information systems. Additional Keywords 
and Phrases: Augmented reality, stereo video see-through head-mounted display, ultrasound echography, 
3D medical imaging, BSP tree, CB #3175 Sitterson Hall, Chapel Hill, NC 27599-3175. Tel: +1-919-962-1700. 
E-mail: {state, livingst, garrett, hirota, whitton, fuchs}@cs.unc.edu *Department of Radiology, 503 Old 
Infirmary 226, CB #7510, Chapel Hill, NC 27599-7510. Tel: +1-919-966-6957. E-mail: pisano@rad.unc.edu 
Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 calibration, registration.  1 INTRODUCTION Since the early days 
of computer graphics, people have wanted to merge synthetic imagery with their view of the surroundings 
to create an enhanced view of reality. The range of applications that can potentially benefit from augmented 
reality (AR) technology includes architecture, mechanical repair, circuit wiring, and health care. Due 
to a few key technical problems the quality of merged display systems, occlusion conflicts between real 
and synthetic objects, real­time image generation, and registration of real and synthetic objects very 
few AR systems have been placed in users' hands. We made advances on the above issues while building 
an AR system designed to assist a physician with ultrasound­guided needle biopsy of the breast or with 
cyst aspiration. Figure 1 shows the stereo view displayed in the head-mounted display (HMD) worn by the 
physician during the AR-guided insertion procedure. With conventional methods the physician has only 
a non-registered two-dimensional ultrasound image (and perhaps pre-scan 2D medical imagery such as X-ray 
images) to assist her in the inherently three-dimensional task of guiding a needle to a biopsy target. 
Our system displays a synthetic opening, or pit, into the patient and one or more ultrasound slices that 
are emitted by the tracked hand-held ultrasound probe. We hope that presenting such imagery in the proper 
context of patient anatomy will make these widely­practiced needle biopsies easier to perform both in 
the breast and, eventually, also in other, less accessible parts of the body. Figure 2 shows the physician 
examining a patient in our lab and Figure 3 shows the image she sees in her HMD. This paper describes 
the hardware and software of the AR system used by the physician during experiments with training models 
and human subjects. Section 2 briefly summarizes previous work in AR, and Section 3 discusses some of 
the issues in building such systems. We present our system in detail in Section 4. Section 5 focuses 
on issues related to the AR HMD. Section 6 describes how we achieve proper occlusion relationships. Section 7 
presents our new real-time volume visualization technique for ultrasound data. Section 8 discusses techniques 
used to improve registration. We discuss what we have learned from the development and the operation 
of our system in Section 9. We conclude with a discussion of likely future work in Section 10.  2 PREVIOUS 
WORK AR is not a new concept. Ivan Sutherland's original HMD allowed the user to see both the real world 
and virtual objects [Sutherland68]. The VCASS system [Furness86] used an  Figure 1. Real-time stereo 
HMD views from AR system in use during ultrasound-guided needle insertion into phantom training breast. 
Both the needle and the target lesion are visible in the live ultrasound slice attached to the tracked 
hand-held probe. Note synthetic opening into the breast and accurate registration between the needle 
and its image in the ultrasound slice. optical see-through HMD to superimpose flight and target data 
onto a pilot's view. The Boeing Company has a group working on an AR system to guide a technician in 
building a wiring harness for an airplane electrical system [Sims94]. A similar system places text labels 
of engine parts in the user's view when he points at the real object [Rose94]. Medical applications of 
AR include visualization and training for surgical interventions. Such systems have been demonstrated 
by groups at the MIT AI Lab [Mellor95], at Brigham &#38; Women s Hospital [Lorensen93], and elsewhere. 
These systems use MRI or CT data that is collected before the procedure, then registered to the patient 
during the procedure. Previous systems built in our lab have demonstrated AR ultrasound visualization 
[Bajura92, State95, State94]. These earlier systems were limited to non-invasive procedures such as exploratory 
visualization of a fetus in the womb.  3 AR SYSTEM DESIGN To a computer graphics veteran, building our 
AR system may Figure 2. Physician wearing AR HMD with stereo cameras examines patient in preparation 
for biopsy. The ultrasound probe is attached to a mechanical tracking arm. appear straightforward or 
even simple. It appears that all we must do is add a real-time rendering of ultrasound data to a live 
video background. However, realizing a working system requires addressing the technical problems of stereo 
AR, real­time volume rendering of time-and position-varying ultrasound data, and precise registration 
of real and synthetic image elements. In this section, we describe and justify our choice of technologies 
for various system components. 3.1 Merging real and synthetic imagery By definition, an AR system must 
allow the user to see the real world. There are two common technologies for AR HMDs: optical see-through 
and video see-through. In the former, beam splitters (e. g., half-silvered mirrors) optically combine 
light from the environment with computer-generated display elements. In the latter, video cameras mounted 
on the HMD acquire images of the real world which are then electronically combined with computer-generated 
imagery. This can be accomplished via analog technology (e.g. luminance keying or chroma keying) or via 
real-time digital video capture and digital compositing, as in our current system. The combined video 
images are then displayed in the (conventional, opaque ) HMD. More details about these technologies can 
be found in [Azuma95]. For our system, we chose video see-through (a decision which dates back to [Bajura92]). 
The four key advantages of video see-through over optical see-through which led to this decision are: 
(1) support for proper occlusion relationships between real and virtual objects (provided that depth 
of real objects is known or can be determined) [Wloka95]. (2) ability to balance the brightness of synthetic 
and real   Figure 3. Real-time stereo HMD view during patient examination (Figure 2) showing a single 
ultrasound slice properly positioned within the patient's breast. The physician s left index finger points 
towards the cyst (visible as a dark spot in the ultrasound slice). imagery on a pixel-by-pixel basis. 
(3) ability to match the latencies (delays) of synthetic and real imagery [Bajura95]. (4) ability to 
use vision-based registration strategies, such as video tracking of landmarks provided that the video 
image is digitized and available for image processing.  The above items are critical to the design of 
our system. We accept, for now, the drawbacks of video see-through technology: low resolution for the 
real-world imagery and the spatial offset between the user s eyes and the locations of the HMD-mounted 
cameras used to acquire the real world images. These problems will be solved in future HMDs [Colucci95]. 
In video see-through, the cameras introduce several issues not present in optical see-through. The field 
of view and angle of convergence (and thus the amount of stereo overlap) of the cameras should match 
those of the HMD in order for the synthetic imagery to have the same alignment and perspective as the 
real world imagery. This requires measuring the intrinsic parameters of the cameras.  3.2 Tracking Accurate 
tracking is crucial for precise registration of real and synthetic imagery, especially in a medical application 
where surgical intervention is to be performed under AR guidance. In our system we must track the physician 
s head and the hand­held ultrasound probe. In selecting a head tracking system, we wanted to avoid encumbering 
the physician, so we chose a magnetic tracker. Unfortunately, the metallic structures in our lab interfere 
with its accuracy. To overcome this, we combine magnetic tracking with vision-based landmark tracking 
for improved registration. Tracking of the ultrasound probe must be extremely precise for correct registration 
of ultrasound slices (both slice­to-slice and slice-to-patient). The probe is usually positioned over 
a confined area of the patient s body and gathers data only when in contact with the patient, so it has 
a small working and tracking volume. Furthermore, the probe is already tethered to the ultrasound machine. 
We therefore elected to accurately track the probe with a 6-degree-of-freedom mechanical tracker even 
though it hinders probe motion to a certain extent. 3.3 Image generation platform The success of [Cullip93, 
Cabral94] and the availability of fast hardware for rendering texture-mapped polygons led us to choose 
hardware texture mapping for ultrasound data visualization. Our system requires an image generation platform 
that supports real-time video acquisition for three channels: one ultrasound video stream and two camera 
video streams. The latter are inspected by vision-based registration algorithms. On the output side, 
the system must be able to generate two video output streams for the HMD as well as a user interface 
screen.  4 SYSTEM DESCRIPTION 4.1 System Configuration The principal hardware platform of our system 
is a Silicon Graphics Onyx Reality Engine2 graphics workstation (Onyx) equipped with a Sirius Video real-time 
video capture device (Sirius) and a Multi-Channel Option (MCO) that outputs multiple video streams that 
can be used for the left and right eye displays. The Sirius simultaneously acquires video signals from 
a PIE Medical Scanner 200 ultrasound machine (PIE) and from two Panasonic GP-KS102 head-mounted CCD video 
cameras equipped with Cosmicar F1.8 12.5 mm lenses (28° field of view, selected for minimal optical distortion). 
Through its serial ports, the Onyx acquires tracking data from two trackers: a FARO Metrecom IND-1 mechanical 
arm (FARO), which tracks the ultrasound probe, and an Ascension Flock of Birds magnetic tracker for the 
user s head. The Onyx generates stereo video signals to be displayed within the Virtual Research VR-4 
HMD (VR-4). PIE, VR-4 with cameras and Flock sensor, and FARO are all visible in Figure 2. 4.1.1 Video 
Input The Sirius allows simultaneous acquisition and digital processing of two video streams. This constraint 
required us to combine the video streams from the two head-mounted cameras into a single stream for the 
purpose of acquisition (and to devise a software method to split them again after capture for the purpose 
of stereo output). A commercial analog multiplexer (QD Technology QD-1110) combines the camera video 
signals into a single analog signal by selecting odd video fields from one camera and even fields from 
the other. The Sirius has two digital video inputs but only one analog video input. Both the ultrasound 
machine and the multiplexer produce analog video streams. Hence, one of these two streams had to be converted 
into digital format. Since we were willing, at the time, to compromise the ultrasound imaging subsystem 
but not the overall AR feel, and since the PIE s video had to be time-base corrected, we opted to convert 
the ultrasound video to digital format. The time-base correction and the conversion to digital format 
both introduce lag into this video stream. The constraints imposed by our system and by the hardware 
platform are summarized in the following list: (1) The Sirius can capture and process only two video 
streams simultaneously. (2) Video streams captured by the Sirius can be routed to main memory, texture 
memory, or the frame buffer. The frame buffer and texture memory, however, cannot be used as destinations 
at the same time. Hence main memory must be one of the destinations for our two streams, while the other 
can be either frame buffer or texture memory. (3) Separation of a field-multiplexed video stream is 
best done if the stream is captured into the frame buffer (fast). It can also be done if captured into 
main memory (albeit slow), but it is virtually impossible if captured into texture memory. (4) The camera 
video contains landmarks which must be detected; therefore it cannot be captured into texture memory. 
If it is captured into main memory (convenient for inspection), it will have to be copied to the frame 
buffer for display. If it goes into the frame buffer, it will have to be copied into main memory for 
landmark search (moderately slow, since in practice only the areas of the image containing landmarks 
need to be transferred and inspected). (5) The Sirius captures video into texture memory only in a 512× 
1024× 24-bit format, severely limiting the number of frames that can be held at any one time. This is 
unacceptable since we want to be able to simultaneously display multiple textured ultrasound slices. 
However, the ultrasound video images are monochrome and could easily fit into 256× 256× 8 bits of texture 
memory each (with downsampling). Therefore ultrasound video cannot be captured directly into texture 
memory, although it must eventually be loaded into texture memory so that we can take advantage of the 
hardware texturing capability. (6) We had only one Sirius unit and one graphics pipeline. From this 
(over-determined) set of constraints it follows that the (digital) ultrasound video signal must be captured 
into main memory, where it is resampled (by the CPU) into the 256× 256× 8-bit format, and then transferred 
into texture    .......... System-dependent calibrations Experiment-dependent calibrations Real-time 
processes ......... Figure 4. Data flow within stereo AR system for ultrasound visualization memory. 
The combined (analog) camera video signal is captured by the Sirius into the frame buffer. 4.1.2 Tracking 
Input The Flock magnetic tracker and the FARO mechanical tracker are attached to the Onyx via two dedicated 
38,400 baud serial lines. The FARO operates at a maximum rate of 27 Hz, the Flock at 103 Hz (both, however, 
are read asynchronously within the main loop software on the Onyx). The ultrasound probe is mounted on 
the FARO arm with a custom-built mount. The Flock receiver is mounted on a plastic arm which is rigidly 
attached to the stereo camera rig on the VR-4 HMD. 4.1.3 Video Output We selected an MCO configuration 
that simultaneously transmits a 1280× 1024 high-resolution image and two 640× 480 VGA images. The high-resolution 
image is used for the user interface. The two VGA images contain the viewports for camera image capture 
and hence for the AR imagery. The VGA signals are fed via commercial VGA-to-S-Video scan converters (Extron 
Super Emotia) into the left and right eye displays of the VR-4. We also carry these two signals to standard 
VGA monitors in the lab so that people other than the HMD wearer can observe. The monitors for all three 
output signals are visible in Figure 2. 4.2 System Operation Figure 4 shows the data flow within the 
system. Across the top are the input sources. The system captures input data from four different sources 
that sample the real world two video streams (camera, ultrasound) and two tracking data streams (head, 
probe). These four streams are processed into the stereoscopic AR HMD display that is the system's output 
(bottom of the diagram). The upper third of the diagram shows the calibration procedures that must be 
performed before system operation; the lower two thirds depicts the data flow required to produce a stereo 
image pair. 4.2.1 Calibration The first set of calibration procedures are system­dependent calibrations 
of the input video streams and tracking streams. The ultrasound machine is calibrated to determine a 
transformation between pixels in the ultrasound video stream and the local coordinate system of the tracked 
ultrasound probe (a plane equation and scale factors for the ultrasound slice). The procedure also calibrates 
the area of the ultrasound video image that contains scanned data (a polygonal outline for the ultrasound 
slice) [State94]. During system operation, this area is resampled into the 256× 256× 8 format to be loaded 
into texture memory. A calibration procedure similar to [Bajura95] is performed on the camera-sensor 
rig to determine the transformation between the head tracking sensor and the cameras local coordinate 
systems. The cameras intrinsic parameters (location of the center of projection, field of view) are also 
determined, albeit with limited accuracy [State96]. Finally, a transformation between the coordinate 
systems of the magnetic and mechanical trackers is determined by calibrating each to a reference coordinate 
system (a lab­mounted wooden frame). The second set of calibrations are experiment­dependent calibrations. 
First, we sweep the patient's skin with the mechanical tracker and acquire 3D points on the surface. 
Section 6 describes how we use this surface to generate proper occlusion cues. Second, we record the 
precise location of the landmarks used by the hybrid head tracking algorithm (described in Section 8). 
 4.2.2 Real-time processing The bottom two thirds of Figure 4 shows the real-time processes implemented 
on the Onyx. For each (stereo) frame to be generated, the software captures one frame from the ultrasound 
video stream, one frame from the multiplexed camera image stream, and readings from each of the two trackers. 
The ultrasound video defines a texture for the ultrasound slice polygon to be displayed. The slice s 
position and orientation is determined by the probe tracking data together with the off-line calibration 
parameters. The slice polygon is processed by a dual BSP tree algorithm described in Section 7. The multiplexed 
stereo camera video frame is captured into the frame buffer and split into the left and right eye images 
(odd fields into the left eye image, even fields into the right). Each scan line is duplicated in order 
to preserve aspect ratio. The left and right eye areas of the frame buffer correspond to the output regions 
of the MCO s two VGA channels. The Flock report is used to estimate the locations of the landmarks visible 
in the camera images. The landmarks actual positions in the video images are then determined by image 
analysis, and corrected position and orientation data is computed for each camera. Based on this data, 
the pit and other geometric elements are rendered on top of the video image background, as described 
in Section 6. Finally, the ultrasound slices are rendered using the dual BSP tree. The rendering stages 
are executed twice, once for each eye.   5 STEREO VIDEO-SEE-THROUGH HMD Without stereo depth cues, 
the physician user of our system cannot assess the distance to the patient or the depth of a lesion within 
the breast. The construction of a stereo video­see-through HMD (visible in Figure 2) was key to physician 
acceptance of our system and to the start of patient trials. 5.1 Head-mounted cameras We mounted cameras 
on the front of the HMD on top of the housing for the LCD displays. In this arrangement, the camera s 
centers of projection are located approximately 5 cm above and 8 cm in front of the wearer s eyes, who 
must learn to compensate for this constant eye offset. Even after accommodation training, we expect the 
user's performance to be impaired [Rolland95]. For the mount, we used an interpupillary distance of 64 mm 
and chose a fixed convergence angle of 4° . The horizontal field of view of the lenses is 28° , producing 
a stereo overlap of roughly 80% at a working distance of 50 cm. The 4° convergence is a compromise; we 
could achieve 100% overlap with a larger convergence angle (about 7.4° ), but then viewing the images 
inside the HMD would cause eye strain since the display convergence angle does not match the camera convergence 
angle. (The convergence angle of 4° also makes the stereograms in this paper slightly difficult to fuse.) 
The limited light sensitivity of our cameras causes problems for the image analysis technique mentioned 
in Section 8.2. In order to get enough light for good image quality and landmark tracking, the iris of 
the cameras must be opened to the point that the depth of field is less than the depth extent of the 
working volume in our application. We therefore manually adjust the focus on the cameras as necessary. 
 5.2 Head-mounted display The VR-4 weighs over 2 pounds before the 1.5 pounds of camera and fixtures 
are added. Most of this weight is concentrated around the user's eyes, making the device very front-heavy. 
A counterweight provides balance, but nearly doubles the weight of the HMD. The horizontal field of view 
in the VR-4 is approximately 40° for each eye, compared to 28° in each camera. This mismatch leads to 
a telephoto viewing experience similar to that of using binoculars. The VR-4 displays can be set to a 
convergence angle of either zero or three degrees. The stereo images acquired by the Sirius have a resolution 
of 646× 243 for each eye (due to left-right field multiplexing). The complete AR views generated by the 
system have a final resolution of 640× 480 for each eye. However, the resolution of the VR-4 is only 
roughly 250× 230. Within those pixels we map the 256× 256 ultrasound data slice (downsampled from the 
original 512× 512) to a small fraction of the screen about 40× 40 for the image in Figure 1. In Figure 1, 
a 3 mm breast lesion would image on approximately 3× 3 VR4 HMD pixels, and a 22-gauge needle would appear 
0.7 pixels thick. The HMD resolution is hence adequate for training phantom experiments where thicker 
needles can be used but insufficient for human subject trials.  6 OCCLUSION To present correct occlusion 
cues in our visualization, we must enhance the pure RGB color information acquired by the HMD­mounted 
cameras with proper depth (or z) values. We use a set of geometric elements that are rendered in depth 
(z) only (and not in RGB). The top edges of the pit must be spatially aligned with the patient's skin, 
otherwise the pit and the rendering of the ultrasound data will appear pasted on or swimming across the 
patient's skin, rather than appearing (and staying) properly positioned with respect to the patient (Figure 5). 
 For both correct occlusion and correct registration of the pit, we must know the location and shape 
of the patient s skin in 3D lab space. To acquire this information, we sweep the tip of the FARO over 
the patient's skin and collect (unordered) 3D points from the surface. (This assumes that the patient 
will not move during or after the procedure.) The collection procedure is one of the experiment-specific 
calibrations in Section 4.2.1. To resolve occlusion, the unordered set of points must be converted into 
a polygonal surface. Techniques such as Delaunay triangulation can be applied only to 2D arrays of points; 
we therefore exploit the shape of the human body and convert the points into cylindrical coordinates 
using a horizontal cylinder axis that is roughly aligned with the patient's spinal column. Delaunay triangulation 
is then applied in the cylinder s height-angle domain, under the assumption that the surface to be constructed 
is a radius field i. e., that it can be expressed as a function radius = radius(height, angle), similar 
to Cyberware scans. We then resample the mesh output from the Delaunay triangulation into a regular grid 
in the height and angle dimensions. The regular grid of the resampled triangles, together with the cylindrical 
coordinate system in which it is defined, is then used to create a polygonal model of the patient's skin 
surface, as well as a polygonal model of the pit, which is embedded within the skin surface model. Z-rendering 
of the surface model minus the pit opening results in a correct z-buffer for the patient surface. RGBZ-rendering 
of the pit model results in a colored and z-buffered pit; As a result of this process, synthetic elements 
such as the ultrasound slice attached to the transducer can now penetrate into or disappear below the 
skin of the patient, except within the pit, where they remain visible. In addition, we z-render a polygonal 
model for the (FARO­tracked) ultrasound probe, which enables the probe to occlude synthetic image elements 
if it passes in front of them. For example, if the probe is positioned between the HMD cameras and the 
pit, the probe obscures the pit (Figures 1, 3). We do not track other real world elements such as the 
physician s hands, which hold the probe and may also pass between the HMD cameras and the patient s breast, 
so we cannot eliminate all depth conflicts from the user's field of view. Nevertheless, the implementation 
of depth images for certain components of the real world has significantly enhanced our visualization. 
 7 REAL-TIME INCREMENTAL VOLUME VISUALIZATION The AR system must produce stereo visualizations in near 
real­time (at least 10 stereo frames per second) from a dynamic volumetric target (for example, a cyst 
within the breast plus a moving needle). While the dataset is a 3D volume, the data is acquired as a 
sequence of 2D slices. We have implemented a   Figure 6. Dual parallel BSP tree for n = 3. The shaded 
areas represent the tree selected for rendering. Both trees are updated. dynamic volumetric display that 
maintains a set of such slices in the system. During the generation of each output frame, as a new frame 
is acquired from the ultrasound video stream, one new slice is added to the set and the oldest slice 
is removed from it. Thus the system always displays the n most recent slices. Volume reconstruction (even 
incremental reconstruction [Ohbuchi92]) of a set of slices into a regular grid is computationally too 
expensive. Instead, we use the Onyx texturing hardware to visualize the slices as polygons with translucent 
textures. The shape of the polygon is determined as part of the calibrations described in Section 4.2.1. 
The texture for the polygon comes from the ultrasound video frame. Rendering of the volume is accomplished 
by rendering the collection of (possibly intersecting) translucent textured polygons. Due to its hardware 
texturing capability and large texture memory, the Onyx is well-suited for this kind of volume visualization. 
The translucent polygons must be presented to the graphics pipeline in back-to-front order. We use a 
binary space partition (BSP) tree to establish the order [Fuchs80]. The set of n polygons contained in 
the BSP tree constantly changes. We delete a polygonal slice from the BSP tree as each new slice is added. 
Unfortunately deletion of a polygon from a BSP tree is more expensive than insertion, particularly if 
polygon fragmentation and the associated decrease in efficiency are to be avoided. Leaving expired polygons 
in the tree while tagging them as expired (so that they are not rendered) is also problematic when new 
slices are being added at each frame: the size of the tree can grow as the square of the number of insertions 
[Fuchs80]. We solve the BSP tree update problem by constructing and maintaining two BSP trees, out of 
phase in time, in the following manner: Let n be the number of ultrasound slices the user wants displayed 
(Figure 6). As the first n slices arrive during frames 1 through n, they are added to only the first 
tree. When slice n+1 arrives, it is inserted into the first BSP tree, slice 1 is marked expired (but 
is not removed) from the tree, and the second BSP tree is started with this single slice, n+1. With slices 
n+2 through 2n, the new slice is inserted into both trees, the appropriate old slice in the first tree 
is marked expired, and the first tree continues to be rendered. After processing slice 2n, the second 
tree contains (exactly) the most recent n slices and no expired slices and unnecessary fragments to slow 
down the traversal and rendering. The rendering is now switched to the second tree, the first tree is 
deleted (to be initialized next with slice 2n+1). The procedure continues in this way, always rendering 
from the older tree until the newer one contains n slice images. Figure 7 is an HMD view with a volume 
visualization of a lesion within a breast training phantom penetrated by a needle.  8 IMPROVING REGISTRATION 
The breast biopsy task requires very high precision. The physician may be required to place a thin needle 
 for example, 22 gauge (0.7 mm diameter) for cyst aspiration, 14 gauge (2.1 mm diameter) for biopsy 
into a 3 mm cyst. Of the trackers we have tried, none have the accuracy and precision required for this 
medical application. We therefore combine a mechanical tracker (FARO), a magnetic tracker (Flock) corrected 
by a lookup table, and vision-based tracking to achieve improved registration of real imagery (patient, 
ultrasound probe, biopsy needle) and synthetic imagery (ultrasound slices, rendered visual and occlusion 
cues). 8.1 Correction table for magnetic tracker The Flock is the primary head tracker. Since our lab 
has metal in the floor, ceiling, and light fixtures, and since we use metal objects and electric fields 
within the work environment (FARO, PIE), there is a significant amount of static distortion of the magnetic 
field. Distortion is one of the possible sources for the Flock s tracking errors of up to 10 cm in position 
and up to 10° in orientation within our tracking area. Removing the sources of distortion is often not 
desirable (in the case of  Figure 7. Real-time stereo HMD view with ultrasound volume display. A needle 
has been inserted into the breast phantom; the inside of the phantom has been imaged with the probe, 
resulting in a volume representation of the inside of the breast. The needle is registered with its image 
inside the ultrasound volume. The phantom also contains needle traces from previous insertion attempts. 
FARO and PIE) or not possible (floor, ceiling, lights). Expanding upon the work of others [Bryson92, 
Ghazisaedy95], we therefore statically calibrated the Flock. For the calibration procedure, we affix 
the Flock sensor to the FARO (buffered by plastic) and collect sample points at thousands of arbitrary 
locations in the work environment. We then determine the error in the Flock reports by comparing Flock 
and FARO readings. We then resample these error values into a rectilinear look-up table. The calibrated 
Flock performs quite well for position, with errors by 80 percent, down to an average post-correction 
error of 0.5 cm. The calibration does not enjoy such success for orientation correction, however. It 
reduces orientation error by only 40 percent, down to 1.4 degrees on average. Further details can be 
found in [Livingston95].  8.2 Vision-based tracking Even after table-based correction, the Flock is 
not sufficiently accurate for the application s registration requirements. We therefore use a hybrid 
head tracking algorithm and image landmarks to obtain higher accuracy. Our landmarks are fluorescent 
discs positioned in view of the HMD-mounted cameras, typically close to the sterile field (visible in 
Figure 1.). The positions of the landmarks in world space are known; they are calibrated with the FARO 
as part of the experiment­specific calibrations outlined in Section 4.2.1. Landmark tracking is performed 
by the Onyx CPU, using the stereo images captured into the frame buffer by the Sirius. The software attempts 
to predict expected landmark positions. This minimizes the size of the pixel arrays that must be transferred 
from the frame buffer into main memory and searched for landmarks. If a single landmark is detected, 
two out of three degrees of freedom of the camera s orientation can be corrected under simplifying assumptions 
[Bajura95]. With two landmarks, orientation can be corrected fully, again under the same assumptions. 
With three landmarks, camera position and orientation can be determined completely to within a sign. 
With knowledge of the transformation between the camera coordinate systems of the stereo cameras a system-dependent 
calibration not mentioned in Section 4.2.1 the system can even correct both cameras with landmarks detected 
in only one of the cameras. Furthermore, the system stores the correction and applies it to the raw Flock 
reading even if no landmarks are visible. However, the quality of the correction degrades rapidly in 
such cases and the HMD wearer is required to keep the landmarks in view. An improved version of our hybrid 
tracking system is described in [State96]. 9 SUMMARY AND CONCLUSIONS Using the AR system described here, 
a physician successfully guided a biopsy needle into an artificial tumor within a life­sized breast model. 
The system is sufficiently robust and accurate for the physician to report that the procedure on the 
breast model was easy. The key efforts that led to this milestone were the construction of a stereo input 
and stereo output video see-through head-mounted display, methods for properly resolving occlusion between 
real and synthetic objects, a new real-time volume visualization method using parallel BSP trees, a closed-loop 
vision-based head tracking algorithm, and judiciously applied calibration techniques for all input data 
streams (cameras, trackers, ultrasound probe). An AR visualization viewed by an over-the-shoulder observer 
during two patient case studies demonstrated some of the remaining problems with the AR system. The resolution 
of the HMD is insufficient for good visualization of the ultrasound image. The HMD with stereo cameras 
is too heavy. The image landmarks are difficult to keep in view and unoccluded for the physician. Finally, 
the data captured in real-time from the four input streams is not synchronized. While we have significantly 
improved spatial registration, temporal registration (synchronization) remains a problem. 10 FUTURE WORK 
We envision the introduction of a system such as ours into the operating room, but advances in several 
areas are required before this goal can be realized. First, the HMD should be considerably lighter and 
feature higher-quality image acquisition (cameras) as well as higher-resolution displays. In future video-see-through 
HMDs the optical paths of camera and user s eye should be aligned, in order to eliminate the eye offset 
problem. This can be accomplished by folding the camera s optical path with mirrors. Second, head tracking 
should be more accurate and less dependent on (or even completely independent of) landmarks. We are investigating 
a better orientation calibration for the magnetic tracker and are considering alternatives such as optical 
trackers, to be used alone or as part of a hybrid tracking technique. The input streams must be synchronized 
(temporally registered). Data from these is currently captured asynchronously, at discrete intervals, 
from the four input devices. Ideally, all four signals should sample the real world at the same moment 
in time, thus ensuring that the stereo AR display shows a consistent enhanced view of the real world, 
albeit delayed with respect to the real world by the time it took to synthesize the view. In practice, 
each of the streams has a certain amount of lag associated with it. Precise knowledge about the lag in 
each stream holds the potential for eliminating or compensating for lag differences between streams. 
We have begun to devise experiments and software organization strategies for this purpose. An operative 
system should address a number of additional problems. The noisy quality of ultrasound images of human 
tissue makes targets such as cysts or tumors difficult to recognize and even more challenging to visualize 
volumetrically. (The image in Figure 7 was acquired in a training phantom.) It is therefore necessary 
to explore techniques for improved real-time identification (segmentation) and visualization of cysts 
and lesions in human breast tissue. Finally, methods to track the skin surface and its deformations in 
real time are required.  ACKNOWLEDGMENTS It takes many people to realize a complex system like the 
one described here. We wish to express our gratitude to John Airey, Ronald T. Azuma, Michael Bajura, 
Andrew Brandt, Gary Bishop, David T. Chen, D nardo Colucci, Darlene Freedman, Arthur Gregory, Stefan 
Gottschalk, David Harrison, Linda A. Houseman, Marco Jacobs, Fred Jordan, Kurtis Keller, Amy Kreiling, 
Shankar Krishnan, Dinesh Manocha, Mark McCarthy, Michael North, Ryutarou Ohbuchi, Stephen M. Pizer, Scott 
Pritchett, Russell M. Taylor II, Chris Tector, Kathy Tesh, John Thomas, Greg Turk, Peggy Wetzel, Steve 
Work, the anonymous patients, the Geometry Center at the University of Minnesota, PIE Medical Equipment 
B.V., Silicon Graphics, Inc., and the UNC Medical Image Program Project (NCI P01 CA47982). We thank the 
anonymous reviewers for their comments and criticism. This work was supported in part by ARPA DABT63-93-C­0048 
( Enabling Technologies and Application Demonstrations for Synthetic Environments ). Approved by ARPA 
for Public Release Distribution Unlimited. Additional support was provided by the National Science Foundation 
Science and Technology Center for Computer Graphics and Scientific Visualization (NSF prime contract 
8920219). REFERENCES AZUMA, R. A Survey of Augmented Reality. SIGGRAPH 1995 Course Notes #9 (Developing 
Advanced Virtual Reality Applications), pp. 20-1 through 20-38. AZUMA, R., BISHOP, G. Improving Static 
and Dynamic Registration in an Optical See-through HMD. Proceedings of SIGGRAPH 94 (Orlando, Florida, 
July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 
197-204. BAJURA, M., FUCHS, H., OHBUCHI, R. Merging Virtual Objects with the Real World. Proceedings 
of SIGGRAPH '92 (Chicago, Illinois, July 26­31, 1992). In Computer Graphics 26, 2 (July 1992), ACM SIGGRAPH, 
New York, 1992, pp. 203-210. BAJURA, M., NEUMANN, U. Dynamic Registration Correction in Video-Based Augmented 
Reality Systems. IEEE Computer Graphics and Applications (September, 1995), pp. 52-60. BARBER, C.B., 
DOBKIN, D.P., HUHDANPAA, H. The Quickhull Algorithm for Convex Hull. Geometry Center Technical Report 
GCG53, 1993. BRYSON, S. Measurement and Calibration of Static Distortion of Position Data from 3D Trackers. 
SPIE Stereoscopic Displays and Applications III, 1992, pp. 244-255. CABRAL, B., CAM, N., FORAN, J. Accelerated 
Volume Rendering and Tomographic Reconstruction Using Texture Mapping Hardware. Proceedings of the 1994 
Symposium on Volume Visualization (Washington, D. C., October 1994), pp. 91-97, 131, back cover. COLUCCI, 
D., CHI, V. Computer Glasses: A Compact, Lightweight, and Cost-effective Display for Monocular and Tiled 
Wide Field-of-View Systems. Proceedings of SPIE, Vol. 2537, pp. 61-70 (1995). CULLIP, T.J., NEUMANN, 
U. Accelerating Volume Reconstruction With 3D Texture Hardware. University of North Carolina Department 
of Computer Science Technical Report TR93-027. FUCHS, H., ABRAM, G., GRANT, E. Near Real-time Shaded 
Display of Rigid Objects. Proceedings of SIGGRAPH '83 (July 1983). In Computer Graphics; 17, 3 (July 
1983), ACM SIGGRAPH, New York, 1983, pp. 65­ 72. FUCHS, H., KEDEM, Z., NAYLOR, B. On Visible Surface 
Generation by a Priori Tree Structures. Proceedings of SIGGRAPH '80 (July 1980). In Computer Graphics; 
14, 3 (July 1980), ACM SIGGRAPH, New York, 1980, pp. 124-133. FURNESS, T. The super cockpit and its human 
factors challenges. Proceedings of the Human Factors Society, 30, 1968, pp. 48-52. GARRETT, W.F., FUCHS, 
H., STATE, A., WHITTON, M.C. Real-Time Incremental Visualization of Dynamic Ultrasound Volumes Using 
Parallel BSP Trees. University of North Carolina Department of Computer Science Technical Report TR96-018. 
GHAZISAEDY, M., ADAMCZYK, D., SANDIN, D., KENYON, R., DEFANTI, T. Ultrasonic Calibration of a Magnetic 
Tracker in a Virtual Reality Space. Proceedings of the Virtual Reality Annual International Symposium 
'95, IEEE Computer Society, Los Alamitos, CA, 1995, pp. 179-188. LALOUCHE, R.C., BICKMORE, D., TESSLER, 
F., MANKOVICH, H.K., KANGARALOO, H. Three-dimensional reconstruction of ultrasound images. SPIE 89, Medical 
Imaging, SPIE, 1989, pp. 59-66. LIVINGSTON, M.A., STATE, A. Magnetic Tracker Calibration for Improved 
Registration in Augmented Reality Systems. University of North Carolina Department of Computer Science 
Technical Report TR95­ 037. LORENSEN, W., CLINE, H., NAFIS, C., KIKINIS, R., ALTOBELLI, D., GLEASON, 
L. Enhancing reality in the operating room. Proceedings of Visualization 1993 (Los Alamitos, CA, October 
1993), pp. 410-415. MELLOR, J.P. Enhanced Reality Visualization in a Surgical Environment. MS Thesis, 
MIT Department of Electrical Engineering, 1995. NAYLOR, B., AMANATIDES, J., THIBAULT, W. Merging BSP 
Tree Yields Polyhedral Set Operations. Proceedings of SIGGRAPH '90 (Dallas, Texas, August 1990). In Computer 
Graphics; 24, 4 (August 1990), pp. 115-124. NELSON, T., ELVINS, T. Visualization of 3D Ultrasound Data. 
IEEE Computer Graphics and Applications, (November, 1993), pp. 50-57. OHBUCHI, R., CHEN, D.T., FUCHS, 
H. Incremental Volume Reconstruction and Rendering for 3D Ultrasound Imaging. University of North Carolina 
Department of Computer Science Technical Report TR92­ 037. ROBINETT, W., ROLLAND, J.P. A Computational 
Model for the Stereoscopic Optics of a Head-Mounted Display. Presence, Vol. 1, No. 1 (Winter 1992), pp. 
45-62. ROLLAND, J.P., BIOCCA, F., BARLOW, T., KANCHERLA, A.R. Quantification of Adaptation to Virtual-Eye 
Location in See-Thru Head-Mounted Displays. Proceedings of the Virtual Reality: Annual International 
Symposium '95, IEEE Computer Society, Los Alamitos, CA, 1995, pp. 56-66. ROSE, E., BREEN, D., AHLERS, 
K., CRAMPTON, C., TUCERYAN, M., WHITAKER, R., GREER, D. Annotating Real-World Objects Using Augmented 
Reality. Proceedings of Computer Graphics International 1995 (Leeds, UK, June, 25-30, 1995), pp. 357-370. 
SIMS, D. New Realities in Aircraft Design and Manufacture. IEEE Computer Graphics and Applications, (March, 
1994), p. 91. STATE, A., CHEN, D.T., TECTOR, C., BRANDT, A., CHEN, H., OHBUCHI, R., BAJURA, M., FUCHS, 
H. Case Study: Observing a Volume-Rendered Fetus within a Pregnant Patient. Proceedings of IEEE Visualization 
'94, October 1994, pp. 364-368, CP-41. STATE, A., HIROTA, G., CHEN, D.T., GARRETT, W.F., LIVINGSTON, 
M.A. Superior Augmented Reality Registration by Integrating Landmark Tracking and Magnetic Tracking. 
Proceedings of SIGGRAPH 96 (New Orleans, LA, August 4-9, 1996). In Computer Graphics Proceedings, Annual 
Conference Series, 1996, ACM SIGGRAPH. STATE, A., MCALLISTER, J., NEUMANN, U., CHEN, H., CULLIP, T.J., 
CHEN, D.T., FUCHS, H. Interactive Volume Visualization on a Heterogeneous Message-Passing Multicomputer. 
Proceedings of the 1995 Symposium on Interactive 3D Graphics (Monterey, CA, April 1995), pp. 69-74, 208, 
front cover. SUTHERLAND, I. A head-mounted three dimensional display. Proceedings of the Fall Joint Computer 
Conference, 1968. WLOKA, M.M., ANDERSON, B.G. Resolving Occlusion in Augmented Reality. Proceedings of 
the 1995 Symposium on Interactive 3D Graphics (Monterey, CA, April 1995), pp. 5-12.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237284</article_id>
		<sort_key>447</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[Adding force feedback to graphics systems]]></title>
		<subtitle><![CDATA[issues and solutions]]></subtitle>
		<page_from>447</page_from>
		<page_to>452</page_to>
		<doi_number>10.1145/237170.237284</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237284</url>
		<keywords>
			<kw><![CDATA[force feedback]]></kw>
			<kw><![CDATA[friction model]]></kw>
			<kw><![CDATA[haptic]]></kw>
			<kw><![CDATA[interactive graphics]]></kw>
			<kw><![CDATA[intermediate surface representation]]></kw>
			<kw><![CDATA[scientific visualization]]></kw>
			<kw><![CDATA[virtual environment]]></kw>
			<kw><![CDATA[virtual world]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Distributed</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.3</cat_node>
				<descriptor>Real-time and embedded systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010356</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Distributed simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010570</concept_id>
				<concept_desc>CCS->Computer systems organization->Real-time systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39035744</person_id>
				<author_profile_id><![CDATA[81100279370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Mark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175, Sitterson Hall; Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P261099</person_id>
				<author_profile_id><![CDATA[81100344791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Randolph]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175, Sitterson Hall; Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31077942</person_id>
				<author_profile_id><![CDATA[81341489999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175, Sitterson Hall; Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P133728</person_id>
				<author_profile_id><![CDATA[81542839556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Van Verth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175, Sitterson Hall; Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77037436</person_id>
				<author_profile_id><![CDATA[81405594834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Russell]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Taylor]]></last_name>
				<suffix><![CDATA[II]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB #3175, Sitterson Hall; Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>836002</ref_obj_id>
				<ref_obj_pid>527216</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ADACHI, Y., KUMANO, T., OGINO, K. Intermediate Representation for Stiff Virtual Objects. Proc. IEEE Virtual Reality Annual Intl. Symposium '95 (Research Triangle Park, N. Carolina, March 11- 15), pp. 203-210.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BAILEY, M., JOHNSON, D., KRAMER, J. MASSIE, T., TAYLOR, R. So Real I Can Almost Touch It: The Use of Touch as an I/O Device for Graphics and Visualization. SIGGRAPH 96 Course Notes #37 (New Orleans, Louisiana, August 1996).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BATTER, J. J., BROOKS, F. P. JR. GROPE-I: A computer display to the sense of feel. Proc. Intl. Federation of Information Processing Congress '71 (Ljubljana, Yugosolavia, Aug. 23-28). Information Processing '71, vol. 1, pp. 759-763.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97899</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BROOKS, F. P. JR., OUH-YOUNG, M., BATTER, J. J., KILPATRICK, P. J. Project GROPE--Haptic displays for scientific visualization. Proc. SIGGRAPH 90 (Dallas, Texas, Aug. 6-10, 1990). In Computer Graphics 24, 4 (August 1990), pp. 177-185.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BUTTOLO, P., Kt~G, D., ~afom~, B. Manipulation in Real, Virtual and Remote Environments. Proc. IEEE Conf. on Systems, Man and Cybernetics (Vancouver, BC, Oct. 1995), vol. 5, pp. 4656- 4661.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COLGATE, J. E., BROWN, J. M. Factors Affecting the Z-Width of a Haptic Display. Proc. IEEE Intl. Conf. on Robotics and Automation (San Diego, Calif., May 8-13, 1994), vol. 4, pp. 3205-3210.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[COLGATE, J. E., STANLEY, M. C., BROWN, J. M. Issues in the Haptic Display of Tool Use. ASME Haptic Interfaces for Virtual Environment and Teleoperator Systems 1994, In Dynamic Systems and Control 1994 (Chicago, Illinois, Nov. 6-11), vol. 1, pp. 140-144.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199406</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[FINCH, M., CHI, V., TAYLOR, R. M. II, FaLvo, M., WASHBURN, S., SteEP, FINE, R. Surface Modification Tools in a Virtual Environment Interface to a Scanning Probe Microscope. Proc. 1995 Symposium on Interactive 3D Graphics (Monterey, CA, April 9-12, 1995), pp. 13-18.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27013</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Fu, K. S., GONZALEZ R. C., LEE, C. S. G. Robotics control, sensing, vision and intelligence. McGraw-Hill, New York, 1987.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>836001</ref_obj_id>
				<ref_obj_pid>527216</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[GOMEZ, D., BURDEA, G., LANGRANA, N. Integration of the Rutgers Master II in a Virtual Reality Simulation. Proc. IEEE Virtual Reality Annual Intl. Symposium '95 (Research Triangle Park, N. Carolina, March 11-15), pp. 199-202.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[GOSSWEILER, R., LONG, C., KOGA, S., PAUSCH, R. DIVER: A Distributed Virtual Environment Research Platform. Proc. IEEE 1993 Symposium on Research Frontiers in Virtual Reality (San Jose, Calif., Oct. 25-26, 1993), pp. 10-15.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[KIM, W. S., HANNAFORD, B., BEJCZY, A. K. Force-Reflection and Shared Compliant Control in Operating Telemanipulators with Time Delay. IEEE Transactions on Robotics and Automation, April 1992, pp. 176-185.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897843</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[MARK, W. R., RANDOLPH, S. C., FINCH, M., VAN VERTH, J. M. UNC- CH Force-Feedback Library, Revision C. University of North Carolina at Chapel Hill, Computer Science Technical Report #TR96-012, Jan. 30, 1996. {Available at http://www.cs.unc.edu}; Also: ibid, Revision C.2. May 10, 1996. {Not a TR. Available at ftp://ftp.cs.unc.edu/pub/packages/GRIP/armlib}]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[MASSlE, T. M., SALISBURY, J.K. The PHANTOM Haptic Interface: A Device for Probing Virtual Objects. ASME Haptic Interfaces for Virtual Environment and Teleoperator Systems 1994, In Dynamic Systems and Control 1994 (Chicago, Illinois, Nov. 6-11), vol. 1, pp. 295-301.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91451</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[MINSKY M., OUH-YOUNG, M., STEELE, M., BROOKS, F. P. JR., BEHENSKY, M. Feeling and Seeing: Issues in Force Display. Proc. 1990 Symposium on Interactive 3D Graphics (Snowbird, Utah, March 25-28, 1990). In Computer Graphics 24, 2, pp 235-243.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[}V{ITSUISHI, M., HORI, T., HATAMURA, Y., NAGAO, T., KRAMER, B. Operational environment transmission for manufacturing globalization. Proc. 1994 Japan-U.S.A. Symposium on Flexible Automation (Kobe, Japan, July 11-18, 1994), vol. 1, pp. 379-382.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>917155</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[OUH-YOUNG, M., Force Display In Molecular Docking. Ph. D. Dissertation, University of North Carolina at Chapel Hill, UNC-CH Computer Science TR90-004, February, 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[SALCUDEAN, S. E., VLAAR, T. D. On the Emulation of Stiff Walls and Static Friction with a Magnetically Levitated Input/Output Device. ASME Haptic Interfaces for Virtual Environment and Teleoperator Systems 1994, In Dynamic Systems and Control 1994 (Chicago, Illinois, Nov. 6-11), vol. 1, pp. 303-309.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199426</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SALISBURY, K., BROCK, D., MASSIE, T., Swamn', N., ZILLES, C. Haptic Rendering: Programming Touch Interaction with Virtual Objects. Proc. 1995 Symposium on Interactive 3D Graphics (April 9-12, Monterey, Calif.), pp. 123-130.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142824</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[SHAW, C.,LIANG, J, GREEN, M., SUN, Y. The decoupled simulation model for VR systems. Proc. 1992 Conf. on Human Factors in Computer Systems (CHI '92) (Monterey, Calif., May 3-7, 1992), pp. 321-328.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134065</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[SURLES, M.C. An Algorithm With Linear Complexity For Interactive, Physically-based Modeling of Large Proteins. Proc. SIGGRAPH 92 (Chicago, Illinois, July 26-31, 1992). In Computer Graphics, 26, 2 (July 1992), pp. 221-230.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134291</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[SHERIDAN, T.B. Telerobotics, Automation, and Supervisory Control. MIT Press, Cambridge, Mass., 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>537088</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SNYDER, W. E. Industrial Robots: Computer Interfacing and Control. Prentice-Hall, Englewood Cliffs, New Jersey, 1985.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[ZILLES, C. B., SALISBURY, J. K. A Constraint-based God-object Method for Haptic Display. ASME Haptic Interfaces for Virtual Environment and Teleoperator Systems 1994, In Dynamic Systems and Control 1994 (Chicago, Illinois, Nov. 6-11), vol. 1, pp. 146-150.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adding Force Feedback to Graphics Systems: Issues and Solutions William R. Mark1 Scott C. Randolph2 
Mark Finch3 James M. Van Verth4 Russell M. Taylor II5 Department of Computer Science* University of North 
Carolina at Chapel Hill ABSTRACT Integrating force feedback with a complete real-time virtual environment 
system presents problems which are more difficult than those encountered in building simpler force­feedback 
systems. In particular, lengthy computations for graphics or simulation require a decoupling of the haptic 
servo loop from the main application loop if high-quality forces are to be produced. We present some 
approaches to these problems and describe our force-feedback software library which implements these 
techniques and provides other benefits including haptic-textured surfaces, device independence, distributed 
operation and easy enhancement. CR Descriptors: H.1.2 [Models and Principles]: User/Machine Systems; 
C.3 [Special-Purpose and Application-Based Systems]: Real-time systems; I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism Virtual Reality; I.6.8 [Simulation and Modeling]: Types of Simulation Distributed. 
Additional Keywords: haptic, force feedback, friction model, intermediate surface representation, scientific 
visualization, interactive graphics, virtual environment, virtual world. 1. INTRODUCTION As designers 
of interactive computer systems work to increase the information flow between the computer and the user, 
sensory modalities other than vision become increasingly important. One such modality is force feedback. 
The sensing of forces is closely coupled to both the visual system and one s sense of three-dimensional 
space; the eyes and hands work in concert to explore and manipulate objects. * CB #3175, Sitterson Hall; 
Chapel Hill, NC 27599. Tel. +1.919.962.1700 Authors current organizations and contact information: 1 
UNC-CH; markw@cs.unc.edu; www.cs.unc.edu/~markw 2 Spectrum Holobyte; randolph@holobyte.com; www.holobyte.com 
3 Numerical Design, Ltd.; mf@ndl.com; www.ndl.com/ndl 4 Virtus Corp.; jim.van.verth@virtus.com; www.cs.unc.edu/~vanverth 
5 UNC-CH; taylorr@cs.unc.edu; www.cs.unc.edu/~taylorr Permission to make digital or hard copies of part 
or all of this work or personal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to 
lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
Force feedback usefully enhances the capabilities of virtual environment systems; [17] showed that force 
feedback increases productivity in solving rigid-body placement problems and [8] demonstrated an atomic-surface 
modification system which would not have been feasible with graphics alone. Virtual environment force 
displays use models and algorithms described in the robotics and teleoperation literature for low-level 
control see for example [9][22][23]. When combining a computer graphics engine, a simulation, and a force-feedback 
device into one system, there are several areas of concern in addition to that of low-level control. 
The force-feedback component of such a system should: Maintain a high update rate in the force servo 
loop.  Present high quality forces without detectable artifacts.  Transparently support different force-feedback 
devices.  Interface easily and cleanly with the rest of the system.  We discuss some approaches to 
these problems and present the Armlib force-feedback library [13] as one solution.  2. PROBLEMS AND 
SOLUTIONS It has been clearly shown that it is necessary to run the simulation and graphics loops of 
virtual environment (VE) systems asynchronously in order to maintain reasonable display update rates 
(around 20 Hz) in the presence of long simulation computations. [11][20] Such a decoupling is even more 
critical for force display, where update rates of several hundred Hz are required to produce high-quality 
forces. The necessary rate depends somewhat on the characteristics of the force-feedback device and control 
algorithm, but, for example, [1] required an update rate of 500 Hz for their system. If the update rate 
falls below the required minimum, the user begins to notice high-frequency discontinuities and hard surfaces 
become either soft or unstable. We can decouple the simulation and haptic loops on a single machine by 
using either multiple processors or very frequent context switches. However, it is often more practical 
to dedicate one real-time machine to the haptic servo loop, and use other machine(s) for the rest of 
the virtual environment tasks (simulation, high-performance graphics, etc.). This strategy allows each 
machine to be well matched to its task. It also allows for flexible system configuration, which is particularly 
useful in a research environment. The general case of such a split system connects the force­feedback 
device directly to a force server. This server tracks the probe of the force-feedback device (held in 
the user s hand) and executes the force-feedback servo loop. The application connects to this force server 
through some communication channel, retrieving position information from the server and sending descriptions 
of forces or force fields to it. We currently use a TCP/IP Ethernet communications channel because we 
must connect to existing graphics and research equipment; a low-latency, high-bandwidth channel such 
as shared memory would be superior. Kim et al. [12] did the first work in this area, showing that teleoperation 
systems benefit from a decoupling of low-level force servo loops from higher-level control. Adachi et 
al. [1] were the first to apply the technique to virtual environment force-feedback systems. Rather than 
simply supplying a single force vector to the force-feedback controller, they supply an intermediate 
representation (their term, which we adopt) for a force model. This representation is updated infrequently 
by the application code, but is evaluated at a high update rate by the force-feedback controller. Gomez 
et al. [10] demonstrate a system which takes almost the opposite approach. Their main simulation runs 
on the force-feedback machine and sends state updates to a graphics machine. 2.1 Intermediate representations 
The kind of intermediate representation that is most useful depends on the application. A molecular modeling 
system might use spheres of contact. An immersive design system could send a representation of nearby 
surfaces. A simulation meant to teach understanding of physics force fields [3] might send equations 
to the server that describe the field. Mitsuishi et al. [16] demonstrate a remote milling system which 
uses an intermediate representation of average tool force. We describe two general intermediate representations, 
plane and probe and point-to-point spring, and our extensions to these types. Plane and probe In the 
plane and probe model, the force server keeps models of a plane which the probe can contact. [1] When 
the probe penetrates the plane, a restorative spring force that depends on the depth of the penetration 
is applied. The result is a surface with controllable sponginess against which the user can push (see 
Figure 1). Using this model, the application computes a local planar approximation to the surface at 
the user s hand location each time through its main loop. The user feels a firm plane (forces updated 
at ~1 kHz by the force server), while the plane s position is updated more slowly (at ~20 Hz) by the 
application. The increase in local force update rate from 20 Hz to 1 kHz dramatically increases the maximum 
firmness of the surface while maintaining a stable system. Figure 2 shows how this this technique works 
in one application, the Nanomanipulator, which allows the user to control the motion of a microscopic 
tip as it travels over a Force-Feedback Device Force server measures User 1 probe location  k Probe 
   Figure 1: A hard surface is approximated by a plane connected to a spring. When the probe encounters 
the plane, a spring force with spring constant k is applied. Very high values of k produce a surface 
that feels hard. The force is normal to the plane. surface. [8] The initial implementation of the Nanomanip­ulator 
system performed its force computations on the graphics host, using Armlib only to read positions and 
send forces to the force-feedback device. This method restricted the force updates to at most the system 
loop update rate, which was around 20 Hz. The result was either a soft surface with sluggish response 
or an unusably unstable surface. Decoupling the application and force servo loops using our plane and 
probe model resulted in a much more stable and stiff surface. Surface friction and texture The surface 
model just described produces forces which are always perpendicular to the surface. The resulting surfaces 
feel like oiled glass, with the probe tending to slip off convex surface areas and into concave ones. 
Previous researchers have demonstrated the importance of surface friction models in allowing the user 
to explore a surface without slipping. Several researchers [5][18][19] investigated models that combine 
static friction (which holds the probe at a fixed spot) with kinetic friction (which slows the probe 
s movement once it breaks free of the static friction). Adachi et al. [1] model only kinetic friction, 
using a velocity-based term. Minsky et al. [15] model haptic surface textures using a friction-like technique. 
Rather than directly representing variations in surface height, they represent these variations using 
a 2D lateral force field. This field is proportional to the gradient of the surface height function. 
Because their force­feedback device has only two degrees of freedom, lateral forces are not proportional 
to the normal force, as is typical for a friction model. We have implemented a friction model that includes 
both static and kinetic components and can represent simple surface textures. Adjustment of the parameters 
produces surfaces that feel like concrete, sand, rubber, skin, or cloth. The model is rapidly computable, 
allowing a high update rate. Figure 3 shows the parameters of our model graphically; an explanation follows. 
  Atomic Force Microscope Application commands 2 microscope tip to move  Nanomanipulator Application 
 Plane is presented Three samples yield 3  to user, using the a tangent plane to the plane-and-probe 
model surface at contact point Figure 2: The Nanomanipulator application uses surface height readings 
from the microscope tip to determine a local plane approximation which is sent to the force server. 
Figure 3: Surface friction model. The tip slides across the surface against kinetic friction kK until 
it hits a snag. Snags populate the surface with mean distance dMean between them, uniformly distributed 
within dSpread. The tip sticks in the snag, bending with spring constant kStick until moved more than 
dSnap, then it jumps free. Our friction model is that of a surface populated by snags being probed by 
a flexible tip. When the tip is not stuck in a snag, it moves across the surface opposed by a friction 
force that is proportional to the normal force (with coefficient of kinetic friction kK). When the probe 
encounters a snag, it sticks there until the probe moves more than dSnap units away from the sticking 
point, in any tangent direction. While it is snagged, a force tangent to the surface pulls the tip towards 
the center of the snag. This force is proportional to both the normal force and to the distance from 
the snag center (with spring constant kStick). The snags tend to hold the probe in place on the surface. 
This tendency provides a natural station keeping on surfaces with high snag density (such as sandpaper). 
The snags are placed around the surface with a mean distance between snags of dMean, uniformly distributed 
within dSpread. In fact, we populate the surface with snags dynamically. After leaving a snag, the tip 
will encounter another placed with uniform probability between dMean­dSpread/2 and dMean+dSpread/2 units 
away from the first snag, regardless of the tangent direction traveled. Additionally, if the forward 
motion of the tip (movement away from the previous snag) ceases, it is considered to have encountered 
a snag at the point where forward motion stopped. Although actual surfaces could be measured to determine 
parameters for our model, in practice we have explored the parameter space interactively in order to 
produce different surfaces. This parameterized snag distribution, which controls the transition from 
kinetic to static friction, is what sets our friction model apart from previous static/kinetic friction 
models. Salcudean and Vlaar [18] based their kinetic-to-static transition entirely on probe velocity. 
Salisbury et. al. [19] transitioned immediately, without providing steady-state kinetic friction. Our 
technique allows simulation of simple surface textures in addition to modeling standard friction. It 
provides these benefits while using only simple computations that allow us to maintain a high update 
rate. Multiple planes One plane often suffices to model a smooth surface, as it can be continually positioned 
in the correct orientation to provide the normal to the surface at the point of contact. However, a model 
of an object with a sharp inner edge (such as the inside corner of a box) requires multiple planes to 
constrain probe motion in several directions at once. Armlib extends [1] by providing this multi-plane 
capability, although as [24] points out, this technique can result in errors when the planes are not 
at right angles to each other. Multiple probes It is sometimes necessary to simulate a probe that is 
larger than a single point. An application that allows users to feel around in a virtual room with their 
hand is an example of such a system. One virtual probe is created for each finger on the hand and one 
for the palm, allowing the user to feel multiple contacts between the world and the hand (for example, 
resting the hand flat on a virtual desktop). Since the points might be contacting different objects, 
each has its own local surface with which it can collide. For example, when pulling out a chair, the 
thumb may rest on top of the chair while the index finger pulls it away from the table. Since the force-feedback 
device has only one physical probe, the user experiences the sum of the virtual probe forces; the effect 
is similar to sticking a single finger in a very stiff glove. Point-to-point springs Some applications 
allow the user to pick and drag objects which are subject to complex forces. Examples of such objects 
include rigid bodies participating in a many-body simulation and atoms in a protein, which are subject 
to forces determined by a molecular dynamics simulation. Often the calculation of the forces acting on 
the object is so complex that it can only be performed once or twice a second. Furthermore, there is 
usually no rapidly-computable local approximation to these forces which will remain valid for the entire 
interval between full calculations. We approach this problem by implementing a compliant connection between 
the application loop and the force­feedback servo loop. The technique uses a simulated spring to connect 
the probe endpoint to the appropriate body in the simulation, as shown in Figure 4. The user experiences 
forces which are both reasonable and stable. f = k*(x-r) Endpoint moved by application Endpoint moved 
by force server k r (rest length) Figure 4: Point-to-point spring, which couples one point moved by 
the application (at ~1 Hz) to another moved by the force server (at ~1 kHz). The spring constant is k. 
The method is based on that used in [21] for mouse-based interaction. Another member of our lab, Yunshan 
Zhu, implemented it for a force-feedback device using two asychronous loops on the graphics host. That 
success led us t o integrate the technique into Armlib s force server. In this method, the application 
controls the motion of one endpoint of the spring at its slower update rate, while the other endpoint 
follows the probe motion at the force update rate. The spring applies force both to the probe (pulling 
the user s hand towards the point of contact) and to the application (typically adding forces into the 
simulation). Adjustment of the spring constant controls the tightness of the coupling between application 
and probe; a weaker spring produces small forces in the application while a tighter spring causes more 
discontinuity in the force when the application endpoint moves. In order to prevent the user from moving 
the probe too rapidly, we may in the future add adjustable viscosity to the force-server loop. Viscosity 
would tend to keep the probe from moving large distances (and thus adding large forces) between simulation 
time steps. Multiple springs Using a single point of contact between the application and the force server, 
it is only possible to specify forces, not torques. This restriction is overcome by attaching springs 
to multiple application points and multiple virtual probes. Acting together, multiple springs can specify 
a force and general torque on the probe and the application model (see Figure 5).  2.2 Preventing force 
discontinuity artifacts As pointed out in [1], the plane-and-probe model works well only when the plane 
equation is updated frequently compared to the lateral speed of probe motion. As shown in Figure 6, this 
restriction is most severe on sharply-curving surfaces. A sharp discontinuity occurs in the force model 
when the probe is allowed to move large distances before the new surface approximation is computed. If 
the discontinuity leaves the probe outside the surface, the probe drops suddenly onto the new level. 
Worse, if the probe is embedded in the new surface, it is violently accelerated until it leaves the surface 
(and sometimes the user s hand).  ab Figure 6: Probe motion that is rapid compared to the surface curvature 
causes a sharp discontinuity when the new plane equation arrives. Case a shows the free-fall that occurs 
for convex surfaces. The more severe case b shows the sudden force caused by being deeply embedded in 
the surface. To solve the problem of extreme forces when the probe is embedded in the new surface, we 
have developed a recovery time method. This method is applied during the time immediately after new surface 
parameters arrive. If the probe is outside the surface at the time the parameters change, the system 
works as described above, dropping suddenly to the surface. If the probe is within the surface, then 
the normal direction for the force remains as above but the force magnitude is reduced so as to bring 
the tip out of the surface over a period of time, rather than instantaneously. This period of time is 
adjustable, and serves to move the probe out of the surface gently, while still maintaining proper direction 
for the force at all times. Figure 7 illustrates this algorithm. Figure 7: When a new plane equation 
would cause the probe to be embedded in the surface, the recovery time algorithm artificially lowers 
the plane to the probe position then raises it linearly to the correct position over n force loop cycles. 
This method allows the presentation of much stiffer­feeling surfaces (higher spring constant) without 
noticeable discontinuities. By using recovery times of up to 0.05 second, the Nanomanipulator application 
was able to increase the surface spring constant by a factor of 10. A recovery-time algorithm is also 
required in the point-to­point spring model. When the only adjustable parameter is the spring constant, 
there is a trade-off between how tightly the probe is tied to the application endpoint (higher k is better) 
and how smooth the transition is when the application moves its endpoint (lower k is better). We avoid 
this tradeoff by allowing the application to specify the rate of motion for its endpoint after an endpoint 
update. When the application sets a new position for its endpoint (or a new rest length for the spring), 
the point smoothly moves from its current location to the new location over the specified number of server 
loop iterations. 2.3 Flexibility and extensibility Our force-feedback software has evolved from application­specific 
device-driver routines [4], through a device-specific but application-independent library controlling 
our Argonne-III Remote Manipulator, to the current device-independent remote-access library, Armlib. 
Armlib provides connectivity to widely-used graphics engines (SGI, HP and Sun workstations) over commonly-used 
networks (Ethernet and other TCP/IP). It supports commercially-available force displays (several varieties 
of SensAble Devices PHANToM [14], and Sarcos Research Corporation Dexterous Master), as well as our Argonne-III 
Remote Manipulator from Argonne National Laboratories. Armlib supports the simultaneous use of multiple 
force­feedback devices, for multi-user or multi-hand applications. The application selects the device(s) 
it needs to use at runtime. Armlib structure Armlib provides device independence at the API level by 
using a cartesian coordinate system with an origin at the center of the device s working volume. Forces 
and positions can be automatically scaled so that software will work unchanged with devices of different 
sizes. The device independence extends to Armlib s internal structure (Figure 8). Device-dependencies 
are compart­mentalized in a set of simple low-level device-driver routines, which handle the reading 
of joint positions, the writing of joint forces, and the serializing of the robot link configuration. 
Higher levels of the library, including the intermediate representation servo loops, function in cartesian 
space. The conversion from joint space to cartesian space and back is handled by a common set of routines 
which utilize a Denavit-Hartenberg based description of each device to compute the forward kinematics 
and Jacobian matrix at runtime (see e.g. [9]). These routines effectively discard most torque information 
for three DOF devices such as the standard PHANToM. The compartmentalization of device dependencies facilitates 
the addition of both new device types and new library capabilities. Because code for intermediate representations 
uses only cartesian space, this code works automatically for all devices. The ease of making changes 
is illustrated by the fact that it took only two days to add support for the PHANToM device to our library, 
and less than two days to add the code for our spring-based intermediate representation. Application 
Machine Force Server Intermediate Representation Parameters (sent over Ethernet) Figure 8: Armlib s 
structure. parameters pass between the client (application &#38; API) and the force server over the Ethernet. 
The intermediate representation servo loop functions entirely in cartesian space. Device-dependencies 
are contained in the joint-space device-driver. Client/Server communications There are two types of 
information passed between the application and the force server. Commands affecting system state (starting, 
stopping, initiating local force computation) must be delivered intact and not lost. In contrast, position 
reports and updates to intermediate representation parameters are sent frequently, so a lost packet can 
be ignored since a new one will arrive shortly. (In fact, ignoring these lost packets is the correct 
approach; retransmission is time consuming). We decided to use two channels between the client and server, 
the command and data channels. We currently use a TCP stream connection for the command channel (reliable, 
high overhead) and use UDP datagrams (unreliable, low overhead) for the data channel. Our client-server 
communications routines are well­compartmentalized, so the substitution of different protocols would 
be simple. Armlib provides an asychronous continual report mode, in which the server sends position reports 
at regular intervals (using the data channel), rather than on request. This mode avoids the wait for 
a round-trip network message which is required by standard requests. The application can poll for these 
continual reports or block for them. Armlib also provides the application with a file descriptor indicating 
report arrival which can be select() d by event-driven applications such as those written under X-Windows. 
Performance All of the intermediate representation features (plane-vs.­probe, multiple probes, recovery 
time, friction, and point-to­point springs) are orthogonal, and can be used singly or in combination. 
These tools produce a rich interaction environment at high update rates. We achieve 1 kHz on a 133 MHz 
Pentium processor for our custom six sense-DOF, three force-DOF PHANToM. The rate is even higher on a 
standard PHANToM. When we use a recovery time with our plane-and­probe model, we achieve stable hard 
surface stiffnesses of 2100 N/m on our custom PHANToM.   3. RESULTS AND SIGNIFICANCE We have presented 
a system-based approach to solving the problems encountered when integrating force feedback into real-time 
computer graphics applications. Armlib combines and extends earlier work in intermediate representation 
of surfaces and in surface friction. It: Extends the intermediate representation of [1] by adding multiple 
surfaces.  Introduces point-to-point springs as a form of intermediate representation on a force server. 
 Extends previous work on surface friction by adding to the friction model the capability to produce 
simple haptic textures at high update rates.  Armlib also provides new functions and features. It: 
Presents a recovery-time algorithm to reduce sudden forces due to changes in the intermediate representation, 
both for local planes and point-to-point springs.  Provides device independence, a simple interface, 
and easy extensibility through a compartmentalized and multi­layered design.  Provides a fully-functional 
system running on readily­available networks and commercial hardware. Several groups outside our lab 
are already using our system.  4. FUTURE DIRECTIONS The development of Armlib is driven by the needs 
of particular applications in our lab; we add capabilities as they become necessary. Use of existing 
library features to explore new areas is already underway. The Nanomanipulator project is working to 
adjust the friction parameters based on characteristics of the surface under the microscope. One useful 
intermediate representation we would like to add is a 3-D linear approximation to the nearby force field; 
i.e. a first order Taylor series expansion of the force field about the most recent position. Such a 
representation would be useful for smoothly varying force fields. Some applications might benefit from 
an enhancement of our plane-and-probe model to allow for half-planes, or even general convex planar polygons. 
This capability would allow several plane-and-probe constraints to be used simultaneously at convex points 
of intersection, such as the outside of a box. But this approach can also produce problems of its own; 
Zilles and Salisbury [24] provide a good discussion of some of these issues and discuss a technique to 
attack them. It might be worthwhile in some applications to support simple curved surfaces as an intermediate 
representation type. The implementation of the force servo loop for our plane and probe model is relatively 
simple. Work by Colgate and Brown [6] provides guidance on how to do better in attacking this virtual 
wall problem. Doing so would require providing our library with information about the dynamic behavior 
of each supported force-feedback device. We would also like to add a braking pulse like that described 
by Salcudean and Vlaar [18] to our virtual wall. Armlib works with very simple intermediate representations. 
There is a continual temptation to add progressively more complex intermediate representations and associated 
calculations. There is of course a tradeoff in doing so more complex representations take longer to evaluate, 
thus reducing the force update rate. A possible solution is to add another layer to our system. Such 
a layer might be in charge of object-level contacts and dynamics, and the calculation of the plane equations 
for our intermediate representation. It could address the fact that under some circumstances multiple 
simultaneous contacts should not be treated independently [7]. This layer would still run faster than 
the application main loop, but would be more complex (and thus slower) than the force server s intermediate 
representation servo loop. 5. AVAILABILITY The latest information on Armlib is available from our haptics 
research web page, http://www.cs.unc.edu/Research/graphics/ force. The Armlib source code and documentation 
are available by FTP at ftp://ftp.cs.unc.edu/pub/packages/GRIP/armlib/. A SIGGRAPH course this year [2] 
presents some additional tutorial information about Armlib.  ACKNOWLEDGEMENTS Support for this work 
was provided by grant number RR02170 from the National Institutes of Health National Center for Research 
Resources. Our SARCOS arm was provided by DARPA. The Argonne Remote Manipulator is on loan from Argonne 
National Laboratories. We would like to thank other contributors to our work. Frederick P. Brooks, Jr. 
and William V. Wright, the investigators for our NIH grant, provided support and ideas. Other students, 
in particular Yunshan Zhu, Kimberly Passarella-Jones, and Chris Dimattia contributed to Armlib and the 
ideas presented here. John Hughes attended to our force-feedback hardware. Finally, the anonymous reviewers 
(and one in particular) made some very helpful suggestions. REFERENCES [1] ADACHI, Y., KUMANO, T., OGINO, 
K. Intermediate Representation for Stiff Virtual Objects. Proc. IEEE Virtual Reality Annual Intl. Symposium 
95 (Research Triangle Park, N. Carolina, March 11­15), pp. 203-210. [2] BAILEY, M., JOHNSON, D., KRAMER, 
J. MASSIE, T., TAYLOR, R. So Real I Can Almost Touch It: The Use of Touch as an I/O Device for Graphics 
and Visualization. SIGGRAPH 96 Course Notes #37 (New Orleans, Louisiana, August 1996). [3] BATTER, J. 
J., BROOKS, F. P. JR. GROPE-I: A computer display to the sense of feel. Proc. Intl. Federation of Information 
Processing Congress 71 (Ljubljana, Yugosolavia, Aug. 23-28). Information Processing 71, vol. 1, pp. 759-763. 
[4] BROOKS, F. P. JR., OUH-YOUNG, M., BATTER, J. J., KILPATRICK, P. J. Project GROPE Haptic displays 
for scientific visualization. Proc. SIGGRAPH 90 (Dallas, Texas, Aug. 6-10, 1990). In Computer Graphics 
24, 4 (August 1990), pp. 177-185. [5] BUTTOLO, P., KUNG, D., HANNAFORD, B. Manipulation in Real, Virtual 
and Remote Environments. Proc. IEEE Conf. on Systems, Man and Cybernetics (Vancouver, BC, Oct. 1995), 
vol. 5, pp. 4656­4661. [6] COLGATE, J. E., BROWN, J. M. Factors Affecting the Z-Width of a Haptic Display. 
Proc. IEEE Intl. Conf. on Robotics and Automation (San Diego, Calif., May 8-13, 1994), vol. 4, pp. 3205-3210. 
[7] COLGATE, J. E., STANLEY, M. C., BROWN, J. M. Issues in the Haptic Display of Tool Use. ASME Haptic 
Interfaces for Virtual Environment and Teleoperator Systems 1994, In Dynamic Systems and Control 1994 
(Chicago, Illinois, Nov. 6-11), vol. 1, pp. 140-144. [8] FINCH, M., CHI, V., TAYLOR, R. M. II, FALVO, 
M., WASHBURN, S., SUPERFINE, R. Surface Modification Tools in a Virtual Environment Interface to a Scanning 
Probe Microscope. Proc. 1995 Symposium on Interactive 3D Graphics (Monterey, CA, April 9-12, 1995), pp. 
13-18. [9] FU, K. S., GONZALEZ R. C., LEE, C. S. G. Robotics control, sensing, vision and intelligence. 
McGraw-Hill, New York, 1987. [10] GOMEZ, D., BURDEA, G., LANGRANA, N. Integration of the Rutgers Master 
II in a Virtual Reality Simulation. Proc. IEEE Virtual Reality Annual Intl. Symposium 95 (Research Triangle 
Park, N. Carolina, March 11-15), pp. 199-202. [11] GOSSWEILER, R., LONG, C., KOGA, S., PAUSCH, R. DIVER: 
A Distributed Virtual Environment Research Platform. Proc. IEEE 1993 Symposium on Research Frontiers 
in Virtual Reality (San Jose, Calif., Oct. 25-26, 1993), pp. 10-15. [12] KIM, W. S., HANNAFORD, B., BEJCZY, 
A. K. Force-Reflection and Shared Compliant Control in Operating Telemanipulators with Time Delay. IEEE 
Transactions on Robotics and Automation, April 1992, pp. 176-185. [13] MARK, W. R., RANDOLPH, S. C., 
FINCH, M., VAN VERTH, J. M. UNC-CH Force-Feedback Library, Revision C. University of North Carolina at 
Chapel Hill, Computer Science Technical Report #TR96-012, Jan. 30, 1996. [Available at http://www.cs.unc.edu]; 
Also: ibid, Revision C.2. May 10, 1996. [Not a TR. Available at ftp://ftp.cs.unc.edu/pub/packages/GRIP/armlib] 
[14] MASSIE, T. M., SALISBURY, J. K. The PHANToM Haptic Interface: A Device for Probing Virtual Objects. 
ASME Haptic Interfaces for Virtual Environment and Teleoperator Systems 1994, In Dynamic Systems and 
Control 1994 (Chicago, Illinois, Nov. 6-11), vol. 1, pp. 295-301. [15] MINSKY M., OUH-YOUNG, M., STEELE, 
M., BROOKS, F. P. JR., BEHENSKY, M. Feeling and Seeing: Issues in Force Display. Proc. 1990 Symposium 
on Interactive 3D Graphics (Snowbird, Utah, March 25-28, 1990). In Computer Graphics 24, 2, pp 235-243. 
[16] MITSUISHI, M., HORI, T., HATAMURA, Y., NAGAO, T., KRAMER, B. Operational environment transmission 
for manufacturing globalization. Proc. 1994 Japan-U.S.A. Symposium on Flexible Automation (Kobe, Japan, 
July 11-18, 1994), vol. 1, pp. 379-382. [17] OUH-YOUNG, M., Force Display In Molecular Docking. Ph. D. 
Dissertation, University of North Carolina at Chapel Hill, UNC-CH Computer Science TR90-004, February, 
1990. [18] SALCUDEAN, S. E., VLAAR, T. D. On the Emulation of Stiff Walls and Static Friction with a 
Magnetically Levitated Input/Output Device. ASME Haptic Interfaces for Virtual Environment and Teleoperator 
Systems 1994, In Dynamic Systems and Control 1994 (Chicago, Illinois, Nov. 6-11), vol. 1, pp. 303-309. 
[19] SALISBURY, K., BROCK, D., MASSIE, T., SWARUP, N., ZILLES, C. Haptic Rendering: Programming Touch 
Interaction with Virtual Objects. Proc. 1995 Symposium on Interactive 3D Graphics (April 9-12, Monterey, 
Calif.), pp. 123-130. [20] SHAW, C., LIANG, J, GREEN, M., SUN, Y. The decoupled simulation model for 
VR systems. Proc. 1992 Conf. on Human Factors in Computer Systems (CHI 92) (Monterey, Calif., May 3-7, 
1992), pp. 321-328. [21] SURLES, M. C. An Algorithm With Linear Complexity For Interactive, Physically-based 
Modeling of Large Proteins. Proc. SIGGRAPH 92 (Chicago, Illinois, July 26-31, 1992). In Computer Graphics, 
26, 2 (July 1992), pp. 221-230. [22] SHERIDAN, T. B. Telerobotics, Automation, and Supervisory Control. 
MIT Press, Cambridge, Mass., 1992. [23] SNYDER, W. E. Industrial Robots: Computer Interfacing and Control. 
Prentice-Hall, Englewood Cliffs, New Jersey, 1985. [24] ZILLES, C. B., SALISBURY, J. K. A Constraint-based 
God-object Method for Haptic Display. ASME Haptic Interfaces for Virtual Environment and Teleoperator 
Systems 1994, In Dynamic Systems and Control 1994 (Chicago, Illinois, Nov. 6-11), vol. 1, pp. 146-150. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237285</article_id>
		<sort_key>453</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[Image-guided streamline placement]]></title>
		<page_from>453</page_from>
		<page_to>460</page_to>
		<doi_number>10.1145/237170.237285</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237285</url>
		<keywords>
			<kw><![CDATA[flow visualization]]></kw>
			<kw><![CDATA[random descent]]></kw>
			<kw><![CDATA[random optimization]]></kw>
			<kw><![CDATA[streamline]]></kw>
			<kw><![CDATA[vector field visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003716</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003716</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Mathematical optimization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39080372</person_id>
				<author_profile_id><![CDATA[81100457973]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Turk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31043335</person_id>
				<author_profile_id><![CDATA[81100462296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Banks]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mississippi State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1095597</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bertin, Jacques, Semiology of Graphics, translated from French, The University of Wisconsin Press 1983.]]></ref_text>
				<ref_id>Bertin 83</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949612</ref_obj_id>
				<ref_obj_pid>949607</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bryson, Steve and Creon Levit, "The Virtual Wind Tunnel: An Environment for the Exploration of Three-Dimensional Unsteady Flows," Proceedings Visualization '91, San Diego, California, October 22-25, pp. 17-24.]]></ref_text>
				<ref_id>Bryson &amp;Levit 91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166151</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cabral, Brian and Leith (Casey) Leedom, "Imaging Vector Fields Using Line Integral Convolution," Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH '93), pp. 263-270.]]></ref_text>
				<ref_id>Cabral &amp; Leedom 93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147150</ref_obj_id>
				<ref_obj_pid>147130</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Crawfis, Roger and Nelson Max, "Direct Volume Visualization of Three Dimensional Vector Fields," Proceedings of the 1992 Workshop on Volume Visualization, pp. 55-60.]]></ref_text>
				<ref_id>Crawfis &amp; Max 92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>833880</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[de Leeuw, Willem C., and Jarke van Wijk, "Enhanced Spot Noise for Vector Field Visualization," Proceedings Visualization '95, Atlanta, Georgia, Oct. 29 - Nov. 3, pp. 233-239.]]></ref_text>
				<ref_id>de Leeuw &amp; van Wijk 95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951115</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Delmarcelle, Thierry and Lambertus Hesselink, "The Topology of Symmetric, Second-Order Tensor Fields," Proceedings Visualization '94, Washington, D.C., October 17-21, pp. 140-147.]]></ref_text>
				<ref_id>Delmarcelle &amp; Hesselink 94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>833878</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dovey, Don, "Vector Plots for Irregular Grids," Proceedings Visualization '95, Atlanta, Georgia, Oct. 29 - Nov. 3, pp. 248-253.]]></ref_text>
				<ref_id>Dovey 95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Feibush, Eliot, Marc Levoy and Robert Cook, "Synthetic Texturing Using Digital Filters," Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH '80), pp. 294- 301.]]></ref_text>
				<ref_id>Feibush et al. 80</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Feynman, Richard R, Robert B. Leighton and Matthew Sands, The Feynman Lectures on Physics, Addison-Wesley, Reading, Massachusetts, 1964.]]></ref_text>
				<ref_id>Feynman 64</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951132</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Forsell, Lisa K., "Visualizing Flow over Curvalinear Grid Surfaces Using Line Integral Convolution," Proceedings Visualization '94, Washington, D.C., October 17-21, pp. 240-247.]]></ref_text>
				<ref_id>Forssell 94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Fowler, David and Colin Ware, "Strokes for Representing Univariate Vector Field Maps," Graphics Interface '89, London, Ontario, June 19-23, 1989, pp. 249-253.]]></ref_text>
				<ref_id>Fowler &amp; Ware 89</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949614</ref_obj_id>
				<ref_obj_pid>949607</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Globus, A., C. Levit and T. Lasinski, "A Tool for Visualizing the Topology of Three-Dimensional Vector Fields," Proceedings Visualization '91, San Diego, California, October 22- 25, pp. 33-40.]]></ref_text>
				<ref_id>Globus et al. 91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617655</ref_obj_id>
				<ref_obj_pid>616017</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Helman, J. L. and L. Hesselink, "Visualization of Vector Field Topology in Fluid Flows," IEEE Computer Graphics and Applications, Vol. 11, No. 3, pp. 36-46.]]></ref_text>
				<ref_id>Helman &amp; Hesselink 91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hoppe, Hugues, Tony DeRose, Tom Duchamp, John McDonald and Werner Stuetzel, "Mesh Optimization," Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH '93), pp. 19-26.]]></ref_text>
				<ref_id>Hoppe et al. 93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Kundu, Pijush K., Fluid Mechanics, Academic Press, Inc., San Diego, 1990.]]></ref_text>
				<ref_id>Kundu 90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951133</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Max, Nelson, Roger Crawfis and Charles Grant, "Visualizing 3D Velocity Fields Near Contour Surfaces," Proceedings Visualization' 94, Washington, D.C., October 17-21, pp. 248- 255.]]></ref_text>
				<ref_id>Max et al. 94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Saito, Takafumi and Tokiichiro Takahashi, "Comprehensible Rendering of 3-D Shapes," Computer Graphics, Vol. 24, No. 4 (SIGGRAPH '90), pp. 197-206.]]></ref_text>
				<ref_id>Saito &amp; Takahashi 90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Salisbury, Michael R, Sean E. Anderson, Ronen Barzel and David H. Salesin, "Interactive Pen-and-Ink Illustration', Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH '94), pp. 101-108.]]></ref_text>
				<ref_id>Salisbury et al. 94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218448</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Stalling, Detlev and Hans-Christian Hege, "Fast and Resolution Independent Line Integral Convolution," Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH '95), pp. 249-256.]]></ref_text>
				<ref_id>Stalling &amp; Hege 95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Turk, Greg, "Generating Textures on Arbitrary Surfaces Using Reaction-Diffusion," Computer Graphics, Vol. 25, No. 4 (SIGGRAPH '91), pp. 289-298.]]></ref_text>
				<ref_id>Turk 91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122751</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[van Wijk, Jarke J., "Spot Noise: Texture Synthesis for Data Visualization," Computer Graphics, Vol. 25, No. 4 (SIGGRAPH '91), pp. 309-318.]]></ref_text>
				<ref_id>van Wijk 91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, Georges and David H. Salesin, "Computer-Generated Pen-and-Ink Illustrations," Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH '94), pp. 91-98.]]></ref_text>
				<ref_id>Winkenbach &amp; Salesin 94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192227</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Witkin, Andrew and Paul Heckbert, "Using Particles to Sample and Control Implicit Surfaces," Computer Graphics Proceedings, Annual Conference Series (SIGGRAPH 94), pp. 269-277.]]></ref_text>
				<ref_id>Witkin &amp; Heckbert 94</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image-Guided Streamline Placement Greg Turk, University of North Carolina at Chapel Hill David Banks, 
Mississippi State University Abstract Accurate control of streamline density is key to producing several 
effective forms of visualization of two-dimensional vector fields. We introduce a technique that uses 
an energy function to guide the placement of streamlines at a specified density. This energy func­tion 
uses a low-pass filtered version of the image to measure the difference between the current image and 
the desired visual den­sity. We reduce the energy (and thereby improve the placement of streamlines) 
by (1) changing the positions and lengths of stream­lines, (2) joining streamlines that nearly abut, 
and (3) creating new streamlines to fill sufficiently large gaps. The entire process is iter­ated to 
produce streamlines that are neither too crowded nor too sparse. The resulting streamlines manifest a 
more hand-placed ap­pearance than do regularly- or randomly-placed streamlines. Ar­rows can be added 
to the streamlines to disambiguate flow direc­tion, and flow magnitude can be represented by the thickness, 
den­sity, or intensity of the lines. CR Categories: I.3.3 [Computer Graphics]: Picture/Image genera­tion; 
I.4.3 [Image Processing]: Enhancement. Additional Key Words: Vector field visualization, flow visualiza­tion, 
streamline, random optimization, random descent.  1 Introduction The need to visualize vector fields 
is common in many scientific and engineering disciplines. Examples of vector fields include ve­locities 
of wind and ocean currents (e.g., for weather forecasting), results of fluid dynamics simulation (e.g., 
for calculating drag over a body), magnetic fields, blood flow, components of stress and strain in materials, 
and cell migration during embryo development. Ex­isting techniques for vector field visualization differ 
in how well they represent such attributes of the vector field as magnitude, di­rection, and critical 
points. This work was motivated by two recent innovations for displaying vector fields: spot noise [van 
Wijk 91] and line-integral convolu­tion (LIC) [Cabral &#38; Leedom 93]. We wondered how to compare the 
results of the techniques. What is the gauge that measures how well a certain method depicts a vector 
field? Evidently the place­ment of the graphical elements is tremendously important. The graphical elements 
(e.g. coherent streaks) should follow the flow direction, but they should not be spaced too close together 
or too far apart. Both spot noise and LIC can produce images where stream­aligned streaks are evenly 
distributed, but that is more an indirect Permission to make digital or hard copies of part or all of 
this work or personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation on the first 
page. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 result than a guiding 
principle in the algorithms. How can the stream­lines be positioned to explicitly satisfy a desired distribution? 
The elegant hand-designed streamline drawings in physics texts (for example in Figure 1a) provide ample 
inspiration for vector field illustrations. The streamlines in such illustrations are placed so that 
no region is devoid of streamlines and no region is overpopulated with them. The eye is drawn to regions 
where the density of ink in one place differs greatly from that of the surrounding region. When the 
density of the streamlines is allowed to vary in such illustra­tions, it is usually to represent field 
magnitude, where denser line spacing shows greater field strength. Bertin shows another effective hand-designed 
representation of flow where the direction of ocean current is represented by chains of arrows that are 
laid out end-to-end so that the eye connects arrows into streamlines and thus gets a stronger sense of 
flow orientation [Bertin 83]. The success of this representation depends on having chosen proper endpoints 
for these chains so that nowhere does the image become cluttered. The techniques presented in our paper 
will permit designers of vector-field visualizations to control stream­line-spacing automatically in 
order to achieve results that mimic hand-drawn figures.  2 Previous Work A streamline is an integral 
curve that is everywhere tangent to a given vector field (see, for example, [Kundu 90]). Many research­ers 
have examined how to effectively and accurately integrate streamline paths through both regular and irregular 
meshes. To our surprise, however, discussions of how best to place streamlines are almost nonexistent 
in the visualization literature. We are aware of three techniques that are used to seed streamlines within 
a vector field: regular grids, random sampling, and user-specified seed points for initiation of streamlines. 
Our knowledge of random and regular grid seeding of streamlines is almost entirely limited to private 
com­munications with visualization researchers. The one published tech­nique that we have found uses 
particle traces on a 3D surface that are terminated when they come too close to the paths of other par­ticles 
[Max et al. 94]. The virtual wind tunnel (a 3D immersive display system for flow visualization) allows 
users to initiate stream­lines singly or in bundles [Bryson &#38; Levit 91]. Recently there have been 
several exciting developments in display­ing vector fields using texture synthesis. Line integral convolution 
is a procedure that stretches a given image along paths that are dic­tated by a vector field [Cabral 
&#38; Leedom 93] [Forsell 94] [Stalling &#38; Hege 95]. Spot noise is a method of creating noise-like 
texture by compositing many replicas of a shape [van Wijk 91] [de Leeuw &#38; van Wijk 95]. When the 
shapes that create spot noise textures are stretched according to a given vector field, the resulting 
images illustrate the vector field s direction. Both line integral convolution and spot noise are well-suited 
to depicting the fine detail of flow orientation. They are somewhat less successful (in a single, static 
image) at showing the flow magnitude; moreover, the local flow direction is ambiguous in the sense that 
it can be interpreted to be either of two directions that are 180 degrees apart. A very different method 
of illustrating vector field data is to show the important topological features of the flow. In general, 
stream­lines that lie in a small neighborhood follow nearly-parallel paths. The exceptions (in a continuously 
differentiable vector field) occur in neighborhoods of points with zero-valued vectors. Several re­searchers 
have developed techniques to identify these critical points (sources, sinks, spirals, centers, and saddles) 
and the streamlines that issue from them in eigen-directions [Globus et al. 91] [Helman &#38; Hesselink 
91]. These particular points and curves partition a vec­tor field into simpler regions where a texture-based 
method suffices to display details of the vector field [Delmarcelle &#38; Hesselink 94]. The remainder 
of this paper is organized as follows. In Section 3 we present a key concept in our work a visual quality 
measure for flow illustrations and show how this measure can create visually pleasing illustrations containing 
short arrows. In Section 4 we dem­onstrate the creation of illustrations that contain well-placed long 
streamlines. Section 5 discusses how these streamlines can be en­hanced to produce a final illustration. 
We conclude by discussing other applications that might use optimization based on a visual quality measure. 
 3 Placement of Streamlets Hedgehog illustrations (sometimes called vector plots) are perhaps the most 
commonly used method of illustrating a two-dimensional vector field. These are short field-aligned segments 
or arrows whose base points lie on a regular grid (see Figure 2). The lengths of the segments are often 
varied according to the field magnitude. The popularity of hedgehog illustrations is almost surely due 
to their ease of implementation. The resulting images can be slightly en­hanced by using short streamlines 
(streamlets) that curve with the flow instead of using straight lines. We use such streamlets in our 
figures 2, 3, and 4. Two artifacts are often present in hedgehog plots. First, the human eye often picks 
out runs of adjacent arrows and groups them to­gether visually, despite the fact that these groups are 
an artifact of the underlying grid pattern and not related to the vector field being  Figure 2: (a) 
Short streamlines with Figure 3: (a) Short streamlines with Figure 4: (a) Short streamlines placed centers 
placed on a regular grid (top); centers placed on a jittered grid (top); by optimization (top); (b) filtered 
ver­ (b) filtered version of same (bottom). (b) filtered version showing bright and sion showing fairly 
even gray value (bot­dark regions (bottom). tom). illustrated. This effect can be seen in Figure 2a where 
three vertical columns of streamlets erroneously suggest the presence of three parallel field lines. 
One way to lessen this problem is to oversample the seed points that produce the short segments. The 
drawback with oversampling is that the resulting image becomes so filled with streamlets that the eye 
can no longer discern individual elements. A better solution to the sampling problem is to introduce 
noise, slightly jittering the positions of the arrows to make their regularity less noticeable [Crawfis 
&#38; Max 92] [Dovey 95]. This strategy is illus­trated in Figure 3a. The second problem with hedgehogs 
is that as streamlets are placed close together, portions of neighboring arrows come very close to one 
another and may even overlap. Jittering the streamlets may in fact make the overlaps more frequent (compare 
Figures 2a and 3a). The twin problems of overlapped streamlets and grid regularity both distract the 
viewer from the data being visualized; we would like to reduce such distractions. We achieve this goal 
by using an energy measure to guide streamlet placement and thus improve the quality of the final image. 
3.1 Optimization of Streamlet Positions In the discussion that follows, S represents a collection of 
streamlets sn for a given vector field V. The elements of S are idealized zero­width curves, distinct 
from the geometric primitives (e.g., line seg­ments or anti-aliased curves) employed to render them. 
We denote by I(x,y) the idealized two-dimensional image of the streamlets in S, with I(x,y) = 0 except 
along streamlines in S where it behaves like the Dirac delta function. Our method creates hedgehog plots 
by incrementally improving an initial collection of streamlets. The initial collection can be created 
by placing the streamlets either on a regular grid or in some random fashion, and the final results appear 
to be independent of which initialization method is chosen. An image may be improved by selecting one 
streamlet at random and moving it a small amount in a random direction. If the resulting image has a 
lower energy mea­sure (lower energy means better quality) then that change is ac­cepted. This process 
is repeated many times, terminating when the energy reaches a threshold or when acceptance of random 
changes become rare. Such a process is sometimes referred to as random optimization or random descent. 
Figure 4a shows the result of this algorithm applied to the same vector field as in Figures 2 and 3. 
Notice how the streamlets of Figure 4 are more evenly spaced than in Figures 2 and 3. The energy measure 
that guides the optimization is based on a low­pass-filtered (blurred) version of the image of S which 
is compared against a uniform gray-level. Let L * I represent a low-pass-filtered version of the image 
I, where L is a given filter function. If t is the target gray-scale value, then we define the energy 
measure E as the squared error integrated over the domain: E(I) = .x .y [(L * I) (x,y) -t]2 dx dy The 
motivation for this energy measure is that the eye is drawn to regions of an illustration where the density 
of ink is uneven, and in a hedgehog plot we do not want to draw the eye to any inadvert­ently bright 
or dim places. The streamlets should be evenly placed across the image instead of being crowded in any 
one location. A blurred image contains high values where the streamlets are too close together and low 
values in regions that are devoid of streamlets. Salisbury and his co-workers made similar used of low-pass 
filter­ing to decide whether or not to lay down strokes for pen-and-ink illustrations [Salisbury et al. 
94]. Figure 2b and 3b show low-pass­filtered versions of Figures 2a and 3a. Locations where two streamlets 
crowd together in Figures 2a and 3a appear as a high intensity (black) spot in Figures 2b and 3b. Figure 
4a and 4b show the corresponding images after the optimization routine has been run. The intensity level 
in Figure 4b is more uniform than in Fig­ures 2b and 3b. When the optimization process is animated it 
looks as though each streamlet is pushing away other nearby streamlets, reminiscent of methods that use 
repulsion between points to evenly distribute samples on a surface [Turk 91] [Witkin &#38; Heckbert 94]. 
This simi­larity should come as no surprise, since both methods are designed to minimize an energy term 
by making small changes in the posi­tion of graphical elements. In fact, we too have implemented streamlet-repulsion 
as a method for creating hedgehog plots. The visual results of the repulsion method are very similar 
to the results of random optimization, and the running times are also similar. We pursued the random-descent 
technique rather than the repulsion method because we expected random descent to be easily exten­sible 
to the more complicated task of placing longer streamlines within V (Section 4). 3.2 Implementation 
of Low-Pass Filter This section describes the implementation details for efficiently computing the energy 
term for a given set S of streamlets. There are three components to this computation: the representation 
of the blurred image, the low-pass filter used to perform the blur, and the manner in which we apply 
the filter to calculate this blurred image. It would be computationally prohibitive to calculate the 
energy term E by actually filtering an entire image each time we consider a ran­dom change to some streamlet 
sn . Instead, we associate with sn certain information about how it affects the low-pass-filtered im­age. 
The blurred image B contains pixel values for an image of S. A streamlet maintains a list of pixels that 
it affects in B, together with the values that it contributes to each of those pixels. To test whether 
moving sn would improve the value of E, we first remove the contribution of sn from its list of pixels 
in B and correct the value of E based on the changes. Next, we add in the pixel contri­butions for the 
new position of sn and recalculate E. We retain the change to sn if the new value of E is better; otherwise 
we revert to the old position for sn. The (un-blurred) image I is purely a concep­tual aid, and at no 
time during optimization do we generate an ac­tual representation of I. Two criteria influence the choice 
of a filter to create the blurred image B. First, the filter kernel should have compact support so that 
filtering operations are fast to compute. Second, the point­spread function should fall off smoothly 
so that the quality measure changes smoothly with small changes in streamline position. This allows the 
optimization to detect changes in E even for small changes in the image. We use the following circularly 
symmetric filter kernel (from a ba­sis function of cubic Hermite interpolation) to blur the image: K(x, 
y) = 2r3 - 3r2 +1, r < 1 K(x, y) = 0, r >= 1 where r = sqrt(x2 + y2) / R. This function has a similar 
shape to a two-dimensional Gaussian filter, but it falls off to zero at a distance R away from its center. 
The ideal density for a set of streamlines may be varied across the image by stretching or shrinking 
the radius R of the filter. We sample a streamlet sn at a finite number of points, resulting in a piecewise-linear 
curve composed of zero-width line segments. We calculate the filtered image of each segment by considering 
those pixels in the filtered image that are within a distance R of the seg­ment. The contribution of 
the line segment to a particular pixel in the filtered image can quickly be computed by a variant of 
the tech­nique used by Feibush for polygon anti-aliasing [Feibush et al. 80]. The line segment is rotated 
about the pixel center so that it lies horizontally, and then two table-lookups based on the segment 
s endpoints are used to determine the kernel-weighted contribution to the pixel. We have found that a 
very coarse low-pass filtered image suffices to guide the placement of streamlines. Typically we use 
a filter kernel that extends just two or three pixels in radius. The filtered images in Figures 2, 3 
and 4 were computed at a much higher resolution than this for expository purposes.  4 Long Streamlines 
This section describes how the optimization technique from Sec­tion 3 can be extended to create images 
containing long, evenly­distributed streamlines. One goal of this procedure is to enable fine control 
over the distance between adjacent streamlines, whether that target spacing be constant-valued or position-dependent. 
A second goal is to avoid interrupting the streamlines. Since each endpoint of a streamline distracts 
from the visual flow of the image, our im­ages should favor fewer, longer streamlines over numerous, 
shorter streamlets. It is not always possible to satisfy the two goals of uni­form streamline separation 
and infrequent streamline breaks. In places where the vector field converges (e.g. near a sink) these 
two goals are at odds with one another. Our solution to the dilemma is to let the energy function be 
the arbiter between uniform spacing and long streamlines. The optimization procedure for creating a hedgehog 
plot consists of repeatedly considering small changes to the positions of the streamlets, accepting only 
the changes that improve the measure E. The procedure for creating a set of longer streamlines sn is 
similar. We improve a set S of streamlines by considering several kinds of changes to the streamlines. 
In addition to changes in a streamline s position, the algorithm also allows the operations of streamline 
in­sertion/deletion, lengthening/shortening of streamlines, and com­bination of two streamlines, end-to-end, 
into a single streamline. We use the same quality measure E to determine which changes will be accepted. 
In pseudo-code, the process for creating the col­lection S of long streamlines is as follows. S . null 
{ S begins as an empty set of streamlines } { find an initial group of streamlets } foreach position 
(x,y) on a grid insert streamlet s at (x,y) into S to produce S if E(S ) < E(S) then S . S { improve 
the collection of streamlines in S } repeat until accepted changes are rare choose an operation apply 
operation to random element(s) of S to produce S if E(S ) < E(S) then S . S Figure 5b shows a collection 
of streamlines created with the above optimization procedure. The streamlines are evenly spaced and their 
 Figure 5: (a) Long streamlines with centers regularly placed on a grid (top); (b) Streamlines placed 
by density-based op­timization (bottom). This data is a randomly generated vec­tor field. endpoints are 
generally located where the vector field diverges or converges. For comparison, Figure 5a shows long 
streamlines whose seed points lie on a regular grid so that streamline density varies greatly. 4.1 The 
Allowable Operations The primitive streamline operations that we employ to improve the quality of an 
image are described in more detail below. Move: Change the position of the seed point of the stream­line. 
Each streamline is defined in terms of this seed point and a length to travel forward and backward through 
the flow. Insert: Create a new streamlet. Delete: Remove a streamline entirely from S. Lengthen: Add 
a positive value to the length of the streamline (relative to the seed point) in the forward or back­ward 
direction. Shorten: Subtract from the length of the streamline (relative to the seed point) in the forward 
or backward direc­tion. Combine: Connect two streamlines whose endpoints are suffi­ciently close to one 
another. The location of the join is a weighted average of the two endpoints based on the relative lengths 
of the streamlines. The length of the new streamline is the sum of the lengths of the two parent streamlines. 
 Why do we allow so many kinds of changes during the optimiza­tion process? Presumably we could create 
any possible collection of streamlines using only insert and delete operations if we allow newly-inserted 
streamlines to assume any length and position. However, an actual implementation of the optimization 
process us­ing such a restricted set of operations would be prohibitively slow to converge. We use the 
larger complement of operations so that the optimization procedure can move smoothly through the space 
of all collections of streamlines. For example, suppose that joining two particular streamlines would 
greatly improve the measure E. The optimization routine could choose at random to remove each of these 
streamlines and, also at random, create another streamline that fills the void left by the two that were 
removed. It is very unlikely that these three independent events would happen by chance. Explicitly providing 
a combine operation makes this small change in visual appearance much more likely to occur. We find candidate 
pairs of streamlines for the combine operation by querying a data structure that maintains the positions 
of stream­line endpoints and can return pairs of endpoints whose distance is less than a given tolerance. 
There are several ways in which we can favor joining together streamlines. We could add a term to the 
en­ergy function that gives a higher energy to those images that con­tain more streamlines. Instead of 
this approach, however, we choose to accept combine operations if they result in a new value of E that 
is no greater than the old energy value plus a tolerance. We can animate the optimization process by 
displaying the collec­tion of streamlines every time a favorable change occurs. An ani­mation of the 
optimization indicates the role of each operation. First, streamlets are inserted throughout the image. 
After this initial phase is finished the result looks much like a hedgehog plot using a jit­tered grid, 
reminiscent of a Poisson-disk distribution of points. Next, many of the streamlines gradually lengthen. 
As streamline end­points approach one another, pairs of streamlines combine to form longer streamlines. 
This dual process of lengthening and joining creates many longer streamlines that typically follow nearly-paral­lel 
trajectories. Gradually the changes in the image become minor, and many of the changes at this stage 
are streamlines moving a small distance, evening the spacing between neighbors. Changes are accepted 
with decreasing frequency, and the process is termi­nated when accepted changes become sufficiently rare. 
 4.2 Acceleration Using an Oracle The stochastic optimization produces good results, but it spends considerable 
time entertaining changes that are unlikely to improve the image. The method can be accelerated by using 
an oracle that suggests changes that are likely to decrease the energy function E. An oracle is only 
effective if it can be consulted quickly and its answers are generally reliable. The oracle described 
in this section typically speeds up the convergence of the optimization by a factor of three to five. 
There are two systematic ways for an oracle to select changes to propose: an image-based approach, and 
a streamline-based ap­proach. Our oracle uses a combination of the two. The image­based approach examines 
the blurred image B to identify places where the streamlines are too sparse. The oracle makes insert 
sug­gestions in these places. The streamline-based approach examines the neighborhood of each individual 
streamline to decide if an op­eration applied to the streamline is likely to improve the image. The oracle 
uses information gathered from around a streamline to decide whether to suggest a lengthen, shorten or 
move operation. More precisely, the oracle keeps a running measure of how ener­getic a given streamline 
is, and it maintains a priority queue that orders the streamlines based on their individual level of 
energy. When consulted, the oracle returns one of the most energetic stream­lines, along with a suggestion 
of how to lessen its measure of en­ergy. The energy of a streamline is the sum on three factors: desire 
to lengthen, desire to shorten, and desire to move. Each of these factors is calculated by sampling the 
image B at a small number of positions near the streamline. The desire to lengthen is computed by comparing 
the target gray level t with the image values a short distance beyond the endpoints of the streamline. 
The lower these values are with respect to t, the greater the streamline desires to grow into this empty 
region. The desire to shorten is found by sampling B on either side of the streamline endpoints. If these 
val­ues are too high, the streamline desires to shrink. The desire to move is computed by comparing the 
image values on one side of the streamline with the values on the other side. The greater the difference 
between these two values, the more the streamline de­sires to change its position. We typically consult 
20 samples of the image B to determine each of the three factors that determine a streamline s energy. 
This sampling is an inexpensive task in com­parison to creating the entire path of a streamline and then 
low­pass-filtering the resulting curve. The oracle need not bother suggesting that a streamline be deleted. 
Every time the optimization routine attempts to modify a stream­line it can easily check whether entirely 
removing the streamline improves the total energy measure E of the image. This is done by evaluating 
E after the contribution of the streamline to the image B is removed and before the altered streamline 
s effect is added to B. The oracle is important for improving efficiency, but it is the en­ergy measure 
E that drives the optimization. The oracle is used purely as a source of suggestions for how to reduce 
E, not as a source of directives that are applied blindly. The oracle s sugges­tions are only accepted 
if the change improves the image quality. We have found it effective for the oracle to propose 50% of 
the changes, and for the other changes to be chosen completely at ran­dom. Thus any change to the collection 
of streamlines is possible, which makes it unlikely that the optimization will overlook a worth­while 
improvement arising from any systematic bias of the oracle. 4.3 Intensity Tapering at Streamline Ends 
Some streamlines must terminate within a region of converging flow or else the target density of the 
image cannot be preserved there. The resulting break of the streamline is visually jarring if it is 
ren­dered as a rectangular end cap. We make the termination less abrupt by gradually decreasing the width 
or intensity of the streamline near its endpoint. We can gently fade a streamline by allowing yet an­other 
operation, namely streamline tapering. Each streamline car­ries with it (in addition to its center and 
length) two positions along its length that indicate where to begin linearly fading to the back­ground 
color at either endpoint. This intensity tapering is used to weight the contribution of the streamline 
to the filtered image B. Streamline tapering allows the optimization to find an even closer match to 
the ideal gray-scale value in regions near the streamline ends. In practice we have found it most effective 
to let intensity tapering be a separate optimization phase, after the streamlines have settled into their 
final position. In this final phase each streamline is allowed to perform only two operations: 1) changes 
in length, and 2) changes in the locations at which to begin intensity tapering. Performing the intensity 
tapering after long streamlines have been formed avoids the possibility that the optimization will produce 
many short, intensity-tapered streamlines to satisfy the target density. Figures 6 and 8 are rendered 
using the tapering information to modu­late streamline width and intensity, respectively. Saito and Takahashi 
have demonstrated a similar tapering effect for drawing contour lines of a scalar field [Saito &#38; 
Takahashi 90]. They use information about the gradient of the scalar field to guide the fading out of 
the contour lines. Their technique can also be used for drawing streamlines of vector fields where the 
divergence is zero everywhere, but it has no obvious generalization when the diver­gence is non-zero 
(e.g. fields with sources and sinks).  4.4 Optimization Issues Two recent techniques in computer graphics 
provided inspiration for the optimization approach described here. The first of these is the work by 
Andrew Witkin and Paul Heckbert for distributing par­ticles over an implicit surface [Witkin &#38; Heckbert 
94]. In their constrained optimization method, they let a small number of seed particles repel one another 
in order to distribute themselves evenly over a surface. They found that it is helpful to allow the initial 
particles to grow, split, shrink or die to accommodate any change in surface area when the surface geometry 
is being edited. Their op­erations on particles are analogous to our operations on streamlines. Figure 
7: Chains of arrows indicate wind direction and magnitude over Australia. The arrows were deposited along 
streamlines created by streamline optimization. Higher velocity is indicated by larger arrows. The vector 
field data was calculated using a numerical weather model. A second source of inspiration was the mesh 
optimization work by Hugues Hoppe and co-workers [Hoppe et al. 93]. Their technique uses three fundamental 
operations to automatically simplify a po­lygonal mesh: edge split, edge collapse, and edge swap. They 
used an energy measure to guide the optimization by random descent. The high quality of the results produced 
by this method encouraged us to try random descent in streamline optimization. One frequently-voiced 
concern about optimization techniques is that the behavior of the system is highly sensitive to the values 
of many parameters. An example of such a parameter for streamline optimi­zation is the maximum distance 
a streamline can move. The fear is that the system may require a large amount of parameter tweak­ing. 
Happily, we have found it unnecessary to change our param­eter settings between datasets. The single 
parameter that we specify for an illustration is the desired distance between neighboring stream­lines 
(which can even be position-dependent). Other parameters are derived from this target-distance. We believe 
that researchers who implement the techniques described here will not have diffi­culty replicating our 
results. To relieve the burden of re-imple­menting our technique, we are making our source code publicly 
available at http://www.cs.msstate.edu/~banks/IGSP.  5 Binding Visual Attributes to Streamlines There 
is an important distinction between a streamline (a zero-width integral curve) and the geometric elements 
associated with its dis­play. A simple approach for displaying a streamline is to draw an anti-aliased 
curve that connects vertices sampled along the stream­line, but such a constant-width, constant-intensity 
curve is not nec­essarily the best way to visualize the flow. For example, the curves are unchanged if 
all the vectors reverse direction in the underlying vector field; that is, the sense of flow direction 
is ambiguous in a simple streamline display. Arrows can be inserted into the image to disambiguate the 
flow direction. We apply two different techniques to bind arrow-shaped glyphs to streamlines. The first 
technique is to traverse the streamlines and deposit an arrow whenever the inte­grated arc-length along 
a streamline is sufficient to accommodate the arrow s length. Such an object-order traversal is appropriate 
for binding a long chain of glyphs onto a streamline. The second approach is to distribute arrow-glyphs 
uniformly throughout the image and then snap them to the nearest point on a streamline. Such an image-order 
traversal is appropriate for images with only a few scattered arrows serving as reminders of the flow 
direction. Often some important scalar quantity is associated with a vector field. The scalar value might 
be the temperature or density in a fluid flow, or it might be the magnitude of the vector field at each 
point. We would like to bind visual attributes to display such a sca­lar quantity along with the streamlines. 
The thickness and the grayscale-intensity of a streamline offer two convenient visual at­tributes to 
convey a scalar quantity. Figure 6 shows a vector field whose magnitude is bound to the width of the 
streamlines and where the streamlines themselves have been placed so that the scalar field determines 
the distance between neighboring streamlines.  6 Results In this section we show some examples of images 
constructed us­ing the optimization method for positioning long streamlines. The first example is Figure 
1b, which illustrates a numerical simulation of flow around a cylinder. The arrowheads in this figure 
disam­biguate flow orientation in the eddies. Figure 7 shows computed wind velocity in the vicinity of 
Australia. First, the long streamline optimization method placed streamlines through the image. Then 
arrows were bound to these streamlines. The size of the arrow indi­cates the wind magnitude. The arrows 
line up head-to-tail so that the eye can easily follow from one to the next, as is favored by illustrators 
[Bertin 83]. Human-subject studies have shown that if a graphical stroke varies in width from large to 
small, people have a strong sense that the direction is towards the larger end [Fowler &#38; Ware 89]. 
This guided our choice of tapered arrows in Figure 7. Figure 8: Pears. The texture in this image was 
created by combining streamlines in two directions: along the gradient of the blurred intensity and at 
90 degrees to the gradient. Original photograph courtesy of Herb Stokes. Another application of the streamline-placement 
technique is to cre­ate iso-intensity contours that are evenly spaced. Consider the ef­fect of highlighting 
several discrete intensity levels in a gray-scale image: even if the intensity-values are chosen in equal 
increments, the resulting contours are likely to clump together in some regions and spread apart in others. 
Our optimization technique provides a convenient way to adaptively sample the intensity values so that 
the curves are uniformly distributed in the image. Figure 8 shows how the technique can be applied to 
a color photograph. We con­verted the image to monochrome, blurred it, and then calculated its gradient 
vector field. We ran the optimization on the gradient vec­tor field and on a vector field orthogonal 
to it (and thus aligned with the iso-value lines). The two sets of streamlines that resulted were combined 
and used as a mask to apply the original color values to the grayscale image. The effect is akin to weaving, 
with constant­intensity thread being used along the contours. Our streamline optimization program was 
written in C++, and the calculations for the figures herein were performed on a Silicon Graphics Indigo2 
with an R4400 processor operating at 250 MHz. Figures 4 (a) and 5 (b) were created in under one minute, 
and the streamlines for Figures 6, 7 and 8 required roughly 15 minutes each. We expect that fine-tuning 
the code would improve the speed by a factor of two to four.  7 Conclusion and Future Work There are 
several logical extensions to the streamline optimization method presented in this paper. This same process 
can be used to create streamlines on curved surfaces by running the optimization in the parametric space 
of the surface and correcting for mapping distortions. The technique could also be used to create streamlines 
in three dimensions, although computational efficiency will prob­ably become an issue. The density of 
3D streamlines could be made dependent on additional properties of the vector field, such as prox­imity 
to vortex cores. Another research area is in creating illustra­tions that reveal different levels of 
detail when the viewer is at vari­ous distances. We expect that the notion of guiding the placement of 
graphical elements by a visual measure of quality will have applications be­yond vector field visualization. 
For instance, a similar optimiza­tion method might prove useful in placing graphical elements in a texture. 
Another potential use for such techniques is for computer generation of illustrations that have a hand-drawn 
appearance [Saito &#38; Takahashi 90] [Winkenbach &#38; Salesin 94]. 8 Acknowledgments We thank Glenn 
Wightwick of IBM Australia and Lloyd Treinish of the IBM T. J. Watson Research Center for the Australia 
wind data. Earth image is courtesy of Geosphere, Inc. The fluid flow data of Figure 1 was provided courtesy 
of David Rudy, NASA Lan­gley Research Center. We thank Peggy Wetzel and Mary Whitton for help in making 
video of this work. Funding for this work was provided in part by the NSF Science and Technology Center 
for Computer Graphics and Scientific Visualization. Travel support was provided by ICASE and the NSF 
Engineering Research Center at MSU.  9 Bibliography [Bertin 83] Bertin, Jacques, Semiology of Graphics, 
translated from French, The University of Wisconsin Press 1983. [Bryson &#38;Levit 91] Bryson, Steve 
and Creon Levit, The Virtual Wind Tunnel: An Environment for the Exploration of Three-Di­mensional Unsteady 
Flows, Proceedings Visualization 91, San Diego, California, October 22 25, pp. 17 24. [Cabral &#38; Leedom 
93] Cabral, Brian and Leith (Casey) Leedom, Imaging Vector Fields Using Line Integral Convolution, Com­puter 
Graphics Proceedings, Annual Conference Series (SIGGRAPH 93), pp. 263 270. [Crawfis &#38; Max 92] Crawfis, 
Roger and Nelson Max, Direct Vol­ume Visualization of Three Dimensional Vector Fields, Proceed­ings of 
the 1992 Workshop on Volume Visualization, pp. 55 60. [de Leeuw &#38; van Wijk 95] de Leeuw, Willem C., 
and Jarke van Wijk, Enhanced Spot Noise for Vector Field Visualization, Pro­ceedings Visualization 95, 
Atlanta, Georgia, Oct. 29 Nov. 3, pp. 233 239. [Delmarcelle &#38; Hesselink 94] Delmarcelle, Thierry 
and Lambertus Hesselink, The Topology of Symmetric, Second-Order Tensor Fields, Proceedings Visualization 
94, Washington, D.C., October 17 21, pp. 140 147. [Dovey 95] Dovey, Don, Vector Plots for Irregular Grids, 
Pro­ceedings Visualization 95, Atlanta, Georgia, Oct. 29 Nov. 3, pp. 248 253. [Feibush et al. 80] Feibush, 
Eliot, Marc Levoy and Robert Cook, Synthetic Texturing Using Digital Filters, Computer Graphics Proceedings, 
Annual Conference Series (SIGGRAPH 80), pp. 294 301. [Feynman 64] Feynman, Richard P., Robert B. Leighton 
and Mat­thew Sands, The Feynman Lectures on Physics, Addison-Wesley, Reading, Massachusetts, 1964. [Forssell 
94] Forsell, Lisa K., Visualizing Flow over Curvalinear Grid Surfaces Using Line Integral Convolution, 
Proceedings Vi­sualization 94, Washington, D.C., October 17 21, pp. 240 247. [Fowler &#38; Ware 89] Fowler, 
David and Colin Ware, Strokes for Representing Univariate Vector Field Maps, Graphics Interface 89, London, 
Ontario, June 19 23, 1989, pp. 249 253. [Globus et al. 91] Globus, A., C. Levit and T. Lasinski, A Tool 
for Visualizing the Topology of Three-Dimensional Vector Fields, Proceedings Visualization 91, San Diego, 
California, October 22 25, pp. 33 40. [Helman &#38; Hesselink 91] Helman, J. L. and L. Hesselink, Visual­ization 
of Vector Field Topology in Fluid Flows, IEEE Computer Graphics and Applications, Vol. 11, No. 3, pp. 
36 46. [Hoppe et al. 93] Hoppe, Hugues, Tony DeRose, Tom Duchamp, John McDonald and Werner Stuetzel, 
Mesh Optimization, Com­puter Graphics Proceedings, Annual Conference Series (SIGGRAPH 93), pp. 19 26. 
[Kundu 90] Kundu, Pijush K., Fluid Mechanics, Academic Press, Inc., San Diego, 1990. [Max et al. 94] 
Max, Nelson, Roger Crawfis and Charles Grant, Visualizing 3D Velocity Fields Near Contour Surfaces, 
Proceed­ings Visualization 94, Washington, D.C., October 17 21, pp. 248 255. [Saito &#38; Takahashi 90] 
Saito, Takafumi and Tokiichiro Takahashi, Comprehensible Rendering of 3-D Shapes, Computer Graphics, 
Vol. 24, No. 4 (SIGGRAPH 90), pp. 197 206. [Salisbury et al. 94] Salisbury, Michael P., Sean E. Anderson, 
Ronen Barzel and David H. Salesin, Interactive Pen-and-Ink Illustration , Computer Graphics Proceedings, 
Annual Conference Series (SIGGRAPH 94), pp. 101 108. [Stalling &#38; Hege 95] Stalling, Detlev and Hans-Christian 
Hege, Fast and Resolution Independent Line Integral Convolution, Com­puter Graphics Proceedings, Annual 
Conference Series (SIGGRAPH 95), pp. 249 256. [Turk 91] Turk, Greg, Generating Textures on Arbitrary 
Surfaces Using Reaction-Diffusion, Computer Graphics, Vol. 25, No. 4 (SIGGRAPH 91), pp. 289 298. [van 
Wijk 91] van Wijk, Jarke J., Spot Noise: Texture Synthesis for Data Visualization, Computer Graphics, 
Vol. 25, No. 4 (SIGGRAPH 91), pp. 309 318. [Winkenbach &#38; Salesin 94] Winkenbach, Georges and David 
H. Salesin, Computer-Generated Pen-and-Ink Illustrations, Computer Graphics Proceedings, Annual Conference 
Series (SIGGRAPH 94), pp. 91 98. [Witkin &#38; Heckbert 94] Witkin, Andrew and Paul Heckbert, Us­ing 
Particles to Sample and Control Implicit Surfaces, Computer Graphics Proceedings, Annual Conference Series 
(SIGGRAPH 94), pp. 269 277.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237286</article_id>
		<sort_key>461</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[Scale-dependent reproduction of pen-and-ink illustrations]]></title>
		<page_from>461</page_from>
		<page_to>468</page_to>
		<doi_number>10.1145/237170.237286</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237286</url>
		<keywords>
			<kw><![CDATA[discontinuity edges]]></kw>
			<kw><![CDATA[image magnification]]></kw>
			<kw><![CDATA[image resampling]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[scale-dependent rendering]]></kw>
			<kw><![CDATA[stroke textures]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Device independence**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Grayscale manipulation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14210832</person_id>
				<author_profile_id><![CDATA[81100611347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Salisbury]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P50075</person_id>
				<author_profile_id><![CDATA[81540249756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Corin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14153841</person_id>
				<author_profile_id><![CDATA[81311486606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dani]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lischinski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>297697</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adobe Systems Incorporated. PostScript Language Reference Manual. Addison Wesley, Reading, Massachusetts, 2nd edition, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>577958</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. Data Structures and Algorithms. Addison-Wesley, Reading, Massachusetts, 1987.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Arthur Beck. Photograph. In Photographis 81, The International Annual of Advertising and Editorial Photography, p. 151. Graphis Press Corp., 1981.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Deborah F. Berman, Jason T. Bartell, and David H. Salesin. Multiresolution painting and compositing. In Computer Graphics Proceedings, Annual Conference Series, pp. 85-90. ACM Press, July 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>11275</ref_obj_id>
				<ref_obj_pid>11274</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[John Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 8(6):679-698, November 1986.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[L. Paul Chew. Constrained delaunay triangulations. Algorithmica, 4:97-108, 1989.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook. Stochastic sampling in computer graphics. Transactions on Graphics, 5(1):51-72, January 1986.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325182</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Mark A. Z. Dipp6 and Erling Henry Wold. Antialiasing through stochastic sampling. Computer Graphics, 19(3):69-78, July 1985.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614310</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gershon Elber. Line art rendering via a coverage of isoparametric curves. IEEE Transactions on Visualization and Computer Graphics, 1(3):231-239, September 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Richard Franke and Gregory M. Nielson. Surface approximation with imposed conditions. In R. E. Bamhill and W. Boehm, editors, Surfaces in CAGD, pp. 135-146. North-Holland Publishing Company, 1983.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Arthur L. Guptill. Rendering in Pen and Ink. Watson-Guptill Publications, New York, 1976.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Paul E. Haeberli. Paint by numbers: Abstract image representations. Computer Graphics, 24(4):207-214, August 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Paul Heckbert. Discontinuity meshing for radiosity. In Third Eurographics Workshop on Rendering, pp. 203-226, Bristol, UK, May 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618268</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[John Lansdown and Simon Schofield. Expressive rendering: A review of nonphotorealistic techniques. IEEE Computer Graphics and Applications, 15(3):29-37, May 1995.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Dani Lischinski, Filippo Tampieri, and Donald R Greenberg. Discontinuity meshing for accurate radiosity. IEEE Computer Graphics and Applications, 12(6):25-39, November 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[T. Marshall. Lively pictures (Power Mac image editing). BYTE, 20(1): 171-172, January 95.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Randy Miller. Photograph. In Photographis 77, The International Annual of Advertising and Editorial Photography, p. 72. Graphis Press Corp., 1977.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37410</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Don R Mitchell. Generating antialiased images at low sampling densities. Computer Graphics, 21(4):65-72, July 1987.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218437</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin and Luiz Velho. Live Paint: painting with procedural multiscale textures. In Computer Graphics Proceedings, Annual Conference Series, pp. 153-160. ACM Press, August 1995.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>190030</ref_obj_id>
				<ref_obj_pid>190025</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Yachin Pnueli and Alfred M. Bruckstein. DigiDiirer- a digital engraving system. The Visual Computer, 10(5):277-292, 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122741</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Takafumi Saito and Tokiichiro Takahashi. NC machining with G- buffer method. Computer Graphics, 25(4):207-216, July 1991.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[David Salesin, Daniel Lischinski, and Tony DeRose. Reconstructing illumination functions with selected discontinuities. In Third Eurographics Workshop on Rendering, pp. 99-112, Bristol, UK, May 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Michael R Salisbury, Sean E. Anderson, Ronen Barzel, and David H. Salesin. Interactive pen-and-ink illustration. In Computer Graphics Proceedings, Annual Conference Series, pp. 101-108. ACM Press, July 1994.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Gary Simmons. The Technical Pen. Watson-Guptill Publications, New York, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Thomas Strothotte, Bernhard Preim, Andreas Raab, Jutta Schumann, and David R. Forsey. How to render frames and influence people. Computer Graphics Forum, 13(3):455-466, 1994. Eurographics '94 Conference issue.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Walter Swarthout. Photograph. In Photographis 75, The International Annual of Advertising, Editorial, and Television Photography, p. 133. Graphis Press Corp., 1975.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Edward Weston. Aperture Masters of Photography, Number Seven, p. 29. Aperture Foundation, Inc., New York, 1988.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Georges Winkenbach and David H. Salesin. Computer-generated penand-ink illustration. In Computer Graphics Proceedings, Annual Conference Series, pp. 91-100. ACM Press, July 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[George Wolberg. Digital Image Warping. IEEE Computer Society Press, Los Alamitos, California, 1990.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Y. Yomdin and Y. Elichai. Normal forms representation: a technology for image compression. In Image and Video Processing, volume 1903 of Proceedings of the SPIE- The International Society for Optical Engineering, pp. 204-214. SPIE, February 1993.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[S. Zhong and S. Mallat. Compact image representation from multiscale edges. In Proceedings. Third International Conference on Computer Vision, pp. 522-525. IEEE Computing Society Press, December 1990.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Scale-Dependent Reproduction of Pen-and-Ink Illustrations Mike Salisbury Corin Anderson Dani Lischinski 
David H. Salesin Department of Computer Science and Engineering University of Washington Abstract This 
paper describes a representation for pen-and-ink illustrations that allows the creation of high-.delity 
illustrations at any scale or resolution. We represent a pen-and-ink illustration as a low-reso­lution 
grey-scale image, augmented by a set of discontinuity seg­ments, along with a stroke texture. To render 
an illustration at a par­ticular scale, we .rst rescale the grey-scale image to the desired size and 
then hatch the resulting image with pen-and-ink strokes. The main technical contribution of the paper 
is a new reconstruction al­gorithm that magni.es the low-resolution image while keeping the resulting 
image sharp along discontinuities. CR Categories and Subject Descriptors: I.3.3 [Computer Graph­ics]: 
Picture/Image Generation Display algorithms; I.3.6 [Com­puter Graphics]: Methodology and Techniques 
 Device indepen­dence; I.4.3 [Image Processing]: Enhancement Filtering. Additional Key Words: discontinuity 
edges, image magni.cation, image resampling, non-photorealistic rendering, scale-dependent rendering, 
stroke textures.  Introduction The medium of pen and ink offers many advantages for visu­ally communicating 
ideas. Pen-and-ink illustrations can be easily printed alongside text, using the same ink on the same 
paper, without degradation. Moreover, good reproduction quality can be obtained on commonplace 300 or 
600 dot-per-inch laser printers as well as on lower-resolution monochrome displays. Although pen-and-ink 
illustrations allow only monochromatic strokes of the pen, the re­sulting illustrations are often striking 
in their beauty and simplic­ity [11, 24]. While the areas of photorealistic rendering and paint systems 
have received considerable attention in the literature, creating pen-and­ink illustrations on a computer 
is a relatively new area. Recently, Winkenbach and Salesin [28] described an automated rendering sys­tem 
that produces pen-and-ink illustrations from 3D polyhedral ar­chitectural models. This system can render 
an illustration of a model at different scales and resolutions by applying procedural stroke tex­tures 
to an analytic representation of the image. Concurrently, Salisbury et al. [23] proposed an interactive 
system that allows the University of Washington, Box 352350, Seattle WA 98195-2350 fsalisbur jcorin jdanix 
jsalesin g@cs.washington.edu Permission to make digital or hard copies of part or all of this work or 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage and that copies bear this notice and the full citation on the first page. 
To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific 
permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 user to paint an image with 
prioritized stroke textures. This sys­tem is particularly useful for applying stroke textures to a scanned 
or synthetic image, effectively creating an artistically-halftoned monochrome version of the original. 
The user creates the pen-and­ink illustration on the screen, and the illustration is saved as a long 
list of B´ezier strokes. One problem with this straightforward WYSI-WYG approach is that illustrations 
represented in this manner can­not be reproduced at different scales or resolutions without signi.­cantly 
changing their overall appearance. By way of example, consider the three illustrations shown in Fig­ure 
1. Figure 1(b) shows the original illustration at the size for which it was designed by the artist. If 
we wish to double the size of the illustration, we cannot merely scale it by a factor of two; such a 
scaling lightens the tone by spreading the same number of strokes over a larger area, as demonstrated 
by Figure 1(c). While this ef­fect could be mitigated by thickening the strokes in the scaled-up version, 
the character of the illustration would be considerably al­tered. Conversely, scaling the illustration 
down darkens the tone as the density of strokes increases (Figure 1(a)). We would instead like our computer-generated 
illustrations to maintain both the thickness and density of their strokes when they are rescaled. Changing 
reso­lutions could also produce unwanted effects. For instance, all three illustrations in Figure 1 would 
look darker on a 300 dot-per-inch printer as they use strokes that are too thin for that resolution. 
Another problem with storing illustrations as B´ezier strokes is that the sheer number of strokes can 
make such a representation expen­sive to store, slow to transmit, and cumbersome to include in elec­tronic 
documents. For example, the size of each illustration in Fig­ure 1 is about one megabyte in PostScript 
[1]. In this paper, we extend the work of Salisbury et al. by proposing an alternative representation 
for pen-and-ink illustrations that is scale­and resolution-independent as well as compact. Instead of 
storing each of the individual strokes comprising an illustration, we keep an underlying grey-scale image 
for each stroke texture in the illus­tration along with a pointer to the stroke texture itself. To render 
the illustration at a particular scale and resolution, the grey-scale images are .rst rescaled and then 
hatched with strokes. The proposed image-based representation is quite simple; however, maintaining true 
scale-and resolution-independence also requires solving an interesting related problem whose solution 
is not so straightforward. Since pen-and-ink illustrations hatch an image with strokes, they tend to 
be insensitive to .ne texture detail. Thus it is often suf.cient for the underlying grey-scale image 
to have a rela­tively low resolution. However, magnifying a low-resolution image for reproducing a large 
illustration on a high-resolution output de­vice typically results in undesirable blurring of the hard 
edges, or discontinuities, in the image. The question, then, is how to resample images while preserving 
cer­tain discontinuity edges. In order to be able to produce crisp edges in illustrations at all possible 
output scales and resolutions, we need to maintain information about discontinuity edges in the underly­ing 
image and explicitly take these edges into account in the resam­pling process. In this paper, we describe 
a new resampling algo­ (a) (b) (c) Figure 1 The same illustration at three different scales: (b) is the 
original; (a) and (c) demonstrate that naive rescaling changes tone and character. rithm to implement 
this process. This algorithm, which is essentially a discontinuity-sensitive reconstruction .lter, is 
the main technical contribution of the paper. 1.1 Related work Line-art illustration has been explored 
previously by a number of authors. Elber [9], Saito and Takahashi [21], Winkenbach and Salesin [28], 
Strothotte et al. [25], and Lansdown and Scho.eld [14] all use 3D models to generate illustrations. Pneuli 
and Bruck­stein [20] and Salisbury et al. [23] both generate illustrations starting from grey-scale images. 
However, neither of these last two works addresses the problem of rescaling such illustrations. There 
are several paint systems that offer a measure of resolution­independence. Paint By Numbers [12] stores 
images as collections of resolution-independent strokes, Live Picture [16] represents the operations 
on images as a resolution-independent history, and Live-Paint [19] provides resolution-independent procedural 
ink. How­ever, none of these approaches provides any means of magnifying scanned images beyond their 
original resolution while preserving discontinuities. The resampling algorithm described in this paper 
could conceivably be used for this purpose in any of these systems. The idea of making explicit use of 
discontinuities in functions, sur­faces, and images is not new. Discontinuities have been used to construct 
good meshes for radiosity [13, 15] and to .t piecewise­cubic interpolants for radiance functions [22]. 
Franke and Nielson described several methods for surface reconstruction from scattered data with known 
discontinuities [10]. Zhong and Mallat [31] pio­neered work in image compression by storing edges detected 
at mul­tiple scales. Yomdin and Elichai [30] also describe an image com­pression algorithm that locates 
and utilizes various types of edges in images to obtain a lossy compression scheme that avoids recon­struction 
artifacts in the vicinity of edges.  1.2 Overview In the next section we describe in more detail our 
proposed represen­tation scheme for pen-and-ink illustrations, and we present a new algorithm for image 
rescaling that preserves the discontinuities of the original image. Section 3 describes how pen-and-ink 
illustra­tions are created and reproduced in our illustration system. Section 4 presents several examples 
of illustrations and describes our experi­ence with the proposed technique. We conclude and offer directions 
for further research in Section 5.  2 Reconstructing images with discontinuities This section describes 
the core of our pen-and-ink illustration sys­tem: an image-based representation for illustrations, and 
a new re­construction algorithm for resampling images with discontinuities. We have two major requirements 
of our representation. First, it should allow us to produce pen-and-ink illustrations at any scale and 
resolution without changing the tone or character of the illustration. Second, the resulting illustrations 
must keep sharp features sharp and smooth features smooth. By sharp features we mean abrupt changes in 
intensity along certain prescribed boundaries of the im­age, which we refer to as discontinuities. While 
we want our illus­trations to exhibit crisp edges along discontinuities, we would like the tone to change 
smoothly everywhere else. In particular, it is im­portant that the rescaling algorithm not introduce 
any spurious dis­continuities. One could imagine several possible representations that would meet our 
requirements. For example, we could maintain a history of all the operations performed on an image, along 
the lines of Live Pic­ture [16], and then simply replay the history at the desired resolu­tion when rendering 
the output. Although this approach is simple and basically sound, it has two main disadvantages. First, 
the repre­sentation s size and rendering time grow with the number of editing operations, and not necessarily 
with the complexity of the image. Second, this approach does not allow scanned or rendered images to 
be magni.ed beyond their original sizes without blurring their sharp features. Another alternative is 
to use a collection of polynomial patches in order to construct an explicit image function that interpolates 
the image sample values. One dif.culty with this representation is the problem of handling discontinuities, 
since it is not obvious how to modify a smooth patch representation to incorporate arbitrary ar­rangements 
of discontinuities. Another dif.culty is the problem of determining control points so that the surface 
accurately approxi­mates an arbitrary target image without introducing ringing artifacts from maintaining 
the smoothness constraints. For our representation, we have chosen to use a combination of uniformly-spaced 
image samples and piecewise-linear discontinu­ity curves called discontinuity edges. Any arrangement 
of discon­tinuity edges is allowed, provided they intersect only at their end­points. Thus, discontinuity 
edges are not constrained to form closed regions. Instead, we allow open discontinuities that have dangling 
edges not connected to any other. We would like the tone to change smoothly around the open end of such 
a discontinuity but change sharply across the discontinuity edge. It is crucial to allow open dis­continuities, 
as they frequently arise in images, especially when dis­continuity edges are obtained by performing edge 
detection on a scanned image (see Figure 6(b)). In the rest of this section, we ll describe the algorithm 
we use to pro­duce an image of arbitrary scale and resolution from our represen­tation. 2.1 Problem statement 
The problem that we would like to solve can be stated formally as follows: Given: A set of uniformly-spaced 
discrete sample (pixel) locations fxig, a set of corresponding intensity values ffig, and a set of line 
segments f` ig(the discontinuity edges); Find: A function f (x) that is smooth everywhere except across 
the discontinuity edges, such that f (x) interpolates the values fi. The reconstructed function f (x) 
can then be resampled at a higher rate than the original image, resulting in a magni.ed version of the 
image, or it can be resampled at a lower rate after band-limiting, yielding a mini.ed version of the 
image.  2.2 Reconstruction algorithm Reconstruction of a continuous signal f (x) from uniformly-spaced 
discrete samples f(xi, fi)gmay be performed by convolving with a reconstruction kernel k(x), whose weights 
at the sample points sum to one: X f (x)= fi k(x xi). (1) Figure 2 Several discontinuity edges (shown 
as thick lines) intersect the support of a 4 X4 kernel centered at x. The dashed line indicates the shortest 
unobstructed path from x to b. A variety of reconstruction kernels is available, including several interpolation 
kernels that effectively .t interpolants of various de­grees to the input samples [29]. In this section, 
we describe a mod­i.cation to the standard convolution framework that will cause the reconstructed function 
to be discontinuous across the discontinuity edges while preserving smoothness everywhere else. In order 
to reconstruct the value at point x, we .rst check whether there are any discontinuity edges that cross 
the support of the kernel centered at x. If no such discontinuities exist, the reconstructed value f 
(x) is given by equation (1). Things become more interesting when one or more image samples under the 
kernel s support are separated from x by a discontinuity edge. Consider, for example, the situation in 
Figure 2. The thick solid lines indicate discontinuity edges that intersect the square sup­port of the 
kernel centered at x (marked by a black dot). Input sam­ples such as a cannot be reached from x without 
crossing a disconti­nuity edge. Clearly such samples should not have any effect on the value of f (x). 
Input samples such as b cannot be seen directly from x; however, they can be reached by going around 
the discontinuities. To ensure that f (x) changes smoothly as x moves around these dis­continuities, 
the sample b should have some effect on the value f (x). Intuitively, this effect should be commensurate 
with the ease of reaching b from x without crossing discontinuities. Thus, in order to preserve discontinuities 
in the reconstructed func­tion f (x), we replace the reconstruction kernel k with a modi.ed ker­nel k 
, which attenuates k s entries according to each entry s reacha­bility. To describe our new kernel, we 
must .rst de.ne some terms. Let d(x, xi) be the Euclidean distance between x and xi,and let sp(x, xi) 
be the length of the shortest unobstructed path between the two points (see Figure 2). We de.ne the detour 
cost between x and xi as: detour(x, xi)= sp(x, xi) d(x, xi) (2) Our modi.ed kernel k thus attenuates 
k: k(x xi)= (x, xi) k(x xi) (3) where the attenuating function (x, xi)is de.ned as: 8 1 if detour(x, 
xi)= 0 (i.e., if xi is visible from x) 0 if detour(x, xi) :r (x, xi)= (4) (i.e., if xi is too far ) 
 1 3t2 +2t3 if detour(x, xi) <r, : where t = detour(x, xi)!r The constant r above is the detour cost 
beyond which a sample has no effect on x. We have found that a value of r =1 works well for (a) (b) (c) 
Figure 3 A pepper at three different scales. Original photograph by Edward Weston [27]. our 4 .4 .lter. 
The cubic polynomial in the third case above was chosen in order to ensure that(x, xi)is C1 continuous. 
Note that the modi.ed kernel k no longer has the property that its weights at the sample points sum to 
one. To compensate, we turn the convolution in equation (1) into a weighted-average convolution: P fi(x, 
xi) k(xxi) f (x)= P(5) (x, xi) k(xxi) This form of convolution has been used previously for .ltering 
non­uniform samples [7, 8, 18]. The kernel modi.cation described above is applicable to any recon­struction 
kernel. In our implementation, we chose the cubic convo­lution kernel described by Wolberg [29]. This 
kernel has negative lobes, as do most interpolating kernels. This property introduces a slight complication 
into our reconstruction algorithm, as it is possi­ble for the magnitude of the sum in the denominator 
of equation (5) to become very small. Dividing by small numbers magni.es any noise in the calculation, 
causing visible bright speckles in the re­constructed image. To overcome this dif.culty, we switch to 
the en­tirely non-negative B-spline kernel [29] whenever the denominator in equation (5) falls below 
a certain threshold. To avoid introduc­ing a discontinuity at places where the switch occurs, we smoothly 
blend between the two kernels as the denominator approaches zero. A precise de.nition of these two kernels 
is given in Appendix A. Our resampling algorithm resembles the distance penalty fault method described 
by Franke and Nielson for surface reconstruction from scattered data with known discontinuities [10]. 
However, our algorithm is specialized to uniform grid data, and by using the gen­eral notion of detour 
cost it can handle arbitrary arrangements of dis­continuity edges. Another difference is that, unlike 
Franke and Niel­son s algorithm, ours is not restricted to positive weighting func­tions, allowing better 
frequency response.  Computing shortest paths To complete the description of our reconstruction algorithm, 
it re­mains to explain how we compute the length of the shortest path between two points. We will refer 
to an endpoint of a discontinu­ity edge as a discontinuity vertex. If a discontinuity vertex is in the 
middle of a chain of discontinuity edges and is thus reachable from multiple sides, we consider each 
side of the vertex to be a distinct discontinuity vertex for the purposes of this minimum distance al­gorithm. 
As a preprocessing step, we compute the distance between all pairs of discontinuity vertices using Dijkstra 
s all-pairs shortest paths algorithm [2]. Then, during reconstruction, to compute the shortest path between 
two particular points x1 and x2 that cannot see each other, we .nd the setsV1 and V2 of discontinuity 
vertices directly visible to x1 and x2, respectively, within a certain ellipse that surrounds them both. 
This ellipse has foci at x1 and x2 and contains all points x such that d(x, x1)+ d(x, x2) .d(x1, x2)+ 
r (6) Any discontinuity vertex beyond this distance would make the de­tour cost larger than the maximum 
detour cost r, thereby forcing the sample s attenuation(x, xi) to zero. Given the sets V1 and V2, the 
length of the shortest path between points x1 and x2 is simply sp(x1, x2)= min fd(x1, v1)+ sp(v1, v2)+ 
d(v2, x2)g(7) v1 2V1,v2 2V2 In order to rapidly determine the set V for any point x we .rst con­struct 
a constrained Delaunay triangulation (CDT) [6] containing (a) (b) Figure 4 An illustration with standard 
.ltering (a) and with our algorithm (b). Both illustrations were produced from images of the same reso­lution. 
Original photograph by Walter Swarthout [26]. all of the discontinuity edges in the image as a preprocessing 
step. Then, given the point x we locate the CDT face containing x. Start­ing from this face, we recursively 
visit all nearby faces that are reachable without crossing discontinuity edges. The vertices of all visited 
faces are tested to see if they are visible from x.To test the visibility of vertex v, we march from 
x towards v in the CDT and stop either when a discontinuity edge is crossed (in which case v is not directly 
visible), or when v is reached (in which case it is visible). It costs O(n3) to compute the shortest 
paths between all pairs of n discontinuity edge vertices. The CDT can be constructed in O(n log n) time. 
Both of these computations are performed only once, in the preprocessing stage. For each point x at which 
the func­tion is reconstructed (i.e., for each pixel location in the resampled image), we need to compute 
the detour cost for as many as sixteen pixels in the original image, as described above. The cost of 
this computation is at worst quadratic in the number of discontinuity ver­tices within the kernel s support, 
but this number is typically small.  Creating and reproducing illustrations Our illustration representation 
consists of a list of discontinuity edges and a grid of grey-scale sample values. To create an illustration, 
we start from a grey-scale image. This im­age can be generated by rendering, digitally painting, or scanning 
in a printed image. We .nd discontinuity edges using Canny s edge detector [5]. Then we compute one sample 
value at each pixel cen­ter. For most pixels, the source image pixel value is a good approxi­mation to 
the sample value. However, for pixels containing discon­tinuities, the pixel value typically corresponds 
to an average of the image function on both sides of the discontinuity and is thus unsuit­able as a sample 
value. In this case we extrapolate the sample value from nearby uncontaminated samples that are reachable 
without crossing discontinuities. Once this process is completed, we reduce the resolution of the sam­ple 
grid as much as possible while maintaining enough detail to create a satisfactory result. If too few 
samples remain, some areas bounded by discontinuities may not contain any reachable sample values. In 
this situation we must allow the reconstruction .lter to cross discontinuities. Thus, reducing the resolution 
too much may have the effect of blurring or eliminating small features altogether, even if their edges 
are part of the representation. Determining the appropriate reduction factor automatically is an interesting 
problem for future work. Once the image has been reduced, we assign to it a stroke texture along with 
an optional set of outline strokes used to surround se­lected regions of the image. The outline strokes 
can be chosen from the set of discontinuity edges. To make use of multiple stroke tex­tures, the image 
can be separated into grey-scale overlays, each of which is associated with its own stroke texture. In 
order to view or print an illustration, we .rst produce a grey-scale image of the desired size with the 
algorithm described in Section 2. Since we want to avoid unnaturally sharp edges in the .nal illustra­tion, 
we only magnify the image to one half of the desired size with our reconstruction algorithm. We then 
expand the image by an addi­tional factor of two using a standard separable reconstruction .lter, which 
can be applied much more quickly in a separate stage. This technique generates just enough blurring along 
the edges to give the illustrations a hand-drawn feel. We .nally re-render the illustration with a stroke 
texture in an auto­matic process called blasting. The blasting algorithm takes a grey­scale image and 
a stroke texture as input and creates an illustration with strokes, which, when taken together, produce 
the same tone as was present in the underlying image. We use the same approach as Salisbury et al. [23] 
for producing strokes and placing them into an illustration. This approach consists of repeatedly selecting 
a stroke from the stroke texture and computing the tone in the vicinity of the stroke that would result 
if it were added. The stroke is rejected if it makes the illustration darker than the tone of the underlying 
image anywhere along its length. Whereas the system of Salisbury et al. generates candidate strokes only 
underneath the user s brush, we automatically place strokes that cover the entire illustration. Figure 
5 Keys. Original photograph by Randy Miller [17].  4Results We present several illustrations that demonstrate 
the various capa­bilities of our representation. Figure 3 shows an illustration rendered at three scales. 
Each of these scales maintains the correct tone and gives the same overall texture effect. Note that 
different collections of actual strokes were used to generate the illustration at the various sizes. 
Compare these illustra­tions to those in Figure 1, where the same set of strokes was used for each scale, 
resulting in undesirable alterations in tone and over­all effect. Figure 4 demonstrates the advantage 
of maintaining and respecting discontinuities. Figure 4(a) is an illustration produced by blasting an 
image that was rescaled without maintaining discontinuities. The il­lustration in 4(b) is the same as 
4(a) except that it was rescaled us­ing our magni.cation algorithm. Notice that the outline edges do 
not align with the edges of the .ngers in illustration 4(a). Also of inter­est is the use of multiple 
textures in both Figures 4 and 5. The illustrations in Figures 6 and 7 are close-ups of the lower-right­hand 
corner of Figure 5 and show the potential for using our rep­resentation to generate poster-sized illustrations. 
Figure 6(a) shows the pixel values stored by the representation, which along with the discontinuity edges 
in 6(b) can be used to generate 6(d) and, in turn, 7(b). The images in 6(c) and 6(d) are the grey-scale 
images that were used to blast the illustrations in 7(a) and 7(b). All of the edges in these and the 
other illustrations we present were found automat­ically with our edge detector. Notice in Figure 6(b) 
that the key is not completely surrounded by edges. In places where no discontinuity edges are present, 
our recon­struction produces smooth changes in grey-scale, even in the vicin­ity of discontinuity endpoints. 
Table 1 gives the storage requirements and reconstruction times re­quiredforthe illustrationsinthispaper. 
The REPRESENTATION col­umn gives the number of pixels stored on disk, the number of dis­continuities 
used, and the total size of both in kilobytes. The OUT-PUT columngivesthesizeofthereconstructed grey-scaleimagethat 
(a) (b) (c) (d)  Figure 6 Close-ups of Figure 5: (a) the underlying low-resolution grey-scale image; 
(b) the discontinuity edges used; (c) the grey-scale image produced using standard magni.cation; and 
(d) the image pro­duced by our resampling algorithm. was used for blasting, and the size, in megabytes, 
of the PostScript .le used for printing. Finally, the TIME column gives the time re­quired to pre-process, 
enlarge, and blast the image with strokes. These times were measured on a Silicon Graphics workstation 
with a 250MHz R4400 processor. To summarize the table, our represen­tation reduces the storage requirements 
of these illustrations by a factor of 100 1000, and it takes from 1 7 minutes to render them. 5 Future 
work Our experience with the proposed technique suggests several areas for future research: PostScript 
renderer. The reduced size of our representation cur­rently offers no practical advantage in transmission 
to printers or web browsers because they do not recognize our representation. One solution is to write 
rendering code in the languages that these devices do understand. For example, a PostScript printer could 
be sent illustration rendering code written in PostScript along with several standard stroke textures. 
This code could then generate an illustration at any requested scale and resolution directly on the output 
device. Similar programs could be written in Java or Ac­robat to allow web browsers to render illustrations. 
 Scalable textures. Currently our stroke textures are applied at a single scale: if the illustration 
is magni.ed, the texture shrinks relative to the image. While this effect is acceptable for uniform textures, 
it could be objectionable for textures with recognizable patterns. It might be better to have a multiresolution 
stroke tex­ture that could change scale with the illustration. Then, as the scale of the texture increased, 
.ner resolution strokes could au­tomatically be added to the illustration.  Combining with multiresolution 
images. Our representation and resampling algorithm are currently limited to traditional unires­olution 
images. We would like to extend our technique to handle multiresolution image representations, such as 
the one described by Berman et al. [4]. In this case, we would also want to develop  (a) (b) Figure 
7 Illustrations produced from Figures 6(c) and 6(d). REPRESENTATION OUTPUT TIME Fig Content ImgSize (pixels) 
# Edges Storage (KB) ImgSize (pixels) PS Size (MB) PreProc (sec) Enlarge (sec) Blast (sec) 3a Pepper 
102.128 270 10.5 204.256 0.2 39 3 5 3b Pepper 102.128 270 10.5 408.512 1.0 39 35 11 3c Pepper 102.128 
270 10.5 816.1024 3.8 39 108 35 4a Hand 64.64  1.2 1024.1024 1.0 3 45 " Egg 128.128  1.4 1024.1024 
1.0  4 128 4b Hand 64.64 237 2.3 1024.1024 1.0 19 130 41 " Egg 128.128 143 1.9 1024.1024 1.0 8 41 134 
5Keys 128.128 504 7.9 1024.1024 2.4 220 142 43 " Shadow 64.64 111 1.5 1024.1024 2.4 3 100 120 7a Key 
closeup 27.22  0.7 864.704 1.9 2 29 " Shadow 27.22  0.2 864.704 1.9 2 88 7b Key closeup 27.22 107 
1.0 864.704 2.0 8 195 27 " Shadow 27.22 6 0.3 864.704 2.0 130 84 8 Billiard 71.61 326 5.4 568.488 1.3 
47 81 19 Table 1 Illustration sizes and speeds. a multiresolution discontinuity representation in which 
different References discontinuities could be present at different scales. [1] Adobe Systems Incorporated. 
PostScript Language Reference Man­ Image compression. Given our algorithm s ability to reproduce ual. 
Addison Wesley, Reading, Massachusetts, 2nd edition, 1994. large images from a compact representation, 
it is natural to con­ [2] Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. Data sider the possibility 
of using it as a more general image compres- Structures and Algorithms. Addison-Wesley, Reading, Massachusetts, 
sion mechanism. One complication with such an approach may 1987. be the lack of texture detail in the 
reconstructed images. [3] Arthur Beck. Photograph. In Photographis 81, The International An­ nual of 
Advertising and Editorial Photography, p. 151. Graphis Press Acknowledgments Corp., 1981. [4] Deborah 
F. Berman, Jason T. Bartell, and David H. Salesin. Multireso- We would like to thank Sean Anderson for 
his help early in the lution painting and compositing. In Computer Graphics Proceedings, project. Annual 
Conference Series, pp. 85 90. ACM Press, July 1994. This work was supported by an Alfred P. Sloan Research 
Fellow-[5] John Canny. A computational approach to edge detection. IEEE Trans­ship (BR-3495), an NSF 
Postdoctoral Research Associates in Ex-actions on Pattern Analysis and Machine Intelligence, 8(6):679 
698, November 1986. perimental Sciences award (CDA-9404959), an NSF Presidential Faculty Fellow award 
(CCR-9553199), an ONR Young Investigator [6] L. Paul Chew. Constrained delaunay triangulations. Algorithmica, 
award (N00014-95-1-0728), a grant from the Washington Technol-4:97 108, 1989. ogy Center, and industrial 
gifts from Interval, Microsoft, and Xerox. [7] Robert L. Cook. Stochastic sampling in computer graphics. 
Transac­ tions on Graphics, 5(1):51 72, January 1986. [8] Mark A. Z. Dipp´e and Erling Henry Wold. Antialiasing 
through stochastic sampling. Computer Graphics, 19(3):69 78, July 1985. [9] Gershon Elber. Line art rendering 
via a coverage of isoparametric curves. IEEE Transactions on Visualization and Computer Graphics, 1(3):231 
239, September 1995. [10] Richard Franke and Gregory M. Nielson. Surface approximation with imposed conditions. 
In R. E. Barnhill and W. Boehm, editors, Surfaces in CAGD, pp. 135 146. North-Holland Publishing Company, 
1983. [11] Arthur L. Guptill. Rendering in Pen and Ink. Watson-Guptill Publica­tions, New York, 1976. 
[12] Paul E. Haeberli. Paint by numbers: Abstract image representations. Computer Graphics, 24(4):207 
214, August 1990. [13] Paul Heckbert. Discontinuity meshing for radiosity. In Third Eu­rographics Workshop 
on Rendering, pp. 203 226, Bristol, UK, May 1992. [14] John Lansdown and Simon Scho.eld. Expressive rendering: 
A review of nonphotorealistic techniques. IEEE Computer Graphics and Appli­cations, 15(3):29 37, May 
1995. [15] Dani Lischinski, Filippo Tampieri, and Donald P. Greenberg. Discon­tinuity meshing for accurate 
radiosity. IEEE Computer Graphics and Applications, 12(6):25 39, November 1992. [16] T. Marshall. Lively 
pictures (Power Mac image editing). BYTE, 20(1):171 172, January 95. [17] Randy Miller. Photograph. In 
Photographis 77, The International An­nual of Advertising and Editorial Photography, p. 72. Graphis Press 
Corp., 1977. [18] Don P. Mitchell. Generating antialiased images at low sampling den­sities. Computer 
Graphics, 21(4):65 72, July 1987. [19] Ken Perlin and Luiz Velho. Live Paint: painting with procedural 
mul­tiscale textures. In Computer Graphics Proceedings, Annual Confer­ence Series, pp. 153 160. ACM Press, 
August 1995. [20] Yachin Pnueli and Alfred M. Bruckstein. D¨Digi urer a digital en­graving system. The 
Visual Computer, 10(5):277 292, 1994. [21] Takafumi Saito and Tokiichiro Takahashi. NC machining with 
G­buffer method. Computer Graphics, 25(4):207 216, July 1991. [22] David Salesin, Daniel Lischinski, 
and Tony DeRose. Reconstructing illumination functions with selected discontinuities. In Third Euro­graphics 
Workshop on Rendering, pp. 99 112, Bristol, UK, May 1992. [23] Michael P. Salisbury, Sean E. Anderson, 
Ronen Barzel, and David H. Salesin. Interactive pen-and-ink illustration. In Computer Graph­ics Proceedings, 
Annual Conference Series, pp. 101 108. ACM Press, July 1994. [24] Gary Simmons. The Technical Pen. Watson-Guptill 
Publications, New York, 1992. [25] Thomas Strothotte, Bernhard Preim, Andreas Raab, Jutta Schumann, and 
David R. Forsey. How to render frames and in.uence people. Computer Graphics Forum, 13(3):455 466, 1994. 
Eurographics 94 Conference issue. [26] Walter Swarthout. Photograph. In Photographis 75, The International 
Annual of Advertising, Editorial, and Television Photography, p. 133. Graphis Press Corp., 1975. [27] 
Edward Weston. Aperture Masters of Photography, Number Seven, p. 29. Aperture Foundation, Inc., New York, 
1988. [28] Georges Winkenbach and David H. Salesin. Computer-generated pen­and-ink illustration. In Computer 
Graphics Proceedings, Annual Con­ference Series, pp. 91 100. ACM Press, July 1994. [29] George Wolberg. 
Digital Image Warping. IEEE Computer Society Press, Los Alamitos, California, 1990. [30] Y. Yomdin and 
Y. Elichai. Normal forms representation: a technology for image compression. In Image and Video Processing, 
volume 1903 of Proceedings of the SPIE The International Society for Optical Engineering, pp. 204 214. 
SPIE, February 1993. [31] S. Zhong and S. Mallat. Compact image representation from multi­scale edges. 
In Proceedings. Third International Conference on Com­puter Vision, pp. 522 525. IEEE Computing Society 
Press, December 1990. Figure 8 Billiard balls. Original photograph by Arthur Beck [3].  Appendix A: 
Kernel de.nitions The cubic convolution kernel we used is a member of the following family of kernels 
[29]: . (a +2)jxj3(a +3)jxj2+1 0 .jxj<1 k(x)= ajxj35ajxj2+8ajxj4a 1 .jxj<2 (8) 02 .jxj We chose the 
value a =0. 5, which makes the Taylor series ap­proximation of the reconstructed function agree in as 
many terms as possible with the original signal. The non-negative B-spline kernel we used is de.ned as 
follows: . 3 13jxj6jxj2+4 0 .jxj<1 k(x)= 6 jxj3+6jxj212jxj+8 1 .jxj<2 (9) 02 .jxj  This kernel is called 
the Parzen window.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237287</article_id>
		<sort_key>469</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[Rendering parametric surfaces in pen and ink]]></title>
		<page_from>469</page_from>
		<page_to>476</page_to>
		<doi_number>10.1145/237170.237287</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237287</url>
		<keywords>
			<kw><![CDATA[comprehensible rendering]]></kw>
			<kw><![CDATA[controlled-density hatching]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[outlining]]></kw>
			<kw><![CDATA[pen-and-ink rendering]]></kw>
			<kw><![CDATA[resolution-dependent rendering]]></kw>
			<kw><![CDATA[shadow algorithms]]></kw>
			<kw><![CDATA[stroke textures]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P96195</person_id>
				<author_profile_id><![CDATA[81100329766]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Georges]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Winkenbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Seattle, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Seattle, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>74343</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Norman Chin and Steven Feiner. Near real-time shadow generation using BSP trees. Computer Graphics, 23(3):99-106, 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Manfredo E do Carmo. Differential Geometry of Curves and Surfaces. Prentice-Hall, Inc., Englewood Cliffs, New Jersey, 1976.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91422</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Debra Dooley and Michael F. Cohen. Automatic illustration of 3D geometric models: Lines. Computer Graphics, 24(2):77-82, March 1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949579</ref_obj_id>
				<ref_obj_pid>949531</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Debra Dooley and Michael F. Cohen. Automatic illustration of 3D geometric models: Surfaces. In Proceedings of Visualization '90, pages 307-314, October 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614310</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Gershon Elber. Line art rendering via a coverage of isoparametric curves. IEEE Transaction on Visualization and Computer Graphics, 1(3):231-239, September 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97890</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gershon Elber and Elaine Cohen. Hidden curve removal for free form surfaces. Computer Graphics, 24(4):95-104, August 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[H. Fuchs, Z. M. Kedem, and B. F. Naylor. On visible surface generation by a priori tree structures. Computer Graphics, 14(3):124-133, July 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74369</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Michel Gangnet, Jean-Claude Herv6, Thierry Pudet, and Jean-Manuel Van Thong. Incremental computation of planar maps. Computer Graphics, 23(3):345-354, July 1989.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Arthur Leighton Guptill. Rendering in Pen and Ink. Watson-Guptill Publications, New York, 1976.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63256</ref_obj_id>
				<ref_obj_pid>63252</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Christoph Hoffman. The problems of accuracy and robustness in geometric computation. Computer, 22:31-42, 1989.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618268</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[John Lansdown and Simon Schofield. Expressive rendering: A review of nonphotorealistic techniques. IEEE Computer Graphics and Applications, 15(3):29-37, May 1995.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Wolfgang Leister. Computer generated copper plates. Computer Graphics Forum, 13(1):69-77, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Frank Lohan. Pen and Ink Techniques. Contemporary Books, Inc., Chicago, 1978.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J6r6me Maillot, Hussein Yahia, and Anne Verrout. Interactive texture mapping. Proceedings of SIGGRAPH 93 (Anaheim, California, August 1-6, 1993). In Computer Graphics, Annual Conference Series, 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Bruce Naylor and Lois Rogers. Constructing partitioning trees from B6zier-curves for efficient intersection and visibility. In Proceedings of Graphics Interface '95, pages 44-55, 1995.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218458</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Hans Kohling Pedersen. Decorating implicit surfaces. Proceedings of SIGGRAPH 95 (Los Angeles, California, July 6-11, 1995). In Computer Graphics, Annual Conference Series, 1995.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Tom Porter and Sue Goodman. Manual of Graphic Techniques 4. Charles Scribner's Sons, New York, 1985.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Takafumi Saito and Tokiichiro Takahashi. Comprehensible rendering of 3D shapes. Computer Graphics, 24(4): 197-206, August 1990.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>145734</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[David H. Salesin. Epsilon Geometry: Building Robust Algorithms from Imprecise Computations. PhD thesis, Stanford University, March 1991. Available as Stanford Report number STAN-CS-91-1398.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Casting curved shadows on curved surfaces. Computer Graphics, 12(3):270-274, August 1978.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>923276</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Georges Winkenbach. Computer-Generated Pen-and-Ink Illustration. PhD thesis, University of Washington, May 1996.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Georges Winkenbach and David H. Salesin. Computer-generated penand-ink illustration. Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24-29, 1994). In Computer Graphics, Annual Conference Series, 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rendering Parametric Surfaces in Pen and Ink Georges Winkenbach David H. Salesin Department of Computer 
Science and Engineering University of Washington Seattle, Washington 98195 Abstract This paper presents 
new algorithms and techniques for rendering parametric free-form surfaces in pen and ink. In particular, 
we intro­duce the idea of controlled-density hatching for conveying tone, texture, and shape. The .ne 
control over tone this method provides allows the use of traditional texture mapping techniques for speci­fying 
the tone of pen-and-ink illustrations. We also show how a pla­nar map, a data structure central to our 
rendering algorithm, can be constructed from parametric surfaces, and used for clipping strokes and generating 
outlines. Finally, we show how curved shadows can be cast onto curved objects for this style of illustration. 
CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Pic­ture/Image Generation; I.3.6 [Computer 
Graphics]: Methodology and Tech­niques. Additional Key Words: non-photorealistic rendering, comprehensible 
ren­dering, pen-and-ink rendering, resolution-dependent rendering, stroke tex­tures, controlled-density 
hatching, outlining, shadow algorithms. 1 Introduction In many applications from architectural design, 
to medical texts, to industrial maintenance and repair manuals a stylized illustration is often more 
effective than photorealism. Illustrations convey infor­mation better, consume less storage, are more 
easily reproduced, are more capable of conveying information at various levels of detail, and are in 
many respects more attractive than photorealistic images. In a previous paper [22], we introduced a system 
for automatically generating pen-and-ink illustrations of three-dimensional architec­tural models. In 
that paper, we showed how many of the principles of traditional pen-and-ink rendering, such as achieving 
tones through texture, could be simulated algorithmically. In particular, we intro­duced the concept 
of a prioritized stroke texture , which is used to reproduce arbitrary tones and convey textures simultaneously. 
However, this earlier work was limited to polyhedral models. With curved surfaces, a number of the fundamental 
assumptions we used break down. Most notably, in this earlier work we assumed .at­shaded surfaces, and 
we used BSP trees both for creating a planar map data structure and for clipping individual strokes quickly. 
In this paper, we generalize our previous work to handle curved surfaces formulated parametrically, such 
as B-splines surfaces, NURBS, and surfaces of revolution. We introduce a mechanism Permission to make 
digital or hard copies of part or all of this work or personal or classroom use is granted without fee 
provided that copies are not made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 1996 ACM-0-89791-746-4/96/008...$3.50 
for creating controlled-density hatching, which allows strokes to gradually disappear in light areas 
of a surface or in areas where too many strokes converge together, and allows new strokes to grad­ually 
come into existence in dark areas or areas in which the ex­isting strokes begin to diverge too much. 
With controlled-density hatching, we are able to exert .ne-grain control over the tone de­picted in various 
areas of a pen-and-ink image. We use this newly ac­quired ability, together with traditional (image-based) 
texture map­ping techniques, to extend considerably the repertoire of effects that can be achieved with 
stroke textures. We demonstrate these effects with texture maps used for surface detail, bump mapping, 
and envi­ronment ( re.ection ) mapping. In addition, we show how a planar map can be created and strokes 
ef.ciently clipped without the use of BSP trees. Finally, we describe a simple method to handle the cast­ing 
of curved shadows onto curved objects. 1.1 Related work A few authors have addressed similar goals in 
their published work. Dooley and Cohen proposed a system to enhance a traditional shaded images with 
illustration techniques [3, 4]. They showed how line and surface qualities could be customized by the 
user to create more effective images. Saito and Takahashi [18] used G-buffers and image processing techniques 
to enhance ray-traced images with illustration features. Their system handles outlining, hatching, and 
shadows. However, the use of strokes that we propose allows perhaps more expressive­ness and extends 
the range of illustrations that can be created auto­matically. Leister presented a technique to emulate 
copper-plate render­ing [12], an engraving technique used for old styles of printing. A ray-tracing approach 
is used to render curves on free-form objects. These curves lie at the intersection of parallel planes 
with the 3D ob­ject being rendered. An advantage of this approach is that it easily handles re.ections 
and shadowing. The Piranesi system proposed by Lansdown and Scho.eld [11] also uses non-photorealistic 
techniques to create illustrations from 3D models. Piranesi uses a standard graphics pipeline to create 
a 2D ref­erence image akin to a G-buffer. The user is then allowed to select speci.c regions of the image 
and apply textures that emulate natural media interactively or automatically. Elber [5] described an 
algorithm to cover NURBS surfaces with iso­parametric curves, thus emulating a form of line-art rendering. 
How­ever, his approach does not address a number of the issues in pen­and-ink illustration considered 
in this paper, such as building tone with stroke textures, outlining objects only when necessary, and 
ren­dering shadows. 1.2 Overview The rest of this paper is organized as follows. Section 2 gives a brief 
review of some of the key principles of pen-and-ink illustration, and summarizes the system architecture 
used for creating illustrations of polyhedral models. Section 3 describes the various algorithms that 
lie at the heart of our system. Section 4 describes the particu­lar stroke textures we used for the .gures 
in this paper, and gives statistics for these results. Finally, Section 5 suggests some areas for future 
work.  2 Background In this section, we brie.y review some of the principles of pen-and­ink illustration; 
much more detailed studies can be found in a num­ber of texts [9, 13, 17]. We then describe some of the 
key architec­tural features of the pen-and-ink illustration system we introduced in our previous work, 
upon which the results in this paper are based. 2.1 Principles of pen-and-ink illustration Some of the 
key principles of pen-and-ink illustration include:  Strokes. Strokes are the fundamental building-blocks 
of pen-and­ink illustration. The thickness and density of the strokes is varied to achieve subtle shading 
effects. In addition, strokes should also have some variation in thickness and waviness so as not to 
appear too mechanical.  Texture. Texture results from a large number of pen strokes placed in juxtaposition. 
The character of the strokes is important for con­veying texture for example, crisp, straight lines are 
good for glass, whereas rough, sketchy lines are good for old mate­rials.  Tone. The perceived grey 
level or tone is a function of the den­sity of the strokes in a particular region of the illustration. 
The same strokes that are used to convey texture must also be used to achieve the desired tone.  Outline. 
Outlining play an essential role in illustration; indeed, outlining is one of the key features that differentiates 
illustration from photorealistic imagery. Outlines are generally introduced only where they are required 
to disambiguate regions of similar tone. The quality of the outline stroke must also be varied to con­vey 
texture.   2.2 System architecture for polyhedral models The system for automatically producing pen-and-ink 
illustrations of polyhedral models, upon which the results in this paper are based, is not very different 
from a traditional photorealistic renderer. The input to the system consists of a 3D polyhedral model, 
one or more light sources, and a camera speci.cation. The output is an illustra­tion in the style of 
pen and ink. To render a scene, the polyhedral renderer begins by computing the visible surfaces and 
the shadow polygons, using 3D BSP trees for both operations [1, 7]. The outcome is a set of convex polygons 
that can be ordered in depth with respect to the view point. The ren­derer uses these polygons to build 
both a 2D BSP tree and a pla­nar map representations of the visible surfaces in the scene. It then renders 
each region in the planar map using a procedural stroke tex­ture. The collection of strokes required 
to render each .at-shaded surface is generated without considering occlusions. Each stroke is then clipped 
against the visible portions of the surface using the 2D BSP tree. Finally, the outline strokes are drawn 
by considering all the edges of the planar map, and rendering only those edges neces­sary for the illustration, 
according to the outlining principles.  3 Algorithms The pen-and-ink rendering system that we describe 
in this paper uti­lizes the same basic architecture as the polygonal renderer we intro­duced in our earlier 
work. It also uses the same procedural stroke texture idea, and relies on a planar map for generating 
outlines and clipping strokes. However, in the presence of free-form surfaces, many of the techniques 
used in the polygonal renderer no longer work. In this section, we present our solutions to these problems. 
3.1 Controlled-density hatching To produce pen-and-ink illustrations from parametric surfaces, the most 
fundamental change from a polyhedral renderer is in the gen­eration of the stroke textures. Curved surfaces 
require a much more sophisticated approach than .at-shaded polygonal surfaces, which can be hatched in 
a uniform fashion. First, we will need a way of orienting the hatching strokes along a surface. Second, 
we will need some mechanism for allowing strokes to gradually disappear in light areas of a surface or 
in areas where too many strokes converge together. Conversely, we will also need to allow new strokes 
to gradually appear in dark areas or areas in which the existing strokes begin to diverge too much. We 
will call such a mechanism controlled-density hatching. 1 To solve the .rst problem, that of choosing 
an orientation for the hatching strokes, we simply use a grid of lines in the parameter do­main (u;v). 
The grid consists of parallel lines running in one or more user-speci.ed directions. For most illustrations, 
we simply use isoparametric curves, which run parallel to uand/or v. We now turn to the second problem, 
that of achieving controlled­density hatching. Achieving a given tone by hatching an arbitrary parametric 
surface is a non-trivial problem. Figure 1 illustrates the dif.culty, even for the case of a simple two-dimensional 
transfor­mation. In this case, rendering isoparametric curves with constant thickness results in an image 
with varying tones. Our solution is to adjust the thickness of the strokes in order to keep the apparent 
tone constant. Figure 2 illustrates the same concept, but in this case, for a perspective view of a sphere. 
In order to solve this problem formally, we begin by de.ning a stroke Ias a pair of functions (A(t);((t)),where 
A(t)is a line in the parameter domain (u;v),and ((t)is a thickness function,which describes the thickness 
used in rendering the stroke at every param­eter value t. Furthermore, we de.ne the apparent tone of 
an image in the neighborhood of a given point in image space (x;y)to be the ratio of the amount of ink 
deposited in that neighborhood to its area. If the point (x;y)happens to lie on a stroke, the apparent 
tone can also be expressed as the ratio (d,where (is the thickness of the stroke and dis its image-space 
separation from adjacent strokes. With these de.nitions, the controlled-density hatching problem can 
be formally stated as follows. Given: A parametric surface a:(u;v)7!(xw;yw;zw), which maps points in 
the parameter domain (u;v)to points in world space (xw;yw;zw);  a perspective viewing transformation 
V:(xw;yw;zw)7! (x;y), which maps (visible) points in world space to points in image space (x;y);  a 
hatching direction h=(hu;hv)in the parameter domain; and  a target tone function T(x;y).  Find: A set 
of strokes Ii=(Ai;(i), with lines Aiin the parame­ter domain running parallel to the hatching direction 
h, such that the apparent tone of mapping the strokes through M=V0a is T(x;y). 1For this work, we consider 
only surfaces with a global parameteriza­tion, such as B-spline surfaces, NURBS, and surfaces of revolution. 
Ideas for generalizing to a broader class of surfaces, such as patch-based surfaces and smoothly-shaded 
polygonal meshes, are discussed in Section 5. Figure 1 Controlled-density hatching for a simple 2-dimensional 
transformation M:(u;v)7!(u;v+v·exp(sin(u))). Rendering isoparametric curves with constant thickness results 
in an image with varying tones (left). We adjust the thickness of the strokes in order to keep the apparent 
tone constant (right). The key step in solving this problem will be to determine exactly how the images 
of two parallel lines in the parameter domain con­verge and diverge when seen in image space.2In particular, 
let Ai(t) and Ai+1(s)be two parallel lines that are dunits apart in the param­eter domain, and let A0 
i(t)and A0 i+1(s)be their images, under M, in image space. We would like to know the distance d0between 
the two image-space curves as a function of t. Oncewe havethis dis­tance function d0 (t), we can use 
it to adjust the thickness and spacing of the strokes to compensate for any spreading or compression. 
A simple closed-form expression for the distance between the two curves A0 i(t)and A0 i+1(s)does not 
exist in general, so we seek to approximate it. We begin by writing the mapping Mas two scalar­valued 
functions M(u;v)=(X(u;v);Y(u;v)) To approximate the behavior of Min a small neighborhood about (u;v)we 
consider the Jacobian matrix hi XuXvJ(u;v)= YuYv where Xu;Xvand Yu;Yvare the partial derivatives of Xand 
Y with respect to uand vrespectively. The Jacobian matrix Jis a lin­ear transformation, which can be 
thought of as a Taylor expansion of Min the neighborhood of (u;v)truncated to the .rst-order terms. Under 
J, the two parallel lines Aiand Ai+1in parameter space map to parallel lines, denoted by J(Ai)and J(Ai+1), 
in image space. To estimate how much the curves A0 iand A0 i+1diverge, we look at the ratio Piof the 
distance d0 ibetween the lines J(Ai)and J(Ai+1) in image space, and the distance dibetween the lines 
Aiand Ai+1 in the parameter domain. If the line Aiis given by the implicit-form coef.cients ha;b;ci, 
then the ratio Piis given by (see Appendix A): r d0 22 i (XuYv-XvYu)(a+b2) Pi == (1) di (aYv-bYu)2 +(bXu-aXv)2 
The ratio Pi(t)is a scalar-valued function that approximates how the distance between strokes is altered 
by the mapping M. We call Pi 2Note that the degree to which strokes in image space converge or diverge 
is dictated not only by the parametric surface, but also by the .nal projection to image space. Figure 
2 Controlled-density hatching for a perspective view of a sphere. Again, rendering isoparametric curves 
with constant thick­ness results in an image with varying tones (left). Using varying stroke thicknesses 
keeps the apparent tone constant (right). the stretching factor of M.When Piis large, the lines spread 
apart; when Piis small, they compress together. The maximum stretching factor Pi =supt(Pi(t))taken along 
the line Aiplays a special role: given two lines Aiand Ai+1offset by the distance d, it allows us to 
0 evaluate the maximum spacing di =Pi dbetween the two corre­sponding strokes. We are now ready to generate 
strokes so as to achieve the target tone T(x;y). We will do this in four steps. First, we must decide 
what the maximum distance d0between two strokes in the image should be. This value is dictated by the 
max­imum (or darkest) tone Tthat must be achieved anywhere on the surface, and the maximum stroke thickness 
(set by the user. To guar­antee that Tcan be achieved, given (, the strokes need to be spaced by no more 
than d0 =(Ton the image plane. Second, we note that because the stretching factor Piis derived from a 
.rst-order approximation of M, it is accurate only for very small steps in parameter space. In practice, 
however, the strokes must be spread apart by a comparatively large distance. To work around this problem, 
we use a stepping technique. To spread two strokes by a distance d0, we take a series of small step of 
size 5in parameter space, updating the stretching factor after each step. Stepping starts from the line 
Ai, and proceeds until the accumulated image-space P distance, given by jP5, equals or exceeds d0. In 
our implemen­ j tation, 5is set to 0:01d0Pi . Third, we must modulate the thickness of the strokes to 
accurately render the tone T(x;y). Two factors in.uence the thickness of the stroke A0 i(t): the actual 
image-space distance d0 (t)between the strokes, and the tone to be achieved. The stepping algorithm devised 
in the .rst step guarantees that two adjacent strokes are spread by at most d0. However, the actual spacing 
d0 (t)can be smaller. To com­pensate for this variation, the thickness of stroke A0 imust be scaled by 
the ratio d0 (t)d0 Pi(t)Pi. Finally, to take into account the varying tone, we also scale the stroke 
thickness by the ratio T(t)T. In summary, the thickness of A0 i(t)is given by T(t)Pi(t)(i(t)= ( TP i 
Finally, we introduce an additional feature to create more interesting hatching. Although the strokes 
generated by the method above ac­curately render the target tone, the thicknesses of all the strokes 
vary simultaneously. A more appealing effect is achieved when short and long strokes are interspersed, 
as depicted on the right side of Fig­ure 1. This effect is created by introducing an additional spreading 
factor a, set by the user. The initial strokes are spread by the distance ad0instead of d0. The extra 
gaps created are then recursively .lled with additional .ller strokes. The expression for the thickness 
(i(t)of a .ller stroke Iiat level ` of recursion is slightly more complicated. To derive it, we .rst 
note that the image-space distance between two adjacent strokes at level ` of recursion is given by adPi(t)a(Pi(t)d0 
` (t)== (2) 2 `P2 `P i Ti With this style of recursive hatching, we would like to achieve the target 
tone by using the thickest possible strokes, before introducing a .ller stroke at level `. Consequently, 
if (i(t)>0for some t,then the thickness of the neighboring strokes at recursion level `-1is (. 0 The 
contribution to the tone from these strokes is T`.1=(d`.1, while stroke Iicontributes T` =(i(t)d0 . Finally, 
the overall target ` 0 tone to achieve is T(t)=T`.1+T` =(d`.1+(d`. Substitut­ing for d0 `.1and d0 `using 
equation (2), and noting that (i(t)cannot exceed (, yields (i(t)=min{( ; aT(t) 2 `T P i(t) P i - 1 2 
 ( The recursion stops when (i(t) 0everywhere along the stroke. The minimum thickness of a stroke is 
dictated either by the pixel size, or by a constant set by the user. However, visually thinner sizes 
can still be obtained by using dashed strokes. The hatching textures of all the .gures shown in this 
paper were gen­erated using this recursive algorithm. Typically, aranges between 2 and 8.  3.2 The planar 
map As discussed in Section 1.2, a key data structure of the pen-and-ink illustration system for polyhedral 
models was a planar map of all the visible surfaces and shadow polygons. This planar map was con­structed 
with the help of 2D and 3D BSP trees [22]. Recent results introduced by Naylor and Rogers [15] show how 
to build 2D BSP trees with B´ezier curves. However, it is not clear how this work can be generalized 
to handle scenes containing parametrically de.ned curved surfaces. It is also not clear how Chin and 
Feiner s BSP­tree-based shadow algorithm can be generalized in the presence of curved surfaces. For these 
reasons, we devised a method for com­puting the planar map and the shadows that does not rely on BSP 
trees. 3.2.1 Constructing the planar map The planar map data structure partitions the image plane into 
homo­geneous regions so that each region corresponds to a single visible object in the scene. In our 
new algorithm, the planar map is con­structed in three main phases. In the .rst phase, we tessellate 
every object in the scene into a polyg­onal mesh. The resolution of the tessellation is chosen so as 
to yield a reasonably-accurate approximation to the object. Our implemen­tation uses a .xed resolution 
set by the user, although a .atness cri­terion could also be used. In the second phase, we compute higher-resolution 
piecewise-linear approximations for all the silhouette curves of the meshed objects. This step is required 
to obtain smooth and accurate silhouettes with­out requiring an unduly .ne tessellation. Our technique 
is very simi­lar to the one developed for hidden-curve removal by Elber and Co­hen [6], only it operates 
on a polygonal mesh, rather than on a para­metric surface directly. To .nd the silhouette curves, we 
.rst iden­tify the mesh edges that span the silhouette. To do this, we examine the normal vectors at 
the two endpoints of every edge. If the projec­tions of the normals on the viewing direction are in opposite 
direc­tions, then the edge spans the silhouette. In this case, the two mesh faces adjoining the edge 
are subdivided, and the process is repeated. Our implementation performs a .xed number of subdivisions 
in this manner. Finally, the silhouette curve is further re.ned, using a root­.nding method to evaluate 
a more precise silhouette point along each remaining mesh edge that spans the silhouette. A piecewise­linear 
silhouette curve is then constructed by connecting all of the silhouette points in the mesh. In the third 
and .nal phase, we construct the planar map itself. Ini­tially, the planar map consists of a single region 
corresponding to the entire display area. Each mesh face is then inserted into the pla­nar map in turn. 
First, the face is projected onto the view plane, and its edges are merged with those of the planar map. 
Next, we resolve occlusions between the new face and the existing faces in the planar map that are covered 
by the new face. Our implementation currently assumes non-intersecting objects; thus, for each existing 
face in the planar map, we merely need to determine whether the existing face or the face being inserted 
is closer to the viewer. We do this by sum­ming up the distance from the view point to the 3D point correspond­ing 
to each edge midpoint. Whichever face yields the smallest sum is considered to be closer to the viewer. 
If the face being inserted is closer, the planar map region is updated to re.ect the new informa­tion. 
Once all objects have been inserted into the planar map, each result­ing region corresponds to a single 
visible 3D face in the mesh de­composition. In our implementation, we maintain a link from each region 
to its 3D face. In turn, each 3D mesh maintains a link to its original object. These links are used by 
the procedural stroke textures to compute a variety of information, as described in Sec­tion 3.3.  3.2.2 
Robustness issues A common problem in geometric algorithms, and one to which our planar map construction 
algorithm is certainly not immune, is that it is not always easy to maintain consistency between the 
topolog­ical and geometric information in the data structure when impre­cise computations like .oating-point 
arithmetic are used [10, 19]. To build the planar map robustly, we use a method inspired by the work 
of Gangnet et al. [8]. Notably, we restrict all the line endpoints to Figure 3 Several cases must be 
considered when tracing outlines (edges labeled o1to o4), and clipping strokes (edges labeled s1 to s3). 
 Figure 4 Creating a pen-and-ink illustration. The steps involved are not so different from those required 
to create an attractive photorealistic rendering. From left to right: constant-density hatching; smooth 
shading with rough strokes, using a single light source; smooth shading with straighter, longer strokes 
adjusted to depict glass; introducing environment mapping; and, .nally, the same image after adjusting 
the re.ection coef.cients, shown at full size in Figure 5. lie on an integer lattice, and we use in.nite-precision 
rational arith­metic to compute all intersections exactly. Because the planar map stores only line segments, 
the number of bits required for the inter­mediate computations is bounded. In particular, we use 14-bit 
inte­gers to represent the lattice points, which allows all intersections to be stored using 32-bit rational 
integer numbers. This choice limits us to a resolution of about 800 dots per inch over a 10.10-inch image-space 
area. (See Winkenbach [21] for more details.)  3.3 Using the planar map As in the original polyhedral 
renderer, the planar map is used for rendering outline edges. In this work, the planar map is additionally 
used for clipping individual strokes to visible regions. Here we con­sider how these two processes can 
be implemented for curved sur­faces. 3.3.1 Outlining Object outlines are constructed from edges of the 
planar map. With curved surfaces, four types of planar map edges can give rise to an outline edge (see 
Figure 3): Case o1: an edge that bounds two regions belonging to different objects. The texture for 
such an outline edge is taken from the ob­ject closest to the view point. In the case of two abutting 
surfaces, this choice is arbitrary.  Case o2: an edge that bounds two regions belonging to the same 
object, but whose corresponding 3D mesh faces have opposite orientations.  Case o3: an edge that bounds 
two regions belonging to the same object and having the same orientation, but at different depths.  
Case o4: an edge that arises from a C1discontinuity on the sur­face.  An outline path is assembled by 
appending as many adjacent outline edges as possible. The darkness of the stroke along an outline edge 
is affected by two factors: The tone value on the surface letting an outline edge fade away in regions 
of highlight reinforces the quality of the shading.  The contrast between two adjoining surfaces if 
the tone differ­ence between the two adjacent surfaces is small, a darker outline is required to mark 
the boundary.  The degree to which these criteria affect the outline is selectable by the user. 3.3.2 
Generating stroke paths Both outline and texture strokes are initially constructed as 3D poly­line curves, 
called stroke paths. Each stroke path vertex stores sev­eral items of information, including the parametric 
coordinate, the corresponding 3D position, and the tone evaluated on the surface at that point. The number 
of vertices in the path is adjusted using a sub­division algorithm, with the goal of accurately capturing 
not only the shape of the stroke, but also any variation in tone. The latter is par­ticularly important 
when texture, bump, or environment maps with small features are present.  3.3.3 Clipping stroke paths 
Using the planar map, we generate strokes for all surfaces that are at least partially visible. However, 
these strokes can potentially extend into invisible regions. To clip the strokes in the presence of curved 
surfaces, we break each stroke at the silhouette points, yielding seg­ments that either face toward or 
away from the view point. Each of these segments is then projected onto the planar map and clipped. (If 
the object being rendered is a solid, then back-facing stroke seg­ments can be rejected immediately.) 
The clipping process starts by locating the planar map region in which the stroke s .rst vertex lies. 
The visibility of the stroke is then tested for every planar map edge that the stroke crosses. When the 
visibility changes, a root-.nding method is used to .nd the intersection between the path and the pla­nar 
map edge. The intersection point is then added to the path, break­ing it into smaller segments. Several 
tests are used to determine the visibility of a stroke segment within a face of the planar map. They 
are, in order of application (see Figure 3): 1. Same object the planar map face must be linked to the 
same 3D object that the stroke is covering (eliminates s1). 2. Same orientation the planar map face 
must be linked to a 3D mesh face that has the same orientation (back-facing or front­facing) with respect 
to the view point as the 3D surface point along the stroke path (eliminates s2). 3. Same depth the 
3D location of the path vertex must be close to the corresponding 3D location of the 3D mesh face. This 
last test is required since, with free-form surfaces, several different points on the same surface can 
project to the same 2D point (elimi­nates s3).  If all three tests succeed, the segment is visible and 
marked as such. Only the visible segments of each stroke are drawn.  Figure 5 Glass bottle. An environment 
map is used to give the illu­sion of a re.ected surrounding.  3.4 Shadows In the polygonal version 
of the renderer, polygons were split along shadow boundaries before being inserted into the 2D BSP tree. 
Hence, the different partitions in the BSP-tree would distinguish be­tween regions that were in and out 
of shadow. The shadows were then rendered with strokes clipped to the shadow regions. Unfortu­nately, 
with curved surfaces, shadow boundaries are much more dif­.cult to generate. Thus, we decided to use 
a simpler two-pass clip­ping approach inspired by Williams [20] instead. Shadow strokes are generated 
for all the visible surfaces. To clip these strokes, we build an additional shadow planar map with re­spect 
to the light source, in addition to the view planar map. Each shadow stroke is .rst clipped against the 
view planar map, just like all other strokes. In a second pass, the remaining visible shadow strokes 
are clipped against the shadow planar map; this time, how­ever, just the portions of the strokes that 
are not visible from the light source (and therefore in shadow) are preserved and rendered. Note that 
the view planar map and the shadow planar map lie in dis­tinct 2D spaces. Therefore, each visible stroke 
left after the .rst clip­ping pass must be re-mapped to the shadow planar map space before the second 
clipping pass can take place. 4Results In this section, we demonstrate the rendering algorithms with 
sev­eral examples. The rendering times for these .gures can be found in Table 1. All of the examples 
in this paper were created using an iterative de­sign process, not unlike the procedure typically used 
in creating an attractive photorealistic rendering. First, we generally set up one or more light sources, 
then adjust the quality of the strokes, then add any texture maps, and .nally adjust the various re.ection-model 
pa- Figure 6 Wooden bucket. The bucket is modeled as a single surface of revolution. The planks are created 
by a prioritized stroke texture. rameters until an appealing illustration is achieved. Figure 4 illus­trates 
this process for the glass bottle, shown at full size in Figure 5. 4.1 Texture mapping Controlled-density 
hatching allows .ne grain control over the tone of a pen-and-ink illustration. With this new capability, 
we can use traditional texture mapping techniques to vary the tone on the surface of an object. For example, 
Figure 5 uses an environment map to enhance the illusion of the glass material. Figure 6 uses a bump 
map to perturb the shading on the wooden planks. Figure 7 uses an ordinary texture map to create the 
geometric pattern on the bowl; it also uses a bump map to emboss the word MILK and create a slightly 
irregular surface on the jug.  4.2 Other texture styles The basic hatching algorithm described in this 
paper can be used to generate many other texture styles: Wood. The wood texture shown in Figure 6 uses 
a variety of strokes, much like the prioritized stroke texture for wood used in the polygonal version 
of the renderer. Thin wavy strokes are used to convey wood grain, while longer strokes of varying thickness 
delineate the gaps between the wood boards.  Stippling. Figures 7 and 8 show the use of stippling to 
build tone values. To create the stipples, we generate hatching strokes as de­scribed previously. However, 
the resulting stroke paths are not rendered directly; instead, they serve as curves along which the stipples 
are drawn. The spacing between the stipple marks along the stroke path is randomized. In addition, the 
stipple marks are also offset from the path by a small random distance.  Crosshatching. Figure 7 also 
shows the use of crosshatching using more than one hatching direction to create dark shadow tones. 
Crosshatching is also used on the cane in Figure 8 .   Figure 7 Ceramic jug and bowl. A traditional 
(image-based) texture map is used to model the details on the bowl as well as the stains on the table. 
A bump map is used to emboss the word MILK on the jug, and to give some irregular variation to its surface. 
FigModelPlanarmapRendering 5 Glassbottle7446 6Woodenbucket2144 7Jugandbowl126128 8Hatandcane230120 Table 
1 Rendering times for various illustrations presented in this paper. All times are in seconds, and were 
measured on a Power Mac 7100/80. 5 Conclusion and future work In this paper, we have introduced the concept 
of controlled-density hatching, which allows strokes to be generated so as to simultane­ously convey 
tone, texture, and shape for parametric surfaces. Be­cause controlled-density hatching provides .ne grain 
control of the tone of an illustration, we were also able to use traditional tex­ture mapping techniques 
to extend the range of effects that can be achieved with pen-and-ink rendering. We have also described 
an al­gorithm to construct a planar map from parametric surfaces, and we have shown how this planar map 
can be used for outlining and stroke clipping, in addition to resolving occlusions. Finally, we have 
de­scribed a simple method to render shadows with strokes. Perhaps the biggest limitation of this work 
is that it deals only with surfaces possessing a global parameterization. Unfortunately, many commonly 
used surface representations, such as patch-based sur­faces, implicit surfaces, subdivision surfaces, 
and smoothly-shaded polygonal meshes, do not have this property. One possible solution is to parameterize 
such surfaces, using for example the methods of Maillot et al. [14] or Pedersen [16]. Another alternative 
is to do away with the parameterization altogether, and to instead generate strokes along directions 
that are more intrinsic to the geometry of the surface for example, along directions of principal curvature 
[2]. This ap­proach may also be suitable for mapping stroke textures on polygo­nal meshes, since surface 
curvature can still be approximated in this case [14].    Acknowledgments We would like to thank Jorge 
Stol. for many useful discussions dur­ing the early phase of this project. This work was supported by 
an Alfred P. Sloan Research Fellow­ship (BR-3495), an NSF Presidential Faculty Fellow award (CCR­9553199), 
an ONR Young Investigator award (N00014-95-1-0728), and industrial gifts from Interval, Microsoft, and 
Xerox.  References [1] Norman Chin and Steven Feiner. Near real-time shadow generation using BSP trees. 
Computer Graphics, 23(3):99 106, 1989. [2] Manfredo P. do Carmo. Differential Geometry of Curves and 
Surfaces. Prentice-Hall, Inc., Englewood Cliffs, New Jersey, 1976. [3] Debra Dooley and Michael F. Cohen. 
Automatic illustration of 3D geo­metric models: Lines. Computer Graphics, 24(2):77 82, March 1990. [4] 
Debra Dooley and Michael F. Cohen. Automatic illustration of 3D ge­ometric models: Surfaces. In Proceedings 
of Visualization 90, pages 307 314, October 1990. [5] Gershon Elber. Line art rendering via a coverage 
of isoparametric curves. IEEE Transaction on Visualization and Computer Graphics, 1(3):231 239, September 
1995. [6] Gershon Elber and Elaine Cohen. Hidden curve removal for free form surfaces. Computer Graphics, 
24(4):95 104, August 1990. [7] H. Fuchs, Z. M. Kedem, and B. F. Naylor. On visible surface generation 
by a priori tree structures. Computer Graphics, 14(3):124 133, July 1980. [8] Michel Gangnet, Jean-Claude 
Herv´e, Thierry Pudet, and Jean-Manuel Van Thong. Incremental computation of planar maps. Computer Graphics, 
23(3):345 354, July 1989. [9] Arthur Leighton Guptill. Rendering in Pen and Ink. Watson-Guptill Publications, 
New York, 1976.  Figure 8 Hat and cane. Both the hat and the cane are modeled with B-spline surfaces. 
The ribbon is modeled as a separate B-spline surface. Note the curved shadow that the hat projects on 
its rim, and the use of crosshatching on the curved portion of the cane. [10] Christoph Hoffman. The 
problems of accuracy and robustness in geo­metric computation. Computer, 22:31 42, 1989. [11] John Lansdown 
and Simon Scho.eld. Expressive rendering: A review of nonphotorealistic techniques. IEEE Computer Graphics 
and Appli­cations, 15(3):29 37, May 1995. [12] Wolfgang Leister. Computer generated copper plates. Computer 
Graphics Forum, 13(1):69 77, 1994. [13] Frank Lohan. Pen and Ink Techniques. Contemporary Books, Inc., 
Chicago, 1978. [14] J´er ome Maillot, Hussein Yahia, and Anne Verrout. Interactive texture mapping. Proceedings 
of SIGGRAPH 93 (Anaheim, California, Au­gust 1-6, 1993). In Computer Graphics, Annual Conference Series, 
1993. [15] Bruce Naylor and Lois Rogers. Constructing partitioning trees from B´ezier-curves for ef.cient 
intersection and visibility. In Proceedings of Graphics Interface 95, pages 44 55, 1995. [16] Hans Køhling 
Pedersen. Decorating implicit surfaces. Proceedings of SIGGRAPH 95 (Los Angeles, California, July 6-11, 
1995). In Com­puter Graphics, Annual Conference Series, 1995. [17] Tom Porter and Sue Goodman. Manual 
of Graphic Techniques 4. Charles Scribner s Sons, New York, 1985. [18] Takafumi Saito and Tokiichiro 
Takahashi. Comprehensible rendering of 3D shapes. Computer Graphics, 24(4):197 206, August 1990. [19] 
David H. Salesin. Epsilon Geometry: Building Robust Algorithms from Imprecise Computations. PhD thesis, 
Stanford University, March 1991. Available as Stanford Report number STAN-CS-91-1398. [20] Lance Williams. 
Casting curved shadows on curved surfaces. Com­puter Graphics, 12(3):270 274, August 1978. [21] Georges 
Winkenbach. Computer-Generated Pen-and-Ink Illustration. PhD thesis, University of Washington, May 1996. 
[22] Georges Winkenbach and David H. Salesin. Computer-generated pen­and-ink illustration. Proceedings 
of SIGGRAPH 94 (Orlando, Florida, July 24-29, 1994). In Computer Graphics, Annual Conference Series, 
1994. A Deriving the stretching factor To derive the expression for the stretching factor Pi, given in 
equa­tion (1), we .rst note that the linear transformation Jmaps points (u;v)in parameter space to points 
(x;y)in image space by hihi ux J= (3) vy Next, we write the implicit equations for line Aiand its image 
J(Ai), 000 using the implicit-form coef.cients ha;b;cifor Ai,and ha;b;ci for J(Ai): h ihi u00x0 [ab]v 
+c=[ab]y +c=0(4) Combining equations (3) and (4), we readily establish that [a 0b0]=[ab]J.1 c 0 =c (5) 
The distance dbetween two parallel lines with implicit-form coef.­ p cients ha;b;ciand ha;b;c+Eiis d=Ea2 
+b2. The stretching factor Piis given by the inverse ratio of the distance dibetween the lines Aiand 
Ai+1, and the distance d0 ibetween their images J(Ai) and J(Ai+1): p d0 i E(a0)2 +(b0)2 Pi ==p di Ea2 
+b2 r (XuYv-XvYu)2(a2 +b2) = (aYv-bYu)2 +(bXu-aXv)2   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237288</article_id>
		<sort_key>477</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Painterly rendering for animation]]></title>
		<page_from>477</page_from>
		<page_to>484</page_to>
		<doi_number>10.1145/237170.237288</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237288</url>
		<keywords>
			<kw><![CDATA[abstract images]]></kw>
			<kw><![CDATA[non-photorealistic rendering]]></kw>
			<kw><![CDATA[painterly rendering]]></kw>
			<kw><![CDATA[painting]]></kw>
			<kw><![CDATA[particle systems]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P27729</person_id>
				<author_profile_id><![CDATA[81100412576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Meier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hammerhead Productions and Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Deborah F. Berman, Jason T. Bartell, and David H. Salesin. Multiresolution painting and compositing. In Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 85-90. ACM SIGGRAPH, ACM Press, July 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218447</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kurt W. Fleischer, David H. Laidlaw, Bena L. Currin, and Alan H. Barr. Cellular texture generation. In SIGGRAPH 95 Conference Proceedings, Annual Conference Series, pages 239-248. ACM SIGGRAPH, Addison Wesley, August 1995.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fractal Design Corporation. Fractal Design Sketcher. Aptos, California, 1993.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Paul E. Haeberli. Paint by numbers: Abstract image representations. In Computer Graphics (SIGGRAPH'90 Proceedings), volume 24, pages 207-214, August 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Carole Katchen. Creative Painting with Pastel. North Light Books, 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gregg Kreutz. Problem Solving for Oil Painters. Watson- Guptill Publications, 1986.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Martin E. Newell, R. G. Newell, and T. L. Sancha. A solution to the hidden surface problem. In Proc. ACM Nat. Mtg. 1972.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Parallax Software Limited. Matador Paint System. London, 1995.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357320</ref_obj_id>
				<ref_obj_pid>357318</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[W. T. Reeves. Particle systems- a technique for modeling a class of fuzzy objects. ACM Trans. Graphics, 2:91-108, April 1983.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325250</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves and Ricki Blau. Approximate and probabilistic algorithms for shading and rendering structured particle systems. In Computer Graphics (SIGGRAPH '85 Proceedings), volume 19, pages 313-322, July 1985.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Takafumi Saito and Tokiichiro Takahashi. Comprehensible rendering of 3-D shapes. In Computer Graphics (SIGGRAPH '90 Proceedings), volume 24, pages 197-206, August 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Michael E Salisbury, Sean E. Anderson, Ronen Barzel, and David H. Salesin. Interactive pen-and-ink illustration. In Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 101-108. ACM SIGGRAPH, ACM Press, July 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[S. Allyn Schaeffer. The Big Book of Painting Nature in Oil. Watson-Guptill Publications, 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15911</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Steve Strassmann. Hairy brushes. In Computer Graphics (SIGGRAPH '86 Proceedings), volume 20, pages 225-232, August 1986.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134037</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Richard Szeliski and David Tonnesen. Surface modeling with oriented particle systems. In Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 185-194, July 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Frank Thomas and Ollie Johnston. Disney Animation-The Illusion of Life. Abbeville Press, 1981.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Georges Winkenbach and David H. Salesin. Computergenerated pen-and-ink illustration. In Proceedings of SIG- GRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 91- 100. ACM SIGGRAPH, ACM Press, July 1994.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192227</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Andrew E Witkin and Paul S. Heckbert. Using particles to sample and control implicit surfaces. In Proceedings of SIG- GRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 269- 278. ACM SIGGRAPH, ACM Press, July 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Painterly Rendering for Animation Barbara J. Meier Walt Disney Feature Animation Abstract We present 
a technique for rendering animations in a painterly style. The dif.culty in using existing still frame 
methods for animation is getting the paint to stick to surfaces rather than randomly change with each 
frame, while still retaining a hand-crafted look. We extend the still frame method to animation by solving 
two major speci.c problems of previous techniques. First our method elimi­nates the shower door effect 
in which an animation appears as if it were being viewed through textured glass because brush strokes 
stick to the viewplane not to the animating surfaces. Second, our technique provides for frame-to-frame 
coherence in animations so that the resulting frames do not randomly change every frame. To maintain 
coherence, we model surfaces as 3d particle sets which are rendered as 2d paint brush strokes in screen 
space much like an artist lays down brush strokes on a canvas. We use geometric and lighting properties 
ofthesurfacesto controltheappearanceofbrush strokes. This powerful combination of using 3d particles, 
surface lighting information, and rendering 2d brush strokes in screen space gives us the painterly style 
we desire and forces the brush strokes to stick to animating surfaces. By varying lighting and choosing 
brush stroke parameters we can create many varied painterly styles. We illustrate the method with images 
and animated sequences and present speci.c technical and creative suggestions for achieving different 
looks. CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation; I.3.5 
[Computer Graphics]: Three-Dimensional Graphics and Realism Color, Shading, Shad­owing, and Texture. 
Key Words: painterly rendering, non-photorealistic rendering, particle systems, painting, abstract images. 
Author s current af.liation: Hammerhead Productions. email: bjm@gg.caltech.edu or barb@hammerhead.com 
 Permission to make digital or hard copies of part or all of this work or personal or classroom use is 
granted without fee provided that copies are not made or distributed for profit or commercial advantage 
and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, 
to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; 
1996 ACM-0-89791-746-4/96/008...$3.50 1 Introduction A painting reduces a subject to its essence. The 
process of paint­ing is an artist s interpretation of the world, real or imagined, to a two-dimensional 
canvas. By not depicting every detail, the painter allows the viewer to complete the picture, to share 
in the interpre­tative process. Of course the process begins with the painter who, by abstracting a scene, 
can direct the viewer s eye to the area of interest by simplifying unimportant details. A painter can 
exag­gerate the effect of light to create a wide tonal range that creates richness and drama at the center 
of interest. By using the largest brush stroke possible to represent small forms and textures, the painter 
creates a shorthand for conveying details. The character of brush strokes de.ne the character of a surface 
and how light is re.ected from it; surfaces that are well-blended imply smoothness or softness while 
direct, unblended strokes imply stronger lighting or more pronounced surface texture. Painters use varying 
edge de.nition, edges that are distinct in one place and lose themselves in another, to add rhythm to 
a composition. Letting brush strokes cross edge boundaries can also help unify an entire composition. 
By varying brush stroke texture, size, and direction, the artist can not only de.ne forms, but also provide 
rhythm and energy that help direct the viewer s eye. Larger, smoother brush strokes tend to recede in 
depth while small, textured strokes depict foreground detail. A painter can even use brush strokes to 
represent light and atmosphere. Whatever the painting style, a certain amount of ab­straction, or economy 
of description, strengthens the composition and provides focus [5, 6, 13]. Computer rendering provides 
an easy, automated way to render everything in a scene with .ne detail. This creates static images that 
do not invite the viewer into the process. In particular, when creating images for animation, focus and 
simpli.cation are essen­tial to showing action in a clear way since the temporal nature of the image 
gives the viewer much less time to let their eyes wan­der about the scene [16]. Certainly focus and simplicity 
can be achieved with computer rendering tools by carefully controlling lighting and surface attributes 
and unnecessary detail can be ob­scured using hierarchical modeling, but it is still dif.cult to obtain 
the level of abstraction that is evident in a good painting. Even the brush strokes of a painting contribute 
to the abstraction of its subject and add another dimension to which a viewer can respond. One could 
not imagine looking at a Van Gogh painting without ex­periencing the energy of his brush strokes. Hand-drawn 
and hand­painted animations have an energetic quality that is lacking in most computer-rendered animation. 
Often when computer methods try to mimic the wavering quality of hand-drawn animation, too much randomness 
creeps in and makes the animation noisy. A human artist drawing each frame is better able to control 
frame-to-frame coherence, while maintaining a hand-crafted look.  Figure 1: Frames from a painterly 
rendered animation. The painterly renderer is particularly well-suited for abstracting natural textures 
like the cloudy sky, hay, and plowed ground in this example. Note that the haystack texture does not 
exhibit the problems of traditional texture-mapping in which the gift-wrapped texture gets dense near 
silhouette edges. The overlapping brush strokes on the plowed ground imply volume rather than .at, painted 
texture as the view animates, even though the surface is planar. We use the largest brush strokes to 
paint the sky, using brush texture and random hue variation to create clouds that do not exist in the 
color reference picture. The original haystack geometry is simply a cone resting on a cylinder. We represent 
the hay with a brush stroke shorthand that eliminates the need to model and color every piece of hay. 
We want to take advantage of the bene.ts of a painterly look on computer-rendered animating geometry. 
Aesthetically, a still frame should have the characteristics of an oil or pastel painting: details should 
be abstracted by shorthand brush strokes, the roundness of forms should be de.ned by brush stroke directions, 
color should break the boundaries of surfaces to create rhythm in the composi­tion, brush stroke size 
and texture should be varied according to the kind of surface being depicted, and the effects of light 
should be exaggerated to help provide focus, all as if an artist had painted on a physical canvas. Technically, 
the rendered images should main­tain coherence in animated sequences and should not change in a random 
way every frame. Images should not have the gift-wrapped look of painted textures that are mapped onto 
the geometry using traditional methods. Our goal is not to eliminate the need for ob­servational understanding 
and artistic vision, but rather to provide a tool that automates the drawing of brush strokes, but leaves 
the artis­tic decisions about lighting, color, and brush stroke characteristics to the user. The focus 
of most rendering research in the last two decades has been on the creation of photorealistic imagery. 
These methods are quite sophisticated, but tend to create imagery that is mechanical­looking because 
detail is represented very accurately. Recently there has been a movement toward more creative and expressive 
imagery in computer graphics but few techniques that provide ways to achieve different looks, especially 
for animation. Some computer painting tools can mimic successfully the hand-drawn line quality, painterly 
look, and energy of traditional media, but these tools typically work only for still frames. These tools 
and related work are discussed in section 2. Our solution, presented in section 3, is to generate a set 
of particles that describe a surface, depth-sort the particles in camera space, and render them as 2d 
brush strokes in screen space using a painter s algorithm [7]. The look of the 2d brush strokes, including 
color, size, and orientation, is derived from the geometry, surface attributes, and lighting characteristics 
of the surface. These at­tributes are designed by the user and either associated directly with the particles 
or encoded in rendered images of the geometry, called reference pictures. We illustrate our work with 
images and anima­tions that have been successfully rendered to achieve a painterly look using this algorithm 
(Figures 1, 5, and 7), and in section 4 we discuss the images. Finally, in section 5, we present aesthetic 
techniques and technical considerations for creating various image styles.  2 Related Work Our work 
combines core ideas from two areas of previous work: 1) painterly rendering of still images from reference 
pictures and 2) particle rendering. From the .rst research area, our work was most directly inspired 
by [4]. Haeberli described a system for cre­ating painterly images from a collection of brush strokes 
that obtain create particles to represent geometry for each frame of animation create reference pictures 
using geometry, surface attributes, and lighting transform particles based on animation parameters sort 
particles by distance from viewpoint for each particle, starting with furthest from viewpoint transform 
particle to screen space determine brush stroke attributes from reference pictures or particles and randomly 
perturb them based on user-selected parameters composite brush stroke into paint buffer end (for each 
particle) end (for each frame) Figure 2: Painterly rendering algorithm. their attributes, such as position, 
color, size, and orientation, from synthetically rendered or photographic reference pictures. Several 
commercial systems, such as [3] and [8], have incorporated the idea of reference pictures, and Saito 
and Takahashi use a similar concept, the G-buffer, to create simpli.ed illustration-type images [11]. 
Our system also uses reference pictures to obtain brush stroke attributes. In Haeberli s system brush 
stroke positions are randomly dis­tributed, so successive frames of an animation would change ran­domly. 
Alternatively, the positions and sizes of brush strokes could remain constant over the animation, but 
this creates the shower door effect, becausebrushstrokesareeffectively stuckto theview­plane not to the 
animating surfaces. The University of Washington illustration systems [12, 17] provide methods for rendering 
images in a pen-and-ink style, but again, the randomness that is employed to achieve the hand-drawn look 
would cause successive frames to change randomly. We solve the temporal randomness problem by using particle 
rendering methods. If we treat brush strokes as particles that are stuck to surfaces, we eliminate both 
the shower door effect and random temporal noisiness. Reeves .rst presented an algorithm for rendering 
particles without using traditional 3D models to repre­sent them, instead drawing them as circles and 
motion-blurred line segments in screen space [10]. We also render particles in screen space, but use 
2d brush stroke shapes instead of circles and line segments. Rendering 2d shapes in screen space is one 
of the core concepts of our work. Fleischer et al. [2] described a similar method, except they place 
3d geometric elements on surfaces in model space, which are then rendered traditionally as geometric 
textures such as scales, feathers, and thorns. The appearance of their 3d shapes compared to our 2d brush 
strokes is quite different. Finally, Strassmann presented a technique for modeling brush strokes as splines 
for Sumi-E style painting, a Japanese brush-and­ink technique [14]. This system is designed primarily 
for still images, but does provide a simple method for animation. The user speci.es key frames for each 
brush stroke that are interpolated over time. Our approach is different in that we provide a rendering 
technique rather than an interactive system and we are emulating a more impressionistic style of painting 
with short paint dabs rather than long graceful strokes. Painterly Rendering In this section, we describe 
our painterly rendering algorithm as shown in Figure 2. We begin by creating a particle set that represents 
geometry such as a surface. The particles are transformed to screen space and sorted in order of their 
distance from the viewpoint. We use a painter s algorithm to render particles as 2d brush strokes starting 
with the particles furthest from the viewpoint, and continuing until all particles are exhausted. Each 
brush stroke renders one particle. The look of the rendered brush strokes, including color, shape, size, 
texture, and orientation, is speci.ed by a set of reference pictures or by data that is stored with the 
particles. Reference pictures are rendered pictures of the underlying geometry that use lighting and 
surface attributes to achieve different looks. The attributes for a particle are looked up in the reference 
pictures in the same screen space location at which a particle will be rendered .nally. Figure 3 illustrates 
the painterly rendering pipeline. In the following sections, we begin by discussing particle place­ment. 
Next we explain brush stroke attributes, how they are applied, andhowthereferencepicturesthatencodethe 
attributes arecreated. Finally, we present various ways of manipulating the brush stroke attributes to 
produce painterly images. 3.1 Generating Particles There are many methods of populating a surface with 
particles, such as those described in [15] and [18]. We employ a simple method that starts with a parametric 
surface and a desired number of particles. We tessellate the surface into triangles that approximate 
the surface. Then, for each triangle, we compute its surface area and randomly distribute particles within 
it. The number of particles for a triangle is determined by the ratio of its surface area to the surface 
area of the entire surface. The particle placer may store additional information with the particles such 
as color, size, and orientation. After the initial particle placement, these additional attributes or 
the particles positions may be modi.ed by performing various functions on them. Alternatively, the entire 
particle set can be generated from a particle system simulation [9].  3.2 Specifying and Applying Brush 
Attributes In order to render a brush stroke, we need the following attributes: image, color, orientation, 
size, and position. The brush image is a color image with alpha. The image may be solid or it may contain 
texture as shown in Figure 4. A single image may be used as is or it may be used to cut a shape from 
a random position in a sheet of texture, providing each brush stroke with unique texture. Although the 
brush can be a full color image, we typically use monochrome images that are the same in all channels 
so that the brush itself does not impart color, just texture. Orientation, color, and size are either 
stored with the individ­ual particles or obtained from reference pictures. If these attributes are associated 
with the particles, then they are used directly by the renderer; otherwise, the attributes are sampled 
from reference pic­tures which encode information about surface geometry and lighting characteristics 
by screen space location. Reference pictures can be generated in several ways, but typically are rendered 
images of the particle set or surface. After a particle s position is transformed to screen space, we 
use the 2d transformed position to look up color, orientation, and size information in the same 2d location 
in the ap­propriate reference pictures. Example reference pictures for these attributes are shown in 
Figure 3. The reference picture used for color information is typically a smooth-shaded rendered image 
of the surface with appropriate color attributes and lighting. Texture maps are generally not nec­essary 
except to describe broad color changes across the surface. The painterly rendering will provide texture 
and high frequency variations in color.  Particles in World Space Reference Pictures Color   Output 
Image Orientation Size Figure 3: An example of the painterly rendering pipeline. The particle placer 
populates a surface with particles. The surface geometry is rendered using various shaders to create 
brush stroke attribute reference pictures. Note that the arrows in the orientation image are representational 
in this diagram; the orientations are actually encoded in the color channels of the image. The particles, 
which are transformed into screen space, the reference pictures, and the brush image are input to the 
painterly renderer. The renderer looks up brush stroke attributes in the reference pictures at the screen 
space location given by each particle s position and renders brush strokes that are composited into the 
.nal rendered image. The reference picture that encodes orientation information is an image made with 
a specialized shader that encodes surface nor­mals in the resulting image. This surface normal shader 
projects the 3d surface normals into two dimensions along the view vector or another speci.ed vector. 
Alternatively, we may constrain orien­tations to line up with the direction of a surface parameter or 
texture coordinate. Finally, the brush size reference picture is a scalar image that encodes x and y 
scaling information. We linearly map the range of values in the image to the range of user-speci.ed sizes 
so that the areas with small values are painted with the smallest brushes and the areas with high values 
are painted with the largest brushes. Again, we can use lighting, texture maps, or specialized shaders 
to achieve the desired look. Brush stroke position comes from the particle s position in screen space. 
Position may be modi.ed by a function such as moving it in the direction of a velocity vector or adding 
noise. To apply the attributes, the brush image is either used directly or cut from a sheet of texture, 
multiplied by the color and alpha, scaled by the size, and rotated to the orientation, each as speci.ed 
in the corresponding reference picture or by data stored with the particle. Figure 4: Some brush images 
used to create the paintings in this paper. Once attributes are applied, brush strokes are composited 
into the .nal rendered image at the position speci.ed by the particle.  3.3 Animating Parameters and 
Randomness It is possible to animate brush stroke attributes by animating charac­teristics ofthereferencepictures,butit 
isnecessaryforthereference Figure 5: Four styles of painterly rendered fruit. By choosing different brush 
images and painting parameters, we have created four different looks from the same set of reference pictures. 
The upper left image has the soft, blended quality of a pastel painting. The pointillistic version, in 
the upper right, remaps the original saturations and values from the color reference picture to a new 
range. A squiggle brush image and increased hue variation were used to create marker-style strokes in 
the lower left image. The brush used to create the lower right contained some opaque black that helps 
to create a woodcut print style.  pictures to change smoothly over time so that the .nal rendered im­ages 
are not temporally noisy. Using randomness is important in achieving a hand-crafted look; therefore, 
we can randomly perturb the brush stroke attributes based on user-selected parameters. Figure 6 illustrates 
the lack of richness and texture that results when randomness is not used. To maintain coherence, a seed 
is stored with each particle so that the same random perturbations will be used for a particular particle 
throughout an animation. The user speci.es the amount of randomness by choosing a range about the given 
attribute. For example, we may specify that brush rotations be determined by an orientation reference 
picture, but to eliminate the mechanical look of the brushes lining up perfectly, we specify that we 
are willing to have brush orientations fall within the range of -10 to +20 degrees from the orientation 
given in the reference picture. The resulting slightly random orientations give the strokes a more hand-crafted 
look. 4Results Figures 1, 5, and 7 are images rendered using our algorithm that show a variety of different 
painterly looks. In Figure 1, we show frames from a Monet-style haystack animation. The still frames 
look like oil paintings and the brush strokes animate smoothly throughout the animation. The painterly 
renderer is particularly well-suited to the impressionist style because it composes a paint­ing with 
many small brush strokes. In this example, we are not Figure 6: Applying randomnessto brush stroke attributes. 
This image was rendered without color, orientation, or scale variation. Compare it to the images in Figure 
1 which were painted with all of those attributes jittered. Note how the painterly texture of the sky 
and mountains is dependent on random color variations. In the haystacks, orientation and scale changes 
make them look less mechanical in the jittered version. concerned with de.ning exact boundaries and 
instead let the over­lapping brush strokes create a rhythm that uni.es the composition. Large brush strokes 
tend to extend beyond the silhouette edge, cre­Figure 7: Beach ball animation frames. In this example 
the beach ball is bouncing, squashing, and stretching from frame to frame. Our technique works as well 
for animating objects as for the haystack example where only the camera position is animating. ating 
a semi-transparent look that is most apparent when surfaces are animated. We believe this adds to the 
painterly look. Using smaller, denser, or more opaque strokes near the edges would create a more opaque, 
solid look. We have used the painterly technique of abstraction to depict many of the surfaces in this 
animation. For example, in the sky we used the brush texture and color variation to abstractly depict 
sweeping clouds. The hay is captured with a brush stroke texture that shows an appropriate amount of 
detail. Finally because our technique uses overlapping 2d brush strokes, we have avoided the gift-wrapped 
look of a smooth-edged, texture-mapped surface. In Figure 5, we show a plate of fruit rendered in four 
styles. The reference pictures used to create the images were the same for all four, with the exception 
of the orientation image for the lower right image. The different looks were achieved by varying the 
brush image, the amount of jittering, and the brush size. Of course even more looks could be created 
by changing the reference pictures, but one of the strengths of the painterly renderer is the richness 
of the user-selectable parameter set. For example the upper right image was brightened and desaturated 
by mapping colors in the color reference picture to new saturation and value ranges. Conversely, the 
colors in the lower right image are richer because the brush image contained some opaque black. In this 
painting, the brush strokes become the dominant subject of the painting. Finally, in Figure 7, we show 
three frames from an animated bouncing ball sequence. Our technique works equally well for an animating, 
deforming object like the squashing and stretching ball in this example, as for an animated camera as 
shown in the haystacks example. Large brush strokes give the ball an imprecise boundary which gives the 
ball animation a hand-drawn look quite different from the mechanical look that a traditionally-rendered 
version would have. 5 Discussion and Techniques As with any image creation process, it takes some experimenta­tion 
to get the desired image. In this section, we describe some of the techniques that we ve discovered. 
We begin with our strate­gies for achieving creative images and then present some technical discussion 
on how we achieve them. 5.1 Creative Techniques We have discovered many techniques for rendering aesthetically 
pleasing images. Chief among these is separately rendering subsets of the particle set and compositing 
these layers into a .nished image. We .nd that our most successfulimages are created using traditional 
painting methods such as creating a rough value underpainting with large brush strokes, adding layers 
of color to de.ne the form, and then adding small brush strokes where we want more detail. Our implementation 
provides a skip operation that allows us to render every nth particle. We typically render the surface 
in two or three layers using image processing techniques to shrink the silhouette edge toward the center 
of the object. The outside layers are painted sparsely, while the inside layers are painted thickly. 
We also use image processing techniques to isolate highlight and shadow areas to be rendered separately. 
Building up layers of semi-transparent textured brush strokes, perhaps even rendering the same particle 
multiple times with different brush stroke characteristics, is impor­tant in achieving the painterly 
look. In the haystacks example, the haystacks consist of four layers: a rough dark blue underpainting, 
an overall orange layer, a yellow detail layer, and a sparse white highlight layer. These layers and 
how they contribute to the .nal imageare shown inFigure8. We also usually render the objects in a scene 
as separate layers. In the haystack example, we painted the sky, mountains, .eld, each haystack, and 
each haystack shadow as a separate layer. This allowed us to use very large brush strokes on the sky 
and not worry about them creeping too far into the mountains. By rendering these layers separately, we 
were better able to use the painting parameters most appropriate for each layer. The fruit images in 
Figure 5 were rendered in three layers: the wall, the table, and the plate of fruit. In this case, because 
the brush stroke characteristics of each fruit were similar, we wanted the brushes strokes to interact 
as much as possible to enhance the painterly look. We typically use only one light source to maintain 
focus in the composition. We use exaggerated hue as well as value variations to distinguish light and 
shadow areas. For example, the sunset light on the haystacks is emphasized through exaggerated use of 
orange and blue. Shadows may be rendered by compositing a shadow element onto the color reference picture 
and rendering the surface and the shadow at the same time, or shadows may be painted as a separate layer 
and composited, giving the user more creative control. We use many traditional painting techniques such 
as using back­ground color in shadow areas to help them recede and juxtaposing complementary colors, 
such as the orange and blue of the haystacks, to create a shimmering light effect. We repeat brush stroke 
color, size, and texture in different areas of the scene, as shown in the fruit example, to marry the 
various elements into a uni.ed composition. Users of the painterly renderer are encouraged to examine 
the nu­merous existing texts on traditional painting techniques for more possibilities. 5.2 Technical 
Considerations If reference pictures are used, it is often helpful to grow the reference image outward 
using image processing techniques, so that when we look up particular screen locations we don t fall 
off the edge of the surface onto anti-aliased or unrendered parts of the image. This is applicable only 
if we are rendering layers separately and then compositing them afterwards. To ensure that individual 
Figure 8: Compositing a haystack from several layers. Each layer of the haystack is shown by itself on 
the left while its con­tribution to the composited image is shown on the right. We used image processingtechniques 
on the color reference picture to isolate the shadow and highlight areas to be painted separately. Following 
traditional painting techniques, we created a dark blue underpaint­ing of the shadow areas as shown in 
the top row. The next layer provides most of the color and texture of the haystack, but allows some of 
the blue underpainting to show through. The bottom two rows show two separate detailed highlight layers 
and a .nal shadow layer that helps integrate the haystack with the .eld. For each layer, we changed the 
brush size and the amount of color variation. brush strokes do not jitter in size and orientation slightly 
with every frame, it is also useful to blur the orientation and size images slightly. Perfect particle 
placement and sub-pixel sampling would eliminate the need for these steps, but we have found that these 
techniques work well in practice. The simple surface normal shader that we described previously provides 
surface normal information based on a particular orienta­tion of the surface after it has been through 
a camera transformation. But as a surface animates, so does its orientation with respect to the camera. 
This gives a particular look, but we prefer to have brush strokes oriented with respect to the surface 
and not change as the surface animates. To achieve this, we have speci.ed our desired orientations with 
respect to the (u, v) surface parameters in texture maps. A special shader looks up values in the maps 
and then ap­plies the camera transformation to them to obtain the screen space orientation that is output 
to the reference picture. Brush stroke attributes may be stored with the particles or en­coded in reference 
pictures. An advantage to storing attributes with the particles is that we avoid aliasing errors looking 
up values in reference pictures. An advantage to using reference pictures is that they are usually quickly 
rendered and thus easily changed and can encode more complex lighting information. Storing attributes 
with particles is better for those that are unlikely to change because rerunning the particle placement 
or simulation may be costly. In practice, a mixture of the two methods works well. At .rst glance, one 
might suggest we not render back-facing particles, but this is very important in animation since particles 
will pop on and off as they become visible and invisible if we cull the back-facing ones. If we always 
render them, however, they will be revealed gradually as they become visible. Front-Facing brush strokes 
must be dense enough to obscure back-facing particles, unless a translucent effect is desired. In practice, 
we .nd that we do not always want to render a completely opaque object if we are building up textured 
layers of paint, but we also do not want to see through to the back-facing brush strokes if they are 
animating. In this case, we do cull back-facing particles, letting them fade in as they get close to 
front-facing to eliminate the popping effect as particles come into view. This was necessary in the haystack 
example so that as the view animates, we do not see the back side of each sparse layer through the front. 
One should also note that because particles are sorted by distance from the viewpoint at each frame, 
there will be some popping of brush strokes in front or behind one another as particles animate, but 
with some attention to brush stroke size and translucency, this effect is not visually problematic and 
can add to the painterly effect.  6 Future Directions Although our use of the renderer thus far has 
been to create im­ages that are entirely painted, we can imagine incorporating this look with traditional 
rendering methods. For example, when artists depict foliage, they don t paint every leaf. Instead, they 
use brush strokes to abstractly represent the leaves. Certainly particle render­ing methods have been 
used for this purpose before [10], but we believe our technique can eliminate complex modeling issues 
such as generating realistic tree models made of particles, and that the level of control we provide 
for achieving different looks will prove to be a more powerful but easier-to-use tool. We foresee using 
this method to render surfaces that are dif.cult to model and render using traditional geometry and texture 
maps. This class of objects, which includes many of those found in nature, must be abstracted when rendered 
because of their high complexity. Our renderer does not handle changing object sizes in an auto­mated 
way. We can address this issue with staging or by animating the brush size reference picture; however, 
it would be helpful if the renderer could automate brush stroke size based on the screen surface covered, 
and then change the size smoothly as the object changes size using multi-resolution techniques such as 
those used by [1]. We would like to use a better particle placement method that covers both the geometric 
surface and screen space more evenly. While we can address this situation with the layer rendering tech­nique 
described above, this is not always satisfactory and also re­quires active intervention by the user. 
Metric tensor techniques [18] could be used to specify particle density for surfaces that do not rad­ically 
change their orientation with respect to the viewpoint within an animation, but other multi-resolution 
methods might be required for those surfaces that do change orientation. Finally, although we are unlimited 
in brush stroke shape, we .nd a rectangular or oval shape works best to show changes in orientation, 
but these shapes stick out along the edges of curved surfaces. We would like to implement longer, deformable 
brushes than can follow curves on a surface. 7 Conclusions We have presented a new technique for rendering 
animations in a painterly style. Our work has brought together two previous ren­dering methods: using 
reference pictures to de.ne 2d brush stroke attributes and using particles to de.ne the locations where 
brush strokes will be rendered. Our algorithm solves the two major prob­lems of rendering animations 
with previous painterly techniques. First, images created by our renderer are coherent over time and 
do not exhibit random frame-by-frame changes. Second, brush strokes stick to animating surfaces, not 
to the viewplane, thus elim­inating the shower door effect. We haveillustrated our algorithm with images 
that have painterly qualities such as exaggerated use of light, broken silhouette edges that create rhythm, 
brush stroke textures and sizes that describe surface qualities, and abstracting the subject to strengthen 
and unify the composition. 8 Acknowledgments Many thanks to Ken Hahn, Scott Johnston, Jason Herschaft, 
and Craig Thayer for turning the painterly renderer prototype into a pro­duction program, contributing 
many new ideas and features along the way. Ken Hahn also wrote the particle placer and went beyond the 
call of duty to make many last minute bug .xes. Thanks to Dave Mullins and Andrea Losch for modeling 
and rendering support, Craig Thayer and Scott Johnston for valuable comments on early drafts of the paper, 
and Nancy Smith for video production support. We are grateful to Al Barr and Scott Fraser of Caltech 
and to Ham­merhead Productions for providing production facilities. Finally, many thanks to David Laidlaw 
for technical discussions about the painterly renderer, extensive paper reviews, diagrams, many hours 
of paper production support, and help coping with my pregnancy madness.   References [1] Deborah F. 
Berman, Jason T. Bartell, and David H. Salesin. Multiresolution painting and compositing. In Proceedings 
of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994),Com­puter Graphics Proceedings, Annual Conference 
Series, pages 85 90. ACM SIGGRAPH, ACM Press, July 1994. [2] Kurt W. Fleischer, David H. Laidlaw, Bena 
L. Currin, and Alan H. Barr. Cellular texture generation. In SIGGRAPH 95 Conference Proceedings, Annual 
Conference Series, pages 239 248. ACM SIGGRAPH, Addison Wesley, August 1995. [3] Fractal Design Corporation. 
Fractal Design Sketcher. Aptos, California, 1993. [4] Paul E. Haeberli. Paint by numbers: Abstract image 
represen­tations. In Computer Graphics (SIGGRAPH 90 Proceedings), volume 24, pages 207 214, August 1990. 
[5] Carole Katchen. Creative Painting with Pastel. North Light Books, 1990. [6] Gregg Kreutz. Problem 
Solving for Oil Painters. Watson-Guptill Publications, 1986. [7] Martin E. Newell, R. G. Newell, and 
T. L. Sancha. A solution to the hidden surface problem. In Proc. ACM Nat. Mtg. 1972. [8] Parallax Software 
Limited. Matador Paint System. London, 1995. [9] W. T. Reeves. Particle systems a technique for modeling 
a class of fuzzy objects. ACM Trans. Graphics, 2:91 108, April 1983. [10] William T. Reeves and Ricki 
Blau. Approximate and proba­bilistic algorithms for shading and rendering structured par­ticle systems. 
In Computer Graphics (SIGGRAPH 85 Pro­ceedings), volume 19, pages 313 322, July 1985. [11] Takafumi Saito 
and Tokiichiro Takahashi. Comprehensible rendering of 3-D shapes. In Computer Graphics (SIGGRAPH 90 Proceedings), 
volume 24, pages 197 206, August 1990. [12] Michael P. Salisbury, Sean E. Anderson, Ronen Barzel, and 
DavidH.Salesin.Interactivepen and inkillustration. In Pro­ceedings of SIGGRAPH 94 (Orlando, Florida, 
July 24 29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 101 108. ACM SIGGRAPH, 
ACM Press, July 1994. [13] S. Allyn Schaeffer. The Big Book of Painting Nature in Oil. Watson-Guptill 
Publications, 1991. [14] Steve Strassmann. Hairy brushes. In Computer Graphics (SIGGRAPH 86 Proceedings), 
volume 20, pages 225 232, August 1986. [15] Richard Szeliski and David Tonnesen. Surface modeling with 
oriented particle systems. In Computer Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 185 194, 
July 1992. [16] Frank Thomas and Ollie Johnston. Disney Animation The Illusion of Life. Abbeville Press, 
1981. [17] Georges Winkenbach and David H. Salesin. Computer generated pen and ink illustration. In Proceedings 
of SIG-GRAPH 94 (Orlando, Florida, July 24 29, 1994), Computer Graphics Proceedings, Annual Conference 
Series, pages 91 100. ACM SIGGRAPH, ACM Press, July 1994. [18] Andrew P. Witkin and Paul S. Heckbert. 
Using particles to sample and control implicit surfaces. In Proceedings of SIG-GRAPH 94 (Orlando, Florida, 
July 24 29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 269 278. ACM SIGGRAPH, 
ACM Press, July 1994.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237289</article_id>
		<sort_key>485</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[The future of virtual reality]]></title>
		<subtitle><![CDATA[head mounted displays versus spatially immersive displays (panel)]]></subtitle>
		<page_from>485</page_from>
		<page_to>486</page_to>
		<doi_number>10.1145/237170.237289</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237289</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP35045209</person_id>
				<author_profile_id><![CDATA[81100425708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lantz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spitz, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[David Bennett, "Providing Solutions Using Virtual Reality," Press Release, Alternate Realities Corp., Research Triangle Park, NC, e-mail davidb@arc.tda.com, http://www.arc.com/ARC.html]]></ref_text>
				<ref_id>Bennett 95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166134</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Carolina Cruz-Neira, Daniel J. Sandin, and Thomas A. DeFanti, "Surround-Screen Projection-Based Virtual Reality: The Design and Implementation of the CAVE," Computer Graphics, Annual Conference Proceedings Series, 1993.]]></ref_text>
				<ref_id>Cruz-Neira 93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jasek, M., N. Pioch and D. Zeltzer, "Effects of Enhanced Visual Displays on Collision Prediction for Air Traffic Control," Proc. 6th IFAC/IFIP/IFORS/IEA Symposium on Analysis, Design and Evaluation of Man-Machine Systems, Cambridge MA. (1995).]]></ref_text>
				<ref_id>Jasek 95</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[David McCutchen, Method and Apparatus for Dodecahedral Imaging System, U.S. Patent #5,023,725, June 11, 1991.]]></ref_text>
				<ref_id>McCutchen 91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Capt. Brian A. Reno, "Full Field of View Dome Display System," Proceedings of AIAA/FSTC, pp. 390-394, 1989.]]></ref_text>
				<ref_id>Reno 89</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Future of Virtual Reality: Head Mounted Displays Versus Spatially Immersive Displays Organizer: 
Ed Lantz, Spitz, Inc. Panelists: Steve Bryson, MRJ/NASA Ames Research Center David Zeltzer, Sensory Communication 
Group, MIT Research Lab of Electronics Mark T. Bolas, Fakespace Bertrand de La Chapelle, VIRTOOLS David 
Bennett, Alternate Realities Corporation Introduction Since its inception, the field of virtual reality 
(VR) has revolved around the head-mounted display (HMD) as the essential visual display device. VR futurists 
depicted VR as a personal experience wherein the user interacts with a virtual environment (VE) in a 
manner synonymous with reality: looking, pointing, walking, physics, etc. Early on, some even declared 
the classic HMD with data glove to be the only true VR. However, our concept of VR continues to broaden. 
An emerging alternative to the HMD is the (walk-in) spatially immersive display (SID). These displays 
physically surround the viewer with a panorama of imagery, typically produced by video projection. The 
first application of a SID to VR systems is the CAVE, developed at the University of Illinois at Chicago 
[Cruz-Neira 93]. More advanced SIDs are now in development utilizing domed video projection technology, 
which could eventually replace the rectilinear CAVE configuration [Bennett 95, McCutchen 91]. Domed SIDs 
have been used for many years in military flight simulators [Reno 89]. SIDs offer advantages over HMDs, 
including group viewing and interaction, wide field of view, high resolution, no cumbersome headgear, 
and low user fatigue. Also, angular viewing is accomplished without head rotation tracking and its associated 
response time requirements. Stereoscopic displays are also possible using eye-sequential glasses. A number 
of technical challenges remain in the development of both HMDs and SIDs. Currently, VR researchers are 
consumed with refining the HMD. Advances are being made in wide field-of-view, high resolution HMD technology. 
Very little research is currently underway on SID implementations. This panel compares the ultimate utility 
of HMDs versus SIDs in emerging VR applications, such as entertainment, education, computer-aided design, 
simulators, scientific visualization, 3D animation production, biomedicine, and other potential markets. 
Important issues include cost, size, user mobility, single and multi-user interactivity, stereoscopic 
viewing, applicability to augmented reality, special hardware/software requirements, physiological concerns, 
visual quality, and sense of presence. Steve Bryson Let the Task Determine the Display As the field 
of VR matures, there has appeared an almost bewildering variety of display technologies that support 
the VR effect. While there are common measures of display quality, such as resolution, field of view, 
pixel spacing and so forth, there are other considerations that are very difficult to compare from display 
to display. These other considerations include comfort, mobility, privacy, opacity, and immersiveness. 
Rather than try to find the unique best display in this very high-dimensional space, I feel that one 
should analyze the task for which the display is being used. Some tasks, such as an architectural walkthrough, 
require medium resolution wide field immersive displays with a high degree of mobility. Other tasks, 
such as information or CAD visualization, may require a collaborative, less immersive high­resolution 
display that several people can see at once. I will propose a task analysis framework which aids in the 
selection of a display for a particular task. David Zeltzer Specifying a Visual Display System The visual 
display system is one element of the human/machine interface to any computer system, and the requirements 
of the display subsystems visual, auditory and haptic are strongly dependent on the application for 
which they are intended. So-called immersive displays are not always the best strategy. For example, 
we have shown in our lab that well-designed 2D presentations consistently lead to better performance 
than stereoscopic displays of 3D scenes for certain air traffic control tasks [Jasek 95]. But if immersion 
is important for a particular application, careful task analysis and requirements engineering can help 
system designers to formulate specifications for the display systems, as well as to understand the engineering 
tradeoffs and human factors issues involved. At MIT we have developed a number of VE systems and applications 
since the late 1980s. Each of these systems has had differing display requirements. Multimodal displays 
and immersive presentations were often called for; but for some VE applications a workstation CRT was 
sufficient. We have implemented immersive presentations using different techniques, including a head-coupled, 
stereoscopic HDTV system;  various HMDs; as well as the walk-in  CAVE system.  In this presentation, 
I will briefly describe several of these applica­tions and the display systems that were used, and I 
will discuss the methodology we employ for specifying a display for a given applica­tion. Mark T. Bolas 
Alternative Displays While the head-mounted display serves as a visual icon of VR, alternative immersive 
technologies have taken root and grown in industries utilizing 3D computer graphics. Having spearheaded 
alternative immersive peripherals for more than seven years, I will focus my presentation on the lessons 
learned and the viewpoints formed by working with hundreds of different users and applications. The presentation 
will concentrate on the following three areas. The first is to question what constitutes an immersive 
display to begin with. Is it a strong feeling of immersion? Is it a First Person Point of View? What 
sparked the original interest in HMDs, and why are we so eager to abandon it? Is this panel simply full 
of lazy panelists who are shying away from the hard problems of cutting edge immersive displays to move 
toward the relative utility and security found in projection based and other alternative displays? The 
second area concerns mature media and technologies to help make predictions for the media and technologies 
we are discussing here. Will HMDs follow the same price drop over time as flat panel displays? If the 
HMD is similar to a pair of audio headphones, are projection systems analogous to audio speakers? How 
does content development compare? Finally, a look toward applications and examples is in order. Starting 
from a clean slate and armed with all the technology SIGGRAPH has to offer, what is the best solution 
for a small set of example applications? Is there a best flavor of VR? Is there a best flavor of Ice 
Cream? What does it take to make both the display technology and the content work together to form a 
seamless immersive experience? Bertrand de La Chapelle Considering the Manufacturing and Engineering 
End-User VIRTOOLS is pioneering the implementation of Shared Virtual Workspaces for concurrent engineering, 
collaborative design, and scene layout for the aerospace, automotive, and nuclear power industries. Based 
on the experience gathered at VIRTOOLS with manufacturing and engineering clients and on extensive contacts 
with other potential end-users, we strongly believe in the potential of SIDs for such professional applications. 
Two aspects will be put forward. 1) HMDs are globally ill-adapted for day-to-day professional applications. 
A key founding component of the VR concept, HMDs have surely become less cumbersome, less expensive and 
have increased performance. But: even if high resolutions and wide field of view (required for professional 
applications) ultimately appear, price/performance ratio is mostly driven by games; therefore, price 
will decrease faster than performance improves.  weight, eye, and neck fatigue prevent use over several 
hours; this is not likely to change even with greatly improved performances.  psychological factors 
are an important limitation: engineers and decision-makers are very reluctant to use such apparatus, 
considered game gear.  HMDs isolate users from one another; collaborative work in the same room requires 
the creation of sophisticated clones, and people can bump into one another.  The main obstacle for HMDs 
will not be performance, but seclusion. Therefore, they will prosper in applications where people work 
in isolation for short periods of time and really need to look around them as if they were in a static 
real environment. Apart from games, the best applications include training and some maintenance assessment. 
In most other individual uses, devices like the BOOM or the Push from Fakespace seem more appropriate, 
offering high resolution, wide field of view, and less fatigue. 2) SIDs offer the best potential for 
collaborative applications. They still suffer major drawbacks: underdeveloped, very expensive, requiring 
much more space and hardware (three channels for the CAVE), they are not yet fully industrialized or 
standardized. But: they provide a better sense of presence through a very large field of view (up to 
180x for the ARC Dome) and a high resolution (2000 x 2000 and up).  they allow prolonged work through 
reduced fatigue, including in stereoscopy.  they allow the presence of multiple users in the same environ­ment, 
who can communicate naturally together.  large models can be displayed (cars, plane segments, plant 
sections) at once, whereas you need to turn your head around with an HMD.  they are very well adapted 
for applications in which the user interacts strongly with the environment through Virtual Tools (3D 
widgets) and a 3D interface.  As VR applications evolve from simple walkthroughs towards virtual working 
environments, SIDs might become the new paradigm for professional use. Key developments in graphics hardware 
(new generation SGIs) and projection devices (mono-lens high power light valve or future micromirrors) 
will create a range of standard systems, from individual large screen displays to full-fledged multi-participant 
domed environments. Present prices will go down, thanks to entertain­ment applications (including immersive 
prerecorded rides), and such environments are the key to implementation of full concurrent engineering 
in manufacturing. David Bennett Dome and Shared Spaces We are on the threshold of new methodologies 
in visualizing and interacting with information. The emphasis here is on why spatial immersion and, in 
particular, dome projection provides a better solution than other alternatives for groups of people. 
The primary focus is on teams of people experiencing 3D information space, and experiencing both immersion 
and interaction as groups rather than as individuals. The focus is also changing from what is acceptable 
to a techie to what is required by mainstream users. Business people, scientists, moms and dads all 
need to feel that VE s are not complex and unusable, but rather as simple to interact with as television. 
There is a unique characteristic of domes that makes it ideal for groups, particularly for training and 
education. Within reasonable constraints, the viewpoint or perspective is the same for everyone. This 
means that an instructor can be assured that the student is seeing exactly what the instructor sees. 
Equally important is the sense of presence created with a 180-degree field of view onto a hemisphere, 
and in a way that is both comfortable and consistent with a real-world experience. The downside of domes 
is the computational expense of doing distortion correction, limited individual tracking ability, and 
the space requirements for setting up a dome at your facility. There is as yet no perceived cybersickness; 
however, motion sickness is still present, just as in any other environment, including the real world. 
As we move these technologies forward into everyday life, it is important to focus on simplicity, affordability, 
portability, and comfort. We have found that acceptance of domes for VR is greater than for HMD s, since 
it does not require wearing any restrictive devices and feels more like what is expected of VR. References 
[Bennett 95] David Bennett, Providing Solutions Using Virtual Reality, Press Release, Alternate Realities 
Corp., Research Triangle Park, NC, e-mail davidb@arc.tda.com, http://www.arc.com/ARC.html [Cruz-Neira 
93] Carolina Cruz-Neira, Daniel J. Sandin, and Thomas A. DeFanti, Surround-Screen Projection-Based Virtual 
Reality: The Design and Implementation of the CAVE, Computer Graphics, Annual Conference Proceedings 
Series, 1993. [Jasek 95] Jasek, M., N. Pioch and D. Zeltzer, Effects of Enhanced Visual Displays on Collision 
Prediction for Air Traffic Control, Proc. 6th IFAC/IFIP/IFORS/IEA Symposium on Analysis, Design and Evaluation 
of Man-Machine Systems, Cambridge MA. (1995). [McCutchen 91] David McCutchen, Method and Apparatus for 
Dodecahedral Imaging System, U.S. Patent #5,023,725, June 11, 1991. [Reno 89] Capt. Brian A. Reno, Full 
Field of View Dome Display System, Proceedings of AIAA/FSTC, pp. 390-394, 1989.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237290</article_id>
		<sort_key>487</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Art on the Web, the Web as art (panel)]]></title>
		<page_from>487</page_from>
		<page_to>488</page_to>
		<doi_number>10.1145/237170.237290</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237290</url>
		<categories>
			<primary_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>World Wide Web (WWW)</descriptor>
				<type>P</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Arts, fine and performing**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31099227</person_id>
				<author_profile_id><![CDATA[81100372570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Annette]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weintraub]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The City College of New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://gertrude.art.uiuc.edu/@art/gallery.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://gertrude.art.uiuc.edu/ad319/paperl.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[http://adaweb.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[http://artnetweb, corn]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[http://moobird.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[http://artnetweb'c~m/pr~jects/blast/h~me'html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[http://artnetweb.com/projects/ahneed/first.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[http://artnetweb.com/gh]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[http://artnetweb.com/iola/home.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[http://artnetweb.com/projects/realms/notes.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[http://www jodi.org/betalab]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[http://www ahip.getty.edu/ahip/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[http://www gold.net/ellipsis/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[http://www irational.org/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[http://pon pseudo.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[http://www cicv.fr]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[http://www scifi.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Art on the Web, the Web as Art Organizer: Annette Weintraub, The City College of New York Panelists: 
Remo Campopiano, ArtNetWeb Nan Goggin, University of Illinois at Urbana-Champaign John F. Simon, Jr., 
Independent artist Sharleen Smith, USA Network Web-specific art work and art sites on the Web are fostering 
a seamless environment in which the boundary between the artwork itself and the space it inhabits is 
vanishing. The Web offers the promise of a direct and intimate connection between artist and audience, 
and requires a shift in aesthetics and approaches to storytelling, narrative and interaction. This panel 
explores this interface of art and art space and the dynamic qualities of art created for the Web. Roundtable 
Overview For many artists, the World Wide Web is no longer merely a means for viewing or distributing 
art, but has become an art medium in itself with its own distinct characteristics. The shift from making 
art work available on the Web to utilizing the Web as a dynamic artform has been dramatic, as the gallery 
metaphor is replaced by a variety of approaches that exploit the inherent characteristics of Web-based 
communication. The Web has become one of the fastest-growing venues for new art, and an active laboratory 
for the developmental of graphics and media integration. The evolving environment of the Web provides 
a unique challenge to artists to explore conjunctions of text and image, narrative and storytelling, 
and strategies for participation and interaction in real­time all with direct audience input. The speed 
of development, the constant changes in the development environment, and the interaction among fine art 
and commercial site developers and their audience are also important factors in this synergy. This roundtable 
will examine some of the kinds of art being developed for the Web. The speakers will offer their perspectives 
on the components of successful Web-based artwork. Each will present the work of one or more artists 
making Web-based work, and will focus on sites that integrate Web art works into an aggregate art space. 
The roundtable will also address the impact of the growing integration of moving images, virtual reality, 
3D modeling, interactivity and sound in real time on the Web. Panelists will explore aesthetics, content, 
interface, and interaction in current work and will examine their implications for future development 
of art on the Web. This roundtable includes artists doing Web-based art, developers of art Web sites, 
and the designer of a commercial Web site with a strong art component. A particular focus of the roundtable 
is to explore the exchange of ideas and energy between commercial and fine arts sites, and the integration 
of works of art created for the Web within a particular Web site. The panel of will address these core 
issues from a variety of distinct perspectives, and will provide an overview of some of the most interesting 
art work and art sites on the Web. The roundtable will address the question of what differentiates Web-based 
artwork from other interactive art, consider the interaction of aesthetics and technology, and describe 
the potential evolution of this form. Remo Campopiano will present his ideas about the Web as live organism 
and speak about the development of ArtNetWeb. Before I found myself tumbling head over heels down the 
rabbit hole of cyberspace, I was creating installation art. More specifically, I created environments 
for live animals to act out metaphor for human phenomena. ArtNetWeb grew out of an art project called 
Virtual Real Estate. Simply put, the idea was to create a digital environment for artists to explore 
this new medium. It quickly became evident that we were part of a much bigger phenomenon. Artnets were 
popping up all over the world. When you look at this phenomenon from a global/naturalist perspective, 
it felt like the living earth was developing a nervous system through telecommuni­cations. Add to this 
revolutionary breakthroughs in Complexity Theory (Chaos), and you have a unique and provocative view 
of cyberspace. From this perspective, the telecommunications revolution is not just a new tool, but a 
living force that seeks to maximize its potential by existing on the edge of chaos...the place of pure 
creativity. ArtNetWeb is made up of a group of artists, all exploring the same issues, who are trying 
to determine what constitutes a Web-based artform, and what the role of the artist is in the telecommunications 
revolution. As a member of this panel, I will introduce my own ideas on the Web as a dynamic art environment, 
as well as present work by the group of artists that make up ArtNetWeb. Nan Goggin will represent the 
ad319 site, a collaborative art research group based at the University of Illinois. ad319 s projects 
include a spectrum of activity related to digital tools and their impact on making, viewing, and distributing 
art. These concerns led to the development of one of the first curated WorldWideWeb electronic art galleries, 
the @art gallery . In creating the @art gallery we had several goals: 1) creating a moderated virtual 
gallery space on the Internet; 2) focusing on work created specifically for the medium; and 3) as an 
experiment to evaluate whether experiencing art on the Internet is emotionally and intellectu­ally fulfilling. 
In September 1994, when the @art gallery went on-line, web-based art work was very early in gestation. 
Today, it is just learning to crawl. Partially, we are simply limited by bandwidth; video, sound, and 
even picture files can still be unwieldy. But more impor­tantly, artists are still becoming acquainted 
with the medium. What are the inherent and most conceptually profound characteristics of this medium? 
Is it appropriate to look at models from print media for form, such as book or magazine; or does this 
medium require completely new forms? One thing we have learned is that there are no absolutes. While 
there are forms yet to be created as the tools continue to evolve at an incredible pace, there is also 
value on emphasizing distribution as a key characteristic of the medium. Several of our exhibitions have 
acted as virtual exhibition catalogs for their physical counterparts. To address the unanticipated dilemma 
of so few artists equipped with the html programming skills, an extended goal is to develop a residency 
program for artists. The University of Illinois at Urbana-Champaign is a fertile environment for web-based 
development, and there is frequent interdisciplinary activity between the arts and sciences. If we begin 
to think of art on the net as an experience and not an object, questions about copyright and ownership 
can parallel other media. Just as we pay for specific cable channels on TV or subscribe to magazines, 
there will be similar mechanisms in place on the net in the not too distant future. (See the January 
1996 issue of Popular Mechan­ics magazine for a review of the state of digital dollars ). For me personally, 
the jury is still out on the experience of art on the net. I have seen very little that I find emotionally 
and intellectually compelling, but I look to the future with great anticipation. John F. Simon Jr. will 
present the work of artists using the Web as a mechanism for real-time participation and speak about 
the design of adaweb. Mechanisms for creating art online are continually invented, exploited, and then 
quickly reinvented. How can Web-based creative projects be understood in the context of art? For example, 
what distinguishes a Web-based gallery from a gallery stored on CD-ROM? Categorizing the underlying mechanisms 
of Web-based art helps to understand the work s unique presence. Art projects can use the Web for more 
than delivering hyperlinked media, because the Web is structured as millions of real-time interconnections. 
The feedback loop between client and server may be exploited through mechanisms such as creating original 
content through submission of information, initiating and documenting ongoing dialog both on and off 
the Web, and creating images and new links based on navigational decisions. Three projects in particular 
at adaweb (http://adaweb.com) have explored this kind of participatory work. The most popular section 
of Jenny Holzer s project, Please Change Beliefs, was based on her Truisms work from the 1980s. The Change 
section was a Web-based adaptation of a project that involved photographing truism posters on city walls 
as they accumulated graffiti. One such photograph is used to open the Web project. Turning to the Web 
as a new kind of public thoroughfare, the project allows visitors to choose and then modify or rewrite 
a truism. The altered version of the truism is presented in the style of the original truism poster (alphabetized, 
all caps) on a page with other people s modified truisms. Several thousand modified truisms were collected 
within the first few months of the project. Ben Kinmont s art is based on sharing common tasks and exchanging 
items with individuals. He often initiates exchange with these people in the street or the space where 
his work is installed. We Both Belong, extends his work by considering the Web as a kind of public space. 
Visitors to the site are invited to write to Ben, via the Web, stating a desire to participate in his 
art project. Ben asks the participants to photograph themselves washing dishes and to mail him the photograph. 
As a record of the interaction, the letters, the photograph, and other items created in the course of 
making the project are shown together as an archive both inside and outside the Web. This project highlights 
an overlap of online and offline spaces by extending Ben s previous artwork into the Web and drawing 
connections made with people through the Web into Ben s space. John F. Simon, Jr. worked with Web-based 
interaction as an experiment in Web viewing in his Alter Stats project. Described as a self-modifying 
Web self-visualization, the project uses the statistics of the visits (hits) to the project to create 
images for visualizing those statistics. Each visit to the project further modifies the database of hits 
to the page and therefore modifies the images created. No visit is identical to any other visit and each 
visit effects future images. This accumulation of statistics happens at every site, leaving traces of 
activity. Alter Stats is situated to use the activity of viewing pages as the input for creating images. 
Sharleen Smith will comment on the interchange of influence and ideas among commercial and fine-arts 
sites from her perspective as the art director and developer of The Dominion on the Sci-Fi Channel. Although 
purists and academicians have decried the commercializa­tion of the Internet, this gentrification by 
corporate America has, in fact, served both to inspire mass culture and to educate artists in the pursuit 
of their own vision. Current trends suggest a need for collaboration between the artist and this growing 
commercial presence. With source code readily accessible to the curious and brave everywhere, it is the 
commercial sites which push the envelope every day. Why? Because this new medium demands creative thinking 
and problem solving. Because our metaphors are shifting, and our interfaces must to keep up with them. 
Communication is evolving... and as business understands all too well, evolution is about the survival 
of the fittest. For now, corporate America is in a unique leadership position. Given the rising complexity 
and cost associated with development of compelling Web sites, it is perhaps unsurprising that commercial 
sites have become the essential point of reference for the cutting-edge and vanguard of the World Wide 
Web. Bracque once described his relationship with Picasso as that of two mountain climbers. One supporting 
the other in turn to advance to new heights and vistas. It is my belief that only through such synergy 
of the commercial and artistic can the Web truly evolve. URLs @art gallery: http://gertrude.art.uiuc.edu/@art/gallery.html 
 Electronic Culture and the Training of the 21st Century Artist: http://gertrude.art.uiuc.edu/ad319/paper1.html 
 adaweb: http://adaweb.com artnetweb: http://artnetweb.com The Brown Moo Bird: http://moobird.com 
Blast Conversional Archive: http://artnetweb.com/projects/blast/home.html The Electronic Chronicles: 
http://artnetweb.com/projects/ahneed/first.html G.H. Hovagimyan Home Page: http://artnetweb.com/gh 
Iola: http://artnetweb.com/iola/home.html Realms: http://artnetweb.com/projects/realms/notes.html Betalab: 
http://www.jodi.org/betalab Getty AHIP: http://www.ahip.getty.edu/ahip/ Ellipsis Publishing: http://www.gold.net/ellipsis/ 
 Heath Bunting: http://www.irational.org/ Pseudo Online Radio: http://pon.pseudo.com CICV - Centre 
de Recherche Pierre Schaeffer: http://www.cicv.fr USA Networks | Sci-Fi Channel: http://www.scifi.com 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237291</article_id>
		<sort_key>489</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[VRML]]></title>
		<subtitle><![CDATA[prelude and future (panel)]]></subtitle>
		<page_from>489</page_from>
		<page_to>490</page_to>
		<doi_number>10.1145/237170.237291</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237291</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Internet</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>World Wide Web (WWW)</descriptor>
				<type>P</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39027348</person_id>
				<author_profile_id><![CDATA[81100105415]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Don]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brutzman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naval Postgraduate School]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http //www. stl. nps . navy.mil/~brutzman/ vrml/siggraph96panel, html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 VRML: Prelude and Future Organizer: Don Brutzman, Naval Postgraduate School Panelists: Mark Pesce, Author, 
VRML List Moderator Gavin Bell, Silicon Graphics Inc., VRML 2.0 Co-Architect Andries van Dam, Brown University 
Salim AbiEzzi, Microsoft ActiveX Animation Don Brutzman brutzman@nps.navy.mil The meteoric development 
of the Virtual Reality Modeling Language (VRML) is one of the most important standards developments in 
graphics and internetworking. An informal, intense, open, and collaborative design process has worked. 
VRML version one defined a concise and workable scene description language. Version two refined this 
language to incorporate behavioral animation mechanisms compatible with the current World Wide Web. Networked 
interoperable interactive 3D graphics are now feasible for everyone s computer. We examine lessons learned 
over the past two years, where VRML is going, and how VRML is triggering fundamental changes in the economics, 
mindset, and membership of the graphics community. Although networking has been considered different 
than computer graphics, network considerations are integral to large-scale interactive 3D graphics. Graphics 
and networks are now two interlocking halves of a greater whole: distributed virtual environments. New 
capabilities, new applications, and new ideas abound in this rich intersection. Our ultimate goal is 
to use networked interactive 3D graphics to take full advantage of all computation, content, and people 
resources available on the Internet. Realizing the lofty ambitions of VRML has required hands-on attention 
to myriad technical and people challenges. How do you specify a 3D scene both concisely and compatibly, 
given a plethora of other formats? How are working (i.e. successful) relationships built among individuals, 
academia, and companies of all sizes? What is a networked behavior? How do we sustainably capture both 
the specification and the standards process? Does VRML = graphics + the Web + networking + behaviors 
+ everything in the world? In other words, where (or does) VRML stop? Which steps are next? Graphics, 
networking, and interoperability breakthroughs repeatedly remove bottlenecks and provide new opportunities. 
A pattern appears as we attempt to scale up in capability and capacity without limit: every old bottleneck 
broken reveals another. Understanding bottlenecks, corresponding solutions, and potential upper bounds 
to growth permits us to develop effective networked graphics. Technically and socially, SIGGRAPH has 
crossed a threshold in capability. As we overcome current bottlenecks, effectively networked graphics 
will simply mean applications. Mark Pesce mpesce@netcom.com Perhaps the most singular aspects of the 
VRML movement are its out-of-control nature and its persistent strength in the face of well­organized 
opposition. From the very beginning when Tony Parisi and I began to share our work with Tim Berners-Lee 
and others we practiced a politic of inclusion, keeping the door open to relationships which could be 
leveraged into successes for VRML. This speaks more of a process of social engineering than software 
engineering, and articulates the heart of the difference between VRML and any of its potential competitors 
(ActiveVRML, OpenFlight, WIRL, etc.). Because VRML has remained open in deed as well as word, because 
anyone can become a member of the community and contribute, VRML has garnered the support of communities 
across the graphics and networking industries. In fact, despite the persistent lobbying of many large 
companies including Netscape, Silicon Graphics, and Microsoft the consensus process which brought us 
both VRML 1.0 and VRML 2.0 has proved resistant to tampering by press release or marketing hype. That 
s one of the real lessons of VRML: the social fabric of the VRML community is the real key to its success. 
It s my belief that this lesson has wide application outside the limited domain of VRML; wherever virtual 
communities are to spring up and flourish, the same conditions must apply. Gavin Bell gavin@sgi.com 
 VRML 2.0 is an incredibly rich file format for creating interactive 3Dmultimedia experiences that are 
distributed across the Internet. It isalso a solid foundation for solutions to the next hard problem: 
multi­userworlds. VRML is a success because it hasn t tried to solve all of the problems of computer 
graphics, simulation, and networking all at once. When creating something to meet the needs of a very 
large group of people, it is difficult to balance the limitless number of features requested against 
the limited amount of design and implementation time available. Rough agreement on both constraints and 
goals is the key to getting anything accomplished. Its design is both solid and practical, in part because 
the Internet gives system designers an invaluable tool: direct feedback from knowledgeable users. We 
had to convince users that our proposal would solve their problems or they would take their business 
elsewhere (i.e. vote for another proposal). Giving concrete answers to the stream of can I do this... 
questions ensured that we were solving relevant problems and constantly testing the design. What s next? 
Tackling the multi-user problem will first require agreement on exactly which problem should be solved: 
multi-user chat is a mucheasier problem than general multi-user collaboration in a shared virtual world. 
Solving the more general problem will require additions to both VRML and the infrastructure of the World 
Wide Web. Andries van Dam avd@cs.brown.edu One of the cliches about standards is that standards bodies 
produce camels, horses designed by committee. (This cynical comment ignores the clear utility of camels 
in the desert environment.) Whether they are de facto industry standards promulgated by a leading company 
or industrial consortium, or de jure official standards promulgated by standards bodies such as the IEEE, 
the ANSI, and ISO, standards reflect both the strengths and the weaknesses of a technopolitical consensus 
process that favors compromise. Recently new standards processes specific to the Internet have emerged, 
aiming for both greater speed and greater democratic input, e.g. the Internet Engineering Task Force 
(IETF). The design and adoption of VRML 1.0 and more recently of VRML 2.0 via the VRML list and the ad 
hoc, self-selected VRML Architecture Group (VAG) is a prime example of an even more rapid process. Both 
the process and the specification should be of great interest to the graphics community. In particular, 
I believe that this successful process should force ANSI/ISO to redesign its heavy-weight, overly lengthy 
standards process. They should now consider a light-weight, fast-track process to review both VRML 2.0 
and its legitimate competitors for a net-based multimedia standard. This standard should support various 
visions of cyberspace: multiple participants, distributed virtual environments that contain autonomous 
objects whose behavioral interactions with participants and each other must be simulated in real time, 
etc. I hope that the pressure of various companies to simply ratify the evolving VRML 2.0 spec (or its 
competitors) will be resisted, and that an open, technically sound, extensible standard will be designed 
to last us well into the next century. VRML 2.0 should certainly be considered as a baseline for the 
future standard. Needless to say, if the standards process is not ultra-fast (12-18 months), VRML 2.0 
WILL become the de facto standard, with all the advantages and disadvantages that implies. Then the official 
standard, if it is not VRML 2.0 (or any other solution) will only displace VRML 2.0 if is demonstrably 
superior, using the Darwinian Web-virus competition model that many Web enthusiasts believe is the dominant 
force for change on the Web. In summary, I believe that it is important to reexamine the good work the 
VRML community has done and to take into account other models, and to do that with all deliberate speed. 
 Salim AbiEzzi salimabi@microsoft.com  Graphics and multimedia for the Internet are needed for both 
interactive illustrations embedded in Web pages, and for distributed immersive shared spaces. On the 
one hand, the majority of Internet users can benefit greatly from added life to Web pages through interactive 
illustrations, which could be used for advertising, information, and artistic and entertainment purposes. 
On the other hand, advanced users are interested in shared spaces and their potential for spectacular 
applications in group entertainment, collaborative engineering, and other far-reaching experiential applications 
of the Internet. The VRML community has been focused on 3D spaces and models primarily; for example, 
the present VRML 2.0 is not designed for sprite animation, hot spots, and synthetic audio, which could 
be building blocks as essential for interactive illustrations as 3D might be. Furthermore, in order for 
large 3D spaces to become viable for the majority of Internet users, much faster 3D texturing, higher-bandwidth 
networking, and more reliable low-latency communication are required It is going to be quite some time 
before these capabilities become common aspects of the infrastructure. As VRML 2.0 is considered for 
standardization by formal organiza­tions, it is important to carefully consider broader Internet needs 
in the area of graphics and multimedia. I believe that either VRML 2.0 needs to be extended to better 
support the broader needs, especially in the area of illustrations, or the situation calls for other, 
possibly complementary, standards. ActiveX Animation (formerly ActiveVRML) is a product out of Microsoft. 
It includes a modeling language and a run-time environment that primarily targets Web-interactive illustrations. 
It provides a novel approach for modeling rich behaviors, media integration, interaction, and events. 
ActiveX Animation facilitates multimedia coordination and rich media composition. It is suitable for 
sprite animation, just as it is for 3D and the interplay between the two. I will contrast it with VRML 
2.0 and highlight its value for Web-interactive illustrations. This panel statement can be found online 
at: http://www.stl.nps.navy.mil/~brutzman/ vrml/siggraph96panel.html  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237292</article_id>
		<sort_key>491</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Breaking the myth]]></title>
		<subtitle><![CDATA[one picture is NOT (always) worth a thousand words (panel)]]></subtitle>
		<page_from>491</page_from>
		<page_to>492</page_to>
		<doi_number>10.1145/237170.237292</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237292</url>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>World Wide Web (WWW)</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP311434800</person_id>
				<author_profile_id><![CDATA[81502684610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nahum]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Gershon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The MITRE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Berger, J., Ways of Seeing, British Broadcasting Corporation and Penguin Books, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gregory, R.L., Eye and Brain, The Psychology of Seeing, Princeton University Press, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hanson, N.R., in The Nature and Function of Scientific Theories, R.G. Colodny (ed.), University of Pittsburgh Press, 1970.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Breaking the Myth: One Picture is NOT (Always) Worth a Thousand Words Organizer: Nahum D. Gershon, The 
MITRE Corporation Panelists: Robert Braham, IEEE Spectrum, New York, NY David Fracchia, Simon Fraser 
University, Vancouver, Canada Andrew Glassner, Microsoft Corporation, Redmond, WA Barbara Mones-Hattal, 
George Mason University, Fairfax, VA Russ Rose, Office of Research and Development, Washington, DC Summary 
We need to understand well both the power and frailty of images (e.g., compared to words) to be able 
to use effectively new visualization and computer graphics technologies in science, education, entertainment, 
and life, and most importantly in the Internet s World Wide Web. The panel and the audience will discuss 
and debate the weaknesses of images and the difficulty in representing information clearly; the dependency 
of visual and information perception on past memories, experiences, beliefs, and culture; the difficulty 
in making effective use of color; and what could be learned from art and design. One picture is worth 
a thousand words Fred R. Barnard But words are words Shakespeare, Much Ado About Nothing ... words 
are images of thought refin d John Keats, O Solitude! Recent developments of computer visual display 
hardware on one hand and computer graphics and visualization methods and software on the other have generated 
new interest in images and visual representa­tions. It is now possible with a flip of a button to generate 
visual depictions of data and information or to take existing images and modify them at ease. This renaissance 
of visual representation has highlighted the notion of the power of images. One picture is worth a 1,000 
words, goes the popular saying. People in the graphics and visualization community have perceived it 
to mean not only that images could portray anything that words can, but that images could do it better 
than words. However, images may have some disadvantages, and words are sometimes more effective (or powerful) 
than pictures. To use images effectively, we need to understand when they are equivalent to words, when 
they are more appropriate than words, and when they are not. This issue has become extremely important 
with the spread of the World Wide Web (WWW), where many document authors do not know when to use an image 
and when to express an idea with words. One problem is that computers representing abstract (e.g., non­numerical) 
information, and visual computing and display are both new media. The understanding of the characteristics, 
advantages, and disadvantages of these new media is crucial to their optimal and effective use. This 
will take some time, however. Similarly, we need to stop relating to the new medium of visual computing 
and display as if it were a replica of paper. This new technology allows us go beyond what is possible 
with paper, and we need to understand the differences between traditional and computer generated images. 
This panel and the audience will discuss and debate situations where images (both traditional and computer-generated) 
do and do not convey information effectively or correctly and where images and words could complement 
each other. Nahum Gershon A Picture is Not a Picture is Not a Picture...: A Picture Could be Worth a 
1,000, 1/1,000, or -1,000 Words The difficulty in representing information clearly, the dependency of 
visual and information perception on past memories, experiences, beliefs, and culture, and the difficulty 
in making effective use of color are some examples illustrating the frailty of image representations. 
To make full and correct use of what display, graphics, and visualization technologies can offer us, 
we need to take these considerations into account when generating images or when viewing them. It is 
true, however, that for certain purposes, images do not need to portray reality exactly. But in these 
cases, we must be sure the viewers are aware of this fact deep in their minds. Otherwise, we might create 
pictures that are worth 1/1,000 (Hanson, 1970) of a word or even -1,000 words. We need to make sure people 
understand that not everything could effectively be put in a visual form. An example is the term text 
visualization. People usually imply that images could always represent effectively all the information 
contained within a collection of text documents, and that it is much easier to get this information from 
images than from words. Is this really possible? If yes, why was language created, why did silent movies 
contain textual information, and why was sound introduced to film? Robert Braham The Shibboleths of 
Pictorial Elites The shibboleth, in its original meaning, was nothing more than a single graphical sign 
(a word in Hebrew) used as a military password/ passgraph. An enemy user of that graphical sign was caught 
when even though interpreting its place in the graphical system rationally, he lacked a crucial bit of 
knowledge about the ambiguities of the graphics known only to the other side the pictorial elite. Pictorial 
elites, even if more well meaning, are now springing up with alarming rapidity, aided by the tools of 
computer graphics. Basics of semiotic analysis, such as the ramifications of symbolic, indexical and 
iconic signs, were developed decades ago in linguistic theory, but are powerful concepts for understanding 
graphics and their power to shape and be shaped by graphic communities. Using these and other analytical 
tools, we would do well to compare the graphical/cognitive turning points we are now going through with 
those that parallel them in earlier times in the West. In the following cases, oral communities word 
users confronted radically new graphics technology: the composition of epic poetry, the Renaissance 
rebirth of 3-D projection, the change from scroll to pamphlet to book, medieval memory technique, and 
early scientific diagramming, including animation. The history of the earliest notations for Western 
music, devised for plainchant an oral communicative system par excellence holds a particularly interesting 
position in this light. David Fracchia Towards Image Understanding The phrase One picture is worth a 
thousand words has become cliche in our vernacular. Part of the reason we say it is because we assume 
that, to quote another cliché, seeing is believing. That is, as we look around our world, we perceive 
it at face value as reality. In contrast, while we may acknowledge that words, particularly poetry, may 
bring to mind multiple images, it is not commonplace to claim that a phrase is worth a thousand pictures. 
The ambiguity of multiple personal images that arises from verbal communication means that we as a culture 
do not automatically believe words. To compensate, our educational system focuses on verbal literacy, 
allowing us to express ourselves precisely as in mathematics or evocatively as in poetry. In fact, without 
verbal ambiguity we would lose much of our humor and pleasure in language. Unfortunately, there is no 
parallel education in visual literacy. While this may not have been a critical issue in the past, now 
with the advent of advertising, trick photography, and computer graphics we have the potential for creating 
visual ambiguity. This puts our whole culture at high risk of being fooled by what they see. As disturbing 
as this may sound, the extension of humans propensity to introduce ambiguity into visual images is a 
healthy indication of the maturity of visual languages as a communication medium. In fact, we revel in 
being able to create visual forgeries. What is needed now is to parallel the development of visual language 
with the exploration of visual literacy. If we simultaneously want to be able to generate images for 
our amusement and for information dissemination, we need to understand how pictorial ambiguity arises. 
We know from work by people such as Bertin, Goodman, Laursen, Tufte, and Ware, that these problems can 
arise in most aspects of visual representation. It has been suggested that before we can disambiguate 
images, we need to discover the basic components of visual language. However, exactly what these would 
be is unclear, particularly in the case of computer images. It is possible that our real clues will come 
from graphic design and/or perceptual psychology. Andrew Glassner Different Media Means Different Messages 
 Words and images speak to different parts of our experience. Both can be primal or abstract, direct 
or vague. But novels and paintings are not interchangeable; a poem is not a child s finger-painting. 
These media are complementary, and neither identical nor antagonistic. Artists can combine these languages 
to produce interesting and meaningful work, but this composite is yet a third form, and does not subsume 
or replace the others. Words often fail to describe images, but images just as often fail to capture 
what can be said by words. Barbara Mones-Hattal A Picture May Be (Or May Not Be) Worth A Thousand Words: 
Lessons From The WWW?  A picture may be worth a thousand words, but those words may not be the same 
from one person to the next. It has always been a challenge for the artist to design with simplicity, 
subtlety, and sensitivity. The integration of text and image is not new to the artist. The emergence 
of the World Wide Web, however, has made for unusual and distinctive design issues. A certain urgency 
has emerged in order to further refine these relationships as on-line design becomes big business. As 
the WWW becomes a more effective design multimedia tool, it becomes even more important for us to realize 
when to use text or graphics, or a combination of the two, in order more successfully utilize this new 
venue. Studying the WWW affords us new opportunities to gather enormous amounts of information about 
successful and less successful design strategies. The design and implementation of icons, the use and 
overuse/misuse of backgrounds, the potential for audio, 3D and interactive 3D have led to both unique 
and creative spaces and confusing and/or boring ones. An initial concern about bland and limited design 
options has replaced itself with new and potentially more exciting ones. However, it becomes important 
to start to discuss and identify what successful Web design might mean, so that we may be more able to 
both recognize and utilize these spaces with greater confidence. Russ Rose P1000: A Picture is Worth 
1000 Words In this era of the information explosion, there exists the need to take advantage of the 
power provided by the human s visual processing system. Visual exploitation will help in understanding 
the content of the vast uncharted mountains of information, as well facilitate meaningful analyses of 
that information. Hence, visual representation of the information can be a powerful enabling force relative 
to improved understanding. A picture is worth 1000 words; that is, in 1/1000 the time, a visual image 
can be processed and analyzed, rather than being represented and processed as words. If represented as 
words, it would often require more than 1000 words, would take 1000 times as long to understand, and 
would still not communicate the content as comprehensively as a visual image can. The visual representation 
of the information, however, must be based on a comprehensive and information rich structure. If not, 
it will easily not be worth a single word, let alone the 1000 words (we all have set through tiring briefings 
centered around charts cluttered with information-free clip art that provides no more than visual noise). 
Progress must continue, and be accelerated, in the area of visual representation of information. This 
progress will only be made through significant commitment of resources, as well as the focus of intellectual 
energies for the long term. Afterword We need to understand well both the advantages and disadvantages 
of using words, images, or their combinations. This is essential to the effective use of new visualization 
and computer graphics technologies in science, education, entertainment, and life, and most importantly 
in the Internet s World Wide Web. We also need to understand better the new media of visual computing 
and display, and we must take advantage of the experiences and insights of the art and design communities. 
 References Berger, J., Ways of Seeing, British Broadcasting Corporation and Penguin Books, 1977. Gregory, 
R.L., Eye and Brain, The Psychology of Seeing, Princeton University Press, 1990. Hanson, N.R., in The 
Nature and Function of Scientific Theories, R.G. Colodny (ed.), University of Pittsburgh Press, 1970. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237293</article_id>
		<sort_key>493</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[Digital stunt doubles]]></title>
		<subtitle><![CDATA[safety through numbers (panel)]]></subtitle>
		<page_from>493</page_from>
		<page_to>494</page_to>
		<doi_number>10.1145/237170.237293</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237293</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Arts, fine and performing**</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31082865</person_id>
				<author_profile_id><![CDATA[81332509273]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kleiser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walczak Construction Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital Stunt Doubles: Safety Through Numbers Organizer: Jeff Kleiser, Kleiser-Walczak Construction 
Co. Panelists: Frank Vitz, Kleiser-Walczak Construction Co. Jeff Light, Industrial Light and Magic Shahril 
Ibrahim, Boss Films Richard Chuang, Pacific Data Images Our company has been researching the creation 
of computer-generated actors, or Synthespians (TM) as we call them, since 1985. Diana Walczak, a partner 
in the company, sculpted human figures and faces that were digitized and animated for several experimental 
films shown at SIGGRAPH: SEXTONE FOR PRESIDENT (1988) and DON T TOUCH ME (1989). As rendering algorithms 
relentlessly approach photo-realism, the ability to create life-like characters becomes a realistic and 
cost-effective prospect. The level of detail required to create a convincingly realistic person varies 
directly as a function of the distance of the person to the camera; a distant figure can be made to look 
realistic with a fraction of the effort required to render a convinc­ing close-up. Acting, in the pure 
sense of the word, requires the camera to be at least in the proximity of a two-shot, and the industry 
is on the verge of this capability, although it is extremely complex and costly to produce. An interim 
application of computer-generated characters lies in the representation of distant humans performing 
motion that is inconvenient or impossible to perform by a live stunt person. Digital Stunt Doubles have 
recently been used in a variety of applications to substitute for stunt people in the interest of safety, 
cost, or simply because the stunt would be impossible to accomplish with a live human being. This panel 
session is intended to give four industry experts an opportunity to show some of the applications they 
have found for this technology in recent feature films, and to discuss the timetable for computer graphics 
as an industry to move up to the next level: from life-like digital stunt doubles to lifelike digital 
actors. Frank Vitz At Kleiser Walczak Construction Company we have been working in the area of computer 
generated actors for many years. We coined the term Synthespian to describe these synthetic actors. We 
have created a variety of Synthespians... some human, some exotic, some realistic, some fantastic. But 
it is only relatively recently that our technology has improved to the point where we can now create 
Synthespians at feature film resolution that are indistinguishable in certain cases from real human actors... 
Synthespians that can act as Digital Stunt Doubles. The movie Judge Dredd, starring Sylvester Stallone, 
was one project that provided us with an opportunity to develop the realistic character animation techniques 
necessary for Digital Stunt Doubles. The screenplay called for a large number of digital effects shots, 
including realistic vehicles, explosions, weapons effects, and most exciting: an extended aerial chase 
sequence through the streets of a futuristic city on flying motorcycles! The Lawmaster Sequence, as it 
was called, would involve impossibly fast flybys, aerial maneuvers, and stunts . . . all to be seamlessly 
matted into model city backgrounds. This was a perfect place to use Digital Stunt Doubles. The sequence 
was divided into two types of shots, Live Actor and Digital Stunt Double shots. The live shots were typically 
those that involved closeups of the actors speaking, while the Stunt Double shots had difficult camera 
angles, extreme action, explosions, and so forth. We created digital stunt double versions of two actors, 
faithfully reproducing their facial features, body proportions, and costumes. We created CGI versions 
of the Lawmaster Motorcycles that matched the live stunt bikes. We choreographed the motion of the digital 
bikes to fit into model photography of Mega City. We animated the Stunt Doubles to move realistically 
on their bikes. We added a variety of additional digital effects, such as explosions, smoke, lens flares, 
and motion blur to make the illusion more convincing. In this panel discussion I hope to share some of 
the details of what we went through during the creation of these Digital Stunt Doubles for Judge Dredd. 
We learned a lot during the process, and have some new ideas as a result... ideas that are finding expression 
in our current projects. The future of computer-generated actors is wide open. I am looking forward to 
sharing with the other panel members some speculation about where we might be headed. Jeff Light As 
film effects are driven to dizzying heights of more intense action and to create more amazing fantasy, 
the pressure to create convincing digital stunt doubles is increasing, and the technology to make it 
possible is becoming more readily available. The use of puppets, dummies, or stunt-persons as stunt doubles 
has been practiced since the beginning of movie making. Digital stunt doubles are simply the logical 
extension of this idea. The need for digital stunt doubles is driven by a number of factors. The performance 
may be too dangerous or impossible for an actor. A shot that requires thousands of extras may be too 
expensive to shoot with modern financial constraints. Some scenes may already be composed of sufficiently 
complex synthetic imagery, such that it is simply easier to composite a digital character interacting 
with other elements in a scene. There are daunting obstacles to overcome in the process of creating a 
digital stunt double. Primarily, it should not call attention to the fact that it s an effect. The modeling, 
animation, and rendering must blend seamlessly with surrounding shots. Often, the switch between live 
action and the CG double will happen during the shot, when the audience is focusing their full attention 
on the character that is being switched! Modeling and rendering hair, skin, and cloth remain difficult 
obstacles to imitating the look of the original per­former. Animating the subtleties of real human or 
animal motion is highly complex. Motion capture or procedural animation approaches may provide partial 
answers, but are not a panacea. Since the drive to create an increasing amount of a performance with 
a digital character is inexorable, the implications of not simply supplanting but actually replacing 
actors, living or dead, raises important artistic and ethical issues, which I hope we can approach with 
wisdom. Shahril Ibrahim Digital doubles are perhaps the first use of virtual actors. They can be utilized 
in dangerous and risky situations, and can be used in short segments. The short amount of screen time 
reduces the complexity of modelling/animating and rendering of a complete human. Digital doubles also 
allow plasticity of the actor to achieve shots that would be otherwise impossible, for example facial 
and body morphs. Currently these doubles are produced by either imaging real humans and manipulating 
the digital information, or by painstakingly animating the humans with a combination of kinematics and 
dynamics. A hybrid approach is the use of motion capture to drive animated characters. It would be interesting 
if these animated characters could mimic their physical counterparts to achieve nuances and subtlety 
of motion. As technology progresses and our understanding of human motions and behavior increases, these 
digital stunt doubles may move out of being just a double to being the star character. Some of this has 
already happened, as in the movie Species.  Richard Chuang In 1995, eight of the ten top box office 
grossing films were dominated by digital visual effects. Audiences flock to these special effects extravaganzas, 
and directors are increasingly willing to embrace digital techniques to enhance or replace traditional 
effects. One of the latest techniques to be explored in Hollywood is the use of digital stunt doubles 
in difficult or dangerous stunt sequences. What is the difference between replacing a highly-paid actor 
with a stunt double, and having a performer s data captured digitally and used to create a digital stunt 
double? In both instances, production creatives are hoping to match the performance qualities of a principal 
actor without subjecting them to physically challenging or dangerous scenes. Digital stunt doubles provide 
certain advantages over human stunt doubles. The digital stunt double can provide a greater degree of 
flexibility when shooting on a model or miniature set. It is time-consuming to track witness points for 
integration of live actors into model and miniature plates. A digital stunt double provides greater control 
over scale, motion, and lighting, and is more easily matched to the set. Concurrently with the demand 
for action-heavy effects films, stunts are becoming increasingly dangerous and spectacular. Using a digital 
stunt double is advantageous when a particularly complex or dangerous stunt is required. Rather than 
placing talent in a dangerous situation, a stunt double can perform a set of movements that match the 
shot in a safe, controlled environment. The motion data is captured digitally and applied to a computer-generated 
model, which is based on the physical qualities of the principal actor. Once the raw motion is captured 
and applied to the model, it is refined to create a scene that would have been unsafe or impossible to 
shoot practically. Digital stunt doubles offer directors greater flexibility in changing camera angles, 
lighting, timing, and even location without having to re­shoot the live action scene. However, a digital 
stunt double is only as good as the original data captured from human performance and the artistry of 
the CG animators who translate raw data into finished animation. In most cases, computer animation is 
not yet suited to creating realistic motion, complete with nuances of expression and gesture that are 
inherent in human performance. The amount of work it currently takes to perfect a digital character without 
motion capture data precludes many creative budgets. On the other hand, motion capture data is raw, and 
must be refined with hand-animation techniques to convey the unique qualities of an individual actor 
s performance. By combining motion capture data and computer animation, it is possible to get the best 
of both worlds: a realistic performance, plus the flexibility of being able to modify and adjust the 
scene in post. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237294</article_id>
		<sort_key>495</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[Global multi-user virtual environments (panel)]]></title>
		<page_from>495</page_from>
		<page_to>496</page_to>
		<doi_number>10.1145/237170.237294</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237294</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.2.1</cat_node>
				<descriptor>Distributed networks</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456</concept_id>
				<concept_desc>CCS->Social and professional topics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010537</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Distributed architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003033.10003034</concept_id>
				<concept_desc>CCS->Networks->Network architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P300840</person_id>
				<author_profile_id><![CDATA[81100368380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Felger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Computer Graphics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Global Multi-User Virtual Environments Organizer: Wolfgang Felger, Fraunhofer Institute for Computer 
Graphics Panelists: Lennart E. Fahlen, Swedish Institute of Computer Science R. Bowen Loftin, NASA Johnson 
Space Center &#38; University of Houston Michael R. Macedonia, Fraunhofer Center for Research in Computer 
Graphics Gurminder Singh, National University of Singapore Summary: Wolfgang Felger Research and development 
activities are facing a continuing globaliza­tion. Networks are getting more important than companies 
or institutes. Main reasons for this trend are: (Faster) access to new knowledge and technology and 
its improved transfer.  Economic reasons.  Use the particular advantages of multicultural work environments. 
 Together with a steadily improving technology, the global informa­tion society is not far away. In 
particular, the networking and virtual environment technologies are making much progress and are ready 
to enable shared, distributed, cooperative activities. This panel addresses network infrastructure, systems, 
and applica­tions related to multi-user virtual environments, especially the global (intercontinental/international) 
aspects of these issues. The panelists will summarize their lessons learned, will highlight latest activities, 
and share with the audience their short/mid-term vision on global, multi-user virtual environments. The 
panelists intend to enrich their presentations with (multicontinental) live demonstrations. Furthermore, 
specific applica­tions are showcased at Digital Bayou (Where No Man Has Gone Before). Lennart E. Fahlen 
The objective of this panel is to discuss the long term development of future large-scale social electronic 
spaces. In this time and age, this means networked computer systems that exploit the idea of a place 
inhabited by users who communicate with one another using the natural and informal means appropriate 
to a range of social situations (e.g. work, family, leisure, etc.). Ideally, such systems should support 
many thousands of users engaged in real time interaction with one another. Below are a number notions 
relevant to the above goals: Users as inhabitants of electronic spaces. Users should experience a sense 
of immersion within a computer system, as opposed to only interacting with it.  Infrastructures need 
to support real-time events similar in scale to gatherings of people in the physical world (e.g. on the 
scale of today s largest sports and entertainment events). The construction of such large scale electronic 
spaces introduces critical problems of scale. Some issues to be considered are network-, processing­and 
perceptual scaleability and the unavoidable latencies caused by the geographical separation of users 
and nodes.  The network and software infrastructure required to deliver such applications to the general 
citizen. Today the WWW provides for large scale multimedia information access and distribution. But seen 
from the perspective of users being social beings inhabiting an electronic space, the Web carries severe 
shortcomings: asymmetry between information provision and consumption, the environment is strangely uninhabited, 
users are only visible through the information they provide or through a number-of­visitors statistic, 
there is no provision for real-time interaction and feedback, etc.  Interaction metaphors and techniques 
which seamlessly combine information visualization, and access with social interaction. How should socially 
inhabited electronic spaces be structured, and what tools are needed to construct them? Furthermore, 
how should people be supported in exploring and navigating such spaces, and how might the structure afford 
possibilities for social interaction?  Spatial metaphors seem to present solutions to some technical 
issues such as access to computational resources, as well as to other people. These alternatives are 
more promising than traditional solutions (i.e. password protection, passing of tokens etc.).  How to 
provide a sense of personal presence and awareness, both direct and peripherally, with other people within 
an electronic space, and how to achieve this through user embodiment and other representation techniques. 
A number of social studies show the importance of mutual and peripheral awareness phenomena to the coordination 
of social interaction. Most current systems have major difficulties conveying presence of other users, 
awareness of what these other users are doing, and providing mechanisms to represent a user as an embodiment 
within a single application.  Techniques for integrating electronic spaces with physical spaces, including 
notions of shared augmented reality and shared augmented virtuality.  Intelligent agents as inhabitants 
of social computing environments  (i.e. in the presence of many humans and other agents). To see such 
real and virtual information spaces as inhabited social environments, capable of supporting participation 
in many different activities and social relations. Also, one should not forget that such environments 
must be expressive, aesthetic, dramatic, motivational, and inherently enjoyable to use and inhabit. In 
summary my position is very much defined by terms such as inhabited, social interaction, awareness, 
 spatial metaphors, and mass participation. Furthermore, it has very little to do with photo realistic 
rendering, and concerns itself much more with a paradigm shift that views virtual reality and related 
technologies as providing inhabited social spaces and by conceiving of users as citizens and social beings 
both at work and play. R. Bowen Loftin Opportunities to exploit shared virtual environments abound. 
Collabo­rations in art, business, education, engineering, and science could all benefit from the ability 
of collaborators to share the same environments and the same experiences while directly interacting with 
information displayed in useful ways. Endeavors in all these spheres of activity are increasingly global 
in scope, challenging the ability of communications technologies and display/interaction metaphors to 
enable the sharing of complex environments in real-time. This panel presentation will describe a successful 
demonstration of a shared virtual environment for training an international astronaut team, and will 
conclude with an overview of current applications in computational chemistry and science education. Historically, 
NASA has trained teams of astronauts by bringing them to the Johnson Space Center in Houston to undergo 
generic training followed by mission-specific training. The latter begins after a crew has been selected 
for a mission, often as much as two years before launch. While some Space Shuttle flights have included 
an astronaut from a foreign country, the International Space Station will be consistently crewed by teams 
of astronauts from two or more of the partner nations. Not surprisingly, the international partners in 
the Space Station program would prefer to significantly reduce the need for their citizen astronauts 
to travel to and remain in Houston for training. As a means of demonstrating the feasibility of using 
shared virtual environments to support the training of international astronaut teams, an experiment was 
conducted on September 20, 1995. Astronaut Bernard Harris (physically located at the Johnson Space Center 
in Houston) entered a virtual environment with Astronaut Ulf Merbold (physically located at the Fraunhofer 
Institute for Computer Graphics in Darmstadt, Germany). Their shared environment consisted of models 
of the Space Shuttle payload bay and the Hubble Space Telescope (HST). The two astronauts spent over 
thirty minutes performing the major activities associated with the changeout of the HST s Solar Array 
Drive Electronics (SADE). Their work included the real-time hand-off of the replacement SADE in exchange 
for the original SADE. At the conclusion of the task the two astronauts shook hands and waved good­bye. 
The positive reaction of both astronauts to this experiment, has led to plans for the development of 
applications in support of future international missions, and an increase from two to three sites. Before 
the end of this century, efforts will be made to support shared environment between the ground-based 
installations and Low Earth Orbit for both just-in-time training and performance support for maintenance, 
science, and emergency medical services. Applications are also under development that will support the 
training of multi service military teams assigned to Operations Other Than War (such as the current peacekeeping 
operations in Bosnia, or humanitarian relief). Finally, this same technology is being explored as a means 
of allowing students from widely separated geographical locations to jointly perform experiments and 
observations in simulated environments. The computer graphics community must not believe that the only 
barriers to the effective use of shared virtual environments are bandwidth and rendering performance. 
A multitude of psychological, cultural, and human-computer interaction problems must also be successfully 
solved. The task of creating the technological infrastruc­ture for sharing virtual environments on a 
global scale is surpassed only by the task of understanding how best to utilize applications built within 
the context of this infrastructure.  Michael R. Macedonia This presentation will describe the infrastructure 
required to support VR in a global environment. Moreover, it will discuss a project that CRCG is participating 
in to develop that infrastructure the MAY project. CRCG has focused its research efforts toward determining 
how computer networks can be made to transform the workplace into a shared environment, allowing real-time 
interaction among people and processes without regard to their location. It is this illusion that allows 
the use of VR and telepresence for applications like distance learning, distributed interactive simulation, 
and group entertainment. Emerging international broadband networks will enable the use of VR for applications 
that span the globe. These tools will support communication among multiple users in order to bridge long 
distances and different times. Common research, and business and social contacts across continents will 
become as simple as meetings between partners in one room. For example, the goal of manufacturing enterprises 
will be able to design, develop, prototype, and test new products in a seamless virtual environment with 
engineers in Detroit, designers in Munich, and managers in Milan. This is one of the goals of the MAY 
project. The MAY project will test emerging communication facilities between research and industrial 
sites in Germany and the United States of America. The communication makes use of advanced collaboration 
tools like BERKOM, GroupX and Mbone over a high-speed interconti­nental ATM link. MAY comprises several 
different goals: MAY is intended to gain experience with multimegabit communi­cation facilities bridging 
large geographical distances and grossly different time zones. The usefulness of group applications for 
research and industrial activities under the above circumstances will be evaluated.  MAY shall provide 
solutions for problems that occur using long distance links like echo in headset-free environments, and 
the small working time overlap in continuous development around the clock scenarios.  Data load patterns 
of the link will be monitored to gain knowledge about the resource allocation requirements for these 
advanced high speed applications.  Gurminder Singh Nearly half the world s computers are idle at any 
given time. But at the same time, people face serious deficiency of computing and network resources when 
it comes to running large scale multi-user virtual worlds. We have been developing architectures to support 
large numbers of concurrent virtual world users on off-the-shelf PCs and Internet. The first system we 
developed, called WorldNet, is currently being used to develop several commercial products. Based on 
the WorldNet experience, we are currently developing an architecture, named NetEffect, to support and 
run large scale multi-user virtual worlds that span across continents. The idea is to utilize idle computers 
located around the world to facilitate communication among virtual worlds. Our architecture is based 
on using distributed cooperat­ing servers that are able to migrate from one workstation to another. People 
around the world are able to volunteer their workstations to act as servers when they are not using them. 
For example, a person in Vancouver, Canada can volunteer his workstation from 7PM to 7AM every day; this 
is the time slot when he does not use his machine and is happy if it can be put a better use. We run 
a collection of master servers around the world on fixed Internet addresses which know about the availability 
of volunteered machines. The master servers ensure that when a particular server workstation s free time 
is up, the server is migrated to another available machine; all virtual world clients connected to that 
server are also migrated to the new server. The client virtual worlds, during their session keep moving 
around from one server to another without being affected in any significant way. Based on the above architecture, 
we are developing interactive discovery learning applications for children. These applications enable 
children to learn new, useful information through interaction with a game-like interface, and through 
collaboration with other children on the network. Collaboration among children is supported by enabling 
and encouraging them to share objects and information with one another.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237295</article_id>
		<sort_key>497</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[How can SIGGRAPH be more effective in promoting computer graphics? (panel)]]></title>
		<page_from>497</page_from>
		<page_to>498</page_to>
		<doi_number>10.1145/237170.237295</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237295</url>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.7.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003580.10003584</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing profession->Computing organizations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456</concept_id>
				<concept_desc>CCS->Social and professional topics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP311435200</person_id>
				<author_profile_id><![CDATA[81502684610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nahum]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Gershon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The MITRE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ellis, Bob, "SIGGRAPH Public Policy," Computer Graphics, Vol. 30, No. 1, Feb. 1996, pp. 45-46.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Encarna~ao, Jose Luis, acceptance speech of SIGGRAPH's Steven A. Coons Award, Proceedings of SIGGRAPH 95 (Los Angeles, CA, August 6-1, 1995). In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH, pp. 7-9.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 How Can SIGGRAPH be More Effective in Promoting Computer Graphics? Organizer: Nahum D. Gershon, The 
MITRE Corporation Panelists: Alain Chesnais, Alias Wavefront, Paris, France Bob Ellis, Fountain Hills, 
AZ Jose Encarnação, Fraunhofer Institute for Computer Graphics, Darmstadt, Germany Donald P. Greenberg, 
Cornell University, Ithaca, NY Summary In spite of its glorious past and present, computer graphics 
is not always appropriately recognized by funding agencies, academia, and industry. The panel members 
will discuss and debate how SIGGRAPH can or should foster a vigorous agenda for improving the stature 
of computer graphics research, development, and applications. SIGGRAPH has the potential to affect all 
walks of life, industry, and academia by actively addressing the right issues. We hope this panel will 
lead to a working group within the SIGGRAPH Public Policy Task Group to address these issues, formulate 
recommendations to SIGGRAPH, and carry out activities with the goal of promoting computer graphics. 
How Can SIGGRAPH be More Effective in Promoting Computer Graphics? If SIGGRAPH does not do it, either 
somebody will do it or computer graphics will lose importance and weight or recognition over time. Jose 
Luis Encarnação, acceptance speech of SIGGRAPH s Steven A. Coons Award, 1995 Computer graphics (CG) has 
a distinguished record of achievements over the past 30 years, and SIGGRAPH has played a large part in 
this success. Computer graphics is the enabling technology of important areas such as multimedia, visualization, 
and the Internet s World Wide Web. Application areas such as 3D medical imaging, entertainment (film 
and TV), education, information visualization, pharmaceutical research, weather modeling, automotive 
and computer industries, and science and engineering have benefited from computer graphics as well. In 
spite of CG s impressive influence, the situation today is somewhat different. Computer graphics is not 
always as well recog­nized as it should be, and funding agencies do not have programs designated particularly 
for computer graphics. Few academic computer science departments perceive computer graphics to have basic 
research value. Few foster (or encourage) research careers in CG for their junior staff members. As a 
result, there are few graduate programs in computer graphics. The panel and the audience will discuss 
the problems facing computer graphics as a discipline, suggest solutions, and explore areas where SIGGRAPH 
could take action. We hope the panel will inspire a group to work with the SIGGRAPH Public Policy Task 
Force on these issues to formulate recommendations to SIGGRAPH on how to improve the standing of computer 
graphics. Central questions of this panel include: How can computer graphics get the proper recognition 
for its achievements and impact on society?  How can we increase the level of funding given to computer 
graphics basic research and R&#38;D?  How can the academic stature of computer graphics be improved? 
 How can we encourage careers in computer graphics?  The above issues raise more specific key issues 
related to SIGGRAPH. The panel and the audience will discuss these specific key issues for developing 
an increased effectiveness of CG which include: Should SIGGRAPH promote increasing the levels of funding 
and investments from government agencies and industry? If yes, how? How to increase the credibility 
and support in academia? Would an increased level of funding be sufficient?  How can SIGGRAPH speak 
for its members? Options include (informal) lobbying, writing papers in leading magazines, sending representatives 
to other organizations (e.g., international), polling members and incorporating their recommendations, 
and providing education to the public, industry s program managers, and policy makers relative to public 
policy issues.  SIGGRAPH is a scientific educational organization: Is it appropriate for an educational 
organization to take positions on social issues?  Due to change in times: Should SIGGRAPH broaden its 
charter to go beyond an educational organization an organization that promotes research and development 
in computer graphics, its application and implementation across industry, governments, and education? 
What are the limits on our activities (ACM policies, budget, volunteer time, etc.)?  How to increase 
the international involvement of SIGGRAPH and support from international sources?  Alain Chesnais SIGGRAPH 
Needs to Define its Public Policy Strategy and its Role in Coming Years SIGGRAPH as an organization needs 
to define its public policy strategy, taking the needs of its members into account. A large portion of 
our member base comes from academia, and SIGGRAPH could play a substantial role in raising the awareness 
of decision makers concern­ing their policy regarding computer graphics research and education. Is this 
desirable? Could we do it in a manner that respects the various situations presented in the different 
countries that our members come from? Is academic research and education the only identifiable member 
community that would want SIGGRAPH to take active measures on their behalf? These are questions that 
we need to address as SIGGRAPH considers its role in coming years. Bob Ellis What are Appropriate Public 
Policy Activities for SIGGRAPH? At SIGGRAPH s 1994 Snowbird strategic planning meeting, attendees called 
for SIGGRAPH to take an increased role in public policy, and this was determined to be one of the seven 
focus areas for consideration in SIGGRAPH strategic planning. Suggested activities included increased 
involvement with the public, fostering a social conscience, taking leadership on legal issues, providing 
accessibility to the technologically disadvantaged, and playing an increased role in political and social 
issues. Since that time, working meetings have been held at the annual conferences in 1994 and 1995, 
and an open meeting was held at the 1995 conference. These meetings have attracted an increasing number 
of interested participants with a diversity of backgrounds and interests. Key issues that have come from 
the meetings include the need to inform political policy makers and program managers about computer graphics, 
and the need to impact the funding for computer graphics research and education. To many people, public 
policy activity means taking positions on issues and advocating those positions to members of government 
and the public. I personally believe that SIGGRAPH s public policy activities, as part of a scientific, 
educational society, should be limited to providing education on the implications of alternatives for 
others who do advocate policy positions. This activity serves our members who I believe joined SIGGRAPH 
primarily for technical reasons. Two projects were started before I became Chair of SIGGRAPH s Public 
Policy Task Force, and I support them fully: a report on the role of graphics in the Global Information 
Infrastructure, which has received funding from the SIGGRAPH Special Projects committee, and a white 
paper identifying significant computer graphics research topics, which has been formally endorsed by 
the SIGGRAPH Executive Committee.  Jose Luis Encarnação Promote the Role of Computer Graphics as Enabling 
Technology and as Technical Professor ACM SIGGRAPH is extremely successful in being an association for 
publishing scientists results, promoting in this way research in Computer Graphics and therefore also 
by setting trends. ACM SIGGRAPH has also been extremely successful in supporting educational activities 
in Computer Graphics. Computer Graphics has evolved to a very high level of importance as a technical 
discipline and as a key enabling tool for a large spectrum of technologies and applications. I can see 
a new role for ACM SIGGRAPH in the area of building relationships and synergies between academia, R&#38;D 
institutions, and industry for the reinforcement of applied and market-oriented research in Computer 
Graphics. I see another role in supporting the speeding-up of the technology transfer process from academic 
results into applications and products. An additional role should be to advise policy makers, industry 
leaders, and program managers at funding agencies on the strategic and technologi­cal importance of Computer 
Graphics. The objective here is to lobby for the entire field of Computer Graphics. For these purposes, 
ACM SIGGRAPH should assume a strong position of technological, scientific, and trend-setting leadership 
by extending its profile to also include the role of a professional association. One way of ACM SIGGRAPH 
implementing these roles could be to establish a high-ranking forum with executives from policy and industry 
(suppliers and users) with the task of developing a series of SIGGRAPH Strategic Workshops on specific 
topics, which are important to promote the role of Computer Graphics as an accepted enabling technology 
and as a technical profession. Nahum Gershon Taking a More Active Role In his acceptance speech of SIGGRAPH 
s Steven A. Coons Award, 1995, Jose Encarnação, posed the rhetorical question Is there is a more pro-active 
role for ACM SIGGRAPH to play? The answer is yes. SIGGRAPH needs to take a role of a professional organization 
pursuing the interests of the computer graphics field, as well as the public interest. Governments and 
industry need to recognize the constructive contribution of CG to many areas and to society in general. 
Funding must increase, as well as the recognition of academic institutions. This could be achieved by 
rethinking the roles of SIGGRAPH and adjusting them to the times. We also need to strengthen the tie 
between academic and applied research crossing over international boundaries. An effective and assertive 
public policy agenda is the key for achieving the well deserved-support and recognition of computer graphics, 
and its contribution to society and technology. Don Greenberg The Area is Perceived as Not Having Basic 
Research Values One of my biggest concerns is that a number of the most basic research issues have not 
been addressed. The what you see is what you get (WYSIWYG) paradigm has existed for too long. Although 
this situation may be acceptable to the entertainment and gaming industries, it is not satisfactory for 
computer simulations, such as pilot training or architectural design, scientific visualizations, or the 
medical profession, to name a few. Perhaps the cause of this might even be the success of SIGGRAPH; images 
that are created are so visually impressive and plausible that they are accepted as the real thing. But 
it is now time for change! The computer graphics field is maturing. SIGGRAPH is entering its 23rd year. 
Processing power is now available, and specialized hardware accelerators are now commonplace. But the 
same incremental algorithms, developed without a real scientific basis in the late 1960s and early 1970 
s, are still in use. Such issues as computational complexity, perceptual thresholds, and metrics for 
evaluating user interfaces have not been given much attention. Even worse, the feedback loop, the hypothesis, 
testing, and experimental verification, so prevalent in most scientific disciplines, is almost ignored 
in computer graphics. With the exponential growth in processing power and bandwidth, we need to set a 
research agenda to achieve a better foundation for the next generation of computer graphics. To achieve 
these goals, it will be necessary to have a multi­disciplinary approach. Computer graphics, at least 
the rendering portion, has fundamental research issues in the fields of optics, physics, geometry, color 
science, the software/hardware engineering fields, and a large number of scientific areas normally falling 
within the domain of computer science or electrical engineering. Maybe this is why the subject does not 
neatly fall within a computer science department or graduate field, but it is not a sufficient reason 
for the scarcity of graduate programs (note: this paragraph relates to the paucity of graduate computer 
graphics programs in the United States). Afterword ACM SIGGRAPH, as a society, has the clout to influence 
changes in universities, government, and funding agencies. How and when should this clout be used? Should 
SIGGRAPH, a professional, educational society try to influence public policy? For those who say, SIGGRAPH 
should ... , who is SIGGRAPH? We hope the panel will inspire a group working with SIGGRAPH Public Policy 
Task Group on these issues to form recommendations to SIGGRAPH on how to improve the standing of computer 
graphics. References 1. Ellis, Bob, SIGGRAPH Public Policy, Computer Graphics, Vol. 30, No. 1, Feb. 
1996, pp. 45-46. 2. Encarnação, Jose Luis, acceptance speech of SIGGRAPH s Steven A. Coons Award, Proceedings 
of SIGGRAPH 95 (Los Angeles, CA, August 6 1, 1995). In Computer Graphics Proceedings, Annual Conference 
Series, 1995, ACM SIGGRAPH, pp. 7 9.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237296</article_id>
		<sort_key>499</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>60</seq_no>
		<title><![CDATA[Webbed spaces]]></title>
		<subtitle><![CDATA[between exhibition and network (panel)]]></subtitle>
		<page_from>499</page_from>
		<page_to>500</page_to>
		<doi_number>10.1145/237170.237296</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237296</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Arts, fine and performing**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Internet</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P223391</person_id>
				<author_profile_id><![CDATA[81100260999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Perry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoberman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telepresence Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31041539</person_id>
				<author_profile_id><![CDATA[81100421713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Victoria]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vesna]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Santa Barbara]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Webbed Spaces: Between Exhibition and Network Organizers: Perry Hoberman, Telepresence Research Victoria 
Vesna, University of California at Santa Barbara Panelists: Ken Feingold, School of Visual Arts Stelarc, 
Artist, Sydney, Australia Lorne Falk, Consultant, Los Angeles Laura Kurgan, University of Pennsylvania 
 Artists, curators and theorists, each using the Internet in their own practice, will discuss works that 
emphasize public installation and multi-user approaches, using these works points of departure to begin 
a critical discussion of the Internet and its implications for artmaking, representation, and interactivity. 
Panel Overview The Internet (and in particular the World Wide Web) has seen unprecedented growth in the 
last several years. What started as a mode of communication linking remote sites has become a new public 
arena. As the Internet continues to mushroom, it is continuously accruing new technological capabilities, 
such as VRML, Java, graphical MOOs and MUDs, CU-SeeMe and the Mbone. As a result, a variety of cultural 
entities are beginning to gravitate towards it, including galleries, artists, and public institutions. 
Because the explosion of the Internet has been so sudden, and because its growth has been accompanied 
by a huge measure of real excitement as well as calculated hype, there has been little critical discussion 
of emerging cultural practices in this new realm. By bringing together artists, curators and theorists, 
each of whom is actively engaged with the Internet, Webbed Spaces will focus on the ramifications of 
the newly-expanded Internet for artmaking practices, addressing the following issues: How can two very 
different publics one physically present, the other tele-present be brought into relation to one another? 
What characteristics distinguish one public from the other? What kinds of interaction become possible 
between them?  Is the way the Internet serves to connect geographically distant individuals at the same 
time that it de-emphasizes physical contact an inevitable consequence of online communication? Does wired 
intimacy go hand in hand with wired isolation? What are the ramifications of being wired?  To what extent 
is the Internet creating new forms and forums for artmaking, and to what extent is it altering (or merely 
replacing) existing ones? Should we consider the Internet a new medium? If so, what are its specific 
characteristics? What is the critical language that can be used to represent it?  What are the implications 
of webbed and hybrid work for visual representation, experience, and interactivity?  How can the Internet 
be used to extend the fixed space of galleries and other exhibition venues?  The rhetoric of the Internet 
often makes the assumption that everything is instantly accessible everywhere to everyone. What are the 
ramifications of this assumption, especially for the incubation and dissemination of new work? And, despite 
this rhetoric, what things, places, and people remain excluded, and why?  Lorne Falk Brave New Audience 
The audience for this presentation involves real people living in telepresent neighborhoods, live buildings, 
and hot rooms. Wired and unwired people in digital niches a brave new audience. I want to map some of 
the attributes of this brave new audience in a way that subjectively maps the desires of the people who 
comprise it. A keyword is symbiosis. There is a new kind of symbiosis in the broadest terms, how real 
and artificial organisms live attached to one another (or one as a tenant of the other) and contribute 
to each other s support. As an attribute of the brave new audience, symbiosis doesn t negatively contrast 
different kinds of presence. It recognizes the interrelations between the physical and digital as something 
mutually beneficial and natural to do. This is both adventurous and consequential hence, brave. Physical 
and digital audiences are, for example, able to interact with one another as a consequence of the (creative) 
environments they find themselves in. They also eagerly interact with these environments. They have a 
powerful motivation: the concept of the original, which has dominated aesthetics for most of the twentieth 
century, is no longer relevant. Instead, the aspiration is to create (spaces) whose identities are infinitely 
malleable and fully shared with the audience. The brave new audience is encouraged to construct new rules 
for social conduct, revise concepts of social integration, and even imagine an aesthetics of community 
unlike what we re used to. In other words, there is an ethical dimension to their behavior. What happens 
when a creative space is unable to let go of the notion of the original enough to allow the audience 
to reform or transform, if not the whole space, at least some significant aspect of it? Ken Feingold 
Shared Virtual Environments as/and Art Spaces This presentation will discuss the evolution of Shared 
Virtual Environ­ments on the Internet as social environments, spaces for artmaking, and their intersection 
with actual shared environments. I will assert that it is not information which drives the social and 
aesthetic experience of using the Net, but rather communication with others, the ability to experience 
extended powers in the physical world, and a suspension of normal formations of identity; and that the 
underlying goal of many works created for the Net is not in their widespread distribution, but rather 
their ability to create linked spaces which are inhabited simultaneously by people in diverse physical 
locations, and their ability to bridge physical spaces and virtual spaces. I will discuss and develop 
the idea of such uses of the Net as liminal spaces, and discuss notions of personal agency, fantasy, 
and magic in these spaces, as grounding ideas upon which some recent works of art are being created. 
I will discuss the development from text-based MOOs and MUDs and early artists experiments in their uses 
as public performance spaces, to my recent experiments using the Mbone to create hybrid actual/virtual 
spaces in which remote participants meet as telerobotically controlled puppets. I will discuss the social 
and expressive limits of uninhabited spaces, such as publishing on the Web, CGI-driven interaction, and 
VRML 1.x, contrasting these with emerging forms such as VRML 2.0 and The Palace, in which virtual environments 
are inhabited simultaneously by communicating participants. Further, I will explore the possibility that 
art is fundamen­tally related to the human body and physical space, and discuss the relevance of this 
notion to the topic. Laura Kurgan You Are Here: The World Wide Web As it exists now, the Web gives 
us a lot to think about in the way of architecture s relation to interface, networks, and data flows. 
As an environment, the Web incorporates many and diverse spaces within its own very specific limitations. 
It can be used to illustrate examples of utopian spaces where people can exchange identities, morph their 
bodies, or form communities; or it can be used by local transportation authorities to upload information 
from traffic surveillance camera sites on freeways to visualize traffic flows, so that a commuter can 
see where the traffic jams are located. The interfaces are as diverse as the spaces implied by them. 
Whatever the scenario, the Web has become obsessed with mapping itself from corporate sponsors trying 
to get profiles and numbers of their users (and a site can capture quite a lot about the user instantly), 
in order to decide whether this is truly a profitable environment, to speculative mappers of self-organizing 
systems trying to visualize the constantly changing environments of the links in their home pages, to 
sites which try to locate the geographical position of their users on a map, to events which are designed 
to construct a map of those providing input to the site. What s interesting (precisely because it s so 
unsatisfying) about most of these representations is their imposition of conventional architec­tures, 
and the conventional language of maps, to orient and draw familiar pictures of such an unfamiliar space. 
Why does it make a difference at what longitude and latitude the server is located? Although they often 
produce interesting superimpositions of different representa­tional grids, these projects like so many 
of Web maps seem strangely unconscious of their own condition: the map of the Web is on the Web. Their 
limit is, simply, the inability to find an outside in Web space. In the end, no matter how totalizing 
the representation of the site wants to be, it s just another address on the Web. No one site dominates 
another... not just because the Web is decentralized, but because of the paradoxical spatial structure 
implied: inside and outside are no longer separated by anything like a solid boundary, or a wall. Stelarc 
Fractal Flesh Consider a body of FRACTAL FLESH, a body whose agency can be electronically extruded on 
the Net from one body to another body elsewhere. Not a kind of Cyber-Voodoo. Not of remote control, 
but of DISPLACING MOTIONS from one physical body to another physical body Net-connected. Such a body 
s awareness would neither be all­here nor all-there. Awareness and action would slide and shift between 
bodies. Agency could be shared in the one body or in a multiplicity of bodies in an ELECTRONIC SPACE 
OF DISTRIBUTED INTELLIGENCE... Imagine a body directly wired into the Net a body that moves not because 
of its internal stimulation, a body that moves not because of being remotely guided by another (or a 
cluster of remote agents), BUT A BODY THAT QUIVERS AND OSCILLATES TO THE EBB AND FLOW OF NET ACTIVITY. 
A body that manifests the statistical and collective data flow. A body whose proprioception responds 
not to its internal nervous system, but to the external stimulation of globally connected computer networks. 
THE INTERNET IS AWASH WITH OUTMODED METAPHYSI-CAL YEARNINGS AND FAR-FETCHED FANTASIES OF DISEMBODIMENT. 
THE NET IS NOT MIND TO MIND -RATHER, IT IS AT PRESENT MERELY A MODE OF TEXTUAL COMMUNI-CATION, A REDUCTIVE 
VISUALITY DEPRIVED OF THE COMPLEXITY OF PHYSICAL ACTION &#38; KINESTHETIC POSITION AND ORIENTATION. BODIES 
ELECTRONICALLY CONNECTED AND ACTUATED TRANSFORM THE NET FROM A MEANS OF INFORMATION TRANSMISSION TO A 
MODE OF TRANSDUCTION OF EFFECTING ACTION IN OTHER BODIES AND BITS OF BODIES IN OTHER PLACES. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237297</article_id>
		<sort_key>501</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>61</seq_no>
		<title><![CDATA[Advanced television for the United States]]></title>
		<subtitle><![CDATA[status and issues (panel)]]></subtitle>
		<page_from>501</page_from>
		<page_to>502</page_to>
		<doi_number>10.1145/237170.237297</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237297</url>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.1</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Internet</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003458</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing industry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011122.10003459</concept_id>
				<concept_desc>CCS->General and reference->Document types->Computing standards, RFCs and guidelines</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31074354</person_id>
				<author_profile_id><![CDATA[81100498693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Demos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DemoGraFX]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Advanced Television For The United States: Status and Issues Organizer: Gary Demos, DemoGraFX Panelists: 
Alvy Ray Smith, Microsoft Craig Birkmaier, Pcube Labs Mark Richer, Advanced Television Systems Committee 
Glenn Reitmeier, Sarnoff Labs The Federal Communications Commission is about to select a standard for 
Advanced Television for the United States to replace our existing NTSC television system. An advisory 
commission on advanced television service, ACATS, appointed by the FCC, is recommending that we deploy 
a new television system which includes interlace, 59.94 and 60 Hz, and non-square pixel spacing.These 
parameters are fundamentally incompatible with modern computer graphics displays, which do not use interlace 
and which operate at display rates exceeding 70 Hz. Those involved in developing the ACATS do not feel 
a need to restrict their proposed formats to those that are compatible with computer displays. They further 
are asserting that the cost and quality loss associated with converting interlaced 60 Hz video to 70+ 
Hz non­interlaced (progressive scan) pictures is acceptable. However, under their scenario, responsibility 
to de-interlace and frame rate convert these proposed formats for every computer display would fall upon 
the computer industry. Other problematic issues include interlace and 59.94/60Hz, lack of a robust data 
capacity, non-square pixel spacing in some formats, lack of defined overlay planes, a controversial 16:9 
image aspect ratio, overscan issues, and limited colorimetry. It is also proposed that receiving devices 
decode each of 18 different image formats, spanning a 6:1 range in resolution and data rate. ACATS proposes 
that format problems be solved by a later migration strategy away from admittedly obsolete techniques, 
such as interlaced scanning. However, no scenario is suggested for accomplishing this after deployment 
of a new television infrastructure, after which some it may be impossible to replace the interlaced portions 
of the system. The deployment of a new national television infrastructure is an opportunity to leave 
behind the obsolete NTSC television system. Since the primary distribution media for computer graphics 
are film and video, the introduction of new video formats will affect the work of digital production 
facilities. Gary Demos ACATS claims that their proposal is an appropriate compromise, containing some 
non-interlaced, and some interlaced formats. ACATS is aware that the computer industry needs display 
rates exceeding 70 Hz for large and bright screens containing computer information. However, they assert 
that interlace and 59.94 and 60 Hz are needed more than non-interlace 70+ Hz by the existing NTSC broadcast 
infrastructure and by existing television manufacturers. This panel offers an opportunity to challenge 
these assertions. Motion picture film runs at 24 frames per second. The display rate of 72 Hz is naturally 
suggested since 72 is three times 24. This would satisfy the need of computer displays to exceed 70 Hz. 
The frame rate of 36 would also form a new natural image motion rate for 72 Hz display. Image layering 
represents a powerful alternative to the ACATS proposal. ACATS proposes selection among numerous widely-differing 
formats. Layering provides a single layered format providing multiple layers of quality in resolution 
and frame rate within a common data format that would be used by all. The data layer within the ACATS 
proposal is also not a true layer, since its error rate is not sufficient to carry the multitude of data 
and code types that will be useful. ACATS proponents have asserted that 60 Hz and interlace are required 
because wide screen 72 Hz non-interlace images of a thousand lines cannot fit within the broadcast television 
channel s capacity of 19 mbits/second. ACATS has further asserted that resolution layering is not feasible. 
However, it has now been demonstrated that these assumptions are incorrect. Layered images running at 
72 frames per second have been demonstrated at 2k x 1k for the highest resolution layer, and 1k x 512 
for the base resolution layer, all fitting within 18.5 mbits/second. Movies at 24 frames per second can 
fit within even less data. The data layer could also be made to be sufficiently error-free to allow carriage 
of highly desirable multimedia code and data types. It is therefore unnecessary to continue to debate 
obsolete image format parameters. The ACATS proposal, which includes interlace, 59.94 and 60 Hz should 
be rejected, and should be replaced by a layered system operating at 72 Hz. No new interlaced formats 
should be deployed, since the obsolete interlace technique forms a fundamental barrier of computer display 
incompatibility. Adoption of the ACATS proposal would most likely prevent the realization of a National 
Information Infrastructure. Alvy Ray Smith What if the Internet, a well-known digital communications 
channel, had been standardized five years ago to carry only video data in a compressed form that subsequently 
became obsolete due to technologi­cal advances. This example is not unlike the standardization of the 
digital broadcast spectrum that recently was proposed quite seriously. It is simply a digitization of 
old analog thought, rather than an exploita­tion of new digital concepts. The largest possible view of 
broadcast television is as a collection of digital channels that can carry any kind of digital information, 
not just video. Any standardization that prohibits the full ramifications of this view will appear ridiculous 
in a very few years. The Internet is an example of a non-broadcast digital communica­tions medium that 
was standardized at a very fundamental level only. Atop this minimum protocol standardization, free enterprise 
is busily constructing numerous useful and exciting businesses. I believe the digital broadcast television 
spectrum should likewise be minimally standardized, and then allowed to develop with full digital cleverness. 
One goal is complete interoperability between broadcast TV and home computers. Digital technology is 
now sufficiently developed to actually implement the digital convergence between the two. There is no 
reason, other than insufficiently well thought-out infrastructure, for this not to happen now. Key to 
television and computer interoperability is data sharing, not merely display of television images on 
a computer screen. Of the several strictly technical issues concerning the use of the digital spectrum 
for video data, the most important is this: Interlaced scanning should no longer be supported. If video 
were being created today, interlace would not be suggested as a new standard. There are far superior 
ways available to utilize the same bandwidth. We fully expect there to be even better ways in the future. 
Furthermore, a frame rate of at least 70 Hz should be supported, non-square pixel spacing should be disallowed, 
an aspect ratio amenable to film should be used, and a true family of resolutions should be defined. 
All of these issues, however, pale in comparison to the overriding importance of the definition of a 
digital broadcast data transmission standard. Craig Birkmaier A properly conceived digital television 
system for local, regional, national, and international distribution of digital media will provide a 
solid foundation for the distribution of all forms of digital media. The digital television system proposed 
by the Grand Alliance and the Advanced Television Systems Committee (ATSC) was conceived as a higher 
resolution clone of our existing television system. It carries significant excess baggage for compatibility 
with existing broadcast practices. This shortsighted approach will limit the opportunity for broadcasters 
to compete with every other infrastructure provider and programming service. Nearly a year ago, I submitted 
the following observations to the ATSC. The Challenge 1. To develop enabling standards for a digital 
television system interoperable with all digital communications infrastructures, backward compatibility 
with existing analog and digital video program archives, and a migration strategy which allows digital 
television to be delivered using existing NTSC and ITU-Rec-601 acquisition and receiver infrastructures. 
 2. To stimulate a rapid migration to artifact-free high-resolution video acquisition and display systems, 
and rapid voluntary replacement of analog NTSC receivers.  The Opportunity 1. To influence and manage 
the evolution of digital communications systems.  2. To provide a framework within which all effected 
industries and stakeholders can participate in rapidly developing digital communi­cation system standards 
and extending them in the future. 3. To ensure that these standards be developed by industry-led initiatives, 
rather than by government-imposed standards. 4. To ensure the ability to interoperate and rapidly evolve 
with underlying technology.  The Solution 1. Develop enabling standards for digital television based 
on a layered, open architecture, which will provide artifact free imagery at multiple quality of service 
(QOS) levels, to interoperable information appliances. 2. Establish minimum and maximum performance 
limits for each QOS level rather than rigid point standards. 3. Establish a modular framework. 4. Allow 
the marketplace to drive the evolution process. 5. The process of maintaining and extending DTV standards 
and recommended practices should rest with national and international standards organizations.   Mark 
Richer The ATSC Digital Television Standard is the result of an eight year long open process. Hundreds 
of people contributed thousands of hours of effort to create a digital television standard for the United 
States. The result of this process is a system that utilizes a layered architecture and is compliant 
with the MPEG-2 international standard for video compression and transport. The ATSC standard provides 
a flexible system that offers the user a variety of options. The system includes multiple options for 
video input and compression. The two HDTV formats (1920 x 1080 and 1280 x 720) result in square pixels. 
The system supports use of both progressive and interlaced scan. The issue of interoperability is one 
that has been central in the choice of ATV system parameters. The goal was to provide interoperability 
with a variety of media including existing NTSC program material and consumer receivers, film, existing 
HDTV production formats, and computers. There is no consensus on a single approach to achieve interoperability 
across all media. However, the inherent flexibility of the technology provides options to both service 
providers and consumers: The choice of progressive or interlace scan source material will be made by 
the program producer.  The choice of progressive or interlace for transmission will be made by the program 
service provider.  The choice of progressive or interlace scan display is an independent choice that 
will be made by the consumer.  The ATSC digital television standard is the only existing standard in 
the world that offers the option of progressive scan and square pixels for both standard definition and 
high definition television. On the other hand, the DBS and cable industries are in the midst of rapidly 
deploying digital systems that do not incorporate progressive scan and square pixels. For this reason, 
the ATSC Digital Television Standard should be adopted immediately.  Glenn Reitmeier The adoption of 
the Advanced Television Systems Committee s Digital Television Standard and the recommendation of the 
Advisory Committee on Advanced Television Service that the FCC approve it as a national standard for 
terrestrial HDTV broadcasting are landmark developments in the convergence of computing and television. 
The ATSC Advanced Television (ATV) standard is the most flexible, most computer friendly, most broadly 
interoperable television system ever developed. It provides powerful interoperability by using: A layered 
digital system architecture that conforms to interna­tional data communications models  Header/descriptors 
that allow a flexible system today and extensibility for future improvements  Multiple video formats 
and frame rates with a heavy emphasis on progressive scan and square pixel formats that facilitate easy 
computer interoperability  MPEG-2 video compression that conforms to draft international standards, 
and that will likely form the basis for most computer multimedia use of motion video  MPEG-2 transport 
(packet) format that meets the needs of broadcasting while being designed to be easily interoperable 
with ATM networks.  In developing the technical basis for the ATSC standard, the Grand Alliance carefully 
balanced diverse and often conflicting needs from film and television post-production (including computer 
generated images), broadcasting, cable television, consumer electronics, computing and telecommunications 
industries. Of course, interoperability must be balanced against other design goals such as HDTV picture 
quality, restricted-power simulcasting and low cost. Because no rigid single approach could simultaneously 
meed such a broad spectrum of needs, the result is a standard that is flexible and inclusive. The ATV 
standard recommended to the FCC represents the consensus of over 100 companies that participated in the 
open ACATS process over an eight-year period. Delay in idealistic pursuit of perfection by an interest 
group can only result in the destruction of consensus and a giant step backwards for television and computer 
interoperability. After huge investments, failure to promptly commer­cialize ATV technology will result 
in the only option being a European or Japanese system - one that has ONLY interlaced scanning and non­square 
pixels. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237298</article_id>
		<sort_key>503</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>62</seq_no>
		<title><![CDATA[The soul of the machine]]></title>
		<subtitle><![CDATA[the search for spirituality in cyberspace (panel)]]></subtitle>
		<page_from>503</page_from>
		<page_to>504</page_to>
		<doi_number>10.1145/237170.237298</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237298</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Internet</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>K.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003462</concept_id>
				<concept_desc>CCS->Social and professional topics->Computing / technology policy</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP35045852</person_id>
				<author_profile_id><![CDATA[81332520442]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Celia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pearce]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P78737</person_id>
				<author_profile_id><![CDATA[81100601484]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Soul of the Machine: The Search for Spirituality in Cyberspace Organizers: Celia Pearce Erik Davis, 
Author/Journalist Panelists: Mark Pesce, VRML Guru/Author Paul Godwin, Composer/Musician Char Davies, 
Artist Rita Addison, Artist If the work of the city is the remaking or translating of man into a more 
suitable form than his nomadic ancestors achieved, then might not our current translation of our entire 
lives into the spiritual form of information seem to make of the entire globe, and of the human family, 
a single con­ sciousness? Marshall McLuhan, Understanding Media In 1964, Marshall McLuhan first identified 
the rise of the information age and introduced the concept of The Global Village, an electronic universe 
of information, ideas and images that could be shared by all. Although his ideas were precipitated by 
television, the telephone, and the embryonic computer, McLuhan anticipated that all of this would grow 
together into a new form of collective consciousness. Today, McLuhan s prophesy is being realized beyond 
even his wildest imaginings. The astronomical rise of the Internet as a mass medium, continuing advances 
in immersive virtual reality, and the dramatic shift of media from analog to digital are all leading 
us to a new level of consciousness, understanding and connection. From Virtual Reality, to electronic 
music, to the Internet, The Soul in the Machine: The Search for Spirituality in Cyberspace, brings together 
a group of esteemed artists, producers, inventors, and authors who are actively investigating, exploring 
and creating spiritual experiences in the digisphere. Celia Pearce Perspective Celia Pearce s work as 
both a producer and writer/lecturer has been dedicated to creating and supporting alternatives to the 
highly visceral content which has dominated commercial interactive multimedia. She is very concerned 
with the psychological, sociological, and cultural implications of interactive media, and as such has 
written, taught, and lectured on such topics and interactive art, nonlinear storytelling, alternative 
VR, and fundamentals of game structure. Her concerns about the human side of technological content led 
to the idea of proposing a panel on the topic of spirituality in cyberspace. As panel organizer, Celia 
s emphasis will be to provide an irreverent, entertain­ing and informed philosophical overview of the 
subject matter. Position Quoting from Marshall McLuhan, Aldus Huxley, and others, I want to introduce 
the topic in a lively way. My vision of the panel is that it will be more about questions than answers, 
more about waking people up to issues than giving them information. I want to engage people and challenge 
them to address issues which might otherwise be taboo. To many, any discussion of spirituality seems 
antithetical to the computer. But nothing could be further from the truth. As people begin to use cyberspace 
as a gathering place on a mass level, the result is everything from on-line romance to terrorism. The 
inevitable digitization of everything has profound implications on a spiritual level. Aldus Huxley defines 
The Perennial Philosophy as the divine unity of all things. By converting all media into a single form, 
we have created a new paradigm for such unity. It is also important to me that the panel not limit itself 
to pundits pontificating on the possibilities, but that it represent creative individuals who are making 
real work that addresses some of these issues. This, combined with my own philosophical overview and 
Erik Davis closing historical and cultural views, will allow us to deal with this issues at a variety 
of levels of resolution. Mark Pesce Perspective The development of VRML, the authoring language for 
creating 3D environments on the Internet, was one of the lead stories in computer technology of 1995. 
Mark Pesce s contribution to making on-line cyberspace a reality at the exact moment when the Internet 
has become a mass medium is destined to revolutionize the world. This is clearly Mark s intention. Mark, 
who was featured on the cover of Wired magazine for its Techno-Pagans issue, makes no bones about his 
commitment to the spiritual applications of computer technology. He will provide his unique perspective 
on the transcendent qualities of cyberspace as a means to self-awareness through collective conscious­ness. 
 Position Cyberspace is the realm where communication occurs, at the boundary between self and other. 
In a world which is entirely self-created with the single exception of this boundary between the self 
and the other, the other assumes the role of the agent of divine novelty, preserving us from the boredom 
of our own reflection, and acting as the agent of initiation, presenting us with that which removes us 
from ourselves and places us into a new understanding. Reading from sections of my soon­to-be published 
Self/Sacred: Body and Being in Cyberspace, I will discuss the nature of the other, the divine, and the 
essence of communi­cation as the encounter with the sacred self. Paul Godwin Perspective As a composer 
and musician, Paul has a great deal of experience in musical collaboration, both in composition and performance. 
His interest in improvisation has led him to create a number of collaborate virtual music spaces that 
allow people to participate in virtual jam sessions. Paul will discuss the process of simulating live 
jams in cyberspace, and the idea of creating a collective consciousness through real-time, remote collaboration. 
 Position I believe that Cyberspace is a vessel for the transmission of human consciousness. The species 
is literally investing itself into the digital noosphere. The World Wide Web parallels the real world 
in many ways through commerce, education, entertainment, information, and social interaction. But where 
does the spiritual aspect of human life fall in the entire spectrum of digital activity? How do we feel 
each other s presence in a consensual, digital world? How do we harness the psychic biological energy 
fields that resonate in a brainstorming or jam session? To address these questions, I will present some 
of my work in this area, including: The Worldsong Project, created in collaboration with Mark Pesce and 
Dr. Bill Martens, is a distributed network project using VRML to navigate a stratified matrix of the 
world s audio material. Presented at The Doors of Perception, Amsterdam. Ritual Ground Zero, YORB-TV, 
(Time-Warner Cable TV, NYC) developed with interactive designer Amee Evans, is a live jamspace on Interactive 
Television is controllable by four separate callers, who trigger Gamelan music samples using their telephone 
s touch-tone buttons. The physical jamspace was rendered using SGI Alias and is in full operation each 
Thursday night on NYC Cable Television.  SOLACE, or Sacred On-Line Active Communal Environment, is a 
generic term for a type of space that can be shared spiritually on the Net. One example is an on-line 
Zendo or vocal meditation space. Here, participants sing or tone into microphones on remote workstations; 
they hear the other members of their group singing along; as their voices reach unison, the visual interface 
reflects that unity (a 1-5 point visual mandala generator is employed) and a space for attunement is 
created.  Char Davies  Perspective Char Davies seminal work OSMOSE has been described as a digital 
meditation space. In contrast with the other projects, all of which emphasize communal consciousness 
in remote or shared physical spaces, Char s work is based on full-body solitary immersion. OSMOSE is 
a place for quiet self-discovery and reflection. The visual aesthetic and sounds of OSMOSE are evocative 
and ambiguous: the user interface is based on intuitive biofeedback of breathing and balance. Together 
these tend to create a euphoric sense of being unbounded, while simultaneously grounding the experience 
in the body-core. Quite often, immersion in OSMOSE appears to induce a shift of awareness in participants, 
in which conceptual boundaries between inner/outer, self/world dissolve, and desire for control and speed 
is replaced by a serene and contemplative free-fall. Drawing on insights gained from her experience with 
OSMOSE, Char will discuss the potential of immersive virtual space as a medium for exploring the perception 
of consciousness. Position My work in cyberspace is focused on immersive virtual space as a spatial-temporal 
arena for exploring being-in-the-world. I am somewhat wary of the trend towards collectivity and mind 
in cyberspace: such a realm, in which the subjective body is denied, may in fact prove to be the epitome 
of Cartesian desire, symptomatic of an almost pathological denial of our mortality and our materiality, 
seducing us to turn away even further from the earthly environment in which we, as incarnate beings, 
are embedded. My goal is to use the technology to suggest an alternative, re­affirming the role of the 
body in cyberspace, approaching this space as a medium for stripping away habitual assumptions and re-configuring 
how we experience ourselves as embodied consciousness enveloped by the world. Forty years ago, the philosopher 
Gaston Bachelard wrote in La Poetique de l espace ...by changing space, by leaving the space of one 
s usual sensibilities, one enters into communication with a space that is psychically innovating. For 
we do not change place, we change our Nature. OSMOSE is such a space. Rita Addison Perspective Rita 
Addison s work is profound on several levels. In her landmark work DETOUR: Brain Deconstruction Ahead, 
Rita created an immersive experience simulating the perceptual damage that she underwent as a result 
of her 1992 automobile accident. This type of experience cannot be effectively expressed in any other 
medium than immersive Virtual Reality. The emotional impact of this is immense; one can literally live 
in another s mind and see the world from their perspective. Unlike most VR, which emphasizes imagination 
and fantasy, Rita s work is concerned with simulating real shifts in perception as a mode of creative 
expression. Though highly personal, Rita s work is also collaborative in nature, putting people together 
in a shared space to experience these perceptual shifts as a group. Synes­thesia has the feeling of an 
audio-visual jam session in which participants combined biofeedback triggers movement in the environ­ment. 
Rita s work is also a testimony to her courage in taking a tragic experience and transforming it into 
an expression of understanding and enlightenment to be shared with others. As such, her personal journey 
is the ultimate expression of the resilience of the human spirit. Position: Authoring Your Own Virtual 
Environment Journey I m keenly interested in discovering new interfaces that will enable us all to more 
freely create and explore virtual environments. One of the paths I m exploring is the potential for biosignals 
to function as not only a joystick, i.e., as a control, but also as a bridge over which emerging, real-time 
neurophysiological data can travel to a computer which serves much as a launching pad and returns the 
data to the observer in a different way than which it was first perceived. I certainly do not have any 
answers. Rather, I will share what I ve learned so far on my exploration with bio-sensing interfaces, 
including my recent experience producing and showing Synesthesia at SuperComputing 95. Erik Davis Perspective 
Erik Davis is an author and journalist who has devoted his work to the study of digital culture. He has 
emerged as one of the leaders in discussions of techno-spirituality and has written on this topic for 
both the technological and theological communities. Erik s background and knowledge of religious traditions 
and his diligent research into history and trends in digital culture bring a broad scholarly perspective 
to the subject matter. Erik s presentation will also include a glimpse into the darker side of cyberspace. 
Just as the opening statement provides an overview to set the stage for work-specific presentations, 
Erik s closing remarks will bring the topic back into the larger perspective, touching on the past, present, 
and future of techno-spirituality. Position In my brief concluding talk, I will put the other panelists 
discussions about spirituality in cyberspace into a larger cultural context, focusing in particular on 
the role of the religious or spiritual imagination in navigating and responding to the dizzying possibilities 
and dangers of the telecommunications revolution. I will discuss a few concrete examples of the crossover 
of spirituality and high technology, and explore what these visions tell us about the cultural and cognitive 
potentials of computers in general, and the Internet in particular. But while I am encouraged by the 
magical and utopian strains of thought that have accompanied the massive spread of digital technology 
into our lives, I also believe we have as much to gain by looking at the dark and paranoid mythologies 
that have also grown up around the new machines. In our faith in virtual light, we believe our monitors 
to be windows onto a new world, but we should also consider that we are still seeing through a glass, 
darkly.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237299</article_id>
		<sort_key>505</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>63</seq_no>
		<title><![CDATA[Issues in networking for entertainment, graphics, and data (panel)]]></title>
		<page_from>505</page_from>
		<doi_number>10.1145/237170.237299</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237299</url>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Computer conferencing, teleconferencing, and videoconferencing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286.10003291</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools->Web conferencing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31072674</person_id>
				<author_profile_id><![CDATA[81100542703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FORE Systems, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Issues in Networking for Entertainment, Graphics, and Data Organizer: Marke Clinger, FORE Systems, Inc. 
Panelists: Robert Amen, Cinesite, Inc. Ray Feeny, RFX, Inc. Chuck Garsha, Paramount Studios Jim McCabe, 
Full Spectrum Networks Mark Valenti, Sextant Group This panel will not only discuss the state-of-the-art 
in networking graphics, video, audio, voice, and data, but will use the latest technolo­gies to create 
a virtual panel. An audience at MIT in Boston, MA will be linked via a country wide Asynchronous Transfer 
Mode (ATM) network to the New Orleans Convention Center to participate virtually in this panel. Two way 
video and graphics will be trans­ported over the ATM network to allow for full participation by the remote 
audience. That same ATM network will extend to Paramount Studios in Los Angeles, CA to allow a panelist 
to participate virtually. This panelist will deliver his presentation from LA using an integrated platform 
to transport high quality graphics over the same infrastructure as the two-way full motion video. During 
the panel the video-to-ATM equipment will be changed, providing the audience and panelists an opportunity 
to experience the different technologies. The panelists are from different parts of the industry, but 
all are using advanced networking techniques and technologies to build the networks that transport multimedia. 
In addition to delivery of traditional data sets, these networks are built to deliver video, audio, and 
massive graphics files in real time. One of the networks that will be described was built to allow several 
auditoriums full of students to observe brain surgery and interact with the surgeon during the surgery. 
It now delivers traditional data and voice services too. Another network is changing the way a user will 
look at a network connection. When a studio space is rented to do filming, the production company will 
rent a network connection providing them connectivity and software services they need while on-site. 
The computers used can either be rented along with the network, or the customers can bring their own. 
The latest video-to-ATM devices will be installed at the three locations. Equipment from several different 
vendors will be used to give the participants an idea of the quality and capabilities of different types 
of products. All equipment will support 30 frames per second video transported over ATM. Some of the 
equipment will also support integrated graphics and video support. Panelists will present one of their 
projects that combines state-of-the­art networking with one or more of the following technologies: Computer 
Graphics  Video  Audio  Voice (as in telephony)  The presentations will consist of an overview of 
the business drivers behind the project, the technical details, and the issues that arose during or after 
the implementation. At SIGGRAPH 95, Eastern Carolina University used GraphicsNet 95 s country wide ATM 
network to deliver two video streams and multiple data channels from their university to the LA Convention 
Center. Using the high quality video link, conference attendees were able to consult with doctors and 
specialists in North Carolina on health issues. A nurse at the convention center used stethoscopes and 
thermometers that were attached to the data network so that the remote doctor could view the results. 
With the high quality video connection and the data flow, a doctor could diagnose a patent remotely; 
however, these doctors are not licensed in California, therefore they could only offer advice. This is 
a good example of where technology has surpassed our laws that govern the way society interacts. What 
other issues do people deploying advanced networking technologies run into? In local area networking, 
what are the concerns of the networking specialist? How well are non-technology savvy users adapting 
to the technology? Are the goals behind the deployment of the technology being realized? What type of 
functionality is missing? How easy is it to integrate graphics, voice, video, etc. into a network? What 
issues arose when doing this integration? In local area networking bandwidth is considered free. The 
cost to run a fiber optic cable through the building is a relatively cheap one­time charge, so users 
tend to waste bandwidth. In the wide area network, bandwidth must be purchased, and it is very expensive. 
New technologies on the market are bringing down the cost to the service providers of this bandwidth. 
Is this bringing a reduction in the price to the user? What are the issues that need to be considered 
when connecting to a phone company? What technologies have the panelists employed? Which were successful? 
What problems did they run into with the different technologies? During the course of the panelist presentations 
and the follow-on interactive discussion, these questions and issues will be explored. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237300</article_id>
		<sort_key>506</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>64</seq_no>
		<title><![CDATA[Graphics PCs will put workstation graphics in the Smithsonian (panel)]]></title>
		<page_from>506</page_from>
		<doi_number>10.1145/237170.237300</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237300</url>
		<categories>
			<primary_category>
				<cat_node>K.8.1</cat_node>
				<descriptor>Graphics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003145.10003147</concept_id>
				<concept_desc>CCS->Human-centered computing->Visualization->Visualization application domains</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Economics</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P259285</person_id>
				<author_profile_id><![CDATA[81545104956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Samuel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uselton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MRJ, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphics PCs will put Workstation Graphics in the Smithsonian Organizer: Sam Uselton, MRJ, Inc. Panelists: 
Michael Cox, S3, Inc. Michael Deering, Sun Microsystems Jay Torborg, Microsoft Kurt Akeley, Silicon Graphics 
 Graphics accelerators for Personal Computers are becoming more powerful and cheaper very rapidly. The 
panelists will argue whether this development spells the end of graphics workstations as we have known 
them. Calligraphic displays were supplanted by raster frame buffers; workstations with internal graphics 
replaced minicomputers with attached frame buffers. Has the next transition arrived? What are the implications? 
Should we fight the tide or hail the conquerors? Are there other options? Note: The esteemed panelists 
have been promised immunity from prosecution and/or persecution based on any statements, positions, posing, 
or posturing that may occur in the course of this debate, to free them from any concerns of propriety, 
modesty, self-incrimination, and image maintenance. Their employers have been promised plausible deniability. 
First affirmative: Michael Cox PC graphics will drive workstations (WS) into Chapter 11. At the top of 
the market pyramid are the few users who need astronomical performance at astronomical prices. At the 
bottom are the PC masses. PCs will ultimately take the whole pyramid because of unyielding forces: business 
model, technical requirements, and volume. 1. The WS business model makes workstation graphics conservative 
by necessity. The PC business model requires innovation. PC graphics will get better and faster, FASTER 
than WS graphics. WS advances are limited by silicon advances (more gates, more possible). WS customers 
and ISVs are conservative. Legacy applications drive WS graphics. WS product cycles are 2.5 years. SW 
and HW development are tightly coupled. Innovation is THE source of value-added in PC graphics; advances 
will come both with silicon and with changes in the fundamental algorithms and HW/SW partitioning. There 
are no legacy applications, and interfaces on PCs are fluid anyway. PC product cycles are 1 year. SW 
and HW development are independent. 2. Technical requirements in the PC market are richer, and will 
drive solutions to be better and faster. PC graphics are today playback graphics, requiring guaranteed 
real-time rates; WS are authoring, requiring only interactive rates. As PCs match WS quality, frame rates 
will be higher. Playback requires much more integrated MEDIA support than authoring. New, richer applications 
will happen first on PCs. Price insensitivity has driven WS graphics down an evolutionary dead-end (e.g. 
2 Gbytes of redundant texture memory, with no solution for the texture download bottleneck); PC price 
sensitivity will drive superior technical solutions (e.g. UMA instead). 3. More PC volume means more 
opportunity, more start-ups, more projects, more people, more innovation, greater investment in technology 
development, better technology. Expect a brain drain (who s at Microsoft?). More PC volume also means 
lower part cost. Even with the same algorithms, PCs win.  New technologies develop in three phases: 
games, retrofit, innovation. First, toys. Second, the new technology replaces the old. Third, new applications 
are invented. PCs obviously will dominate the toys. Over time PCs will replace WSs. Where will new applications 
be developed? First (and perhaps only) on PCs, because of market opportunity, richer media platform, 
and because that s where most people will be working! WSs will keep the top of the pyramid for a while. 
Once, big iron ruled. As the smaller, more numerous, more nimble workstations evolved, big iron disappeared. 
Sure, there are today still die-hard applications that need big iron. Cray, CDC, and Thinking Machines 
are still in business, but would you want to be one of these companies? First negative: Michael Deering 
3D graphics hardware for PCs have finally found their killer app: home games. Such entertainment cards 
have become successful by no longer trying to compete with workstation 3D graphics hardware for industrial 
applications. In multi­billion dollar aerospace and automotive design efforts, time to market is more 
important than cutting a few percent off the overall design budget. Modern 3D graphics workstations are 
just that: well integrated computing and rendering engines designed from scratch as a single system, 
both hardware and software. Such systems now, and for the foreseeable future, deliver system graphical 
throughput well above anything available on commodity PC platforms. 3D graphics workstations also pay 
attention to quality: high resolution, crack free surface rendering, stable numeric algorithms, high 
quality anti-aliased lines, and adherence to standards also differentiate most 3D workstations from most 
3D PC graphics. Indeed, the most successful PC vendors are busily dumping their half­hearted attempts 
to achieve such workstation quality in an effort to shave a little cost off to be more price competitive 
in the cutthroat 3D home game market, where such features are not yet needed. Second affirmative: Jay 
Torborg Personal computers have rapidly become a ubiquitous tool for a broad range of applications ranging 
from games to financial modeling to video servers to computer aided design. Applications that were previously 
the domain of expensive specialty computer systems are migrating to the PC every day. The significant 
investment this ubiquity affords has allowed the PC to rapidly close the gap in performance and provide 
dramatic advantages in price-performance, relative to workstations and other specialized systems. One 
significant area near and dear to our hearts in which the PC has yet to catch up is 3D graphics at least 
as far as functionality and performance are concerned. There are actually more 3D applications (measured 
in units) running on PCs now than on the entire installed base of workstations; most of these happen 
to be games. But the PC is catching up quickly. This significant market is supporting significant investment 
in this area. In the next few years, tens of millions of HW 3D graphics accelerators will be sold for 
the PC, many of them far outstripping the 3D performance of a typical workstation today. The PC has several 
key advantages over workstations that will allow it to dominate in this area. The CAD market, which has 
driven the 3D workstation architectures we know today, has very different 3D graphics requirements than 
the immersive animate environments I expect will dominate PC applications. Since the PC does not have 
the huge 3D graphics legacy carried by the workstation, we can afford to explore new architectural approaches 
that leverage current technology to create 3D systems that deliver significant price perfor­mance advantages 
for interactive animate environments. The huge volume potential in the PC market allows aggressive use 
of advanced semiconductor processes, an alternative that is not economically feasible for low-volume 
workstations. And the huge base of application developers will insure the availability of compelling 
content. These key advantages, which are unique to the PC platform, will allow the PC to rapidly become 
the 3D graphics platform of choice. Second negative: Kurt Akeley 3D graphics are increasingly central 
to interactive computing. Soon all interactive systems will include hardware support for 3D graphics, 
and will be optimized for interactive 3D rendering. When all PCs and workstations support 3D graphics, 
it will be true that most graphics applications are PC based, and that PCs command most of the market 
share for 3D graphics. That this will be true, however, no more implies that PCs will be the 3D graphics 
platform of choice than the current situation, in which PCs dominate the general-purpose computation 
market, implies that PCs are the platform of choice for computing. There will always be demand for systems 
that are faster and more capable than commodity machines, and this market demand will continue to be 
met by computer vendors. PC graphics will flourish, and workstation graphics will continue to lead the 
way. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237301</article_id>
		<sort_key>507</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>65</seq_no>
		<title><![CDATA[Cognition, perception, and experience in the virtual environment]]></title>
		<subtitle><![CDATA[do you see what I see? (panel)]]></subtitle>
		<page_from>507</page_from>
		<page_to>508</page_to>
		<doi_number>10.1145/237170.237301</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237301</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14023307</person_id>
				<author_profile_id><![CDATA[81100030453]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Linda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jacobson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http ://www.sgi.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cognition, Perception and Experience in the Virtual Environment: Do You See What I See? Organizer: Linda 
Jacobson, Silicon Graphics, Inc. Panelists: Charlotte Davies, SoftImage Brenda Laurel, Interval Research 
Dr. Creve Maples, Muse Technologies Mark D. Pesce, Big Book Dr. Mark Schlager, SRI International Rob 
Tow, Interval Research Description Virtual reality is real. We will see high-resolution, low-lag systems 
doing serious applications within three years, predicted Frederick Brooks, Jr. at SIGGRAPH 94. And indeed 
we have. We now possess the algorithms, architecture and hardware. We know the techniques: we understand 
how to define geometries and assign object behaviors, and we map textures, incorporate collision detection 
and implement level-of-detail switching. We adopt psychological depth cues from the field of 3D design, 
implementing linear perspective, motion parallax and occlusion to impart the illusion of three-dimen­sional 
space. In giving virtual objects their attributes, we consider the participant s relationship to those 
objects. Nonetheless, as various SIGGRAPH 95 panelists pointed out, Little is understood about how to 
usefully interact in three dimensions in ways that really help perform tasks and There has been a surprising 
lack of real-world applications in the virtual world...We are unfamiliar with this new medium, unable 
to utilize its power and to compensate for its limitations. That is because the primary difficulties 
facing application developers are not technological, but conceptual.We know the language but we don t 
know the grammar. We haven t defined the stylistic guidelines. We know how to apply real-time, immersive, 
interactive techniques using virtual reality technologies, but we don t take a uniform approach to applying 
our knowledge of how these inputs affect us, and as a result have not developed a consistent UI. Through 
virtual reality, we move beyond cognitive computing into the realm of experiential computing. To build 
useful applications, we must understand how and why experiential computing is fundamentally different 
from cognitive computing, and how it lets us tap into human capabilities in a way no interface and no 
form of computer graphics has done before. This is especially important when we consider that today s 
successful commercial virtual reality applications are used for training in cognitive tasks and design 
of products we use everyday. Previous SIGGRAPH panels wondered how to work effectively within immersive 
environ­ments and explored the state of aesthetics in the virtual environment. If we want to discover 
how to work effectively and implement aesthetics within immersive environments, we must discover how 
we learn in those environments and establish a set of guidelines outlining our discoveries. We must bring 
a greater degree of involvement of cognitive psychology and human-perception experts into the mix of 
those developing VR technologies and applications.To move into experiential computing, we must be able 
to communicate how and where humans operate cognitively, perceptually, and experientially. We must create 
the cyberspatial equivalent of Strunk &#38; White s Elements of Style. Some issues to be aired during 
this panel include: how humans perceive information visually, auditorially and proprioceptively; how 
the mind works when seeking known or unknown information; imagery and associative memory techniques and 
how they relate to virtual environments; perceptual and cognitive constancy when updating displays. This 
panel aims to launch discussion in the SIGGRAPH community of how we see in cyberspace...and to integrate 
into our understanding of virtual reality s potential these facts registered by Diane Ackerman in her 
Natural History of the Senses: The body edits and prunes experience before sending it to the brain for 
contemplation or action. Not every vagary of sunlight registers on the retina. Not everything we feel 
is felt powerfully enough to send a message to the brain...[m]uch is lost in translation, or is censored, 
and in any case our nerves don t fire all at once. Some of them remain silent, while others respond. 
Our senses also crave novelty. Any change alerts them, and they send a signal to the brain. If there 
s no change, no novelty, they doze and register little or nothing....A constant state even of excitement 
in time becomes tedious, fades into the background, because our senses have evolved to report changes, 
what s new, something startling that has to be appraised, a morsel to eat, a sudden danger....The body 
s quest isn t for truth, it s for survival!  Panel Format This is an issue panel, in which each panelist 
will provide a brief statement describing his or her perspective of cognition/perception/ experience 
relative to virtual reality, followed by a lively discussion in which it is expected that the panelists 
will in some cases disagree with each other, and in others, elaborate upon each other s statements. It 
is the goal of the panel organizer to provide a summation of the points made through the Silicon Graphics 
Web site at http://www.sgi.com/.  Charlotte Davies Conventional design for virtual reality tends to 
reflect our cultural world-view, resulting in virtual environments filled with static, solid, hard-edged 
objects in empty space. Similarly, most user interaction tends to be based on metaphors for manipulating 
machines. Such approaches, ignoring how we subjectively experience being-in-the­world, limit the expressive 
potential of the medium. There are, however, alternatives. OSMOSE, the immersive virtual environment 
that I created with my team at Softimage last year, is an example: photo-realism, linear perspective 
and Cartesian notions of space have been abandoned for an aesthetic based on transparency, tonal subtlety 
and spatial ambiguity, for the goal is to evoke multiple meaningful associations rather than merely illustrate. 
The sound in OSMOSE also seeks to achieve this, further emotionally involving the participant within 
the space. Directly manipulative interface methods, such as joystick or glove, have been replaced by 
bodily processes of breathing and balance. Whereas the former methods tend to reduce participation to 
disembodied eye and controlling hand, these far more intuitive techniques ground the immersive experience 
in the participant s own body, creating a calming and centering effect and leading to heightened awareness 
and receptiv­ity. Several thousand people have been immersed in OSMOSE in the past year. Their responses 
have confirmed the author s belief that immersive virtual space, when approached with a certain sensibility, 
has intriguing potential as an educational medium. Brenda Laurel Style is a very sticky wicket. The 
purpose that a work is intended to serve, its form or genre, the materials it encompasses, and the maker 
s skills, beliefs, and values all influence style strongly. Although Aristotle did not discuss style 
directly in the Poetics, some twentieth-century structuralists would say that style is the point of articulation 
between the conceptual elements of a form and the sensible aspects of its unfolding. This provides a 
neat analytical springboard for examining the root assumptions about the powers of the medium and its 
fundamen­tal forms. From there we should be able to see ideas embedded in the medium that illuminate 
the relationships among senses, emotions, thought and reason, and action in virtual environments. These 
relationships, variously worked out, are the elements of style. Dr. Creve Maples Richard Hamming said, 
The purpose of computing is insight, not numbers. Today, as the Information Age moves forward, computers 
touch all aspects of our lives. Far from achieving insight, however, there is often a feeling that we 
are slowly sinking under the enormous volumes of data. Helping people to explore, question, and understand 
complex information is an important criterion for future computational environments. Highly interactive 
human-computer environments can allow practical solutions to some problems far more rapidly than either 
human or computer operating independently. User interaction with synthetic, immersive environments presents 
an exciting and relatively uncharted area. The human mind is capable of absorbing and processing large 
volumes of information. Most of this processing, however, occurs at a precognitive level, the results 
of which serve to alert the cognitive mind to areas of potential interest. Toward that end, and in concert 
with the idea of a humanistically organized software environment, five areas of human-computer interaction 
have been defined and investigated: Exploration, Navigation, Presentation, Interaction, and Examination. 
These five areas serve to define a functional arena for interactive environments and provide the foundation 
for device and model independent tools. Mark D. Pesce We re all taught how to read and write how many 
of us are taught to sculpt, or to design a building? Although we all experience the immediacy of space, 
we re not trained in the canons of architecture. How then, can we expect to develop any interface of 
immediacy, when we re functionally ignorant of the processes which underlie this experience? The ideal 
 an interface which disappears into a virtual reality has seen its realization in only a few works, 
such as PLACEHOLDER (by Brenda Laurel, Rachel Strickland and Rob Tow) and OSMOSE (by Char Davies), in 
part because researchers are unable to leave their own thought processes behind and design transparently. 
In looking to the real, we find a design guide for the virtual; in looking inside ourselves, we find 
ontology as interface, being as doing. Dr. Mark Schlager Virtual environment technology allows developers 
great freedom in designing spaces, objects, and systems and the perceptual mechanisms through which users 
interact with them and each other. Steps can be taken to ensure that resulting designs support cognitive 
aspects of learning activities. Developing virtual environments that support learning requires an understanding 
of the relationships between the cognitive capacities of the learner, the environment and activities 
being modeled, and the technical affordances of VR that support information encoding, development of 
knowledge structures, and performance. For example, input modality and display perspective (e.g., position, 
ground, and field of view) can be used to enhance information encoding and retrieval. The model of the 
world represented by the VR designer can support the formation or enhancement of the user s own mental 
representation by simplifying a complex system and revealing patterns that are difficult to discern. 
My hope is that the VR research and VR development communities can begin to establish a common language 
for discussing these issues and jointly formulate a set of design guidelines. Rob Tow Twelve percent 
of women are heterozygous for anomalous color vision, and are tetrachromatic they see subtle colors 
that men never will, because men have only one X chromosome, and can at best be trichromatic. People 
who grow up in Western architectures, with sharp right angles and many vertical and horizontal edges, 
have more cortical detectors for vertical and horizontal lines than do people who grow up in societies 
which are uncarpentered and agrarian; they see media such as halftoned pictures with forty-five degree 
angled screens subtly differently. Kamala and Amala, the famous wolf-girls of Bengal, never learned language 
after they were found in the wild and taken into an English family. We are all different; from our various 
genetics, and from the differing interactions of our growing bodies with the external world, both physical 
and social. Our brains and our bodies gain much of their structure from interacting with the world in 
the realm of the senses; the structure of the external world is reflected within our own structure and 
just as cats raised in an environment with no vertical components don t build cortical detectors for 
vertical shapes, we have not built cognitive structures for the worlds in which our children will develop 
 VR will change the very nature of the human mind and soul as it changes the sensory and social worlds 
in which we live and act. Our children will grow up immersed in this Novum Organum, and we will stand 
at the edge of the flow that separates them from us, unable to follow them because we literally will 
not see what they see. Unlike Moses promised land, this New Atlantis will be of our own making, created 
as we exteriorize our minds and culture into Turing complete agencies. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237302</article_id>
		<sort_key>509</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>66</seq_no>
		<title><![CDATA[Virtual reality and mental disorders (panel)]]></title>
		<page_from>509</page_from>
		<page_to>510</page_to>
		<doi_number>10.1145/237170.237302</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237302</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.4</cat_node>
				<descriptor>Psychology</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010455.10010459</concept_id>
				<concept_desc>CCS->Applied computing->Law, social and behavioral sciences->Psychology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31098342</person_id>
				<author_profile_id><![CDATA[81100620754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dorothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Strickland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtual Reality and Mental Disorders Organizer: Dorothy Strickland Panelists: Larry F. Hodges Suzanne 
Weghorst Nat Durlach This panel discusses the use of virtual reality and augmented reality treatment 
for individuals with a variety of mental disorders, including phobias, autism, and Parkinson s Disease. 
Panelists will describe their actual case studies and the insights and concerns they discovered in their 
research. The psychologists and computer scientists on the panel will address the ethical, psychological, 
and social questions. as well as the technical issues which arise from such applications. They will discuss 
ideas and suggestions for future directions in using virtual reality with mental disorders, and describe 
what is needed from the graphics community to refine and extend this work. Despite enthusiasm from the 
press and media, the virtual reality worlds experienced in a headset are generally disappointing. There 
are a great many things we can not do well in VR. Visually, virtual environ­ments are still cartoonish 
and lacking in realism. The combination of current tracker technology and the graphics pipeline guarantee 
a lag between head movement and response of the visual image. The spaces we can track are small, and 
tracker accuracy is poor. Haptic cues in virtual environments are usually nonexistent or very limited. 
As a result, examples of VR applications that offer sufficient value beyond that available from less 
exotic technologies are still rare. To create successful applications with today s VR technology, we 
must begin by asking: what are we good at? Despite all the obvious technical limitations, psychological 
studies are showing that VR provides a convincing illusion of actually inhabiting a computer­generated 
space. VR creates and controls sensory stimulation in formerly unattainable ways. Augmented reality allows 
an overlay of a real world with an imaginary world. Both VR and augmented reality can add, delete, or 
emphasize details to better help mental patients perform basic functions. These unique features can provide 
the mental patient with specialized, safer treatment techniques for problems that previously were expensive 
or impossible to treat in traditional training and therapy. Larry F. Hodges Virtual reality offers a 
new human-computer interaction paradigm in which users no longer are simply external observers of images 
on a computer screen, but are active participants within a computer­generated three-dimensional virtual 
world. Exposure therapy involves exposing the subject to anxiety-producing stimuli while allowing the 
anxiety to attenuate. These stimuli are traditionally generated through a variety of modalities including 
imaginal (subject generates stimulus via imagination) and in vivo (subject is exposed to real situations). 
Virtual Reality Exposure (VRE) Therapy involves exposing the patient to a virtual environment containing 
the feared stimulus in place of taking the patient into a real environment or having the patient imagine 
the stimulus. In a controlled study, VRE Therapy has been shown to be very effective in reducing acrophobic 
patient anxiety and avoidance of heights. VRE has also been shown in case studies to be successful in 
treating fear of flying. VRE Therapy has several advantages, as compared to more traditional exposure 
modalities. Many stimuli for exposure are difficult to arrange or control, and when exposure is conducted 
outside of the therapist s office, it becomes more expensive in terms of time and money. The ability 
to conduct exposures of virtual airplanes for flying phobics or virtual highways for driving phobics, 
for example, without leaving the therapist s office, would make better treatment available to more sufferers 
at a lower cost. can provide stimuli for patients who can not imagine well. Unlike therapist-assisted 
in vivo techniques, VRE Therapy will be performed within the confines of a room, thus avoiding public 
embarrassment and violation of patient confidentiality. Virtual environments have the added advantage 
of giving the therapist greater control over multiple stimulus parameters, as well as the ability to 
isolate the particular parameters that are most essential in generating the phobic response. VRE Therapy 
could also be used as an intermediate step in preparing patients for maintenance therapy involving self-directed 
in vivo exposure. Virtual reality exposure therapy is also appropriate for networked delivery of clinical 
psychology and psychiatry services to remote locations. Since the patient is receiving therapy within 
a virtual environment, the clinician conducting the therapy session could be present physically or participate 
via computer networks from a remote location. Dorothy Strickland Another potential application for VR 
is in treating autism. Autism is a pervasive developmental disorder characterized by severe impairment 
in social, communicative, cognitive, and behavioral functioning. Approximately half of individuals with 
autism never gain useful communicative speech. Serious difficulties exist with generalization. For example, 
if an individual learns to identify a wooden chair in one room, the word chair may be limited to that 
example in that identical setting. Because normal input can be overpowering, individuals with autism 
may not respond to real environments. VR affords control of the environment in ways previously unattain­able. 
Regulation of visual and auditory stimulation allows forced attention and focused directions. Input stimuli 
can be reduced to an individually acceptable level. Distortions in size and character of the components 
of reality allow matches to the user s expectations or abilities. Distracting visual complexity, sounds, 
and touch can be removed and introduced in a slow, regulated manner. Minimal modifications across similar 
scenes may allow generalization to another scene if differences are reduced until the similarities are 
recognizable. The measurement of user motion response to the environment permits dynamic scene modification 
and learning emphasis based on response, rather than speech. Two case studies with non-high functioning 
children with autism indicated that they could be taught the beginning steps necessary to learn how to 
independently cross a virtual street, an example of using VR to provide a less hazardous and more forgiving 
environment for developing skills associated with activities of daily living. VR with autism might provide 
safe, customized training for situations which would be difficult, if not impossible, to learn from real 
world exposure. The technology needs are different from general graphic applications. High-speed rendering 
of complex scene genera­tion is not required. Only limited color ranges recognized by the user are needed. 
Sound was removed in our study, because the children with autism found processing of vision alone preferable. 
Larger field of view may be necessary to provide the sense of immersion. Since patients may not be able 
to indicate if a problem exists in the helmet, judging of safety factors becomes more critical. Standard 
user input controls need to be simplified. Vestibular mismatch from latency may actually provide a treatment 
tool with autism where the vestibular system appears to differ from normal systems. In addition to cost 
reduction, VRE Therapy offers innovative treatment alternatives for patients. Like in vivo therapy, VRE 
Therapy The social and ethical issues are more complex. Individuals with autism are often already isolated 
within their own worlds. To create an artificial computer reality that only extends this isolation may 
be a disservice to them. Any use of VR should involve limited exposures, with the goal of integrating 
the artificial setting into its real equivalent.  Suzanne Weghorst Traditional psychotherapy has focused 
on modifying behavior by changing the client s internal processes, be they perceptions, interpreta­tions, 
articulations, contingency associations, or deeper psychoanalytic processes. The goal of therapy, from 
these perspectives, is to change the client to better fit reality. Augmented reality, or the merging 
of artificial and natural stimuli, affords the possibility of modifying behavior by altering the client 
s sensory inputs, in essence changing reality to better fit the client. This approach may be quite effective 
for certain disorders, particularly those due to specific neurological dysfunction, and may provide an 
alternative to pharmacological treatment. I anticipate at least two broad applications of AR in psychotherapy: 
(1) as a tool to enhance face-to-face therapeutic techniques, and (2) as a perceptual prosthesis for 
everyday use by the client. A candidate example of the former might be in directing the client s attention 
to the therapist or to some object of discourse. This application would benefit from a collaborative 
form of augmented reality, termed shared space. One fortuitous application of the second approach to 
AR therapy has been demonstrated in the treatment of Parkinson s Disease. Capitalizing on a well-known 
but little used visual cueing phenomenon (kinesia paradoxa), appropriate artificial cues can enable walking 
in akinetic patients and reduce the severity of dyskinesia resulting from long-term drug treatment. Ancillary 
positive effects on affect and cognitive functioning have also been observed. Augmented reality technology 
is currently in its infancy, and its practical applications are somewhat limited. In particular, current 
visual display methods suffer from restricted field of view, relatively poor spatial resolution, and 
insufficient brightness in competition with normal ambient light levels. Solutions to these technological 
problems are in sight, however, and long-term prospects for AR therapy are intriguing. Nat Durlach NEEDED 
RESEARCH ON VR-ASSISTED THERAPY: During the last three years, a number of individuals have begun to explore 
the use of VR technology in psychotherapy. Most studies to date have focused on behavioral therapy (exposure 
and desensitization) for phobias such as fear of heights. Although the results of these preliminary efforts 
appear promising, much work needs to be done in order to determine the ways in which, and the extent 
to which, VR technology can be truly useful (i.e., cost effective) in this area. In this presentation, 
we outline briefly some of the R &#38; D issues that we believe require attention. Research On VR-Assisted 
Behavioral Therapy For Anxiety Disorders: Previous work in this area needs to be extended by (a) developing 
improved, less expensive, and easier-to-use VR facilities, (b) systematically evaluating the usefulness 
of VR by comparing VR­assisted therapy to therapy without VR on various types of clinical populations, 
and (c) extending the use of VR to a wider variety of anxiety disorders. Other Types Of Therapeutic Applications: 
Efforts are also emerging to conceptualize and probe the use of VR for other types of psycho­therapeutic 
interventions and other types of disorders. In addition to the use of VR for expressive purposes, and 
in addition to the work discussed above concerning autism and Parkinson s Disease, consider­ation is 
now being given to its use for treating individuals with distorted body images associated with eating 
disorders. Relevant Technical Issues: Among the relevant technical issues that are being considered (apart 
from those associated with general power, flexibility, ease-of-use, and cost) are those concerned with 
(a) providing an improved sense of presence for the patient, (b) incorporating appropriate virtual actors 
in the therapeutic process, (c) designing appropriate VR stations for both the patient and the therapist, 
as well as the communication channels between them, (d) integrating physiologi­cal-response monitoring 
equipment with traditional VR equipment in an appropriate manner for the patient s VR station, and (e) 
exploring the potential of haptic interfaces for psychotherapy. Simplified Systems For Increased Cost-Effectiveness- 
The Search For The Effective Stimulus: It is easy to envision ideal VR systems that are likely to be 
useful in psychotherapy. However, realization of these ideal systems lies far in the future. In the meantime, 
we have to determine which dimensions of the VR experience are most crucial to the success of therapy 
and focus on the development of systems that perform well along these dimensions. Potential Hazards: 
Despite the enthusiasm for VR-assisted therapy among technologists, patients, and some therapists, there 
are a variety of potential hazards. One class of such hazards relates to sensorimotor and perceptual 
phenomena such as eye strain, simulator sickness, and distorted perceptual fields. A second class concerns 
possible psycho­logical damage to the patient resulting from unwanted and unexpected effects of VR on 
the patient s sense of identity, ability to distinguish between reality and fantasy, etc. A third class 
concerns psychosocial issues related to societal values about such issues as sex. A fourth class, which 
has not yet received much attention but probably will in the not­too-distant future, concerns the extent 
to which VR systems, particu­larly when augmented by physiological sensors (or displays), constitute 
ideal systems for brain washing. Clearly, it makes sense to consider these potential hazards, as well 
as the potential benefits of VR when making plans for the future.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237303</article_id>
		<sort_key>511</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>67</seq_no>
		<title><![CDATA[Building compelling VRML worlds (panel)]]></title>
		<page_from>511</page_from>
		<page_to>512</page_to>
		<doi_number>10.1145/237170.237303</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237303</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>World Wide Web (WWW)</descriptor>
				<type>P</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003152</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31088401</person_id>
				<author_profile_id><![CDATA[81100352009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Delle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maxwell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http'//reality.sgi.com/employees/clay/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http'//bug.village.virginia.edu/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[http'//www.construct.net/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Building Compelling VRML Worlds Organizer: Delle Maxwell Panelists: Clay Graham, Silicon Graphics David 
Blair, Artist Delle Maxwell, Independent Designer James Waldrop, Construct This panel proposes to explore 
and compare several different directions in large-scale sites built using VRML that show what 3D on the 
Web can offer: multimedia, and responsive, changeable, and expandable worlds. Being large-scale is not 
necessarily correlated with the heft of the files; if anything, a better goal is a balance between ideas, 
visual design, and navigability. With this balance in mind, each panelist is exploring a different domain: 
the formation of a vocabulary for virtual architecture, the reconstruction of a lost archaeological site, 
the exploration of hybrid narrative and the creation of a procedural cinema, and the creation of a VRML-based 
web site business. The panelists will present the ideas behind their work, and will as address issues 
they all have in common: how one weaves motion, lighting, architecture, sound, animation, history, navigation, 
and narrative into a fabric of interactive experience. A panel is the most suitable format for this kind 
of discussion, as it allows the participants to show their most up-to-date work. This is an important 
consideration in a field that is so fast-moving that three months make up a Web Year. [1] With that it 
mind, we expect features like animation, behaviors, sound, and real-time interaction to be integrated 
into the work that will be shown in August. And in an effort to strike an important balance between the 
artistic and the technical, we have included panelists that represent both these aspects. Clay Graham 
Clay Graham, a Virtual Architect, will present examples and ideas about a new type of architecture, which 
is less about physicality and shelter than it is about Space, Place, Symbol and Sign. [2] His statement 
is a short synopsis of the book he is currently writing on the subject. Introduction VRML holds unknown 
potential, allowing ordinary people to create unique places and share them with the world. With this 
democratization of creation comes some responsibility. Just as the artist uses the language of symbolism 
to communicate the stirrings of the soul, the new architect or designer will communicate through a sculptural 
and architectural language. Space The architectural metaphor serves as a natural point of reference 
from which the user can read the data. The space articulates the domain in which a particular query has 
taken place. All data extracted from the query resides within the architectural domain. This domain should 
be constructed with spatial archetypes that are an extension of the way we already see the world. In 
this way the archetypal elements of wall, floor, datum and column guide the user through the data intuitively 
without coercion by a modal interface. Perspective also infers that there is a human scale and texture, 
and it creates the need to assist the explorer in seeing the spatial domain of optimum interaction. Not 
only does a space have form, it also has context, or an organizing principle. The question Where am I? 
alludes to the need to define context. This leads to the discussion of Morphol­ogy and Topology: the 
criteria of how are things organized and how are they represented. To represent and organize information 
with meaning, the designer must look at how the individual elements will interact to become a collective. 
They will also need to break the information out of its original context to be reorganized dynamically. 
This leads into the discussion of construction and de-construction, and how the architec­tural metaphor 
deals with the creation and reorganization of context. Finally, once one has determined the relations 
that will create the space, it is necessary to apply construction techniques to create the most interactive 
and optimum performing space possible. Place A place is created by the interaction of objects, the space, 
and the user. By combining the context or theme for the application with objects that run as applets, 
a complex and dynamic relationship develops. These relationships can be refined into a set of guidelines 
for creating virtual architectures. By determining the type of place, the designer can determine the 
best model for user interaction and object behavior. For this reason the world of behaviors relates to 
the issue of place, because it will be the method whereby interaction is determined. Right now these 
behaviors are being implemented in scripting languages, such as Java, with the scripts creating events 
for the behaviors. An interest­ing problem is recognizing archetypes of behaviors and determining how 
to incorporate them into successful spaces. Symbol Meaning comes not just out of concepts themselves, 
but through their interrelation and context. Thought is not a thousand islands separated by an ocean, 
but a complex ambiguous rule system derived from the cross reference and overlap between those elements. 
This is the primary thesis behind the concept of symbolic arrays. Art, religion, and culture depend on 
this overlap to communicate meaning. The cathedral, temple, and masterpiece are not representations of 
static sign, but rich and complex symbolic systems meant to communicate an ambiguous concept or concepts 
greater than the sum of their identities. Sign Sometimes a designer may wish to create an index to what 
an object is, or to what it contains explicitly. Signs can be used to create a direct connection to an 
object or space so that it can be identified easily. Signs act as the direct indexes either to what the 
object is or to what it contains. David Blair Though video hasn t easily yet dropped onto the public 
digital networks, 3-D (e.g. VRML) does offer a time-based element that qualifies as a procedural cinema. 
Working from an interest in the instantiation of a unified narrative across multiple media, I am currently 
designing my next feature in parallel with a 3-D based network site. The beginning crossovers are easy 
to understand models for the film become procedural movies on the site; these movies can return to the 
film itself as moving images. The potential technical plasticity of this convergence interacts with the 
strangeness of storymaking; I focus my interest at this point. Delle Maxwell I will present a reconstruction 
of the Aztec ceremonial precinct of Tenochtitlan, the capital of the Aztec/Mexica empire. It is built 
in collaboration with the Inventor/VRML team at Silicon Graphics. The purpose of this site is educational, 
as well as experimental: we wish to create a site where exploration of a model can be done in conjunction 
with readings about its history and meaning in a way that allows one to get a fuller sense of the subject. 
The Aztecs were the most powerful civilization of the New World, alternately amazing and horrifying to 
the Spanish colonizers of the 16th century. The original temples site was almost completely destroyed 
by Hernan Cortez and the conquistadors by 1521, and even the location was lost until 1978, when work 
on Mexico City s subway system uncovered their remains. Archaeologists have since been excavating this 
site in the heart of present-day Mexico City in hopes of gaining more insight into Aztec culture. In 
this case, VRML is used to recreate a place that has, in reality, been lost, except for some building 
fragments, statuary, offerings, and historical and archaeological texts. The model [4] is connected with 
this information about the Aztecs and their lives, mythology, and history. Writings on the confrontation 
of the two cultures, with views from both sides of the same events, are also included. From these fragments 
we can begin to reconstruct what the environment of Aztecs of over 450 years ago might have been like. 
 James Waldrop James and his colleagues at Construct are in the enviable position of building a business 
that lets them experiment with the kind of web site building referred to as fun projects often collaborations 
with artists, designers, and architects. Declaring itself an Internet Design Company, Construct has positioned 
itself at the forefront of 3D interface design. [5] Making use of tools such as VRML and Java, Construct 
has created virtual environments that range from the highly realistic the VRML ArcGallery (for the Interactive 
Media Festival) to the abstractly hyperspatial a network visualization of Tierra. James Waldrop, Construct 
s Technical Director, will present some of Construct s latest and most interesting projects, including 
a real-time visualization of the Internet, a completely virtual technology expo, and a 3D asynchronous 
conferencing interface. Discussion of these projects will include extensive detail surrounding both the 
design and imple­mentation of interactive, networked 3D spaces. Notes [1] Attributed to Mike McCue, 
founder of Paper, Inc. [2] Clay s homepage is available for perusal at: http://reality.sgi.com/employees/clay/ 
 [3] Waxweb can be accessed via http://bug.village.virginia.edu/ Jews In Space is still under construction 
and not yet available for general viewing. The web address will be available at SIGGRAPH. [4] This model 
is based on the original reconstruction in the Museum of Anthropology in Mexico City, by Ignacio Marquina. 
Thanks to Bob Galbraith for his interpretation and model, which I then rebuilt and augmented for interactive 
performance. Since this web site is still under major construction, it is not yet available as a URL 
to the public. The web address will be available at SIGGRAPH. [5] Check out http://www.construct.net/for 
information on Construct. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237304</article_id>
		<sort_key>513</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>68</seq_no>
		<title><![CDATA[Springing into the fifth decade of computer graphics]]></title>
		<subtitle><![CDATA[where we've been and where we're going!]]></subtitle>
		<page_from>513</page_from>
		<page_to>514</page_to>
		<doi_number>10.1145/237170.237304</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237304</url>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.2</cat_node>
				<descriptor>Hardware</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003521.10003523</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->History of computing->History of hardware</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40027810</person_id>
				<author_profile_id><![CDATA[81100488149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machover]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Machover Associates Corp.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>540031</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brooks,Jr., Frederick E, "The Mythical Man-Month," Addison-Wesley Publishing Co., Inc., 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chasen, S. H., "Historical Highlights of Interactive Computer Graphics," Mechanical Engineering, November 1981, pp. 32-41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chasen, S. H. "The Introduction of Man-Computer Graphics into the Aerospace Industry," FJCC, 1965.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>577888</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chasen, S. H. and Dow, J. W., "The Guide for the Evaluation and Implementation of CAD/CAM Systems," CAD/CAM Decisions, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>208249</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Foley, van Dam, Feiner, Hughes,"Computer Graphics Principles and Practice (2nd Edition in C)," Addison-Wesley Publishing Company, Inc., 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Machover, Carl, "A Brief, Personal History of Computer Graphics," IEEE Computer, November 1978, pp 38-45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617926</ref_obj_id>
				<ref_obj_pid>616033</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Machover, Carl, "Four Decades of Computer Graphics." IEEE Computer Graphics and Applications, November, 1994, pp 14-19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>239239</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Machover, Carl, "The CAD/CAM Handbook," McGraw-Hill, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77279</ref_obj_id>
				<ref_obj_pid>77277</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA["Retrospectives 1: The early Years in Computer Graphics at MIT, Lincoln Lab and Harvard," Computer Graphics (SIGGRAPH Panel Proceedings) Vol. 23, No. 5, 1989, pp 1 - 38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Siders, R. A. et al, "Computer Graphics -A revolution in Design", American Management Association, New York, 1966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA["World-Wide State-of-the-Art" Special Issue, SIGGRAPH Computer Graphics Quarterly, May 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Springing into the Fifth Decade of Computer Graphics Where We ve Been and Where We re Going! Organizer: 
Carl Machover, Machover Associates Corp. Panelists: Dr. Frederick P. Fred Brooks, Jr., University of 
North Carolina Dr. Edwin E. Ed Catmull, Pixar Sylvan Chase Chasen, Investment Advisor Robert M. Bob Dunn, 
Enterprise Solutions International Dr. Bertram Bert Herzog, University of Michigan Dr. Andries Andy van 
Dam, Brown University, and NSF Science and Technology Center for Graphics and Visualization It s been 
a helluva ride for the past 40 years! In the 50s... computer graphics was a cure for no known disease... 
a solution in search of a problem. Now, computer graphics has become a cure for EVERY known disease. 
How did this happen, and where is it going? A blue-ribbon panel of computer graphics pioneers/innovators/practitioners 
will discuss their successes and foibles of the past and their vision for the future with each making 
a brief statement about their CG recollections and anecdotes, and making succinct comments about what 
they see coming up. The following are brief comments that each of the panelists prepared about their 
expected presentation. Carl Machover Machover has been involved in computer graphics for almost four 
decades as a consultant, entrepreneur, academic, author, and marketeer and was the first SIGGRAPH panels 
chair. He will discuss the evolution of computer graphics from being a cure for no know disease to being 
a cure for every known disease. Early users were risk takers since there was little available application 
software and the user, without really having ROI experi­ence, elected to make significant hardware investments 
in the hope that the investment would pay off by saving time, saving money, improving decision making, 
or doing something that couldn t be done any other way. By the early 70s, users in such applications 
as process control and CAD were reporting ROIs that were attractive to industrial companies in spite 
of the substantial capital investments required. And by the late 80s and 90s, cost of entry became low 
enough that the technology took its place beside telephones, typewriters, answering machines, copiers, 
faxes, and other common office machines, and were acquired just to stay competitive. Dr. Frederick P. 
Fred Brooks, Jr. Dr. Brooks is an ACM Newell Award winner, widely read and respected author, and computer 
graphics pioneer and innovator with particular interest in man-machine interfaces. He will talk about 
the future of synthetic environments, sometimes called artificial reality. Faster graphics engines promise 
to be able to render 10-20 million polygons/second. This allows a great increase in the complexity of 
the world models we can move about in real time. John Poulton points out that with screen resolution 
of about a million pixels and frame update rates on the order of 20 frames/second average, polygon size 
becomes the scene depth complexity, approaching a few pixels/polygon. The whole purpose of polygons in 
the first place was to save computation by allowing full computation on vertices and interpolation between. 
As polygon size becomes less than three, there is no saving. We might as well compute pixels directly. 
The first such approach to be explored is image-based rendering, with 2-D image warping interpolating 
between frames of full 3-D rendering. Just as frame buffers decoupled frame update from image refresh, 
so this technique promises to decouple world-model change from viewpoint change, to the great relaxing 
of urgency for 3-D rendering. Dr. Edwin E. Ed Catmull SIGGRAPH Coons Award recipient Dr. Catmull has 
led pioneering efforts in animation and rendering for the last twenty years. As a student at the University 
of Utah he developed texture mapping, Z-buffers, and techniques for displaying curved surfaces. These 
techniques have become standard in the industry. While a graduate student, he also created one of the 
first computer-generated effects for films a computer generated animation of a hand, which was used 
in the movie Futureworld. He was the first Director of the Computer Graphics Laboratory at the New York 
Institute of Technology in 1974. In 1979, Dr. Catmull joined Lucasfilm, managing the Computer Division 
with the charter to bring high technology into the film industry. In early 1986, when the Computer Division 
split off from Lucasfilm, he became president of the newly formed company, Pixar. Between 1986 and 1989, 
Pixar produced a series of short films, two of which were nominated for Academy Awards, and one, Tin 
Toy, received an Oscar in 1987. Dr. Catmull recently was executive producer of the wildly successful 
Toy Story, the first full-length computer graphics film. He will discuss the pursuit of illusions. Sylvan 
Chase Chasen Chase Chasen pioneered in the use of computer graphics for design and manufacture. Chase 
was responsible for some of the first produc­tion CAD/CAM applications in engineering and design. He 
was an active contributor to CAD/CAM education and received an SME Distinguished Contributor award. In 
1963, officials of Lockheed-Georgia Aircraft Company s newly formed Research Lab, the chief engineer, 
and other members of management visited MIT to witness and to evaluate Ivan Sutherland s PhD project, 
Sketchpad. It was clear that picture-based man-computer communications would give the computer field 
an added dimension, but would be moderately expensive with little or no apparent short term benefit. 
A leap of faith was required. To document our justification, I wrote over 50 pages entitled Prospectus 
on Computer Aided Design. Early in 1964, we formed a dedicated team of people with comple­mentary technical 
expertise. In early January 1965, we asked manage­ment for permission to establish a team of potential 
CAD/CAM users to study alternative application possibilities, and to select one that would demonstrate 
cost-benefit potential by the end of the year. By mid-November, we had demonstrated both the creation 
of a 3D (actually 2 1/ 2 D) prototype part and the path, converted to a machine-driven tape, for the 
numerically controlled milling machine to manufacture the part. This is believed to be the first production-oriented 
CAD/CAM application of any kind not just N/C. By the spring of 1966, this led to a fully operational 
three station time-shared CAD/CAM computer graphic N/C system. Concurrent with the General Motors DAC-1 
system (Design Augmented by computers directed by Don Hart), this is believed to be the first multi-station 
computer graphics system for production usage. During 1965, Lockheed-California began full scale development 
of CADAM, and at the 1995 Fall Joint Computer conference, we presented a paper entitled The Introduction 
of Man-Computer Graphics into the Aerospace Industry. This was done to document Lockheed s role in the 
historical beginnings of CAD/CAM and interactive computer graphics. It is usually easier to start something 
than to finish it. Therefore, the challenge of today is greater than for the pioneers, Today we have 
to determine how we can most efficiently link the islands of automation, make relevant data easy to locate, 
store and retrieve myriad documents electronically, and standardize the many interfaces among many concerns. 
Most importantly, how do we train both computer profession­als and the lay public to know what question 
to ask, and to seek answers without intimidation of hosts of complex, bewildering alternatives? Robert 
M. Bob Dunn About 20 years ago, Bob Dunn, together with Bert Herzog and under the auspices of SIGGRAPH, 
chaired the committee that produced one of the first device-independent computer graphic standards, CORE. 
Bob was SIGGRAPH chair 1973-1975, and together with Jon Meads and Jim Foley, launched the SIGGRAPH Conferences 
and Exhibitions in the early 70s. After promoting the use of computer graphics in the Army, Bob moved 
into several responsible industrial positions. Today, he is focused on IT Industry international corporate 
development. My Occam s Razor for technology is the position taken by Lord Alfred North Whitehead about 
100 years ago, which is Civilization advances by virtue of the number of operations it can do without 
thinking about them. Each time the technology of computing has entered a new setting in which to be of 
value, the technologies of depiction, representation, portrayal, and illusion (i.e. ?computer graphics?) 
have faced new challenges. Thinkers, users, and producers face new challenges. My Occam s Razor for the 
use of technology are the personal judgements each user makes about whether the cost and effort to adopt 
and deploy a technology is offset by the gains/pleasure/?pain? from its use. For the researcher, to have 
devised the more elegant/correct/robust theory/technology/design is the impetus to inquiry and the pursuit 
of research grants. For the early adopter, to have made it work first and gain bragging rights is justification 
enough. For those embroiled in hard, real-world problems, getting a solution where one was not possible 
before is to remove a barrier to success. And for the rest of us mainstream users of technology, if it 
shows up naturally, installs easily, and can be used directly through our GUI controls, we are willing 
to be amazed and grateful to the gods of technology for having made the world wonderful. For Computer 
Graphics, it still takes too long to do some things that we wish to be ordinary. Dr. Bertram Bert Herzog 
As both an academic and an industrialist, Bert Herzog has contributed to, and maintained close contacts 
with, both worlds. Initially, he was a structural engineer who got interested in analog computers at 
Case Institute of Technology, and subsequently worked with digital computers while teaching at the University 
of Michigan. In 1963 he joined Ford Motor Company, where he became involved in modern computer graphics. 
He returned to the University of Michigan (where he originated some of the first CG short courses), then 
to the University of Colorado, back to industry, and then back to the University of Michigan. Herzog 
is EIC of the IEEE publication, Computer Graphics and Applications, co-chair of ACM s 50th Anniversary 
Celebration, as well as SIGGRAPH Awards chair. He will ask: How long does it take a vision to reach fruition? 
There is evidence obtained from now-older but early visionaries that the answer is: Twenty years or more! 
On the other hand, recent events associated with the Internet and World Wide Web would lead us to believe 
that instant gratification is, at last, possible. What is wrong with this picture? Does memory play tricks? 
Can the visionary s optimistic expectations be realized now? Dr. Andries Andy van Dam Andy was a founder 
of SIGGRAPH 30 years ago, and has remained a computer graphics innovator and educator. With Dr. Jim Foley, 
he authored Fundamentals of Computer Graphics, and with Foley, Feiner, and Hughes, the expanded version 
Computer Graphics Principles and Practice. Both are the standard references in the field. Dr. van Dam 
was a recipient of the SIGGRAPH Coons award, and is a member of the National Acadamy of Engineering. 
He will discuss pre-WIMP, WIMP, and post-WIMP user interfaces, the impact that next­generation graphics 
and multimedia hardware and software will have on the evolution of user interfaces, and why the user 
interface is increas­ingly important in society s use of computers. References Brooks,Jr., Frederick 
P., The Mythical Man-Month, Addison-Wesley Publishing Co., Inc., 1975. Chasen, S. H., Historical Highlights 
of Interactive Computer Graphics, Mechanical Engineering, November 1981, pp. 32-41. Chasen, S. H. The 
Introduction of Man-Computer Graphics into the Aerospace Industry, FJCC, 1965. Chasen, S. H. and Dow, 
J. W., The Guide for the Evaluation and Implementation of CAD/CAM Systems, CAD/CAM Decisions, 1983. Foley, 
van Dam, Feiner, Hughes, Computer Graphics Principles and Practice (2nd Edition in C), Addison-Wesley 
Publishing Company, Inc., 1995. Machover, Carl, A Brief, Personal History of Computer Graphics, IEEE 
Computer, November 1978, pp 38-45. Machover, Carl, Four Decades of Computer Graphics. IEEE Computer Graphics 
and Applications, November, 1994, pp 14-19. Machover, Carl, The CAD/CAM Handbook, McGraw-Hill, 1996. 
Retrospectives 1: The early Years in Computer Graphics at MIT, Lincoln Lab and Harvard, Computer Graphics 
(SIGGRAPH Panel Proceedings) Vol. 23, No. 5, 1989 , pp 1 38. Siders, R. A. et al, Computer Graphics 
 A revolution in Design , American Management Association, New York, 1966. World-Wide State-of-the-Art 
Special Issue, SIGGRAPH Computer Graphics Quarterly , May 1996.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237305</article_id>
		<sort_key>515</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>69</seq_no>
		<title><![CDATA[Live computer animation (panel)]]></title>
		<page_from>515</page_from>
		<page_to>516</page_to>
		<doi_number>10.1145/237170.237305</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237305</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P282292</person_id>
				<author_profile_id><![CDATA[81332503500]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Live Computer Animation Organizer: Tim Heidmann Panelists: Ken Fuhrman Tim Heidmann Chuck Molyneaux 
 The array of general purpose graphics workstations typically used for computer aided design, scientific 
computation, visual simulation, and film special effects have become so sophisticated in their ability 
to generate high-quality, real-time computer animation that they can be used for live creation of graphics 
and effects for television broadcast. Although the field typically is dominated by special-purpose video 
processing hardware, these machines are beginning to show up in a variety of innovative applications. 
This panel will present some of those innovative applications, talk about the advantages and difficulties 
of using general-purpose computers for television, discuss their views of the technical and business 
issues in these applications, and prognosticate about future directions in the art. Undoubtedly, numerous 
disagreements will arise as to what is and is not an effective and valuable use of this technology. Attendees 
will be invited to ask questions and share their own views of the industry. Ken Fuhrman Broadcasters 
have just begun to examine the benefits of switching to general purpose graphics workstations from the 
black box solutions currently in use. Computers offer increased flexibility in the look and behavior 
of the graphics, the ability to network several graphics workstations together, and the opportunity to 
share one piece of computer graphics hardware between many different graphics applications. Along with 
these benefits are questions such as how to maintain the high quality standards of the look of the graphics 
required for network broadcast, how to manage using external data feeds to automatically generate text 
and images, and how to manage the creation of graphics and put these powerful and complex new tools into 
the hands of artists. Developers of new graphics applications are continually facing the inertia of industry 
standards. Broadcasters have a set of tools and production methods which are understood and widely available. 
Unions and the availability of artists with specific skills impose other limits on how graphics can be 
created. However, broadcasters are beginning to see the possibilities and benefits of using computers, 
and are starting to bite the bullet and undertake the challenges of using the new technolo­gies. An important 
benefit of using computers is the ability to exchange data between a wide range of applications and platforms. 
EVT has put considerable effort into being able to easily import imagery from several sources, computers 
as well as video equipment and tools, and from different graphics standards such as EPS. This effort 
matches the growth of demand for networked systems. A new question raised by the use of graphics workstations 
is how to manage what can be done in real time at 60 fps. The typical piece of black box video equipment 
has a narrow range of capabilities, but can always be relied upon to do its work without dropping fields. 
Comput­ers offer an unlimited amount of flexibility, but as the complexity increases, so does the calculation 
time. In traditional non-realtime computer animation this is not a problem, since the computation of 
each frame just takes a little longer, but in broadcast applications, exceeding the 1/60 second mark 
has serious implications on the look. There is no simple formula for understanding the limits of a particular 
computer s capabilities, and the use of tools to monitor performance and detect problems appears to be 
an unavoidable part of the design process. This is an important part of the education process for broadcasters 
trying to understand the implications of using the new tools. The increasing pace of education and raised 
expectations of broadcasters, combined with the continuing pace of improvements in technology and price, 
will make this an area of explosive growth over the next few years. Chuck Molyneaux There are real day-to-day 
benefits to be gained by the use of live computer graphics in broadcast. For example, election coverage 
has always been limited by how quickly changing information could be collected, assembled, and presented. 
Computers allow much of the process to be done by one device and automated, while at the same time providing 
flexibility in how the data are to be presented. Among the difficulties in making use of computers is 
the technical expertise currently required. To gain real acceptance, these systems must be usable by 
the artists who currently operate the paint boxes and character generators. Even the process of turning 
on and logging in to a graphics computer can be daunting. Also, it is necessary to address the special 
support, repair, and training issues which arise as well. We are in a period where producers are looking 
for novelty value, using graphics animation for its gee-whiz impact. They are also being driven by fear 
of falling behind, of being perceived to be outclassed by competing stations. The effect is that producers 
are making huge leaps to implement complex new systems without the opportunity to evolve into them to 
understand their use and value, and to gradually introduce viewers to new concepts and to have a chance 
to react to the responses of viewers. Many things are making it to air before the question of is this 
useful? has had a chance to be answered. There is a danger that the quantity and complexity of graphics 
may overwhelm viewers. Most network news and sports shows are already hugely complex in the number and 
type of elements used; computers make it easy to generate even more. At what point does it become too 
much, and begin to diminish the value of the information conveyed to the home? Virtual sets are currently 
generating great interest and excitement. The emphasis is on multimillion dollar systems with extensive 
effects and full virtual worlds. In time, however, the excitement will wear off, and Virtual Sets will 
need to justify their existence on mostly economic reasons. There are many ways parts of this technology 
can improve a wide range of shows live out-the-window imagery inserted into stage shows, for example 
 and the technology has the possibility of becoming yet another tool in the producer s shop. Use of computers 
in broadcast is poised for a real breakthrough in the next few years, brought on by: The advent of a 
low cost computer capable of broadcast-quality graphics and video processing.  The existence of a range 
of easy-to-use applications that all can run together on a shared machine.  The evolution of the business 
model distribution, training, and support to enable widespread use.  As this happens, use of computers 
to generate graphics will become an economic necessity for every television presenter local, cable, 
and network and a core piece of the production process, not just a novelty for the privileged few. 
Tim Heidmann We have begun to see excitement from broadcasters in having graphics computers fill the 
traditional roles of black boxes character generators, compositors, video effects generators, and in 
a few very specific applications, such as Virtual Sets. The excitement is growing, because broadcasters 
are starting to see the benefits of using general purpose workstations: The flexibility of applications 
in news, sports, and entertainment is virtually unlimited.  Complex data can be gathered automatically 
and presented visually.  One piece of hardware can be used for several applications simply with the 
acquisition of additional software.  The amount of computer and graphics power and its configuration 
can grow modularly with the choice of different machines or by adding more machines to the network. 
 Several people can be working simultaneously on one graphic or animation, each on their own workstation 
connected by a network and tied into databases or communication lines.  All these features have tremendous 
impact on such traditional uses of television graphics as character generation and creation of moving 
graphics, but we have only begun to explore the possibilities of how graphics can be used in conjunction 
with live video. Computer animation is a valuable educational tool. It can be used to illustrate news 
events, such as plane crashes or developments in a civil war, for which video is unavailable or impossible. 
In sports, football plays or sailing maneuvers can be illustrated from any point of view. The ability 
to create the animation quickly, with high quality, and to present it live is necessary for this tool 
to be usable in a live broadcast environment. We have also begun to see the possibilities of how computers 
can be used live to enhance the video presentation. FoxTrax uses a very simple highlight to make it easier 
for a hockey viewer to follow a tiny black puck as it speeds over the white ice. The data collection, 
communica­tion, and calculation necessary to achieve that result in real time are substantial, but the 
graphic itself must be subtle or risk obscuring or distracting from the live action. The inclusion of 
a virtual athlete in a foot race a simple marker or a synthetic image of a person running at the world-record 
pace can make a race in which the lead runner is far ahead interesting by allowing the broadcaster to 
concentrate on the real story: how close the athlete is to breaking the record. The current trend in 
graphics is to present more information more often, since more data and statistics are available. By 
making the information visual in the form of a picture, its possible to present more data which can be 
understood more easily by the viewer, actually simplifying the process. Live animated graphics, if done 
well, can also have great entertain­ment value. Arresting visuals and complex motion can be a great complement 
to the video alone, can make a show more fun to watch, and can set it apart from its competition. The 
biggest problems facing the use of live graphics in broadcast are in helping the broadcaster understand 
the opportunities and limitations of the technology, and in getting the tools into the hands of the artists 
in a way that gives them the freedom to independently explore new things. The current approach to making 
a system usable is to present the operator with a monolithic application, which prevents them from getting 
into trouble, but which also places severe limits on flexibility and creativity. The tools must continue 
to evolve to be more open, simple, and powerful. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>237306</article_id>
		<sort_key>517</sort_key>
		<display_label></display_label>
		<article_publication_date>08-01-1996</article_publication_date>
		<seq_no>70</seq_no>
		<title><![CDATA[Imaging features in advanced graphics architectures (panel)]]></title>
		<page_from>517</page_from>
		<page_to>518</page_to>
		<doi_number>10.1145/237170.237306</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=237306</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Medical information systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010447</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health care information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P31290</person_id>
				<author_profile_id><![CDATA[81332517337]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Murphy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Imaging Features of Advanced Graphics Architectures Organizer: Bob Murphy, Silicon Graphics Panelists: 
Randy Crane, Hewlett-Packard Kurt Akeley, Silicon Graphics Steve Howell, Sun Microsystems Arie Kaufman, 
SUNY Stonybrook Why have HP, Sun and SGI focused so heavily on imaging and volume rendering in their 
latest graphics systems and software interfaces? The various design approaches of HP s image processing 
accelerator, OpenGL s visualization extensions, and Sun s Visualization Instructions Set (VIS) will be 
presented, followed by an alternative approach from academia. Panel members will identify the problems 
each design tried to address, and will discuss both commonalties and differences in design approaches. 
The panel will conclude with a discussion on where interactive computer graphics systems design is going, 
and how much impact non-polygonal rendering will have in the future. The panel will be moderated by Dr. 
Henry Fuchs, Professor of Computer Science and Radiation Oncology at the University of North Carolina 
at Chapel Hill. Dr. Fuchs has been working in computer graphics since 1969, and is currently focusing 
research on interactive 3D medical imaging, virtual environments, and scalable interactive computer graphics 
architectures. The three industrial panelists will present their respective product goals and designs 
for 15 minutes, then the university participant will present alternate approaches. A 20-minute discussion 
on opportunities for applying the capabilities of these systems will conclude the panel. Randy Crane 
The landscape of the medical imaging market has changed dramatically over the last few years. It is transitioning 
from custom hardware-based solutions to off-the-shelf workstation and PC-based solutions. Given the increasing 
cost sensitivity of the market, HP decided to build a low­cost image processing accelerator. Based on 
customer feedback, we concluded that the current market needs are primarily 2D and are characterized 
by filtering, resampling, and window leveling (data mapping). HP sees 3D volumetric rendering as becoming 
very important in the near future. To achieve our goal of low cost, we decided to pursue a single chip 
architecture. As a result of this decision, we focused only on the most common 2D operations. While this 
solution does not provide the functionality available from a complete texture mapping implementa­tion, 
it allowed us to meet the project cost goals. OpenGL was the standard API we chose to expose the capabilities 
of our hardware. Most of the functionality was already available in the 2D pipeline definition. We merely 
defined a few extensions that added scaling, translation, and rotation with either bilinear or bicubic 
interpolation methods. The enhanced 2D pipeline provided our customers with a simplified programming 
model to access our acceleration technology. The result is a product that met cost goals, provided high 
performance, and satisfied the needs of our customers. Kurt Akeley Demands for interactive photo-realistic 
image generation driven by applications varying from broadcast television virtual sets, image exploitation 
in the intelligence community, virtual prototyping in manufacturing industries, and volume rendering 
in medicine and geophysical sciences have outstripped the abilities of traditional polygonal-based 3-D 
graphics systems. At Silicon Graphics, interactive image processing is not a separate discipline. Instead, 
image processing is just one component of the larger discipline of interactive visualization, which merges 
2D graphics, 3D graphics, and n-dimension image processing using the shared technology of texture mapping. 
Each SGI workstation product accelerates 2D and 3D texture-mapped graphics and image processing operations 
(such as convolution, histogram computation, and color table substitution) using a single hardware subsystem. 
The implementations of these visualization subsystems differ radically across the workstation product 
line (accelerated to their greatest extent in the Indigo Impact and Onyx Infinite Reality combined geometry 
and imaging pipelines), but all share the single architecture and programming interface defined by the 
OpenGL specification and its extensions. There are many advantages to the OpenGL visualization architecture. 
Graphics and image processing techniques can be combined in single rendering algorithms, effectively 
solving such problems as rendering embedded polygonal objects in volumetric data, or the reduction of 
geometry to depth-buffered images in complex geometric scenes. Seemingly disparate problems, such as 
distortion correction, shadow projection, and volume rendering, all may be implemented using the common, 
easily accelerated mechanism of texture mapping. Because OpenGL is both orthogonal and procedural, its 
many mechanisms can be combined in an endless variety of creative ways, enabling applica­tions that its 
designers had no intention of addressing. While OpenGL is widely accepted and implemented as a high­performance 
graphics interface, Silicon Graphics is unique in its commitment to OpenGL as its high-performance visualization 
architecture. Steve Howell The Visual Instruction Set (VIS) extensions to the SPARC architecture provide 
a powerful imaging and multimedia engine without the need for additional expensive, special-purpose hardware. 
VIS can perform up to eight integer operations per cycle, making it ideal for compute-intensive tasks 
such as image processing, video compression and decompression, and volume rendering. VIS can be applied 
to both memory and display operations. Putting the acceleration in the processor has several advantages 
over specialized hardware. It allows image data to be treated like any other data: free of any virtual 
memory or caching restrictions. Also, VIS is scalable, both with processor speed and number of processors. 
There will always be applications that require additional processing speed, and as SPARC processors get 
faster, VIS will be able to provide that processing power. Dr. Arie Kaufman The high computational requirements 
of traditional computer graphics led to the development of special-purpose graphics engines, primarily 
for polygon rendering. Similarly, the special needs of volume rendering, where an image must be computed 
rapidly and repeatedly from a volume dataset, lends itself to the development of special-purpose volume 
rendering architectures. A dedicated accelerator, which separates volume rendering from general-purpose 
computing, seems to be best suited to provide true real-time volume rendering on standard deskside or 
desktop computers. Volume rendering hardware may also be used to directly view changes of the 3D data 
over time for 4D (spatial­temporal) visualization, such as in real-time 3D ultrasonography, micro­tomography, 
or confocal microscopy. This may lead to the direct integration of volume visualization hardware with 
real-time acquisition devices, in much the same way as fast signal processing hardware became part of 
today s scanning devices. Cube-4 is a scalable architecture for true real-time ray-casting of large volumetric 
datasets. The unique features of Cube-4 are a high bandwidth skewed memory organization, localized and 
near-neighbor datapaths, and multiple, parallel rendering-pipelines with simple processing units. System 
performance scales linearly with the number of rendering pipelines, limited only by memory access speed. 
Cube-4 performs arbitrary parallel and perspective projections of high­resolution datasets at true real-time 
frame rates. The performance is data- and classification-independent, and can be achieved at a fraction 
of the cost of a multiprocessor computer. Cube-4 uses accurate 3D interpolation and high-quality surface 
normal estimation without any pre-computation or data duplication. Consequently, Cube-4 is also appropriate 
for 4D visualization as an embedded volume visualization hardware system in emerging real-time acquisition 
devices. Possible hardware implementations of Cube-4 for 30-frames-per-second rates range from an inexpensive 
PCI board accelerator for 256x256x256datasets, to a workstation accelerator board for 512x512x512 datasets, 
to a visualization server for 1024x1024x1024 or higher resolutions. The cost-performance of Cube-4 is 
several orders of magnitude better than existing solutions. The choice of whether one adopts a general-purpose 
or a special­purpose solution to volume rendering depends upon the circumstances. If maximum flexibility 
is required, general-purpose appears to be the best way to proceed. However, an important feature of 
graphics accelerators is that they are integrated into a much larger environment, where software can 
shape the form of input and output data, thereby providing the additional flexibility that is needed. 
A good example is the relationship between the needs of conventional computer graphics and special-purpose 
graphics hardware. Nobody would dispute the necessity for polygon graphics acceleration, despite its 
obvious limitations. We are making the exact same argument for volume rendering architectures. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1996</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
