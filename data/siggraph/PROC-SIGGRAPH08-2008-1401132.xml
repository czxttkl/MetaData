<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/11/2008</start_date>
		<end_date>08/15/2008</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1401132</proc_id>
	<acronym>SIGGRAPH '08</acronym>
	<proc_desc>ACM SIGGRAPH 2008 classes</proc_desc>
	<conference_number>2008</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2008</copyright_year>
	<publication_date>08-11-2008</publication_date>
	<pages>5354</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<section>
		<section_id>1401133</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: A gentle introduction to bilateral filtering and its applications]]></section_title>
		<section_page_from>1</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098585</person_id>
				<author_profile_id><![CDATA[81100495713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098586</person_id>
				<author_profile_id><![CDATA[81100343978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kornprobst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098587</person_id>
				<author_profile_id><![CDATA[81100216430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tumblin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098588</person_id>
				<author_profile_id><![CDATA[81100055904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fr&#233;do]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Durand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401134</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>50</pages>
		<display_no>1</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A gentle introduction to bilateral filtering and its applications]]></title>
		<page_from>1</page_from>
		<page_to>50</page_to>
		<doi_number>10.1145/1401132.1401134</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401134</url>
		<abstract>
			<par><![CDATA[<p>The bilateral filter is a non-linear technique that can blur an image while respecting strong edges. Its ability to decompose an image into different scales without causing haloes after modification has made it ubiquitous in computational photography applications such as tone mapping, style transfer, relighting, and desnoising. This text provides a graphical, intuitive introduction to bilateral filtering, a practical guide for efficient implementation, an overview of its numerous applications, as well as mathematical analysis.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Edge and feature detection</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098589</person_id>
				<author_profile_id><![CDATA[81100495713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paris]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adobe Systems, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098590</person_id>
				<author_profile_id><![CDATA[81100343978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kornprobst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, ENS, ENPC, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098591</person_id>
				<author_profile_id><![CDATA[81100216430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tumblin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Northwestern University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098592</person_id>
				<author_profile_id><![CDATA[81100055904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fr&#233;do]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Durand]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Aleksic, M. Smirnov, and S. Goma. Novel bilateral filter approach: Image noise reduction with sharpening. In <i>Proceedings of the Digital Photography II conference</i>, volume 6069. SPIE, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>754489</ref_obj_id>
				<ref_obj_pid>648280</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[V. Aurich and J. Weule. Non-linear gaussian filters performing edge preserving diffusion. In <i>Proceedings of the DAGM Symposium</i>, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141935</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[S. Bae, S. Paris, and F. Durand. Two-scale tone management for photographic look. <i>ACM Transactions on Graphics</i>, 25(3):637--645, 2006. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>568224</ref_obj_id>
				<ref_obj_pid>568214</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Barash. A fundamental relationship between bilateral filtering, adaptive smoothing and the nonlinear diffusion equation. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 24(6):844, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. Barash and D. Comaniciu. A common framework for nonlinear diffusion, adaptive smoothing, bilateral filtering and mean shift. <i>Image and Video Computing</i>, 22(1):73--81, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[B. E. Bayer. Color imaging array. US Patent 3971065, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073272</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[E. P. Bennett and L. McMillan. Video enhancement using per-pixel virtual exposures. <i>ACM Transactions on Graphics</i>, 24(3):845--852, July 2005. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2321347</ref_obj_id>
				<ref_obj_pid>2319071</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[E. P. Bennett, J. L. Mason, and L. McMillan. Multispectral bilateral video fusion. <i>IEEE Transactions on Image Processing</i>, 16(5):1185--1194, May 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319509</ref_obj_id>
				<ref_obj_pid>2318958</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. J. Black, G. Sapiro, D. H. Marimont, and D. Heeger. Robust anisotropic diffusion. <i>IEEE Transactions on Image Processing</i>, 7(3):421--432, March 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[A. Buades, B. Coll, and J. Morel. On image denoising method. Technical report, CMLA, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. Buades, B. Coll, and J. Morel. Neighborhood filters and PDE's. Technical Report 04, CMLA, 2005a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. Buades, B. Coll, and J.-M. Morel. A review of image denoising algorithms, with a new one. <i>Multiscale Modeling and Simulation</i>, 4(2):490--530, 2005b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[A. Buades, B. Coll, and J.-M. Morel. Neighborhood filters and PDE's. Technical Report 2005-04, CMLA, 2005c.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320998</ref_obj_id>
				<ref_obj_pid>2319057</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[A. Buades, B. Coll, and J.-M. Morel. The staircasing effect in neighborhood filters and its solution. <i>IEEE Transactions on Image Processing</i>, 15(6):1499--1505, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>131356</ref_obj_id>
				<ref_obj_pid>131344</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[F. Catt&#233;, P.-L. Lions, J.-M. Morel, and T. Coll. Image selective smoothing and edge detection by nonlinear diffusion. <i>SIAM Journal of Numerical Analysis</i>, 29(1):182--193, February 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276506</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Chen, S. Paris, and F. Durand. Real-time edge-aware image processing with the bilateral grid. <i>ACM Transactions on Graphics</i>, 26(3), 2007. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360660</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[H. Chong, S. Gortler, and T. Zickler. A perception-based color space for illumination-invariant image processing. <i>ACM Transactions on Graphics</i>, 27(3), 2008. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882431</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[P. Choudhury and J. E. Tumblin. The trilateral filter for high contrast images and meshes. <i>In Proceedings of the Eurographics Symposium on Rendering</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566650</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. DeCarlo and A. Santella. Stylization and abstraction of photographs. In <i>Proceedings of the SIGGRAPH conference</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566574</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[F. Durand and J. Dorsey. Fast bilateral filtering for the display of high-dynamic-range images. <i>ACM Transactions on Graphics</i>, 21(3), 2002. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015778</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[E. Eisemann and F. Durand. Flash photography enhancement via intrinsic relighting. <i>ACM Transactions on Graphics</i>, 23(3), July 2004. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320322</ref_obj_id>
				<ref_obj_pid>2319013</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. Elad. On the bilateral filter and ways to improve it. <i>IEEE Transactions On Image Processing</i>, 11(10):1141--1151, October 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2155799</ref_obj_id>
				<ref_obj_pid>2155772</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[M. Elad. Retinex by two bilateral filters. <i>In Proceedings of the Scale-Space conference</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276441</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[R. Fattal, M. Agrawala, and S. Rusinkiewicz. Multiscale shape and detail enhancement from multi-light image collections. <i>ACM Transactions on Graphics</i>, 26(3), 2007. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882368</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[S. Fleishman, I. Drori, and D. Cohen-Or. Bilateral mesh denoising. <i>ACM Transactions on Graphics</i>, 22(3), July 2003. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 6(6):721--741, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[G. Gimel'farb. <i>Image Textures and Gibbs Random Fields</i>. Kluwer Academic Publishers, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[F. R. Hampel, E. M. Ronchetti, P. M. Rousseeuw, and W. A. Stahel. <i>Robust Statistics - The Approach Based on Influence Functions</i>. Wiley Interscience, 1986. ISBN 0-471-73577-9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[P. J. Huber. <i>Robust Statistics</i>. Probability and Statistics. Wiley-Interscience, February 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[L. Itti and C. Koch. Computational modeling of visual attention. <i>Nature Reviews Neuroscience</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1018057</ref_obj_id>
				<ref_obj_pid>1018014</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[T. Jones, F. Durand, and M. Zwicker. Normal improvement for point rendering. <i>IEEE Computer Graphics &amp; Applications</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882367</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[T. R. Jones, F. Durand, and M. Desbrun. Non-iterative, feature-preserving mesh smoothing. <i>ACM Transactions on Graphics</i>, 22(3), July 2003. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141937</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[E. A. Khan, E. Reinhard, R. Fleming, and H. Buelthoff. Image-based material editing. <i>ACM Transactions on Graphics</i>, 25(3), 2006. Proceedings of the ACM SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>598410</ref_obj_id>
				<ref_obj_pid>598402</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[J. J. Koenderink and A. J. Van Doorn. The structure of locally orderless images. <i>International Journal of Computer Vision</i>, 31(2--3), 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276494</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[J. Kopf, M. Uyttendaele, O. Deussen, and M. Cohen. Capturing and viewing gigapixel images. <i>ACM Transactions on Graphics</i>, 26(3), 2007. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>212466</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[S. Li. <i>Markov Random Field Modeling in Computer Vision</i>. Springer-Verlag, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153435</ref_obj_id>
				<ref_obj_pid>1153170</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[C. Liu, W. T. Freeman, R. Szeliski, and S. Kang. Noise estimation from a single image. In <i>Proceedings of the conference on Computer Vision and Pattern Recognition</i>. IEEE, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[B. D. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision. In <i>Proceedings of the Image Understanding Workshop</i>. DARPA, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[S. Mallat. <i>A Wavelet Tour of Signal Processing</i>. Academic Press, 1999. ISBN: 0-12-466606-X.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1217910</ref_obj_id>
				<ref_obj_pid>1217875</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[A. Miropolsky and A. Fischer. Reconstruction with 3D geometric bilateral filter. In <i>Proceedings of the Symposium on Solid Modeling and Applications</i>. ACM, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[P. Mr&#225;zek, J. Weickert, and A. Bruhn. <i>Geometric Properties from Incomplete Data, chapter</i> On Robust Estimation and Smoothing with Spatial and Tonal Kernels. Springer, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[D. A. Murio. <i>The Mollification Method and the Numerical Solution of Ill-Posed Problems</i>. Wiley-Interscience, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383310</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[B. M. Oh, M. Chen, J. Dorsey, and F. Durand. Image-based modeling and photo editing. In <i>Proceedings of the SIGGRAPH conference</i>. ACM, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2094550</ref_obj_id>
				<ref_obj_pid>2094497</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[S. Paris and F. Durand. A fast approximation of the bilateral filter using a signal processing approach. In <i>Proceedings of the European Conference on Computer Vision</i>, 2006a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[S. Paris and F. Durand. A fast approximation of the bilateral filter using a signal processing approach. Technical Report MIT-CSAIL-TR-2006-073, Massachusetts Institute of Technology, 2006b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015784</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[S. Paris, H. Brice&#241;o, and F. Sillion. Capture of hair geometry from multiple images. <i>ACM Transactions on Graphics</i>, 23(3), July 2004. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78304</ref_obj_id>
				<ref_obj_pid>78302</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[P. Perona and J. Malik. Scale-space and edge detection using anisotropic diffusion. <i>IEEE Transactions Pattern Analysis Machine Intelligence</i>, 12(7):629--639, July 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015777</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[G. Petschnigg, M. Agrawala, H. Hoppe, R. Szeliski, M. Cohen, and K. Toyama. Digital photography with flash and no-flash image pairs. <i>ACM Transactions on Graphics</i>, 23(3), July 2004. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[T. Q. Pham. <i>Spatiotonal adaptivity in Super-Resolution of Undersampled Ima ge Sequences</i>. PhD thesis, Delft University of Technology, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[T. Q. Pham and L. J. van Vliet. Separable bilateral filtering for fast video preprocessing. In <i>International Conference on Multimedia and Expo</i>. IEEE, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[R. Ramanath and W. E. Snyder. Adaptive demosaicking. <i>Journal of Electronic Imaging</i>, 12(4):633--642, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153563</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[P. Sand and S. Teller. Particle video: Long-range motion estimation using point trajectories. In <i>Proceedings of the Computer Vision and Pattern Recognition Conference</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258056</ref_obj_id>
				<ref_obj_pid>258049</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[S. M. Smith and J. M. Brady. SUSAN - a new approach to low level image processing. <i>International Journal of Computer Vision</i>, 23(1):45--78, May 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319498</ref_obj_id>
				<ref_obj_pid>2318958</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[N. Sochen, R. Kimmel, and R. Malladi. A general framework for low level vision. <i>IEEE Transactions in Image Processing</i>, 7:310--318, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>570163</ref_obj_id>
				<ref_obj_pid>570162</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[N. Sochen, R. Kimmel, and A. M. Bruckstein. Diffusions and confusions in signal and image processing. <i>Journal of Mathematical Imaging and Vision</i>, 14(3):237--244, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>939190</ref_obj_id>
				<ref_obj_pid>938978</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[C. Tomasi and R. Manduchi. Bilateral filtering for gray and color images. In <i>Proceedings of the International Conference on Computer Vision</i>, pages 839--846. IEEE, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311544</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[J. Tumblin and G. Turk. Low curvature image simplifiers (LCIS): A boundary hierarchy for detail-preserving contrast reduction. In <i>Proceedings of the SIGGRAPH conference</i>. ACM, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[J. van de Weijer and R. van den Boomgaard. Local mode filtering. In <i>Proceedings of the conference on Computer Vision and Pattern Recognition</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>842937</ref_obj_id>
				<ref_obj_pid>839291</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[J. van de Weijer and R. van den Boomgaard. On the equivalence of local-mode finding, robust estimation and mean-shift analysis as used in early vision tasks. In <i>Proceedings of the International Conference on Pattern Recognition</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1137515</ref_obj_id>
				<ref_obj_pid>1137246</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[C. C. Wang. Bilateral recovering of sharp edges on feature-insensitive sampled meshes. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 12(4):629--639, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383888</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[L. Wang, L. Wei, K. Zhou, B. Guo, and H.-Y. Shum. High dynamic range image hallucination. In <i>Proceedings of the Eurographics Symposium on Rendering</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[G. S. Watson. <i>Statistics on spheres</i>. John Wiley and Sons, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141918</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[B. Weiss. Fast median and bilateral filtering. <i>ACM Transactions on Graphics</i>, 25(3):519--526, 2006. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142018</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[H. Winnem&#246;ller, S. C. Olsen, and B. Gooch. Real-time video abstraction. <i>ACM Transactions on Graphics</i>, 25(3):1221--1226, 2006. Proceedings of the SIGGRAPH conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[W. C. K. Wong, A. C. S. Chung, and S. C. H. Yu. Trilateral filtering for biomedical images. In <i>Proceedings of the International Symposium on Biomedical Imaging</i>. IEEE, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2094458</ref_obj_id>
				<ref_obj_pid>2094437</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[J. Xiao, H. Cheng, H. Sawhney, C. Rao, and M. Isnardi. Bilateral filtering-based optical flow estimation with occlusion detection. In <i>Proceedings of the European Conference on Computer Vision</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>536657</ref_obj_id>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[L. P. Yaroslavsky. <i>Digital Picture Processing. An Introduction</i>. Springer Verlag, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1115788</ref_obj_id>
				<ref_obj_pid>1115692</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[K. Yoon and I. Kweon. Adaptive support-weight approach for correspondence search. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 28(4), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153581</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Q. Y&#225;ng, R. Yang, H. Stew&#233;nius, and D. Nist&#233;r. Stereo matching with color-weighted correlation, hierarchical belief propagation and occlusion handling. In <i>Proceedings of the conference on Computer Vision and Pattern Recognition</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Q. Y&#225;ng, R. Yang, J. Davis, and D. Nist&#233;r. Spatial-depth super resolution for range images. In <i>Proceedings of the conference on Computer Vision and Pattern Recognition</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Gentle Introduction to Bilateral Filtering and its Applications Sylvain Paris1 Pierre Kornprobst2 
Jack Tumblin3 Fr´edo Durand4 1 Adobe Systems, Inc. 2 Odyss´ee project team INRIA,ENS,ENPC(France) 3 
Department of Computer Science, Northwestern University 4 Computer Science and Arti.cial Intelligence 
Laboratory Massachusetts Institute of Technology May 31, 2008 The bilateral .lter is a non-linear technique 
that can blur an image while respecting strong edges. Its ability to decompose an image into di.erent 
scales without causing haloes after modi.cation has made it ubiquitous in computational photography applications 
such as tone mapping, style transfer, relighting, and desnoising. This text provides a graphical, intuitive 
introduction to bilateral .ltering, a practical guide for e.cient implementation , an overview of its 
numerous applications, as well as mathematical analysis. 1 Introduction The bilateral .lter is a technique 
to smooth images while preserving edges. It can be traced back to 1995 with the work of Aurich and Weule 
[1995] on nonlinear Gaussian .lters. It has been later rediscovered by Smith and Brady [1997] as part 
of their SUSAN framework, and Tomasi and Manduchi [1998] who gave it its current name. Since then, the 
use of bilateral .ltering has grown rapidly and is now ubiquitous in image-processing applications. It 
has been used in various contexts such as denoising [Bennett and McMillan, 2005; Aleksic et al., 2006; 
Liu et al., 2006], texture editing and relighting [Oh et al., 2001], tone management [Du­rand and Dorsey, 
2002; Petschnigg et al., 2004; Eisemann and Durand, 2004; Bennett and McMillan, 2005; Elad, 2005; Bae 
et al., 2006], demosaicking [Ramanath and Snyder, 2003], stylization [Winnem¨oller et al., 2006], and 
optical-.ow estimation [Xiao et al., 2006; Sand and Teller, 2006]. The bilateral .lter has several qualities 
that explain its success: Its formulation is simple: each pixel is replaced by an average of its neighbors. 
This aspect is important because it makes it easy to acquire intuition about its behavior, to adapt it 
to application-speci.c requirements, and to implement it. Figure 1: The bilateral .lter takes as input 
an image (a) and generates a smooth version of it (b) in which the texture has been mostly removed while 
the main edges have been preserved. It depends only on two parameters that indicate the size and contrast 
of the features to preserve.  It can be used in a non-iterative manner. This makes the parameters easy 
to set since their e.ect is not cumulative over several iterations.  It can be computed at interactive 
speed even on large images thanks to e.cient numer­ical schemes [Durand and Dorsey, 2002; Elad, 2002; 
Pham and van Vliet, 2005; Pham, 2006; Paris and Durand, 2006a; Weiss, 2006] and even in real time if 
graphics hardware is available [Chen et al., 2007].  In parallel to applications, a wealth of theoretical 
studies [Sochen et al., 2001; van de Weijer and van den Boomgaard, 2001, 2002; Elad, 2002; Barash, 2002; 
Barash and Comaniciu, 2004; Durand and Dorsey, 2002; Buades et al., 2005c; Mr´azek et al., 2006; Paris 
and Durand, 2006a] explained and characterized the bilateral .lter s behavior. The strengths and limitations 
of bilateral .ltering are now fairly well understood. As a consequence, several extensions have been 
proposed [Elad, 2002; Choudhury and Tumblin, 2003; Buades et al., 2005c]. This article is organized as 
follows. Section 2 presents linear Gaussian .ltering and the nonlinear extension to the bilateral .lter. 
Section 3 revisits several recent, novel and chal­lenging applications of bilateral .lter. Section 4 
compares di.erent ways to implement the bilateral .lter e.ciently. Section 5 presents several links of 
bilateral .lter with other frame­works and also di.erent ways to interpret it. Section 6 focus on the 
link between bilateral .lter and the framework of partial di.erential equations (PDE) where several contributions 
exist. Section 7 exposes extensions and variants of the bilateral .lter. We also provide an Appendix 
that gives an intuitive view of smoothing and sharpening. 2 From Gaussian Blur to Bilateral Filter To 
introduce bilateral .ltering, we .rst describe in Section 2.2 the Gaussian convolution. This .lter is 
close to the bilateral .lter but is not edge-preserving. We will use it to introduce the notion of local 
average and in Section 2.3 to underscore the speci.cities of the bilateral .lter that make it edge-preserving. 
First, we introduce the notation that we use in the paper. 2.1 Terminology and Notation For simplicity, 
most of the exposition relies on a gray-level image I although everything can be directly translated 
to color images unless otherwise speci.ed. We use the notation Ip for the image value at position p. 
F[I] designates the output of a .lter F applied to the image I. We will regularly consider the set S 
of all possible image locations that we name the spatial domain,and the set R of all possible pixel values 
that we name the range domain. For instance, the notationp.S denotes a sum over all image pixels indexed 
by p.We use |·| for the absolute value and ||· | for the L2 norm, e.g. || p - q|| is the Euclidean distance 
between pixels p and q. 2.2 Image Smoothing with the Gaussian Blur (GB) Blurring an image a simple way 
to smooth an image. The core component is the convolution by a positive kernel which is the basic operation 
in linear shift-invariant image .ltering. It amounts to estimate at each position a local average of 
intensities and corresponds to low-pass .ltering (see also Section 6.2). One de.nes the Gaussian blur 
(GB) .ltered image by: GB[I]= Gs(|| p - q|| ) Iq, (1) p q.S where Gs(x) denotes the two-dimensional Gaussian 
kernel (see Figure 2): 1 x2 Gs(x)= exp- . (2) 2ps2 2 s2 Gaussian .ltering is a weighted average of the 
intensity of the adjacent positions with a weight decreasing with the spatial distance to the center 
position p. This weight is de.ned by the Gaussian Gs(|| p - q|| ), where sis a parameter de.ning the 
extension of the neighborhood. This in.uence function depends only the spatial distance between the pixels 
and completely ignores their value. For instance, a bright pixel has a strong in.uence over an adjacent 
dark pixel although these two pixels are di.erent. As a result, image edges are blurred because pixels 
across discontinuities are averaged together (see Figure 2). . The action of the Gaussian blur is independent 
of the image content, only the distances between positions matter. 1 1 1 1 0.9 0.9 0.9 0.9 0.8 0.8 
0.8 0.8 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.5 0.5 0.5 0.5 0.4 0.4 0.4 0.4 0.3 0.3 0.3 0.3 0.2 0.2 0.2 0.2 
0.1 0.1 0.1 0 0 0 0.1-60-40-20 0 20 40 60-60-40-20 0 20 40 60-60-40-20 0 20 40 60-60-40-20 0 20 40 60 
  ()||-||GGpqssr s =4 s =8 s =16 s =32 Figure 2: Example of Gaussian linear .ltering with di.erent s. 
Top row show the pro.le of a 1D Gaussian kernel and bottom row the result obtained by the corresponding 
2D Gaussian blur biltering. Edges are lost with high values of s since more averaging is performed. s 
Remark An advantage of linear shift-invariant .lters is that they can be implemented e.ciently using 
various techniques such as fast Fourier transform. Unfortunately, these ac­celeration techniques do not 
apply to nonlinear and shift-variant .lters such as the bilateral .lter. Nonetheless, fast numerical 
schemes have been developed speci.cally for the bilateral .lter (see Section 4). 2.3 Edge-preserving 
Filtering with the Bilateral Filter (BF) Similarly to the Gaussian convolution, the bilateral .lter is 
also de.ned as a weighted average of pixels. The di.erence is that the bilateral .lter takes into account 
the variation of intensities to preserve edges. The rationale of bilateral .ltering is that two pixels 
are related to each other not only if they occupy nearby spatial locations but also if they have some 
similarity in the photometric range. The formalization of this idea goes back in the literature to Yaroslavsky 
[1985], then Aurich and Weule [1995], Smith and Brady [1997] and Tomasi and Manduchi [1998]. The bilateral 
.lter, denoted by BF [ · ], is de.ned by: 1 BF [I] = p Wp q.S (| Ip - Iq| ) Iq (3) where Wp is a normalization 
factor: ()||-||GGpqssr s Parameters ss and sr will measure the amount of .ltering for the image I. Equation 
(3) is a 4 Wp = q.S (| Ip - Iq| ) (4) normalized weighted average where Gss is a spatial Gaussian that 
decreases the in.uence of distant pixels, Gsr a range Gaussian that decreases the in.uence of pixels 
q with an intensity value di.erent from Ip. Figure 1 shows a sample output of the bilateral .lter and 
Figure 3 illustrates how the weights are computed on a simple example. Parameters The bilateral .lter 
is controlled by two parameters: ss and sr. Figure 4 illus­trates their e.ect. As the range parameter 
sr increases, the bilateral .lter becomes closer to Gaussian blur because the range Gaussian is .atter 
i.e., almost a constant over the intensity interval covered by the image.  Increasing the spatial parameter 
ss smooths larger features.  An important characteristic of bilateral .ltering is that the weights are 
multiplied, which implies that as soon as one of the weight is close to 0, no smoothing occurs. As an 
example, a large spatial Gaussian coupled with narrow range Gaussian achieves a limited smoothing although 
the .lter has large spatial extent. The range weight enforces a strict preservation of the contours. 
Iterations The bilateral .lter can be iterated. This leads to results that are almost piecewise constant 
as shown on Figure 5. This type of e.ect is desirable for applications such as stylization [Winnem¨oller 
et al., 2006], while computational photography techniques [Durand and Dorsey, 2002; Bennett and McMillan, 
2005; Bae et al., 2006] tend to use a single iteration to be closer to the initial image content. Separation 
The bilateral .lter splits the input image into two layers: a large-scale com­ponent which is a smoothed 
version of the input with the main contours preserved, and a small-scale component which is the residual 
of the .lter. Depending on the settings and the application, this small-scale component can be interpreted 
as noise or texture. This is illus­trated in Figure 6. This property is used in applications such as 
tone management and style transfer (see Section 3). . To conclude, bilateral .lter is then an e.cient 
way to smooth an image while preserv­ing its discontinuities (see Sections 3.1 and 3.5) but also to separate 
structures of di.erent scales (see Section 3.2). As we will see therein, bilateral .lter has many applications 
and this notion of combining space and intensity was also extended (see Section 7) according to speci.c 
applications. Remark One possible objection at this stage of the presentation, is that it may be a time­consuming 
algorithm, since at each position, one needs to estimate a weighted sum over a large neighborhood. In 
Section 4 we will show some e.cient approaches to implement it. More generally, this idea of edge-preserving 
restoration has been studied a lot in the domain bilateral filter weights of the central pixel Figure 
3: The bilateral .lter smooths an input image while preserving its edges. Each pixel is replaced by a 
weighted average of its neighbors. Each neighbor is weighted by a spatial component that penalizes distant 
pixels and range component that penalizes pixels with a di.erent intensity. The combination of both components 
ensures that only nearby similar pixels contribute to the .nal result. The weights are represented for 
the central pixel (under the arrow). The .gure is reproduced from [Durand and Dorsey, 2002]. ss\sr 0.05 
0.2 0.8 GB Figure 4: E.ects of the range and spatial parameters, and comparison with Gaussian blur. 
As soon as one of the weight is close to 0, no smoothing occurs. As a consequence, increasing the spatial 
sigma has no consequence on a edge as long as the range sigma is less than its amplitude. For instance, 
the contour of the roof is una.ected for small range values, independently of the spatial setting. The 
range values are given considering that the intensities span [0, 1]. 1 iteration 2 iteration 4 iteration 
Figure 5: Iterations: Bilateral .lter can be applied iteratively. The result becomes closer from a piecewise 
constant signal. We use ss =8 and sr =0.1. This e.ect can be useful to achieve cartoon-like renditions 
of images [Winnem¨oller et al., 2006]. Input Bilateral .lter Residual Figure 6: Separation: The residual 
of the bilateral .lter reveals the structure of the input image: Ideally for denoising tasks, the residual 
should contain only noise. Nonetheless, the structure visible in the residual is faint and bilateral 
.ltering yields acceptable results in numerous denoising scenarios. of partial di.erential equations 
and one can wonder what is the relationship. We will discuss that point in Section 6.  3 Applications 
The bilateral .lter has been used for a variety of applications: Denoising (Section 3.1): This is of 
course the primaly goal of bilateral .lter, and it has been used in several application such as medical 
images, movie restoration, etc. Some .elds of applications are described. An extension of the bilateral 
.lter will be presented: the cross bilateral .lter.  Texture and Illumination Separation, Tone Mapping, 
Retinex, and Tone Management (Section 3.2): Based on a large-scale / small-scale decomposition of images, 
these ap­plications edit texture and manipulate the tonal distribution of an image to match the capacities 
of a given display or achieve photographic stylization.  Data Fusion (Section 3.4): These applications 
use bilateral .ltering to decompose several images into components and recompose them to form a single 
image that inherits the qualities of the input picture.  Three-dimensional Fairing (Section 3.5): 
This is the counterpart of image denoising for three-dimensional meshes and point clouds. Noise is removed 
from these data sets.  Other Applications (Section 3.6): We present a number of recent promising applications 
based on the bilateral .lter for which only one or two articles have been published so far.  3.1 Denoising 
One major application of bilateral .ltering is denoising [Buades et al., 2005b]. For instance, Adobe 
R. provides the bilateral .lter under the name surface blur (Figure 7); . Photoshop RIt uses a square 
box function as spatial weight and a tent function as range weight. Unlike Gaussian blur that smooths 
images without respecting their visual structure, the bilateral .lter preserves the object contours and 
produces sharp results. It is often used by portrait photographers to smooth skin while preserving contrast 
at the eyes and mouth. (a) input (b) noisy image (c) surface blur .lter Figure 7: Denoising using the 
surface blur .lter from Adobe R.. . Photoshop RWe added noise (b) the input image (a) and applied the 
surface blur .lter. Since the input image has been corrupted by noise, some loss is inevitable, yet the 
.ltered version is signi.cant improved. Liu et al. [2006] show that adapting the range parameter sr to 
the local noise level yields more satisfying results. In practice, they advise a linear dependence: sr 
=1.95sn where sn is the local noise level. However, Buades et al. [2006] remark and demonstrate that 
although bilateral .ltering preserves edges, the preservation is not perfect and some edges are sharpened 
during process, introducing an undesirable staircase e.ect . We discuss this e.ect in more detail in 
Section 6.4. Bilateral .lter has a lot of applications in denoising, and we mention some of them in the 
following sections. 3.1.1 Medical Imagery In the domain of medical imagery, Wong et al. [2004] improve 
the structure preservation by explicitly accounting the structure with an additional weight depending 
on the local shape and orientation of the data. 3.1.2 Videos Structure can also be preserved on videos 
by using temporal smoothing instead of spatial .ltering. Bennett and McMillan [2005] show that for static 
parts of a video sequence, bilateral .ltering along the time axis produces high quality results (Figure 
8). In this con.guration, each frame images the same scene point and the variations of the pixel value 
are due to only noise. Assuming a zero-mean noise, temporal averaging e.ectively reduces the noise level. 
A direct application of this idea would be to apply a temporal Gaussian blur, but this would result in 
undesirable trails for moving objects. Using temporal bilateral .ltering accounts for temporal discontinuities 
and prevent such trails. In regions with large motion temporal bilateral .ltering is ine.cient since 
each point is likely to be isolated on the time axis. In that situation, Bennett and McMillan resort 
to spatial .ltering, and since this occurs only in animated regions, the resulting spatial smoothing 
is unnoticeable. 3.1.3 Orientation Smoothing Paris et al. [2004] use the bilateral .lter to smooth the 
2D orientation .eld that they compute in the context of hairstyle modeling. They propose a measure scheme 
that yields a per-pixel evaluation of the local orientation. However, these measures are not equivalently 
precise on the whole image and a few pixels have erroneous values due to the complex nature of hair images. 
Paris et al. evaluate the success of their measure at pixel p using the variance Vp and incorporate it 
into the .lter. In Paris setup, several illumination conditions are available for each pixel. The maximum 
di.erence G among all illuminations is also used. Considering that the orientation is an angle a between 
0 and p, averaging is performed using a mapping onto a complex exponential: a. [0,p[ 2ia . exp. C, leading 
to the .lter: .exp(2iFParis(a)p)= Gss (|| p - q|| ) GsV (Vp/Vq) GsGG(p,q)exp(2iaq)(5) q Note that this 
.lter acts upon orientation and thus through a mapping onto the complex plane. Only the argument of the 
result is used in Paris application, the amplitude . is (c) output of (a) input (b) naive histogram stretching 
[Bennett and McMillan, 2005] Figure 8: Bennett and McMillan [2005] describe how to combine spatial and 
temporal bilateral .ltering to achieve high-quality video denoising and exposure correction. Figure reproduced 
from [Bennett and McMillan, 2005]. discarded although, if needed, it could be used as an equivalent of 
the standard deviation in the scalar case [Watson, 1983]. This .lter illustrates how application-speci.c 
knowledge can be incorporated in bilateral .ltering. (a) zoom on input image (b) orientations before 
bilateral filtering (c) orientations after bilateral filtering Figure 9: Paris et al. [2004] smooth 
their orientation measure using a variant of bilateral .ltering in which a mapping to the complex plane 
C is used. Figure reproduced from [Paris et al., 2004]. 3.1.4 Discussion and Practical Consideration 
Denoising usually relies on small spatial kernels and the range sigma is usually chosen to match the 
noise level. The bilateral .lter might not be the most advanced denoising technique but its strength 
lies is in its simplicity and .exibility. The weights can be adjusted to take into account any metric 
on the di.erence between two pixels and information about the reliability of a given pixel can be included 
by downweighting it. In the case of impulse noise, the input image might need to be molli.ed before the 
bilateral .lter is applied. This is because pixels a.ected by such noise might be too di.erent from their 
neighbors to be .ltered out. This is why the range Gaussian should be computed on a median­.ltered version 
of the image [Durand and Dorsey, 2002]. This is a standard practice in robust statistics: a very robust 
estimator such as the median is .rst applied before a more precise one re.nes the estimate.  3.2 Contrast 
Management Bilateral .ltering has proven to be particularly successful for various aspects of contrast 
management such as detail enhancement or reduction. Oh et al. [2001] described how to separate an image 
into a large-scale component and a small-scale component using the bilateral .lter. With this decomposition, 
they edit texture in a photograph. Durand and Dorsey [2002] use a similar decomposition for tone mapping. 
Elad [2005] followed the same strategy to estimate the illumination and albedo of the photographed scene. 
Bae et al. [2006] extended this approach to manipulate the look of a photograph. Fattal et al. [2007] 
describe a multi­scale image decomposition that preserves edges and allows for combining multiple images 
to reveal object details. We detail these applications in the following sections. 3.2.1 Texture and Illumination 
Separation In the context of image-based modeling, Oh et al. [2001] use the structure-removal aspect 
of the bilateral .lter. By using a su.ciently large range parameter sr, the bilateral .lter successfully 
removes the variations due to texture while preserving larger discontinuities stemming from illumination 
changes and geometry. Their technique is motivated by the fact that illumination variations typically 
occur at a larger scale than texture patterns. To extract the illumination component, they derive a variant 
of the iterated bilateral .lter for which the initial image is always .ltered. The successive estimates 
are used only to re.ne the range weight: 1 B BFi+1[I]= ()||-|| Gpq sr Gss B- B BFi[I]BFi[I] q Iq, p 
Wp q.S p with B= BF0[I] I. In addition, since a depth estimate is available at each image pixel, they 
adapt the spatial Gaussian size and shape to account for depth foreshortening. At each pixel, a plane 
tangent to the local geometry is estimated, and the spatial Gaussian is set such that it is isotropic 
in this tangent plane, which results in an anisotropic Gaussian once projected onto the image plane. 
 3.2.2 Tone Mapping Durand and Dorsey [2002] show that the use of bilateral .ltering can be extended 
to iso­late small-scale signal variations including texture but also small details of an image. They 
demonstrate this property in the context of tone mapping whose goal is to compress the in­tensity values 
of an high-dynamic range image to visualize it on a low-dynamic range display. Naive solutions such as 
uniform scaling or gamma correction yields unsatisfactory results since scene details are lost because 
of intensity compression. Durand and Dorsey s solution is to isolate the details before compressing the 
intensity. They apply the bilateral .lter on the log-intensities of the HDR image, scale down uniformly 
the result, and add back the .lter residual, thereby ensuring that the small-scale details have not been 
compressed during the process. This strategy is similar to the one adopted by Tumblin and Turk [1999] 
but the use of the bilateral .lter enables a signi.cant speed-up compared to the partial derivative .lter 
proposed by Tumblin and Turk, as well as enhanced stability. Figure 10 shows an HDR image tone-mapped 
with Durand and Dorsey s technique. 3.2.3 Retinex Elad [2005] proposes a di.erent interpretation of 
the tone-mapping technique of Durand and Dorsey using the retinex theory that seeks a separation of images 
into illumination and albedo. Under the assumption that scene objects do not emit light, illumination 
values are greater than the measured intensities since objects always absorb part of the incoming light. 
Elad (a) input (b) naive compression  (c) compression after Gaussian decomposition (d) output of [Durand 
and Dorsey, 2002] Figure 10: Tone Mapping: Direct display of a HDR image (a) is not satisfying because 
of over­and under-exposed areas. Compressing the intensity values solves this problem but details in 
clouds and in the city below the horizon are barely visible (b). Isolating the details using Gaussian 
blur brings back the details but incurs halos near contrasted edges (e.g.,nearthe tree silhouettes) (c). 
Durand and Dorsey use the bilateral .lter to isolate the small variations of the input image without 
incurring halos (d). Figure reproduced from [Durand and Dorsey, 2002]. adapts the bilateral .lter to 
ensure that the .ltered result ful.lls this requirement and is an upper envelop of the image data. He 
replaces the range weight Gsr by a truncated Gaussian H × Gsr where H is a step function whose value 
is 1 for non-negative inputs and 0 otherwise. As a consequence, at a given pixel p, the local averaging 
includes only values greater than intensity at p and guarantees a .ltered value above the input intensity. 
 3.2.4 Tone Management Bae et al. [2006] build upon the separation between large scale and small scale 
of Durand and Dorsey s technique [2002] and describe a sophisticated technique to transfer the visual 
look of an artist picture onto a casual photograph. They explore a larger space of image modi.cations 
by applying an arbitrary transfer function to the large-scale component. With histogram matching, they 
construct a transfer function that matches the global contrast and brightness of the model photograph. 
They also show that the small-scale component can be modi.ed in order to vary the amount of texture visible 
in the image. To this end, they introduce the notion of textureness that quantify the local degree of 
texture of an image using cross bilateral .ltering. With H the high frequencies of the image log-intensity 
log I,the textureness is de.ned by CBF [|H| , log I], that is the amplitude of the high frequencies are 
locally averaged while respecting the edges of the input image. (a) input (b) result after after contrast 
and textureness increase Figure 11: Bae et al. [2006] use the bilateral .lter to separate and process 
separately the large-scale and small-scale variation of an image. This example illustrates an increase 
of the global image contrast and an increase of the amount of texture. Figure reproduced from [Bae et 
al., 2006]. Later, Chen et al. [2007] sped up the bilateral .lter computation using graphics hardware 
and achieved real-time results on high-de.nition videos, thereby enabling on-the-.y control of the photographic 
style. 3.2.5 Detail Enhancement Fattal et al. [2007] extend the small-scale / large-scale decomposition 
to multiple layers to allow for a .ner control over which details are enhanced. The use their decomposition 
on several images taken from the same point of view but under di.erent lighting conditions and demonstrate 
a variety of e.ects by combining the obtained image pyramids. They describe how this combination can 
be controlled to reveal the desired level of details while avoid halos (Figure 12). They also describe 
a numerical scheme to e.ciently compute image pyramids using the bilateral .lter. 3.2.6 High-Dynamic-Range 
Hallucination Wang et al. [2007] use a bilateral decomposition to allow a user to generate a high-dynamic­range 
image from a single low-dynamic-range one. They seek to reconstruct data in over-and under-exposed areas 
of the image. User-guided texture synthesis is applied to the detail layer while smooth Gaussians are 
used to extend the range of the large scale. (a) sample input images (b) output with enhanced details 
 3.2.7 Discussion and practical considerations Contrast management relies on large spatial kernels because 
the small scale needs to include high and medium frequency components. The human visual system is not 
very sensitive to low frequencies but is quite sensitive to medium frequencies. Since the large scale 
component is typically the one that gets it contrast reduced, medium frequencies must be excluded from 
it. For contrast management, the bilateral .lter is usually applied to the log of the original image 
because the human visual system s notion of contrast is multiplicative. Using the log domain makes the 
range sigma act uniformly across di.erent levels of intensity: edges where .ltering should stop are de.ned 
in terms of multiplicative contrast. Similarly, relighting applications deal with a multiplicative process 
where illumination is multiplied by re.ectance. The use of the log domain is not without its problem, 
since zero maps to minus in.nity and noise in the dark regions can get magni.ed. This is why a small 
constant on the order of the noise level must be added to the input before taking the log. The new color 
space proposed by Chong et al. [2008] is particularly promising to handle multiplicative processes. The 
precise de.nition of luminance can have impact on very saturated colors. The stan­dard linear de.nition 
with its heavy weight on the green channel might lead to low luminance for very saturated reds and blue. 
Eisemann and Durand [2004] describe an alternative inten­sity function that improves results. RGB I = 
R + G + B (6) R + G + BR + G + BR + G + B  3.3 Depth Reconstruction Y´ang et al. [2006; 2007] and Yoon 
and Kweon [2006] expose how to use the bilateral .lter in the context of depth reconstruction from two 
images, a.k.a. stereo reconstruction. The objective is to pair pixels of the left image with pixels of 
the right image. Since the distance between the pixels of a pair, the disparity, is inversely proportional 
to the pixel depth, this information is equivalent to recovering the scene geometry. To pair the pixels, 
stereo algorithms compute a similarity score such as color di.erences or local correlation. Y´ang et 
al. andYoonand Kweon show that locally aggregating these scores using bilateral weights signi.cantly 
improves the accuracy of the recovered depth maps. Y´ang et al. [2007] have tested many similarity scores 
and pairing strategies and demonstrate the bilateral aggregation always improve the produced results. 
 (a) sample input image (b) coarse resolution computation (c) refinement using bilateral aggregation 
 3.4 Data Fusion 3.4.1 Flash / No-.ash Imaging Eisemann and Durand [2004] and Petschnigg et al. [2004] 
describe similar techniques to pro­duce satisfying pictures in low-light conditions by combining a .ash 
and a no-.ash photograph. Their work is motivated by the fact that, although the .ash image has an unpleasing 
hard direct lighting, its signal-to-noise ratio is higher than the no-.ash image. On the other side, 
the no-.ash image has a pleasing lighting but its high frequencies are corrupted by noise. The key idea 
is to extract the details of the .ash image and combine them with the large-scale component of the no-.ash 
picture. The bilateral .lter is used to perform the separation. Both articles introduced the cross (joint) 
bilateral .lter to better process the no-.ash photograph whose noise level is often too high to enable 
an accurate edge detection. Since the .ash image F represents the same scene, it is used to de.ne the 
edges and the .ltered no-.ash image is obtained as: 1 CBF [N,F]= Gss (|| p - q|| ) Gsr (| Fp - Fq| ) 
Nq (7) p Wp q.S where N is the original no-.ash image. Figure 14 gives an overview of the process, and 
Figures 15 and 16 show sample results.  3.4.2 Multispectral Fusion Bennett et al. [2007] show how to 
exploit infra-red data in addition to standard RGB data to denoise low-light video streams. They use 
the dual bilateral .lter, a variant of the bilateral .lter with a modi.ed range weight that account for 
both the visible spectrum (RGB) and the infra-red spectrum: 1 DBF [RGB]p = Gss (|| p - q|| ) GsRGB (|| 
RGBp - RGBq|| ) GsIR (| IRp - IRq| ) RGBq Wp q.S (8) where RGBp is a 3-vector representing the RGB component 
at pixel p,and IRp the measured infra-red intensity at the same pixel p. Bennett et al. show that this 
combination better detects edges because it is su.cient for an edge to appear in one of the channels 
(RGB or infra-red) to be accounted for. In combination with temporal .ltering, they demonstrate that 
it is possible to obtain high-quality video streams from sequences shot in very low light.  3.5 Three-dimensional 
Fairing Jones et al. [2003] extend bilateral .ltering to meshes. The di.culty compared to images is that 
all three xyz coordinates are subject to noise, data are not regularly sampled, and (a) photograph with 
flash (b) photograph without flash (c) combination (a) flash picture (b) no-flash picture (c) output 
of [Petschnigg et al., 2004] spatialweight Gss 2004]. the z coordinate is not a function of x and y 
unlike the pixel intensity. To smooth a mesh, Jones et al. assume that it is locally .at. Under this 
assumption and in absence of noise, a vertex p belongs to the plane tangent to the mesh at any nearby 
vertex q.With pq(p)the projection of p onto the plane tangent to the mesh at q, ideally we have p = pq(p). 
However, because of noise and because the mesh is not .at everywhere, this relation does hold in general. 
To smooth the mesh, Jones et al. average the position of p predicted by pq(p), they apply a (|| p - q|| 
) which ensures that only nearby points are considered. They add (p)|| )thatweights down outliers, i.e. 
the predictions pq(p)thatare far ()||-||GGpqssr s r aterm Gs(|| p - pqaway from the original position 
p. Usingaterm aq to account for the sampling density, the resulting .lter is: 1 FJones(p) = aqWp q (|| 
p - pq(p)|| ) pq(p) (9) To improve the results, they mollify the mesh normals used to estimate the tangent 
planes [Hu­ber, 1981; Murio, 1993], that is, they apply a low-pass .lter on the normals. This molli.cation 
is akin to the pre-.ltering step described by Catt´e et al. [1992] for PDE .lters. Figure 17 GG= ssr 
s shows a sample result. Fleishman et al. [2003] simultaneously proposed a similar approach (Figure 18). 
The main di.erence with the technique of Jones et al. is the way the .at neighborhood assumption is expressed. 
Fleishman et al. use the mesh normal np at p and project neighbors onto it. With q is such a neighbor, 
q should project on p,thatis: p +[(q - p) · np] np = p. This results in the following variant of the 
bilateral .lter: np FFleishman(p) (|| p - q|| ) (| (q - p) · np| )[(q - p) · np] (10) p + Wp q GGssr 
s The projection on the normal can be rewritten using the plane projection operator p used by Jones et 
al.:[(q - p) · np] np = q - pp(q). This leads to the following expression equivalent to Equation (10): 
1 FFleishman(p) (|| p - q|| ) (|| q - pp(q)|| ) q - pp(q) (11) p + = Wp q These two formulations underlines 
the di.erences between the approaches of Jones et al. and Fleishman et al.. Equation (10) shows that, 
unlike Jones et al., Fleishman et al. guarantee no vertex drift by moving p only along its normal np. 
On the other hand, Fleishman et al. do not compensate for the density variations. Furthermore, Equation 
(11) shows that the weights between both approaches are similar except that Jones et al. projects p on 
the tangent plane at q and thus exploit both the position and normal of all neighbors q,whereas Fleishman 
et al. projects q on the tangent plane at p, thereby exploiting .rst-order information only from the 
vertex p. This suggests an hybrid .lter that we have not yet evaluated: 1 GGssr s Fhybrid(p) (|| p - 
q|| ) (|| p - pq(p)|| ) q - pp(q) (12) p + = aq Wp q In addition to these di.erences in estimating the 
vertex positions, Fleishman et al. advocate iterating the .lter three times to smooth further the mesh 
geometry. Wang [2006] re.nes the process by explicitly detecting the sharp-edge vertices. He remeshes 
the model at these edges to ensure that sharp features are correctly represented by an edge between two 
triangles. (a) input (b) output of [Fleishman et al., 2003] ()||-||GGcqssr s Later, Jones et al. [2004] 
re.ned their technique to .lter normals. Applying a geometric transformation f to the 3D space, that 
is x . R3 . F (x), transforms the normals by the transposed inverse of the Jacobian of F . The Jacobian 
of F is a 3 × 3 matrix that captures the .rst-order deformation induced by F and is de.ned by Jij(F )= 
.Fi/.xj where Fi is the ith coordinate of F ,and xj the jth coordinate of x.Jones et al. shows that iteratively 
transforming the normals by J-T(FJones) smoothes the normals of a model while respecting its edges and 
without moving its vertices. They argue that not moving the vertices yields a better preservation of 
the .ne details of the meshes. Miropolsky and Fischer [2004] propose a variant of bilateral .ltering 
to smooth and dec­imate 3D point clouds. They assume that a normal np is known for each point p.They 
overlay a regular 3D grid on top of the points and determine a representative point for each grid cell 
by taking into account the point location and normal. With c the cell center and nc the mean normal of 
the cell points, they propose: 1 FMiropolsky(c) = (nc · nq) q (13) Wp q 19 3.6 Other Applications 3.6.1 
Optical Flow Xiao et al. [2006] apply bilateral .ltering to regularize the optical .ow computation. They 
use an iterative scheme to re.ne the .ow vectors between a pair of images. Each iteration consists in 
two steps: .rst the vectors are adjusted using a scheme akin to Lucas-Kanade [1981], then the .ow vectors 
are smoothed using a modi.ed version of bilateral .ltering that has two additional terms, one accounting 
for .ow similarity, and one that ensures that occluded regions are ignored during averaging. This scheme 
also .lls in occluded regions, that is pixels visible in one image of the pair but that are hidden in 
the other one. These occluded points gather information from pixels outside the occluded region covered 
by the bilateral .lter kernel, and the range weight ensures that only similar points contribute, thereby 
avoiding di.using data from the wrong side of the occlusion . An important feature of this technique 
is that it actually regularize the computation i.e. the bilateral .lter does not optimize a tradeo. between 
a data term and smoothness term, it only makes the data smoother. Nonetheless, since bilateral .ltering 
is interleaved with an optimization step, the process as a whole is a regularization. It can be seen 
as a progressive re.nement of the initial guess of a steepest­slope optimization. Sand and Teller [2006] 
accelerate this technique by restricting the use of bilateral .ltering near the .ow discontinuities. 
 3.6.2 Video Stylization Winnem¨oller et al. [2006] iterate the bilateral .lter in order to simplify 
video content and achieve a cartoon look (Figure 19). They demonstrate that the bilateral .lter can be 
computed in real time at video resolution using the numerical scheme of Pham and van Vliet [2005] on 
modern graphics hardware. Later, Chen et al. [2007] ported the bilateral .lter on the GPU us­ing the 
bilateral grid and achieved similar results on high-de.nition videos. Winnem¨oller et al. demonstrate 
that bilateral .ltering is an e.ective preprocessing for edge detection: .ltered images trigger fewer 
spurious edges. To modulate the smoothing strength of the bilateral .lter, they modify it to control 
the degree of edge preservation. The range weight Gsr is replaced by (1 - m) · Gsr + m · u where m is 
afunction varyingbetween 0 and 1to control edge preservation, and u de.nes the local importance of the 
image. To de.ne u and m,Win­nem¨oller et al. suggest using an eye tracker [DeCarlo and Santella, 2002], 
a computational model of saliency [Itti and Koch, 2001], or a user-painted map.  3.6.3 Demosaicking 
Demosaicking is the process of recovering complete color information from partial color sam­pling through 
a Bayer .lter (see Figure 20). Ramanath and Snyder [2003] interpolate missing color values of Bayer patterns 
[Bayer, 1976]. These patterns are used in digital cameras where each sensor measures only a single value 
among red, green, and blue. Bayer patterns are such that, although each pixel is missing two color channels, 
adjacent pixels have measures in these missing channels. Demosaicking is thus a small-scale interpolation 
problem, values are interpolated from neighbor pixels. Directly interpolating the values yield blurry 
images because edges are ignored. Ramanath and Snyder start from such an image and re.ne the result with 
bilateral .ltering. They use a small spatial neighborhood to consider only the pixels within the 1-ring 
of the .ltered pixel, and also ensure that measured values are not altered. The validation shows that 
the obtained results compare favorably to state-of-the-art techniques although the computational cost 
is higher.  3.6.4 Depth Map from Luminance Khan et al. [2006] use bilateral .ltering to process the 
luminance channel of an image and obtain a pseudo-depth map that is su.cient for altering the material 
appearance of the observed object. The originality of this use of the bilateral .lter is that the smoothing 
power of the bilateral .lter determines the geometric characteristics of an object. For instance, a smaller 
intensity tolerance sr results in a depth map that looks like engraved with the object texture because 
the intensity patterns are well preserved and directly transferred as depth variations. 3.6.5 Upsampling 
Kopf et al. [2007] describe joint bilateral upsampling, a method inspired from the bilateral .lter to 
upsample image data. The advantage of their approach is that it is generic and can potentially upsample 
any kind of data such as the exposure map used for tone mapping or hues for colorization. Given a high-resolution 
image and a downsampled version, one compute the data at low resolution and upsample them using a weighted 
average. High-resolution data are produced by averaging the samples in a 5 × 5 window at low-resolution. 
The weights are de.ned similarly to the bilateral .lter; that is, the sample in.uence decreases with 
distance and color di.erence. As a result, Kopf s scheme interpolates low-resolution data while respecting 
the discontinuities of the input image. (a) upsampled result (b) nearest (c) bicubic (d) Gaussian (e) 
joint (f) ground neighbor bilateral truth  4 Implementation In this section, we describe how the bilateral 
.lter can be implemented e.ciently. We begin with the brute-force approach as reference. We then describe 
the techniques based on separa­ble kernels of Pham and van Vliet [2005; 2006], the local histogram of 
Weiss [2006], and the bilateral grid [Paris and Durand, 2006a; Chen et al., 2007]. 4.1 Brute Force A 
direct implementation of the bilateral .lter consists in two nested loops, as presented in Table 1. For 
each pixel p in S 1. Initialization: Ip =0, Wp =0 s 2. For each pixel q in S (a) w = G(|| p - q|| )G(| 
Ip - Iq| ) sr (b) Ip+= wIq s (c) Wp+= w 3. Normalization: Ip = Ip /Wp Table 1: Algorithm for the direct 
implementation of bilateral .lter The complexity of this algorithm is O |S| 2 ,where |S| the size of 
the spatial domain (i.e. the number of pixels). This quadratic complexity quickly makes the computational 
cost explode for large images. A classical improvement is to restrict the inner loop to the neighborhood 
of the pixel p. . The rationale is that the s s s ss Typically, one considers only the pixels q such 
that || p - q|| = 2s contributions of pixels farther away than 2sis negligible because of the spatial 
Gaussian. 2 |S| ssmall spatial kernels, that is, small values of skernels because of the quadratic dependence 
in s This leads to a complexity on the order of O . This implementation is e.cient for but becomes 
quickly prohibitive for large .  4.2 Separable Kernel Pham and van Vliet [2005] propose to approximate 
the 2D bilateral .lter by two 1D bilateral .lters applied one after the other. First, they .lter each 
image column and then each raw. Each time, they use the brute force algorithm restricted to a 1D domain, 
that is, the inner loop on pixels q is restricted to pixels on the same column (or raw) as the pixel 
p.Asa ) since the considered neighborhoods are 1D s consequence, the complexity becomes O (|S| sinstead 
of 2D. This approach yields signi.cantly faster running times but the performance still degrades linearly 
with the kernel size. Furthermore, this approach computes an axis­aligned separable approximation of 
the bilateral .lter kernel. Although this approximation is satisfying for uniform areas and straight 
edges, it poorly matches more complex features such as textured regions. As a consequence, axis-aligned 
streaks appear with large kernels in such regions (Figure 23). Pham [2006] describes how to steer the 
separation according to the local orientation in the image. This approach improves the quality of the 
results, especially on slanted edges, but is computationally more involved since the 1D .lters are not 
axis aligned anymore. 4.3 Local Histograms Weiss [2006] considers the case where the spatial weight 
is a square box function, that is, he rewrites the bilateral .lter as: 1 BF [I]= Wp Gsr (| Ip - Iq| )Iq 
(14a) p q.Nss (p) Wp = Gsr (| Ip - Iq|) (14b) q.Nss (p) where Nss (p)= {q, || p - q|| 1 = ss}. In this 
case, the result depends only on the histogram of the neighborhood Nss (p) because the actual position 
of the pixel within the neighborhood is not taken into account. Following this remark, Weiss exposes 
an e.cient algorithm to compute the histogram of the square neighborhoods of an image. We refer to his 
article for the detail of the algorithm. The intuition behind his approach is that the neighborhoods 
Nss (p1)and Nss (p2)of two adjacent pixels p1 and p2 largely overlap. Based on this remark, Weiss describes 
how to e.ciently compute the histogram of Nss (p1) by exploiting the similarity with the histogram of 
Nss (p2). Once the histogram of Nss (p) is known for a pixel p, the result of the bilateral .lter BF 
[I](Eq. 14) can be computed since each histogram bin indicates how many pixels p q have a given intensity 
value I. A straightforward application of this technique produces band artifacts near strong edges, a.k.a. 
Mach bands, because a frequency spectrum of the box .lter is not band-limited. Weiss addresses this issue 
by iterating three times his .lter, which e.ectively smoothes out the artifacts. Weiss [2006] demonstrates 
that his algorithm has a complexity on the order of O (|S| log ss) which makes it able to handle any 
kernel size in short times. Furthermore, his algorithm is designed such that it can take advantage of 
the vector instruction set of modern CPUs, thereby yielding running times on the order of a second for 
images with several megapixels. The downside of this algorithm is that it deals with color images channel 
per channel which can introduce bleeding artifacts and that its extension to cross bilateral .ltering 
is unclear. 4.4 Layered Approximation Durand and Dorsey [2002] propose an fast approximation based on 
the intuition that the bilateral .lter is almost a convolution of the spatial weight Gss (|| p - q|| 
) with the product Gsr (|Ip - Iq|) Iq (Equation (3). But the bilateral .lter is not a convolution because 
the range weight Gsr (|Ip - Iq|) depends on the pixel value Ip. Durand and Dorsey overcome this by picking 
a .xed intensity value i, computing the product Gsr (|i - Iq|) Iq, and convolving it . After normalization, 
this gives the exact result of the bilateral .lter at all pixels p such that Ip = i. Computing the bilateral 
.lter this way would be extremely slow since one would have to compute a convolution for each possible 
pixel value i. Durand and Dorsey propose a two-step speed-up. First, they select a sparse subset withtheGaussiankernel 
Gsr oftheintensityvalues.Foreachvalue ,theyevaluatetheproduct {}i,...,iiG0sknr Thisproduceslayers .Each 
isthenconvolvedwiththespatialkernel LLLG0s,..., kns (|ik - Iq|)Iq. and normalized to form a new layer 
L k that contain the exact results of the bilateral .lter for pixels with intensity equal to ik. For 
pixels whose intensity has not been sampled, the result is linearly interpolated from the two closest 
layers. To further speed up the process, they downsampletheimage priortocomputingtheproductwiththerangeweight 
IGsr volvingwiththespatialkernel The.nallayers GLLs0,..., s and con­ . are obtained by upsampling 
n the convolution outputs. The bilateral .lter results are still obtained by linearly interpolating the 
two closest layers. r whereindicatesper-pixeldivision and .GG÷ a ssr s r --IiiIkkpp12 s 1. Given a 2D 
image I, compute a low-resolution version I, pick a set of intensities {i0,...,in}, and compute layers 
s L0,..., Ln: L k(q)= Gs( ik - I q ) I q 2. Convolve each layer with the spatial kernel and normalize 
the result: ¯ Lk =(Gs. L k) ÷ (Gs. Gs) corresponds to the sum of the weights at each pixel. ¯ 3. Upsample 
the layers Lk to get Lk. 4. For each pixel p with intensity Ip, find the two closest values ik1 and 
ik2 , and output the linear interpolation:  BF [I]p ik2 - ik1 L k2 + ik2 - ik1 L k1 Table 2: Reformulation 
proposed by Durand and Dorsey [2002] Durand and Dorsey s approximation dramatically speeds up the computation. 
Whereas a brute force implementation requires several minutes of computation for a megapixel image, their 
scheme runs in about a second. The downside of this approach is that in practice, the achieved result 
can be signi.cantly di.erent from the reference brute-force implementation, and there is no formal understanding 
of this di.erence. In the next section, we discuss the scheme of Paris and Durand [2006a] that is inspired 
by the layered approximation, and achieves an equivalent speed-up but with a signi.cantly better accuracy. 
We discuss the relationship between both approaches at the end of the following section. 4.5 Bilateral 
Grid Inspired by the layered approximation of Durand and Dorsey [2002], Paris and Durand [2006a] have 
reformulated the bilateral .lter in a higher-dimensional homogeneous space. They de­scribed a new image 
representation where a gray-level image is represented in a volumetric data structure that they named 
the bilateral grid. In this representation, a 2D image I is represented by a 3D grid G where the .rst 
two dimensions of the grid correspond to the pixel position p and the third dimension correspond to the 
pixel intensity Ip. In addition, this 3D grid stores homogeneous values, that is, the intensity value 
I is associated to a non-negative weight w and stored as an homogeneous vector (wI,w). Using this concept, 
Paris and Durand demonstrate that the bilateral .lter corresponds to a Gaussian convolution applied to 
the grid, followed by sampling and normalization of the homogeneous values. 1. Given a 2D image I, build 
the grid G: S×R . R2 that contains homogeneous values: (I(px,py),1) if r = I(px,py) G(px,py,r)= . (0,0) 
otherwise 2. Downsample G to get G 3. Perform a Gaussian convolution of G, for each component independently 
 . GBG(px,py,r)= Gss,sr G(px,py,r), where Gss,sr is a 3D Gaussian with ss as parameter along the two 
spatial dimensions and sr along the range dimension.  4. Upsample GBGto get G 5. Extracting the result: 
For a pixel p with initial intensity Ip, we denote wI, value at position  ( Iw) the (px,py,Ip) in G 
. The result of the bilateral filter is BF [I] Iw. wI/ p Table 3: Approximation proposed by Paris and 
Durand [2006a]. In practice, downsampling and upsampling is performed on the .y so that the high-resolution 
grid is never built. The bene.t of this formulation is that the Gaussian-convoluted grid GB[G] is a band­limited 
signal because it results from a Gaussian convolution which a low-pass .lter. Paris and Durand use this 
argument to downsample the grid G. As a result, they deal with fewer data and achieve performance on 
the order of a second for images with several megapixels. Chen and colleagues [2007] further improved 
the performances by mapping the algorithm onto modern graphics hardware, obtaining running times on the 
order of a few milliseconds. Paris and Durand advice to use the Gaussian bandwidth parameters s |R| 
for the grid. This yields a complexity of O r s s and swhere |S| is the size of the spatial r as sampling 
rates |S| + |S| 2 s s domain (i.e. the number of pixels) and |R| is the size of the range domain (i.e. 
the extent of the intensity scale). This approach can be easily adapted to cross bilateral .ltering and 
color images. The downside is that color images require a .ve-dimensional grid which no longer maps nicely 
onto graphics hardware and that requires large amount of memory for small kernels (10 pixels or less). 
Link with the Layered Approximation The bilateral grid and the layered approximation share the idea of 
subsampling the intensity axis and downsampling the spatial domain. The major di.erence is in the way 
downsampling is performed. The layered approximation is such that at discontinuities, pixels with di.erent 
values are averaged, e.g. awhite and a black pixels end up being represented by a gray value that poorly 
represents the initial signal. In comparison, the bilateral grid subsampling strategy keeps separated 
pixels with di.erent intensities since they are far apart along the intensity axis. For instance, in 
the white and black pixels case, the bilateral grid retains the information that there is two di.erent 
values involved and thus is able to produce better results. Figure 22 illustrates this behavior. As a 
consequence, since both approaches perform equivalently fast, the bilateral grid should be preferred 
over the layered approximation. (a) downsampling of the layered approximation (b) downsampling of bilateral 
grid approximation Figure 22: The bilateral grid better represents discontinuities and thus yield superior 
results compared to the layered approximation. This .gure is reproduced from [Paris and Durand, 2006b]. 
 4.6 Bilateral Pyramid For a number of applications such as detail enhancement [Fattal et al., 2007], 
it is desirable to decompose the image into more than two layers. Fattal et al. [2007] propose to compute 
such a decomposition by successively applying the bilateral .lter to the image with varying r s parameters: 
the spatial parameter sis doubled at each level and the range parameter sis halved. Based on this scenario, 
they describe a dedicated numerical scheme. Intuitively, instead of computing each level from scratch 
, they use the result from the previous level and rely on the fact that this image has already been smoothed 
to simplify the computation. For each level, they compute a bilateral .lter based on a 5 × 5 kernel. 
At the .rst level, s it corresponds to evaluating the bilateral .lter with a small kernel s27 = 1. Then, 
at each subsequent level, the spatial extent of the kernel is doubled. A naive approach would be to 
use more coe.cients, e.g. a9 × 9 kernel, at a cost of an increase of the computational cost. Fattal keeps 
the cost constant by still using 5 × 5 samples and inserting zeros. For instance, a9 × 9 kernel is approximated 
using 5 × 5 samples spaced with zeros, e.g. a1 - 4 - 6 - 4 - 1 row becomes 1 - 0 - 4 - 0 - 6 - 0 - 4 
- 0 - 1. This strategy is known as an algorithme `atrous, which is proven to yield minimal errors when 
applied to band-limited signals [Mallat, 1999]. In this particular case, the signal is not band-limited 
because bilateral .ltering preserve edges. Yet, Fattal s results show that in practice, this approximation 
achieves results without visual defects. 4.7 Discussion The implementation is a crucial choice to achieve 
satisfying results with good performances. Table 4 summarizes the complexity of the various implementations 
we described. Brute force (Section 4.1) O |S|2 Separable kernel (Section 4.2) O (|S| ss) Local histograms 
(Section 4.3) O (|S| log ss) Layered approximation (Section 4.4) O |S| + |S|ss 2 |R|sr Bilateral grid 
(Section 4.5) O |S| + |S|ss 2 |R|sr Table 4: Recapitulation of algorithms complexity When graphics hardware 
is available, the method of Chen et al. [2007] based on the bilat­eral grid is to be preferred because 
it achieves high quality outputs and real-time performances even on high-resolution images and videos. 
If only the CPU is available, the choice is split between the local-histogram method of Weiss [2006] 
and the bilateral grid of Paris and Du­rand [2006a]. To process color images or compute a cross bilateral 
.lter, the bilateral grid provides a satisfying solution, especially with large kernels. To process gray-level 
images with kernels of any size, e.g. in an image-editing package where users can arbitrarily choose 
the kernel size, the local-histogram approach is preferable because it consistently yields short run­ning 
times. On color images, this approach can yield less satisfying results since channels are processed 
independently, which can cause color bleeding (Figure 23). (b) input (c) exact bilateral filter using 
CIE Lab (d) bilateral-grid implementation using per-channel RGB (e) bilateral-grid implementation using 
RGB (0.48s, PSNRRGB = 38dB, PSNRLab = 34dB) (8.9s, PSNRRGB = 41dB, PSNRLab = 39dB) (f) separable-kernel 
implementation using CIE Lab (g) bilateral-grid implementation using CIE Lab (5.8s, PSNRRGB = 42dB, PSNRLab 
= 42dB) (10.9s, PSNRRGB = 46dB, PSNRLab = 46dB) Figure 23: Comparison of di.erent strategies for .ltering 
a color image (a,b). Processing the red, green, and blue channels independently results in color bleeding 
that makes the cross dis­appear in the sky (d). Dealing with the RGB vector improves this aspect but 
some bleeding still occurs (e). In contrast, working in the CIE-Lab space achieves satisfying results 
(c,g). The separable-kernel implementation is fast but incurs axis-aligned streaks (f) that may unde­sirable 
in a number of applications. These remarks are con.rmed by the numerical precision evaluated with the 
PSNR computed the RGB and CIE-Lab color spaces. The contrast of the close-ups has been increased for 
clarity purpose. This .gure is reproduced from [Paris and Durand, 2006b].  5 Relationship between bilateral 
.lter and other frameworks 5.1 One Iteration of Local Mode Filtering Van de Weijer and van den Boomgaard 
[2001] .rst established the relation between the bilat­eral .lter and local mode .ltering. For a given 
pixel, local mode .ltering consists in replacing its value by the closest highest mode in the local histogram, 
as illustrated in Figure 24. Let us de.ne this operation mathematically. For a given image I :O .R, 
we de.ne the histogram by: H1(i)= d(Iq -i), .i .R, q.S where d is the Dirac function so that d(u)= 1 
if u =0, and d(u) = 0 otherwise. A classical operation consists in smoothing histograms, so that we de.ne 
H2(i,sr)= H1 *Gsr (i)= Gsr (Iq -i), q.S where sr denotes the smoothing donne on the intensity values, 
i.e., on the range. A step further, one can de.ne an histogram locally, i.e., around a given position 
p. To do it, one can introduce another Gaussian kernel centered around p: H3(p,i,sr,ss)= Gss (p -q)Gsr 
(Iq -i), q.S where ss determines the spatial neighborhood around p. Local histograms can be used to study 
image properties [1999] but also to perform image restoration. The idea of local mode .ltering is to 
replace the intensity by the closest local mode. Mathematically, a local mode is de.ned by .H3 (p,i,sr,ss)=0, 
.i and one shows that the optimal i veri.es q.S Gss (p - q)Gsr (Iq - i)Iq i = . (15) q.S Gss (p - q)Gsr 
(Iq - i) The right-hand side term of (15) is very similar to the bilateral .lter formula. In fact, if 
we choose i = Ip in the right-hand side term, then (15) amounts to do one iteration of the bilateral 
.lter which goes in the direction of the closest local mode. We also refer to Section 4.3 where Weiss 
[2006] built his fast numerical scheme on this local-histogram interpretation of the bilateral .lter. 
 5.2 A Particular Case of Robust Statistics Filter Robust statistics o.er a general background to model 
a large class of problems, including image restoration (see [Huber, 1981; Hampel et al., 1986; Li, 1995; 
Gimel farb, 1999; Geman and Geman, 1984] for more details). As an example, image restoration can be formulated 
as a minimization problem in the following way: Given a noisy image In, the problem is to minimize the 
discrete energy min (Ip - In)2 + .(Iq - Ip) , (16) pI p.S q.W(p) where the sum contains two kinds of 
terms. The .rst term is a .delity attach term which prevents the solution to be too far away from the 
noisy input. The second term is a regu­larization term, that will penalyze di.erences of intensities 
between neighboring pixels, with a strengh depending on the function . (called the error norm). Thus 
the regularity of the solution will depend on function .. In particular, this method will be robust if 
we can pre­serve signi.cant intensity di.erences such as edges, i.e., if we can make the di.erence between 
inliers and outliers. Several possible . functions have been proposed in litterature, as we are going 
to show in this section. Let us now focus on the regularization term of (16) to show the relation with 
the bilateral .lter. To do so, we introduce a reweighted version of the regularization operator, so that 
the minmization problem becomes: min Gss (q - p).(Iq - Ip). (17) I p.S q.W(p) To minimize (17), one can 
iterate the following iterative scheme: . Ipt+1 = Ipt + Gss (q - p)..(Iqt - Ipt). (18) |W(p)| q.W(p) 
By choosing .(t)=1 - Gsr (t), we obtain: = It + .Gss (q - p)Gsr (It - It)(It - It). (19) p p qpqp It+1 
|W(p)| q.W(p) This equation has in fact some similarities with the bilateral .lter expression, which 
corre­sponds to a weighted average of the data, that we remind here: It+1 q Gss (q - p)Gsr (Iqt - Ipt)Iqt 
= , (20) p (It - It q Gss (q - p)Gsr qp) and, interestingly, it has been shown that (19) and (20) are 
indeed two equivalent ways to solve the same minimization approach (see, e.g., [Hampel et al., 1986]). 
The conclusion is that the bilateral .lter is a special case of robust .lter. More generally, Durand 
and Dorsey [2002] studied the bilateral .lter in the framework of robust statistics [Huber, 1981; Hampel 
et al., 1986] in a similar manner as the work of Black et al. [1998] on PDE .lters. The authors showed 
that the range weight can be seen as a robust metric, that is, it di.erentiates between inliers and outliers. 
The bilateral .lter replaces each pixel by a weighted average of its neighbors. The weight assigned to 
each neighbor determines its in.uence on the result and is crucial to the output quality. In this context, 
robust statistics estimates if a pixel is relevant, i.e. is an inlier, or if it is not, i.e. is an outlier. 
The strategy followed by the bilateral .lter is that pixels with di.erent intensities are not related 
and should have little in.uence on each other, whereas pixels with similar intensities are closely related 
and should strongly in.uence each other. The way that this intensity di.erence actually contributes is 
de.ned by the range weight. The most common choice is a Gaussian function Gsr . However, Durand and Dorsey 
[2002] have underscored that this Gaussian function is only one of the possible choices among a variety 
of robust weighting functions (cf. Figure 25-top), a.k.a. stopping functions. These functions de.ne the 
weights assigned to a pixel according to its di.erence of intensity with the center pixel. For instance, 
a classical non-robust mean assigns the same weight to all pixels. In comparison, robust functions have 
a bell pro.le that assign lower weights to pixels with a di.erent intensity. The di.erences lie in the 
fall-o. rate which de.nes how narrow is the transition between inliers and outliers, and in the tail 
value: either non-zero, meaning that outliers still have some limited in.uence, or zero, meaning that 
outliers are completely ignored. This behavior is better observed on the in.uence function (Fig. 25-bottom) 
that shows the variations of the output depending on the pixel intensity. The constant weight of classical 
averaging is not robust because its in.uence function is unbounded which re.ects the fact that a single 
pixel can have an unlimited in.uence on the mean value, e.g. a single very bright pixel can make the 
average arbitrarily high. In contrast, robust in.uence functions are bounded, showing that a single pixel 
cannot modify the output beyond a certain point. Some robust functions such as the Gauss, Tukey, and 
Lorentz functions are even redescending, re.ecting the fact that pixels with a large intensity di.erence 
are considered irrelevant and ignored, i.e. they have no in.uence on the output. Durand and Dorsey [2002] 
showed that these concepts can be applied to the bilateral .lter and that the choice of the range function 
de.nes how the pixels across are handled (see some results in Figure 26). For instance, with the classical 
Gaussian function, pixels across edges still have some in.uence though very limited; with a Tukey function, 
these pixels would constant Gauss Tuckey Lorentz Hubert  input Gauss Tuckey  Lorentz Hubert Figure 
26: Example of restoration with the di.erent weighting functions. be ignored. However, according to Durand 
and Dorsey s experiments, the Gauss and Tukey functions perform better for their tone-mapping operator. 
As far as we know, these options have not been tested with other applications. 5.3 Partial Derivative 
Equations and Over-Sharpening In the following section, we summarize the link of bilateral .ltering 
with .lters based on partial derivative equations (PDE). We dedicate Section 6 to the details of this 
relationship. The bilateral .lter smoothes images while preserving strong edges. Numerous .lters based 
on PDEs qualitatively achieve the same result. This motivated several researchers to study possible links 
between both approaches. Durand and Dorsey [2002] have showed that the bilateral .lter restricted to 
the four adjacent neighbors of each pixel actually corresponds to a discrete version of Perona and Malik 
model [1990]: .I = div Gsr (||. I|| ) . I. .t This result has been extended by Elad [2002] and Barash 
and Comaniciu [2004] who have demonstrated that the bilateral .lter can be seen as the sum of several 
Perona Malik .lters at di.erent scales, that is, the image derivatives are computed with pixels at a 
distance, not only with adjacent pixels. In a continuous setting, Buades et al. [2006] have characterized 
that for small neighbor­hoods , the Yaroslavsky .lter (i.e., a bilateral .lter using a box function as 
spatial weight) behaves as the Perona-Malik .lter, and in particular, that it su.ers from shock formation, 
a.k.a. over-sharpening, that creates aliased edges from smooth ones. The proof of Buades et al. is based 
on an asymptotic study which relies on the fact that the image is well approximated by its second-order 
Taylor expansion. Thus, their result holds for any neighborhood as long as it covers a su.ciently regular 
area such a skin or sky region. We refer to Section 6 for more details. In order to prevent over-sharpening, 
Buades et al. proposed a variant to the bilateral .lter, the regression .lter which we discuss in Section 
6.4. 5.4 Linear Filtering Paris and Durand [2006a] have demonstrated that the bilateral .lter corresponds 
to a Gaussian convolution in a higher-dimensional, homogeneous space. Paris and Durand consider the S×R 
domain and represent an image I as a 3D grid G: . . I(px,py),1 if r = I(px,py) G(px,py,r)= (21).(0,0) 
otherwise With this representation, they demonstrate that bilateral .ltering exactly corresponds to convolving 
G with a 3D Gaussian whose parameters are (sssrG=G . Gss,ss,sr ,s,s): .They show that the bilateral .lter 
output is BF [I](px,py)= G px,py,I(px,py) . Thisprocessis illustrated in Figure 27. This result enables 
the use of classical techniques from linear image processing to speed up the computation (Section 4.5). 
 range (.)  . x . x space (x) 1 0.8 0.6 0.4 0.2 0  . x   . x 0 20 40 60 80 100 120 Figure 27: 
Overview on a 1D signal of the reformulation of the bilateral .lter as a linear con­volution in a homogeneous, 
higher-dimensional space. Reproduced from [Paris and Durand, 2006a]. 5.5 Image Manifold Sochen et al. 
[1998] have introduced the notion of image manifolds where an image I is represented by a manifold M 
embedded in S×R: (px,py) .S.M(px,py)= px,py,I(px,py) .S×R (22) With this representation, Barash [2002; 
2004] has demonstrated that bilateral .ltering is based on the Euclidean distance of S×Rinstead of the 
manifold geodesic distance. Note that Paris and Durand [2006a] used a similar metric but in a signal-processing 
context (Section 5.4). Sochen et al. [2001] have also shown that the bilateral .lter is an approximation 
to Gaussian .ltering using the geodesic metric (i.e. using distances measured on the image manifold M) 
when the Gaussian kernel is small.  6 Mathematical Analysis of the Bilateral Filter using PDEs The goal 
of this section is to revisit the method of bilateral .lter in a continuous setting to show its relationship 
with another famous branch of restauration approaches: partial di.eren­tial equations (PDE). Interestingly, 
we will show that bilateral .lter can be seen under certain conditions, as an approximation of the well-known 
Perona Malik equation. Section 6.1 intro­duces the continuous setting and notations. Section 6.2 presents 
the Gaussian .ltering and its low-pass property. Section 6.3 demonstrates the link between bilateral 
.lter and nonlinear PDE-based approaches. Finally Section 6.4 propose a way to overcome the classical 
problem of staircase e.ect. 6.1 From Discrete to Continuous Setting Until now, we considered an image 
as a dicrete set of pixel. Instead, in this section, we will need to consider an image de.ned continuously, 
i.e., an analog image where space is no longer discretized. The motivation becomes clear when one need 
for instance to de.ne a notion of derivative. Formally, keeping the same notations, this introduces little 
changes in the formulation of the bilateral .lter. The only di.erence here is that sums are replaced 
by integrals: Positions p and q now vary on a continuous domain. 6.2 Linear Filtering with Gaussian 
Blur Filtering Let us .rst revisit the linear Gaussian blur presented in Section 2 in a discrete setting. 
For a given image I, one de.ned the image GB[I]by GB[I](p)= (Gs . I)(p)=Gs(|| p - q|| )I(q) dq, . p. 
(23) O In Section 2, we said that Gaussian .ltering was a low-pass .lter. Now, this property can easily 
be proved in the continous setting, in the frequency domain. Let us de.ne the Fourier transform by F 
[I](w)=I(p)exp (- i w · p)dp, R2 where w . R2.It is well known that F [Gs . I](w)= F [Gs](w)F [I](w), 
and since || w|| 2 F [Gs](w)=exp- , 2/s2 it follows that || w|| 2 F [Gs . I](w)=exp-F [I](w), 2/s2 i.e., 
the convolution by a Gaussian is a low-pass .lter that inhibits high frequencies (oscillations in the 
space domain). This explains why edges are also lost. 6.3 Nonlinear Filtering with Neighborhood Filters 
 Buades et al. [2004; 2005a] revisit the notion of bilateral .lter, that the designate by neigh­borhood 
.ltering. Here the notion of neighborhood must be understood broadly: neighboring pixels, neighboring 
or similar intensities, or neighboring neighborhoods. Each of these meanings will correspond to a speci.c 
.lter. Interestingly, the authors also proved the link between these .lters and well-known PDEs such 
as the heat equation and the Perona Malik equation. A general neighborhood .ltering can be described 
as follows. Let I be an image to be denoised and let wss : R+ . R+ and wsr : R+ . R+ be two functions 
whose roles will be to enforce respectively photometric and geometric locality (in Section 2, w and w 
are both Gaussian kernels). Parameters ss and sr will measure the amount of .ltering for the image I. 
The .ltered image at scale (sr,ss)is given by 1 BF[I](p)= wsr (| I(q) - I(p)| ) wss (|| p - q|| )I(q)dq, 
W(p) S where W(p) is a normalization factor W(p)= wsr (| I(q) - I(p)| ) wss (|| p - q|| )dq. S For simplicity 
we suppose that the image has been extended from the domain image S (a rectangle) to the whole of R2, 
bysymmetryand periodicity. With this formalism we can easily recover the classical spatial linear Gaussian 
.ltering by - t2 choosing wsr = 1and wss (t)=exp ss 2 . Now let us consider bilateral .lters. As mentioned 
before, the idea is to take an average of the values of pixels that are both close in gray level value 
and spatial distance. Of course many choices are possible for the kernels wsr and wss . Classical choices 
are t2 wsr (t)=exp - sr 2 and t2 wss (t)=exp - or wss (t)= .B(p,ss)(t), 2 ss where .B(p,ss) denotes the 
characteristic function of the ball of center p and radius ss.With the former choice of wss , we get 
the SUSAN .lter [Smith and Brady, 1997] or the bilateral .lter [Tomasi and Manduchi, 1998] (see also 
Section 2): 1 | I(q) - I(p)| 2 || q - p|| 2 Sss,sr [I](p)= exp - exp - I(q)dq. 22 W(p) sr ss R2   
  With the latter choice of wss , we recover the Yaroslavsky .lter 1 | I(q) - I(p)| 2 Yss,sr [I](p)= 
exp - I(q)dq. (24) 2 W(p) sr B(p,ss) The SUSAN and Yaroslavsky .lters have similar behaviors. Inside 
a homogeneous region, the gray level values slightly .uctuate because of the noise. Nearby sharp boundaries, 
between a dark and a bright region, both .lters compute averages of pixels belonging to the same region 
as the reference pixel: edges are not blurred. Interestingly, the estimation of the residue Iss,sr (p) 
- I(p) gives some analogies with well­known PDEs. Theorem 6.1 Suppose I .C 2(S ) and let ss, sr,and a> 
0 such that ss,sr . 0 and sr = O(ssa). Let us consider the continuous function t 1 texp(- t2)1 g(t)= 
 for t =0,g(0) = where E(t)= exp(- s 2)ds. 3 E(t)6 0 Let f be the continuous function g(t)1 1 f(t)=3g(t)+3- 
, for t=0 and f(0) = . t2 2t2 6 Then for x.S , .I(p)2 if a<1, Yss,sr [I](p) - I(p) ss , 6 ss 2 if a 
=1, Ys[I](p) - I(p) g( s|| DI(p)|| )ITT (p)+ f( s|| DI(p)|| )INN (p) s, s,sr sr sr s 2 if 1 <a< 3 , 
Yss,sr [I](p) - I(p) g(ss 1- a || DI(p)|| )[ITT (p)+3INN (p)] ss , 2 DI. DI. DI DI where ITT = D2u and 
INN = D2u . || DI|| ,|| DI|| || DI|| ,|| DI|| We refer to [Buades et al., 2005a] for the proof of the 
theorem. It is not di.cult, somewhat technical, and relies on a Taylor expansion of I(q) and the exponential 
function. More inter­esting is the interpretation of this theorem. For a ranging from 1 to 3 an iterated 
procedure 2 of the Yaroslavsky .lter behaves asymptotically as an evolution PDE involving two terms DI.(p) 
respectively proportional to the direction T = , which is tangent to the level passing || DI(p)||DI(p) 
through p and to the direction N = , which is orthogonal to the level passing through || DI(p)|| p. In 
fact, we may write Yss,sr [I](p) - I(p) 2 = c1ITT + c2INN . ss The .ltering or enhancing properties of 
the model depend on the sign of c1 and c2. Following Theorem 6.1, we have: [I](p)- I(p).I(p) If a<1, 
then Yss,sr 2 , which corresponds to a Gaussian .ltering. ss 6 If a = 1, the neighborhood .lter acts 
as a .ltering/enhancing algorithm. Since the func­tion g is positive (and decreasing) there is always 
a di.usion in the tangent direction, but since the function I can take positive or negative values, we 
may have .ltering/enhancing sr e.ects depending of the values of || DI(p)|| . For example, if || DI(p)|| 
>a,where a is ss such that I(a) = 0, then we get an enhancing e.ect. Let us remark since g(t) . 0as t 
.8 , points with large gradient are preserved. If 1 <a< 3 ,then ss tends to in.nity and g( ss || DI|| 
) tends to zero and consequently 2 sr sr the original image is hardly deteriorated. Finally, let us observe 
that when a = 1, the Yaroslavsky .lter behaves asymptotically like the Perona Malik equation .I =div( 
c( |. I(t, x)| 2) . I(t, x)) = c(|| DI|| 2)ITT + b(|| DI|| 2)INN, (25) .t where c :[0, +8 [. ]0, +8 [ 
is a smooth decreasing function and b(s)=2sc.(s)+ c(s). By v choosing c(s)= g(s ) in (25) we get .I = 
g(|| DI|| 2)ITT + h(|| DI|| 2)INN, .t with h(s)= g(s)+ sg.(s). We have h(s)= f(s) but the coe.cients 
in the tangent direction for the Perona Malik equation and the Yaroslavsky .lter are equal, and the functions 
h and f have the same behavior. Therefore both models share the same qualitative properties, which can 
be observed in Figure 29. In particular, the staircase e.ect appears. 6.4 How to Suppress the Staircase 
E.ect? When a = 1, the Yaroslavsky .lter (24) can create unexpected features such as arti.cial contours 
inside .at zones, also called the staircase e.ect (Figure 29). The main reason is the di.culty to choose 
an appropriate threshold for the gradient. The origin of the staircase e.ect can be explained with a 
1-D convex increasing signal (respectively a 1-D increasing concave signal) (Figure 28). For each p,the 
number of points q such that I(p)- h<I(q) = I(p) is larger (respectively smaller) than the number of 
points satisfying I(p) = I(q) = I(p)+h. Thus, the average value Yss,h is smaller (respectively larger) 
than I(p). Since edges correspond to in.ection points (i.e., points where I.. = 0), the signal is enhanced 
at in.ection points; the discontinuities become more marked. To overcome this di.culty, Buades et al. 
[Buades et al., 2005a] introduced an intermediate regression correction in order to better approximate 
the signal locally. For every p in a 2-D image, one searches for a triplet (a, b, c) minimizing w(p, 
q)(I(q) - aq1 - bq2 - c)2dq, (26) B(p,ss) where w(p, q)= exp -|I(q)-I(p)|2 , and then replacing I(p)by 
(apx - bpy - c). Let us denote sr 2 this improved version of the original Yaroslavsky .lter by Lqss,sr 
. Theorem 6.2 Suppose I . C2(S ),and let ss,h > 0 be such that ss,sr . 0 and O(ss)= e-8te-tE(t)+2E(t)2 
O(s).Let g be the continuous function de.ned by g(0) = 1 and g(t)= 8t2-t22 , r6 t2(4E(t)2-8te-t2 E(t)) 
t 2 for t 0 e-s=0,where E(t)=ds.Then 1 ss 2 Lyss,sr I(p) - I(p) I.. + g || DI|| I.. ss . (27) 6 sr According 
to Theorem 6.2, the enhancing e.ect has disappeared; the coe.cient in the normal direction is now always 
positive and decreasing. When the gradient is large, the weighting function in the normal direction tends 
to 0 and the image is .ltered only in the tangent direction. Figure 28 shows how regression can improve 
the results.   u(x+h) u(x)+v u(x) u(x) u(x-h) u(x)-v  x-h xx+h x-h x x+h Yaroslavsky .lter Linear 
regression Original image Bilateral .lter result Correction of staircase e.ect Figure 29: The staircase 
e.ect can be eliminated with regression (see Section 6.4). 7 Extensions of Bilateral Filtering 7.1 About 
This Section This section describes several extensions to the bilateral .lter. Two main directions have 
been followed. First, variants have been developed to better handle gradients by taking the slope of 
the signal into account (Section 7.2). Second, bilateral .ltering has been extended to handle several 
images in order to better control the way edges are detected (Section 7.3). 7.2 Accounting for the Local 
Slope Humans consistently identify at least three visually distinctive image features as edges or boundaries: 
a sharp, step-like intensity change, a sharp, ridge-or valley-like gradient change, or both. The bilateral 
.lter is particularly good at preserving step-like edges, because the range domain R .lter averages together 
all similar values in within the neighborhood space domain, and also assigns tiny to dissimilar values 
on the opposite side of the step, as shown in Figure 3 helps maintain the step-like changes without smoothing. 
Several researchers [Elad, 2002; Choudhury and Tumblin, 2003; Buades et al., 2005c] have proposed extensions 
to the bilateral .lter to improve edge-preserving results for ridge-and valley-like edges as well. As 
explained by Elad [2002] most noted that the bilateral .lter smoothes images towards a piecewise constant-intensity 
approximation of the original signal, and instead, each proposes smoothing towards piecewise constant-gradient 
(or low curvature) results instead. 7.2.1 Trilateral Filter Sharp changes in gradients and large, high-gradient 
areas degrade the desirable smoothing abilities of the bilateral .lter. As shown for one image scan-line 
in Figure 30(b), we can approximate the extent of the combined spatial and range .lters as a rectangle 
centered around each input pixel: position within this rectangle sets the weight assigned to all its 
neighboring pixels. At ridge-like or valley-like edges, gradients change abruptly but intensities do 
not, as shown in Figure 31 feature (1). Applying the bilateral .lter here is troublesome, because the 
rectangular .lter extent encloses pixels that span the peak of the ridge or valley, and the .lter blends 
these intensities to form a blunt feature instead of the sharp, clean edge with disjoint gradients. High 
gradient regions between ridge-or valley-like edges also reduce the bilateral .lter s e.ectiveness. As 
shown in Figure 30(b) and Figure 31 feature (2) the spatial .lter extent (the box width) has little e.ect, 
as only a narrow portion of the input signal falls within the box, and the range .lter s extent (box 
height) dominates the .ltering. Figure 31 feature (3) also shows that applying the bilateral .lter near 
sharply peaked valley­or ridge-like features may permit the spatial extent (box width) to include disjoint 
portions of the input signal, averaging together image regions that may belong to unrelated objects in 
the image. The trilateral .lter introduced by Choudhury and Tumblin [Choudhury and Tumblin, 2003] addressed 
these problems by combining modi.ed bilateral .lters with a pyramid-based method to limit .lter extent. 
First, they applied a bilateral .lter to the image gradients to help estimate the slopes any separate 
image regions. Using these slopes, they tilt the .lter extent of a bilateral .lter applied to image intensity; 
this a.ne transform of the range .lter, as shown in Figure 30(c), restores the e.ectiveness of the spatial 
.lter term. Finally, for each output pixel, they limit the extent of this tilted bilateral .lter to a 
connected set of pixel that share similar .ltered-gradient values. To reduce the substantial computing 
time required to .nd these connected components, they describe a pyramid-like structure suitable for 
fast evaluation. They also automatically set all but two of the parameters of their .ltering method, 
so that user control resembles the bilateral .lter s two parameters. When applied to tone mapping or 
mesh fairing, the the trilateral .lter results in Figures ref.g:trilat-HDRandMesh are visibly comparable 
or better than the bilateral .lter alone, but these improvements come at a high computational cost. 
7.2.2 Regression Filter Buades et al. [2006] have demonstrated that replacing the mean of neighboring 
pixels by a linear regression prevents over-sharpening. Each pixel is assigned the same weight as with 
a classical bilateral .lter but instead of averaging their value, one computes a linear regression Figure32:large,smoothlyvariedgradientscancause 
stair-stepping inisotropicdi.usion, andweaksmoothingintheBilateral.lter. Higher-orderPDEs(e.g. LCIS)andbilateral 
variantsthatsmoothtowardspiecewiselinearresultsformstairstepsingradientsinstead. ofthepixelvalues, i.e. 
approximateslocallytheimagebyaplane. Theoutputofthe.lteris theplanevalueatthepixellocation. Section6.4providesmoredetailsonthis.lter. 
Elad[2002]proposestoaccountfortheimage slope bycomparingtheintensityofthe .lteredpixelwiththeaverageofanotherpixelanditssymmetricpoint: 
r 7.2.3 Symmetric Bilateral Filter s 1 Gs(|| p - q|| ) Gs SBF [I] (|| Ip - Is|| ) Is (28) = p Wp q.S 
()||-||GGpqssr s where Is is the average between the pixel q and its symmetric with respect to p,that 
is: 1 Is = I(q)+ I(2p - q) . As far as we know, the performance of this extension is unclear 2 since 
it has not been extensively tested.  7.3 Using Several Images 7.3.1 Cross and Joint Bilateral Filter 
Eisemann and Durand [2004] and Petschnigg et al. [2004] introduced simultaneously the cross bilateral 
.lter,also known as the joint bilateral .lter, a variant of the bilateral .lter that decouples the notion 
of edges to preserve from the image to smooth. Given an image I,the cross bilateral .lter smoothes I 
while preserving the edges of a second image E. In practice, the range weight is computed using E instead 
of I: 1 CBF [I,E] (Ep - Eq) Iq, Gss = p Wp q.S with Wp (Ep - Eq). = q.S ()||-|| Gpqsr Figure 33 shows 
a simple use of cross bilateral .lter to .lter a low-light picture. (a) (b) (c) (d)  7.3.2 Dual Bilateral 
Filter Bennett et al. [2007] introduced dual bilateral .ltering as a variant of bilateral .ltering and 
cross bilateral .ltering. As the cross bilateral .lter, the dual bilateral .lter takes two images I and 
J as input and .lters I1. The di.erence is that both I and J are used to de.ned the edges whereas the 
cross bilateral .lter uses only J. The dual bilateral is de.ned by: 1 DBF[I]p = Gss (|| p - q|| ) GsI 
(|| Ip - Iq|| ) GsJ (|| Jp - Jq|| ) Iq (29)Wp q.S The advantage of this formulation is any edge visible 
in I or J is taken into account. Ben­nett et al. have demonstrated the usefulness of this strategy in 
the context of low-light imaging where I is a classical RGB video stream and J comes infra-red cameras. 
The infra-red cam­era captures more edges but lacks the colors of a standard RGB camera. In this context, 
the strength of dual bilateral .ltering is that the noise properties of I and J can be accounted for 
separately by setting sI and sJ independently. From a formal standpoint, the dual bilateral .lter can 
be interpreted as a normal bi­lateral .lter based on extended range data (I,J), that is, the channels 
of I are glued to those of J to form a single image with more channels. The range weight is then a classical 
one except that it involves higher-dimensional data. A minor di.erence with the formulation of Bennett 
et al. is that the J data are .ltered as well, but one can discard them if needed to obtain the exact 
same result. Acknowledgement The work of Sylvain Paris and Fr´edo Durand was supported by a Na­tional 
Science Foundation CAREER award 0447561 Transient Signal Processing for Realistic Imagery, an NSF Grant 
No. 0429739 Parametric Analysis and Transfer of Pictorial Style, and a grant from Royal Dutch/Shell Group. 
Fr´edo Durand acknowledges a Microsoft Re­search New Faculty Fellowship and a Sloan Fellowship. Jack 
Tumblin s work was supported in part by National Science Foundation grants NSF-IIS 0535236 and NSF-SGER 
0645973. He also acknowledges and thanks Adobe Systems, Inc. for their support of his research by two 
tax-deductible, unrestricted gifts to support research on topics in computational photography.  References 
M. Aleksic, M. Smirnov, and S. Goma. Novel bilateral .lter approach: Image noise reduction with sharpening. 
In Proceedings of the Digital Photography II conference, volume 6069. SPIE, 2006. V. Aurich and J. Weule. 
Non-linear gaussian .lters performing edge preserving di.usion. In Proceedings of the DAGM Symposium, 
1995. S. Bae, S. Paris, and F. Durand. Two-scale tone management for photographic look. ACM Transactions 
on Graphics, 25(3):637 645, 2006. Proceedings of the SIGGRAPH confer­ence. D. Barash. A fundamental 
relationship between bilateral .ltering, adaptive smoothing and the nonlinear di.usion equation. IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 24(6):844, 2002. D. Barash and D. Comaniciu. 
A common framework for nonlinear di.usion, adaptive smooth­ing, bilateral .ltering and mean shift. Image 
and Video Computing, 22(1):73 81, 2004. B. E. Bayer. Color imaging array. US Patent 3971065, 1976. E. 
P. Bennett and L. McMillan. Video enhancement using per-pixel virtual exposures. ACM Transactions on 
Graphics, 24(3):845 852, July 2005. Proceedings of the SIGGRAPH conference. E. P. Bennett, J. L. Mason, 
and L. McMillan. Multispectral bilateral video fusion. IEEE Transactions on Image Processing, 16(5):1185 
1194, May 2007. M. J. Black, G. Sapiro, D. H. Marimont, and D. Heeger. Robust anisotropic di.usion. IEEE 
Transactions on Image Processing, 7(3):421 432, March 1998. A. Buades, B. Coll, and J. Morel. On image 
denoising method. Technical report, CMLA, 2004. A. Buades, B. Coll, and J. Morel. Neighborhood .lters 
and PDE s. Technical Report 04, CMLA, 2005a. A. Buades, B. Coll, and J.-M. Morel. A review of image denoising 
algorithms, with a new one. Multiscale Modeling and Simulation, 4(2):490 530, 2005b. A. Buades, B. Coll, 
and J.-M. Morel. Neighborhood .lters and PDE s. Technical Report 2005-04, CMLA, 2005c. A. Buades, B. 
Coll, and J.-M. Morel. The staircasing e.ect in neighborhood .lters and its solution. IEEE Transactions 
on Image Processing, 15(6):1499 1505, 2006. F. Catt´e, P.-L. Lions, J.-M. Morel, and T. Coll. Image selective 
smoothing and edge detection by nonlinear di.usion. SIAM Journal of Numerical Analysis, 29(1):182 193, 
February 1992. J. Chen, S. Paris, and F. Durand. Real-time edge-aware image processing with the bilat­eral 
grid. ACM Transactions on Graphics, 26(3), 2007. Proceedings of the SIGGRAPH conference. H. Chong, S. 
Gortler, and T. Zickler. A perception-based color space for illumination-invariant image processing. 
ACM Transactions on Graphics, 27(3), 2008. Proceedings of the SIG-GRAPH conference. P. Choudhury and 
J. E. Tumblin. The trilateral .lter for high contrast images and meshes. In Proceedings of the Eurographics 
Symposium on Rendering, 2003. D. DeCarlo and A. Santella. Stylization and abstraction of photographs. 
In Proceedings of the SIGGRAPH conference, 2002. F. Durand and J. Dorsey. Fast bilateral .ltering for 
the display of high-dynamic-range images. ACM Transactions on Graphics, 21(3), 2002. Proceedings of the 
SIGGRAPH conference. E. Eisemann and F. Durand. Flash photography enhancement via intrinsic relighting. 
ACM Transactions on Graphics, 23(3), July 2004. Proceedings of the SIGGRAPH conference. M. Elad. On the 
bilateral .lter and ways to improve it. IEEE Transactions On Image Processing, 11(10):1141 1151, October 
2002. M. Elad. Retinex by two bilateral .lters. In Proceedings of the Scale-Space conference, 2005. R. 
Fattal, M. Agrawala, and S. Rusinkiewicz. Multiscale shape and detail enhancement from multi-light image 
collections. ACM Transactions on Graphics, 26(3), 2007. Proceedings of the SIGGRAPH conference. S. Fleishman, 
I. Drori, and D. Cohen-Or. Bilateral mesh denoising. ACM Transactions on Graphics, 22(3), July 2003. 
Proceedings of the SIGGRAPH conference. S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, 
and the Bayesian restora­tion of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 
6(6): 721 741, 1984. G. Gimel farb. Image Textures and Gibbs Random Fields. Kluwer Academic Publishers, 
1999. F. R. Hampel,E. M.Ronchetti, P. M.Rousseeuw, and W.A. Stahel. Robust Statistics The Approach Based 
on In.uence Functions. Wiley Interscience, 1986. ISBN 0-471-73577-9. P. J. Huber. Robust Statistics. 
Probability and Statistics. Wiley-Interscience, February 1981. L. Itti and C. Koch. Computational modeling 
of visual attention. Nature Reviews Neuro­science, 2001. T. Jones, F. Durand, and M. Zwicker. Normal 
improvement for point rendering. IEEE Computer Graphics &#38; Applications, 2004. T. R. Jones, F. Durand, 
and M. Desbrun. Non-iterative, feature-preserving mesh smooth­ing. ACM Transactions on Graphics, 22(3), 
July 2003. Proceedings of the SIGGRAPH conference. E. A. Khan, E. Reinhard, R. Fleming, and H. Bueltho.. 
Image-based material editing. ACM Transactions on Graphics, 25(3), 2006. Proceedings of the ACM SIGGRAPH 
conference. J. J. Koenderink and A. J. Van Doorn. The structure of locally orderless images. International 
Journal of Computer Vision, 31(2-3), 1999. J. Kopf, M. Uyttendaele, O. Deussen, and M. Cohen. Capturing 
and viewing gigapixel images. ACM Transactions on Graphics, 26(3), 2007. Proceedings of the SIGGRAPH 
conference. S. Li. Markov Random Field Modeling in Computer Vision. Springer Verlag, 1995. C. Liu, W. 
T. Freeman, R. Szeliski, and S. Kang. Noise estimation from a single image. In Proceedings of the conference 
on Computer Vision and Pattern Recognition. IEEE, 2006. B. D. Lucas and T. Kanade. An iterative image 
registration technique with an application to stereo vision. In Proceedings of the Image Understanding 
Workshop. DARPA, 1981. S. Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1999. ISBN : 0-12-466606-X. 
A. Miropolsky and A. Fischer. Reconstruction with 3D geometric bilateral .lter. In Proceed­ings of the 
Symposium on Solid Modeling and Applications. ACM, 2004. P. Mr´azek, J. Weickert, and A. Bruhn. Geometric 
Properties from Incomplete Data, chapter On Robust Estimation and Smoothing with Spatial and Tonal Kernels. 
Springer, 2006. D. A. Murio. The Molli.cation Method and the Numerical Solution of Ill-Posed Problems. 
Wiley-Interscience, 1993. B. M. Oh, M. Chen, J. Dorsey, and F. Durand. Image-based modeling and photo 
editing. In Proceedings of the SIGGRAPH conference. ACM, 2001. S. Paris and F. Durand. A fast approximation 
of the bilateral .lter using a signal processing approach. In Proceedings of the European Conference 
on Computer Vision, 2006a. S. Paris and F. Durand. A fast approximation of the bilateral .lter using 
a signal processing approach. Technical Report MIT-CSAIL-TR-2006-073, Massachusetts Institute of Technol­ogy, 
2006b. S. Paris, H. Brice no, and F. Sillion. Capture of hair geometry from multiple images. ACM Transactions 
on Graphics, 23(3), July 2004. Proceedings of the SIGGRAPH conference. P. Perona and J. Malik. Scale-space 
and edge detection using anisotropic di.usion. IEEE Transactions Pattern Analysis Machine Intelligence, 
12(7):629 639, July 1990. G. Petschnigg, M. Agrawala, H. Hoppe, R. Szeliski, M. Cohen, and K. Toyama. 
Digital photography with .ash and no-.ash image pairs. ACM Transactions on Graphics, 23(3), July 2004. 
Proceedings of the SIGGRAPH conference. T. Q. Pham. Spatiotonal adaptivity in Super-Resolution of Undersampled 
Ima ge Sequences. PhD thesis, Delft University of Technology, 2006. T. Q. Pham and L. J. van Vliet. Separable 
bilateral .ltering for fast video preprocessing. In International Conference on Multimedia and Expo. 
IEEE, 2005. R. Ramanath and W. E. Snyder. Adaptive demosaicking. Journal of Electronic Imaging,12 (4):633 
642, 2003. P. Sand and S. Teller. Particle video: Long-range motion estimation using point trajectories. 
In Proceedings of the Computer Vision and Pattern Recognition Conference, 2006. S. M. Smith and J. M. 
Brady. SUSAN a new approach to low level image processing. International Journal of Computer Vision, 
23(1):45 78, May 1997. N. Sochen, R. Kimmel, and R. Malladi. A general framework for low level vision. 
IEEE Transactions in Image Processing, 7:310 318, 1998. N. Sochen, R. Kimmel, and A. M. Bruckstein. Di.usions 
and confusions in signal and image processing. Journal of Mathematical Imaging and Vision, 14(3):237 
244, 2001. C. Tomasi and R. Manduchi. Bilateral .ltering for gray and color images. In Proceedings of 
the International Conference on Computer Vision, pages 839 846. IEEE, 1998. J. Tumblin and G. Turk. Low 
curvature image simpli.ers (LCIS): A boundary hierarchy for detail-preserving contrast reduction. In 
Proceedings of the SIGGRAPH conference.ACM, 1999. J. van de Weijer and R. van den Boomgaard. Local mode 
.ltering. In Proceedings of the conference on Computer Vision and Pattern Recognition, 2001. J. van de 
Weijer and R. van den Boomgaard. On the equivalence of local-mode .nding, robust estimation and mean-shift 
analysis as used in early vision tasks. In Proceedings of the International Conference on Pattern Recognition, 
2002. C. C. Wang. Bilateral recovering of sharp edges on feature-insensitive sampled meshes. IEEE Transactions 
on Visualization and Computer Graphics, 12(4):629 639, 2006. L. Wang, L. Wei, K. Zhou, B. Guo, and H.-Y. 
Shum. High dynamic range image hallucination. In Proceedings of the Eurographics Symposium on Rendering, 
2007. G. S. Watson. Statistics on spheres. John Wiley and Sons, 1983. B. Weiss. Fast median and bilateral 
.ltering. ACM Transactions on Graphics, 25(3):519 526, 2006. Proceedings of the SIGGRAPH conference. 
H. Winnem¨oller, S. C. Olsen, and B. Gooch. Real-time video abstraction. ACM Transactions on Graphics, 
25(3):1221 1226, 2006. Proceedings of the SIGGRAPH conference. W. C. K. Wong, A. C. S. Chung, and S. 
C. H. Yu. Trilateral .ltering for biomedical images. In Proceedings of the International Symposium on 
Biomedical Imaging. IEEE, 2004. J. Xiao, H. Cheng, H. Sawhney, C. Rao, and M. Isnardi. Bilateral .ltering-based 
optical .ow estimation with occlusion detection. In Proceedings of the European Conference on Computer 
Vision, 2006. L. P. Yaroslavsky. Digital Picture Processing. An Introduction. Springer Verlag, 1985. 
K. Yoon and I. Kweon. Adaptive support-weight approach for correspondence search. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 28(4), 2006. Q. Y´ang, R. Yang, H. Stew´enius, and D. Nist´er. 
Stereo matching with color-weighted correla­tion, hierarchical belief propagation and occlusion handling. 
In Proceedings of the conference on Computer Vision and Pattern Recognition, 2006. Q. Y´ang, R. Yang, 
J. Davis, and D. Nist´er. Spatial-depth super resolution for range images. In Proceedings of the conference 
on Computer Vision and Pattern Recognition, 2007. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation CAREER</funding_agency>
			<grant_numbers>
				<grant_number>0447561</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>NSF</funding_agency>
			<grant_numbers>
				<grant_number>0429739</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>NSF-SGER</funding_agency>
			<grant_numbers>
				<grant_number>0645973</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
	<section>
		<section_id>1401135</section_id>
		<sort_key>30</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Advanced global illumination using photon mapping]]></section_title>
		<section_page_from>2</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098593</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098594</person_id>
				<author_profile_id><![CDATA[81365591640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jansen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098595</person_id>
				<author_profile_id><![CDATA[81100639133]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Donner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401136</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>112</pages>
		<display_no>2</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Advanced global illumination using photon mapping]]></title>
		<page_from>1</page_from>
		<page_to>112</page_to>
		<doi_number>10.1145/1401132.1401136</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401136</url>
		<abstract>
			<par><![CDATA[<p>Photon mapping provides a practical way of efficiently simulating global illumination including interreflections, caustics, color bleeding, participating media and subsurface scattering in scenes with complicated geometry and advanced material models. This class will provide the insight necessary to efficiently implement and use photon mapping to simulate global illumination in complex scenes. The presentation will briefly cover the fundamentals of photon mapping including efficient techniques and datastructures for managing large numbers of rays and photons. In addition, we will describe how to integrate the information from the photon maps in shading algorithms to render global illumination. A large portion of the class will be dedicated to advanced techniques for photon mapping. In addition to a fast recap of introductory material, we will describe more recent developments including efficient methods for using photon mapping in scene with participating media and subsurface scattering.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098596</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098597</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098598</person_id>
				<author_profile_id><![CDATA[81100639133]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Donner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Jensen95a} Henrik Wann Jensen and Niels J&#248;rgen Christensen. "Photon maps in Bidirectional Monte Carlo Ray Tracing of Complex Objects". <i>Computers &amp; Graphics</i> 19 (2), pages 215--224, 1995. The first paper describing the photon map. The paper suggested the use of a mixture of photon maps and illumination maps, where photon maps would be used for complex surfaces such as fractals. 49, 50, 63, 64]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Jensen95b} Henrik Wann Jensen. "Importance driven path tracing using the photon map". <i>Rendering Techniques '95 (Proceedings of the Sixth Eurographics Workshop on Rendering)</i>, pages 326--335. Springer Verlag, 1995. Introduces the use of photons for importance sampling in path tracing. By combining the knowledge of the incoming flux with the BRDF it is possible to get better results using fewer sample rays. 72]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Jensen95c} Henrik Wann Jensen and Niels J&#248;rgen Christensen. "Efficiently Rendering Shadows using the Photon Maps". In <i>Proceedings of Compugraphics'95</i>, pages 285--291, Alvor, December 1995. Introduces the use of shadow photons for an approximate classification of the light source visibility in a scene. 70]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>241056</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Jensen96a} Henrik Wann Jensen. "Rendering caustics on non-Lambertian surfaces". <i>Proceedings of Graphics Interface'96</i>, pages 116--121, Toronto, May 1996 (also selected for publication in Computer Graphics Forum, volume 16, number 1, pages 57--64, March 1997). Extension of the photon map method to render caustics on non-Lambertian surfaces ranging from diffuse to glossy. 55, 58]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Jensen96b} Henrik Wann Jensen. "Global illumination using photon maps". <i>Rendering Techniques '96 (Proceedings of the Seventh Eurographics Workshop on Rendering)</i>, pages 21--30. Springer Verlag, 1996. Presents the global illumination algorithm using photon maps. A caustic and a global photon map is used to optimize the rendering of global illumination including the simulation of caustics. 54, 67]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Jensen96c} Henrik Wann Jensen. <i>The photon map in global illumination</i>. Ph.D. dissertation, Technical University of Denmark, September 1996. An in-depth description of the photon map method based on the presentations in the published photon map papers. 59, 63, 64]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732108</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Christensen97} Per H. Christensen. "Global illumination for professional 3D animation, visualization, and special effects" (invited paper). <i>Rendering Techniques '97 (Proceedings of the Eighth Eurographics Workshop on Rendering)</i>, pages 321--326. Springer Verlag, 1997. Describes the requirements of a global illumination method in a commercial environment, and motivates the choice of the photon map method.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>731974</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Myszkowski97} Karol Myszkowski. "Lighting reconstruction using fast and adaptive density estimation techniques". <i>Rendering Techniques '97 (Proceedings of the Eighth Eurographics Workshop on Rendering)</i>, pages 321--326. Springer Verlag, 1997. Efficient techniques for filtering and visualizing photons. 63, 67]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618510</ref_obj_id>
				<ref_obj_pid>616051</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Slusallek98} Philipp Slusallek, Mark Stamminger, Wolfgang Heidrich, J.-C. Popp, and Hans-Peter Seidel. "Composite Lighting Simulations with Lighting Network". <i>IEEE Computer Graphics &amp; Applications</i>, 18(2), pages 22--31, March/April 1998. Describes a framework in which the photon map can be integrated into a radiosity simulation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Peter98} Ingmar Peter and Georg Pietrek. "Importance driven construction of photon maps." <i>Rendering Techniques '98 (Proceedings of the Ninth Eurographics Workshop on Rendering)</i>, pages 269--280. Springer Verlag, 1998. Use of importance to focus the photons where they contribute most to the visible solution. This requires an initial importance (or "importons") tracing pass from the camera before the photon tracing pass from the light sources. 49, 97, 98]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Jensen98} Henrik Wann Jensen and Per H. Christensen. "Efficient simulation of light transport in scenes with participating media using photon maps". <i>Proceedings of SIGGRAPH 98</i>, pages 311--320. ACM, 1998. Extension of the photon map method to simulate global illumination in scenes with participating media. 55, 56, 57, 65, 73]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Lange98} Thorsten Lange and Georg Pietrek. "Rendering Participating Media using the Photon Map". Technical Report no. 678, University of Dortmund, 1998. Also describes the extension of the photon map method to simulate global illumination in the presence of participating media.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383840</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Jensen99} Henrik Wann Jensen, Justin Legakis and Julie Dorsey. "Rendering of Wet Materials". <i>Proceedings of the Tenth Eurographics Workshop on Rendering</i>, pages 281--290, Granada, June 1999. Simulates subsurface scattering using the volume photon map in order to render wet materials. 85]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Dorsey99} Julie Dorsey, Alan Edelman, Henrik Wann Jensen, Justin Legakis and Hans K&#248;hling Pedersen. "Modeling and Rendering of Weathered Stone". <i>Proceedings of SIGGRAPH 99</i>, pages 223--234, 1999. Describes rendering of volumetric weathering effects in stone based on subsurface scattering optimized using the volume photon map. 85]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340504</ref_obj_id>
				<ref_obj_pid>340501</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Christensen99} Per H. Christensen "Faster Photon Map Global Illumination". <i>Journal of Graphics Tools</i>, 4(3), pages 1--10, 1999. Introduces precomputed irradiance values per photon for faster look-ups.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Jensen00} Henrik Wann Jensen. "Parallel Global Illumination using Photon Mapping". In <i>SIGGRAPH'2000, Course 30</i>, New Orleans, July 2000. Describes how to implement the photon mapping algorithm to take advantage of multiprocessor/multi-host computers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{PMAPCourse} SIGGRAPH 2000 Course Note. "A Practical Guide to Global Illumination Using Photon Maps". Previous SIGGRAPH course on photon mapping.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732120</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{Suykens00} Frank Suykens and Yves Willems. "Density control for photon maps". <i>Rendering Techniques 2000 (Proceedings of the Eleventh Eurographics Workshop on Rendering)</i>, pp. 11--22. Springer-Verlag, 2000. Introduces techniques for limiting the density of the photons in order to get a better distribution of photons. Also presents ideas for using visual importance to construct higher quality photon maps. 97]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{RPK} Ph. Bekaert and F. Suykens. <i>RenderPark, a physically based rendering tool</i>. K. U. Leuven, http://www.renderpark.be, 1996--2001. An open-source renderer that supports photon mapping.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{Jensen01} Henrik Wann Jensen. <i>Realistic Image Synthesis using Photon Mapping</i>. AK Peters, 2001 An in-depth book describing photon mapping, all the theory, and all the practical details. Includes an implementation of the photon map data structure.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Whitted80} Turner Whitted. "An improved illumination model for shaded display". <i>Communications of the ACM</i>, volume 23, number 6, pages 343--349. ACM, June 1975. The classic ray tracing paper. 24]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Arvo86} James Arvo. "Backward ray tracing". <i>Developments in ray tracing</i>, SIGGRAPH 86 seminar notes. ACM, August 1986. Introduces light ray tracing and illumination maps for computing caustics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>94788</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Glassner89} Andrew S. Glassner. <i>An introduction to ray tracing</i>. Academic Press, 1989. The standard reference on ray tracing. Still a pleasure to read. 9, 17, 20, 26, 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>124947</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Shirley91} Peter Shirley. <i>Physically Based Lightning Calculations for Computer Graphics</i>. Ph.d. thesis, University of Illinois at Urbana-Champaign, 1991. Good overview of Monte Carlo ray tracing. Also presents one of the first practical multi-pass global illumination methods.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122737</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Chen91} Eric Shenchang Chen, Holly E. Rushmeier, Gavin Miller, and Douglas Turner. "A progressive multi-pass method for global illumination". <i>Proceedings of SIGGRAPH 91</i>, pages 164--174. ACM, 1991. One of the first multi-pass global illumination methods. Uses illumination maps for caustics, radiosity for indirect light and path tracing for rendering.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{Ward92} Gregory Ward and Paul Heckbert. "Irradiance gradients". <i>Third Eurographics Workshop on Rendering</i>, pages 85--98. Eurographics, 1992. Describes the irradiance gradients method which is used for the final gathering step of the photon map method. 72]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{Pattanaik93} Sumant N. Pattanaik. <i>"Computational Methods for Global Illumination and Visualisation of Complex 3D Environments"</i>. Ph.d. Thesis, Birla Institute of Technology &amp; Science, 1993. Introduces particle tracing where photons are emitted from the light sources and stored in a mesh. 53]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{Rushmeier93} Holly Rushmeier, Ch. Patterson and A. Veerasamy. "Geometric Simplification for Indirect Illumination Calculations". <i>Proceedings of Graphics Interface '93</i>, pages 35--55, 1994. Introduces the concept of geometry simplification for the radiosity step of multipass global illumination computations.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Glassner95} Andrew S. Glassner. <i>Principles of digital image sythesis</i>. Morgan Kaufmann, 1995. Gives an excellent overview of the entire field of image synthesis. Of particular interest here is the description of Monte Carlo photon tracing and Russian roulette. 26, 27, 35, 53]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Lafortune96} Eric P. Lafortune. <i>Mathematical Models and Monte Carlo Algorithms for Physcially Based Rendering</i>. Ph.d. thesis, Katholieke University, Leuven, Belgium 1996. Good overview of Monte Carlo ray tracing techniques including bidirectional path tracing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{Dutre96} Philip Dutr&#233; and Yves D. Willems. <i>Mathematical Frameworks and Monte Carlo Algorithms for Global Illumination in Compute Graphics</i>. Ph.d. thesis, Katholieke Universiteit Leuven, 1996. Another fine overview of Monte Carlo ray tracing and photon tracing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286090</ref_obj_id>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{Ward98} Gregory Ward Larson and Rob Shakespeare. <i>Rendering with Radiance --- the art and science of lighting visualization</i>. Morgan Kaufmann, 1998. An entire book dedicated to the excellent Radiance renderer with many practical examples.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>361007</ref_obj_id>
				<ref_obj_pid>361002</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Bentley75} Jon L. Bentley. "Multidimensional binary search trees used for associative searching". <i>Communications of the ACM</i>, volume 18, number 9, pages 509--517. ACM, 1975. First paper on the kd-tree datastructure. 58, 65]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>4333</ref_obj_id>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Preparata85} Franco P. Preparata and Michael Ian Shamos. <i>Computational Geometry An Introduction</i>, Springer-Verlag, 1985 65, 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{Cormen89} Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. <i>Introduction to algorithms</i>. MIT Press, 1989. Good overview of algorithms including the heap-datastructure. 59, 65]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>150195</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Sedgewick92} Robert Sedgewick. <i>Algorithms in C++</i>. Addison-Wesley, 1992. Also good description of the heap structure, and algorithms for the median search (used in the balancing algorithm). 59, 65, 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{Ansi86} American National Standard Institute. <i>"Nomenclature and Definitions for Illumination Engineering"</i>. ANSI report, ANSI/IES RP-16-1986, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97886</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{Arvo90} James Arvo and David Kirk. "Particle Transport and Image Synthesis". <i>Computer Graphics</i>, &#60;b&#62;24&#60;/b&#62; (4), pages 53--66, 1990. 51]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166137</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{Aupperle93} Larry Aupperle and Pat Hanrahan: "A Hierarchicah Illumination Algorithm for Surfaces with Glossy Reflection". <i>Computer Graphics</i>, pages 155--162, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313802</ref_obj_id>
				<ref_obj_pid>1313340</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{Bentley79a} Jon Louis Bentley. "Multidimensional Binary Search Trees in Database Applications". <i>IEEE Trans. on Soft. Eng.</i> &#60;b&#62;5&#60;/b&#62; (4), pages 333--340, July 1979.65]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>356797</ref_obj_id>
				<ref_obj_pid>356789</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{Bentley79b} Jon Louis Bentley and Jerome H. Friedman. "Data Structures for Range Searching". <i>Computing Surveys</i> &#60;b&#62;11&#60;/b&#62; (4), pages 397--409, 1979. 65]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>355927</ref_obj_id>
				<ref_obj_pid>355921</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{Bentley80} Jon Louis Bentley, Bruce W. Weide, and Andrew C. Yao. "Optimal Expected-Time Algorithm for Closest Point Problems". <i>ACM Trans. on Math. Soft.</i>, &#60;b&#62;6&#60;/b&#62; (4), pages 563--580, dec. 1980. 65]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>772249</ref_obj_id>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Chalmers} Alan Chalmers et al. "Practical Parallel Rendering". ISBN: 1-56881-179-9, A K Peters, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Christensen93} Per Henrik Christensen, David Salesin and Tony DeRose. "A Continuous Adjoint Formulaion for Radiance Transport". Fourth Eurographics Workshop on Rendering, pages 95--104, 1993]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>243459</ref_obj_id>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{Christensen95} Per Henrik Christensen. <i>Hierarchical Techniques for Glossy Global Illumination</i>. PhD thesis, Seattle, Washington, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{Collins94} Steven Collins. "Adaptive Splatting for Specular to Diffuse Light Transport". In <i>Proceedings of the 5th Eurographics Workshop on Rendering</i>, pages 119--135, Darmstadt 1994. 63, 64]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{Cook84} Robert L. Cook. "Distributed Ray Tracing". <i>Computer Graphics</i> &#60;b&#62;18&#60;/b&#62; (3), pages 137--145, 1984. 27]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{Cook86} Robert L. Cook. "Stochastic Sampling in Computer Graphics". <i>ACM Transactions on Graphics</i> &#60;b&#62;5&#60;/b&#62; (1), pages 51--72, Jan. 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{Dutre94} Philip Dutr&#233; and Yves D. Willems. "Importance-driven Monte Carlo Light Tracing". In <i>proceedings of 5. Eurographics Workshop on Rendering</i>, pages 185--194, Darmstadt 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{Dutre95} Philip Dutre and Yves D. Willems. "Potential-Driven Monte Carlo Particle Tracing for Diffuse Environments with Adaptive Probability Density Functions". In P. M. Hanrahan and W. Purgathofer, editors, <i>Rendering Techniques '95</i>, pages 306--315, New York, NY, 1995. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>184932</ref_obj_id>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{Ebert94} David Ebert, Ken Musgrave, Darwyn Peachey, Ken Perlin and Steve Worley. <i>Texturing and Modeling: A Procedural Approach</i>. Academic Press, October 1994. 57]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{Goral84} Cindy Goral, Kenneth Torrance, Donald Greenberg, Bennet Battaile. "Modeling the Interaction of Light Between Diffuse Surfaces". <i>Computer Graphics (SIGGRAPH '84 Proceedings)</i>, volume 18, number 3, pages 213--222, July 1984, Minneapolis, Minnesota.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>643327</ref_obj_id>
				<ref_obj_pid>643323</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{Gritz96} Larry Gritz and J. K. Hahn. "BMRT: A Global Illumination Implementation of the RenderMan Standard". <i>Journal of Graphics Tools</i>, Vol. 1, No. 3, pages 29--47, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>63450</ref_obj_id>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{Hall88} Roy Hall. <i>Illumination and Color Computer Generated Imagery</i>. Springer-Verlag, 1988 50]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97895</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{Heckbert90} Paul S. Heckbert. "Adaptive Radiosity Textures for Bidirectional Ray Tracing". <i>Computer Graphics</i> &#60;b&#62;24&#60;/b&#62; (4), pages 145--154, 1990. 57]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>573687</ref_obj_id>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{Horowitz93} Ellis Horowitz, Sartaj Sahni and Susan Anderson-Freed. <i>Fundamentals of Data Structures in C</i>, Computer Science Press, 1993 65, 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311555</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{Igehy99} Homan Igehy. Tracing ray differentials. <i>Computer Graphics</i>, 33(Annual Conference Series):179--186, 1999. 34]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{Jensen93} Henrik Wann Jensen. <i>Global Illumination using Bidirectional Monte Carlo Ray Tracing</i>. M.Sc. thesis, Technical University of Denmark (in Danish), 1993. 49, 50]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{Jensen95f} Henrik Wann Jensen and Niels J&#248;rgen Christensen. "Optimizing Path Tracing using Noise Reduction Filters". In <i>Proceedings of WSCG 95</i>, pages 134--142, Plzen 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{Kajiya86} James T. Kajiya. "The Rendering Equation". <i>Computer Graphics</i> &#60;b&#62;20&#60;/b&#62; (4), pages 143--149, 1986. 28]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>7050</ref_obj_id>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{Kalos86} M. Kalos and P. Whitlock. <i>Monte Carlo Methods, Volume 1: Basics</i>. J. Wiley, New York, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275469</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{Keller96} Alexander Keller. "Quasi-Monte Carlo Radiosity". In proceedings of <i>7th Eurographics Workshop on Rendering</i>, pages 102--111, Porto 1996. 48]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{Keller00} Alexander Keller and Ingo Wald. "Efficient Importance Sampling Techniques for the Photon Map". In <i>Vision Modelling and Visualization 2000</i>, pages 271--279, Saarbruecken, Germany, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{Kilauea} Kilauea, SquareUSA's rendering software with photon maps. http://www.squareusa.com/kilauea/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{Kopp99} Nathan Kopp. Personal communication. 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{Lafortune93} Eric P. Lafortune and Yves D. Willems. "Bidirectional Path Tracing". In <i>Proceedings of CompuGraphics</i>, pages 95--104, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{MegaPov00} A free ray tracer that supports photon maps. Source code and examples are available at: http://www.nathan.kopp.com/patched.htm, Mar. 2000 66]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{Nicodemus77} F. E. Nicodemus, J. C. Richmond, J. J. Hsia. I. W. Ginsberg and T. Limperis: "Geometric Considerations and Nomenclature for Reflectance". <i>National Bureau of Standards</i>, 1977 60]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130653</ref_obj_id>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{Niederreiter92} Harald Niederreiter. <i>Random Number Generation and Quasi-Monte Carlo Methods</i>, SIAM, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>200985</ref_obj_id>
				<ref_obj_pid>200972</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{Pattanaik95} S. N. Pattanaik and S. P. Mudur. "Adjoint equations and random walks for illumination computation". <i>ACM Transactions on Graphics</i>, 14(1):77--102, January 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>90803</ref_obj_id>
				<ref_obj_pid>90767</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{Pavicic90} Mark J. Pavicic. "Convenient Anti-Aliasing Filters that Minimize Bumpy Sampling". In <i>Graphics Gems I, eds.</i> Andrew S. Glassner, pages 144--146, 1990. 64]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258791</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{Pharr97} Matt Pharr, Craig Kolb, Reid Gershbein, Pat Hanrahan. "Rendering Complex Scenes with Memory-Coherent Ray Tracing". <i>Computer Graphics (SIGGRAPH '97 Proceedings)</i>, pages 101--108, August 1997, Los Angels, California. 40]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>539488</ref_obj_id>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{Rubinstein81} Reuven Y. Rubinstein. <i>Simulation and the Monte Carlo Method</i>. John Wiley &amp; Sons, 1981. 48]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>914720</ref_obj_id>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{Rushmeier88} Holly Rushmeier. <i>Realistic Image Synthesis for Scenes with Radiatively Participating Media</i>. Ph.d. thesis, Cornell University, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{Schlick93} Christophe Schlick. "Customizable Reflectance Model for Everyday Rendering". In <i>proceedings of 4. Eurographics Workshop on Rendering</i>, pages 73--84, Paris 1993. 26, 27]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>93316</ref_obj_id>
				<ref_obj_pid>93267</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{Shirley90} Peter Shirley. "A Ray Tracing Method for Illumination Calculation in Diffuse-Specular Scenes". <i>Proceedings of Graphics Interface '90</i>, pages 205--212, 1990. 53]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130761</ref_obj_id>
				<ref_obj_pid>130745</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{Shirley92} Peter Shirley. "Nonuniform Random Point Sets via Warping". <i>Graphics Gems III</i> (David Kirk ed.), Academic Press, pages 80--83, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{Shirley95} Peter Shirley; Bretton Wade; Phillip Hubbard; David Zareski; Bruce Walter and Donald P. Greenberg. "Global Illumination via Density Estimation". In "Rendering Techniques '95". Eds. P. M. Hanrahan and W. Purgathofer, <i>Springer-Verlag</i>, pages 219--230, 1995. 73]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>226151</ref_obj_id>
				<ref_obj_pid>226150</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{Shirley96} Peter Shirley; C. Wang and Kurt. Zimmerman. "Monte Carlo Techniques for Direct Lighting Calculations". <i>ACM Transactions on Graphics</i> &#60;b&#62;15&#60;/b&#62; (1), 1996. 38]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345205</ref_obj_id>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{Shirley00} Peter Shirley. "Realistic Ray Tracing". ISBN: 1-56881-110-1, A K Peters, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{S2000Course38} SIGGRAPH 2001 Course Note. "A Practical Guide to Global Illumination Using Photon Mapping".]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{S2000Course40} SIGGRAPH 2001 Course Note. "Parallel Rendering and the Quest for Realism: The 'Kilauea' Massively Parallel Ray Tracer".]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{Kilauea} SIGGRAPH 2002 Course Note. "The 'Kilauea' Massively Parallel Global Illumination Renderer".]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{Silverman86} B. W. Silverman. <i>Density Estimation for Statistics and Data Analysis</i>. Chapmann and Hall, New York, NY, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134080</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{Smits92} Brian E. Smits, James R. Arvo, and David H. Salesin. "An importance-driven radiosity algorithm". <i>Computer Graphics</i>, 26(2):273--282, July 1992. 96]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732286</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{Suykens01} Frank Suykens and Yves D. Willems. "Path Differentials and Applications". In "Rendering Techniques 2001 (Proceedings of the Twelfth Eurographics Workshop on Rendering", Eds. S. J. Gortler and K. Myszkowski. <i>Springer-Verlag</i>, pages 257--268, Londen, UK, 2001 34]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{Suykens01TR} Frank Suykens and Yves D. Willems. "Path differentials and applications". Technical Report CW307, Department of Computer Science, Katholieke Universiteit Leuven, Leuven, Belgium, May 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>731970</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{Tamstorf97} Rasmus Tamstorf and Henrik Wann Jensen. "Adaptive Sampling and Bias Estimation in Path Tracing". In "Rendering Techniques '97". Eds. J. Dorsey and Ph. Slusallek. <i>Springer-Verlag</i>, pages 285--295, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{Veach94} Eric Veach and Leonidas Guibas. "Bidirectional Estimators for Light Transport". In <i>Proceedings of the 5th Eurographics Workshop on Rendering</i>, pages 147--162, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{Veach95} Eric Veach and Leonidas Guibas. "Optimally Combinig Sampling Techniques for Monte Carlo Rendering". <i>Computer Graphics</i> &#60;b&#62;29&#60;/b&#62; (4), pages 419--428, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{Veach97} Eric Veach and Leonidas Guibas. "Metropolis Light Transport". <i>Computer Graphics</i> &#60;b&#62;31&#60;/b&#62; (3), pages 65--76, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{Volevich99} Vladimir Volevich, Karol Myszkowski, Andrei Khodulev and Edward A. Kopylov. "Perceptually-Informed Progressive Global Illumination Solution". Technical Report 99-1-002, University of Aizu, Japan, 1999. 67]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{Ward88} Greg Ward, Francis M. Rubinstein, and Robert D. Clear. "A Ray Tracing Solution for Diffuse Interreflection". <i>Computer Graphics</i> &#60;b&#62;22&#60;/b&#62; (4), pages 85--92, 1988. 30, 72]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{Ward91} Greg Ward. "Real pixels". In <i>Graphics Gems II</i>, James Arvo (ed.), <i>Academic Press</i>, pages 80--83, 1991. 38, 55]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927976</ref_obj_id>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{Zimmerman98} Kurt Zimmerman. <i>Density Prediction for Importance Sampling in Realistic Image Synthesis</i>. Ph.d. thesis, Indiana University, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Oliver Abert, Markus Geimer, and Stefan M&#252;ller. Direct and fast ray tracing of NURBS surfaces. In <i>Proceedings of the IEEE Symposium on Interactive Ray Tracing 2006</i>, pages 161--168. IEEE, 2006. 20]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>555371</ref_obj_id>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Anthony A. Apodaca and Larry Gritz. <i>Advanced RenderMan: Creating CGI for Motion Pictures</i>. Morgan Kaufmann Publishers, 2000. 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Arthur Appel. Some techniques for shading machine renderings of solids. In <i>Proceedings of the AFIPS Spring Joint Computer Conference</i>, volume 32, pages 37--45, 1968. 15, 23]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97886</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[James R. Arvo and David B. Kirk. Particle transport and image synthesis. <i>Computer Graphics (Proceedings of SIGGRAPH '90)</i>, 24(4):63--66, 1990. 90]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>272314</ref_obj_id>
				<ref_obj_pid>272313</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Ronen Barzel. Lighting controls for computer cinematography. <i>Journal of Graphics Tools</i>, 2(1):1--20, 1997. 22, 89]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn. Models of light reflection for computer synthesized pictures. <i>Computer Graphics (Proceedings of SIGGRAPH '77)</i>, 11(2):192--198, 1977. 22]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn and Martin E. Newell. Texture and reflection in computer generated images. <i>Communications of the ACM</i>, 19(10):542--547, 1976. 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Phong Bui Tuong. Illumination for computer generated pictures. <i>Communications of the ACM</i>, 18(3):311--317, 1975. 22]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Edwin E. Catmull. <i>A Subdivision Algorithm for Computer Display of Curved Surfaces</i>. PhD thesis, University of Utah, Salt Lake City, 1974. 22]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Edwin E. Catmull and James H. Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. <i>Computer-Aided Design</i>, 10(6):350--355, 1978. 21]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340504</ref_obj_id>
				<ref_obj_pid>340501</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[Per H. Christensen. Faster photon map global illumination. <i>Journal of Graphics Tools</i>, 4(3):1--10, 1999. 91]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Per H. Christensen. Photon mapping tricks. In <i>SIGGRAPH 2002 Course Note #43: A Practical Guide to Global Illumination using Photon Mapping</i>, pages 93--121, 2002. 91, 98]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231890</ref_obj_id>
				<ref_obj_pid>2231876</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Per H. Christensen. Adjoints and importance in rendering: an overview. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 9(3):329--340, 2003. 96]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[Per H. Christensen. Point clouds and brick maps for movie production. In Markus Gross and Hanspeter Pfister, editors, <i>Point-Based Graphics</i>, chapter 8.4. Morgan Kaufmann Publishers, 2007. 95]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383551</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Per H. Christensen and Dana Batali. An irradiance atlas for global illumination in complex production scenes. <i>In Rendering Techniques 2004 (Proceedings of the Eurographics Symposium on Rendering 2004)</i>, pages 133--141. Eurographics, 2004. 95]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Per H. Christensen, Julian Fong, David M. Laur, and Dana Batali. Ray tracing for the movie 'Cars'. In <i>Proceedings of the IEEE Symposium on Interactive Ray Tracing 2006</i>, pages 1--6. IEEE, 2006. 41, 42, 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[Per H. Christensen, David M. Laur, Julian Fong, Wayne L. Wooten, and Dana Batali. Ray differentials and multiresolution geometry caching for distribution ray tracing in complex scenes. <i>Computer Graphics Forum (Proceedings of Eurographics 2003)</i>, 22(3):543--552, 2003. 21, 39, 41, 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook, Loren Carpenter, and Edwin Catmull. The Reyes image rendering architecture. <i>Computer Graphics (Proceedings of SIGGRAPH '87)</i>, 21(4):95--102, 1987. 40, 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook, Thomas Porter, and Loren Carpenter. Distributed ray tracing. <i>Computer Graphics (Proceedings of SIGGRAPH '84)</i>, 18(3):137--145, 1984. 27]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[Tony D. DeRose, Michael Kass, and Tien Truong. Subdivision surfaces in character animation. <i>Computer Graphics (Proceedings of SIGGRAPH '98)</i>, pages 85--94, 1998. 21]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[Daniel Doo and Malcolm A. Sabin. Behaviour of recursive division surfaces near extraordinary points. <i>Computer-Aided Design</i>, 10(6):356--360, 1978. 21]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[Albrecht D&#252;rer. <i>Treatise on measurement with compasses and straightedge (Underweysung der Messung mit dem Zirkel und Richtscheyt)</i>. Nuremberg, 1525. 14]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>501891</ref_obj_id>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[Gerald Farin. <i>Curves and Surfaces for CAGD: A Practical Guide</i>. Academic Press, 3rd edition, 1993. 20]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. <i>Computer Graphics: Principles and Practice</i>. Addison-Wesley Publishing Company, 2nd edition, 1990. 22, 26, 27, 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>94788</ref_obj_id>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[Andrew S. Glassner. <i>An Introduction to Ray Tracing</i>. Academic Press, 1989. 9, 17, 20, 26, 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[Andrew S. Glassner. <i>Principles of Digital Image Synthesis</i>. Morgan Kaufmann Publishers, 1995. 26, 27, 35, 53]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>31469</ref_obj_id>
				<ref_obj_pid>31468</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[Jeffrey Goldsmith and John Salmon. Automatic creation of object hierarchies for ray tracing. <i>IEEE Computer Graphics and Applications</i>, 7(5):14--20, 1987. 33]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>13023</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[Ned Greene. Environment mapping and other applications of world projections. <i>IEEE Computer Graphics and Applications</i>, 6(11):21--29, 1986. 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[Eric Haines. <i>Ray Tracing News</i>. 1987--present. (Web page: www.acm.org/tog/resources/RTNews/-html). 33, 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[Vlastimil Havran. <i>Heuristic Ray Shooting Algorithms</i>. PhD thesis, Czech Technical University, Prague, 2001. 33]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141998</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[Milo&#353; Ha&#353;an, Fabio Pellacini, and Kavita Bala. Direct-to-indirect transfer for cinematic relighting. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2006)</i>, 25(3):1089--1097, 2006. 90]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[Paul S. Heckbert. Survey of texture mapping. <i>IEEE Computer Graphics and Applications</i>, 6(11):56--67, 1986. 22]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[Abu Sad al-Ala ibn Sahl. <i>On Burning Mirrors and Lenses</i>. Baghdad, 984. (Translated by Roshdi Rashed, 1990). 26]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311555</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>129</ref_seq_no>
				<ref_text><![CDATA[Homan Igehy. Tracing ray differentials. <i>Computer Graphics (Proceedings of SIGGRAPH '99)</i>, pages 179--186, 1999. 34]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>130</ref_seq_no>
				<ref_text><![CDATA[Henrik Wann Jensen, James Arvo, Philip Dutr&#233;, Alexander Keller, Art Oven, Matt Pharr, and Peter Shirley. <i>SIGGRAPH 2003 Course Note #44: Monte Carlo Ray Tracing</i>. 2003. 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801287</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>131</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya. Ray tracing parametric patches. <i>Computer Graphics (Proceedings of SIGGRAPH '82)</i>, 16(3):245--254, 1982. 20]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>132</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya. The rendering equation. <i>Computer Graphics (Proceedings of SIGGRAPH '86)</i>, 20(4):143--150, 1986. 28]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>772258</ref_obj_id>
				<ref_obj_pid>772249</ref_obj_pid>
				<ref_seq_no>133</ref_seq_no>
				<ref_text><![CDATA[Toshiaki Kato. The Kilauea massively parallel ray tracer. In Alan Chalmers, Timothy Davis, and Erik Reinhard, editors, <i>Practical Parallel Rendering</i>, chapter 8. A K Peters, 2002. 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15916</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>134</ref_seq_no>
				<ref_text><![CDATA[Timothy L. Kay and James T. Kajiya. Ray tracing complex scenes. <i>Computer Graphics (Proceedings of SIGGRAPH '86)</i>, 20(4):269--278, 1986. 32]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>135</ref_seq_no>
				<ref_text><![CDATA[Alexander Keller and Ingo Wald. Efficient importance sampling techniques for the photon map. In <i>Proceedings of the 5th Fall Workshop on Vision, Modeling, and Visualization</i>, pages 271--279, 2000. 97]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>136</ref_seq_no>
				<ref_text><![CDATA[Leif Kobbelt, Katja Daubert, and Hans-Peter Seidel. Ray tracing of subdivision surfaces. In <i>Rendering Techniques '98 (Proceedings of the 9th Eurographics Workshop on Rendering)</i>, pages 69--80, 1998. 21]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218463</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>137</ref_seq_no>
				<ref_text><![CDATA[Craig Kolb, Pat Hanrahan, and Don Mitchell. A realistic camera model for computer graphics. <i>Computer Graphics (Proceedings of SIGGRAPH '95)</i>, pages 317--324, 1995. 31]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>138</ref_seq_no>
				<ref_text><![CDATA[Ares Lagae and Philip Dutr&#233;. An efficient ray-quadrilateral intersection test. <i>Journal of Graphics Tools</i>, 10(4):23--32, 2005. 19]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>139</ref_seq_no>
				<ref_text><![CDATA[Johann H. Lambert. <i>Photometry: or on the Measure and Gradations of Light, Colors, and Shade</i>. 1760. (Translated from the Latin by David L. DiLaura, 2001). 22]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>140</ref_seq_no>
				<ref_text><![CDATA[Hayden Landis. Production-ready global illumination. In <i>SIGGRAPH 2002 Course Note #16: RenderMan in Production</i>, pages 87--102, 2002. 29]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>141</ref_seq_no>
				<ref_text><![CDATA[Charles Loop. Smooth subdivision surfaces based on triangles. Master's thesis, University of Utah, Salt Lake City, 1987. 21]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360541</ref_obj_id>
				<ref_obj_pid>360529</ref_obj_pid>
				<ref_seq_no>142</ref_seq_no>
				<ref_text><![CDATA[William Martin, Elaine Cohen, Russel Fish, and Peter Shirley. Practical ray tracing of trimmed NURBS surfaces. <i>Journal of Graphics Tools</i>, 5(1):27--52, 2000. 20]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>272315</ref_obj_id>
				<ref_obj_pid>272313</ref_obj_pid>
				<ref_seq_no>143</ref_seq_no>
				<ref_text><![CDATA[Tomas M&#246;ller and Ben Trumbore. Fast, minimum storage ray triangle intersection. <i>Journal of Graphics Tools</i>, 2(1):21--28, 1997. 17]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>144</ref_seq_no>
				<ref_text><![CDATA[Michael J. Muuss. Rt and remrt --- shared memory parallel and network distributed ray-tracing programs. In <i>USENIX: Proceedings of the Fourth Computer Graphics Workshop</i>, 1987. 42]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>145</ref_seq_no>
				<ref_text><![CDATA[Isaac Newton. <i>Opticks: A Treatise on the Reflections, Refractions, Inflections and Colours of Light</i>. London, 1704. 14]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192213</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>146</ref_seq_no>
				<ref_text><![CDATA[Michael Oren and Shree K. Nayar. Generalization of Lambert's reflectance model. <i>Computer Graphics (Proceedings of SIGGRAPH '94)</i>, pages 239--246, 1994. 22]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300537</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>147</ref_seq_no>
				<ref_text><![CDATA[Steven Parker, William Martin, Peter-Pike J. Sloan, Peter Shirley, Brian Smits, and Charles Hansen. Interactive ray tracing. <i>In Symposium on Interactive 3D Graphics</i>, pages 119--126, 1999. 42]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>148</ref_seq_no>
				<ref_text><![CDATA[Darwyn Peachey. Texture on demand. Technical Report #217, Pixar, 1990. (Available at graphics.pixar.com). 38, 39]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>149</ref_seq_no>
				<ref_text><![CDATA[Ingmar Peter and Georg Pietrek. Importance driven construction of photon maps. In <i>Rendering Techniques '98 (Proceedings of the 9th Eurographics Workshop on Rendering)</i>, pages 269--280, 1998. 49, 97, 98]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275462</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>150</ref_seq_no>
				<ref_text><![CDATA[Matt Pharr and Pat Hanrahan. Geometry caching for ray-tracing displacement maps. <i>In Rendering Techniques '96 (Proceedings of the 7th Eurographics Workshop on Rendering)</i>, pages 31--40, 1996. 21, 41]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975275</ref_obj_id>
				<ref_seq_no>151</ref_seq_no>
				<ref_text><![CDATA[Matt Pharr and Greg Humphreys. <i>Physically Based Rendering: From Theory to Implementation</i>. Morgan Kaufmann Publishers, 2004. 21, 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258791</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>152</ref_seq_no>
				<ref_text><![CDATA[Matt Pharr, Craig Kolb, Reid Gershbein, and Pat Hanrahan. Rendering complex scenes with memorycoherent ray tracing. <i>Computer Graphics (Proceedings of SIGGRAPH '97)</i>, pages 101--108, 1997. 40]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>208469</ref_obj_id>
				<ref_seq_no>153</ref_seq_no>
				<ref_text><![CDATA[Les Piegl and Wayne Tiller. <i>The NURBS Book</i>. Springer-Verlag, 1997. 20]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>154</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves, David H. Salesin, and Robert L. Cook. Rendering antialiased shadows with depth maps. <i>Computer Graphics (Proceedings of SIGGRAPH '87)</i>, 21(4):283--291, 1987. 37, 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732126</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>155</ref_seq_no>
				<ref_text><![CDATA[Erik Reinhard, Brian Smits, and Chuck Hansen. Dynamic acceleration structures for interactive ray tracing. <i>In Rendering Techniques 2000 (Proceedings of the 11th Eurographics Workshop on Rendering)</i>, pages 299--306, 2000. 32]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>156</ref_seq_no>
				<ref_text><![CDATA[Christophe Schlick. A customizable reflectance model for everyday rendering. <i>In Proceedings of the 4th Eurographics Workshop on Rendering</i>, pages 73--83, 1993. 26, 27]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>157</ref_seq_no>
				<ref_text><![CDATA[Andrei Sherstyuk. Fast ray tracing of implicit surfaces. <i>Computer Graphics Forum</i>, 18(2):139--147, 1999. 20]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>515330</ref_obj_id>
				<ref_seq_no>158</ref_seq_no>
				<ref_text><![CDATA[Peter Shirley. <i>Fundamentals of Computer Graphics</i>. A K Peters, 2002. 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>940410</ref_obj_id>
				<ref_seq_no>159</ref_seq_no>
				<ref_text><![CDATA[Peter Shirley and R. Keith Morley. <i>Realistic Ray Tracing</i>. A K Peters, 2nd edition, 2005. 26, 27, 35, 38]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>160</ref_seq_no>
				<ref_text><![CDATA[Peter Shirley, Philipp Slusallek, Ingo Wald, et al. <i>SIGGRAPH 2006 Course Note #4: State of the Art in Interactive Ray Tracing</i>. 2006. 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>226151</ref_obj_id>
				<ref_obj_pid>226150</ref_obj_pid>
				<ref_seq_no>161</ref_seq_no>
				<ref_text><![CDATA[Peter Shirley, Changyaw Wang, and Kurt Zimmerman. Monte Carlo techniques for direct lighting calculations. <i>ACM Transactions on Graphics</i>, 15(1):1--36, 1996. 38]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781210</ref_obj_id>
				<ref_obj_pid>781209</ref_obj_pid>
				<ref_seq_no>162</ref_seq_no>
				<ref_text><![CDATA[Brian Smits. Efficiency issues for ray tracing. <i>Journal of Graphics Tools</i>, 3(2):1--14, 1998. 21, 33]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732125</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>163</ref_seq_no>
				<ref_text><![CDATA[Brian Smits, Peter Shirley, and Michael M. Stark. Direct ray tracing of displacement mapped triangles. <i>In Rendering Techniques 2000 (Proceedings of the 11th Eurographics Workshop on Rendering)</i>, pages 307--318, 2000. 21]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134080</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>164</ref_seq_no>
				<ref_text><![CDATA[Brian E. Smits, James R. Arvo, and David H. Salesin. An importance-driven radiosity algorithm. <i>Computer Graphics (Proceedings of SIGGRAPH '92)</i>, 26(2):273--282, 1992. 96]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>993676</ref_obj_id>
				<ref_seq_no>165</ref_seq_no>
				<ref_text><![CDATA[Ian Stephenson, editor. <i>Production Rendering: Design and Implementation</i>. Springer-Verlag, 2005. 18]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2019634</ref_obj_id>
				<ref_obj_pid>2019627</ref_obj_pid>
				<ref_seq_no>166</ref_seq_no>
				<ref_text><![CDATA[Gordon Stoll, William R. Mark, Peter Djeu, Rui Wang, and Ikrima Elhassan. Razor: an architecture for dynamic multiresolution ray tracing. Technical Report TR-06-21, University of Texas at Austin, 2006. (Updated version to appear in <i>ACM Transactions on Graphics)</i>. 40]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732120</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>167</ref_seq_no>
				<ref_text><![CDATA[Frank Suykens and Yves D. Willems. Density control for photon maps. In <i>Rendering Techniques 2000 (Proceedings of the 11th Eurographics Workshop on Rendering)</i>, pages 11--22, 2000. 97]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732286</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>168</ref_seq_no>
				<ref_text><![CDATA[Frank Suykens and Yves D. Willems. Path differentials and applications. <i>In Rendering Techniques 2001 (Proceedings of the 12th Eurographics Workshop on Rendering)</i>, pages 257--268, 2001. 34]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>169</ref_seq_no>
				<ref_text><![CDATA[Steve Upstill. <i>The RenderMan Companion</i>. Addison Wesley Publishers, 1990. 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383545</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>170</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald, Andreas Dietrich, and Philipp Slusallek. An interactive out-of-core rendering framework for visualizing massively complex models. <i>In Rendering Techniques 2004 (Proceedings of the Eurographics Symposium on Rendering 2004)</i>, pages 81--92, 2004. 40]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141913</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>171</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald, Thiago Ize, Andrew Kensler, Aaron Knoll, and Steven G. Parker. Ray tracing animated scenes using coherent grid traversal. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2006)</i>, 25(3):485--493, 2006. 32]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>172</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald and Steven G. Parker, editors. <i>Proceedings of the IEEE Symposium on Interactive Ray Tracing 2006</i>. IEEE, 2006. (Web page: www.sci.utah.edu/RT06). 35]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732298</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>173</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald, Philipp Slusallek, Carsten Benthin, and Michael Wagner. Interactive distributed ray tracing of highly complex models. In <i>Rendering Techniques 2001 (Proceedings of the 12th Eurographics Workshop on Rendering)</i>, pages 277--288, 2001. 43]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>174</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald, Philipp Slusallek, Carsten Benthin, and Michael Wagner. Interactive rendering with coherent raytracing. <i>Computer Graphics Forum (Proceedings of Eurographics 2001)</i>, 20(3):153--164, 2001. 42]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>175</ref_seq_no>
				<ref_text><![CDATA[Gregory J. Ward. Adaptive shadow testing for ray tracing. In <i>Proceedings of the 2nd Eurographics Workshop on Rendering</i>, pages 11--20, 1991. 38, 55]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>176</ref_seq_no>
				<ref_text><![CDATA[Gregory J. Ward. Measuring and modeling anisotropic reflection. <i>Computer Graphics (Proceedings of SIGGRAPH '92)</i>, 26(2):265--272, 1992. 22, 30]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>177</ref_seq_no>
				<ref_text><![CDATA[Gregory J. Ward, Francis M. Rubinstein, and Robert D. Clear. A ray tracing solution for diffuse interreflection. <i>Computer Graphics (Proceedings of SIGGRAPH '88)</i>, 22(4):85--92, 1988. 30, 72]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>178</ref_seq_no>
				<ref_text><![CDATA[Turner Whitted. An improved illumination model for shaded display. <i>Communications of the ACM</i>, 23(6):343--349, 1980. 24]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>964967</ref_obj_pid>
				<ref_seq_no>179</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Pyramidal parametrics. <i>Computer Graphics (Proceedings of SIGGRAPH '83)</i>, 17(3):1--11, 1983. 38]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>180</ref_seq_no>
				<ref_text><![CDATA[Sergei Zhukov, Andrei Iones, and Gregorij Kronin. An ambient light illumination model. In <i>Rendering Techniques '98 (Proceedings of the 9th Eurographics Workshop on Rendering)</i>, pages 45--55, 1998. 29]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Advanced Global Illumination Using Photon Mapping Siggraph 2008 Friday, August 15, 2008 WojciechJarosz 
University of California, San Diego wjarosz@ucsd.edu http://graphics.ucsd.edu/ wjarosz HenrikWannJensen 
University of California, San Diego henrik@graphics.ucsd.edu http://graphics.ucsd.edu/ henrik and Craig 
Donner Columbia University cdonner@cs.columbia.edu http://www.cs.columbia.edu/ cdonner Abstract Photon 
mapping provides a practical way of ef.ciently simulating global illumination including interre.ections, 
caustics, color bleeding, participat­ing media and subsurface scattering in scenes with complicated geometry 
and advanced material models. This class will provide the insight neces­sary to ef.ciently implement 
and use photon mappingto simulate global illumination in complex scenes. Thepresentation will brie.y 
cover the fundamentals of photon mapping including ef.cient techniques and data­structures for managing 
large numbers of rays and photons. In addition, we will describe how to integrate the information from 
the photon maps inshading algorithmstorenderglobal illumination.Alargeportionofthe class will be dedicated 
to advanced techniques for photon mapping. In additiontoafastrecapof introductorymaterial,wewill describe 
morere­cent developments including ef.cient methods for using photon mapping in scene with participating 
media and subsurface scattering. About the Lecturers WojciechJarosz Universityof California, SanDiego 
wjarosz@ucsd.edu http://graphics.ucsd.edu/ wjarosz Wojciech Jarosz is a computer graphics Ph.D. student 
at UC San Diego. His main research interest is production-quality global illumination techniques, speci.cally 
for participating media. He received his B.S. in Computer Science from University of Illinois, Urbana-Champaign 
and his M.S. in Computer Science from UC San Diego. HenrikWannJensen Universityof California, SanDiego 
henrik@graphics.ucsd.edu http://graphics.ucsd.edu/ henrik Henrik Wann Jensen is an Associate Professor 
at UC San Diego where he teaches computer graphics. His main research interest is in the area of global 
illumination and appearance modeling. He received his M.Sc. degree and his Ph.D. degree from the TechnicalUniversityof 
Denmark.Heisthe authorof RealisticImage Synthesisusing Photon Mapping, AK Peters 2001. Craig Donner Columbia 
University cdonner@cs.columbia.edu http://www.cs.columbia.edu/ cdonner Craig Donner is currently a Postdoctoral 
Researcher in the Computer Graphics group at Columbia University, with Professor Ravi Ramamoorthi. Before 
joining thelab at Columbia, Craig was a postdoc in the Graphics group of the University of California, 
San Diego, wherehe also did his Ph.D. under Professor HenrikWann Jensen. Class Syllabus 13:45 Jarosz 
13:50 Jarosz 14:05 Jensen 14:45 Jensen 15:30 15:45 Jarosz 16:30 Donner 17:15 All Introduction and Welcome 
(5 min) Overview of Global Illumination (15 min) Photon Tracing: Building the Photon Maps (40 min) Rendering 
Using Photon Mapping (45 min) . . . . . . . . . . . . . . . . Break . . . . . . . . . . . . . . . . (15 
min) Participating Media (45 min) Subsurface Scattering (45 min) Final Remarks and Questions (15 min) 
  Contents 1 Introduction 9 1.1 Motivation ................................ 9 1.2 Whatisphoton mapping? 
........................ 10 1.3 More information ............................ 11 1.4 Acknowledgements 
........................... 11 Part1:Per Christensen 11 2 Ray-tracing fundamentals 13 2.1 Historical 
background: raytracing in the pre-computer age . . . . . . 14 2.2Visibilityraytracing .......................... 
15 2.3 Ray intersection calculations ...................... 15 2.3.1Triangles ............................ 
16 2.3.2 Quadrilaterals (bilinear patches) . . . . . . . . . . . . . . . . 17 2.3.3 Quadrics ............................ 
19 2.3.4 Implicitsurfaces ........................ 20 2.3.5 NURBSsurfaces ........................ 
20 2.3.6 Subdivisionsurfaces ...................... 21  2.3.7 Displacement-mapped surfaces . . . . 
. . . . . . . . . . . . 21 2.3.8 Boxes .............................. 21 2.4 Re.ection models ............................ 
22 2.5 Shadowraytracing ........................... 23 2.6 Recursiveraytracing .......................... 
24 2.6.1 Re.ection............................ 24 2.6.2 Refraction............................ 26 
2.7 MonteCarloraytracing ......................... 27 2.7.1 Distributionraytracing ..................... 
27 2.7.2Pathtracing ........................... 28 2.7.3 Softshadows .......................... 28 
 2.7.4 Ambient occlusion ....................... 29 2.7.5 Glossy re.ections ........................ 
29 2.7.6 Diffuse re.ections ....................... 30 2.7.7 Depthof.eld .......................... 
30 2.7.8 Motionblur ........................... 31 2.8 Spatial accelerationdata structures ................... 
32 2.8.1 Boundingvolumehierarchy .................. 32 2.8.2 Which acceleration data structure is best? 
. . . . . . . . . . . 33 2.9 Raydifferentials ............................. 33 2.9.1 Ray propagation 
and specular re.ection . . . . . . . . . . . . 34 2.9.2 Glossyanddiffusere.ection .................. 
34 2.10 Furtherreading ............................. 35 3 Ray tracing in complex scenes 37 3.1 Manylight 
sources ........................... 37 3.2Toomanytextures............................ 38 3.2.1 Multiresolutiontextures 
.................... 38 3.2.2Texturetiling .......................... 39 3.2.3 Multiresolution texture 
tile cache . . . . . . . . . . . . . . . 39 3.3 Geometriccomplexity ......................... 39 3.3.1 
Instancing............................ 40 3.3.2 Ray reordering andshading caching . . . . . . . . . 
. . . . . 40 3.3.3 Geometric stand-ins ...................... 40 3.3.4 Multiresolutiontessellation . 
. . . . . . . . . . . . . . . . . . 40 3.4Parallelexecution ............................ 42 3.4.1 SIMD 
instructions ....................... 42 3.4.2 Multiprocessors......................... 42 3.4.3 ClustersofPCs 
......................... 43 3.5 RaytracinginPixarmovies ....................... 43 Part2: HenrikWannJensen 
45 4 APractical Guide to GlobalIllumination using Photon Mapping 47 4.1 Photontracing .............................. 
47 4.1.1 Photon emission ........................ 47 4.1.2 Photontracing ......................... 
50 4.1.3 Photonstoring ......................... 53 4.1.4 Extensionto participating media . . . . . 
. . . . . . . . . . . 55 4.1.5 Threephotonmaps ....................... 57 4.2 Preparingthe photonmapfor 
rendering . . . . . . . . . . . . . . . . 58 4.2.1 The balanced kd-tree ...................... 59 4.2.2 
Balancing............................ 59 4.3 The radiance estimate .......................... 60 4.3.1 
Radiance estimateatasurface ................. 60 4.3.2 Filtering............................. 63 4.3.3 
The radiance estimate in a participating medium . . . . . . . 65 4.3.4 Locatingthe nearestphotons .................. 
65 4.4 Rendering................................ 67 4.4.1 Direct illumination ....................... 
69 4.4.2 Specularandglossyre.ection ................. 71 4.4.3 Caustics............................. 
71 4.4.4 Multiplediffuse re.ections ................... 72 4.4.5Participatingmedia ....................... 
73 4.4.6 Whydistributionraytracing? ................. 73 4.5 Examples ................................ 
74 4.5.1 The Cornellbox ........................ 74 4.5.2 Cornellboxwithwater ..................... 
79 4.5.3 Fractal Cornellbox ....................... 80 4.5.4 Cornellboxwithmultiplelights ................ 
81 4.5.5 Cornellboxwithsmoke .................... 82 4.5.6 Cognacglass .......................... 83 
 4.5.7 Prismwith dispersion ...................... 84 4.5.8 Subsurface scattering ...................... 
85  4.6 Wheretoget programswith photon maps . . . . . . . . . . . . . . . 86 Part3:Per Christensen 86 
5 Photon mappingfor complex scenes 89 5.1 Photon emission from complex light sources . . . . . . . . 
. . . . . . 89 5.2 Photon scatteringfromcomplexsurfaces ................ 90 5.3 The radiositymap ............................ 
91 5.4 The radiosityatlasforlarge scenes ................... 92 5.4.1 Photon emissionandphoton tracing 
. . . . . . . . . . . . . . 93 5.4.2 Radiosity estimation ...................... 94 5.4.3 Generating 
radiositybrickmaps ................ 95 5.4.4 Rendering............................ 96 5.5 Importanceforphotontracing 
..................... 96 5.5.1 Importance ........................... 96   5.5.2 Importance emission 
and estimation . . . . . . . . . . . . . . 96 5.5.3 Photontracing ......................... 97 6 References 
and furtherreading 99 Chapter1  Introduction This course material describes in detail the practical 
aspects of the ray tracing and photon map algorithms. The text is based on published papers as well as 
industry experience. After reading this course material, it shouldbe relatively straightforward to add 
an ef.cient implementation of the photon map algorithm to anyray tracer and to understand the details 
necessary to make ray tracing and photon mapping robust in complex scenes. 1.1 Motivation The photon 
mapping method is an extension of ray tracing. In 1989, Andrew Glassner wrote about ray tracing[25]: 
Todayraytracingisoneofthemostpopularandpowerful techniques in the image synthesis repertoire: it is simple, 
elegant, and easily imple­mented. [However] there are some aspects of the real world that ray trac­ing 
doesn t handle very well (or at all!) as of this writing. Perhaps the most important omissions are diffuse 
inter-re.ections (e.g. the bleeding of colored light from a dull red .le cabinet onto a white carpet, 
giving the carpet a pink tint), and caustics (focused light, like the shimmering waves at the bottom 
of a swimmingpool). At the time of the development of the photon map algorithm in 1993, these problems 
were still not addressed ef.ciently by any ray tracing algorithm. The photon map method offers a solution 
to both problems. Diffuse interre.ections and caustics are both indirect illumination of diffuse surfaces; 
with the photon map method, such il­lumination is estimated using precomputed photon maps. Extending 
ray tracing with photon maps yields a method capable of ef.ciently simulating all types of direct and 
indirect illumination. Furthermore, the photon map method can handle participating media anditisfairly 
simpleto parallelize. 1.2 What is photon mapping? The photon map algorithm was developed in 1993 1994 
and the .rst papers on the method were published in 1995. It is a versatile algorithm capable of simulating 
global illumination including caustics, diffuse interre.ections, and participating me­dia in complex 
scenes. It provides the same .exibility as general Monte Carlo ray tracing methods using only a fraction 
of the computation time. The global illumination algorithm based on photon maps is a two-pass method. 
The .rst passbuilds the photon mapby emitting photons from the light sources into the scene and storing 
them in a photon map when theyhit non-specular objects. The sec­ond pass, the rendering pass, uses statistical 
techniques on the photon map to extract information about incoming .ux and re.ected radiance at anypoint 
in the scene. The photonmapis decoupledfromthe geometric representationofthe scene.Thisisakey feature 
of the algorithm, making it capable of simulating global illumination in com­plex scenes containing millions 
of triangles, instanced geometry, and complex proce­durally de.ned objects. Compared with .nite element 
radiosity, photon maps have the advantage that no mesh­ingis required. The radiosityalgorithmisfaster 
for simple diffuse scenesbut as the complexity of the scene increases, photon maps tend to scale better. 
Also the photon map method handles non-diffuse surfaces and caustics. Monte Carlo ray tracing methods 
such as path tracing, bidirectional path tracing, and Metropolis can simulate all global illumination 
effects in complex scenes with very little memory overhead. The main bene.t of the photon map compared 
with these methodsisef.ciency,andthepricepaidistheextramemoryusedto storethe photons. For most scenes 
the photon map algorithm is signi.cantlyfaster, and the result looks better since the error in the photon 
map method is of low frequency which is less noticeable than the high frequencynoise of general Monte 
Carlo methods. Another big advantage of photon maps (from a commercial point of view) is that there is 
no patent on the method; anyone can add photon maps to their renderer. As a re­sult several commercial 
systems use photon maps for rendering caustics and global illumination. 1.3 More information For more 
information about photon mapping, all the practicaldetails, the theory and the insight for understandingthe 
technique see: HenrikWann Jensen Realistic Image Synthesis using Photon Mapping AK Peters, 2001 This 
book also contains additional information about participating media and subsur­face scattering. Finally, 
it contains an implementation with source code in C++ of the photon map data structure. 1.4 Acknowledgements 
WewouldliketothankPer Christensenforallowingusto includehis coursenotesfrom aprevious incarnationofthis 
class.Perwouldliketothankhis colleaguesatPixarand Pixar s RenderMan team for providing an inspiring environment 
and for their help and support. David Laur and JulianFong implemented large parts of the ray-tracing 
functionalityin Pixar sRenderMan, whichwas usedto generate manyofthe imagesin these course notes. Thanks 
toWayneWooten for digging out the data for the Pixar test scenes. All images from Monsters, Inc., Cars, 
and Ratatouille are copyright c &#38;#169; DisneyEnter­prises, Inc. and Pixar Animation Studios. Chapter2 
 Ray-tracing fundamentals This chapter provides an overview of the fundamental aspects of the ray tracing 
al­gorithm. Ray tracing has its roots in the renaissance (or perhaps even earlier), and was used for 
drawing images with correct perspective foreshortening and for design of mirrors and lenses for telescopes 
and other optical instruments. Raytracing canbe usedfor visibility testing (projectingthe3D scenetoa2Dimage) 
andshadows,butitreally comesintoitsright elementwhenusedtocomputespecular re.ections and refractions. 
The basic ray tracing algorithm is very simple and elegant; nevertheless it can compute visual phenomena 
that are very hard to compute with any other method. Monte Carlo ray tracing adds further effects such 
as soft shadows and ambient occlusion, glossy and diffuse re.ections, depth-of-.eld, and motion blur. 
This chapter starts with a brief history of ray tracing, and then shows how to use ray tracingfor direct 
rendering.Itthenprovidesanoverviewofray intersection calculation methodsforvarioustypesofsurfaces,akeypartofray 
tracing. Then followsa brief overview of re.ection models. With these basics covered, the use of ray 
tracing to render shadows, re.ection and refraction, soft shadows, glossy and diffuse re.ections, depth-of-.eld,andmotionbluris 
described.Variousspatial accelerationdata structures are used to speed up ray tracing in scenes with 
manyobjects; those are only described very super.cially.Attheend thereisa descriptionofraydifferentials,a 
relativelynew concept that is usefulfor manyapplications including texture .ltering and tessellation. 
The last section contains recommendations for further reading. 2.1 Historicalbackground:raytracinginthepre-computer 
age Ray tracing was used long before the electronic computer was invented. Figure 2.1 shows ray tracing 
in the year 1525 back then, the computer was the artist s assis­tant and rayswere strandsof thread. 
AlbrechtD ¨ urer (1471 1528), a German renais­sance painter and engraver, usedthis device to render images 
with correctperspective projection[22]. Points on the object(a lute) are projected onto theimage. Nowadays 
we would call this process a projection of 3D points onto a 2D image . Ray tracing was also used for 
lens design for microscopes, telescopes, binoculars, and cameras. Sir Isaac Newton (1642 1727) showed 
re.ection and refraction of rays in his famous 1704 bookOpticks [50]. One of his illustrations is shown 
in .gure 2.2.  2.2 Visibility ray tracing Rendering consists of computing the color of each point in 
an image. This is done by projectingthe3D sceneontoa2Dimage.Raytracingisoneofthemostpopularren­dering 
methods. The basic ray tracing algorithm consists of two main calculations per pixel: .nd the nearest 
surface point and compute the color at that point. It determines the visibility of object surfaces by 
following imaginary rays from the viewer s eye to the objects in the scene. This limited type of ray 
tracing is sometimes referred to as ray casting. Appel[3]wasthe .rstto usea computerto render ray-traced 
images. The simplest ray tracing algorithm is as follows: for each pixel do compute ray for that pixel 
for each object in scene do if ray intersects object and intersection is nearest so far then record intersection 
distance and object color set pixel color to nearest object color (if any) Computing the ray corresponding 
to a pixel is very simple: the ray origin is at the viewpoint, and the ray direction is from the viewpoint 
to the pixel position in the image plane.Thecomputationofthe intersectionofaraywithanobjectisfairlysimple;we 
cover the detailsof thisin the following section(2.3). A simple way to improve the image quality is to 
shoot several rays per pixel. This reduces aliasing effects such as jaggies and staircase effects along 
object silhouettes. Figure 2.3 shows a simple ray-traced image of two teapots. The image shows the shapeofthe 
teapots,as seenfromtheviewpoint.However,theimageisvery simplistic since the teapots are simply rendered 
black. To improve on this, we need shading and re.ection models which are described in section 2.4. 
2.3 Ray intersection calculations At the heart of all ray tracing algorithms is the computation of ray-object 
intersec­tions. If there is more than one intersection we usually want the nearest. (However, for shadow 
rays we often just need to know whether there is any intersection; it doesn t matterifit sthe nearestone.) 
This sectionprovidesanoverviewof ray-object intersec­tion calculation methods. First a de.nition: a ray 
is a semi-in.nite line. It is de.ned by an origin o and a direc­tion d: p(t)= o + td for all t = 0. 
2.3.1 Triangles Atriangleabc is de.ned by its three vertices a, b, c. The normal of the triangle can 
be computed (on-the-.y or just once and stored with the triangle data) using the cross product of two 
of the triangle edges, for example (b - a) and (c - a): n =(b - a) × (c - a) . The ray-triangle intersection 
point p must be along the ray (i.e. p = o + td)and mustbeintheplaneofthe triangle(i.e.thevectorfroma 
trianglevertexto p must be perpendicular to the triangle normal), (p - a) · n =0. From these two equations 
we get: 0=(p - a) · n =(o + td - a) · n =(o - a) · n + td · n Solving for t we get: (a - o) · n t = . 
d · n If the dot product d · n is0therayis paralleltotheplaneand thereisnointersection. If the computed 
t is negative there is an intersectionbut it is behind the ray origin, so werejectit.Wealsorejectthe 
intersectionifitis furtherawaythanapreviouslyfound intersection for that ray. Given the distance t, we 
can compute the ray-plane intersection point: p = o + td. Next we must check whether this pointis inside 
or outside the triangle.Wedo thisby computing the barycentric coordinates (u, v) of the hit point. The 
barycentric coordi­nates of p are de.nedby a + u(b - a)+ v(c - a)= p 16 as illustrated in .gure 2.4. 
 Figure 2.4:Triangle abc and barycentric coordinates (u, v) of point p. Sincethevectorsinthe equationabovehave 
three components(x, y, and z)we have three equations to determine the two unknowns u and v, and we are 
free to choose two of the three equations to solve. In practice, one should pick the two equations where 
the .oating point number precision is highest. Finally, when the barycentric coordinates have been computed, 
we can determine whether the intersection point is inside the triangle or not. If u = 0 and v = 0 and 
u + v = 1 then the point is inside (or on the edge of) the triangle and we .nally have an actual intersection! 
Further implementation details, optimizations, and pseudo-code can be found in e.g. Glassner[25]andM¨ 
oller andTrumbore[48]. 2.3.2 Quadrilaterals (bilinear patches) Aquadrilateral (or bilinear patch)abcd 
is de.ned by four vertices a, b, c, d.We can express all points p on the surface of the quadrilateral 
as a bilinear combination of the four vertices: (1 - u)(1 - v)a + u(1 - v)b + (1 - u)vc + uvd = p . Figure 
2.5 shows a quadrilateral with iso-lines for u and v. The point p is on the quadrilateral if both u and 
v are in the range [0, 1]. Quadrilaterals are not .at in general, but for .at quadrilaterals we can compute 
the intersectionpointmoreef.cientlythanforthegeneral case.Wewilllookatthegeneral, non-planar case .rst. 
 Non-planar quadrilaterals Aray can intersecta non-planar quadrilateralat0,1,or2points. Plugging the 
ray equation p = o + td into the quadrilateral equation above gives 3equations (one for each of thex, 
y, and z coordinates) with3unknowns t, u, and v.  Unfortunately the equations are non-linear. However, 
afairly ef.cient method is to .rst solve a quadratic equation for u. If u is in the [0, 1] range then 
compute the corresponding v. If v is also in [0, 1] then compute t. Reject the intersection if t is negative 
or larger than the previous nearest hit distance. Implementation details for this methodcanbefoundin 
Stephenson[70].The normalatthehitpointcanbecomputed e.g.by bilinear combinationofthe fourvertex normals. 
Amoreef.cientmethodcanbeusedifthe quadrilateralissmall.Inthatcaseitisoften suf.cient to approximate the 
quadrilateral as two triangles. The intersection points might be slightly incorrect, and the u and v 
iso-lines geta kink along the diagonal as shown in .gure 2.6. However, if the quadrilateral is smaller 
than e.g. a pixel this is a very useful optimization,anditis usedalotin practicefor calculatingray intersections 
with .nely tessellated geometry. Planar quadrilaterals Araycan intersectaplanar quadrilateralat0or1points.Onemethod 
computestheray intersection with the plane the quadrilateral is in(using the same calculation as for 
ray­triangle tests) and then determines whether the intersection point is inside or outside the quadrilateral. 
An even more ef.cient method for convex planar quadrilaterals has been presented by Lagae and Dutr´e[43]. 
If the quadrilateral is a rectangle or square it is even simpler to determine the bilinear coordinates 
u and v it can be done with just dot products (projecting p onto two of the edges). 2.3.3 Quadrics 
The class of quadric surfaces consists of disks, spheres, cylinders, cones, ellipsoids, paraboloids, 
andhyperboloids. Disks A disk is de.ned by its center c, normal n, and radius r. The center and normal 
of thedisk de.nea plane.Findinga ray-disk intersectionisvery similarto ray-triangle intersection testing. 
We .rst compute the ray-plane intersection point p, and check that the distance t is positive and smaller 
than the previous nearest intersection. The 2 intersection point is on the disk if (p - c)2 = r. Disks 
are used a lot in practical rendering, for example for rendering of particle sys­tems. Spheres Asphereis 
de.nedby its centerc and radius r.If thereis an intersection,the intersec­tion point must be somewhere 
along the ray, and must be on the surface of the sphere. To .nd the intersection point we plug the ray 
equation p = o + td into the sphere equation (p - c)2 = r2: 2 0=(p - c)2 - r 2 = p2 - 2(p · c)+ c2 - 
r 2 =(o + td)2 - 2(o + td) · c + c2 - r 2 = o2 +2t(o · d)+ t2d2 - 2(o · c) - 2t(d · c)+ c2 - r 2 = d2t2 
+2d · (o - c)t +(o - c)2 - r This is a quadratic equation in t. The two solutions are -B + D -B - D t1 
= and t2 = 2A 2A with A = d2 , B =2d · (o - c), C =(o - c)2 - r2, and the discriminant D is v D = B2 
- 4AC. (If we know in advance that the ray direction is normalized then A =1.) If the discriminant D 
is negative there is no (real) solution and the ray does not hit the sphere. If the discriminant is zero 
the ray is tangent to the sphere, and there is only one intersection point. If the discriminant is positive 
there are two intersection points; the nearest intersection point is the one with the smallest non-negative 
value of t. Given the intersection distance t we can then compute the intersection point p. The normal 
at the intersection point is n = p - c. Other quadrics Ray intersections withcylinders, cones, ellipsoids, 
paraboloids, andhyperboloids can be computedbysolvingaquadratic equationinasimilarfashion, see e.g. Glassner[25]. 
Alternatively,ellipsoidscanalsobe testedbytransformingtheraybythe same transfor­mation that transforms 
the ellipsoid to a sphere; then the intersection can be computed as a ray-sphere intersection. (If there 
is a hit, the hit point and normal must be trans­ formed back.) 2.3.4 Implicit surfaces An implicit 
surface is de.ned by a function f: the surface is the set of pointsp where thevalueofthe functionis0, 
f(p)=0.So to .nd the ray-surface intersection we have to determine the (nearest) point p along the ray 
where f(p) is 0: f(o + td)=0 This can be done using e.g. Newton-Raphson iteration or other iterative 
methods. An ef.cient algorithmis describedby Sherstyuk[62].Thesurface normalatthe intersec­ tionpointisgivenbythegradientofthe 
functionatthatpoint: .f(p) .f(p) .f(p) n = \f(p)=,, . .x .y .z 2.3.5 NURBS surfaces NURBS surfaces[23,58]arewidely 
usedin the CAD industry for modeling cars, air­planes, etc. NURBS surfaces canbe intersection tested 
directly (seeKajiya[36], Mar­ tin et al.[47], and Abert et al.[1]) or tessellated into polygon (triangle 
or quadrilateral) meshes and then intersectiontested. NURBS surfaces often have trimming curves, indicating 
parts of the surface that are cut away . In this case, the ray-NURBS intersection point must be checked 
against the trimming curve to determine whether the intersection point is inside or outside the trim 
curves. If the hit point is outside the trimming curve it should be rejected. 2.3.6 Subdivision surfaces 
Subdivision surfaces[10, 21]are widely used in the movie industry to model smooth surfaces with complex 
topology, forexample humans, animals, etc.[20]. The most common subdivision surface types are Catmull-Clark[10]and 
Loop[46]. Direct ray tracing of subdivision surfaces is somewhat tricky, particularly near extraordinary 
ver­tices; seeKobbelt et al.[41]and Pharr and Humpheys[56]for algorithms. Another strategy is to tessellate 
the subdivision surface into a polygon mesh and then ray trace the mesh. 2.3.7 Displacement-mapped surfaces 
Displacement maps are used to alter the shape of surfaces, for example to add details such as wrinkles, 
dents, largebumps, scratches, reptile scales, etc. Amethodby Smitsetal.[68]computesthe displacements 
onthe.y todetermine the ray intersection points. The advantage of their method is that no extra memory 
is required. The disadvantage is that the displacements have to be along the nor­mals and that the displacement 
function is evaluated repeatedly (which can be very time-consiming).Itisfasterand more general(but also 
more memory consuming)to tessellate larger patches of the surfaces, displace the tessellated points, 
and store the displaced points in a cache[55, 17]. 2.3.8 Boxes As we shall see later, bounding boxes 
are extremely useful for speeding up raytracing of complex scenes. Ageneral boxis de.nedbyavertex and 
threevectors, see .gure2.7(a).Astraightfor­ ward intersection testwould test eachof the sixfaces for 
intersection.Afaster testcan be implementedby utilizing thefact that only the threefaces thatface toward 
the ray origin need to be tested. Axis-aligned boxes can be intersection-tested even more ef.ciently. 
An axis-aligned box consists of two rectangles in each of the xy, xz, and yz planes. An axis-aligned 
boxis de.nedbyits minimumand maximumvertices pmin and pmax as illustrated in .gure 2.7(b). We can consider 
the box the intersection of three in.nite slabs of space. Smits[67]describes a very ef.cient ray intersection 
test that utilizes IEEE .oating-pointconventionstodealgracefullyandef.cientlywithdivisionsby0, thereby 
streamlining the code. If the boxes are used as bounding boxes, we don tneed to know the nearest intersection 
point and normal, we just needto know whether the ray intersects the box or not.  2.4 Re.ection models 
There are manyre.ection models that canbe used withray tracing,but since re.ection models are not a priority 
for this course, we will only give a very brief overview here. Diffuse re.ection can be modeled with 
Lambert s cosine law: the re.ected light is proportional to the cosine of the angle between the incident 
light and the surface nor­mal[44].A more accurate modelof diffuse re.ectionwasdevelopedby Oren and Na­yar[51]. 
Glossy and specular surfaces havehighlights. The highlights can be computed withthe Phongcosine-power 
formula[8,6], withWard s isotropic and anisotropic re­ .ection models[81], or witha numberof other re.ection 
models[24]. Surfaces can alsohavetextures assignedtothemto modulatethe re.ection parameters[9,32].The 
textures can be 2D images or 3D tables, or can be computed procedurally. The illuminationofthesurfacesisprovidedbylight 
sources.Thesimplestlight source typesarepointlights,spotlights,and directionallights.In contrast,verycomplexlight 
sources[5]are used in movie production. Such light sources can project images like a slide projector 
and canhave complex intensityfall-off, barn doors, cucoloris ( cook­ies ), etc. In addition, it is very 
common to add an ambient term to the illumination, i.e. a constant amount of illumination independent 
of surface position and orien­tation. The ambient light .lls in the color on surfacesfacingaway from 
the light sources (otherwise those surfaces would be completely black). The ambient term is a cheaphackto 
approximatethelightthat bounces aroundinareal scene. In this chapter we will only use very simple re.ection 
models: Lambert for diffuse re.ectionandPhongfor specularre.ection.We lluseapointlight sourceand ambient 
for illumination. Figure 2.8 shows two teapots on a square. The surfaces are Lambert diffuse with Phong 
specular highlights. In addition, the square has a checkerboard texture map. The scene is illuminated 
by a point light and a dim ambient light.  2.5 Shadow ray tracing Sofar, we have only used raytracing 
to determine which objects are visible in which image pixels. The .rst additional use of ray tracing 
is for shadow computation: we can determine whether a point is in shadow by tracing a ray from the point 
to the light source[3].Iftherayhitsan opaqueobjectalongtheway,theobjectisinshadow;if not, itis illuminated. 
When computing ray-object intersections for opaque shadows, we only care about hit or no hit; not the 
intersection point and normal. Figure 2.9(a) shows a few examples of shadow rays. For pointlights and 
spot lights we trace rays between the surface points and the light source position. For directional light 
sources, we trace parallel rays from the surface points in the direction of the light. Figure 2.9(b) 
shows shadows from a point light in thefamiliar teapot scene. If the objects are opaque, anyhit will 
suf.ce to determine shadow. But if the objects are semitransparent (as e.g. stained glass), we need to 
get the transmission color of all the intersected surfaces between the point and light source, andthen 
composite the transmission colors by multiplying each color component. Figure 2.9:(a) Shadow rays.(b)Teapots 
with ray-traced shadows. 2.6 Recursive ray tracing When a ray hits a surface with specular re.ection 
or refraction, computing the color there may require tracing more rays called re.ection rays and refraction 
rays, re­spectively. Those rays may hit other specular surfaces, causing more rays tobe traced, and so 
on. Hence the term recursive ray tracing. Figure 2.10 shows a recursive tree of re.ection rays. This 
technique is also known as classicalray tracing or Whitted-style ray tracing sinceitwas introducedbyTurner 
Whittedin 1980[83]. 2.6.1 Re.ection We shoot re.ection rays to compute re.ection from specular surfaces 
such as shiny metals. To compute the re.ection direction we use thelaw of re.ection: the angle of re.ection 
equals the angle of incidence. Furthermore, the re.ection direction is con­strained to be in the plane 
spanned by the incident direction and the surface normal. Figure 2.11 shows the incident direction i, 
the surface normal n, the re.ection direc­tion r, as well as the anglesof incidence and re.ection(.i 
and .r). As can be seen in the .gure, r = i +2d, where d is the projection of -i onto the normal n and 
can be computed as (-i · n) n d = . n2 With this, the re.ection directionr is (i · n) n r = i +2d = i 
- 2 . n2 This formuladoesnot requirethevectorstobe normalized.However,ifweknowthat the normal n is normalized 
we can avoid the division by n2 . Figure 2.12 shows two chrome teapots with ray-traced re.ections. The 
close-up shows how beautifully distorted such re.ections can be, even on relatively simple geometry like 
this. In this image, the re.ection amount was set to 100%. To be more physically correct, one can scale 
the re.ection amount depending on the re.ection angle. The amount of re.ection canbe computed using the 
Fresnel formulas[24,26]or usingafast approxi­ mationby Schlick[61]. 2.6.2 Refraction Dielectric materials 
such as water and glass exhibit both re.ection and refraction. The refraction direction depends on the 
two materials index of refraction .. The index of refractionofvacuumis1,forairitis1.0003,forwateritis1.33,forglassitisinthe 
range 1.5 1.75, and for diamond it is 2.42. Figure 2.13 shows the incident direction i, surface normal 
n, the refraction (transmis­sion) direction t, and the anglesof incidence and refraction(.i and .t). 
.i is the index of refraction of the material of the incident ray, and .t is the index of refraction 
of the transmitting material. The direction of the refraction ray can be computed using the law of refraction 
(often called Snell s law or Descartes law although it was possibly known much earlier by Ibn Sahl[33]). 
The relationship between the incident and transmitted directionis .i sin .i = .t sin .t . From this we 
can derive that the transmitted direction is t = -.i + . cos .i - 1 - .2(1 - cos2 .i) n for . = .i/.t 
and normalized i and n.(Foraderivation please see e.g. Glassner[25] or Shirleyand Morley[64].) If the 
quantity under the square root is negative there is no refraction. This is called total internal re.ection 
and can only occur when the light transfers from a material with high index of refraction to a material 
with lower index of refraction. The angle where the quantity under the square root is zero is called 
the critical angle. The amount of re.ection and refraction depends on the incident angle and index of 
refraction. The amount can be computed with the Fresnel formulas; these formulas are omitted here,butcanbe 
foundin e.g.the booksbyFoley etal.[24]and Glass­ ner[26].Afastandvery useful approximationtothe Fresnel 
formulaswas introduced by Schlick[61]. Figure 2.14 shows two glass teapots with ray-traced re.ections 
and refractions. Notice how the glass bends the light that is refracted through it. The amount of re.ection 
and refraction in this image is determined by Fresnel s formulas. An effect that is often added to render 
glass and water is distance attenuation (expo­nential intensityfall-offaccording to Beer s law). This 
is simple to implement and is describedin e.g. ShirleyandMorley s book[64].  2.7 Monte Carlo ray tracing 
In the previous section,all ray directions were determined deterministically. In this sec­tion we ll 
look at effects that are computed with Monte Carlo ray tracing (also known as stochastic ray tracing), 
i.e. ray tracing where the ray origins, directions, and/or times are computed using random numbers. Monte 
Carlo ray tracing is often divided into two categories: distribution ray tracing and path tracing. 2.7.1 
Distribution ray tracing Distribution ray tracing[19]shoots multiple rays from each surface point to 
sample area lights, glossy and diffuse re.ection, and manyother effects. Figure 2.15 shows a tree of 
re.ection and refraction rays for distribution ray tracing. As the .gure shows, distribution ray tracing 
is prone to an explosion in the number of rays after a few levels of re.ection; to avoid this it is common 
to reduce the number of rays after a few levels of re.ection.With distributionraytracing,itisquiteeasyto 
ensureagood distribution ofray directionsat re.ectionpoints,forexampleby stratifyingthe directions. 
 2.7.2 Path tracing Pathtracing[37]isavariationof distributionraytracingwhereonlyasingle re.ection and 
refraction rayis shot for each point. This avoids the explosion in the number of rays,buta simple implementationwould 
lead tovery noisy images.To compensate for that, manyvisibility rays are traced through each pixel. An 
advantage of path tracing is that since manyvisibility rays are shot per pixel, camera effects like depth-of-.eld 
and motion blur can be incorporated at little extra cost. On the other hand, it is harder to ensureagooddistributionof 
re.ectionrays(forexamplethrough strati.cation)than for distribution ray tracing. Put succinctly, distribution 
ray tracing shoots most rays deeper in the ray tree, while path tracing shoots most visibility rays. 
 2.7.3 Soft shadows Area light sources cause soft shadows. (The region in between complete shadow and 
complete illumination is called the penumbra.) Soft shadows can be computed by shooting shadowraysto 
randompointsonthesurfaceofthe arealight source. Figure 2.16(a) shows shadow rays from three surface points 
to a triangular area light source;someof the rays hit an object. Figure 2.16(b) shows soft shadowsin 
thefa­ miliar teapot scene. In this image, the light source is spherical and the soft shadow is computed 
with distribution ray tracing.  2.7.4 Ambient occlusion Ambient occlusion[85,45]canbe thoughtof as illuminationby 
anextremely large area light source, namely the entire hemisphere above each point. this is similar to 
the illumination outside on an overcast day. Figure 2.17(a) shows ambient occlusion rays from two surface 
points. At the left point mostoftherayshitanobject,sothe occlusionishigh;attherightpointfewrayshitan 
object, so there is little occlusion. Figure 2.17(b) shows ambient occlusion in the teapot scene. This 
.gure shows pure ambient occlusion; this can of course be combined with surface colors, textures, etc. 
 2.7.5 Glossy re.ections Glossy re.ection of indirect light can be computed by shooting rays within 
the direc­tions of the glossy re.ection distribution. For a given incident direction and a pair of random 
numbers, the re.ection modelprovidesa re.ection direction. Figure 2.18 shows glossy re.ections in the 
two teapots; re.ections are computed using Ward s (isotropic) glossy re.ection model[81]. (Thereis also 
an anisotropicversionof this shading model it can be used to render glossy re.ections from e.g. brushed 
metal surfaces.) Similarly,glossy refractioncanbe computedby distributingtherays aroundthe refrac­tion 
direction. This gives the appearance of slightly frosted glass. 2.7.6 Diffuse re.ections Ward et al.[82]usedwide 
distribution ray tracing to compute indirect diffuse light. The distribution of re.ection rays covers 
the entire hemisphere above each point, with a cosine-weighted distribution such that more rays are traced 
in directions toward the pole than near the equator. An example of this can be seen in .gure 2.19. In 
this image there is no ambient light source; anylight in shadow regions is due to diffuse re.ection of 
indirect light. Note in particular how the white checkers are re.ected in the bottom of the teapots, 
and how the spout on the right teapot casts light onto the nearby part of the teapot body. This effect 
is often called color bleeding (although in this case the color is white) and can tint surfaces with 
the colors of nearby objects. 2.7.7 Depth of .eld Real cameras built with lenseshavea .nite aperture 
opening. Asa result, they can only focusata particular distance,and objects that arefar from that distance 
are blurry. (An exception is pin-hole cameras that have a nearly in.nitely-small aperture opening so 
all distances are in focus.) In computer graphics we can simulate the .nite aper­ture opening by tracing 
rays with slightly varying origins and directions as shown in Figure 2.19:Teapots and square with diffuse 
re.ectionof direct and indirect light. .gure 2.20(a). Figure 2.20(b) illustrates the depth-of-.eld effect: 
the front teapot is (mostly)infocus whilethe rearteapotisoutof focus. Distributionray tracingfor more 
realistic camera modelsis describedbyKolbetal.[42]. Figure 2.20:(a) Finiteaperture.(b)Teapots with depthof 
.eld. 2.7.8 Motion blur In real cameras the shutter has to be open a .nite amount of time to capture 
enough light on the .lm or CCD chip. If an object is moving within the shutter opening time, it will 
be blurry in the image. To render this effect, we can shoot the rays at different times within the shutter 
interval. When intersection testing we move the objects to the appropriate time for the ray. Figure 2.21 
showstwo motion blurredteapots.The chrome teapotis moving while the diffuse teapot is both moving and 
rotating around its own axis. The teapots themselves are blurred, and their re.ections and shadows are 
also blurred.  2.8 Spatial acceleration data structures For complex scenes, it would be hopelessly 
inef.cient to test every object for intersec­tion with every ray. We therefore organize the objects in 
a hierarchy so that a large fraction of the objects can be rejected quickly. The most important characteristics 
of an acceleration data structure are construction time, memory use, and ray traversal time. Depending 
on the application, different emphasis may be put on each of these characteristics. For rendering of 
sequences of images (for example for interactive visialization or for shot rendering for movies) it is 
also desirable to choose an acceleration data structure that can be ef.ciently updated with incrementalgeometry 
changes, seeforexample Reinhardetal.[60]andWaldet al.[76]. There is a bewildering array of acceleration 
data structures: bounding volume hi­erarchies, uniform grids, hierarchical grids, BSP-trees, kd-trees, 
octrees, 5D origin­direction trees, bounding interval hierarchies, and so on. Here we will only describe 
one acceleration data structure, the bounding volume hierarchy, in detail. 2.8.1 Bounding volume hierarchy 
Aboundingvolume hierarchy(BVH) organizes the objects and their boundingvolumes intoa tree[39]. The rootof 
the treeisa boundingvolume containingthe entire scene. The most commonly used bounding volume is the 
axis-aligned box since such boxes are easy to compute and combine. For example, the BVH for the teapot 
scene has .ve levels of bounding boxes. The top level consists of a single bounding box for the entire 
scene. The next levels contains the bounding boxes of the two teapots and the square. Each teapot consists 
of four parts: body, lid, handle, and spout. Each part has a bounding box. The teapot body consists of 
eight Bezier patches, each with its own bounding box. For a tessellated Bezier patch, each group of quadrilaterals 
can have a bounding box for ef.cient ray intersection testing. The scene modeling hierarchy can be used 
directly, as in the teapot scene example. Another strategy is to split the geometry such that the surface 
areas are approximately equal in each part[27]. Smits article[67]contains much good advice on ef.cient 
construction and traversal of bounding volume hierarchies. Whenaray needstobe intersection-testedwiththe 
objectsinthe scene,the .rststepis to check for intersection with the bounding box of the entire scene. 
If the ray hits the boundingbox,theboundingboxesofthe childrenaretested,andsoon.Whenaleafof the hierarchyis 
reached, the object represented by the leaf must be intersection tested. 2.8.2 Which acceleration data 
structure is best? None of these acceleration data structures is consistentlyfaster than the other. Which 
one is optimal for a given scene depends on the scene characteristics, and whether the emphasis is onfast 
construction,fast updates,fast ray traversal, or compact memory use.Fora detailed analysis please refertoHavran 
s Ph.D. thesis[30]andthe discus­ sions on RayTracing News [29].  2.9 Ray differentials Eventhoughraydifferentials 
area fundamental propertyof rays, their useforray trac­ing is relatively new. They are useful for manyapplications 
including texture .ltering and tessellation, as will become clear from the following chapters. Aray differential 
describes the differences between a ray and its real or imaginary neighbor rays. The differentials 
give an indication ofthe beam size that each ray represents, as illustrated in .gure 2.22. neighbor 
ray ray beam ray neighbor ray Figure 2.22: Rays and ray beam. 33 2.9.1 Ray propagation and specular re.ection 
Igehy s ray differential method[34]keeps track of ray differentials as rays are prop­ agated and specularly 
re.ected and refracted. The curvature at surface intersection points determines how the ray differentials 
and their associated beams change after specular re.ection and refraction. For example, if a ray hits 
a highly curved, convex surface, the specularly re.ected ray will have a large differential (representing 
highly diverging neighbor rays). Figure 2.23 shows ray-traced specular re.ections. In the left image 
no ray differentials are computedandthetexture .lterwidthiszero; hencethe aliasingartifacts.Intheright 
image, ray differentials are used to determine the proper texture .lter size.To show the differences 
clearly, the resolution of the images is very low (200×200 pixels), only a single re.ectionraywasshotperpixel,andpixel 
.lteringwas turnedoff.  2.9.2 Glossy and diffuse re.ection Suykens andWillems[73]generalized ray differentials 
to glossy and diffuse re.ec­ tions. For distribution ray tracing of diffuse re.ection or ambient occlusion, 
the ray differential corresponds to a fraction of the hemisphere. The more rays are traced from the same 
point, the smaller the subtended hemisphere fraction becomes. If the hemisphere fraction is very small, 
a curvature-dependent differential (as for specular re.ection) becomes dominant.  2.10 Further reading 
For further information about ray tracing, the best starting point is one of the excellent books dedicated 
to ray tracing: An IntroductiontoRayTracing editedbyGlassner[25], and Realistic RayTracing by Shirley 
and Morley[64]. There are also several good books about renderingin generalthat include ray tracing[24,26,63,56]. 
Eric Haines has compiled the on-line RayTracing News [29]since 1987. It contains lots of discussions 
about the .ner points of ray tracing, new developments and insights over the years, etc. Furthermore, 
there have been several SIGGRAPH courses on different aspects of ray tracing, forexample the Monte CarloRayTracing 
course in 2003[35]and theInterac­tive RayTracing coursein 2006[65]. Aseriesof symposia dedicatedtoray 
tracingwas recently started.The .rstRayTrac­ing SymposiumwasinSaltLakeCityin September2006.Itwasagreatevent,andthe 
proceedings[77]are full of the latest ray tracing research results. The next symposium willbein Ulm, 
Germanyin September 2007.Weexpectit and the following symposia to be as inspiring as the .rst.  Chapter3 
Ray tracing in complex scenes In the previous chapters we havemainly looked at simple scenes with fewlight 
sources, relatively few simple objects, and very simplistic shading. However, in practical appli­cations 
such as movie production, the scenes are much more complex: Thousands of light sources.  More textures 
than can .t in memory.  More geometry than can .tin memory (in tessellated form).  Very complex, programmable 
shaders for displacement, illumination, and re.ec­tion. (10,000s lines of code.)  In addition, the images 
have to be of very high quality: High resolution.  Motion blur.  Depth of .eld.  No spatial or temporal 
aliasing (no staircase effects, crawlies , popping, etc.)  In this chapter we will describe various 
techniques to render high-quality images of such complex scenes. 3.1 Many light sources The main expense 
in computing the direct illumination from a light source is typically computingtheshadows.Ifshadowmaps[59]areused,onehasto 
renderand manage a shadow map for each light source. If ray tracing is used and if we had to trace at 
least one shadow ray for each light source, the render times would be unacceptably long. Fortunately, 
shadows from many light sources can be dealt with by sorting the light sources based on their potential 
illumination. Some light sources are so distant and their illumination so dim that they can be approximated 
very coarsely. At each surface point,thedirectilluminationofeachlight sourceiscomputed,thenthelightsaresorted 
accordingto illuminationstrength,and.nallyaprobabilisticselectionisdoneofwhich lights to compute shadows 
for, which ones to compute without shadows, and which onestoskip. Detailscanbe foundinthe articlesbyWard[80]and 
Shirleyetal.[66] andin Shirleyand Morley s book[64]. 3.2 Too many textures When the textures required 
to render an image exceeds the available memory, it be­comes essential to read the textures from disk 
on demand, to read the textures only at the required resolution, and to cache the textures in memory. 
3.2.1 Multiresolution textures AtextureMIPmap[84]isa hierarchyof representationsofatexture.Eachlevelisa 
down-sampled version of the next .ner level; typically a pixel at a given level is the average of four 
pixels at the next .ner level. Figure 3.1 shows the six coarsest levels of a texture MIP map. The coarsest 
level consists of a single pixel, the next level consists of four pixels, and so on. Figure 3.1:Texture 
MIP map. According to Peachey[53], Hanrahan was the .rst to make the observation that for directly visible 
geometry and .xed image resolution, the required number of texture pixelsis roughly constant:Awide-angle 
viewofagiven scene contains manyobjects, but little texture detail is visible on each object. On the 
other hand, a close-up view of oneofthe objectsshowsthetextureof that objectata much higher resolution,but 
does not show the other objects. All in all, the total number of texture pixels seen in the two imagesis 
roughlythe sameifthe appropriateMIPmaplevels are chosen. Fortunately this constant nature is true for 
recursive raytracing as well but only if we use ray differentials to determine the appropriate texture 
resolution at ray hit points. 3.2.2 Texture tiling It is advantageous to tile the textures so that groups 
of nearby pixels are read from disk to memory together. Figure 3.2 shows three levels of a tiled texture 
MIP map. In this example, each tile contains 16×16 pixels. The coarsest MIP map levels (levels 0 3) canbe 
squeezedintoasingletile(notshownhere). MIPmaplevel4 consistsofa single tile. The nextlevelhas2×2tiles 
(still with 16×16 pixels in each tile). The next level has4×4tiles, and so on. Figure 3.2:Tiled textureMIP 
map. 3.2.3 Multiresolution texture tile cache Peachey[53]introduceda multiresolutiontexturetile caching 
scheme.He found that texture accesses are highly coherent for rendering of directly visible geometry, 
and that a cache size of 1% of the total texture size is suf.cient. We have observed a similar result 
for ray tracing when ray differentials are used to select the appropriate MIP maplevelfortexture lookups[17].We 
choosethelevel wherethetexturepixels are approximately the same size as the ray beam cross-section. Incoherent 
rays have wide ray beams, so coarse MIP map levels will be chosen. The .nerMIP map levels will onlybe 
accessedbyrayswith narrowraybeams; fortunatelythoseraysare coherentso the resulting texture cache lookups 
will be coherent as well.  3.3 Geometric complexity This section discusses methods for rendering large, 
complex scenes on a single PC. Utilizing clusters of PCs for parallel speed-ups and even larger scenes 
is described in section 3.4. 3.3.1 Instancing Instancing can sometimes be a great way to generate geometric 
complexity in a scene. For example, it is simple to render a .eld full of sun.owers: only a few unique 
sun­.owershapesneedtobemodeled,andeach sun.owerinstanceisjust representedbyits model ID and transformation 
matrix. This saves a lot of memory compared to having to explicitly copythe sun.ower geometry millions 
of times. It is very simple to ray-trace instanced geometry: simply transform the ray using the inverseofthe 
instance transformationmatrix,andtestfor intersectionwiththe original, untransformed object. If there 
is a hit, transform the hit point and normal back using the instance transformation. 3.3.2 Ray reordering 
and shading caching TheToro renderer[57]reordered the rays to increase the geometric coherency. This 
made it possible to ray-trace scenes that are larger than the main memory of the com­puter. Reordering 
the rays requires that the image contribution of each ray is linear; thisistrueforrealphysical re.ectionsbutnot 
generallytrueforthevery artisticpro­grammable shaders used in movie production. The Razor projectby Stolletal.[71]is 
inspiredbythe REYES algorithm usedfor scanline rendering[18].It shades entiregridsofsurfacepointsatatimeand 
stores the view-independent parts of the shading results. If some of the following rays hit the same 
surface patch, the shading results can be reused. 3.3.3 Geometric stand-ins Waldetal.[75]demonstrated 
interactive renderingof complex scenes(forexamplea Boeing 777 airplane with 350 million triangles) using 
approximate stand-ins for ge­ometry that has not been loaded yet. The precomputation of the stand-ins 
is a rather lengthyprocess,but onceitis completed, the scene canbe ray-traced interactively. 3.3.4 Multiresolution 
tessellation In practical applications, we have found it advantageous to tessellate curves, Bezier patches, 
NURBS surfaces, subdivision surfaces, and any surface with displacement instead of computing ray-surface 
intersections with numeric methods. Patchi.cation and tessellation The surfaces are split into smaller 
surface patchesofa manageable size correspond­ing to tiling of textures. The tessellation rate for a 
directly visible surface patch should depend on viewing distance and surface curvature, and optionally 
also viewing angle. For re.ections or shadows we can often use coarser tessellations. Figure3.3 shows 
an example of .ve tessellations of a surface patch; in this example the .nest tessellation rate is 14×11. 
The coarser levels consist of subsets of the vertices of the .nest tessel­lation. The coarsest tessellation 
is simply the four corners of the patch. One can think of the various levels of tessellation as a MIP 
map of tessellated geometry. Figure 3.3: Multiresolution tessellation example for a surface patch: 14×11 
quads, 7×6quads,4×3quads,2×2quads, and1quad. Multiresolution tessellation cache Pharr and Hanrahan[55]cached 
tessellated geometry for displaced surfacesbut did notexploit multiresolution tessellation.We tessellate 
surface patches ondemand at the required resolution(thendisplacetheverticesif appropriate)and storethe 
tessellations in a cache. Since the size of the tessellations differ so much, the cache can store many 
more coarse tessellations than .ne tessellations. For ray intersection tests, we choose the tessellation 
where the quadrilaterals are ap­proximately the same size as the ray beam cross-section. We have observed 
that ac­cesses to the .ne and medium tessellations are usually very coherent. The accesses to the coarse 
tessellations are rather incoherent,but the cache capacity for coarse tessella­tionsislargeandthose tessellationsarefastto 
recomputeanyway.The.ne tessellations are only neededfor directly visible geometry, for specularre.ections 
and refractions from .at surfaces, and for diffuse and ambient occlusion rays near the ray origins.For 
all other rays, the ray beams are wide andthe medium and coarse tessellations are used. Implementation 
details for our multiresultion tessellation cache can be found in Chris­tensenet al.[17,16].  3.4 Parallel 
execution Ray tracing seems very suitable for parallel speedups: the computations for each pixel is independent 
of all other pixels. This has lead to the common belief that ray tracing is embarassingly parallel . 
However, this is only true if the scene data .t in main memory! If the scene is larger, great care must 
be taken to maintain and exploit data access coherency. It pays offto arrange the execution order such 
that subsequent rays tend to traverse the same geometry and access the same textures, thus ensuring good 
cache behavior. 3.4.1 SIMD instructions Modern CPUs haveSIMD instructions (SSE on Intel, AltiVec on IBM/Motorola, 
3dNow onAMD)thatperformfour operationsin parallel. Waldetal.[79]utilizedthesein­ structions to intersection-test 
four rays in parallel against one triangle. This provides good speedups if the rays are coherent, and 
for visibility rays they reported typical speedups around 3.5. Anotherwayto utilizetheSIMD instructionsisto 
intersection-testonerayagainstfour triangles in parallel. This gives goodspeedups if the triangles are 
coherent as they areiftheycomefromadjacentpositionsonatessellatedsurface anddoesnotrequire theraystobe 
coherent. ThiswasusedbyChristensenetal.[16]. Anotheruseofthe SIMD instructions is to intersection-test 
all three slabs of an axis-aligned bounding box in parallel. 3.4.2 Multiprocessors Muuss[49]implemented 
parallel ray tracers both on multiprocessor machines with shared memory and on distributedcomputers on 
a network. Parkeretal.[52]implementeda parallelray traceronanSGI Origin 2000 supercom­ puterwith64 processors.For 
maximumef.ciency,the scenehadto.t withinthelocal cacheofeach processor(4MBonthe Origin),the shadershadtobeverysimple,andthe 
illumination could consist of only one light source. For such relatively simple scenes theyobtained interactive 
speeds, and despite the restrictions the interactivity was quite an accomplishment. Multiprocessors and 
multi-core architectures seem highly relevant for the future of ray tracing. 3.4.3 Clusters of PCs Waldet 
al.[78]and Kato[38]implemented ray tracerson clustersof standard PCs. Wald srendererwas real-time,while 
Kato sKilaueawasa(non-interactive).lm-quality renderer. Both renderers used a brute-force approach: copy 
the scene to a dozen or more PCs and send raypackets to each PC for intersection testing. Kilauea couldhan­dle 
scenes larger than the memory of a single PC by dividing the scene geometry up into as manyparts as it 
took to store it. Each PC computed ray-packet intersections for the parts of the scene that it contained. 
 3.5 Ray tracing in Pixar movies As an example of the practical use of ray tracing in complex scenes, 
this section de­scribes how ray tracing has been used in the production of recent Pixar movies. Pixar 
s RenderMan renderer (PRMan)[74,2]is based on the REYES scanline render­ ing algorithm[18]. The REYES 
algorithm rendersa small image tileata time; while rendering a tile it can ignore most of the data outside 
that tile hence it can deal with very complex scenes. Traditionally, shadows have been computed with 
shadow maps[59]and re.ectionshave been approximated with re.ection maps[7,28]. Although still based on 
the REYES algorithm, we have extended PRMan with with on-demandray tracing[17,16].With PRMan shybrid 
rendering algorithm there are no visibility rays, but ray tracing can be used to compute e.g. re.ections, 
shadows, and ambient occlusion. Thanks to the use of ray differentials and multiresolution tex­tureand 
tessellation caches,very complex scenes canbe ray-traced withoutextraneous restrictions on the shaders, 
displacements, etc. The.rst useofraytracingina Pixarmovie(asfarasIknow)wasforre.ectionsand refractions 
in a glass bottle in the movie ABug s Life. The ray tracing was done with anexternal plug-in[2]. Sinceno 
algorithms wereinplacetodealwithray tracingof very complex scenes, only a subset of the scene geometry 
was ray traced. The.rst wide-spreaduseofraytracingatPixarwasforambientocclusioninthemovie The Incredibles. 
It is interesting to note that the shaders at Pixar are so complex and time-consuming that the time spent 
tracing rays is less than the time spent evaluating shaders at the ray hit points. This is perhaps the 
main reason for the popularity of ambientocclusion, bothatPixarand elsewhere:itis muchfastertocompute 
ambient occlusion than e.g. ray-traced re.ections. This despite thefact that ambient occlusion usually 
requires tracing many more rays than re.ections. Ray tracing was also used in the movie Cars. In addition 
to ambient occlusion, ray tracing was used to compute realistic re.ections and to compute shadows in 
large out­door scenes with tinyshadow details. Figure 3.4 shows beauty shots of two of the characters 
from Cars. Note in particular the re.ection of the eyes in the hoods. For ef.ciency,the re.ections in 
theCars movie were usually limitedtoa singlelevelof re.ection. There wereonlyafew shots withtwolevelsof 
re.ection;theyare close-ups of chrome parts that needed to re.ect themselves multiple times to get the 
right look. Figure 3.5 shows anexample, the rear chromebumper on Doc Hudson. &#38;#169; 2006 Disney/Pixar.) 
Figure 3.6 shows all the main characters in Cars. (Thisisa sectionofa poster thatwas originally rendered 
3400 pixels wide.) This is a very complex scene with manyshiny cars. The shinycars re.ect other cars, 
as shown in the close-ups. The image also shows ray-traced shadows and ambientocclusion. c Ray tracing 
is also being used in Pixar s latest movie, Ratatouille mostly for ambi­ent occlusion, re.ections in 
pots and pans, and re.ections and refractions in glasses. Figure 3.7 shows an example of ray-tracedre.ections 
and refractions in wine glasses. c ney/Pixar.) Chapter4  APractical Guide to Global Illumination using 
Photon Mapping 4.1 Photon tracing The purpose of the photon tracing pass is to compute indirect illumination 
on diffuse surfaces.Thisisdonebyemittingphotonsfromthelight sources,tracingthemthrough the scene, and 
storing them at diffuse surfaces. 4.1.1 Photon emission This section describes how photons are emitted 
from a single light source and from multiple light sources, and describes the use of projection maps 
which can increase the emission ef.ciencyconsiderably. Emission from a single lightsource The photons 
emitted from a light source should have a distribution corresponding to the distribution of emissive 
power of the light source. This ensures that the emitted photonscarrythesame.ux ie.wedonotwastecomputational 
resourcesonphotons with low power. Photons from a diffuse point light source are emitted in uniformly 
distributed ran­dom directions from the point. Photons from a directional light are all emitted in the 
same direction,but from origins outside the scene. Photons froma diffuse square light source are emitted 
from random positions on the square, with directions limited to a hemisphere. The emission directions 
are chosen from a cosine distribution: there is zero probability of a photon being emitted in the direction 
parallel to the plane of the square, and highest probability of emission is in the direction perpendicular 
to the square. In general, the light source can have any shape and emission characteristics the intensity 
of the emitted light varies with both origin and direction. For example, a (matte) light bulb has a nontrivial 
shape and the intensity of the light emitted from it varies with both position and direction. The photon 
emission should follow this variation, so in general, the probability of emission varies depending on 
the position on the surface of the light source and the direction. Figure 4.1: Emission from light sources: 
(a) point light, (b) directional light, (c) square light, (d) general light. Figure 4.1 shows the emission 
from these different types of light sources. The power ( wattage ) of the light source has to be distributed 
among the photons emitted from it. If the power of the light is Plight and the number of emitted photons 
is ne, the power of each emitted photon is Plight Pphoton = . (4.1) ne Pseudocode for a simple example 
of photon emission from a diffuse point light source is given in Figure 4.2. To further reduce variation 
in the computed indirect illumination (during rendering), it is desirable that the photons are emitted 
as evenly as possible. This can for example be done with strati.cation[Rubinstein81]orby usinglow-discrepancyquasi-random 
sampling[Keller96]. emit photons from diffuse point light() {ne =0 number of emitted photons while (not 
enough photons) { do { use simple rejection sampling to .nd diffuse photon direction x = random number 
between -1 and 1 y = random number between -1 and 1 z = random number between -1 and 1 } while ( x2 + 
y2 + z2 > 1 ) d = < x,y, z > p = light source position trace photon from p in direction d ne = ne +1 
 } scale power of stored photons with 1/ne } Figure 4.2: Pseudocode for emission of photons from a diffuse 
point light Multiple lights If the scene contains multiple light sources, photons should be emitted from 
each light source. More photons should be emitted from brighter lights than from dim lights, to make 
the power of all emitted photons approximately even. (The information in the photon map is best utilizedif 
the power of the stored photons is approximately even). One might worry that scenes with manylight sources 
would require many more pho­tons to be emitted than scenes with a single light source. Fortunately, it 
is not so. In a scene with manylight sources, each light contributes less to the overall illumination, 
and typicallyfewer photons canbe emittedfrom each light. If,however, onlyafew light sources are important 
one might use an importance samplingmap[54]to concen­ trate the photons inthe areas that are of interest 
to the observer. The trickypart about usingan importancemapisthatwedonotwanttogeneratephotonswith energylevels 
that are too different since this will require a larger number of photons in the radiance estimate (see 
section 4.3)to ensure good quality. Projection maps In scenes with sparse geometry, manyemitted photons 
will not hit anyobjects. Emit­ting these photons is a waste of time. To optimize the emission, projection 
maps can be used[Jensen93, Jensen95a].Aprojectionmapis simplyamapofthe geometryas seen from the light 
source. This map consists of many little cells. A cell is on if thereis geometryin thatdirection,and 
off if not.Forexample,a projectionmapis a spherical projectionofthe sceneforapointlight,anditisaplanar 
projectionofthe scene for a directional light. To simplify the projection it is convenient to project 
the bounding sphere around each object or arounda clusterof objects[Jensen95a]. This also signi.cantly 
speeds up the computation of the projection map since we do not have to examine every geometric element 
in the scene. The most important aspect about the projection map is that it gives a conservative estimate 
of the directions in which it is necessary to emit photons from the light source. Had the estimate not 
been conser­vative (e.g. we could have sampled the scene with a few photons .rst), we could risk missing 
important effects, such as caustics. The emission of photons using a projection map is very simple. One 
can either loop over the cells that contain objects and emit a random photon into the directions repre­sented 
by the cell. This method can, however, lead to slightly biased results since the photon map can be full 
before all the cells have been visited. An alternative approach is to generate random directions and 
check if the cell corresponding to that direction has anyobjects (if not a new random direction should 
be tried). This approach usually works well,butit canbe costlyin sparse scenes. For sparse scenesitis 
betterto gen­erate photons randomlyforthe cells whichhave objects.Asimple approachistopick a random cell 
with objects and then pick a random direction for the emitted photon for thatcell[Jensen93].Inall circumstancesitis 
necessarytoscalethe energyofthe stored photons based on the number of active cells in the projection 
map and the number of photons emitted[Jensen93]. This leads to a slight modi.cation of formula 4.1: Plight 
cells with objects Pphoton = . (4.2) ne total number of cells Another important optimization for the 
projection map is to identify objects with specu­lar properties (i.e. objects that can generate caustics)[Jensen93]. 
As it will be described later, caustics aregenerated separately, and since specular objects often are 
distributed sparsely it is very bene.cial to use the projection map for caustics. 4.1.2 Photon tracing 
Once a photon has been emitted, it is traced through the scene using photon tracing (also known as light 
ray tracing , backward ray tracing , forward ray tracing , and backwardpath tracing ). Photon tracingworksinexactlythe 
samewayasray tracing except for thefact that photons propagate .ux whereas raysgather radiance. Thisis 
an important distinction since the interaction of a photon with a material can be different thanthe interactionofaray.Anotableexampleis 
refraction where radianceis changed based on the relative index of refraction[Hall88] this does not happen 
to photons. Figure4.3: Photonpathsinascene(a Cornellbox witha chromesphereonleftandaglass sphereon right):(a)twodiffusere.ections 
followedby absorption,(b)aspecularre.ection followedbytwodiffuse re.ections,(c)two specular transmissions 
followedby absorption. When a photon hits an object, it can either be re.ected, transmitted, or absorbed. 
Whether it is re.ected, transmitted, or absorbed is decided probabilistically based on the material parameters 
of the surface. The technique used to decide the type of in­teractionis known as Russian roulette[Arvo90] 
basically we rolla dice and decide whether the photon should survive andbe allowed to perform another 
photon tracing step. Examples of photon paths are shown in Figure 4.3. Re.ection, transmission, or absorption? 
For a simple example, we .rst consider a monochromatic simulation. For a re.ective surface witha diffuse 
re.ection coef.cient d and specular re.ection coef.cient s (with d + s = 1)we use a uniformlydistributed 
random variable . . [0, 1] (computed with for example drand48())and make the following decision: . . 
[0,d] -. diffuse re.ection . .]d, s + d] -. specular re.ection . .]s + d, 1] -. absorption In this simple 
example, the use of Russian roulette means that we do not have to mod­ifythepowerofthe re.ectedphoton 
the correctnessis ensuredbyaveragingseveral photon interactions over time. Consider for example a surface 
that re.ects 50% of the incoming light. With Russian roulette only half of the incoming photons will 
be re.ected, but with full energy. For example, if you shoot 1000 photons at the sur­face, you can either 
re.ect 1000 photons with halfthe energy or 500 photons with full energy. It can be seen that Russian 
roulette is a powerful technique for reducing the computational requirements for photon tracing. With 
more color bands (forexample RGB colors), the decision gets slightly more com­plicated. Consider again 
a surface with some diffuse re.ection and some specular re.ection,but this time with different re.ection 
coef.cients in the three color bands. The probabilities for specular and diffuse re.ection can be based 
on the total energy re.ected by each type of re.ection or on the maximum energy re.ected in anycolor 
band. If we base the decision on maximum energy, we can for example compute the probability Pd for diffuse 
re.ection as max(drPr,dgPg,dbPb) Pd = max(Pr,Pg,Pb) where (dr,dg,db) arethediffuse re.ectioncoef.cientsinthered, 
green,andblue color bands, and (Pr,Pg,Pb) are the powers of the incident photonin the same three color 
bands. Similarly, the probability Ps for specular re.ection is max(srPr,sgPg,sbPb) Ps = max(Pr,Pg,Pb) 
where (sr,sg,sb) are the specular re.ection coef.cients. The probability of absorbtion is Pa =1 - Pd 
- Ps. With these probabilities, the decision of which type of re.ection or absorption should be chosen 
takes the following form: . . [0,Pd] -. diffuse re.ection . .]Pd,Ps + Pd] -. specular re.ection . .]Ps 
+ Pd, 1] -. absorption The power of the re.ected photon needs to be adjusted to account for the probability 
of survival. If, for example, specular re.ection was chosen in the example above, the power Prefl of 
the re.ected photon is: Prefl,r = Pinc,r sr/Ps Prefl,g = Pinc,g sg/Ps Prefl,b = Pinc,b sb/Ps where Pinc 
is the power of the incident photon. Thecomputed probabilitiesagain ensureusthatwedonotwastetimeemittingphotons 
with very low power. Itissimpletoextendthe selectionschemetoalso handle transmission,to handle more 
than three color bands, and to handle other re.ection types (for example glossy and directional diffuse). 
WhyRussian roulette? Whydo we go through this effort to decide what to do with a photon? Why not just 
trace new photons in the diffuse and specular directions and scale the photon energy accordingly? Therearetwomain 
reasonswhytheuseof Russian rouletteisaverygood idea. Firstly, we preferphotons with similarpowerin the 
photon map. This makes the radiance estimate much better using only a few photons. Secondly, if we generate, 
say, two photons per surface interaction then we will have 28 photons after8interactions. This means 
256 photons after 8 interactions compared to 1 photon coming directly from the light source! Clearly 
this is not good. We want at least as manyphotons that have only 1 2 bounces as photons that have made 
5 8 bounces. The use of Russian roulette is therefore very important in photon tracing. Thereis onecaveat 
with Russian roulette.It increasesvarianceonthe solution. Instead of using the exact values for re.ection 
and transmission to scale the photon energy we nowrelyonasamplingofthesevaluesthatwillconvergetothe correctresultasenough 
photons are used. Detailson photon tracingand Russian roulette canbe foundin[Shirley90,Pattanaik93, 26]. 
 4.1.3 Photon storing This section describes which photon-surface interactions are stored in the photon 
map. It also describes in more detail the photon map data structure. Which photon-surface interactions 
are stored? Photons are only stored where theyhit diffuse surfaces (or,more precisely,non-specular surfaces). 
The reason is that storing photons on specular surfaces does not give any useful information: the probability 
of having a matching incoming photon from the specular direction is zero, so if we want to render accurate 
specular re.ections the best way is to trace a rayin the mirror direction using standard raytracing. 
For all other photon-surface interactions, data is stored in a global data structure, the photon map. 
Note that each emitted photon can be stored several times along its path. Also, infor­mation about a 
photon is stored at the surface where it is absorbed if that surface is diffuse. (a) (b) For each photon-surface 
interaction, the position, incoming photon power, and incident direction are stored. (For practical reasons, 
there is also space reserved for a .ag with eachsetofphoton data.The.agisusedduring sortingand look-upinthephotonmap. 
More on this in the following.) As an example, consider again the simple scene from Figure 4.3, a Cornell 
box with twospheres. Figure 4.4(a)showsa traditionalray tracedimage(direct illuminationand specular re.ection 
and transmission) of this scene. Figure 4.4(b) shows the photons in the photon map generatedfor this 
scene. The high concentration of photons under the glass sphere is caused by focusing of the photons 
by the glass sphere. Data structure Expressed in C the following structureis used for each photon[Jensen96b]: 
Thepowerofthephotonis represented compactlyas4bytesusingWard spackedrgb­format[80].If memoryisnotof concernonecanuse3.oatsto 
storethepowerinthe red,green,and blue color band(or,ingeneral, one .oatper color bandifa spectral simulation 
is performed). struct photon { float x,y,z; // position char p[4]; // power packed as 4 chars char phi, 
theta; // compressed incident direction short flag; // flag used in kdtree } The incident directionisa 
mappingofthe spherical coordinatesofthe photon direction to 65536 possible directions.They are computed 
as: phi = 255 * (atan2(dy,dx)+PI) / (2*PI) theta = 255 * acos(dx) / PI where atan2 is from the standard 
C library. The direction is used to compute the contribution for non-Lambertian surfaces[Jensen96a], 
and for Lambertian surfaces it isusedtocheckifaphotonarrivedatthefrontofthesurface. Sincethephoton direction 
is used often during rendering it pays to have a lookup table that maps the theta, phi direction to three 
.oats directly instead of using the formula for spherical coordinates which involves the use of the costly 
cos() and sin() functions. A minor note is that the .ag in the structure is a short. Only 2 bits of this 
.ag are used(thisisforthe splittingplaneaxisinthekd-tree),anditwouldbe possibletouse just one byte for 
the .ag. However for alignment reasons it is preferable to have a 20 byte photon rather thana19byte photon 
on some architecturesitisevena necessity since the .oat-valuein subsequent photons mustbe aligned ona4byte 
address. Wemightbeableto compressthe information morebyusingthefactthatweknowthe cube in which the photon 
is located. The position is, however, used very often when the photons are processed and by using standard.oat 
we avoid the overhead involved in extracting the true position from a specialized format. During the 
photon tracing pass the photon map is arranged as a .at array of photons. Foref.ciencyreasons this arrayis 
re-organized intoa balanced kdtree before rendering as explained in section 4.2. 4.1.4 Extension to 
participating media Up to this point, all photon interactions have been assumed to happen at object sur­faces; 
all volumes were implicitly assumed to not affect the photons. However, it is simple to extend the photon 
map method to handle participating media,i.e. volumes that participate in the light transport. In scenes 
with participating media, the photons are stored within the mediaina seperate volume photon map [Jensen98]. 
Photon emission, tracing, and storage Photonscanbe emittedfromvolumesaswellasfromsurfacesandpoints.Forexample, 
the light fromacandle .ame canbe simulatedbyemitting photons froma.ame-shaped volume. When a photon travels 
through a participating medium, it has a certain probability of beingscattered or absorpedin the medium. 
The probability dependson thedensityof the medium and on the distance the photon travels through the 
medium: the denser the medium, the shorter theaverage distance beforea photon interaction happens. Photons 
are stored at the positions where a scattering event happens. The exception is photons thatcomedirectlyfromthelight 
sourcesincedirect illuminationisevaluatedusingray tracing. This separationwas introducedin[Jensen98]andit 
allowsusto computethe in-scattered radiance in a medium simply by a lookup in the photon map. Asanexample, 
consider a glass sphereinfog illuminatedbydirectionallight. Fig­ure4.5(a)showsa schematicdiagramofthephotonpathsasphotonsarebeingfocused 
by refraction in the glass sphere. Figure 4.5(b) shows the caustic photons stored in the photon map. 
56 Multiple scattering, anisotropic scattering, and non-homogeneous media The simpleexampleaboveonlyshowsthe 
photon interactioninthefog after refraction by the glass sphere, and the photon paths are terminated 
at the .rst scattering event. General multiple scattering is simulated simply by letting the photons 
scatter every­where and continuously after the .rst interaction. The path can be terminated using Russian 
roulette. The fog in the example has uniform density, but it is not dif.cult to handle media with nonuniform 
density (aka. nonhomogeneous media), since we use ray marching to integrate the properties of the medium. 
A simple ray marcher works by dividing the medium into little steps[Ebert94]. The accumulated density 
(integratedextinction coef.cient) is updated at each step, and based on a precomputed probability it 
is de­termined whether the photon should be absorbed, scattered, or whether another step is necessary. 
For more complicatedexamplesof scatteringin participating media, including anisotropic and nonhomogeneous 
media and complex geometry, see[Jensen98]. 4.1.5 Three photon maps For ef.ciencyreasons, it pays offto 
divide the stored photons into three photon maps: Caustic photon map: contains photons that have been 
through at least one specular re.ection before hitting a diffuse surface: LS+D. Global photon map: an 
approximate representation of the global illumination solu­tion for the scene for all diffuse surfaces: 
L{S|D|V }*D Volume photon map: indirect illumination of a participating medium: L{S|D|V }+V . Here,weusedthe 
grammarfrom[Heckbert90]to describethephotonpaths:L means emission from the light source, S is specular 
re.ection or transmission, D is diffuse (ie. non-specular) re.ection or transmission, and V is volume 
scattering. The notation * {x|y|z} means one of x, y, or z , x+ means one or several repeats of x, and 
x means zero or several repeats of x. The reason forkeeping three separate photon maps will become clear 
in section 4.4. Aseparatephoton tracingpassis performedforthe causticphotonmap sinceit should be of 
highquality and therefore often needs more photons than the global photon map and the volume photon map. 
 The construction of the photon maps is most easily achieved by using two separate photon tracing steps 
in order tobuild the caustics photon map and the global photon map (including the volume photon map). 
This is illustrated in Figure 4.6 for a simple test scenewithaglasssphereand2diffusewalls. Figure 4.6(a)showsthe 
construction of the caustics photon map with a dense distribution of photons, and Figure 4.6(b) shows 
the construction of the global photon map with a more coarse distribution of photons.  4.2 Preparingthephotonmapforrendering 
Photons are only generated during the photon tracing pass in the rendering pass the photon map is a 
static data structure that is used to compute estimates of the incoming .ux and the re.ected radiance 
at manypoints in the scene. To do this it is necessary to locate the nearest photons in the photon map. 
This is an operation that is done extremely often, and it is therefore a good idea to optimize the representation 
of the photonmapbeforethe renderingpasssuchthat.ndingthe nearestphotonsisasfastas possible. First, we 
need to select a good data structure for representing the photon map. The data structure shouldbe compactandatthe 
same time allowforfast nearest neighbor searching.It should alsobe ableto handle highlynon-uniform distributions 
 thisis very often the case in the caustics photon map. Anatural candidate that handles these requirementsis 
a balanced kd-tree [Bentley75]. Examples of using a balanced versus an unbalanced kd-tree canbe foundin[Jensen96a]. 
4.2.1 The balanced kd-tree The time it takes to locate one photon in a balanced kd-tree has a worst time 
perfor­mance of O(log N), where N is the number of photons in the tree. Since the photon mapis createdbytracing 
photons randomlythrougha modelonemight thinkthatady­namicallybuilt kd-treewouldbequitewell balanced already.However,thefactthatthe 
generation of the photons at the light source is based on the projection map combined with thefact that 
models often contain highly directional re.ectance models easily re­sults in a skewed tree. Since the 
tree is created only once and used manytimes during rendering itis quite natural to consider balancing 
the tree. Another argument that is perhapseven more importantis thefact thata balanced kd-tree canbe 
represented us­inga heap-like data-structure[Sedgewick92]whichmeans thatexplicitly storing the pointers 
to the sub-trees at each node is no longer necessary. (Array element1is the tree root, and element i 
has element 2i as left child and element 2i +1 as right child.) This can lead to considerable savings 
in memory when a large number of photons is used. 4.2.2 Balancing Balancing a kd-tree is similar to 
balancing a binary tree. The main difference is the choice at each node of a splitting dimension. When 
a splitting dimension of a set is selected, the median ofthe points in that dimension is chosen as the 
root node of the tree representing the set and the left and right subtrees are constructed from the two 
sets separatedby the median point. The choiceofa splitting dimensionis based on the distributionofpointswithintheset.Onemightuse 
eitherthevarianceorthe maximum distance between the points as a criterion. We prefer a choice based upon 
maximum distance since it can be computed very ef.ciently (even though a choice based upon variance might 
be slightly better). The splitting dimension is thus chosen as the one which has the largest maximum 
distance between the points. Figure 4.7 containsa pseudocode outline for the balancing algorithm[Jensen96c]. 
To speed up the balancing process it is convenient to use an array of pointers to the photons. In this 
way only pointers needs to be shuf.ed during the median search. An ef.cient median search algorithm can 
be found in most textbooks on algorithms see forexample[Sedgewick92]or[Cormen89]. The complexity of 
the balancing algorithm is O(N log N) where N is the number of photons in the photon map. In practice, 
this step only takes a few seconds even for several million photons. kdtree *balance( points ) { Find 
the cube surrounding the points Select dimension dim in which the cube is largest Find median of the 
points in dim s1 = all points below median s2 = all points above median node = median node.left = balance( 
s1 ) node.right = balance( s2 ) return node } Figure 4.7: Pseudocode for balancing the photon map  4.3 
The radiance estimate Afundamentalcomponent of the photon map method is the ability to compute radiance 
estimates at anynon-specular surface point in anygiven direction. 4.3.1 Radiance estimate at a surface 
The photon map can be seen as a representation of the incoming .ux; to compute radiance we need to integrate 
this information. This can be done using the expression for re.ected radiance: Lr(x, .)=fr(x, .I,.)Li(x, 
.I)|nx · .I| d.iI, (4.3) Ox where Lr is the re.ected radiance at x in direction .. Ox is the (hemi)sphere 
of incom­ing directions, fr is the BRDF (bidirectional re.ectance distribution function)[Nicodemus77] 
at x and Li is the incoming radiance. To evaluate this integral we need information about the incoming 
radiance. Since the photon map provides information about the incoming .ux we need to rewrite this term. 
This can be done using the relationship between radiance and .ux: d2Fi(x, .I) Li(x, .I)= , (4.4) cos 
.i d.I dAi i and we can rewrite the integral as d2Fi(x, .I) (x, .)=(x, .I,.) |nx · .I| d.I Lrfri cos 
.i d.I dAi i Ox d2Fi(x, .I) = fr(x, .I, .) dAi . (4.5) Ox  Figure 4.8: Radiance is estimated using 
the nearest photons in the photon map. The incoming .ux Fi is approximated using thephoton mapby locating 
the n photons that has the shortest distance to x. Each photon p has the power .Fp(.p)and by assuming 
that the photons intersects the surface at x we obtain n .Fp(x, .p) Lr(x, .) fr(x, .p,.) . (4.6) .A 
p=1 The procedure can be imagined as expanding a sphere around x until it contains n photons (see Figure 
4.8)and then using thesen photons to estimate the radiance. Equation 4.6 still contains .A whichis related 
to the density of the photons around x. By assuming that the surface is locally .at around x we can compute 
this area by projecting the sphere onto the surface and use the area of the resulting circle. This is 
indicatedby the hatched areain Figure 4.8 and equals: .A = pr2 , (4.7) where r is the radius of the sphere 
 ie. the largest distance between x and each of the photons. This results in the following equation for 
computing re.ected radiance at a surface using the photon map:  1 N Lr(x, .) fr(x, .p,.).Fp(x, .p) 
. (4.8) pr2 p=1 This estimate is based on manyassumptions and the accuracydepends on the number of photons 
used in the photon map and in the formula. Since a sphere is used to locate the photons one might easily 
include wrong photons in the estimate in particular in corners and at sharp edges of objects. Edges and 
corners also causes the area estimate to be wrong. The size of those regions in which these errors occur 
depends largely on Figure 4.9: Using a sphere (left) and using a disc (right) to locate the photons. 
the number of photons in the photon map and in the estimate. As more photons are used in the estimate 
and in the photon map, formula 4.8 becomes more accurate. If we ignore the error due to limited accuracyof 
the representation of the position, direction and .ux, then we can go to the limit and increase the number 
of photons to in.nity. This gives the following interesting result where N is the number of photons in 
the photon map: .Na. 1 lim fr(x, .p,.).Fp(x, .p)= Lr(x, .) for a .]0, 1[ . (4.9) N.8 pr2 p=1 This formulation 
applies to all points x located on a locally .at part of a surface for which the BRDF, does notcontain 
the Dirac delta function (this excludes perfect spec­ular re.ection). The principle in equation 4.9 is 
that not only will an in.nite amount of photonsbeusedto representthe.uxwithinthemodelbutan in.nite amountofphotons 
will also be used to estimate radiance and the photons in the estimate will be located within an in.nitesimal 
sphere. The different degrees of in.nity are controlled by the term Na where a .]0, 1[. This ensures 
that the number of photons in the estimate will be in.nitely fewer than the number of photons in the 
photon map. Equation 4.9 means that we can obtain arbitrarily good radiance estimatesbyjust using enoughphotons!In 
.nite element based approachesitis more complicatedto obtain arbitrary accuracysince the error depends 
on the resolution of the mesh, the resolution of the directional representation of radiance and the accuracyof 
the light simulation. Figure 4.8 shows how locating the nearest photons is similar to expanding a sphere 
around x andusingthephotons withinthissphere.Itis possibletouseothervolumes than the sphere in this process. 
One might use a cube instead, a cylinder or perhaps a disc. This could be useful to either obtain an 
algorithm that isfaster at locating the nearest photonsorperhaps moreaccurateinthe selectionof photons.Ifadifferentvol­ume 
is used then .A in equation 4.6 shouldbe replacedby the areaofthe intersection between the volume and 
the tangent plane touching the surface at x. The sphere has the obvious advantage that the projected 
area and the distance computations are very simple and thus ef.ciently computed. A more accurate volume 
can be obtained by modifying the sphere into a disc (ellipsoid) by compressing the sphere in the direction 
of the surface normal at x (shown in Figure 4.9)[Jensen96c]. The advantage of using a disc would be that 
fewer false photons are used in the estimate at edges and in corners. This modi.cation works pretty wellat 
the edges in a room, for instance, since it prevents photons on the walls to leak down to the .oor. One 
issue that still occurs, however, is that the area estimate might be wrong or photons may leak into areas 
where theydo not belong. This problemis handled primarilyby the useof .ltering. 4.3.2 Filtering If the 
number of photons in the photon map is too low, the radiance estimates becomes blurry at the edges. This 
artifact can be pleasing when the photon map is used to esti­mate indirect illumination for a distribution 
raytracer (see section 4.4 and Figure 4.15) but it is unwanted in situations where the radiance estimate 
represents caustics. Caus­ticsoftenhavesharpedgesanditwouldbeniceto preservetheseedges withoutrequir­ing 
too manyphotons. To reduce the amount of blur at edges, the radiance estimate is .ltered. The idea behind 
.ltering is to increase the weight of photons that are close to the point of interest, x. Since we use 
a sphere to locate the photons it would be natural to assume that the .lters should be three-dimensional. 
However, photons are stored at surfaces which are two-dimensional. The area estimate is also based on 
the assumption that photons are located on a surface. We therefore need a 2d-.lter (similar to image 
.lters) which is normalized over the region de.nedby the photons. Theideaof .ltering causticsisnotnew. 
Collins[Collins94]hasexaminedseveral .lters in combination with illumination maps. The .lters we have 
examined are two radially symmetric .lters: the cone .lter and the Gaussian .lter[Jensen96c], and the 
special­ izeddifferential .lter introducedin[Jensen95a].Forexamplesof moreadvanced .lters see Myszkowski 
et al.[Myszkowski97]. The cone .lter The cone-.lter[Jensen96c]assignsaweight,wpc,to each photon based 
on the distance, dp, between x and the photon p. This weight is: dp wpc =1 - , (4.10) kr 63 where k = 
1 isa .lter constant characterizingthe.lterand r is the maximum distance. 2 The normalization of the 
.lter based on a 2d-distribution of the photons is 1 - and 3k the .ltered radiance estimate becomes: 
N fr(x, .p,.).Fp(x, .p)wpc Lr(x, .) p=1 . (4.11) 2 (1 - )pr2 3k The Gaussian .lter The Gaussian .lter[Jensen96c]has 
previously been reported togive good results when .ltering caustics in illumination maps[Collins94]. 
It is easy to use the Gaussian .lter with the photon map since we do not need to warp the .lter to some 
surface function. Instead we use the assumption about the locally .at surfaces and we can use a simple 
image based Gaussian .lter[Pavicic90]and the weightwpg of each photon becomes . 2 . 2 1 - e -ß dp wpg 
= a .1 - 2r. , (4.12) 1 - e-ß where dp is the distance between the photon p and x and a =0.918 and ß 
=1.953 (see[Pavicic90]for details). This .lter is normalized and the only change to equa­tion 4.8is that 
each photon contributionis multipliedby wpg: N Lr(x, .) fr(x, .p,.).Fp(x, .p)wpg . (4.13) p=1 Differential 
checking In[Jensen95a]itwas suggestedtousea.lterbasedondifferential checking.Theidea is to detect regions 
near edges in the estimation process and use less photons in these regions.Inthiswaywemightgetsomenoiseinthe 
estimatebutthatisoften preferable to blurry edges. The radiance estimate is modi.ed based on the following 
observation: when adding photons to the estimate, near an edge the changes of the estimate willbe monotonic. 
That is, if we are just outside a caustic and we begin to add photons to the estimate (by increasing 
the size of the sphere centered at x that contains the photons), then it can be observed that the value 
of the estimate is increasing as we add more photons; and vice versa when we are inside the caustic. 
Based on this observation, differential checking can be added to the estimate we stop adding photons 
and use the estimate available if we observe that the estimate is either constantly increasing or decreasing 
as more photons are added. 4.3.3 The radiance estimate in a participating medium Fortheradiance estimate 
presentedsofarwehaveassumedthatthephotonsare located onasurface.For photonsina participating mediumthe 
formula changesto[Jensen98]: Li(x, .)= f(x, .I,.) L(x, .I) d.I O d2F(x, .I) = f(x, .I,.) d.I O ss(x) 
d.I dV 1 d2F(x, .I) = f(x, .I,.) ss(x) O dV n 1 .Fp(x, .pI ) f(x, .I ,.) , (4.14) p4 ss(x) pr3 p=1 3 
4 where Li is the in-scattered radiance, and the volume dV = 3 pr3 is the volume of the sphere containing 
the photons. ss(x) is the scattering coef.cient at x and f is the phase-function. 4.3.4 Locating the 
nearest photons Ef.ciently locating the nearest photons is critical for good performance of the photon 
map algorithm. In scenes with caustics, multiple diffuse re.ections, and/or participat­ing media there 
can be a large number of photon map queries. Fortunatelythe simplicityofthe kd-tree permitsusto implementa 
simplebut quite ef.cient search algorithm. This search algorithm is a straight forward extension of standard 
search algorithms for binary trees[Cormen89, Sedgewick92, Horowitz93]. It is also related to range searching 
where kd-trees are commonly used as they have optimal storage and good performance [Preparata85]. The 
nearest neighbors query for kd-trees has been described extensively in several publications by Bentley 
et al. including[Bentley75, Bentley79a, Bentley79b, Bentley80]. More recent publications include[Preparata85, 
Sedgewick92]. Someof these papersgobeyond our description of a nearest neighbors query by adding modi.cations 
and extensions to the kd-tree to furtherreducethecostof searching.Wedonotimplementtheseextensionsbecausewe 
want to maintain the low storage overhead of the kd-tree as this is an important aspect of the photon 
map. Locatingthe nearest neighborsina kd-treeis similarto range searching[Preparata85] inthe sensethatwewantto 
locatephotons withinagivenvolume.Forthephotonmap it makes sense to restrict the size ofthe initial search 
range since the contribution from a .xed number of photons becomes small for large regions. This simple 
observation is particularly important for caustics since theyoften are concentrated in a small region. 
Asearch algorithm that does not limit the search range will be slow in such situations since a large 
part of the kd-tree will be visited for regions with a sparse number of photons. Ageneric nearest neighbors 
search algorithm begins at the root of the kd-tree, andadds photons to a list if they are within a certain 
distance. For then nearest neighbors the listis sorted such thatthe photon thatis furthestaway canbe 
deletedifthe list contains n photons and a new closer photon is found. Instead of naive sorting of the 
full list it is betterto usea max-heap[Preparata85, Sedgewick92, Horowitz93].Amax-heap (also knownasa 
priorityqueue)isaveryef.cientwayofkeepingtrackofthe element that is furthestaway from the pointof interest. 
When the max-heapis full, we can use the distance d to the root element (ie. the photon that is furthest 
away) to adjust the range of the query. Thus we skip parts ofthe kd-tree that are further away than d. 
Another simple observationis thatwe can use squared distances wedo not need the real distance. This 
removes the need of a square root calculation per distance check. The pseudo-codeforthe searchalgorithmisgivenin 
Figure 4.10.Asimple implemen­ tationof this routineisavailable with source codeat[MegaPov00]. For this 
search algorithm it is necessary to provide an initial maximum search radius. A well-chosen radius allows 
for good pruning of the search reducing the number of photons tested. A maximum radius that is too low 
will on the other hand introduce noise in the photon map estimates. The radius can be chosen based on 
an error metric or the size of the scene. The error metric could for example take the average energy 
of the storedphotons into account and computea maximum radius from that assuming some allowed error in 
the radiance estimate. Afewextra optimizationscanbeaddedtothis routine,forexampleadelayed construc­tion 
of the max heap to the time when the number of photons needed has been found. This is particularly useful 
when the requested number of photons is large. NathanKopp has implementeda slightly different optimizationin 
anextendedversion ofthe PersistenceOfVisionRayTracer(POV)called MegaPov (available at[MegaPov00]). In 
his implementation the initial maximum search radius is set to a very low value. If this value turns 
out to be too low, another search is performed with a higher maximum radius. He reports good timings 
and results from this technique[Kopp99]. Another change to the search routine is to use the disc check 
as described earlier. This is usefultoavoid incorrectcolor bleedingand particularly helpfulifthegatheringstep 
is not used and the photons are visualized directly. given the photon map, a position x and a max searchdistance 
d2 this recursive function returns a heap h with the nearest photons. Call with locate photons(1) to 
initiate searchat the root of the kd-tree locate photons( p ) {if ( 2p +1 < number of photons ) { examine 
child nodes Compute distance to plane (just a subtract) d = signed distance to splitting plane of node 
n if (d< 0) { We are left of the plane -search left subtree .rst locate photons( 2p ) if ( d2 <d2 ) locate 
photons( 2p +1 ) checkright subtree } else { We are right of the plane -search right subtree .rst locate 
photons( 2p +1 ) if ( d2 <d2 ) locate photons( 2p ) checkleft subtree } } Compute true squared distance 
to photon d2 = squared distance from photon p to x if ( d2 <d2 ) { Check if the photon is close enough? 
insert photon into max heap h Adjust maximum distance to prune the search d2 = squared distance to photon 
in root node of h }} Figure 4.10: Pseudocode for locating the nearest photons in the photon map  4.4 
Rendering Given the photon map and the ability to compute a radiance estimate from it, we can proceed 
with the rendering pass. The photon map is view independent, and therefore a single photonmap constructuredforanenvironment 
canbe utilizedto renderthe scene from anydesired view. There are several different ways in which the 
photon map can be visualized.A veryfast visualization technique has been presentedby Myszkowski etal.[Myszkowski97,Volevich99]wherephotonsareusedtocompute 
radiosityvalues at the vertices of a mesh. In this note we will focus on the full global illumination 
approach as presented in[Jensen96b]. Initially we will ignore the presence of participating media; at 
the end of the note we have added some notes for this case. The .nal image is rendered using distribution 
ray tracing in which the pixel radiance is computedbyaveraginga numberof sample estimates. Each sample 
consistsof tracing 67 Figure 4.11:Tracingaray throughapixel. a ray from the eye through a pixel into 
the scene (see Figure 4.11). The radiance returnedbyeachrayequalsthe outgoing radianceinthe directionoftherayleavingthe 
point of intersection at the .rst surface intersected by the ray. The outgoing radiance, Lo, is the sum 
of the emitted, Le, and the re.ected radiance Lo(x, .)= Le(x, .)+ Lr(x, .) , (4.15) where the re.ected 
radiance, Lr, is computed by integrating the contribution from the incoming radiance, Li, Lr(x, .)= fr(x, 
.I,.)Li(x, .I) cos .i d.i I , (4.16) Ox where fr is the bidirectional re.ectance distribution function 
(BRDF), and Ox is the set of incoming directions around x. Lr can be computed using Monte Carlo integration 
techniques likepath tracing and distribution ray tracing. These methods are very costly in terms of rendering 
time and a more ef.cient approach can be obtained by using the photon map in combination with our knowledge 
of the BRDF and the incoming radiance. The BRDF is separated into a sum of two components: Aspecular/glossy, 
fr,s, and a diffuse, fr,d fr(x, .I,.)= fr,s(x, .I,.)+ fr,d(x, .I,.) . (4.17) The incoming radiance is 
classi.ed using three components: Li,l(x, .I) is direct illumination by light coming from the light 
sources.  Li,c(x, .I) is caustics indirect illumination from the light sources via specular re.ection 
or transmission.  Li,d(x, .I) is indirect illumination from the light sources which has been re­.ected 
diffusely at least once. The incoming radiance is the sum of these three components: Li(x, .I)= Li,l(x, 
.I)+ Li,c(x, .I)+ Li,d(x, .I) . (4.18) By using the classi.cations of the BRDF and the incoming radiance 
we can split the expression for re.ected radiance into a sum of four integrals: Lr(x, .) = fr(x, .I, 
.)Li(x, .I) cos .i d.I i Ox = fr(x, .I, .)Li,l(x, .I) cos .i d.I i + Ox fr,s(x, .I, .)(Li,c(x, .I) + 
Li,d(x, .I)) cos .i d.I i + Ox fr,d(x, .I, .)Li,c(x, .I) cos .i d.I i + Ox fr,d(x, .I, .)Li,d(x, .I) 
cos .i d.I i . (4.19) Ox This is the equation used whenever we need to compute the re.ected radiance 
from a surface. In the following sections we discuss the evaluation of each of the integrals in the equation 
in more detail. We distinguish between two different situations: an accurate and an approximate. The 
accurate computation is used if the surface is seen directly by the eye or perhaps via a few specular 
re.ections. It is also used if the distance between the ray origin and the intersection point is below 
a small threshold value to eliminate potential inaccurate color bleeding effects in corners. The approximate 
evaluation is used if the ray intersectingthesurfacehasbeen re.ecteddiffuselysinceitlefttheeyeoriftheray 
contributes only little to the pixel radiance. 4.4.1 Direct illumination Direct illumination is given 
by the term fr(x, .I,.)Li,l(x, .I) cos .i d.i I , Ox and it represents the contribution to the re.ected 
radiance due to direct illumination. This term is often the most important part of the re.ected radiance 
and it has to be computed accuratelysinceit determineslighteffectstowhichtheeyeishighly sensitive such 
as shadow edges. Computing the contribution from the light sources is quite simple in ray tracing based 
methods. At the point of interest shadow rays are sent towards the light sources to test forpossible 
occlusionby objects. Thisis illustratedin Figure 4.12. Ifa shadow ray does not hit an object the contribution 
from the light source is includedin the integral otherwise it is neglected. For large area light sources 
several shadow rays are used to properly integrate the contribution and correctly render penumbra regions. 
This strategy can however be very costly since a large number of shadow rays is needed to properly integrate 
the direct illumination. Usingaderivativeofthephotonmap methodwecancomputeshadowsmoreef.ciently using 
shadow photons[Jensen95c]. This approach can lead to considerable speedups in scenes with large penumbra-regions 
that are normally very costly to render using standard ray tracing. The approach is stochastic though, 
so it mightmiss shadows from small objectsin case these aren t intersectedby anyphotons. Thisisa problem 
with all techniques that use stochastic evaluation of visibility. The approximate evaluation is simply 
the radiance estimate obtained from the global photon map (no shadow rays or light source evaluations 
are used). This is seen in Figure 4.15 where the global photon map is used in the evaluation of the incoming 
light for the secondary diffuse re.ection.  4.4.2 Specular and glossy re.ection Specular and glossy 
re.ection is computed by evaluation of the term fr,s(x, .I,.)(Li,c(x, .I)+ Li,d(x, .I)) cos .i d.i I 
. Ox The photon map is not used in the evaluation of this integral since it is strongly dom­inated by 
fr,s which hasa narrow peak aroundthe mirror direction. Using the photon mapto optimizetheintegralwouldrequireahuge 
numberofphotonsinordertomakea useful classi.cation of the different directions within the narrow peak 
of fr,s. To save memory this strategy is not used and the integral is evaluated using standard Monte 
Carlo ray tracing optimized with importance sampling based on fr,s. This is still quite ef.cientforglossysurfacesandtheintegralcaninmost 
situationsbe computedusing only a small number of sample rays. This is illustrated in Figure 4.13. 4.4.3 
Caustics Caustics are representedbythe integral fr,d(x, .I,.)Li,c(x, .I) cos .i d.i I . Ox The evaluation 
of this term is dependent on whether an accurate or an approximate computation is required. In the accurate 
computation, the term is solved by using a radiance estimate from the caustics photon map. The number 
of photons in the caustics photon map is high and we can expect good quality of the estimate. Caustics 
are never computed using Monte Carlo ray tracing since this is a very inef.cient method when it comes 
to rendering caustics. The approximate evaluation of the integral is included in the radiance estimate 
from the global photon map. This is illustrated in Figure 4.14.  4.4.4 Multiple diffuse re.ections The 
last term in equation 4.19 is fr,d(x, .I,.)Li,d(x, .I) cos .i d.i I . Ox This term represents incoming 
light that has been re.ected diffusely at least once since it left the light source. The light is then 
re.ected diffusely by the surface (using fr,d). Consequently the resultingillumination is very soft . 
The approximate evaluation of this integral is a part of the radiance estimate based on the global photon 
map. The accurate evaluation of the integral is calculated using Monte Carlo ray tracing op­timized using 
the BRDF with an estimate of the .ux as describedin[Jensen95b]. An important optimizationat LambertiansurfacesistheuseofWard 
s irradiance gradient caching scheme[82,Ward92]. This means that we only compute indirect illumination 
on Lambertian surfaces if we cannot interpolate with suf.cient accuracy from pre­viously computed values. 
The advantage of using the photon map compared to just using the irradiance gradient caching method is 
that we avoid having to trace multiple bounces of indirect illumination and we can use the information 
in the photon map to concentrate our samples into the important directions. This is illustrated in Figure 
4.15.  4.4.5 Participating media In the presence of participating media we can still use the framework 
as presented so far. The main difference is that we need to take the media into account as we trace rays 
through the scene. This can be done quite ef.ciently using ray marching and the volume radiance estimate 
as described in[Jensen98]. 4.4.6 Whydistribution ray tracing? The renderingmethod presented hereisa 
combinationof manyalgorithms.In orderto render accurateimages withoutusingtoomanyphotonsa distributionraytracerisused 
to compute illumination seen directly by the eye. One might consider visualizing the globalphotonmap 
directly,andthiswould indeedbeafullglobal illumination solution (itwouldbe similartothe density estimation 
approach presentedin[Shirley95]). The problem with this approach is that an accurate solution requires 
a large number of photons. Signi.cantly fewer photons are necessary when a distribution ray tracer is 
used to evaluate the .rst diffuse re.ection. If a blurry solution is not a problem (for example for previewing) 
then a direct visualization of the photon map can be used. For more accurate results it is often necessary 
to use more than 1000 photons in the radiance estimate (see the results section for some examples). 
 4.5 Examples Inthissectionwepresentsomeexamplesof scenes renderedusingphotonmaps. Please see the photon 
map web-page at http://graphics.ucsd.edu/ henrik/images/ forthelatest results. Also refertothe papers 
includedin these notesfor moreexamples. All the images have been rendered using the Dali rendering program. 
Dali is an extremely .exible renderer that supports ray tracing with global illumination and par­ticipating 
media. The global illumination simulation code based on photon maps is a module in Dali that is loaded 
at runtime. All material and geometry code is also rep­resented via modules that are loaded at runtime. 
Dali is multithreaded and all images have been rendered on a dual PentiumII-400 PC running Linux. The 
width of each imageis1024pixelsand4samplesperpixelhavebeen used. 4.5.1 The Cornell box Most global illumination 
papers feature a simulation of the Cornell box, and so does this note. Since we are not limited to radiosity 
our version of the Cornell box is slightlydifferent. Ithasa mirrorsphereandaglasssphere insteadofthetwo 
cubes featured in the original Cornell box (the original Cornell box can be found at http: //www.graphics.cornell.edu/online/box/). 
Classic radiosity methods have dif.culties handling curvedspecular objects,but ray tracing methods (including 
the photon map method) have no problems with these. Ray tracing The image in Figure 4.16 shows the ray 
traced version of the Cornell box. Notice the sharp shadows and the black ceiling of the box due to lack 
of area lights and global illumination. Rendering time was 3.5 seconds. Ray tracing with soft shadows 
In Figure 4.17 soft shadows have been added. It has been reported that some people associatesoft shadows 
with global illumination,butin the Cornell boxexampleitis still obvious that something is missing. The 
ceiling is still black. Rendering time was 21 seconds. Adding caustics The image in Figure 4.18 includes 
the caustics photon map. Notice the bright spot below the glass sphere and on the right wall (due to 
light re.ected of the mirror sphere and transmitted through the glass sphere). Also notice the faint 
illumination of the ceiling. The caustics photon map has 50000 photons and the estimate uses up to 60 
photons. Photontracingtook2seconds. Renderingtimewas34 seconds.Wedidnot useany.lteringofthe caustics 
photons.Amaximum search distanceof0.15wasused forthe caustics photonmap(thedepthofthe Cornellboxis5units). 
Usinga search distance of 0.5 increased the rendering time to 42 seconds. For an unlimited initial search 
radius the rendering time was 43 seconds. The computed images looked very similar. Thefaint illuminationoftheceilingisa 
caustic (createdbythe bright caustic on the .oor) it becomesa little softer with the increased search 
radius.Fora search radius of 0.01the caustics became more noisy, andthe rendering time was 25 seconds. 
For other scenes where the caustics are more localized the in.uence of the maximum search radius on the 
rendering time can be more dramatic than for the Cornell box.  Global illumination In Figure 4.19 global 
illumination has been computed. The image is much brighter and the ceiling is illuminated. 200000 photons 
were used in the global photon map and100 photons in the estimate. The caustic photon map parameters 
are the same. Photon tracing took4seconds. Rendering timewas66 seconds.  The radiance estimate from 
the global photon map Finally in Figure 4.20 we havevisualized the radiance estimates from the global 
photon map directly.Wehaveshown images with100and500 photonsinthe estimate. Notice how the illumination 
becomes softer and more pleasing with more photons,but also moreblurryandwith morefalsecolor bleedingattheedges.Theedgeproblemcanbe 
solvedpartiallybyusinganellipsoidordisctolocatethephotons(see section 4.3) with 500 photons in the estimate 
and the ellipsoid search activated we get the image in Figure 4.21 These images took 30 35 seconds to 
render. Notice how the quality of the direct visualizationgivesa reasonableestimateof theoverall illuminationinthe 
scene. This is the information we bene.t from in the full rendering step since we do not have to sample 
the incoming light recursively. Fast global illumination estimate Forfast visualizationof globalillumination 
one can useveryfew photonsin the global photon map. In Figure 4.22 we have visualized the radiance estimate 
from a global photonmapwithjust200 photons!Weusedupto50 photonsinthe radiance estimate. The illumination 
is very blurry and as a consequence the shadows and the caustics are missing,but theoverall illuminationis 
approximately correct, and thisvisualizationis representative of the .nal rendering as shown in Figure 
4.19. Photon tracing took 0.03 secondsandthe renderingtimefortheimagewas4seconds. Thisis almostasfastas 
the simple ray tracing version, and the main reason is that we only used ray tracing to compute the .rst 
intersection and the mirror re.ections and transmissions. The global photon map was used to estimate 
both indirect and direct light. 4.5.2 Cornell box with water In the Cornell box in Figure 4.23 we have 
inserted a displacement-mapped water sur­face.To render this scene we used 500000 photonsin both the 
caustics and the global photonmap,andupto100photonsintheradiance estimate.Weusedahigher number of caustic 
photons due to the water surface which causes the entire .oor to be illumi­nated by the photons in the 
caustics photon map. Also the number of photons in the global photon map have been increased to account 
for the more complex indirect illu­mination in the scene. The water surface is made of 20000 triangles. 
The rendering time for the image was 11 minutes.  4.5.3 Fractal Cornell box An example of a more complex 
scene is shown in Figure 4.24. The walls have been replaced with displacement mapped surfaces (generated 
using a fractal midpoint sub­division algorithm) and the model contains a little more than 1.6 million 
elements. Notice that each wall segment is an instanced copyof the same fractal surface. With photonmapsitiseasytotakeadvantageof 
instancingandthe geometrydoesnothave to be explicitly represented. We used 200000 photons in the global 
photon map and 50000 in the caustics photon map. This is the same number of photons as in the simple 
Cornell box and our reasoning for choosing the same values are that the complexity of the illumination 
is more or less the same as in the simple Cornell box. We want to capture the color bleeding from the 
colored walls and the indirect illumination of the ceiling. All in all we used the same parameters for 
the photon map as in the simple Cornell box. We only changed the parameters for the acceleration structure 
to handle the larger amount of geometry. The rendering time for the scene was 14 minutes.  4.5.4 Cornell 
box with multiple lights Asimple example of a scene with multiple light sources is the variation of the 
Cornell box scene shownin Figure 4.25.We generated 100000 photons from each light source and the resulting 
global photon map has 400000photons. Other than that the render­ing parameters were the same as for the 
otherCornell box with1 light source. The rendering time for this scene was 90 seconds.  4.5.5 Cornell 
box with smoke The Cornell box scene shown in Figure 4.26 is an example of a scene with a uniform participating 
medium. To simulate this scene we used 100000 photons in the global photon map and 150000 photons in 
the volume photon map. A simple non-adaptive ray marcher has been implemented so the stepsize had to 
be set to a low value which is extra costly. The rendering time for the scene was 44 minutes.  4.5.6 
Cognac glass Figure 4.27 shows an example of a caustic from a cognac glass. The glass is an object of 
revolution approximated with 12000 triangles. To generate the caustic we used 200000 photons. The radiance 
estimates for the caustic were computed using up to 40 photons. The rendering time forthe imagewas8minutes 
and10 seconds partof this renderingtimeisduetotheray traceddepthof.eld simulation. Figure 4.27:Acognac 
glass witha caustic. 4.5.7 Prism with dispersion The classic example of dispersion with glass prism 
is shown in Figure 4.28. Even though only three separate wavelengths have been sampled, the color variations 
in the caustics are smooth. An accurate color representation wouldrequire more wavelength samples; such 
an extension to the photon map is easy to implement. 500000 photons were used in the caustics and 80 
photons were used in the radiance estimate. The rendering time for the image was 32 seconds.  4.5.8 
Subsurface scattering Arecent addition to the photon map is the simulation of subsurface scattering[Jensen99, 
Dorsey99]. For subsurface scattering we use the photon map to compute the effect of multiple scatteringwithin 
a given material. This is often very costly to compute and therefore mostly omitted from approaches dealing 
with subsurface scattering. This is unfortunatesincemultiple scatteringleadstoveryniceandsubtlecolorbleedingeffects 
inside the material which improves the quality of the rendering. Figure4.29showsagranitebunny nexttoa 
marblebunny.Bothofthese stone models are rendered using subsurface scattering with 100000 photons used 
to simulate multi­ple scattering. The rendering time for the picturewas21 minutes. Thisbunnyis the original 
Stanford bunny and the scene contains 140000 triangles, and it is rendered with full global illumination 
and depth of .eld. Figure 4.30 showsabustof Diana the Huntress madeof translucent marble. For this scenethelight 
sourcewas behindthebustto emphasizetheeffectof subsurface scatter­ing. Notice the translucencyof the 
hair and the nose region. This image was rendered in 21 minutes using 200000 photons.  4.6 Where to 
get programs with photon maps Photonmaps arealreadyavailable on the Internet fordownloading.Wehave collected 
 the following links as of the writing of these notes. RenderPark(a photorealistic rendering tool) has 
photon maps(as well as manyother global illumination algorithms). See http://www.cs.kuleuven.ac.be/cwis/research/graphics/RENDERPARK/ 
for more in­formation. Most commercial renderers now supports photon mapping for global illumination. 
 Figure 4.30:Translucent marblebust illuminated from behind Chapter5  Photon mappingfor complex scenes 
In this chapter, we present methods for photon emission for complex light sources and photon scattering 
for complex surface shaders (including displacement shaders), show how to precompute radiosity estimates 
for ef.cient .nalgathering, describe the radiosity atlas method for photon mappinginvery large scenes, 
and .nally discuss the use of importance to guide the storage of photons. In this chapter we will use 
two scenes from the Pixar movies Ratatouille and Monsters, Inc. as examples. However, it should be emphasized 
that photon mapping was not used in the production of those movies; we re just using these scenes here 
as examples of how photon mapping could be used in a movie production setting with very complex illumination, 
shaders, and geometry. 5.1 Photon emission from complex light sources Emitting photons corresponding 
to simple light sources such as point lights, spot lights, and directional lights isfairly straightforward. 
However, movie production light source shaders arevery complex: theycan project images likea slide projector 
and theycanhave barn doors, cucoloris ( cookies ), unnatural distancefall-off, fake shadows, artistic 
positioningof highlights, etc.[5].For photon emission correspondingto sucha light source, we would need 
to compute a photon emission probability distribution function that corresponds to thelight sourceshader.Thiscanbedif.cultsincethe 
programmablefall-offrequiresevaluationoftheshader not only at different anglesbut also at different distances. 
However, there is a simple method to create photon emission distributions that exactly match very general 
light sources. The method starts by evaluating the light source shader on the surface points. This is 
done by rendering the scene with direct illumination from the light source(s) and storing a point cloud 
of the direct illumination. The point cloud contains an illumination point for each surface shading point 
(roughly one point per pixel). This is a sampling of the light source shader exactly at the positions 
where its values matter (namely at the surfaces in the scene), and ensures that the photon distribution 
exactly matches the illumination no matter how complex and unpredictable the light source shader is. 
We basically treat the light source shaderasa blackbox thatis characterizedonlybyits illuminationonthesurfacesinthe 
scene. On specular or partially specular surfaces, the illumination has to be computed for each light 
source sepa­rately, and the illumination at each point must be stored with a vector indicating the incident 
light direction. For purely diffuse surfaces, the illumination from all light sources can simply be added 
up (and stored with­out the light direction vector). Figure 5.1 shows the direct illumination on the 
diffuse surfaces of the Ratatouille kitchen. The illumination is speci.ed with more than 200 light sources, 
each with rather complexillumination characteristics. (Rendering this image at resolution 720×405takes 
around42 minuteson1processor thisisan indicationofjusthow complex the light source shaders are.) The 
generated point cloud contains 4.8 million points. The next step is to emit photons with probability 
proportional to the stored illumination power. As always with photon mapping, we must ensure that all 
emitted (and all stored) photons have the same power. This is necessary in order to minimize noise in 
the irradiance estimates. What follows is a more detailed overview of the photon emission algorithm: 
First we compute the total power of all illumination in the direct illumination point cloud. (The incident 
power at each point is the point s irradiance times its area.) The target power per photon is the total 
power in the point cloud divided by the speci.ed number of photons to emit. Then, for each point in the 
illumination point cloud, we compute the numberofphotonstoemitforthatpointasthepowerofthepointdividedbythetargetpowerperphoton. 
Thiswillusuallyresultina fractionalnumberofphotons;weuseRussian roulette[4]toroundupordownto an integer 
number and adjust the power of the emitted photon(s) accordingly. The photon(s) are emitted by shooting 
them from a point just above the position of the point cloud point (jittered within the radius of the 
point). Since the photons are emitted probabilistically for each point cloud point, the .nal number of 
emitted photons may end up being slightly different from the speci.ed target number. (A possible improvement 
of the method would stratify the random numbers such that no matter what se­quence of pseudorandom numbers 
is encountered, the Russian roulette will not result in entire regions where the number of emitted photons 
is rounded down and other regions where it is rounded up. Quasi-random numbers could be used as well.) 
The idea of emitting photons from a point cloud of direct illumination values has also been utilized 
by Ha.sanetal.[31].Theirphoton emissionwasfromandtoasparsepointcloud,andwasusedtocomputean estimate of 
the multi-bounce global illumination in a very coarse point-cloud representation of a scene. 5.2 Photon 
scattering from complex surfaces In this section we ll look at photon scattering from surfaces with complex 
scattering characteristics. For simple surface shaders, it is straightforward to write a corresponding 
photon shader. Given an incident photon, the photon shader must stochastically determine whether the 
photon should be scattered and if so, the power and direction of the scattered photon. However, just 
as for the photon emission case discussed above, the surface shaders used in movie production are very, 
very complex. Theyhave hundreds or thou­sands of parameters, and consists of tens of thousands lines 
of code. In the worst case, some shaders might be legacycode that very few people, if any, have a complete 
understanding of. Writing photon shaders that correspond to such surface shaders would be quite a nightmare. 
Instead, we can pry the surface re.ection coef.cients out of the shader. In the simplest case, the re.ection 
parameterisjustadiffusecolor;wecan obtainthatcolorby illuminatingthe scenewithsingle ambientlight source 
with intensity 1. Figure 5.2 shows the diffuse colors (diffuse re.ection coef.cients) in the Ratatouille 
kitchen scene. The scene has around 150 different surface shaders and more than 1000 textures (including 
a few shadow maps). (a) (b) Figure 5.2: Diffuse colors in kitchen: (a) rendered image; (b) point cloud. 
For more complex shaders, there might be coef.cients for diffuse re.ection, specular re.ection, diffuse 
transmission, and specular transmission, as well as one or more values specifying the narrowness of the 
specular (glossy) scattering. The re.ection coef.cients may depend on dozens of textures combined in 
strange ways. But no matter how complex the shader is, we assume that the result of it can be expressed 
by a few re.ection coef.cients that are stored in a point cloud. The stored colors are used as scattering 
coef.cients during the photon scattering step. Theyare used to calcu­late the scattering probability 
and the power of scattered photons. The point cloud of scattering coef.cients is organized intoa kd-tree 
forfast lookups. Whena photon hitsa surface, wedoa kd-tree lookupin the point cloud to determine the 
coef.cients at the hit point. The photon is stored if appropriate and scattered if appropriate (using 
the usual Russian roulette strategy for computing photon scattering probilities and new photon power). 
This method also handles displacement-mapped surfaces gracefully. If a surface is displaced, that displace­ment 
will be present both in the point positions in the stored point cloud of scattering coef.cients and in 
the photon hit positions used for lookups in the kd-tree. 5.3 The radiosity map In the standard photon 
mapping method, whena.nalgather ray hitsapoint, the nearest n photons are looked up in the photon map 
kd-tree in order to estimate the irradiance at the hit point. However, it is more ef.cient to precompute 
the irradiance at some or all the photon positions. Then only the nearest photon needs to be looked up 
at .nalgather ray hit points[11]. The irradiance precomputation is quick, and makes the rendering 5 7 
timesfaster for typical scenes without degrading the image quality. Afurther optimization is to store 
the local diffuse surface color with each photon, and estimating radiosity instead of irradiance at the 
photon positions[12]. Estimating radiosity takes only a tiny bit longer than estimating irradiance(onlyafewextra 
multiplicationsper photon),butmakesitfasterto computethe indirect illumination especially if the scene 
has complex shaders and many textures. Each radiosity estimate is computed by locating the nearest photons, 
adding up their power, dividing by the area they cover, and multiplyingby the diffuse surface color. 
Theresultofthisstepisanewpointcloudwith position,normal,radiusand radiositydata.Wecallthispoint cloud 
a radiosity map. (The radiosity name here denotes that the data in the point cloud are radiosities; it 
does not imply that theyhave been computed with a .nite-element radiosity method .) The point positions 
divide the surfaces intoaVoronoi diagram with constant radiosity inside eachVoronoi cell. These radiosity 
values can be visualized directly for a rough estimate of the global illumination in the scene. Figure 
5.3 shows the kitchen scene rendered with the radiosity map values. Figure 5.4(a) shows the Ratatouille 
kitchen rendered with only direct illumination. Figure 5.4(b) shows the same scene with added indirect 
illumination computed with .nalgathering using the radiosity map shown above.  (a) (b) Figure 5.4: Ratatouille 
kitchen: (a) direct illumination; (b) global illumination. (Copy­right &#38;#169;c2007 Disney/Pixar.) 
Figure 5.5 shows another example of photon map global illumination. This scene has simple base geometry 
but texture maps and displaced surfaces: the .oor, left and right wall are textured, and the left and 
back wall and front teapot are displacement mapped. Figure 5.5(a) shows the direct illumination in the 
scene. Figure 5.5(b) clearly shows the tinting typically associated with color bleeding. Furthermore, 
notice that all illumination on the ceiling is indirect light. The same precomputation method can also 
be used for precomputing radiances in a volume photon map. This can speed up ray marching through an 
isotropically re.ecting participating medium quite signi.cantly.  5.4 The radiosity atlasfor large scenes 
Hereweextendthephotonmappingmethodtoenableittohandle sceneswithalotofgeometry.Alimitation of the original 
photon map method, and also the radiosity map method presented in the previous section, is that they 
assume that the entire photon map is stored in memory. If the scene is so complex (contains so much geometry) 
that the number of photons required to represent the illumination in it exceeds the memory capacity of 
the computer, we need to divide the photon map into chunks that can be read, processed, and cached independently. 
The sceneusedasexampleinthis sectionisfromthePixarmovie Monsters, Inc.: acity block of Monstropolis with 
manyindividually modeledbuildings, trees, cars, etc. The scene geometry consistsof 36,000 high-level 
primitives, mostly NURBS patches and subdivision surfaces. The shaders have been simpli.ed for these 
tests, and only a single light source is used. Figure 5.6 shows the Monstropolis city block with direct 
illumination from a single point light source and purely diffuse re.ection. Large parts of the scene 
are completely black since no direct light reaches them.  5.4.1 Photon emission and photon tracing In 
the .rst step, we divide the objects into groups. For the Monstropolis scene we use the groups created 
during modeling, so e.g. each house and each car is a group of objects. Each group gets a separate photon 
map. Figure 5.7 shows two of the photon maps for the Monstropolis scene. Both the photon powers and the 
diffuse surface colors are shown. The photon map for the car contains 76,000 photons, while the photon 
map forthebuilding contains2.2 million photons.Thephotonspowersgetagreentintwhen refractedthrough the 
windshield. Also notice the red and blue diffusely re.ected photons on thebuilding. We call a collection 
of photon maps aphoton atlas. Figure 5.8 shows the photon atlas for the Monstropolis scene.For clarity, 
the .gure only shows 0.1%of the52 million photonsin the photon atlas.  Figure5.8: Coarsephotonatlas 
(collectionofphotonmaps)forthe Monstropolis scene. The photon powers are shown. 5.4.2 Radiosity estimation 
As in section 5.3, we sort the photons in a photon map into a kd-tree and precompute the radiosity at 
each photon location. We do this independently for each photon map. Figure 5.9 shows two of the resulting 
radiosity maps for the Monstropolis scene.  5.4.3 Generating radiosity brick maps The next step is to 
compute a brick map representation of each of the radiosity point clouds. Abrickmap isatiled3DMIPmapdata 
structure[15].Itisausefuldata structurefor representinggeneral3D textures bothtexturesonsurfacesandinvolumes. 
(Brickmapscanalsobe renderedandray traced directly as geometric primitives[14].)Abrick consistsof8×8×8voxel 
positions. The coarsest representation in a brickmap consistsofa single brick. Thenextlevel consistsof(upto)2×2×2bricks. 
The next level consists of (up to)4×4×4bricks, etc. The advantage of a brick map representation over 
a point cloud is that the brick map is hierarchical and tiled, and that the bricks can be cached ef.ciently. 
For ef.cient global illumination, we use brick maps to represent the radiosity data in each radiosity 
map. Two of the radiosity brick maps for the Monstropolis scene are shown in .gure5.10. We call the 
collection of radiosity brick maps a radiosity brickatlas or simply a radiosity atlas.  level0 level1 
level2 level3 level4 Figure 5.11 shows the entire Monstropolis scene rendered with radiosity directly 
from the radiosity atlas. Thisimagegivesarough indicationoftheglobal illuminationinthe scene,butitisfartoonoisyandblurry 
to be used in a movie.  5.4.4 Rendering Whena.nalgatherrayhitsasurfacepoint,theraydifferential determinesthe 
appropriatebrickmaplevelto lookupin:we choosethebrickmaplevelwherethebrickvoxelsare approximatelythesizeoftheraybeam 
cross-section (similar to how we select 2D texture levels and tessellation levels). Figure 5.12 showsa 
.nalgather renderingof the scene. At the .nalgather ray hit points, the radiosityis determinedfromthe 
radiosityatlasshownin.gure 5.11. (Renderingthisimagetook5.7hoursona2GHz AppleG5.It requiredthe shootingof413 
million.nalgatherraysand3.8 millionshadowrays. There were 3.4 billion brick cache lookups with a hit 
rate of 99.9%.) &#38;#169; Disney/Pixar.)  5.5 Importancefor photon tracing This section describes 
the use of importance to concentrate the photons in those parts of the scene where theycontribute most 
to the .nal image. 5.5.1 Importance The .rst use of importance was for computer simulations of neutron 
transport for the development of the hydrogen bomb (right afterWorldWar II).Fortunately importance can 
also be used for more peaceful pur­poses. Smits et al.[69]formally introduced importance to computer 
graphics. Theyde.ned importance as the adjoint of radiosity that has a source term at the viewpoint, 
and used importance to reduce the number of links in a hierarchical radiosity solution. More generally, 
importance can be de.ned as the adjoint of any representation of light with the source term at the viewpoint 
or at the directly visible points. An overview of importance and its use in computer graphics can be 
found in Christensen[13]. The savings obtained using importancecanbe arbitrarilyhigh:just chooseasuf.cientlylarge 
scenewithasuf.cientlysmall visiblepart. Figure 5.13(a) shows a modest example scene, four rooms with 
closed doors between them. Figure 5.13(b) is the image we are interested in computing, a close-up of 
the tabletop in the upper right room. The red pyramid in .gure5.13(a) indicates the viewpoint and viewing 
frustum for .gure 5.13(b). 5.5.2 Importance emission and estimation Importance can be used to focus 
the photon storage in those parts of the scene where the photons will contribute most to the .nal image. 
 (a) (b) The .rst step is to emit importance particles ( importons )[54, 40, 72]from the intended viewpoint 
in directions within the viewing frustum. These particles are traced around the scene as if they were 
photons and stored every time theyhit a diffuse surface. The stored particles for the example scene are 
shown in .gure 5.14(a). (The emitted importance particles are white,but theychange color at each bounce 
according to the color of the surfaces theyhit. In this scene most particles turn orange.) After the 
importance particle tracing, the importance is estimated at each importance particle location using the 
local density of importance particles. These importance estimates are stored with the importance particles; 
the estimates are shown in .gure 5.14(b). Note that no importance reaches adjacent rooms since the doors 
are closed; this re.ects the fact that no light from adjacent rooms reaches the visible parts of the 
scene. Figure5.14: Importance in interior scene, seen from above: (a) 100,000 importance particles; 
(b) importance estimates. These precomputed importancesmakeit muchfasterto determinethe importanceatvariouslocationsduring 
photon tracing, as described in the following. 5.5.3 Photon tracing In the photon tracing phase, the 
importance estimates are used to determine photon storage probabilities. At locations with low importance, 
we use Russian roulette to decide whether to store the photon or not; if the photon is stored, its power 
is increased to compensate for the low storage probability. Compare .gures 5.15(a c) with 5.15(d f): 
Importance was not used in .gures 5.15(a c), so most photons are stored in bright regions, no matter 
how unimportant those regions are. In contrast, .gures 5.15(d f) show the gain from using importance 
 most photons are stored in areas that are either directly visible from the intended viewpoint, or are 
signi.cantly in.uencing the illumination there. Due to the higher concentration of photons, the illumination 
is approximated much more accurately in .gure 5.15(f) than in .gure 5.15(c). The most visible difference 
is the sharper shadows. (a) (b) (c) (d) (e) (f)  In this example, importance was only used to determine 
photon storage. It is also possible to use importance to guide photon emission and scattering[54,12],but 
thatisbeyond the scopeof this course note. Chapter6  References and further reading This chapter lists 
the references referenced in these course notes plus additional background material relevant to the photon 
map method. The material is divided into three groups: the photon map method, ray tracing and photon 
tracing and data-structures with focus on kd-trees. Each part is in chronological order with annotations. 
In addition we have listed a number of animations rendered with photon maps and.nally we have provided 
a more detailed list of relevant background literature. Bibliography [Jensen95a] HenrikWann Jensen 
and Niels Jørgen Christensen. Photonmapsin BidirectionalMonteCarloRayTracingofComplex Objects . Computers&#38;Graphics 
19 (2), pages 215 224, 1995. The .rst paper describing the photon map. The paper suggested the use of 
a mixture of photon maps and illumination maps, where photon maps would be used for complex surfaces 
such as fractals. 49, 50, 63, 64 [Jensen95b] HenrikWann Jensen. Importance driven path tracing using 
the photon map . RenderingTechniques 95 (Proceedings of the Sixth EurographicsWorkshop on Rendering), 
pages 326 335. SpringerVerlag, 1995. Introduces the use of photons for importance sampling in path tracing. 
By combining the knowledge of the incoming .ux with the BRDF it is possible to get better results using 
fewer sample rays. 72 [Jensen95c] HenrikWann Jensen and Niels Jørgen Christensen. Ef.ciently Rendering 
Shadows using the Photon Maps . In Proceedings of Compugraphics 95, pages 285 291, Alvor, December 1995. 
Introduces the use of shadow photons for an approximate classi.cation of the light source visibility 
in a scene. 70 [Jensen96a] HenrikWann Jensen. Rendering caustics on non-Lambertian surfaces . Proceedings 
of Graphics Interface 96, pages 116-121,Toronto, May 1996 (also selected for publicationin Computer GraphicsForum,volume16, 
number1, pages 57 64, March 1997). Extension of the photon map method to render caustics on non-Lambertian 
surfaces ranging from diffuse to glossy. 55, 58 [Jensen96b] HenrikWann Jensen. Global illumination using 
photon maps . RenderingTechniques 96(ProceedingsoftheSeventhEurographicsWorkshopon Rendering), pages 
21 30. SpringerVerlag, 1996. Presents the global illumination algorithm using photon maps. A caustic 
and a global pho­ton map is used to optimize the rendering of global illumination including the simulation 
of caustics. 54, 67 [Jensen96c] HenrikWann Jensen. The photon map in global illumination. Ph.D. dissertation,Technical 
Universityof Denmark, September 1996. An in-depth description of the photon map method based on the presentations 
in the published photon map papers. 59, 63, 64 [Christensen97] Per H. Christensen. Global illumination 
for professional 3D animation, visualization, and special effects (invited paper). RenderingTechniques 
97(Proceedingsofthe EighthEurographicsWorkshopon Rendering), pages 321 326. SpringerVerlag, 1997. Describesthe 
requirementsofaglobal illumination methodina commercialenvironment,and motivates the choice of the photon 
map method. [Myszkowski97] Karol Myszkowski. Lighting reconstruction usingfast and adaptive density estimation 
techniques . RenderingTechniques 97(Proceedingsof the Eighth EurographicsWorkshop on Rendering), pages 
321 326. SpringerVerlag, 1997. Ef.cient techniques for .ltering and visualizing photons. 63, 67 [Slusallek98] 
Philipp Slusallek, Mark Stamminger,Wolfgang Heidrich, J.-C. Popp, and Hans-Peter Seidel. Composite Lighting 
Simulations with Lighting Network . IEEE ComputerGraphics&#38;Applications, 18(2), pages 22-31, March/April 
1998. Describesaframeworkinwhichthephotonmapcanbeintegratedintoa radiosity simulation. [Peter98] Ingmar 
Peter and GeorgPietrek. Importance driven construction of photon maps. RenderingTechniques 98 (Proceedings 
of the Ninth EurographicsWorkshop on Rendering), pages 269 280. SpringerVerlag, 1998. Use of importance 
to focus the photons where theycontribute most to the visible solution. This requires an initial importance 
(or importons ) tracing pass from the camera before the photon tracing pass from the light sources. 49, 
97, 98 [Jensen98] HenrikWann Jensen and PerH. Christensen. Ef.cient simulation of light transport in 
scenes with participating media using photon maps . Proceedings of SIGGRAPH 98, pages 311 320.ACM, 1998. 
Extension of the photon map method to simulate global illumination in scenes with participat­ing media. 
55, 56, 57, 65, 73 [Lange98] Thorsten Lange and GeorgPietrek. RenderingParticipating Media using the 
Photon Map . Technical Report no. 678, University of Dortmund, 1998. Also describes the extension of 
the photon map method to simulate global illumination in the presence of participating media. [Jensen99] 
HenrikWann Jensen, JustinLegakis and Julie Dorsey. RenderingofWet Materials . ProceedingsoftheTenthEurographicsWorkshopon 
Rendering,pages 281-290, Granada, June 1999. Simulatessubsurfacescatteringusingthevolumephotonmapinordertorenderwet 
materials. 85 [Dorsey99] Julie Dorsey, Alan Edelman, HenrikWann Jensen, JustinLegakisand Hans Køhling 
Pedersen. Modeling and RenderingofWeathered Stone . Proceedings of SIGGRAPH 99, pages 223 234, 1999. 
Describes rendering of volumetric weathering effects in stone based on subsurface scattering optimized 
using the volume photon map. 85 [Christensen99] Per H. Christensen Faster Photon Map Global Illumination 
. Journal of GraphicsTools, 4(3), pages 1 10, 1999. Introduces precomputed irradiancevaluesper photonforfaster 
look-ups. [Jensen00] HenrikWann Jensen. Parallel Global Illumination using Photon Mapping . In SIGGRAPH 
2000, Course 30, New Orleans, July 2000. Describes how to implement the photon mapping algorithm to take 
advantage of multi­processor/multi-host computers. [PMAPCourse] SIGGRAPH 2000 Course Note. APractical 
Guide to Global Illumination Using Photon Maps . Previous SIGGRAPH course on photon mapping. [Suykens00] 
Frank Suykens andYvesWillems. Density control for photon maps . RenderingTechniques 2000(Proceedingsof 
the Eleventh EurographicsWorkshop on Render­ing), pp. 11 22. Springer-Verlag, 2000. Introduces techniquesfor 
limitingthe densityofthe photonsin ordertogetabetter distribution of photons. Also presents ideas for 
using visual importance to construct higher quality photon maps. 97 [RPK] Ph. Bekaert andF. Suykens. 
RenderPark, a physically based rendering tool. K.U. Leuven, http://www.renderpark.be, 1996-2001. An 
open-source renderer that supports photon mapping. [Jensen01] HenrikWann Jensen. Realistic Image Synthesis 
using Photon Mapping. AK Peters, 2001 An in-depth book describing photon mapping, all the theory, and 
all the practical details. In­cludes an implementation of the photon map data structure. [Whitted80] 
Turner Whitted. An improved illumination model for shaded display . Communicationsof theACM,volume23, 
number6, pages 343 349.ACM, June 1975. The classic ray tracing paper. 24 [Arvo86] James Arvo. Backward 
ray tracing . Developments in ray tracing, SIGGRAPH86 seminar notes.ACM, August 1986. Introduces light 
ray tracing and illumination maps for computing caustics. [Glassner89] Andrew S. Glassner. An introduction 
to ray tracing. Academic Press, 1989. The standard reference on ray tracing. Still a pleasure to read. 
9, 17, 20, 26, 35 [Shirley91] Peter Shirley. Physically Based Lightning Calculations for Computer Graphics. 
Ph.d. thesis, University of Illinois at Urbana-Champaign, 1991. Good overview of Monte Carlo ray tracing. 
Also presents one of the .rst practical multi-pass global illumination methods. [Chen91] Eric Shenchang 
Chen, HollyE. Rushmeier,Gavin Miller, and DouglasTurner. Aprogressive multi-pass method for global illumination 
. Proceedings of SIGGRAPH 91, pages 164 174.ACM, 1991. One of the .rst multi-pass global illumination 
methods. Uses illumination maps for caustics, radiosity for indirect light and path tracing for rendering. 
[Ward92] GregoryWard andPaul Heckbert. Irradiance gradients . ThirdEurographicsWorkshop on Rendering, 
pages 85 98. Eurographics, 1992. Describes the irradiance gradients method which is used for the .nalgathering 
step of the photon map method. 72 [Pattanaik93] SumantN.Pattanaik. Computational Methods for Global Illumination 
andVisualisation of Complex 3D Environ­ments . Ph.d. Thesis, Birla InstituteofTechnology&#38;Science, 
1993 Introduces particle tracing where photons are emitted from the light sources and stored in a mesh. 
53 [Rushmeier93] Holly Rushmeier, Ch.Patterson andA.Veerasamy. Geometric Simpli.cation for Indirect Illumination 
Calculations . Proceedings of Graphics Interface 93, pages 35-55, 1994. Introduces the concept of geometry 
simpli.cation for the radiosity step of multipass global illumination computations. [Glassner95] Andrew 
S. Glassner. Principles of digital image sythesis. Morgan Kaufmann, 1995. Givesanexcellentoverviewofthe 
entire.eldofimage synthesis.Of particular interesthereis the description of Monte Carlo photon tracing 
and Russian roulette. 26, 27, 35, 53 [Lafortune96] EricP. Lafortune. Mathematical Models and Monte Carlo 
Algorithms for Physcially Based Rendering. Ph.d. thesis, Katholieke University, Leuven, Belgium 1996. 
Good overview of Monte Carlo ray tracing techniques including bidirectional path tracing. [Dutre96] Philip 
Dutr´e andYvesD.Willems. MathematicalFrameworks and Monte Carlo Algorithms for Global Illumination in 
Compute Graphics. Ph.d. thesis, Katholieke Universiteit Leuven, 1996. Another .ne overview of Monte Carlo 
ray tracing and photon tracing. [Ward98] GregoryWard Larson and Rob Shakespeare. Rendering with Radiance 
 the art and science of lighting visualization. Morgan Kaufmann, 1998. An entire book dedicated to the 
excellent Radiance renderer with manypractical examples. [Bentley75] Jon L. Bentley. Multidimensional 
binary search trees used for associative searching . Communicationsof theACM,volume18, number9, pages 
509 517.ACM, 1975. First paper on the kd-tree datastructure. 58, 65 [Preparata85] FrancoP. Preparata 
and Michael Ian Shamos. Computational Geometry An Introduction, Springer-Verlag, 1985 65, 66 [Cormen89] 
Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. Introduction to algorithms. MIT Press, 
1989. Good overview of algorithms including the heap-datastructure. 59, 65  [Sedgewick92] Robert Sedgewick. 
Algorithms in C++. Addison-Wesley, 1992. Also good description of the heap structure, and algorithms 
for the median search (used in the balancing algorithm). 59, 65, 66 [Ansi86] American National Standard 
Institute. Nomenclature and De.nitions for Illumination Engineering . ANSI report, ANSI/IES RP-16-1986, 
1986. [Arvo90] James Arvo and David Kirk. ParticleTransport and Image Synthesis . Computer Graphics 24 
(4), pages 53 66, 1990. 51 [Aupperle93] Larry Aupperle andPat Hanrahan: AHierarchicah Illumination Algorithm 
for Surfaces with Glossy Re.ection . Computer Graphics, pages 155 162, 1993. [Bentley79a] Jon Louis Bentley. 
Multidimensional Binary SearchTreesin Database Applications . IEEETrans. on Soft. Eng. 5(4), pages 333 
340, July 1979. 65 [Bentley79b] Jon Louis Bentleyand Jerome H. Friedman. Data Structures for Range Searching 
. Computing Surveys 11 (4), pages 397 409, 1979. 65 [Bentley80] Jon Louis Bentley, BruceW.Weide, and 
AndrewC.Yao. Optimal Expected-Time Algorithm for Closest Point Problems . ACMTrans. on Math. Soft., 6(4), 
pages 563 580, dec. 1980. 65 [Chalmers] Alan Chalmers et al. PracticalParallel Rendering . ISBN: 1-56881-179-9,AKPeters, 
2002. [Christensen93] Per Henrik Christensen,David Salesin andTonyDeRose. A Continuous AdjointFormulaion 
for RadianceTransport . Fourth EurographicsWorkshop on Rendering, pages 95 104, 1993 [Christensen95] 
Per Henrik Christensen. HierarchicalTechniques for Glossy Global Illumination. PhD thesis, Seattle,Washington, 
1995. [Collins94] Steven Collins. Adaptive Splatting for Specular to Diffuse LightTransport . In Proceedings 
of the 5th EurographicsWorkshop on Rendering, pages 119 135, Darmstadt 1994. 63, 64 [Cook84] Robert L. 
Cook. Distributed RayTracing . Computer Graphics 18 (3), pages 137 145, 1984. 27 [Cook86] Robert L. Cook. 
Stochastic Sampling in Computer Graphics . ACMTransactions onGraphics5(1), pages 51 72, Jan. 1986. [Dutre94] 
Philip Dutr´e andYvesD.Willems. Importance-driven Monte Carlo LightTracing . In proceedingsof5. EurographicsWorkshop 
on Rendering, pages 185 194, Darmstadt 1994. [Dutre95] Philip Dutre andYvesD.Willems. Potential-DrivenMonte 
CarloParticleTracingforDiffuseEnvironmentswithAdaptiveProb­ability Density Functions . InP.M. Hanrahan 
andW. Purgathofer, editors, RenderingTechniques 95, pages 306 315, NewYork,NY, 1995. Springer-Verlag. 
[Ebert94] David Ebert,Ken Musgrave, Darwyn Peachey,Ken Perlin and SteveWorley. Texturing and Modeling:AProcedural 
Approach. Academic Press, October 1994. 57 [Goral84] Cindy Goral,KennethTorrance, Donald Greenberg, Bennet 
Battaile. Modeling the Interac­tion of Light Between Diffuse Surfaces . Computer Graphics (SIGGRAPH 84 
Proceedings), volume 18, number 3, pages 213-222, July 1984, Minneapolis, Minnesota. [Gritz96] Larry 
Gritz and J. K. Hahn. BMRT:AGlobal Illumination Implementationof the RenderMan Standard . Journal of 
GraphicsTools,Vol. 1, No. 3, pages 29-47, 1996. [Hall88] RoyHall. Illumination and Color Computer Generated 
Imagery. Springer-Verlag, 1988 50 [Heckbert90] Paul S. Heckbert. Adaptive RadiosityTextures for Bidirectional 
RayTracing . Computer Graphics 24 (4), pages 145 154, 1990. 57 [Horowitz93] Ellis Horowitz, Sartaj Sahni 
and Susan Anderson-Freed. Fundamentalsof Data StructuresinC, Computer Science Press, 1993 65, 66 [Igehy99] 
Homan Igehy. Tracing ray differentials. Computer Graphics, 33(Annual Conference Series):179 186, 1999. 
34 [Jensen93] HenrikWann Jensen. Global Illumination using Bidirectional Monte CarloRayTracing. M.Sc. 
thesis,TechnicalUniversityof Denmark(in Danish), 1993. 49,50 [Jensen95f] HenrikWann Jensen and Niels 
Jørgen Christensen. OptimizingPathTracing using Noise Reduction Filters . In Proceedings of WSCG 95, 
pages 134 142, Plzen 1995. [Kajiya86] JamesT. Kajiya. The Rendering Equation . Computer Graphics 20 (4), 
pages 143 149, 1986. 28 Personal communication. 66 [Kalos86] M. Kalos and P. Whitlock. Monte Carlo Methods, 
Volume 1: Basics. J. Wiley, New York, 1986. [Keller96] Alexander Keller. Quasi-Monte Carlo Radiosity 
. In proceedings of 7th Eurographics Workshop on Rendering, pages 102 111, Porto 1996. 48 [Keller00] 
Alexander Keller and Ingo Wald. Ef.cient Importance Sampling Techniques for the Photon Map . In Vision 
Modelling and Visualization 2000, pages 271 279, Saarbruecken, Germany, 2000. [Kilauea] Kilauea, SquareUSA 
s rendering software with photon maps. http://www.squareusa.com/kilauea/. [Kopp99] Nathan Kopp. [Lafortune93] 
EricP. Lafortune andYvesD.Willems. BidirectionalPathTracing . In Proceedings of CompuGraphics, pages 
95 104, 1993. [MegaPov00] Afree ray tracer that supports photon maps. Source code and examples are available 
at: http://www.nathan.kopp.com/patched.htm, Mar. 2000 66 [Nicodemus77]F.E. Nicodemus,J.C. Richmond,J.J. 
Hsia.I.W. GinsbergandT. Limperis: Geometric Considerations and Nomenclature for Re.ectance . National 
Bureau of Standards, 1977 60 [Niederreiter92] Harald Niederreiter. Random Number Generation and Quasi-Monte 
Carlo Methods, SIAM, 1992. [Pattanaik95] S.N.Pattanaik andS.P. Mudur. Adjoint equations and random walks 
for illumination computation . ACMTransactions onGraphics, 14(1):77 102, January 1995. [Pavicic90] MarkJ.Pavicic. 
Convenient Anti-Aliasing Filters that Minimize BumpySampling . In Graphics GemsI, eds. Andrew S. Glassner, 
pages 144 146, 1990. 64 [Pharr97] Matt Pharr, Craig Kolb, Reid Gershbein, Pat Hanrahan. Rendering Complex 
Scenes with Memory-Coherent RayTracing . Computer Graphics (SIGGRAPH 97 Proceedings), pages 101-108, 
August 1997, Los Angels, California. 40 [Rubinstein81] ReuvenY. Rubinstein. Simulation and the Monte 
Carlo Method. JohnWiley&#38;Sons, 1981. 48 [Rushmeier88] Holly Rushmeier. Realistic Image Synthesis for 
Scenes with RadiativelyParticipating Media. Ph.d. thesis, Cornell University, 1988. [Schlick93] Christophe 
Schlick. Customizable Re.ectance Model forEveryday Rendering .In proceedings of 4. Eurograph­icsWorkshop 
on Rendering, pages 73-84,Paris 1993. 26,27 [Shirley90] Peter Shirley. A RayTracing Method for Illumination 
Calculation in Diffuse-Specular Scenes . Proceed­ings of Graphics Interface 90, pages 205 212, 1990. 
53 [Shirley92] Peter Shirley. Nonuniform Random Point Sets viaWarping . Graphics Gems III (David Kirk 
ed.), Academic Press, pages 80 83, 1992. [Shirley95] Peter Shirley; BrettonWade; Phillip Hubbard; David 
Zareski; BruceWalter and DonaldP. Greenberg. Global Illumination via Density Estimation . In Rendering 
Techniques 95 . Eds. P.M. Hanrahan and W. Purgathofer, Springer-Verlag, pages 219-230, 1995. 73 [Shirley96] 
Peter Shirley;C.Wang andKurt. Zimmerman. Monte CarloTechniques for Direct Lighting Calculations . ACMTransactions 
onGraphics15 (1), 1996. 38 [Shirley00] Peter Shirley. RealisticRayTracing . ISBN: 1-56881-110-1,AKPeters, 
2000. [S2000Course38] SIGGRAPH 2001 Course Note. APractical Guide to Global Illumination Using Photon 
Mapping . [S2000Course40] SIGGRAPH 2001 Course Note. Parallel Rendering and the Quest for Realism: The 
Kilauea MassivelyParallel RayTracer . [Kilauea] SIGGRAPH 2002 Course Note. The Kilauea MassivelyParallel 
Global Illumination Ren­derer . [Silverman86] B.W. Silverman. Density Estimation for Statistics and Data 
Analysis. Chapmann and Hall,NewYork,NY, 1986. [Smits92] Brian E. Smits, James R. Arvo, and David H. Salesin. 
An importance-driven radiosity algorithm . Computer Graphics, 26(2):273 282, July 1992. 96 [Suykens01] 
Frank Suykens andYvesD.Willems. Path Differentials and Applications . In RenderingTechniques 2001 (Proceedingsof 
theTwelfth EurographicsWorkshop on Ren­dering , Eds. S.J. Gortler and K. Myszkowski. Springer-Verlag, 
pages 257 268, Londen, UK, 2001 34 [Suykens01TR] Frank Suykens andYvesD.Willems. Path differentials and 
applications . Technical Report CW307, Department of Computer Science, Katholieke Universiteit Leuven, 
Leuven, Belgium, May 2001. [Tamstorf97] RasmusTamstorf and HenrikWann Jensen. Adaptive Sampling and Bias 
EstimationinPathTracing . In RenderingTechniques 97 . Eds.J. DorseyandPh. Slusallek. Springer-Verlag, 
pages 285 295, 1997. [Veach94] EricVeach and Leonidas Guibas. Bidirectional Estimators for LightTransport 
. In Proceedingsof the 5th EurographicsWorkshop on Rendering, pages 147 162, 1994. [Veach95] EricVeach 
and Leonidas Guibas. Optimally Combinig SamplingTechniques for Monte Carlo Rendering . Computer Graphics 
29 (4), pages 419 428, 1995. [Veach97] EricVeach and Leonidas Guibas. Metropolis LightTransport . Computer 
Graphics 31 (3), pages 65 76, 1997. [Volevich99] VladimirVolevich, Karol Myszkowski, Andrei Khodulev 
and EdwardA.Kopylov. Perceptually-Informed Progressive Global Illumination Solution . Technical Report 
99-1-002, University of Aizu, Japan, 1999. 67 [Ward88] GregWard, FrancisM. Rubinstein, and RobertD. Clear. 
ARayTracing Solution for Diffuse Interre.ection . Computer Graphics 22 (4), pages 85-92, 1988. 30, 72 
[Ward91] GregWard. Real pixels . In Graphics Gems II, James Arvo (ed.), Academic Press, pages 80-83, 
1991. 38, 55 [Zimmerman98] Kurt Zimmerman. Density Prediction for Importance Sampling in Realistic Image 
Synthesis. Ph.d. thesis, Indiana University, 1998. Bibliography [1] Oliver Abert, Markus Geimer, and 
StefanM¨uller. Direct andfast ray tracing of NURBS surfaces. In Proceedingsofthe IEEE Symposiumon InteractiveRayTracing 
2006, pages 161 168. IEEE, 2006. 20 [2] Anthony A. Apodaca and Larry Gritz. Advanced RenderMan: Creating 
CGI for Motion Pictures. Morgan Kaufmann Publishers, 2000. 43 [3] Arthur Appel. Some techniques for shading 
machine renderings of solids. In Proceedings of the AFIPS SpringJoint Computer Conference, volume 32, 
pages 37 45, 1968. 15, 23 [4] James R. Arvo and David B. Kirk. Particle transport and image synthesis. 
Computer Graphics (Pro­ceedings of SIGGRAPH 90), 24(4):63 66, 1990. 90 [5] Ronen Barzel. Lighting controls 
for computer cinematography. Journal of GraphicsTools, 2(1):1 20, 1997. 22, 89 [6] James F. Blinn. Models 
of light re.ection for computer synthesized pictures. Computer Graphics (Proceedings of SIGGRAPH 77), 
11(2):192 198, 1977. 22 [7] JamesF. Blinnand MartinE.Newell.Textureand re.ectionin computer generated 
images. Commu­nicationsof theACM, 19(10):542 547, 1976. 43 [8] Phong Bui Tuong. Illumination for computer 
generated pictures. Communications of the ACM, 18(3):311 317, 1975. 22 [9] Edwin E. Catmull. ASubdivision 
Algorithm for Computer Display of Curved Surfaces. PhD thesis, University of Utah, Salt Lake City, 1974. 
22 [10] Edwin E. Catmull and James H. Clark. Recursively generated B-spline surfaces on arbitrary topological 
meshes. Computer-Aided Design, 10(6):350 355, 1978. 21 [11] Per H. Christensen. Faster photon map global 
illumination. Journal of GraphicsTools, 4(3):1 10, 1999. 91 [12] Per H. Christensen. Photon mapping tricks. 
In SIGGRAPH 2002 Course Note #43:APractical Guide to Global Illumination using Photon Mapping, pages 
93 121, 2002. 91, 98 [13] Per H. Christensen. Adjoints and importance in rendering: an overview. IEEETransactions 
onVisual­ization and Computer Graphics, 9(3):329 340, 2003. 96 [14] Per H. Christensen. Point clouds 
and brick maps for movie production. In Markus Gross and Hanspeter P.ster, editors, Point-Based Graphics, 
chapter 8.4. Morgan Kaufmann Publishers, 2007. 95 [15] Per H. Christensen and Dana Batali. An irradiance 
atlas for global illumination in complex production scenes. In RenderingTechniques 2004 (Proceedings 
of the Eurographics Symposium on Rendering 2004), pages 133 141. Eurographics, 2004. 95 [16] PerH. Christensen, 
JulianFong,DavidM.Laur,andDana Batali.Ray tracingforthemovie Cars .In Proceedingsof the IEEE Symposium 
on InteractiveRayTracing 2006, pages 1 6. IEEE, 2006. 41, 42, 43 [17] Per H. Christensen, David M. Laur, 
JulianFong,Wayne L.Wooten, and Dana Batali. Ray differen­tials and multiresolution geometry caching for 
distribution ray tracing in complex scenes. Computer GraphicsForum(Proceedingsof Eurographics 2003), 
22(3):543 552, 2003. 21, 39, 41, 43 [18] Robert L. Cook, Loren Carpenter, and Edwin Catmull. The Reyes 
image rendering architecture. Com­puter Graphics (Proceedings of SIGGRAPH 87), 21(4):95 102, 1987. 40, 
43 [19] Robert L. Cook, Thomas Porter, and Loren Carpenter. Distributed ray tracing. Computer Graphics 
(Proceedings of SIGGRAPH 84), 18(3):137 145, 1984. 27 [20]TonyD. DeRose, MichaelKass,andTienTruong. Subdivisionsurfacesin 
character animation. Com­puter Graphics (Proceedings of SIGGRAPH 98), pages 85 94, 1998. 21 [21] DanielDooand 
MalcolmA. Sabin. Behaviourof recursivedivision surfaces nearextraordinarypoints. Computer-Aided Design, 
10(6):356 360, 1978. 21 [22] AlbrechtD¨urer. Treatise on measurement with compasses and straightedge 
(Underweysung der Mes­sung mit dem Zirkel und Richtscheyt). Nuremberg, 1525. 14 [23] GeraldFarin. Curves 
and Surfaces forCAGD:APractical Guide. Academic Press, 3rd edition, 1993. 20 [24] JamesD.Foley, AndriesvanDam,StevenK. 
Feiner,and JohnF. Hughes. Computer Graphics: Prin­ciples and Practice. Addison-WesleyPublishing Company, 
2nd edition, 1990. 22, 26, 27, 35 [25] Andrew S. Glassner. An Introduction to RayTracing. Academic Press, 
1989. 9, 17, 20, 26, 35 [26] Andrew S. Glassner. Principles of Digital Image Synthesis. Morgan Kaufmann 
Publishers, 1995. 26, 27, 35, 53 [27] JeffreyGoldsmith and John Salmon. Automatic creation of object 
hierarchies for ray tracing. IEEE Computer Graphics and Applications, 7(5):14 20, 1987. 33 [28] Ned Greene. 
Environment mapping and other applications of world projections. IEEE Computer Graphics and Applications, 
6(11):21 29, 1986. 43 [29] Eric Haines. RayTracing News. 1987 present. (Web page: www.acm.org/tog/resources/RTNews/­html). 
33, 35 [30] Vlastimil Havran. Heuristic Ray Shooting Algorithms. PhD thesis, CzechTechnical University,Prague, 
2001. 33 [31] Milos Ha. san, Fabio Pellacini, and Kavita Bala.. Direct-to-indirect transfer for cinematic 
relighting. ACMTransactionsonGraphics(Proceedingsof SIGGRAPH2006), 25(3):1089 1097, 2006. 90 [32] Paul 
S. Heckbert. Surveyof texture mapping. IEEE Computer Graphics and Applications, 6(11):56 67, 1986. 22 
[33] Abu Sad al-Ala ibn Sahl. On Burning Mirrors and Lenses. Baghdad, 984. (Translated by Roshdi Rashed, 
1990). 26 [34] Homan Igehy. Tracing ray differentials. Computer Graphics (Proceedings of SIGGRAPH 99), 
pages 179 186, 1999. 34 [35] HenrikWann Jensen, James Arvo, Philip Dutr´e, AlexanderKeller, Art Oven, 
Matt Pharr, and Peter Shirley. SIGGRAPH 2003 Course Note #44: Monte CarloRayTracing. 2003. 35 [36] JamesT. 
Kajiya. Ray tracing parametric patches. Computer Graphics (Proceedings of SIGGRAPH 82), 16(3):245 254, 
1982. 20 [37] James T. Kajiya. The rendering equation. Computer Graphics (Proceedings of SIGGRAPH 86), 
20(4):143 150, 1986. 28 [38]ToshiakiKato.The Kilaueamassively parallelray tracer.InAlan Chalmers,TimothyDavis,andErik 
Reinhard, editors, PracticalParallel Rendering, chapter8.AKPeters, 2002. 43 [39]TimothyL.Kayand JamesT. 
Kajiya.Ray tracing complex scenes. Computer Graphics (Proceedings of SIGGRAPH 86), 20(4):269 278, 1986. 
32 [40] AlexanderKeller and IngoWald. Ef.cient importance sampling techniques for the photon map. In 
Proceedings of the 5thFallWorkshop onVision, Modeling, andVisualization, pages 271 279, 2000. 97 [41] 
LeifKobbelt,KatjaDaubert,and Hans-PeterSeidel.Raytracingofsubdivisionsurfaces.In Rendering Techniques 
98(Proceedingsofthe9thEurographicsWorkshopon Rendering), pages 69 80, 1998. 21 [42] CraigKolb,Pat Hanrahan, 
and Don Mitchell.Arealistic camera model for computer graphics. Com­puter Graphics (Proceedings of SIGGRAPH 
95), pages 317 324, 1995. 31 [43] AresLagaeandPhilipDutr´e.Anef.cient ray-quadrilateral intersection 
test. Journal of GraphicsTools, 10(4):23 32, 2005. 19 [44] Johann H. Lambert. Photometry: or on the Measure 
and Gradations of Light, Colors, and Shade. 1760.(Translated from the LatinbyDavidL. DiLaura, 2001). 
22 [45] Hayden Landis. Production-ready global illumination. In SIGGRAPH 2002 Course Note #16: Ren­derMan 
in Production, pages 87 102, 2002. 29 [46] Charles Loop. Smooth subdivision surfaces based on triangles. 
Master s thesis, University of Utah, Salt Lake City, 1987. 21 [47] William Martin, Elaine Cohen, Russel 
Fish, and Peter Shirley. Practical ray tracing of trimmed NURBS surfaces. Journal of GraphicsTools, 5(1):27 
52, 2000. 20 [48] TomasM¨oller and BenTrumbore.Fast, minimum storage ray triangle intersection. Journal 
of Graphics Tools, 2(1):21 28, 1997. 17 [49] Michael J. Muuss. Rt and remrt shared memory parallel and 
network distributed ray-tracing pro­grams. In USENIX:Proceedingsof theFourth ComputerGraphicsWorkshop, 
1987. 42 [50] Isaac Newton. Opticks:ATreatise on the Re.ections, Refractions, In.ections and Coloursof 
Light. London, 1704. 14 [51] MichaelOrenandShreeK.Nayar. Generalizationof Lambert sre.ectance model. 
Computer Graphics (Proceedings of SIGGRAPH 94), pages 239 246, 1994. 22 [52] StevenParker,William Martin, 
Peter-Pike J. Sloan, Peter Shirley, Brian Smits, and Charles Hansen. Interactive ray tracing. In Symposium 
on Interactive 3D Graphics, pages 119 126, 1999. 42 [53] Darwyn Peachey. Texture on demand. Technical 
Report #217, Pixar, 1990. (Available at graph­ics.pixar.com). 38, 39 [54] Ingmar Peter and GeorgPietrek. 
Importance driven construction of photon maps. In RenderingTech­niques 98(Proceedingsofthe9thEurographicsWorkshopon 
Rendering), pages 269 280, 1998. 49, 97, 98 [55] Matt Pharr andPat Hanrahan. Geometry caching for ray-tracing 
displacement maps. In Rendering Techniques 96 (Proceedings of the 7th EurographicsWorkshop on Rendering), 
pages 31 40, 1996. 21, 41 [56] Matt Pharr and GregHumphreys. Physically Based Rendering:From Theory to 
Implementation. Mor­gan Kaufmann Publishers, 2004. 21, 35 [57] Matt Pharr, CraigKolb, Reid Gershbein, 
andPat Hanrahan. Rendering complex scenes with memory­coherent ray tracing. Computer Graphics (Proceedings 
of SIGGRAPH 97), pages 101 108, 1997. 40 [58] Les Piegl andWayneTiller. The NURBS Book. Springer-Verlag, 
1997. 20 [59] WilliamT.Reeves,DavidH. Salesin,and RobertL.Cook. Rendering antialiasedshadowswithdepth 
maps. Computer Graphics (Proceedings of SIGGRAPH 87), 21(4):283 291, 1987. 37, 43 [60] Erik Reinhard, 
Brian Smits, and Chuck Hansen. Dynamic acceleration structures for interactive ray tracing. In RenderingTechniques 
2000 (Proceedings of the 11th EurographicsWorkshop on Render­ing), pages 299 306, 2000. 32 [61] Christophe 
Schlick.Acustomizable re.ectance model foreveryday rendering. In Proceedings of the 4th EurographicsWorkshop 
on Rendering, pages 73 83, 1993. 26, 27 [62] Andrei Sherstyuk. Fast ray tracing of implicit surfaces. 
Computer GraphicsForum, 18(2):139 147, 1999. 20 [63] Peter Shirley. Fundamentals of Computer Graphics.AKPeters, 
2002. 35 [64] Peter Shirleyand R.Keith Morley. Realistic RayTracing.AKPeters, 2nd edition, 2005. 26, 
27, 35, 38 [65] Peter Shirley, Philipp Slusallek, IngoWald, et al. SIGGRAPH 2006 Course Note #4: State 
of the Art in Interactive RayTracing. 2006. 35 [66] Peter Shirley, Changyaw Wang, and Kurt Zimmerman. 
Monte Carlo techniques for direct lighting calculations. ACMTransactions onGraphics, 15(1):1 36, 1996. 
38 [67] Brian Smits. Ef.ciencyissues for ray tracing. Journal of GraphicsTools, 3(2):1 14, 1998. 21, 
33 [68] Brian Smits, Peter Shirley, and Michael M. Stark. Direct ray tracing of displacement mapped triangles. 
In RenderingTechniques 2000(Proceedingsof the 11th EurographicsWorkshop on Rendering), pages 307 318, 
2000. 21 [69] Brian E. Smits, James R. Arvo, and David H. Salesin. An importance-driven radiosity algorithm. 
Computer Graphics (Proceedings of SIGGRAPH 92), 26(2):273 282, 1992. 96 [70] Ian Stephenson, editor. 
Production Rendering: Design and Implementation. Springer-Verlag, 2005. 18 [71] Gordon Stoll,William 
R. Mark, Peter Djeu, RuiWang, and Ikrima Elhassan. Razor: an architecture for dynamic multiresolution 
ray tracing. Technical Report TR-06-21, University ofTexas at Austin, 2006. (Updated version to appear 
in ACMTransactions onGraphics). 40 [72] FrankSuykensandYvesD.Willems. Density controlfor photon maps.In 
RenderingTechniques 2000 (Proceedingsof the 11th EurographicsWorkshop on Rendering), pages 11 22, 2000. 
97 [73] Frank Suykens and Yves D.Willems. Path differentials and applications. In RenderingTechniques 
2001(Proceedingsof the 12th EurographicsWorkshop on Rendering), pages 257 268, 2001. 34 [74] Steve Upstill. 
The RenderMan Companion. AddisonWesleyPublishers, 1990. 43 [75] IngoWald, Andreas Dietrich, and Philipp 
Slusallek. An interactive out-of-core rendering framework for visualizing massively complex models. In 
RenderingTechniques 2004 (Proceedings of the Euro­graphics Symposium on Rendering 2004), pages 81 92, 
2004. 40 [76] IngoWald, Thiago Ize, AndrewKensler, Aaron Knoll, and Steven G.Parker. Ray tracing animated 
scenes using coherent grid traversal. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2006), 25(3):485 
493, 2006. 32 [77] Ingo Wald and Steven G. Parker, editors. Proceedings of the IEEE Symposium on Interactive 
Ray Tracing 2006. IEEE, 2006. (Web page: www.sci.utah.edu/RT06). 35 [78] IngoWald, Philipp Slusallek, 
Carsten Benthin, and MichaelWagner. Interactive distributed raytracing of highly complex models. In Rendering 
Techniques 2001 (Proceedings of the 12th Eurographics Workshop on Rendering), pages 277 288, 2001. 43 
[79] IngoWald, Philipp Slusallek, Carsten Benthin, and MichaelWagner. Interactive rendering with coher­ent 
raytracing. ComputerGraphicsForum(Proceedingsof Eurographics 2001), 20(3):153 164, 2001. 42 [80] Gregory 
J.Ward. Adaptive shadow testing for ray tracing. In Proceedings of the 2nd Eurographics Workshop on Rendering, 
pages 11 20, 1991. 38, 55 [81] GregoryJ.Ward. Measuring and modeling anisotropic re.ection. Computer 
Graphics (Proceedings of SIGGRAPH 92), 26(2):265 272, 1992. 22, 30 [82] Gregory J. Ward, Francis M. Rubinstein, 
and Robert D. Clear. A ray tracing solution for diffuse interre.ection. Computer Graphics (Proceedings 
of SIGGRAPH 88), 22(4):85 92, 1988. 30, 72 [83] Turner Whitted. An improved illumination model for shaded 
display. Communications of theACM, 23(6):343 349, 1980. 24 [84] Lance Williams. Pyramidal parametrics. 
Computer Graphics (Proceedings of SIGGRAPH 83), 17(3):1 11, 1983. 38 [85] Sergei Zhukov, Andrei Iones, 
and Gregorij Kronin. An ambient light illumination model. In Rendering Techniques 98(Proceedingsofthe9thEurographicsWorkshopon 
Rendering), pages 45 55, 1998. 29 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401137</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>112</pages>
		<display_no>3</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[The beam radiance estimate for volumetric photon mapping]]></title>
		<page_from>1</page_from>
		<page_to>112</page_to>
		<doi_number>10.1145/1401132.1401137</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401137</url>
		<abstract>
			<par><![CDATA[<p>We present a new method for efficiently simulating the scattering of light within participating media. Using a theoretical reformulation of volumetric photon mapping, we develop a novel photon gathering technique for participating media. Traditional volumetric photon mapping samples the in-scattered radiance at numerous points along the length of a single ray by performing costly range queries within the photon map. Our technique replaces these multiple point-queries with a single beam-query, which explicitly gathers all photons along the length of an entire ray. These photons are used to estimate the accumulated in-scattered radiance arriving from a particular direction and need to be gathered only once per ray. Our method handles both fixed and adaptive kernels, is faster than regular volumetric photon mapping, and produces images with less noise.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[light transport]]></kw>
			<kw><![CDATA[nearest neighbor]]></kw>
			<kw><![CDATA[participating media]]></kw>
			<kw><![CDATA[photon map]]></kw>
			<kw><![CDATA[photon tracing]]></kw>
			<kw><![CDATA[ray marching]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[variable kernel method]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.1.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003738</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Integral equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098599</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098600</person_id>
				<author_profile_id><![CDATA[81100289561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zwicker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098601</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{BMP77} Breiman L. Meisel W., Purcell E.: Variable kernel estimates of multivariate densities. <i>Technometrics 19</i>, 2 (May 1977), 135--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1101430</ref_obj_id>
				<ref_obj_pid>1101389</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{BPPP05} Boudet A., Pitot P., Pratmarty D., Paulin M.: Photon splatting for participating media. In <i>GRAPHITE '05: Proceedings of the 3rd international conference on Computer graphics and interactive techniques in Australasia and South East Asia</i> (New York, NY, USA, 2005), ACM Press, pp. 197--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Cha60} Chandrasekar S.: <i>Radiative Transfer</i>. Dover Publications, New York, 1960.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581915</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{CJ02} Cammarano M., Jensen H. W.: Time dependent photon mapping. In <i>EGRW '02: Proceedings of the 13th Eurographics workshop on Rendering</i> (Aire-la-Ville, Switzerland, Switzerland, 2002), Eurographics Association, pp. 135--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{CPP*05} Cerezo E., P&#233;rez F., Pueyo X., Seron F. J., Cois X. Sillion F.: A survey on participating media rendering techniques. <i>The Visual Computer 21</i>, 5 (June 2005), 303--328.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{JC98} Jensen H. W., Christensen P. H.: Efficient simulation of light transport in scences with participating media using photon maps. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 1998), ACM Press, pp. 311--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{JMLH01} Jensen H. W., Marschner S. R., Levoy M., Hanrahan P.: A practical model for subsurface light transport. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 2001), ACM Press, pp. 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Kaj86} Kajiya J. T.: The rendering equation. In <i>SIGGRAPH '86: Proceedings of the 13th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 1986), ACM Press, pp. 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{KH84} Kajiya J. T., Herzen B. P. V.: Ray tracing volume densities. In <i>SIGGRAPH '84: Proceedings of the 11th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 1984), ACM Press, pp. 165--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275468</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{LW96} Lafortune E. P., Willems Y. D.: Rendering Participating Media with Bidirectional Path Tracing. In <i>Proceedings of the eurographics workshop on Rendering techniques '96</i> (London, UK, 1996), Springer-Verlag, pp. 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383583</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{PARN04} Premoze S., Ashikhmin M., Ramamoorthi R., Nayar S. K.: Practical rendering of multiple scattering effects in participating media. In <i>Rendering Techniques</i> (2004), pp. 363--373.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732117</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{PKK00} Pauly M., Kollig T., Keller A.: Metropolis light transport for participating media. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques 2000</i> (London, UK, 2000), Springer-Verlag, pp. 11--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{PM93} Pattanaik S. N., Mudur S. P.: Computation of global illumination in a participating medium by Monte Carlo simulation. <i>The Journal of Visualization and Computer Animation 4</i>, 3 (July--Sept. 1993), 133--152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{RT87} Rushmeier H. E., Torrance K. E.: The zonal method for calculating light intensities in the presence of a participating medium. <i>Computer Graphics (SIGGRAPH '87 Proceedings) 21</i>, 4 (July 1987), 293--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Sil86} Silverman B.: <i>Density Estimation for Statistics and Data Analysis</i>. Chapman and Hall, New York, NY, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073309</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{SRNN05} Sun B., Ramamoorthi R., Narasimhan S. G., Nayar S. K.: A practical analytic single scattering model for real-time rendering. <i>ACM Trans. Graph. 24</i>, 3 (2005), 1040--1049.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{Sta95} Stam J.: Multiple Scattering as a Diffusion Process. In <i>Rendering Techniques '95 (Proceedings of the Sixth Eurographics Workshop on Rendering)</i> (New York, NY, 1995), Hanrahan P. M., Purgathofer W., (Eds.), Springer-Verlag, pp. 41--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{Vea98} Veach E.: <i>Robust Monte Carlo Methods for Light Transport Simulation</i>. PhD thesis, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{WMG*07} Wald I., Mark W. R., G&#252;nther J., Boulos S., Ize T., Hunt W., Parker S. G., Shirley P.: State of the art in ray tracing animated scenes. In <i>STAR Proceedings of Eurographics 2007</i> (Prague, Czech Republic, September 2007), Eurographics Association, pp. 0--0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Beam Radiance Estimate for Volumetric Photon Mapping Wojciech Jarosz, Matthias Zwicker, and Henrik 
Wann Jensen University of California, San Diego Abstract We present a new method for ef.ciently simulating 
the scattering of light within participating media. Using a theoretical reformulation of volumetric photon 
mapping, we develop a novel photon gathering technique for participating media. Traditional volumetric 
photon mapping samples the in-scattered radiance at numerous points along the length of a single ray 
by performing costly range queries within the photon map. Our technique replaces these multiple point-queries 
with a single beam-query, which explicitly gathers all photons along the length of an entire ray. These 
photons are used to estimate the accumulated in-scattered radiance arriving from a particular direction 
and need to be gathered only once per ray. Our method handles both .xed and adaptive kernels, is faster 
than regular volumetric photon mapping, and produces images with less noise. Keywords: participating 
media, light transport, global illumination, rendering, photon tracing, photon map, ray marching, nearest 
neighbor, variable kernel method. Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer 
Graphics]: raytracing; color, shading, shadowing, and texture; I.6.8 [Simulation and Modeling]: Monte 
Carlo; G.1.9 [Numerical Analysis]: Fredholm equations; integro-differential equations. 1. Introduction 
The appearance of many natural phenomena, such as human skin, clouds, .re, water, or the atmosphere, 
are strongly in­.uenced by the interaction of light with volumetric media. Therefore, ef.ciently rendering 
scenes with participating me­dia has been an area of interest within computer graphics. This problem 
is challenging, however, because accurately simulating light transport in participating media is compu­tationally 
very expensive. Cerezo et al. [CPP*05] provide a recent and comprehensive overview of the wealth of research 
that has been devoted to address this problem. Light transport in arbitrary participating media is modeled 
by the radiative transfer equation [Cha60]. The pioneering contributions by Kajiya and von Herzen [KH84] 
and Rush­ meier and Torrance [RT87] are at the beginning of a long list of work on rendering participating 
media in the computer graphics community. Some of the most popular techniques to date are based on stochastic 
path-tracing and Monte Carlo integration [PM93, LW96, PKK00]. These approaches are attractive because 
of their sound underlying theoretical frame­work and their generality. They are unbiased and guaranteed 
to converge to the exact solution. In addition, it is straight­forward to include heterogeneous media, 
anisotropic phase functions, and scattering from surfaces. The downside of these approaches is that they 
suffer from noise that can only be overcome with a huge computational effort. One strategy to solve this 
issue is to make simplifying as­sumptions about the participating media. For example, homo­geneous media 
with a high scattering albedo can be modeled accurately using a diffusion approximation [Sta95, JMLH01], 
which leads to very ef.cient rendering algorithms. Premoze et al. [PARN04], under the assumption that 
the medium is tenuous and strongly forward scattering, use a path integral formulation to derive ef.cient 
rendering algorithms. Sun et al. [SRNN05] render single scattering in real time, but with­ out shadowing 
effects. In contrast, photon mapping [JC98] improves the ef.ciency of path-tracing without making additional 
assumptions about the properties of the medium being rendered. Similar to Monte Carlo methods, photon 
mapping handles isotropic, anisotropic, homogeneous, and heterogeneous media of ar­bitrary albedo. A 
disadvantage of photon mapping is that it introduces bias to the solution of the radiative transfer equa­tion. 
In practice, however, this bias is preferable to the noisy solutions of pure Monte Carlo methods. Intuitively, 
photon mapping works by splitting the energy emitted by each light source into discrete packets, so called 
photons. In a .rst pass, the propagation of light is simulated W. Jarosz &#38; M. Zwicker &#38; H. W. 
Jensen / The Beam Radiance Estimate Symbol Units Description ss(x) m-1 Scattering coef.cient at x sa(x) 
m-1 Absorption coef.cient at x st (x) m-1 Extinction coef.cient at x p(x,&#38;.,&#38;.') t(x .x') Tr(x 
.x') sr-1 unitless unitless Normalized phase function Optical thickness: R x x: st (x) dx Transmittance: 
e-t(x.x:) A m2 Surface area V m3 Medium volume O sr Hemisphere of directions O4p sr Sphere of directions 
L(x .x') W m-2sr-1 Outgoing radiance at x towards x' L(x,&#38;.) W m-2sr-1 Incident radiance at x from 
&#38;. We(x,&#38;.) m-3sr-1 Importance at x towards &#38;. We(x.x') m-3sr-1 Importance at x towards x' 
 Table 1: De.nitions of quantities used throughout the paper. by scattering photons in the scene. At 
each scattering event (at surfaces or within the medium) the incident energy carried by a photon is stored 
in a photon map. In a second pass, the photon map is used to evaluate the radiance at discrete points 
in the scene by locally computing the photon density. To com­pute the radiance carried along each ray 
towards the eye, the radiance is estimated within the medium at several sample points along the ray [JC98]. 
At each point the attenuation through the medium to the eye is computed, and the attenu­ated radiance 
is added to the ray. The main disadvantage of this procedure, however, is that it is dif.cult to .nd 
a good dis­tribution of sample points along the ray. On one hand, if not enough sample points are used, 
the result is likely to be noisy. On the other hand, increasing the number of sample points is very costly 
and can slow down rendering signi.cantly. In this paper, we propose a novel approach for computing the 
contribution of in-scattered radiance. We gather photons along viewing rays and analytically compute 
their contri­butions, without point sampling. We present the following contributions: We combine the 
theory from Veach [Vea98] and Pauly et al. [PKK00] to derive a reformulation of volumetric photon mapping 
as a solution to the measurement equation. This theory allows for arbitrary measurements of radiance 
to be computed within participating media, where a measure­ment is simply an integral of the radiance 
multiplied with a weighting function.  Using this new theory, we present an improved radiance estimate 
for volumetric photon mapping based on beam gathering. This technique eliminates the need for stepping 
through the medium to .nd photons. Instead, it gathers all photons along a ray. We show how to ef.ciently 
implement this new gathering technique for both .xed and adaptive smoothing kernels and demonstrate that 
our method pro­duces images with less noise than conventional photon mapping.  The rest of this paper 
is organized as follows. In Section 2, we review the theory of radiance transport within partici­pating 
media and the volumetric photon mapping method. In Section 3, we reformulate volumetric photon mapping 
in terms of the measurement equation and show how the photon map can be used to estimate any measurement 
of radiance within the scene. In Section 4, we present our new beam radiance estimate using this theory 
and describe the data structures needed to evaluate it ef.ciently. Finally, we show comparisons of our 
approach to conventional photon mapping in Section 5 and discuss avenues of future work in Section 6. 
2. Photon Mapping in Participating Media Light transport within participating media is described by the 
radiative transfer equation (RTE) [Cha60], which de.nes the radiance that reaches a point x from direction 
&#38;. as a sum of the exitant radiance from the nearest surface from this direction, and the accumulated 
in-scattered radiance from the medium between the surface and x (see Figure 1). This can be expressed 
as: L(x,&#38;.)= Tr(x.xs)L(xs,&#38;.)+ Z s Tr(x .xt )ss(xt )Li(xt ,&#38;.) dt, (1) 0 where Tr is the 
transmittance, s is the distance through the medium to the nearest surface at xs = x-s&#38;., and xt 
= x -t&#38;. with t . (0, s). We de.ne the remaining quantities in Table 1. The surface radiance, L(xs,&#38;.), 
at the boundary of the medium is governed by the rendering equation [Kaj86]. The in-scattered radiance, 
Li(xt ,&#38;.), depends on the radiance ar­riving at xt from all directions &#38;.t over the sphere of 
directions O4p and is de.ned as: Z Li(xt ,&#38;.)= p(xt ,&#38;.,&#38;.t )L(xt ,&#38;.t ) d&#38;.t , (2) 
O4p where p is the normalized phase function. Volumetric photon mapping [JC98] solves the RTE using a 
combination of photon tracing, ray-marching, and density estimation. In a preprocess, packets of energy 
are shot from light sources, scattered at surfaces and within the medium, and their interactions are 
stored in a global data structure. Dur­ing rendering, ray marching is used to numerically integrate 
W. Jarosz &#38; M. Zwicker &#38; H. W. Jensen / The Beam Radiance Estimate A, if bi(l)= 1 Figure 2: 
Conventional gathering (left) searches for photons in a sphere around numerous samples along the ray. 
Our method (right) assigns a radius to each photon and performs a single range search to .nd all photons 
along the length of the entire ray. Equation 1 for radiance seen directly by the observer, L(x,&#38;.) 
 Tr(x.xs)L(xs,&#38;.)+ discrete points using Equation 4, our main contribution is to directly estimate 
Z s 0 Tr(x.xt )ss(xt )Li(xt ,&#38;.) dt (5) along rays. Though the explanation of photon mapping from 
the pre­vious section is appealing at an intuitive level, it does not rigorously present the algorithm 
as a numerical solution to the RTE. Furthermore, this explanation is heavily tied to the geometric interpretation 
of gathering photons within a disc (on surfaces) or within a sphere (in participating media). In order 
to avoid these limitations and use the photon map to estimate general radiometric quantities in the volume, 
such as Equation 5, we use a more .exible derivation of parti­ cle tracing methods presented by Veach 
[Vea98]. We extend this derivation to handle participating media by combining it with the generalized 
path integral formulation of the radiative transport equation [PKK00] (Section 3.1) and show how to represent 
particle tracing algorithms like volumetric photon  S-1 mapping in terms of the measurement equation 
(Sections 3.2 . Tr(x .xt )ss(xt )Li(xt ,&#38;.).t, (3) and 3.3). Finally, we show how to use the same 
photon maps t=0 where .s is the length of each segment along the ray and x0,..., xs are the sample points 
for each segment (x0 is the point where the ray enters the medium and xs is a point on a surface past 
the medium). The most expensive part to compute in Equation 3 is the in-scattered radiance Li, because 
it involves accounting for all light arriving at each point xt along the ray from any other point in 
the scene. Instead of computing these values indepen­dently for each location, photon mapping gains ef.ciency 
by reusing the computation performed during the photon tracing stage. The in-scattered radiance is approximated 
using den­sity estimation by gathering photons within a small spherical neighborhood of radius r around 
each sample location xt , n p(xt ,&#38;.,&#38;.i).Fi Li(xt ,&#38;.) . , (4) 4 3 pr3 i=1 where .Fi is 
the power of photon i, and &#38;.i is its incident direction [JC98]. Though photon mapping is much more 
ef.cient than brute­force techniques like path tracing, the density estimation requires searching for 
photons within a global data structure, which is quite expensive. This formulation is suboptimal, to 
estimate more general quantities of radiance (Section 3.4). 3.1. Generalized Path Integral Formulation 
We use the path integral formulation of the RTE, which arises by recursively expanding the right hand 
side of Equation 1. Instead of expressing the radiance equilibrium recursively, the resulting path integral 
formulation is a sum over light­carrying paths of different lengths. In order to do this, we de.ne the 
path space, the corresponding differential measure, and generalized radiometric terms using notation 
inspired by Pauly et al. [PKK00]. A light path x¯kl is a set of k + 1 vertices xi. Each path is classi.ed 
according to its path characteristic l . N, which determines for each vertex whether it is in the volume 
or on a surface. This allows us to integrate over different measures for scattering events at surfaces 
and within the volume. We l de.ne the path characteristic l of a path x¯k such that the ith bit of l, 
bi(l), equals 1 if vertex xi is on a surface and bi(l)= 0 if it is in the volume. The space of all paths 
of length k with characteristic l is therefore:   V, if bi(l)= 0 l Xlk =x¯ , (6) xi . = x0,x1,...,xk 
k .rstly because it may gather the same photons more than once if the spherical neighborhoods overlap 
and, secondly, for 1 = k < 8 and 0 = l < 2k+1, and where V and A are the because it can lead to noise 
if the step size is too large and media volume and surface area respectively (see Figure 3 for photons 
are omitted (see Figure 2). an illustration of this notation). We de.ne the corresponding differential 
measure at a path vertex xi as:  3. Reformulation of Volumetric Photon Mapping Our technique solves 
these shortcomings by querying once dV(xi), if xi .V, i.e. when bi(l)= 0 dµl (xi)=. (7) dA(xi), if xi 
.A, i.e. when bi(l)= 1 for photons along the length of an entire ray, instead of mul­tiple times near 
points along the ray (see Figure 2). More Additionally, in order to express Equation 1 in terms of formally, 
whereas regular photon mapping estimates Li at paths we need to transform the integration domain from 
solid W. Jarosz &#38; M. Zwicker &#38; H. W. Jensen / The Beam Radiance Estimate x13 Figure 3: An example 
path, ¯3 , with length k = 3. The path characteristic, l = 1101b = 13, concatenates the type of scattering 
event from each path vertex. The radiance transported along the path ¯x13 L(¯3 ) is the emitted radiance 
at the light multiplied by a series of scattering events (blue) and geometry terms (green). angle (O 
and O4p), to area or volume (A and V) depending on the type of scattering event. This transformation 
is achieved using the generalized geometry term: V (x .y)Dx(y)Dy(x)s(x)Tr(x .y) G (x .y)= (8) Ix - yI2 
where 1, if x .V Dx(y)= . (9) &#38;n(x) ·&#38;.xy, if x .A We de.ne &#38;.xy to be the unit direction 
vector from x to y, and Dy(x) is de.ned symmetrically to Dx(y) for both cases. The visibility function, 
V , is 1 if x and y are mutually visible, and 0 otherwise. The s(x) function returns the scattering coef.­cient 
ss(x) if x .V and 1 otherwise. Note that Equation 8 simpli.es to the regular geometry term if no participating 
media is present. Similarly, we generalize scattering events and emitted ra­diance. We de.ne f to be 
the generalized scattering function combining the phase function and the surface BRDF p(xi+1 .xi .xi-1), 
if xi .V f (xi)= , (10) fr(xi+1 .xi .xi-1), if xi .A and L e is the generalized emitted radiance sa(xi) 
ss (xi) Le(xi .xi-1), xi .V L e(xi .xi-1)= , (11) Le(xi .xi-1), xi .A where the emitted radiance in 
a volume needs to be multiplied by the absorption, not the scattering, coef.cient. Given this notation, 
the generalized path integral formu­lation expresses the outgoing radiance Lo at x1 towards x0 as a sum 
over all possible light paths arriving at x1. This includes light paths of all lengths k, as well as 
all possible characteristics l for each length 8 2k+1-1 Lo(x1 .x0)= .. L(x¯lk). (12) ¯k=1 l=0 ¯ L(x¯kl 
) measures the amount of radiance transported along a path x¯kl and is de.ned as ZZ l L¯(x¯k)= ··· L 
e(xk .xk-1) (13) µl (xk) µl (x2) k-1 . f (x j)G (x j+1 .xj) dµl (x2) ···dµl (xk). j=1 The radiance transported 
along an example path is shown in Figure 3. 3.2. The Measurement Equation Many global illumination algorithms 
can be described in terms of the measurement equation. The measurement equa­tion describes an abstract 
measurement of incident radiance taken over some set of rays in a scene ZZ I = (We,L) = We(x,&#38;.)L(x,&#38;.) 
d&#38;. dV(x). (14) V O4p The importance function We represents an abstract measuring sensor and is de.ned 
over the whole ray space V× O4p (though typically We is non-zero for only a small subset of this domain). 
Path tracing, for instance, measures the contribution of ra­diance arriving over the area of a pixel. 
Radiosity algorithms integrate the contribution of radiance over basis functions de­.ned on the scene 
geometry. Both of these approaches can be described using Equation 14 with an appropriate importance 
function. In his dissertation, Veach [Vea98] showed how particle trac­ ing methods for surface illumination 
can also be expressed as a solution to the measurement equation by using the path in­tegral form of the 
rendering equation. We extend on this idea and use the generalized path integral formulation to describe 
volumetric photon tracing in the same way. 3.3. Volumetric Photon Tracing Photon tracing methods can 
be thought of as a way of gen­erating samples from the scene s equilibrium radiance dis­tribution and 
then using this single collection of samples to render the entire image. The photon tracing stage generates 
N weighted sample rays, or photons, (ai,xi,&#38;.i), where each (xi,&#38;.i) is a ray and ai is a corresponding 
weight. Our goal is to use these samples to take unbiased estimates of the radiance as a weighted sum, 
 N E1 .We(xi,&#38;.i)ai= (We, L), (15) N i=1 for an arbitrary importance function We. We must therefore 
determine the proper distribution of samples for Equation 15 to hold. We start by manipulating the measurement 
equation on the right-hand side. To express the measurement equation using paths, we expand Equation 
14 in terms of the outgoing W. Jarosz &#38; M. Zwicker &#38; H. W. Jensen / The Beam Radiance Estimate 
radiance and convert to area form, introducing an additional geometry term, ZZ (We,L) = We(x0 .x1)Lo(x1 
.x0) (16) µl (x1) µl (x0) G (x1 .x0) dµl (x0) dµl (x1). By combining with Equations 12 and 13 and moving 
the summations outside the integrals, this becomes 8 2k+1-1 ZZ .. ··· We(x0 .x1)L e(xk .xk-1) (17) µl 
(xk) µl (x0) k=1 l=0 k-1 . f (x j)G (x j+1 .xj) G (x1 .x0)dµl (x0) ···dµl (xk) . j=1 The Monte Carlo 
estimator for Equation 17 using N samples is N E 1 .We(xi,0 .xi,1)Ri , (18) N i=1 where each Ri is a 
random-walk path of length ki generated using Russian roulette L e(xi,ki .xi,ki-1 ) Ri = (19) pdf (xi,ki 
, xi,ki-1) ki-1 f (xi, j) 1 G(xi, j+1 .xi, j). G (xi,1 .xi,0), qi, j pdf (xi, j-1) j=1 and qi, j was 
the probability of terminating Ri at the jth vertex. Comparing Equation 18 with 15 we see that in order 
to satisfy the requirements we need to set ai = Ri. Connection to Conventional Photon Tracing. Though 
de­rived in a different fashion, Equation 19 is exactly how con­ventional photon mapping distributes 
photons within the scene. For instance, for a diffuse area light, photons are emit­ted using a cosine 
distribution with the power of the light source. In Equation 19 photons are emitted with the radiance 
of the light source divided by the pdf of choosing a position and direction on the light. These quantities 
are equivalent. Hence the particles generated above represent differential .ux. The correspondence between 
the photon powers [JC98] and the sample weights is .Fi = aNi . 3.4. Radiance Estimation Using the Measurement 
Equation The main advantage of the reformulation in Section 3.3 is that it naturally accommodates computation 
of any measurement of radiance within the scene simply by using an appropriately de.ned importance function 
We. In this section, we .rst show how the conventional estimate for in-scattered radiance can be expressed 
as a measurement. We then go one step further and show how to derive a beam radiance estimate which approximates 
Equation 5 along rays directly. Conventional Radiance Estimate. The conventional radi­ance estimate approximates 
the value of the in-scattered ra­diance Li at .xed points within the scene. To express this using the 
theory from Section 3, we need to transform Equa­ tion 2 into the measurement equation. Since the measurement 
equation is an integral over all of ray space (V× O4p), we arti.cially expand Li to also integrate over 
the volume Z Li(xt ,&#38;.)= p(xt ,&#38;.,&#38;.t)L(xt ,&#38;.t) d&#38;.t (20) O4p ZZ '' = d(Ix- xt I)p(x,&#38;.,&#38;.t) 
V O4p ' L(x,&#38;.t) d&#38;.tdV(x'). In order to keep the expressions equivalent when we add the integration 
over volume, we also introduce a Dirac delta function d. The bottom row of the above equation is now 
the measure­ ' ment equation where We = d(Ix' - xt I)p(x,&#38;.,&#38;.t ). Hence we can compute an unbiased 
estimate using the photon map by evaluating Equation 15 with this importance function. However, in order 
to obtain a useful estimate of radiance at all points in the scene a normalized kernel function is used 
in place of the delta function. This is where bias is introduced in the photon mapping method. Another 
interpretation is that by replacing the delta function with a kernel, photon mapping computes an unbiased 
estimate of blurred radiance. Jensen and Christensen [JC98] use a constant three-dimensional ker­ nel 
with a radius based on the nth nearest neighbor. This results in the following radiance estimate by applying 
Equa­tion 15 ZZ '' Li(xt ,&#38;.) Kn(Ix- xt I)p(x,&#38;.,&#38;.t) (21) V O4p ' L(x,&#38;.t) d&#38;.tdV(x'), 
N 1 . Kn(Ixi - xt I)p(xi,&#38;.,&#38;.i)ai (22) N i=1 where the kernel Kn is de.ned as 3 if r . [0,dn]Kn(r)= 
4pdn 3 , (23)0 otherwise and dn is the distance to the nth photon. Note that this is equivalent to the 
conventional volumetric radiance estimate in Equation 4. Beam Radiance Estimate. A similar procedure 
can be used to derive an estimate for the accumulated in-scattered radi­ance along an entire ray. To 
accomplish this, we .rst expand out Li in Equation 5 and then arti.cially in.ate the resulting W. Jarosz 
&#38; M. Zwicker &#38; H. W. Jensen / The Beam Radiance Estimate  Figure 4: In the beam radiance estimate, 
x ' is parameterized in cylindrical coordinates, (t, ., r), about the ray (x,&#38;.). An unbiased estimate 
would only consider points directly on the ray, while a biased version uses a kernel (shown in grey) 
to blur the radiance within a beam. expression to integrate over the whole volume, ZZ s Tr(x .xt)ss(xt)p(xt 
,&#38;.,&#38;.t)L(xt ,&#38;.t) d&#38;.t dt = (24) 0 O4p ZZ 2pZZ d(r)(H(t) - H(t - s))Tr(x .x ' )ss(x 
' ) R 0 R O4p '' p(x ,&#38;.,&#38;.t)L(x ,&#38;.t) d&#38;.t drd. dt. (25) Ris the set of real numbers 
and x ' is expressed in cylindrical coordinates, (t, .,r), about (x,&#38;.), where r is the radius to 
the ray (see Figure 4). We have added a Dirac delta function d as before, and the Heaviside step functions 
(H(x)= 1 when x > 0 and 0 otherwise) constrain the computation to t . (0, s). Equation 25 is now equivalent 
to the measurement equation where the integral over volume has been converted into cylindrical coordinates 
and where We = d(r)(H(t) - ' H(t - s))Tr(x.x ')ss(x ')p(x ,&#38;.,&#38;.t). Since the probability of 
photons landing exactly on the ray (x,&#38;.) is zero, we introduce bias by blurring the radiance and 
replacing the delta and step functions with a smooth kernel, K. This integral can then be estimated with 
the measurement equation using the photons as: ZZ 2pZZ ' K(t,.,r)Tr(x.x ' )ss(x ' )p(x ,&#38;.,&#38;.t) 
R 0 R O4p ' L(x ,&#38;.t) d&#38;.t drd. dt = (26) N1 . K(ti, .i,ri)Tr(x.xi)ss(xi)p(xi,&#38;.,&#38;.i)ai, 
(27) N i=1 where (ti,.i,ri)= xi are the cylindrical coordinates of photon i about the ray. The blurring 
in the conventional radiance estimate is spher­ical and so the kernel needs to be normalized for 3D. 
However, with the beam radiance estimate, we blur in two dimensions (perpendicular to the ray) since 
the radiance we are com­puting already includes the integration along the ray itself. Therefore, the 
kernel in the beam estimate is normalized for 2D. 3.5. Kernel Radiance Estimation For both the conventional 
and beam radiance estimates the characteristics of the bias and blur are determined by the smoothing 
function chosen. Several options exist for applying a smoothing kernel to the photon map data. The kernel 
method uses a .xed-radius smoothing kernel and results in a uniform blur of radiance within the scene. 
In practice, using a .xed-width circular kernel implies that in order to evaluate the beam radiance estimate 
(Equation 26) using the photon map (Equation 15) we only need to con­ sider photons which are located 
within a .xed-radius cylinder about the ray (x,&#38;.). Alternatively, the equivalent dual inter­pretation 
considers each photon as the center of an oriented disc facing the ray and all photon-discs that intersect 
the ray need to be found. If the density of photons varies signi.cantly it can be dif.­cult to choose 
a single radius that works well for all regions of the scene. This can be solved by allowing the size 
and shape of the blurring kernel to vary spatially. In conventional photon mapping, the nearest neighbor 
method (k-NN) is used to adapt the kernel width to the local density. Generaliz­ing point-based k-NN 
to a visually comparable range search along rays is challenging. However, spatial variation can eas­ily 
be applied to the dual photon-discs interpretation using the variable kernel method (VK) [BMP77]. A smoothing 
kernel is attached to each photon and the radius of the ker­nel is allowed to vary from one photon to 
another, based on the local density. In contrast to k-NN estimation, where the kernel widths vary based 
on the distance from the evalua­tion location to the data points, in the VK method the kernel widths 
only depend on the data points themselves. In order to facilitate this, the method relies on a pilot 
estimate of the local density at each data point in order to assign the kernel widths. This is the approach 
we take. 4. Algorithm In order to use the dual interpretation to evaluate the beam radiance estimate 
(Equation 26), we need an ef.cient way of locating all photon-discs that intersect an arbitrary ray. 
Additionally, to use variable width kernels we need to ef.­ciently compute a radius for each photon in 
the photon map. At a high level, our volumetric photon mapping technique performs the following .ve steps: 
1. Shoot photons from light sources. 2. Construct a balanced kd-tree for the photons. 3. Assign a radius 
for each photon. 4. Construct a bounding-box hierarchy over the photon-discs. 5. Use the photon BBH 
to render the image.  Steps 1 and 2 are identical to conventional photon mapping while 3 5 are unique 
to our approach and are explained in more detail below. Photon Radius Computation. We augment the traditional 
photon map by associating a radius with each photon. For .xed width kernels the radius is a constant 
for all photons W. Jarosz &#38; M. Zwicker &#38; H. W. Jensen / The Beam Radiance Estimate Figure 5: 
After photons have been distributed in the scene, our algorithm constructs a balanced kd-tree (left). 
We assign a valid radius to each photon by querying in the kd-tree (middle). Finally, we rapidly construct 
a bounding-box hierarchy over the photon-discs (right) by reusing the same hierarchical structure (shown 
in red) as the kd-tree. Algorithm 1 CONSTRUCTBBH(p) Require: p is a node in a balanced photon map. Ensure: 
The subtree at p contains a valid BBH. 1: B . BOUNDINGBOX(p.position, p.radius) 2: if p.leftChild then 
3: B . B . CONSTRUCTBBH(p.leftChild) 4: end if 5: if p.rightChild then 6: B . B . CONSTRUCTBBH(p.rightChild) 
7: end if 8: p.bbox . B 9: return B and does not need to be explicitly stored. For variable width kernels 
using the VK method, we perform a density estima­tion at each photon to assign a radius. At each photon 
we compute the local density by estimating the distance to the nth nearest photon and use this as the 
photon-disc s radius. This pilot estimate is performed using the photon map kd-tree. The value n plays 
the same role as in the conventional radiance estimate, it controls the amount of blur. As an optimization, 
we only search for the nearest m « n photons and estimate the necessary radius for n photons. By assuming 
locally uniform photon density, if di,m is the dis­tance to the mth photon from photon i, we estimate 
the radius m n as ri = di,m 3 . The m parameter controls the sensitivity of m the computed radius to 
the local variation in density. Very small values of m, m < 5, can produce noisy radii, which change 
drastically between neighboring photons, while large values are more expensive to compute. In practice, 
we have v found that m = n works well as a default value and this value was used for all our scenes, 
signi.cantly accelerating the preprocessing step. Bounding Box Hierarchy Construction. In order to ef.­ciently 
locate all photons-discs which intersect a ray, we construct a bounding box hierarchy. Heuristics for 
construct­ing ef.cient BBHs have been extensively studied within the context of ray tracing [WMG*07]. 
However, the performance characteristics of our ray intersections are different than for regular ray 
tracing since we are interested in all intersections, not just the .rst intersection along a ray. Furthermore, 
the best heuristics tend to induce long construction times. We employ a rapid construction scheme by 
exploiting the information in the photon map kd-tree and re-using that hierarchy for our BBH. For each 
photon in the photon map, we compute the bound­ing box of all descendent photon-discs. The bounding box 
of each node encloses the node s photon radius and the bound­ing boxes of its two child nodes. The computation 
starts at the leaves and propagates upwards through the tree. This procedure results in a balanced median-split-style 
BBH, but unlike traditional BBHs our hierarchy contains photons at interior nodes, not just at the leaves. 
Figure 5 illustrates the relationship between the kd-tree and the BBH. The BBH can be constructed by 
passing the root of the photon map kd-tree to Algorithm 4. Given a balanced kd-tree, this linear-time 
construction procedure is extremely fast and produces BBHs which can be ef.ciently traversed for nearby 
photons during rendering. After the BBH is constructed the photon map kd-tree is no longer used and can 
be freed from memory. Using a BBH and a per-photon radius, an additional 7 .oating-point values need 
to be stored, increasing the size of each photon from 20 bytes to 46 bytes. The Beam Radiance Estimate. 
During rendering we es­timate the accumulated in-scattered radiance (Equation 5) along viewing rays by 
locating all photons whose bounding spheres intersect the ray. These photons are found using a standard 
ray-BBH intersection traversal. The contribution from each photon (ai, xi,&#38;.i) is accumulated using 
Equa­tion 26; however, with the variable kernel method, a kernel Ki is de.ned per photon. This leads 
to the following radiance estimate N 1 '' .,&#38;(28) N . Ki(x,&#38;.,s,xi,ri)Tr(x .xi )ss(xi )p(xi,&#38;.i)ai, 
i=1 ' where xi = x + ti&#38;. is the projection of the photon location xi onto the ray s direction &#38;., 
and ti =(xi - x) ·&#38;.. We de.ne W. Jarosz &#38; M. Zwicker &#38; H. W. Jensen / The Beam Radiance 
Estimate Photon Tracing Fixed Radius Comparison Adaptive Radius Comparison Scene N Shoot (s) Balance 
(s) Radius (s) r .t C (m) O (m) r+ n .t C (m) O (m) Cornell 0.4M 1.50 0.30 2.0 0.4 0.40 3:19 3:00 0.6 
1.5K 0.80 4:03 3:35 Stage 1M 3.25 0.76 5.0 0.3 0.20 4:21 4:15 0.5 0.5K 0.70 6:38 6:22 Cars 2M 19.0 1.50 
2.0 0.4 1.25 1:30 1:30 0.5 1K 1.25 2:02 1:53 Lighthouse 1M 2.83 0.78 6.0 0.4 0.25 1:07 0:59 0.5 0.4K 
1.00 1:12 1:05 Table 2: Rendering parameters and timings (in seconds (s) and minutes (m)) for all example 
scenes. Statistics relating to the photon tracing preprocess are shown in the .rst table. The middle 
and right tables compare our method (O) to conventional photon mapping (C) using a .xed width kernel 
and adaptive width kernel. The r column represents the .xed-width kernel radius, while r+ is the maximum 
search radius and the number of nearest neighbors is n. Scene ss sa g Cornell 0.225 0.225 0.00 Stage 
0.225 0.225 0.75 Cars 0.06 0.015 0.00 Lighthouse 0.24 0.010 0.75  Table 3: Scattering properties of 
the participating media used in each of the example scenes. the kernel as -2 di rK2if di . [0,ri] Ki(x,&#38;., 
s, xi, ri)= i ri, (29) 0 otherwise where ri is the pre-computed radius for photon i. We use Sil­verman 
s two-dimensional biweight kernel [Sil86] along the ray, K2(x)= 3p-1(1 - x2)2, where di is the shortest 
distance from photon i to the ray. We chose this kernel because it is smooth, ef.cient to evaluate, and 
has local support. Equa­tion 28 is the beam radiance estimate, and it replaces the ray marching computation 
from conventional photon mapping (second row of Equation 3). Heterogeneous Media. For homogeneous media, 
the trans­ ' mission terms, Tr(x . xi ), can be computed explicitly for each photon during gathering. 
Beam gathering in heteroge­neous media can also be handled ef.ciently by .rst marching along the ray 
and saving the transmission values in a 1D lookup table. Then, during gathering, each photon s trans­mission 
is computed by interpolating within the lookup table. This preprocess needs to be performed independently 
for each ray, just prior to gathering, so the lookup table can be reused. Furthermore, if single scattering 
is simulated sepa­rately by directly sampling light sources the lookup table can be populated during 
this marching step. 5. Results We compared our beam gathering technique against conven­tional volumetric 
photon mapping using ray marching. In order to isolate just the performance of the photon gathering methods, 
we use the photon map for both single and multiple scattering. We compared results on four test scenes: 
Cars, Lighthouse, Stage, and a Cornell box. For each scene we compare using a .xed gathering radius for 
both types of esti­mates, and we also compare the conventional estimate using k-NN to the beam estimate 
using the VK method. The images were all rendered with a maximum dimension of 1024 pixels with up to 
4 samples per pixels on an Intel Core 2 Duo 2.4 GHz machine using one core. In our experimental setup, 
we .rst choose suitable gather­ing parameters (search radius and number of nearest neigh­bors n) and 
render the scenes using our method. We then use the same parameters using conventional photon mapping 
but tune the minimum step-size .t to obtain approximately equal render times. Note that .t is the minimum 
step-size and that exponential stepping is used to sample the ray according to transmission. Finally, 
we render a high quality result with conventional photon mapping using a very small step size as a reference. 
We show visual comparisons of the methods in Figures 6 7. All images of each scene are rendered using 
the same photon map. The only differences in quality and performance are due to the gathering method 
used. We report the render times and gathering parameters, as well as timings for constructing the photon 
maps in Table 2. We used the Henyey-Greenstein phase function for all scenes with the medium parameters 
speci.ed in Table 3. In all cases, our method produces signi.cantly higher qual­ity images than conventional 
photon mapping. This is because querying once for all photons along a ray is more ef.cient than repeatedly 
querying for photons near numerous samples along the ray. Not surprisingly, we see that the reduced blur­ring 
of the adaptive kernel gathering methods is essential for scenes like the Stage and Lighthouse, where 
concentrated beams of light are visible. However, at the same render time this advantage is dif.cult 
to discern in the conventional pho­ton mapping images. Though the k-NN and VK methods both adapt the 
width of the kernel to the local photon density, they are distinct approaches which result in similar, 
but not identical, density estimates. This is what accounts for the small differences in blurring between 
our adaptive results and the k-NN refer­ence images. However, as our results show, the same value of 
n produces visually comparable renderings using the two methods. W. Jarosz &#38; M. Zwicker &#38; H. 
W. Jensen / The Beam Radiance Estimate Figure 6: A comparison between the convention radiance estimate 
and our beam radiance estimate on the Stage scene with render times provided as (hours:minutes:seconds). 
Our method (right) produces images with much less noise than an equal time rendering using conventional 
volumetric photon mapping (middle) for both a .xed radius and an adaptive radius gathering approach. 
Our method does not require stepping but matches the quality of conventional photon mapping if a very 
small step size is used (left). 6. Discussion and Future Work Speci.c trade-offs between the k-NN and 
VK methods have been extensively studied within the density estimation liter­ature [Sil86]. For computer 
graphics, a visual advantage of the VK method is that the estimated function inherits all the differential 
properties of the smoothing kernel. In contrast, even with smooth kernels, the k-NN method results in 
esti­mates with a discontinuous .rst derivative. The VK method does, unfortunately, require a pre-process 
to compute the ker­nel width for each photon. However, the amount of time to compute adaptive radii for 
each photon using our method is relatively inexpensive (typically less than 1 2% of the total render 
time). On the other hand, k-NN gathering involves maintaining a priority queue and is much more costly 
than a .xed radius query. With our method, the gathering procedure is identical whether an adaptive or 
.xed radius kernel is used and no priority queue is needed. Though adaptive kernel widths can be a distinct 
advantage, in uniformly illuminated scenes a .xed radius may be suf.­cient. While assigning the radii 
is inexpensive, each resulting BBH node consumes more memory than a kd-tree node due to the additional 
7 .oating-point values needed to store the bounding box and radius. If memory is limited and a .xed ra­dius 
search is acceptable, then the regular photon map kd-tree can be used to perform beam gathering by tracing 
a cylinder through the kd-tree. This approach could be implemented in the spirit of ray-bundle traversals 
[WMG*07] by bounding the cylinder using 6 planes. This offers the additional bene­.t of re-using the 
exact same data structure as conventional photon mapping. Our photon-disc approach using a BBH is not 
inherently tied to participating media. VK density estimation could also be applied to photon mapping 
at surfaces. It would be inter­esting to see whether a similar technique could be bene.cial for surfaces 
by querying the BBH for all photons that overlap with a surface location. The pilot estimation preprocess 
may also be bene.cial at detecting and reducing boundary bias in photon mapping for surfaces. The advantages 
of beam gathering are more pronounced in large open environments where ray marching needs to be per­formed 
over large distances and where potentially interesting lighting is visible far away. One advantage of 
the conventional radiance estimate, however, is that a fast preview render can be obtained by using a 
large step-size. Since no step-size is used for beam gathering, all photons intersecting a ray need to 
be considered. In fact, these two approaches estimate the lighting integral using different pdfs. The 
ray marching computation employs exponential stepping, which distributes photon queries with a pdf proportional 
to the transmission term Tr. In contrast, our approach visits every photon that intersects a line and 
so concentrates effort where the lighting is important. Since the radiance seen by the eye is a product 
of the lighting and the transmission, optimally we should concentrate effort according to this product. 
One way of ex­ploiting this could be to prune the BBH ray traversal using Russian roulette based on some 
upper-bound on the transmis­sion term and the number of photons in each subtree. Such a scheme could 
further reduce render times by not considering photons which are too distant and faint to contribute 
much to the image. We leave this optimization as future work. It would be interesting to explore what 
other useful radiance estimate could be formed using the measure­ W. Jarosz &#38; M. Zwicker &#38; H. 
W. Jensen / The Beam Radiance Estimate Figure 7: The Cornell box, Cars, and Lighthouse scenes. Render 
times are shown as (minutes:seconds). For both the .xed and adaptive gathering approaches our method 
produces noise-free results while conventional photon mapping suffers from signi.cant noise, especially 
around distant light sources. W. Jarosz &#38; M. Zwicker &#38; H. W. Jensen / The Beam Radiance Estimate 
ment equation formulation. For instance, Cammarano and Jensen [CJ02] considered the problem of estimating 
the den­ sity of photons within four dimensional cylinders to prop­erly simulate motion blurred caustics. 
This process could easily be formulated as a measurement by de.ning the impor­tance function over space-time. 
Photon splatting for participating media was presented by Boudet et al. [BPPP05], where the conventional 
photon mapping method needed to be meticulously transformed into a splatting algorithm. An extra bene.t 
of our reformulation is that Equation 28 can immediately be seen as a splatting operation and could therefore 
be ef.ciently implemented in graphics hardware. Furthermore, whereas Boudet et al. only considered .xed-size 
splats, our VK approach could easily be used to adapt the splat size to the local photon density. A splatting 
approach could also be used in software to accelerate the computation for eye rays; however, the more 
general beam gathering would need to be used for any secondary rays. 7. Conclusion In this paper, we 
showed how volumetric photon mapping can be expressed as a solution to the measurement equation. This 
formulation showed that any measurement of radiance within participating can be estimated using the photon 
map. We applied this formulation by using the photon map to di­rectly estimate accumulated in-scattered 
radiance along rays. This approach was implemented using an ef.cient beam gath­ering method which can 
be used for both .xed and adaptive width kernels. The resulting algorithm produces images with signi.cantly 
less noise than conventional volumetric photon mapping using the same render time. References [BMP77] 
BREIMAN L., MEISEL W., PURCELL E.: Variable ker­ nel estimates of multivariate densities. Technometrics 
19, 2 (May 1977), 135 144. [BPPP05] BOUDET A., PITOT P., PRATMARTY D., PAULIN M.: Photon splatting for 
participating media. In GRAPHITE 05: Proceedings of the 3rd international conference on Computer graphics 
and interactive techniques in Australasia and South East Asia (New York, NY, USA, 2005), ACM Press, pp. 
197 204. [Cha60] CHANDRASEKAR S.: Radiative Transfer. Dover Publica­tions, New York, 1960. [CJ02] CAMMARANO 
M., JENSEN H. W.: Time dependent photon mapping. In EGRW 02: Proceedings of the 13th Eurographics workshop 
on Rendering (Aire-la-Ville, Switzerland, Switzerland, 2002), Eurographics Association, pp. 135 144. 
[CPP*05] CEREZO E., PÉREZ F., PUEYO X., SERON F. J., COIS X. SILLION F.: A survey on participating media 
rendering tech­niques. The Visual Computer 21, 5 (June 2005), 303 328. [JC98] JENSEN H. W., CHRISTENSEN 
P. H.: Ef.cient simulation of light transport in scences with participating media using photon maps. 
In SIGGRAPH 98: Proceedings of the 25th annual con­ference on Computer graphics and interactive techniques 
(New York, NY, USA, 1998), ACM Press, pp. 311 320. [JMLH01] JENSEN H. W., MARSCHNER S. R., LEVOY M., 
HAN-RAHAN P.: A practical model for subsurface light transport. In SIGGRAPH 01: Proceedings of the 28th 
annual conference on Computer graphics and interactive techniques (New York, NY, USA, 2001), ACM Press, 
pp. 511 518. [Kaj86] KAJIYA J. T.: The rendering equation. In SIGGRAPH 86: Proceedings of the 13th annual 
conference on Computer graphics and interactive techniques (New York, NY, USA, 1986), ACM Press, pp. 
143 150. [KH84] KAJIYA J. T., HERZEN B. P. V.: Ray tracing volume densities. In SIGGRAPH 84: Proceedings 
of the 11th annual con­ference on Computer graphics and interactive techniques (New York, NY, USA, 1984), 
ACM Press, pp. 165 174. [LW96] LAFORTUNE E. P., WILLEMS Y. D.: Rendering Partici­pating Media with Bidirectional 
Path Tracing. In Proceedings of the eurographics workshop on Rendering techniques 96 (London, UK, 1996), 
Springer-Verlag, pp. 91 100. [PARN04] PREMOZE S., ASHIKHMIN M., RAMAMOORTHI R., NAYAR S. K.: Practical 
rendering of multiple scattering effects in participating media. In Rendering Techniques (2004), pp. 
363 373. [PKK00] PAULY M., KOLLIG T., KELLER A.: Metropolis light transport for participating media. 
In Proceedings of the Euro­graphics Workshop on Rendering Techniques 2000 (London, UK, 2000), Springer-Verlag, 
pp. 11 22. [PM93] PATTANAIK S. N., MUDUR S. P.: Computation of global illumination in a participating 
medium by Monte Carlo simulation. The Journal of Visualization and Computer Animation 4, 3 (July Sept. 
1993), 133 152. [RT87] RUSHMEIER H. E., TORRANCE K. E.: The zonal method for calculating light intensities 
in the presence of a participating medium. Computer Graphics (SIGGRAPH 87 Proceedings) 21, 4 (July 1987), 
293 302. [Sil86] SILVERMAN B.: Density Estimation for Statistics and Data Analysis. Chapman and Hall, 
New York, NY, 1986. [SRNN05] SUN B., RAMAMOORTHI R., NARASIMHAN S. G., NAYAR S. K.: A practical analytic 
single scattering model for real-time rendering. ACM Trans. Graph. 24, 3 (2005), 1040 1049. [Sta95] STAM 
J.: Multiple Scattering as a Diffusion Process. In Rendering Techniques 95 (Proceedings of the Sixth 
Eurographics Workshop on Rendering) (New York, NY, 1995), Hanrahan P. M., Purgathofer W., (Eds.), Springer-Verlag, 
pp. 41 50. [Vea98] VEACH E.: Robust Monte Carlo Methods for Light Trans­port Simulation. PhD thesis, 
1998. [WMG*07] WALD I., MARK W. R., GÜNTHER J., BOULOS S., IZE T., HUNT W., PARKER S. G., SHIRLEY P.: 
State of the art in ray tracing animated scenes. In STAR Proceedings of Eurograph­ics 2007 (Prague, Czech 
Republic, September 2007), Eurograph­ics Association, pp. 0 0.  * In this talk, we are interested in 
rendering scene with participating media, or scenes wherethe volume or medium participates in the lighting 
interactions. * These are just a few example photographs of the types of effects that are caused byparticipating 
media. * The radiance, L, arriving at the eye along a ray can be expressed using the volumerendering 
equation. * at a high-level the meaning is pretty simple. * the radiance arriving at the eye is the 
sum of two terms: * the right-hand term incorporates lighting arriving from a surface * before reaching 
the eye, this radiance must travel through the medium and so is attenuatedby a transmission term * the 
left-hand term integrates the scattering of light from the medium along the wholelength of the ray * 
the main quantity that is integrated, Li, is inscattered radiance * Li itself is an integral. it represents 
the amount of light that reaches some point in thevolume (from any other location in the scene), and 
then subsequently scatterers towards theeye * Li brings about a recursive nature of the volume rendering 
equation and is extremelyexpensive to compute * as this scattered light travels towards the eye it is 
also dissipated by extinction through themedium * this computation is very expensive and there has been 
a lot of work on how to solve thisproblem efficiently       Previous Work Finite Element Zonal 
Method [Rushmeier and Torrance 87; Bhate and Tokuta 92] Diffusion [Stam 95] -Requires discretization 
 Monte Carlo Path tracing [Kajiya and Herzen 84; Kajiya 86; Lafortune and Willems 96]  Photon mapping 
[Jensen and Christensen 1998]  Metropolis [Pauly, Kollig, and Keller 00]  Path Integration [Premo e 
03] -Slow convergence/noisy results. 9 * previous methods can roughly be split up into two main categories. 
 * One of the techniques that has proven popular is photon mapping  * volumetric photon mapping starts 
by shooting photons from light sources * these photons carry energy and are deposited at surfaces and 
within the volume atscattering events * after the photon tracing stage the photon density represents 
the distribution of radiancewithin the scene. * the local information in the photon map is used to efficiently 
estimate values of inscatteredradiance * at any location within the medium inscattered radiance is computed 
by taking a localaverage of the photon energy. * efficiency is gained by reusing a relatively small 
collection of photons to computeinscattered radiance at all locations in the image (no new rays need 
to be traced to computeLi) * by reusing photons during this process, the lighting is blurred or smoothed 
out, whichreduces high frequency noise, but introduces bias * However, in order to approximate the integral 
along the ray, photon mapping uses raymarching. * ray marching is a 1D numerical integration technique 
which is computed by taking smallsteps along the ray and evaluating the inscattered radiance at each 
discrete step. * if the step size is too small, then we may .nd the same photons multiple times (shown 
inblue) * if the step size is too big, we miss features (as shown in orange). * if the step size is 
too small, then we may .nd the same photons multiple times (shown inblue) * if the step size is too 
big, we miss features (as shown in orange). * the way this manifests itself in renderings is high-frequency 
noise * with a large step-size we may completely jump over the narrow lighthouse beam * fficiency and 
noise in setting this parameters * the way this manifests itself in renderings is high-frequency noise 
 * with a large step-size we may completely jump over the narrow lighthouse beam * there is a tension 
between efficiency and noise in setting this parameters * the way this manifests itself in renderings 
is high-frequency noise * with a large step-size we may completely jump over the narrow lighthouse beam 
 * there is a tension between efficiency and noise in setting this parameters Render high-quality, noise-free 
images using photon mapping, faster. Eliminate ray marching by .nding all photons which contribute to 
theentire length of a ray. * Find all photons which contribute to the entire length of a ray. * Given 
all photons along ray, how do we use them to compute a radiance estimate? * We ll cover these in reverse 
order.  * We ll cover these in reverse order.  * We ll cover these in reverse order.  * this reformulation 
allows us to mathematically express higher level radiometric quantities * for instance, if we don t 
just want the radiance at a point, but want the total .ux on asurface, or the accumulate radiance along 
a line. * and it shows us how to estimate these values using the photon map. * concisely written as 
an inner product between the radiance .eld and a weighting function * the weighting function is typically 
non-zero only within a small region of the whole domain * concisely written as an inner product between 
the radiance .eld and a weighting function * the weighting function is typically non-zero only within 
a small region of the whole domain * concisely written as an inner product between the radiance .eld 
and a weighting function * the weighting function is typically non-zero only within a small region of 
the whole domain * concisely written as an inner product between the radiance .eld and a weighting function 
 * the weighting function is typically non-zero only within a small region of the whole domain * concisely 
written as an inner product between the radiance .eld and a weighting function * the weighting function 
is typically non-zero only within a small region of the whole domain * concisely written as an inner 
product between the radiance .eld and a weighting function * the weighting function is typically non-zero 
only within a small region of the whole domain * concisely written as an inner product between the radiance 
.eld and a weighting function * the weighting function is typically non-zero only within a small region 
of the whole domain * concisely written as an inner product between the radiance .eld and a weighting 
function * the weighting function is typically non-zero only within a small region of the whole domain 
 * Veach showed that given certain constraints on how the photons are distributed, unbiasedmeasurements 
can be estimated as a weighted sum * Veach showed this for particle tracing on surfaces, and we extend 
his derivation to includeparticipating media * Arbitrary measurements can be computed using the photon 
map           Goal 22         Photon Mapping as a Measurement Photon tracing generates 
N weighted sample rays, or photons : ray  : corresponding weight Unbiased measurements can be estimated 
as a weighted sum of photons: 29 * Veach showed that given certain constraints on how the photons are 
distributed, unbiasedmeasurements can be estimated as a weighted sum * Veach showed this for particle 
tracing on surfaces, and we extend his derivation to includeparticipating media * Arbitrary measurements 
can be computed using the photon map * Veach showed that given certain constraints on how the photons 
are distributed, unbiasedmeasurements can be estimated as a weighted sum * Veach showed this for particle 
tracing on surfaces, and we extend his derivation to includeparticipating media * Arbitrary measurements 
can be computed using the photon map   Conventional Radiance Estimate Veach showed that the conventional 
radiance estimate (for surfaces) is a measurement, where blurs photon contributions across surfaces. 
Also true for conventional volumetric radiance estimate, but blurs withinvolume. 30  * if we can represent 
the quantity we want to compute as a measurement, then we cancompute estimates of that quantity using 
the measurement equation * if we can represent the quantity we want to compute as a measurement, then 
we cancompute estimates of that quantity using the measurement equation * delta function means we only 
get a useable estimate if a photon falls directly on the line  * so in practice we replace the delta 
function with a blurring kernel which blurs radiance fromthe line into a cylinder. * the kernel allows 
photons that are not directly on the line to be used in the estimate * we have the freedom to choose 
the exact form of this blurring kernel * so in practice we replace the delta function with a blurring 
kernel which blurs radiance fromthe line into a cylinder. * the kernel allows photons that are not directly 
on the line to be used in the estimate * we have the freedom to choose the exact form of this blurring 
kernel  A .xed-size kernel results in a uniform blur of the photon map.  In this case, we need to .nd 
photons in.xed-radius cylinder about ray.  * When using a constant blurring radius, in the limit the 
conventional and beam radianceestimates are equivalent. * uses exactly the same photon map * When using 
a constant blurring radius, in the limit the conventional and beam radianceestimates are equivalent. 
 * uses exactly the same photon map * When using a constant blurring radius, in the limit the conventional 
and beam radianceestimates are equivalent. * uses exactly the same photon map * When using a constant 
blurring radius, in the limit the conventional and beam radianceestimates are equivalent. * uses exactly 
the same photon map * however, in practice a .xed radius is rarely used, and the nearest neighbors method 
is usedto adapt the radius to the local density of photons * The conventional radiance estimate uses 
the k-nearest neighbor method at a point. * How can we generalize this along a line? * The conventional 
radiance estimate uses the k-nearest neighbor method at a point. * How can we generalize this along 
a line? * in order to address this we turn to the primal vs. dual interpretation of density estimation 
 * two different interpretations of density estimation * exactly equivalent for .xed-radius searches 
 * in order to address this we turn to the primal vs. dual interpretation of density estimation * two 
different interpretations of density estimation * exactly equivalent for .xed-radius searches * in 
order to address this we turn to the primal vs. dual interpretation of density estimation * two different 
interpretations of density estimation * exactly equivalent for .xed-radius searches * .rst two steps 
identical to regular photon mapping  * .rst two steps identical to regular photon mapping  * .rst two 
steps identical to regular photon mapping  * if we use a .xed kernel, then each radius is the same, 
otherwise the radius is computedfrom the local density of each photon            What is 
                   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401138</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>9</pages>
		<display_no>4</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Rendering translucent materials using photon diffusion]]></title>
		<page_from>1</page_from>
		<page_to>9</page_to>
		<doi_number>10.1145/1401132.1401138</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401138</url>
		<abstract>
			<par><![CDATA[<p>We present a new algorithm for rendering translucent materials that combines photon tracing with diffusion. This combination makes it possible to efficiently render highly scattering translucent materials while accounting for internal blockers, complex geometry, translucent inter-scattering, and transmission and refraction of light at the boundary causing internal caustics. These effects cannot be accounted for with previous rendering approaches using the dipole or multipole diffusion approximations that only sample the incident illumination at the surface of the material. Instead of sampling lighting at the surface we trace photons into the material and store them volumetrically at their first scattering interaction with the material. We hierarchically integrate the diffusion of light from the photons to compute the radiant emittance at points on the surface of the material. For increased accuracy we use the incidence plane of the photon and the viewpoint on the surface to blend between three analytic diffusion approximations that best describe the geometric configuration between the photon and the shading point. For this purpose we introduce a new quadpole diffusion approximation that models diffusion at right angled edges, and an attenuation kernel to more accurately model multiple scattering near a light source. The photon diffusion approach is as efficient as previous Monte Carlo sampling approaches based on the dipole or multipole diffusion approximations, and our results demonstrate that it is more accurate and capable of capturing several illumination effects previously ignored when simulating the diffusion of light in translucent materials.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098602</person_id>
				<author_profile_id><![CDATA[81100639133]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Donner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098603</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015726</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{CTW*04} Chen Y., Tong X., Wang J., Lin S., Guo B., Shum H.-Y.: Shell texture functions. <i>ACM Trans. Graphic. 23</i> (2004), 343--353.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{DEJ*99} Dorsey J., Edelman A., Jensen H. W., Legakis J., Pedersen H. K.: Modeling and rendering of weathered stone. In <i>Proceedings of ACM SIGGRAPH 1999</i> (1999), pp. 225--234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073308</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{DJ05} Donner C., Jensen H. W.: Light diffusion in multi-layered translucent materials. <i>ACM Trans. Graphic. 24</i>, 3 (2005), 1032--1039.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882433</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{DS03} Dachsbacher C., Stamminger M.: Translucent shadow maps. In <i>Rendering Techniques</i> (2003), pp. 197--201.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{EHR73} Egan W. G., Hilgeman T. W., Reichman J.: Determination of absorption and scattering coefficients for nonhomogeneous media. 2: Experiment. <i>Appl. Opt. 12</i> (1973), 1816--1823.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{FPW92} Farrell T. J., Patterson M. S., Wilson B.: A diffusion theory model of spatially resolved, steadystate diffuse reflections for the noninvasive determination of tissue optical properties <i>in vivo. Med. Phys. 19</i>, 4 (1992), 879--888.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089522</ref_obj_id>
				<ref_obj_pid>1089508</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{HMBR05} Haber T., Mertens T., Bekaert P., Reeth F. V.: A computational approach to simulate subsurface light diffusion in arbitrarily shaped objects. In <i>Proceedings of the 2005 conference on Graphics interface</i> (2005), pp. 79--86.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Ish78} Ishimaru A.: <i>Wave Propagation and Scattering in Random Media</i>. Oxford University Press, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566619</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{JB02} Jensen H. W., Buhler J.: A rapid hierarchical rendering technique for translucent materials. <i>ACM Trans. Graphic. 21</i> (2002), 576--581.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{JC98} Jensen H. W., Christensen P. H.: Efficient simulation of light transport in scences with participating media using photon maps. In <i>Proceedings of ACM SIGGRAPH 1998</i> (1998), pp. 311--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Jen01} Jensen H. W.: <i>Realistic Image Synthesis Using Photon Mapping</i>. AK Peters, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383840</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{JLD99} Jensen H. W., Legakis J., Dorsey J.: Rendering of wet materials. In <i>Rendering Techniques</i> (1999), pp. 273--282.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{JMLH01} Jensen H. W., Marschner S. R., Levoy M., Hanrahan P.: A practical model for subsurface light transport. In <i>Proceedings of ACM SIGGRAPH 2001</i> (2001), pp. 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Kie05} Kienle A.: Light diffusion through a turbid parallelepiped. <i>J. Opt. Soc. Am. 22</i>, 9 (2005), 1883--1888.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383694</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{LPT05} Li H., Pellacini F., Torrance K.: A hybrid monte carlo method for accurate and efficient subsurface scattering. In <i>Rendering Techniques</i> (2005), pp. 283--290.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275468</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{LW96} Lafortune E. P., Willems Y. D.: Rendering participating media with bidirectional path tracing. In <i>Rendering Techniques</i> (1996), pp. 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946976</ref_obj_id>
				<ref_obj_pid>946250</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{MKB*03} Mertens T., Kautz J., Bekaert P., Reeth F. V., Seidel H.-P.: Efficient rendering of local subsurface scattering. In <i>Proceedings of the 11th Pacific Conference on Computer Graphics and Applications</i> (2003), pp. 51--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{NRH*77} Nicodemus F. E., Richmond J. C., Hsia J. J., Ginsberg I. W., Limperis T.: <i>Geometrical Considerations and Nomenclature for Reflectance</i>. National Bureau of Standards, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732117</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{PKK00} Pauly M., Kollig T., Keller A.: Metropolis light transport for participating media. In <i>Rendering Techniques</i> (2000), pp. 11--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{Sta95} Stam J.: Multiple scattering as a diffusion process. In <i>Rendering Techniques</i> (1995), pp. 41--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Eurographics Symposium on Rendering (2007) Jan Kautz and SumantaPattanaik (Editors) Rendering Translucent 
Materials Using Photon Diffusion Craig Donner HenrikWann Jensen University of California, San Diego 
Abstract We present a new algorithm for rendering translucent materials that combines photon tracing 
with diffusion. This combinationmakesitpossibletoef.cientlyrender highlyscatteringtranslucent materialswhile 
accountingfor internalblockers, complexgeometry,translucentinter-scattering,andtransmissionandrefractionoflightatthe 
boundary causinginternal caustics. Theseeffects cannotbe accounted for withpreviousrendering approaches 
usingthedipoleormultipolediffusionapproximationsthatonly sampletheincidentilluminationatthe surface ofthe 
material.Insteadofsamplinglightingatthesurfacewetracephotonsintothe materialandstorethem volumetricallyattheir.rst 
scatteringinteractionwiththematerial.Wehierarchicallyintegratethediffusionof lightfromthephotonstocomputetheradiantemittanceatpointsonthesurfaceofthe 
material.Forincreased accuracy we use the incidence plane of the photon and the viewpoint on the surface 
to blend between three analytic diffusion approximations that bestdescribethegeometric con.guration between 
the photon and the shading point. Forthis purposeweintroduceanew quadpolediffusionapproximationthat modelsdiffusionatrightanglededges, 
and an attenuation kernel to more accurately model multiple scattering near a light source. The photon 
diffusion approachis asef.cient aspreviousMonte Carlosampling approaches basedon the dipole ormultipole 
diffusion approximations, and ourresults demonstrate thatitismore accurate and capableof capturingseveralillumination 
effects previously ignored when simulating the diffusion of light in translucent materials. Categories 
and Subject Descriptors (according to ACM CCS): I.3.7 [Computing Methodologies]: Computer GraphicsThree-Dimensional 
Graphics and Realism Introduction Simulating lighttransportin naturalmaterials such asmilk, marble, 
leaves, skin, and wax is a challenging problem in computer graphics.Light penetrates thesetranslucentmateri­ 
als, scattersinside,andmayexitatadifferent locationand direction thanitentered. This scatteringoflightcausesmany 
complex phenomena,suchasvolumetric caustics,translucent inter-scattering between surfaces, and volumetric 
shadows. Theseeffects areimportantfor conveyingrealism when ren­ dering images of translucent materials. 
The most general methods in computer graphics capable of simulating theseeffectsare Monte Carlo methods.Volumet­ 
ric caustics and shadows have been rendered with Monte Carloraytracing[LW96,JLD99,PKK00]and photonmap­ 
 {cdonner, henrik}@graphics.ucsd.edu . The Eurographics Association 2007. ping[JC98,DEJ*99]. Unfortunately, 
when rendering highly scattering materials, these methods have high computational cost. Many recent appearance 
models for translucent materials are based on thediffusion approximation[Sta95]. These meth­ ods arelimitedto 
homogeneousmaterialswithhighalbedo, and cannotdirectlysimulate fullvolumetriceffects.Jensen et al.[JMLH01]use 
an analytic diffusion approximation to compute light transportin scattering materials, usingMonte Carlo 
methods to sample the direct illumination. Donner and Jensen[DJ05]extendthisdiffusion modeltothin and 
layered translucentmaterials,but usethe samesampling technique. Toinpart captureglobaleffects such asindirectillumina­tion, 
Jensen and Buhler[JB02]samplethe fullirradiance of surfaces directly. Sampling the surface can be expensive 
forcomplexgeometry,andmayleadtoa biasedresult with­ out suf.cient sampling density. Others have used 
shadow maps [DS03] or importance sampling [MKB*03] to estimate  C. Donner&#38;H.W.Jensen/RenderingTranslucent 
Materials Using Photon Diffusion the incidentillumination. Allofthesemethodssimulate only surfaceto surfacetransport, 
and cannot rendereffects such as inter-scattering and volumetric caustics and shadows. Haber et al.[HMBR05]extend 
the multigrid method to account for internal visibility in translucent material, but cannot render caustics 
or translucent inter-scattering. Other hybrid methods couple diffusion with a pre­computation step. Chen 
et al.[CTW*04]generate shell texture functions usingvolumetric photonmapping[JC98]. Their method requires 
discretizing geometry, and a complex acquisition step to determine the shell volume s structure. Photons 
are traced into the materialfrom manydirections over the sphere, and contribute to bothsingle and multiple 
scattering.Thisrequiresa correctiontermduring renderingto handle non-diffuse incident light. Photons 
thatpass through the shell are not directly used, and inter-scattering is not simulatedby this method. 
ThehybridmethodbyLi etal.[LPT05]usespathtracing until eye rays reach a central core of the material, 
at which pointthediffusion dipoleisused. This method, while general, does not account for multiple inter-scattering 
betweentranslu­ cent surfaces, or caustics, and is computationally expensive for highly scattering materials. 
In this paper,we presenta methodthatincorporates global illuminationeffects suchas caustics,inter-scattering, 
andvol­ umetric shadows, fortranslucent materials.Our technique usesaphotontracingsteptodistribute photonsinthematerial, 
butusesdiffusionto accountformultiple scattering.Wealso extend previous work to handle oblique incident 
illumination and introducea new techniquefor approximating the bound­ aryconditions caused by complex 
geometry. This avoids the potentially complex surface sampling step associated with previouswork, andmakes 
no assumptions on theincident lightingdistribution (e.g. normal, diffuse).Photondiffusion can simulate 
light transportin translucent materials not han­ dledby existing techniques based on diffusion, and would 
be prohibitively expensive to compute using other methods. These effects include: refraction at the 
interface leading to internal caustics,  anisotropic surface pro.les due to oblique illumination,  
transmission of light through thin areas,  volumetric effects such as shadows and caustics, and  translucent 
inter-scattering from multiply-scattered light.  In addition, our source-based approachlendsitselftoef.cient 
volumetric hierarchical integration by clustering sources to­ gether that are near each other volumetrically. 
Diffusion from point sources Light scatteringintranslucent materialsis describedbythe bidirectional 
scattering surface re.ectance distribution func­tion (BSSRDF) S[NRH*77]. The BSSRDF de.nes the gen­ 
eraltransportoflight betweentwopointsanddirectionsasthe (a) Dipole source position (b) Reduced intensity 
sources Figure 1: (a)An incident beamis usuallytransformed into an isotropic source onemeanfreepathbelowtheincident 
position. (b) The actual source distribution is an in.nite set of point sources with exponentially decaying 
intensity. fraction of the .ux, Fi(wxi,w.i) incident at wxi thatcontributes to the radiance Lo(wxo,wxo 
in direction .o) exiting atpositionww.o dLo(wxo,wxi,wxo,wxi,w(1) .o)= S(w.i;w.o)dF(w.i). Inhighly scatteringmaterialsthe 
diffusionoflight canbe approximated by a diffuse BSSRDF Sm [JMLH01,JB02] Sm(wxi,w.i;wxo,w.o)= F(w.i)R(||wxi-wxo||)F(w.o) 
(2) where R(d) isadiffusionkernel,and F(w.i) and F(w.o) are the Fresnel terms for a smoothsurface. Jensen 
et al.[JMLH01] useadipolediffusion approximationfor Rd thatcomputes the diffusionoflightduetoavirtuallight 
sourceplaced onemean freepathinsidethematerial.Inthefollowingwe generalize theplacementofthis virtuallight 
sourceandshowhowthis can be used to compute diffusion with higher accuracy. Consider the case of a beam 
of light normally incident on an isotropically scattering semi-in.nite homogeneous material. The distance 
light in this beam travels before interacting (absorbing or scattering) withthematerialis describedbythe 
-s' PDF stIetx. Thustheaveragedistance lighttravelsisthe expected value of the PDF Z 8 -s' 1 x stI e 
txdx= (3) I 0 st or one mean free path. Here stI= sa + sIs is the reduced extinction coef.cient, and 
sa and sIs are the absorption and reduced scattering coef.cients. As it propagates through the material, 
the beam has a reduced intensity Lri due to absorp­tion and out-scattering of light[Ish78] tx Lri(z)= 
Lie -s'(4) where Li istheirradianceatthesurface.Sincethe material scatters isotropically, thebeamisequivalentto 
an in.niteset of point sources along its length with decaying intensity. The average intensity scattered 
out of the beam is Z 8 sI s Lie -st'xdx= aILi. (5) 0 where aI = sIs/stIis the reducedalbedo.Equations 
3and5 c C. Donner&#38;H.W.Jensen/RenderingTranslucent Materials Using Photon Diffusion 0. 30. 60. 80. 
Figure 2: Distributingdiffusing sources along the incident beamclosely approximates the asymmetricexitantradiance.Thetop 
row of images was rendered using the standarddiffusion dipole (DD), the second row using Monte Carlo 
pathtracing (MC), while the third row uses sources distributed alongtheincident beam (PD). Below theimages 
are intensityplotsofthegreen channelhorizontal center scanlineofthe above images.Theincident angleof 
the beam increasesfrom 0. to 80. from leftto right, and the beam has unit radius. imply that a normally 
incident beam of light can be approxi-2.1 Oblique illumination matedbya sourceembeddedinthematerialatonemeanfree 
path, with aintensity of aILi. Usingthe diffusion approxima-To use the diffusion dipole, incident rays 
of light are con­tion to compute the exitant surface radiance due to the point vertedto dipole sources[JMLH01].Regardlessofthe 
inci­ source leadstothediffusiondipoleapproximation[JMLH01] dent angleof theillumination, thesourcesarealwaysmir­ 
rored abouttheincidentpoint(Figure 1a). Thus,thedipole embeddedat onemeanfreepathalwayspredictsaradially 
 -strdr -strdv symmetric radiant emittance (Figure 2, top). ee Rd(r)= zr(1+ strdr)+ zv(1+ strdv) (6) 
4p d34p d3 rv Oblique anglesofincidence causeasigni.cantshiftinthe re.ectance, and produce noticeable 
asymmetric surface ef­where the radiant emittance is found by summing the contri-fects(Figure 2,middle). 
Thisis trueinmany commonma­bution of a real source at the average distance light travels, terials, where 
the scattering and absorption parameters are and a virtual negative source mirrored around an extrapo-spectral. 
3sast I isthe effective lated boundary of height 2AD. str = We approximate theseeffectsfrom obliqueilluminationby 
transport coef.cient, zr = 1/st I and zv = zr + 4AD are the placing sourcesalongtheincidentbeam,andusingthedif­ 
fusionapproximationto compute the radiant emittance from z-coordinates of the real and virtual sources 
relative tothe surface at z = 0. Also, dr = r2+ zr 2 and dv = r2+ zv 2 eachof thesesources. Thisiseffectively 
anintegrationof are the distances to the sources from a given point on the Equation 6over thepropagating 
beam,taking into account surface of the object. D= 1 isthediffusion constant, and 3st ' theexponential 
decayin Equation 4 A=(1+ Fdr)/(1- Fdr) represents the changein .uence due Z 8 tointernalre.ectionatthe 
surface.ThediffuseFresnelre­ -strdr -s' e.(dr)e tx zr(1+ strdr) Rri(r)= .ectance Fdr is approximatedbythefollowingpolynomial 
4p d3 r 0 expansions[EHR73] (8) e-strdv +zv(1+ strdv) dx 4p d3 v Fdr . . .. .. 0.7099 0.3319 0.0636 
-0.4399 + - + , . < 1 .2 .3. . 1.4399 0.7099 Note that we rede.ne the depthofthe source as zr = xcos 
.i, - ++ 0.6681 + 0.0636., . > 1 .2 . where x is the distance along the refracted beam and .i is (7) 
the angle the refracted beam makes with the internal normal or,inthe caseofaroughsurface,isreplacedbyanaverage 
ofthesurface. The othervalues zv, dr and dv are allde.ned BRDF re.ectance[DJ05]. Notethatin thestandarddipole 
intermsof zr as before. .(x) is an attenuation function dis­approximationthe sourcepoweris theaverage 
outscattered cussed below. Unfortunately, this integral has no apparent power aILi. closedform solution[FPW92].Rather 
than attemptingto &#38;#169; The Eurographics Association 2007. C. Donner&#38;H.W.Jensen/RenderingTranslucent 
Materials Using Photon Diffusion Figure 3: Modulating sources that are close to the surface closely 
approximates the actual multiple scattering radiant emittance. numerically integrate it, we sample the 
integral by distribut­ ingdiffusing sources alongthe refracted beam usingthePDF stIe-st ' z.Wetreateachphotonasapositive 
sourceofadipole. The total re.ectanceis the sumof contributionsfromthese real sources and their corresponding 
virtualsources.We dis­ cuss how to distribute sources in translucent materials in Section 4.1. Becausewe 
usediffusionto computethe multiple scattered radiancefromthedistributedsources,thoseclosetothesur­ face, 
where single scattering dominates, will over-estimate theexitantpower.Ratherthanclampthedipole, wemodu­lateit 
with asmoothkernelthat estimateshowmuchofthe source s power contributes to non-single-scattered light 
-stx .(x)= 1- e (9) Notethat thisis an approximation, wemodulateby the ap­ proximate probability thatlightis 
not scattered beforeexiting the surface.Wehavetestedthisattenuateddipolewitharange ofinput parameters, 
and found thatitis more accuratethan the dipole approximation (see Figure 3). Figure 2comparesthe multiple 
scattering re.ectanceofa large body of skim milkilluminatedfrom severalincident anglesbya beamoflightwithunit 
radius.Thestandarddiffu­sion dipolegives an axisymmetricre.ectance, as each photon inthe beamtransformsintoasingle 
source belowthe inci­ dencepoint.Inthismaterial,however,redlightis morehighly scatteringthan greenandbluelight,and 
emergescloserto theleadingedgeof the beam,tothe right.As theangleof in­cidence increases, this effectbecomes 
more prominent. Less scattering blue light emerges more atthe trailing edge of the beam,totheleft.Usingadistributed 
setof sourcescaptures this shift.Italso modelsthe shiftofthe beamtotherightwith increasing angle. Diffusion 
in arbitrary geometry The previous section described placing diffusing sources along refractedincident 
beamsand theirvirtual sourcesabove the surface. To increase the accuracy of rendering using sources at 
arbitrary locations, we introduce a new method to approximate light transport in complex geometry. The 
distribution of sources along incident beams of light satis.es an approximate boundary condition on the 
radiance atthesurfaceinthe samewayastheoriginaldipole.This approximate condition f(r)(-2AD)= 0 (10) states 
thatthe diffuse .ux f is zero at an extrapolatedbound­ ary2AD above the surface. If the materialisa .nite 
slabofthickness d,light mayscatter out through the bottom of the slab, reducing the observed re.ectance. 
Thisis modeledbya similar boundary condition at the bottom of the slab f(r)(d+ 2AD)= 0. (11) Satisfying 
bothconditionsrequiresmirroringthe dipoleofthe topfacearound theextrapolated boundaryofthe bottomface, 
and thenrepeatingthismirroringaboutthetopinterface, ad in.nitum, producing the multipole [DJ05]. The 
z-coordinates of the multipole sources are zr,i = 2i(d+ 4AD)+ e (12) zv,i = 2i(d+ 4AD) -e - 2zb , i= 
-n,...,n, wherethe numberof dipolesis 2n+ 1, and e is the depth of the originalreal source. The total 
re.ectance predictedbythe multipoleisthesumofthe contributionsofeachofthedipole sources n -strdr,i -strdv,i 
zr,i(1+ strdr,i)ezv,i(1+ strdv,i)e Rm(r)=. - 4p d34p d3 i=-nr,iv,i (13)  where dr,i =r2+ zr2 ,i and 
dv,i=r2+ zv2 ,i arethe distances to each source from a point on the surface. C. Donner&#38;H.W.Jensen/RenderingTranslucent 
Materials Using Photon Diffusion  adjacent faces. 3.1 A quadpole diffusion approximation Boththe dipole 
and multipole make the assumption that the surface of the material is .at and in.nite in extent. Real 
objects, however, are often approximated by .nite polygo­nal meshes, and mayhave sharp corners.Forexample,ina 
translucentcube,lightthat entersatthetopfacemayexit a sideface,ratherthanreturningtothetop surfaceaspredicted 
by the dipole. Weaccountforlightexitingafaceadjoining thelitfaceata rightanglebyimposinganadditional 
boundarycondition(see Figure 4). In addition to the extrapolated boundary above the litface,thereis 
another above theadjoiningface.Mirroring the originaldipoleaboutthisside boundarygivesasymmetric setoffour 
sources,a quadpole. Notethat unlikethe multipole, four sources are suf.cienttosatisfy bothboundary conditions, 
duetosymmetry. There.ectancefromthe quadpole onthe sideface is e-strdr e-strdv Rq(r)= xr(1+ strdr)+ 
xr(1+ strdv) 4p d34p d3 rv (14) e-strdr -strdv e + xv(1+ strdrm)+ xv(1+ strdvm) 4p d34p d3 rm vm where 
drm and dvm are the distances fromthe shading point to the second dipole, and xr and xv are the distances 
tothe sideface (seeFigure 4).When calculating the re.ectanceat thetopface, xr and xv arethe distancestothetopface,asin 
the original dipole. The quadpole sources are the corners of a square centered at the intersection of 
the extrapolated boundaries (see Figure 4). Theplaneofthe quadpoleis perpendiculartotheplanesof the 
twofaces[Kie05]. Foraclosedmesh, onecouldimpose boundary conditions on eachface and tryto satisfythem 
simultaneously,butthis would be costly for non-trivial meshes. Instead, we use only information abouttheface 
the source enteredthematerial through, and the viewedface. Thisallowsinterpolation be­tweenthe dipole, 
quadpole, andmultipoleto obtain an ap­ c &#38;#169; The Eurographics Association 2007. Figure 5: (top)The 
quadpolemore accuratelycaptures the changeinillumination acrossaright angle than the dipole. (bottom) 
Interpolating between the dipole, quadpole, and multipole closely approximates the re.ectance of smooth 
surfaces. Both plots are of the blue channel of the center vertical scanline of the image. proximate 
re.ectance.We discuss thisin more detailin Sec­tion 4.4. Figure 5(top) compares the dipole and quadpole 
re.ectance toa MonteCarlopath tracing simulationontwofacesofan opticallythickmarbleboxwith a cornerataboutpixel125. 
Note the quadpole accurately captures the discontinuity at the corner, and closely approximatesthe re.ectancealong 
theadjoiningface.The dipole underestimates the re.ectance of thesidefaceof the box, asitdoes nottakethe 
additional boundary conditioninto account. The bottom of the .gure shows the same comparison in the case 
of a sphere, using an interpolation method described in Section 4.4. 4 Implementation To render with 
photon diffusion we useatwo-pass algorithm. In the .rst pass, photons are tracedthrough the scene, and 
storedattheir single scatteringinteractionswith translucent materials. Photonsmay continuetoscatter,butareonlystored 
againif theyexit and re-enter thematerial, e.g.,inthe caseof re.ection,transmission, or complex geometry 
(seeFigure 6). We thenbuilda hierarchical accelerationstructure fromthe photons forfastevaluation. Inthesecondpass, 
during rendering,we traverse theacceler­ ationstructuretovolumetricallyintegratethe contributions of 
the photons using the the dipole, quadpole, and multipole. 4.1 Source distribution Photons are traced 
from light sources towards translucent objects using standard Monte Carlo methods(e.g.,[Jen01]). Whena 
photonhitsascatteringmaterialandrefractsin,it propagates until it (a)hits anotherinterface,inwhich casethe 
photonis re­ C. Donner&#38;H.W.Jensen/RenderingTranslucent Materials Using Photon Diffusion Standard 
distribution Traced distribution Figure 6: (Left) The diffusion dipole is evaluated by sampling the 
surface of an object to determine the incident .ux, and generating sources one meanfree path belowtheincident 
location.(Right)Tracing photons givesa betterdistribution of sources, and captures important global illumination 
effects, such as(a) actual sourcedepthandshiftsinillumination due to refraction, (b) global illumination 
from re.ections off other surfaces, (c) volumetric caustics, (d) diffuse caustics, (e) translucent inter-scattering, 
and (f) indirectillumination on diffuse surfaces from translucent effects. fractedout ofthe material 
and continues on (internal re­ .ectionis handledby the diffusion models), (b)is absorbed probabilistically 
or hits some absorbing blocker, in which case the photon is terminated, (c)orscatters,inwhich casewestorethephoton 
sposition, power,anditsincidenceplane.Thephotonthen continues to scatteruntil(a)or(b) occur,butisonlystoredagainifit 
exits and re-enters the material. Photonsthat continueto scatterthroughoutthematerialare notstored,asthey 
areinphotonmapping, unlessthey exit and re-enter the material. Multiple scatteringis handledby treating 
the photons as diffusion sources. Tracing photons in translucent materials captures manyim­ portantilluminationeffects, 
such asrefractionattheinterface (Figure 6a).Ifa photonintersects the backsideof the ge­ ometry, it refracts 
out, creating caustics on diffuse surfaces (Figure 6d), or self-lightingthrough an opticallythin areas 
(Figure 6c).Since photonsmayinteractwith other objects beforethe translucentmaterial,theremayalsobelighting 
from indirectglobalillumination(Figure 6b). Photons that scatteroutofthematerialmayproduce sourcesinotherareas 
through translucent inter-scattering (Figure 6e), or contribute to indirect illumination on other surfaces 
(Figure 6f). Thetotalexitantradianceatapointonthesurfaceis found by summing the contributions of all 
stored sources. Note thatsince we are distributingpower from thelights intothe material,thereis no area 
associatedwith the photons.This photon tracing step also avoids the potentially expensive task ofexplicitlysamplingthe 
geometry[JB02]. Photonsthat exit the material and contribute to caustics, translucent inter­ scattering, 
or indirectillumination discretely representexitant globalillumination duetothemultiple scatteringoflight,just 
as in photon mapping, thus energy is conserved.  4.2 Hierarchical approximation Using photon tracing 
to seed diffusion can produce millions of sources. To ef.ciently calculate their contribution to a shading 
point, we hierarchically clustersources together.As Jensen andBuhler[JB02]note, diffusion sexponentialfall­ 
 off with distance facilitates this approximation. Our hier­archical methodis similartotheirs,but we 
group sources together volumetrically and aggregate different information about them. Speci.cally,we 
constructanoctree,witha maximumofeight sourcesperleaf.For eachsource,we storeitsposition,power, andtheplaneofthefaceatwhichit 
enteredthematerial. Once the photon tracing stage is complete, we aggregate the photons in each voxel 
to create a representative source. Each node computes and stores the totalpower, an average postion 
weighted by power, and an average plane. We average planes by taking theaverage normalofthe sources, 
and anaverage depthbelowtheplane.To ensurethestabilityofthediffusion calculations we checkthat the depthis 
positive, otherwise we move the plane along its normal based on the average depth.  4.3 Single scattering 
Mostapproachesforrenderinghighlyscatteringmaterials use an analytical approximationforsinglescattering[JMLH01]. 
 Tocaptureglobalilluminationeffectssuchasvolumetric caus­ tics, we use the single scattered photons storedin 
the photon tracingstep.Wemarchrays throughthevolumeandevaluate volume radiance estimates from a photon 
map[JC98]. This gives accurate refracted beams, and volumetric caustics.  4.4 Multiple scattering The 
multiple scattering contribution uses the octree built after the photon tracing stage. We traverse from 
the root of the octree until we reach a voxel withsolid angle (from theperspectiveofthe shading point)greaterthana 
user-set threshold,oruntilaleafisreached.Therepresentative source (or sourcesofaleaf)inthe nodeare thenusedtocalculate 
the radiant emittance as described in Section 2. We construct a dipole, quadpole, and multipole depending 
on the angle . betweenthe shading point normalwni and the source normal wno ..p . 22 - . Rd(r)+ . Rq(r), 
0= . = p R(r)= .. 2 (15) p p (p - .) Rq(r)+ . - p Rm(r), 2 = . = p 2 where Rd(r) isthe dipole, Rq(r) 
isthe quadpole, and Rm(r) is the multipole. In the case of arbitrarygeometry, we compute the dipole as 
in[JMLH01].We estimatethethicknessofthematerialfor the mutipole as the sum of the depthof the source 
relative c C. Donner&#38;H.W.Jensen/RenderingTranslucent Materials Using Photon Diffusion Dipole diffusion 
model Photon diffusion Figure 7: Photon diffusion simulates indirectlightinginthe complexgeometryof an 
ear.Photons that scatter manytimes distributemultiplesources deepinto the ear. This causesin particularthe 
brighteningof thefrontal earlobe, and thelighting inthe inner ear. Theseeffects are not capturedby the 
dipole ormultipole.Also, notethat photon diffusionretains more high frequencygeometry, asit accounts 
for the high angleof incident illumination. to itsincidentplane, and relativeto the view plane.Note we 
needonly compute the multipole re.ectance using the observed source depth, and do not need separate re.ectance 
and transmittance multipolecomputations.To use the quad­ pole, we assumethegeometrybetween the source 
s incident location and the shading point is two planes at a right angle. 4.5 Volumetric shadowing Weuse 
the diffusion sourcestosimulate .rstordervolumetric shadows. During the photon tracing step, photons 
may inter­ actwithanyblocking geometryinsidethetranslucentvolume. If the material is absorbing, the photon 
is terminated. During rendering,wetracea shadowrayfromthe shadepointtothe volumetricsource.Iftherayintersectsablockingmaterial,no 
contribution fromthe sourceisadded. Although this method does not account for re.ecting blockers, or 
light paths around blockers, we have found it gives a good .rst order shadowing approximation. 5 Results 
Wehaveimplementedphoton diffusioninaMonte Carlo ray tracer that supports photon tracing as describedin[Jen01]. 
&#38;#169; The Eurographics Association 2007. Allimagesinthis paper were renderedonanIntelCore2Duo 2.4GHz. 
Allimagestook about15minutestorender, there was little differencein render time when usingMonte Carlo 
surfacesampling[JMLH01]or photondiffusion.Although there is no sampling noise with photon diffusion, 
a moderate number of sources is requiredfor accurate sampling ofthe lighting. It is unfortunately extremely 
dif.cult to render reference imagesusingpath tracingmethods thatcapturetheeffects showninFigures 7and8.Theprobabilityoftracedrays 
scat­ tering and hittingalight sourceis astronomicallylow. Photon mapping quickly exhausts memory for 
highly scattering ma­ terials. Thus, we directly compare photon diffusion to the diffusion dipole. Figure 
7showsanexampleofabacklit earrendered using photon diffusionand the diffusion dipole.Roughly3million 
photons werestoredintheoctree,andtookabout5minutesto tracetofullycapture translucent inter-scattering.Thebright­ 
eningofthethin partsof the earintherightimageis because the dipoleunderestimatestransmittance[DJ05],whereas 
with photon diffusion the multipole calculation correctly captures thehigh translucency.Theinnerear,frontalearlobe,andthe 
sideofthehead arelitby sourcesdepositedfrommultiply C. Donner&#38;H.W.Jensen/RenderingTranslucent Materials 
Using Photon Diffusion Diffusion dipole model Photon diffusion Figure 8: Ourmethodsimulatesglobalilluminationeffectsintranslucent 
materials.Theleftimage wasrenderedusingthe diffusiondipole,whilethe middleandrightimagesusingphotondiffusion.Notethevolumetric 
causticatthesurfaceofthemilk, andthevolumetric shadowcastbytherimoftheglass.Inthefarrightimage,thebrighteningatthetopofthemilkisfromlight 
penetratinginto the volume, andfromindirectilluminationfromthe .oor. Lightrefracting through the bottomof 
the glass under the milk forms the caustic. Some of this light re.ects offthe bottom of the glass, causing 
a subtle glow. scatteredphotons.This translucent inter-scatteringcannotbe captured by previous diffusion 
techniques that sample only thesurface.Inaddition, photondiffusionshowsmorehigh frequencysurface geometry, 
such as the pores of the skin, asitaccurately handlesthe highanglesof illumination. The dipole,however,blurstheilluminationandsmoothsthe 
skin s appearance. InFigure 8threeimagesofaglassofskimmilk areshown. Theleftimagewas rendered usingthediffusion 
dipole, while the middle and right images with photon diffusion. Roughly 6million photonswerestoredinthemiddle 
andrightimages, taking about oneminutetotrace, with 3000 photonsinthe single scattering radiance estimate. 
Note that photons bounc­ ingoffthe backsideoftheglassforma cardioidcausticin the milk. This caustic is 
volumetric,the photons have pen­etrated into the milk volume. In the right image, the back of themilkislitbylightpenetratingthrough 
themilk and light bouncingoffthe .oor plane, whilethe bottomglows fromlight bouncingoffthe bottomsurfaceofthe 
glass, and the .oorplane.Also,notethatthe shadowfromtherimof theglassisvolumetric.The causticonthegroundplaneis 
formedfromlight refracting through theglassbelowthemilk. Simulating these effects with Monte Carlo path 
tracing or photon mapping would be prohibitively expensive. Figure9shows anexampleofvolumetric shadowscast 
from the bonesina backlit hand.The leftimagewas rendered without bones in the hand, while the right image 
includes the bones. Roughly0.4 million photons were stored, and took about15secondstotrace bothwithandwithoutthe 
bones geometry. Photons that intersected the bones were absorbed, giving a .rstorder approximation of 
shadowing as described in Section 4.5.The shadows clearly showthe bones in the .ngers and in the hand. 
6 Discussion and Conclusions Wehave presenteda methodfor rendering globalillumina­tioneffectsintranslucentmaterialsusing 
photondiffusion. We trace photons into the material and store them at their .rst scatteringinteractions. 
Photons continueto scatterinthe medium and may contribute to translucent inter-scattering between surfaces.Thisallowspenetrationoflightintodeeply 
shadowed areas, and simulates volumetric shadows and caus­ ticsin translucentmaterials.Foref.ciencyweclusterphotons 
together and integrate their contribution hierarchically.To improvethe accuracyof diffusioncalculations, 
wehavein­ troduced a quadpole diffusion approximation, and combined it withthe dipole and multipletosimulatelighttransportin 
complex geometry.Inthefuturewewouldliketoimprove thespeedof renderingwithphotondiffusion,andextenditto 
simulate light transport in a wider range of materials. 7 Acknowledgments This research was supported 
by CalIT2,theUCSDFWGrid Project (NSF EIA-0303622) and the National ScienceFoun­ dation (NSF 0305399). 
The ear model was provided by XYZRGB, and the hand model is courtesy MPII through theAIM@SHAPE ModelRepository. 
Thank you toToshiya Hachisuka for the bone geometry. References [CTW*04] CHEN Y., TONG X., WANG J., LIN 
S., GUO B.,SHUM H.-Y.: Shelltexture functions. ACMTrans. Graphic. 23 (2004), 343 353. [DEJ*99] DORSEY 
J., EDELMAN A., JENSEN H. W., LEGAKIS J.,PEDERSEN H.K.:Modelingand rendering C. Donner&#38;H.W.Jensen/RenderingTranslucent 
Materials Using Photon Diffusion Without bones With bones Figure 9: Standardsamplingmethodsfordiffusiondo 
not accountfor theinternaltransportoflightwithin the medium, onlythe transport between surface points.With 
photon diffusion, we obtaina .rst order approximationof visibilitywithinthe volume. Thisimageofa backlit 
hand with andwithout bones show the signi.canceof includinginternalgeometrythatblocklight.In the right 
image it is easy to see the bones in the hand due to the volumetric shadows. of weathered stone.In Proceedings 
of ACM SIGGRAPH 1999 (1999), pp. 225 234. [DJ05] DONNER C., JENSEN H. W.: Light diffusion in multi-layered 
translucent materials. ACMTrans.Graphic. 24,3(2005), 1032 1039. [DS03] DACHSBACHER C., STAMMINGER M.: 
Translu­cent shadow maps. In Rendering Techniques (2003), pp. 197 201. [EHR73] EGAN W. G.,HILGEMAN T. 
W., REICHMAN J.: Determination of absorption and scattering coef.cients for nonhomogeneousmedia. 2:Experiment. 
Appl.Opt. 12 (1973), 1816 1823. [FPW92] FARRELL T. J., PATTERSON M. S., WILSON  B.: A diffusion theory 
model of spatially resolved, steady­ statediffuse re.ectionsfor the noninvasive determination of tissue 
optical properties in vivo. Med. Phys. 19,4 (1992), 879 888. [HMBR05] HABER T., MERTENS T., BEKAERT 
P., REETH F.V.:Acomputational approachto simulate sub­ surfacelightdiffusionin arbitrarily shaped objects. 
In Proceedings of the 2005 conference on Graphics interface (2005), pp. 79 86. [Ish78] ISHIMARU A.: 
Wave Propagation and Scattering in Random Media. Oxford University Press, 1978. [JB02] JENSEN H.W.,BUHLER 
J.:Arapidhierarchical rendering techniquefortranslucent materials. ACM Trans. Graphic. 21 (2002), 576 
581. [JC98] JENSEN H. W., CHRISTENSEN P. H.: Ef.cient simulation of light transport in scences with 
participat­ing media using photon maps. In Proceedings ofACM SIGGRAPH 1998 (1998), pp. 311 320. &#38;#169; 
The Eurographics Association 2007. [Jen01] JENSEN H.W.: Realistic Image Synthesis Using Photon Mapping. 
AK Peters, 2001. [JLD99] JENSEN H. W., LEGAKIS J., DORSEY J.: Ren­ dering of wet materials. In Rendering 
Techniques (1999), pp. 273 282. [JMLH01] JENSEN H. W., MARSCHNER S. R., LEVOY M., HANRAHAN P.: A practical 
model for subsurface light transport.In ProceedingsofACMSIGGRAPH 2001 (2001), pp. 511 518. [Kie05] KIENLE 
A.: Light diffusion througha turbidpar­allelepiped. J. Opt. Soc. Am. 22,9(2005), 1883 1888. [LPT05] LI 
H., PELLACINI F., TORRANCE K.: Ahybrid monte carlo method for accurate and ef.cient subsurface scattering. 
In Rendering Techniques (2005), pp. 283 290. [LW96] LAFORTUNE E. P., WILLEMS Y.D.: Rendering participating 
media with bidirectional path tracing. In RenderingTechniques (1996), pp. 91 100. [MKB*03] MERTENST.,KAUTZJ.,BEKAERTP.,REETH 
F.V.,SEIDEL H.-P.: Ef.cient renderingoflocal subsur­ face scattering. InProceedingsofthe 11thPaci.c Con­ference 
onComputerGraphics and Applications (2003), pp. 51 58. [NRH*77] NICODEMUS F. E., RICHMOND J. C., HSIA 
J. J., GINSBERG I. W., LIMPERIS T.: Geometrical Con­ siderations and Nomenclaturefor Re.ectance. National 
Bureau of Standards, 1977. [PKK00] PAULY M.,KOLLIG T.,KELLER A.: Metropo­lislight transport for participatingmedia. 
In Rendering Techniques(2000), pp. 11 22. [Sta95] STAM J.: Multiple scattering asa diffusion process. 
In RenderingTechniques (1995), pp. 41 50.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation</funding_agency>
			<grant_numbers>
				<grant_number>NSF 0305399</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>UCSD FWGrid Project</funding_agency>
			<grant_numbers>
				<grant_number>NSF EIA-0303622</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
	<section>
		<section_id>1401139</section_id>
		<sort_key>70</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Advanced material appearance modeling]]></section_title>
		<section_page_from>3</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098604</person_id>
				<author_profile_id><![CDATA[81100255828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Holly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rushmeier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098605</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098606</person_id>
				<author_profile_id><![CDATA[81100402503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fran&#231;ois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sillion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401140</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>145</pages>
		<display_no>5</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Advanced material appearance modeling]]></title>
		<page_from>1</page_from>
		<page_to>145</page_to>
		<doi_number>10.1145/1401132.1401140</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401140</url>
		<abstract>
			<par><![CDATA[<p>For many years appearance models in computer graphics focused on general models for reflectance functions coupled with texture maps. Recently it has been recognized that even very common materials such as hair, skin, fabric, and rusting metal require more sophisticated models to appear realistic. We will begin by briefly reviewing basic reflectance models and the use of texture maps. We then describe common themes in advanced material models that include combining the effects of layers, groups of particles and/or fibers. We will survey the detailed models necessary needed to model materials such as (but not limited to) skin (including pigmentation, pores, subsurface scattering), plants (including internal structure) and automotive paints (including color flop and sparkle). We will then treat the modeling of complex appearance due to aging and weathering processes. A general taxonomy of effects will be presented, as well as methods to simulate and to capture these effects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098607</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098608</person_id>
				<author_profile_id><![CDATA[81100255828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Holly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rushmeier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098609</person_id>
				<author_profile_id><![CDATA[81100402503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fran&#231;ois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sillion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA/Grenoble Rh&#244;ne-Alpes Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882430</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Adabala <i>et al.</i>, 2003} Neeharika Adabala, Guangzheng Fei, and Nadia Magnenat-Thalmann. Visualization of woven cloth. In Philip Dutr&#233;, Frank Suykens, Per H. Christensen, and Daniel Cohen-Or, editors, <i>Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 178--185, Leuven, Belgium, 2003. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826602</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Aoki <i>et al.</i>, 2002} K. Aoki, Ngo Hai Dong, T. Kaneko, and S. Kuriyama. Physically-based simulation of cracks on drying 3d solid. In <i>10th Pacific Graphics Conference on Computer Graphics and Applications</i>, pages 467--468, Beijing China, Oct 2002. IEEE.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358644</ref_obj_id>
				<ref_obj_pid>358636</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Ashikhmin and Shirley, 2000} Michael Ashikhmin and Peter Shirley. An anisotropic Phong BRDF model. <i>Journal of Graphic Tools</i>, 5(2):25--32, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Baranoski and Rokne, 1997} G. V. G. Baranoski and J. G. Rokne. An algorithmic reflectance and transmittance model for plant tissue. <i>Computer Graphics Forum</i>, 16(3):141--150, August 1997. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Baranoski and Rokne, 2001} Gladimir V. G. Baranoski and Jon G. Rokne. Efficiently simulating scattering of light by leaves. <i>The Visual Computer</i>, 17(8):491--505, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Blazej <i>et al.</i>, 1989} A. A. Blazej, J. Galatik, Z. Galatik, Z. Krul, and M. Mladek. <i>Atlas of Microscopic Structures of Fur Skins 1</i>. Elsevier, New York, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Blinn, 1977} James F. Blinn. Models of light reflection for computer synthesized pictures. In <i>Proceedings of the 4th annual conference on Computer graphics and interactive techniques</i>, pages 192--198. ACM Press, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325249</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Bloomenthal, 1985} Jules Bloomenthal. Modeling the mighty maple. In <i>Proceedings of the 12th annual conference on Computer graphics and interactive techniques</i>, pages 305--311. ACM Press, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Boissieux <i>et al.</i>, 2000} L. Boissieux, G. Kiss, N. Magnenat-Thalmann, and P. Kalra. Simulation of skin aging and wrinkles with cosmetics insight. <i>Computer Animation and Simulation 2000</i>, pages 15--27, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Bosch <i>et al.</i>, 2004} C. Bosch, X. Pueyo, S. M&#233;rillou, and D. Ghazanfarpour. A physically-based model for rendering realistic scratches. <i>Computer Graphics Forum</i>, 23(3):361--370, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Buchanan, 1998} John W. Buchanan. Simulating wood using a voxel approach. <i>Computer Graphics Forum</i>, 17(3):105--112, 1998. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Chang and Shih, 2000} Yao-Xun Chang and Zen-Chung Shih. Physically-based patination for underground objects. <i>Computer Graphics Forum</i>, 19(3), August 2000. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Chang and Shih, 2003} Yao-Xun Chang and Zen-Chung Shih. The synthesis of rust in seawater. <i>The Visual Computer</i>, 19(1):50--66, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566628</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Chen <i>et al.</i>, 2002} Yanyun Chen, Yingqing Xu, Baining Guo, and Heung-Yeung Shum. Modeling and rendering of realistic feathers. In <i>Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, pages 630--636. ACM Press, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617946</ref_obj_id>
				<ref_obj_pid>616034</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{chi Hsu and tsin Wong, 1995} Siu chi Hsu and Tien tsin Wong. Simulating dust accumulation. <i>IEEE Comput. Graph. Appl.</i>, 15(1):18--22, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073221</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Chu and Tai, 2005} Nelson S.-H. Chu and Chiew-Lan Tai. Moxi: real-time ink dispersion in absorbent paper. <i>ACM Trans. Graph.</i>, 24(3):504--511, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{Cockshott <i>et al.</i>, 1992} T. Cockshott, J. Patterson, and D. England. Modelling the texture of paint. <i>Computer Graphics Forum</i>, 11(3):C217--C226, C476, ???? 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357293</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{Cook and Torrance, 1982} R. L. Cook and K. E. Torrance. A reflectance model for computer graphics. <i>ACM Transactions on Graphics</i>, 1(1):7--24, January 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258896</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{Curtis <i>et al.</i>, 1997} Cassidy J. Curtis, Sean E. Anderson, Joshua E. Seims, Kurt W. Fleischer, and David H. Salesin. Computer-generated watercolor. In <i>Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, pages 421--430. ACM Press/Addison-Wesley Publishing Co., 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{Dai <i>et al.</i>, 1995} Wen-Kai Dai, Zen-Chung Shih, and Ruei-Chuan Chang. Synthesizing feather textures in galliformes. <i>Computer Graphics Forum</i>, 14(3):407--420, August 1995. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Debevec <i>et al.</i>, 2000} Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring the reflectance field of a human face. In <i>Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 145--156. ACM Press/Addison-Wesley Publishing Co., 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Desbenoit <i>et al.</i>, 2004} Brett Desbenoit, Eric Galin, and Samir Akkouche. Simulating and modeling lichen growth. <i>Computer Graphics Forum</i>, 23(3):341--350, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237278</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Dorsey and Hanrahan, 1996} Julie Dorsey and Pat Hanrahan. Modeling and rendering of metallic patinas. In <i>Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</i>, pages 387--396. ACM Press, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311560</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Dorsey <i>et al.</i>, 1999} Julie Dorsey, Alan Edelman, Henrik Wann Jensen, Justin Legakis, and Hans Kohling Pedersen. Modeling and rendering of weathered stone. In <i>Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, pages 225--234. ACM Press/Addison-Wesley Publishing Co., 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Dumont-B&#232;cle <i>et al.</i>, 2001} P. Dumont-B&#232;cle, A. Kemeny, S. Michelin, and D. Arqu&#232;s. Multi-texturing approach for paint appearance simulation on virtual vehicles. <i>Proceedings of the Driving Simulation Conference 2001</i>, 213, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781008</ref_obj_id>
				<ref_obj_pid>780986</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{Eric Paquette, 2001} George Drettakis Eric Paquette, Pierre Poulin. Surface aging by impacts. In <i>Graphics Interface 2001</i>, pages 175--182, June 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{Ershov <i>et al.</i>, 2001} Sergey Ershov, Konstantin Kolchin, and Karol Myszkowski. Rendering pearlescent appearance based on paint-composition modelling. <i>Computer Graphics Forum</i>, 20(3), 2001. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{Fowler <i>et al.</i>, 1989} Deborah R. Fowler, James Hanan, and Przemyslaw Prusinkiewicz. Modelling spiral phyllotaxis. <i>Computers and Graphics</i>, 13(3):291--296, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134093</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Fowler <i>et al.</i>, 1992} Deborah R. Fowler, Przemyslaw Prusinkiewicz, and Johannes Battjes. A collisionbased model of spiral phyllotaxis. In <i>Proceedings of the 19th annual conference on Computer graphics and interactive techniques</i>, pages 361--368. ACM Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>965540</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Franzke and Deussen, 2003} Oliver Franzke and Oliver Deussen. Rendering plant leaves faithfully. In <i>Proceedings of the SIGGRAPH 2003 conference on Sketches &amp; applications</i>, pages 1--1. ACM Press, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053686</ref_obj_id>
				<ref_obj_pid>1053554</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{Fuchs <i>et al.</i>, 2005} Martin Fuchs, Hendrik Lensch, and Hans-Peter Seidel. Reflectance from images: A model-based approach for human faces. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 11(3):296--305, 2005. Member-Volker Blanz.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1162438</ref_obj_id>
				<ref_obj_pid>1162435</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{Fuhrer <i>et al.</i>, } Martin Fuhrer, Henrik Wann Jensen, and Przemyslaw Prusinkiewicz. Modeling hairy plants. <i>Graphical Models</i>, 68(4), July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882438</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Georghiades, 2003} Athinodoros S. Georghiades. Recovering 3-d shape and reflectance from a small number of photographs. In <i>EGRW '03: Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 230--240, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>883422</ref_obj_id>
				<ref_obj_pid>882473</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Gobron and Chiba, 2001a} S. Gobron and N. Chiba. Simulation of peeling using 3d-surface cellular automata. In <i>9th Pacific Graphics Conference on Computer Graphics and Applications</i>, pages 338--347, Tokyo Japan, Oct 2001. IEEE.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{Gobron and Chiba, 2001b} St&#233;phane Gobron and Norishige Chiba. Crack pattern simulation based on 3d surface cellular automata. <i>The Visual Computer</i>, 17(5):287--309, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614315</ref_obj_id>
				<ref_obj_pid>614260</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Groeller <i>et al.</i>, 1995} Eduard Groeller, Rene T. Rau, and Wolfgang Strasser. Modeling and visualization of knitwear. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 1(4):302--310, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141952</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{Gu <i>et al.</i>, 2006} Jinwei Gu, Chien-I Tu, Ravi Ramamoorthi, Peter Belhumeur, Wojciech Matusik, and Shree Nayar. Time-varying surface appearance: acquisition, modeling and rendering. <i>ACM Trans. Graph.</i>, 25(3):762--771, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{G&#252;nther <i>et al.</i>, 2005} Johannes G&#252;nther, Tongbo Chen, Michael Goesele, Ingo Wald, and Hans-Peter Seidel. Efficient acquisition and realistic rendering of car paint. In G&#252;nther Greiner, Joachim Hornegger, Heinrich Niemann, and Marc Stamminger, editors, <i>Proceedings of 10th International Fall Workshop - Vision, Modeling, and Visualization (VMV) 2005</i>, pages 487--494. Akademische Verlagsgesellschaft Aka GmbH, November 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015708</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{Guy and Soler, 2004} Stephane Guy and Cyril Soler. Graphics gems revisited: fast and physically-based rendering of gemstones. <i>ACM Trans. Graph.</i>, 23(3):231--238, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{Hanrahan and Krueger, 1993} Pat Hanrahan and Wolfgang Krueger. Reflection from layered surfaces due to subsurface scattering. In <i>Proceedings of the 20th annual conference on Computer graphics and interactive techniques</i>, pages 165--174. ACM Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1185719</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{Hiebert <i>et al.</i>, 2006} Brad Hiebert, Jubin Dave, Tae-Yong Kim, Ivan Neulander, Hans Rijpkema, and Will Telford. The chronicles of Narnia: the lion, the crowds and rhythm and hues. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Courses</i>, page 1, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{Hirota <i>et al.</i>, 1998} Koichi Hirota, Yasuyuki Tanoue, and Toyohisa Kaneko. Generation of crack patterns with a physical model. <i>The Visual Computer</i>, 14(3):126--137, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Hirota <i>et al.</i>, 2000} Koichi Hirota, Yasuyuki Tanoue, and Toyohisa Kaneko. Simulation of three-dimensional cracks. <i>The Visual Computer</i>, 16(7):371--378, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383840</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Jensen <i>et al.</i>, 1999} Henrik Wann Jensen, Justin Legakis, and Julie Dorsey. Rendering of wet material. In Dani Lischinski and Greg Ward Larson, editors, <i>Rendering Techniques '99</i>, Eurographics, pages 273--282. Springer-Verlag Wien New York, 1999. Proc. 10th Eurographics Rendering Workshop, Granada, Spain, June 21--23, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{Jensen <i>et al.</i>, 2001} Henrik Wann Jensen, Stephen R. Marschner, Marc Levoy, and Pat Hanrahan. A practical model for subsurface light transport. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 511--518. ACM Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{Kajiya and Kay, 1989} J. T. Kajiya and T. L. Kay. Rendering fur with three dimensional textures. In <i>Proceedings of the 16th annual conference on Computer graphics and interactive techniques</i>, pages 271--280. ACM Press, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>959207</ref_obj_id>
				<ref_obj_pid>959196</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{Koenderink and Pont, 2003} J. Koenderink and S. Pont. The secret of velvety skin. <i>Machine Vision and Applications</i>, 14(4):260--268, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{Koudelka, 2004} M. L. Koudelka. <i>Capture, analysis and synthesis of textured surfaces with variation in illumination, viewpoint, and time</i>. Yale University New Haven, CT, USA, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{Krishnaswamy and Baranoski, 2004} A. Krishnaswamy and G. V. G. Baranoski. A Biophysically-Based Spectral Model of Light Interaction with Human Skin. <i>Computer Graphics Forum</i>, 23(3):331--340, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{Lafortune <i>et al.</i>, 1997} Eric P. F. Lafortune, Sing-Choong Foo, Kenneth E. Torrance, and Donald P. Greenberg. Non-linear approximation of reflectance functions. In <i>Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, pages 117--126. ACM Press/Addison-Wesley Publishing Co., 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{Lam and Baranoski, 2006} Michael W. Y. Lam and Gladimir V. G. Baranoski. A predictive light transport model for the human iris. <i>Computer Graphics Forum</i>, 25(3):359--368, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581911</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{Lefebvre and Neyret, 2002} Sylvain Lefebvre and Fabrice Neyret. Synthesizing bark. In P. Debevec and S. Gibson, editors, <i>13th Eurographics Workshop on Rendering</i>, Pisa, Italy, 2002. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{Lefebvre and Poulin, 2000} Laurent Lefebvre and Pierre Poulin. Analysis and synthesis of structural textures. In <i>Graphics Interface 2000</i>, pages 77--86, May 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>950715</ref_obj_id>
				<ref_obj_pid>950627</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{Lefohn <i>et al.</i>, 2003} Aaron Lefohn, Brian Budge, Peter Shirley, Richard Caruso, and Erik Reinhard. An ocularist's approach to human iris synthesis. <i>IEEE Comput Graphics Appl</i>, 23(6):70--75, November/December 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{Lokovic and Veach, 2000} Tom Lokovic and Eric Veach. Deep shadow maps. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 385--392, New York, NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2381358</ref_obj_id>
				<ref_obj_pid>2381356</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{Lu <i>et al.</i>, 2005} Jianye Lu, Athinodoros S. Georghiades, Holly Rushmeier, Julie Dorsey, and Chen Xu. Synthesis of material drying history: Phenomenon modeling, transferring and rendering. In <i>proceedings of Eurographics Workshop on Natural Phenomena</i>, pages 7--16, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1189765</ref_obj_id>
				<ref_obj_pid>1189762</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{Lu <i>et al.</i>, 2007} Jianye Lu, Athinodoros S. Georghiades, Andreas Glaser, Hongzhi Wu, Li-Yi Wei, Baining Guo, Julie Dorsey, and Holly Rushmeier. Context-aware textures. <i>ACM Trans. Graph.</i>, 26(1):3, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732140</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{Marschner <i>et al.</i>, 2000} Stephen R. Marschner, Brian K. Guenter, and Sashi Raghupathy. Modeling and rendering for realistic facial animation. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques 2000</i>, pages 231--242, London, UK, 2000. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{Marschner <i>et al.</i>, 2003} Stephen R. Marschner, Henrik Wann Jensen, Mike Cammarano, Steve Worley, and Pat Hanrahan. Light scattering from human hair fibers. <i>ACM Trans. Graph.</i>, 22(3):780--791, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073254</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{Marschner <i>et al.</i>, 2005} Stephen R. Marschner, Stephen H. Westin, Adam Arbree, and Jonathan T. Moon. Measuring and modeling the appearance of finished wood. <i>ACM Trans. Graph.</i>, 24(3):727--734, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614469</ref_obj_id>
				<ref_obj_pid>614280</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{Merillou <i>et al.</i>, 2000} S. Merillou, J.-M. Dischler, and D. Ghazanfarpour. A BRDF postprocess to integrate porosity on rendered surface. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 6(4):306--318, October 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{Merillou <i>et al.</i>, 2001a} S. Merillou, J. M. Dischler, and D. Ghazanfarpour. Surface scratches: measuring, modeling and rendering. <i>The Visual Computer</i>, 17(1):30--45, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781007</ref_obj_id>
				<ref_obj_pid>780986</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{Merillou <i>et al.</i>, 2001b} Stephane Merillou, Jean-Michel Dischler, and Djamchid Ghazanfarpour. Corrosion: Simulating and rendering. In <i>GI 2001</i>, pages 167--174, June 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141995</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{Moon and Marschner, 2006} Jonathan T. Moon and Stephen R. Marschner. Simulating multiple scattering in hair using a photon mapping approach. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Papers</i>, pages 1067--1074, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614382</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{Nagata <i>et al.</i>, 1997} Noriko Nagata, Toshimasa Dobashi, Yoshitsugu Manabe, Teruo Usami, and Seiji Inokuchi. Modeling and Visualization for a Pearl-Quality Evaluation Simulator. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 3(4):307--315, October 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97922</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{Nakamae <i>et al.</i>, 1990} E. Nakamae, K. Kaneda, T. Okamoto, and T. Nishita. A lighting model aiming at drive simulators. In <i>Proceedings of Siggraph 1990</i>, pages 395--404. ACM SIGGRAPH, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{Nishita <i>et al.</i>, 1997} T. Nishita, H. Iwasaki, Y. Dobashi, and E. Nakamae. A modeling and rendering method for snow by using metaballs. <i>Computer Graphics Forum</i>, 16(3):357--364, August 1997. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192213</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{Oren and Nayar, 1994} Michael Oren and Shree K. Nayar. Generalization of lambert's reflectance model. In <i>Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, pages 239--246. ACM Press, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{Paquette <i>et al.</i>, 2002} Eric Paquette, Pierre Poulin, and George Drettakis. The simulation of paint cracking and peeling. In <i>Graphics Interface 2002</i>, pages 59--68, May 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{Phong, 1975} Bui Tuong Phong. Illumination for computer generated pictures. <i>Commun. ACM</i>, 18(6):311--317, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378503</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{Prusinkiewicz <i>et al.</i>, 1988} Przemyslaw Prusinkiewicz, Aristid Lindenmayer, and James Hanan. Development models of herbaceous plants for computer imagery purposes. In <i>SIGGRAPH '88: Proceedings of the 15th annual conference on Computer graphics and interactive techniques</i>, pages 141--150, New York, NY, USA, 1988. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{Robertson, 1999} James Robertson. <i>Forensic Examination of Human Hair</i>. CRC Press, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{Rubin, 1998} Barry Rubin. Tailored Fiber Cross Sections. <i>Advanced Materials</i>, 10(15):1225--1227, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{Shimizu <i>et al.</i>, 2003} Clement Shimizu, Gary W. Meyer, and Joseph P. Wingard. Interactive goniochromatic color design. In <i>Eleventh Color Imaging Conference</i>, pages 16--22, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732287</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{Stam, 2001} Jos Stam. An illumination model for a skin layer bounded by rough surfaces. In <i>Proceedings of the 12th Eurographics Workshop on Rendering Techniques</i>, pages 39--52, London, UK, 2001. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{Streit and Heidrich, 2002} L. Streit and W. Heidrich. A biologically-parameterized feather model. <i>Computer Graphics Forum</i>, 21(3):565--565, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{Sun <i>et al.</i>, 2001} Yinlong Sun, F. David Fracchia, Mark S. Drew, and Thomas W. Calvert. A spectrally based framework for realistic image synthesis. <i>The Visual Computer</i>, 17(7):429--444, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1122506</ref_obj_id>
				<ref_obj_pid>1122501</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{Sun, 2006} Yinlong Sun. Rendering biological iridescences with rgb-based renderers. <i>ACM Trans. Graph.</i>, 25(1):100--129, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97908</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{Takagi <i>et al.</i>, 1990} Atsushi Takagi, Hitoshi Takaoka, Tetsuya Oshima, and Yoshinori Ogata. Accurate rendering technique based on colorimetric conception. In <i>Proceedings of the 17th annual conference on Computer graphics and interactive techniques</i>, pages 263--272. ACM Press, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{Vogelmann, 1993} C. Vogelmann. Plant tissue optics. <i>Annual review of plant physiol. plant mol. biol.</i>, 44:231--251, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{Walters, 2002} Kenneth A. Walters. <i>Dermatological and Transdermal Formulations</i>. Marcel Dekker Incorporated, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073252</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{Wang <i>et al.</i>, 2005} Lifeng Wang, Wenle Wang, Julie Dorsey, Xu Yang, Baining Guo, and Heung-Yeung Shum. Real-time rendering of plant leaves. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Papers</i>, pages 712--719, New York, NY, USA, 2005. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141951</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{Wang <i>et al.</i>, 2006} Jiaping Wang, Xin Tong, Stephen Lin, Minghao Pan, Chao Wang, Hujun Bao, Baining Guo, and Heung-Yeung Shum. Appearance manifolds for modeling time-variant appearance of materials. <i>ACM Trans. Graph.</i>, 25(3):754--761, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{Ward, 1992} Gregory J. Ward. Measuring and modeling anisotropic reflection. In <i>Proceedings of the 19th annual conference on Computer graphics and interactive techniques</i>, pages 265--272. ACM Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141987</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{Weyrich <i>et al.</i>, 2006} Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd Bickel, Craig Donner, Chien Tu, Janet McAndless, Jinho Lee, Addy Ngan, Henrik Wann Jensen, and Markus Gross. Analysis of human faces using a measurement-based skin reflectance model. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Papers</i>, pages 1013--1024, New York, NY, USA, 2006. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383303</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{Xu <i>et al.</i>, 2001} Ying-Qing Xu, Yanyun Chen, Stephen Lin, Hua Zhong, Enhua Wu, Baining Guo, and Heung-Yeung Shum. Photorealistic rendering of knitwear using the lumislice. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 391--398. ACM Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{Yuan <i>et al.</i>, 1988} Ying Yuan, Tosiyasu L. Kunii, Naota Inamato, and Lining Sun. Gemstone fire: Adaptive dispersive ray tracing of polyhedrons. <i>The Visual Computer</i>, 4(5):259--70, November 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263273</ref_obj_id>
				<ref_obj_pid>1263129</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{Zinke and Weber, 2007} Arno Zinke and Andreas Weber. Light scattering from filaments. <i>IEEE Trans. on Vis. and Comp. Graphics</i>, 13(2):342--356, March/April 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2008 Class Advanced Material Appearance Modeling  Julie Dorsey Holly Rushmeier Yale University 
 { julie.dorsey, holly.rushmeier (at) yale.edu} François Sillion INRIA/Grenoble Rhône-Alpes Research 
Center {francois.sillion (at) imag.fr}  Table of Contents Course Description   3 Annotated Slides 
   6 Bibliography    139 Supplements: Input for Participating Media  ..146 Holly Rushmeier, from 
SIGGRAPH 1995 Course Notes The Perception of Simulated Materials  .170 Holly Rushmeier, Proceedings 
of Human Vision and Electronic Imaging XIII, (San Jose, CA) January 2008. Light and Materials in Virtual 
Cities  ...182 Julie Dorsey and Holly Rushmeier IEEE Virtual Reality Workshop on Virtual Cityscapes 
(Reno, NV), March 2008 Class Description For many years appearance models in computer graphics focused 
on general models for reflectance functions coupled with texture maps. Recently it has been recognized 
that even very common materials such as hair, skin, fabric, and rusting metal require more sophisticated 
models to appear realistic. We will begin by briefly reviewing basic reflectance models and the use of 
texture maps. We then describe common themes in advanced material models that include combining the effects 
of layers, groups of particles and/or fibers. We will survey the detailed models necessary needed to 
model materials such as (but not limited to) skin (including pigmentation, pores, subsurface scattering), 
plants (including internal structure) and automotive paints (including color flop and sparkle). We will 
then treat the modeling of complex appearance due to aging and weathering processes. A general taxonomy 
of effects will be presented, as well as methods to simulate and to capture these effects. Prerequisites 
 Knowledge of basic rendering and reflectance functions. Syllabus/Approximate Schedule  Introduction: 
15 min.  Background: 40 min.  Specialized Material Models: Common Themes: 20 min Natural Materials: 
35 min. Manufactured/Processed Materials: 30 min.  Aging and Weathering Processes: Taxonomy: 10 min. 
Simulation: 40 min. Capture Approaches: 15 min.  Future Trends and Resources 20 min.    Speakers 
 Julie Dorsey is a Professor of Computer Science at Yale University, where she teaches computer graphics. 
She came to Yale in 2002 from MIT, where she held tenured appointments in both the Department of Electrical 
Engineering and Computer Science (EECS) and the School of Architecture. She received undergraduate degrees 
in Architecture and graduate degrees in Computer Science from Cornell University. With architecture as 
a driving application, she has studied a wide range problems in computer graphics, including sketch-based 
interfaces for early conceptual design, acceleration methods for real-time rendering, and the creation 
of detailed photorealistic renderings. Her contributions also include algorithms for lighting and acoustical 
design and visualization. She is particularly well known for her research in modeling the appearance 
of materials -- for example, she pioneered techniques to model the visual richness of irregular metal 
patinas and eroded stone. Her current research interests include photorealistic image synthesis, material 
and texture models, illustration techniques, and interactive visualization of complex scenes, with an 
application to urban environments. In addition to serving on numerous conference program committees, 
she has served as an associate editor for IEEE Transactions on Visualization and Computer Graphics and 
The Visual Computer, and was Papers Chair for ACM SIGGRAPH 2006. She has received several professional 
awards, including MIT's Edgerton Faculty Achievement Award, a National Science Foundation Career Award, 
and an Alfred P. Sloan Foundation Research Fellowship. Holly Rushmeier received the BS, MS and PhD degrees 
in Mechanical Engineering from Cornell University in 1977, 1986 and 1988 respectively. Between receiving 
the BS and returning to graduate school in 1983 she worked as an engineer at the Boeing Commercial Airplane 
Company and at Washington Natural Gas Company (now a part of Puget Sound Energy). In 1988 she joined 
the Mechanical Engineering faculty at Georgia Tech. While there she conducted sponsored research in the 
area of computer graphics image synthesis and taught classes heat transfer and numerical methods at both 
the undergraduate and graduate levels. At the end of 1991 Dr. Rushmeier joined the computing and mathematics 
staff of the National Institute of Standards and Technology, focusing on scientific data visualization. 
From 1996 to early 2004 Dr. Rushmeier was a research staff member at the IBM T.J. Watson Research Center. 
At IBM she worked on a variety of data visualization problems in applications ranging from engineering 
to finance. She also worked in the area of acquisition of data required for generating realistic computer 
graphics models, including a project to create a digital model of Michelangelo's Florence Pieta, and 
the development of a scanning system to capture shape and appearance data for presenting Egyptian cultural 
artifacts on the World Wide Web.  Dr. Rushmeier was Editor-in-Chief of ACM Transactions on Graphics 
from 1996­ 99. She has also served on the editorial board of IEEE Transactions on Visualization and Computer 
Graphics. She is currently on the editorial boards of Computer Graphics Forum, IEEE Computer Graphics 
and Application, ACM Journal of Computing and Cultural Heritage and ACM Transactions on Applied Perception. 
In 1996 she served as the papers chair for the ACM SIGGRAPH conference, in 1998,2004 and 2005 as the 
papers co-chair for the IEEE Visualization conference and in 2000 as the papers co-chair for the Eurographics 
Rendering Workshop. She has also served in numerous program committees including multiple years on the 
committees for SIGGRAPH, IEEE Visualization, Eurographics, Eurographics Rendering Workshop, and Graphics 
Interface. François Sillion is a senior researcher with INRIA (Institut National de Recherche en Informatique 
et Automatique), and director of INRIA s Grenoble-Rhône-Alpes research center. He obtained a masters 
degree in Solid State Physics and a PhD in Computer Science as a student of the Ecole Normale Supérieure 
in Paris. He has held positions at Cornell University, CNRS and Ecole Polytechnique, and headed the ARTIS 
research project at INRIA. Sillion published many papers in the fields of lighting simulation, real-time 
rendering, visibility and shadow techniques, focusing on the complementary issues of image realism and 
interactive control of visualization. He wrote a comprehensive book on the radiosity method, with Claude 
Puech (Radiosity &#38; Global Illumination, Morgan Kaufmann Publishers, 1994). François Sillion currently 
chairs the Eurographics working group on rendering, and is a member of the editorial board of the Computer 
Graphics Forum and ACM Transactions on Graphics journals. He co-chaired the technical program of the 
Eurographics'96 conference, the 1992 Eurographics workshop on rendering, and organized the 1993 Eurographics 
workshop on rendering. He delivered tutorials in several SIGGRAPH and Eurographics conferences, and invited 
presentations in several international conferences. In 2005 he was elected a fellow of the Eurographics 
Association. Additional information The speakers are the authors of Digital Modeling of Material Appearance 
published by Morgan Kaufmann/Elsevier. Further information about the speakers can be found at http://graphics.cs.yale.edu., 
and at http://artis.imag.fr/~Francois.Sillion/  In these course notes we present principles of defining 
numerical models to be used in rendering realistic imagery of physical materials. Additional information 
can be found at http://graphics.cs.yale.edu/. The in person presentation of this course varies from these 
notes in the interest of timeliness, and considering the fact that fair use materials can not be posted 
for distribution to non-course attendees. These notes build on introductory information on material modeling 
that was presented in a SIGGRAPH course in 2005 and 2006. Notes and presentations from these courses 
can be accessed by ACM Digital Library subscribers. These notes also draw on the text Digital Modeling 
of Material Appearance (Morgan-Kaufmann/Elsevier. The materials here are rendered with models. An artist 
conceived the shape. A purely artistic approach could be used to digitally paint the shades of light 
and dark on the digital shapes to give the illusion of translucent stone or copper metal. However, to 
generate these images material models are expressed numerically and rendered using lighting simulations. 
That is their appearance the colors, shades of light and dark, were computed, rather than being digitally 
painted on the model.  We define a model as taking a physically measurable input and producing a predictive 
output that can be verified by physical measurement. A model of a material makes possible the reliable 
rendering of the appearance of that material in any geometric and lighting conditions. An artistic technique 
as takes an input which is not necessarily measurable, and produces an output that may or may not reproduce 
the appearance of an object under arbitrary circumstances. Human judgment is required to use an artistic 
technique, and to evaluate its success. Our goal is to make predictive images that give a view of a 
scene or object that is the same as if the person were viewing it directly. Material modeling is one 
aspect of this. We need to consider the object s shape, and the light incident on it.  Shape is the 
large scale form or geometry of the object. The shape is needed to place the image of the object correctly 
with respect to other objects in the scene, to determine which other objects are occluded by the object, 
and what areas are cast into shadow by the object. Fine scale geometric variations in the object we 
define as part of the object s material from the point of view of creating digital models in computer 
graphics. For a close view of a tree branch, a leaf is defined by a flat shape, with the number of lobes 
or points depending on the type of tree. In an aerial photograph, a leaf is a small facet in a tree canopy 
material that covers the terrain. Many methods can be used to represent shape. The area of computer­aided 
geometry is devoted to the study of shape representation, and extensive descriptions of representations 
such as NURBs (non-uniformrational B-splines), triangle meshes, subdivision surfaces and implicit surface 
are documented at length in references such as Farin Curves and Surfaces for Computer-Aided Geometric 
Design: A Practical Code. Academic Press, Inc., 1996. Many methods can be used to compute the interreflections 
of light between objects in an environment. These methods, referred to as global illumination methods, 
include ray tracing, radiosity, photon mapping and hybrids of these various approaches. Thorough discussions 
of these methods can be found in Dutre, Bekaert and Bala, Advanced Global Illumination. AK Peters Limited, 
Wellesley, MA, 2003. For rendering appearance, the essential feature of a  global illumination method 
is that for a given ray direction the quantity of light from that direction at a particular point can 
be efficiently computed. An environment consists of a set of objects, each defined by a shape and material 
description, and at least one light source. An infinite number of images could be created of such an 
environment, and to specify a particular image a viewpoint, view direction and view frustum (i.e. field 
of view) need to be specified. The image is formed by projecting the objects in the environment seen 
through the frustum onto an image plane that spans the field of view and is perpendicular to the view 
direction. In a digital image, the image is discretized into pixels, and the display values for that 
pixel are set by determining the light that would arrive at the viewer from the object visible through 
that pixel. There are three important components of a material model that allow us to recognize a material 
 spectral, directional and spatial. We notice the color of an object (resulting from the spectral composition 
of light), its directionality (hazy, glossy, shiny,) and small spatial variations (textures formed by 
light and dark, or little bumps.) Example of introducing spectral and directional variations   Introducing 
spatial variations  Spatially varying spectral and directional variations to make this look like a 
worn,dirty metallic object. Examples of directionality beyond directional reflectance The most familiar 
and basic light scattering is regular or mirror-like reflection, as shown in the photo at the top. Light 
rays reflect into one single direction, and that direction forms the same angle to the surface normal 
as the incident direction, as shown on the  lower left. Because the reflected rays stay organized as 
they were when they left the previous objects, a sharp image is formed just as though you were looking 
directly at the objects. This regular, or mirror-like reflection is referred to as pure or ideal specular 
reflection. Many materials are shiny or glossy, but not purely specular. In these materials, incident 
beams of light are distributed into a cone or lobe of directions centered around the specular, or mirror 
direction. The result of this is when you are looking at such materials the light reflected from each 
point of the surface includes light from an a range of surfaces in the environment, instead of just reflecting 
one point. Instead of seeing sharp edges reflected, everything looks blurred. By observing the reflections 
in the paint can in the image, you can see that how blurred things look depends on how close the objects 
being reflected are to the glossy surface. If they are relatively close, the cross section of the cone 
from which a point is reflecting light is relatively small, and lines like that between the yellow and 
blue surfaces above are only blurred a bit. As the objects get further away, the cross section of the 
cone becomes large, and can include entire objects which then do not appear with any detail when reflected 
in the glossy surface.  Objects that appear to have the same pattern of light and dark regardless of 
how you view them (as long as you don t block a significant source of light from the environment as you 
move to another view) are diffuse. An ideal diffuse (also referred to as Lambertian) object reflects 
an incident beam of light as light rays of much lower magnitude in all directions. The light coming from 
any point on the object in any direction is a product of light coming from many different sources in 
the environment. The contribution of each source in the environment varies very slowly from point to 
point on the object, so the amount of light varies slowly from point to point, and In addition to the 
reflectance that depends on material microstructure and chemical composition, the appearance depends 
on small scale geometric structure. Just as some materials are characterized primarily by the spatial 
variations in reflectance, other materials are characterized primarily by their small scale geometric 
structure. Small is defined as orders of magnitude smaller than the overall object. The image above shows 
a piece of plastic with a pattern pressed into it that changes the surface from smooth to bumpy. The 
small scale geometric structure shown here is characteristic of leather material, and this fact is used 
in the production of physical materials to make a plastic look like leather. The variation of light and 
dark in the image of the plastic is not due to spatial changes in reflectance, but to the change of surface 
orientation caused by the small scale geometry. Even small indentations can cause large changes in the 
surface normal. The surface normal, rather than the absolute surface position, determines in which direction 
incident light will be reflected.  Some materials don t just reflect light from the surface, or just 
transmit the light. In some cases light penetrates the material and scatters in the interior. This is 
referred to as subsurface scattering, and can occur in dielectrics, not metals. Under normal room illumination, 
surfaces which allow subsurface scattering often do not look dramatically different from completely opaque 
surfaces. The image on the right though shows an extreme example of illumination. A green laser is directed 
at the top of a small piece of stone. Besides being reflected from the top, the light is scattered in 
the material and is visible emerging from the sides of the stone. Terminology and Mathematical Descriptions 
Key quantities: Radiance L Bidirectional Reflectance Distribution Function (BRDF) fr : An explanation 
of the mathematics of light transport isn t possible in a brief lecture. However, a couple of key points 
are: -- a lot of the notation in light transport is just denoting that quantities vary with color (spectral 
 dependance .), direction (given by angles . and f) and position (x,y) -- there are two quantities that 
are key, but which take some getting used to . One is the quantity of light we want to compute, the radiance 
L. The other is the function telling how a surface scatters light, the BRDF fr. Components of Material 
Model Spectral .: wavelength (color) dependence Directional .,f : direction (Shiny, matte, glossy, hazy) 
Spatial variation x,y: position (texture)   The key quantity we use to define how a surface redirects 
light is the BRDF, which relates incident and reflected radiance for two given directions. The BRDF is 
a distribution function, not a fraction from zero to one. It can take on values from zero to infinity. 
To conserve energy, the integral of the BRDF over all reflected directions must be less then or equal 
to one. Many common reflectance models are named, generally after the people who developed the models. 
NOTE: There are no compliance standards for claiming that a named model is being used, so you can t be 
absolutely sure that giving the same parameters to a particular model in one software package will produce 
the same results in another package.  The directionality of transmission from a smooth surface is a 
bit more complicated that reflection. First, most metals have a high tendency to absorb electromagnetic 
energy, so transmission of visible light is not observed. For dielectrics, the change in the speed of 
light in the material causes a change in the direction. This change in direction is called refraction, 
and is expressed by Snell s Law as shown above. Unlike the direction of reflection, the direction of 
refraction depends on the properties of the materials. Since light is electromagnetic energy, its interaction 
is governed by the properties that quantify the material s interaction with electric and magnetic fields. 
In the solution to Maxwell s equations these properties are expressed as the index of refraction n and 
a coefficient that captures the tendency to absorb electomagnetic waves k . The value of n is the ratio 
of the speed of light in a vacuum to the speed of light in the material. The value of k is zero for dielectrics, 
which do not conduct electricity, and greater than zero for metals, which do. Values of k and n are found 
by measurement and can be looked up in handbooks or online resources. Generally understanding and applying 
the results of the smooth surface solution requires only knowing some rough estimates of typical values 
of these constants for common materials.  In addition to giving directionality, the fraction of light 
reflected can also be calculated from the solution of Maxwell s equations, and the results are referred 
to as the Fresnel equations. For a dielectric, the light that is not reflected from the surface is transmitted. 
For a metal, the light that is not reflected is absorbed. The Fresnel equations give complicated algebraic 
expressions for reflectance, but only straightforward number crunching is needed to evaluate given values 
of . , n and k . Since metals have a high reflectance for all angles, the Fresnel effect is less pronounced. 
Although it is rarely included in visual simulations, metals all tend to look white or gray at grazing 
angles.  Lambertian, or ideal diffuse reflectance is in a sense the opposite of specular reflection. 
Instead of all light being reflected in a single direction, it is reflected in all directions with the 
same radiance. Unlike specular reflection, this is not the result of solving Maxwell s equations for 
some particular surface configuration. It is an approximation of the observed behavior of many materials. 
While real materials usually deviate from Lambertian for angles of view or incidence greater than 60 
degrees, the Lambertian model is used for its computational simplicity. For measurement purposes, some 
materials have been designed that are very close to being to Lambertian, such as Spectralon® from Labsphere 
Inc. Materials can be modeled as a combination of Lambertian and mirror-like reflectance. The material 
can also have spectral values that vary with position. Here a scanned object is shown as white Lambertian 
(upper left), spectrally varying with position (upper right), with mirror-like reflection of the light 
source (lower left), and with mirror-like reflection of the entire environment.   As noted in R. L. 
Cook and K. E. Torrance. A reflectance model for computer graphics. ACM Transactions on Graphics, 1(1):7 
24, January 1982. The color of specularly reflected light is white for dielectics, and the color of the 
material for metals. The color is predicted using the Fresnel equation for a smooth surface. The original 
Phong reflectance model is described in the classic paper: Bui Tuong Phong Illumination for computer 
generated pictures Communications of the ACM, v.18 n.6, p.311-317, June 1975 . It was expressed as reflectance 
function for light intensity, rather than as a BRDF for computing radiance. However, it was inspired 
by physical observation. The effect of the model in rendering a sphere is compared to a photograph of 
a real sphere in the paper. The fuzziness of the specular reflection  is computed as a function of the 
angle a between the reflected direction and the mirror reflection angle: reflectance = .d (cos ..) + 
.s (cos .s)n In contrast to diffuse reflection, the specular component concentrates the reflected light. 
The larger the value of n, the smaller the specular highlights formed by the reflection of the light 
source. The specular lobe in the Phong model is taking into account roughness at a very small scale. 
At a small scale parts of a surface are oriented to reflect into directions that aren t the mirror direction 
for the flat surface. H is the half way vector, the direction a surface normal would need to be pointing 
for a mirror reflection to be visible for a given pair of light L and view V directions. Many reflectance 
models are computed in terms of this half way vector. Blinn-Phong (using .h instead of .s)  Increasing 
p .d+ .s cosp .h  The specular lobe in the Phong model is taking into account roughness at a very small 
scale. At a small scale parts of a surface are oriented to reflect into directions that aren t the mirror 
direction for the flat surface. H is the half way vector, the direction a surface normal would need 
to be pointing for a mirror reflection to be visible for a given pair of light L and view V directions. 
Many reflectance models are computed in terms of this half way vector.  These image show a macroscopic 
example of the spreading effect of a rough surface. For a surface that is somewhat rough at a microscopic 
level, some portions of the surface are oriented in the direction of the halfway vector even when the 
halfway vector isn t the same as the main surface normal. The Ward reflectance model is similar to the 
Phong model except it is expressed in physical terms it expresses the relationship between incident and 
reflectance radiance and conserves energy. Rather than using the cosine to a power, it uses an exponential 
function, parameterized by an average slope, to express the shape of the specular lobe. Furthermore, 
the lobe can be anisotropic by expressing different slopes for different directions on a surface (e.g. 
for a set of grooves the slope is zero along the grooves, and potentially steep perpendicular to the 
grooves).The model can be applied to regular and diffuse transmission through a thin surface. The model 
is fully described in as described in Ward Larson and Shakespeare, Rendering with radiance: the art and 
science of lighting visualization (Morgan Kaufmann, 1998) Since the Ward model is developed in physical 
terms of incident and reflected radiance, it works (by design) in a system that simulates physically 
accurate global illumination. These variations were rendered using the Radiance software system,  http://radsite.lbl.gov/ 
A point to remember is that physically accurate material models only create realistic appearance when 
used in the context of a physically accurate global illumination system. Another detail to note is that 
a small correction to the original model is available in Arne Duer. An Improved Normalization For The 
Ward Reflectance Model. JOURNAL OF GRAPHICS TOOLS, 11(1):51, 2006. Anisotropic reflection has a significant 
impact on appearance, but for a complicated object its effect is only clear when the effect of isotropic, 
or anisotropic reflection with a different orientation is displayed.  The generalized cosine lobe model 
described in Lafortune, Foo,Torrance, and Greenberg Non-linear approximation of reflectance functions 
In Proceedings of the 24th annual conference on Computer graphics and interactive techniques (1997pp. 
117 126.) Gives a different generalization of the Phong model. Like the Ward model, it is formulated 
in physical terms. It conserves energy. Instead of just describing peaks of reflection around the specular 
direction, it allows the definition of lobes (possibly anisotropic) around any axis defined with respect 
to the surface. Important other axes are just off the specular direction, the normal direction and the 
direction of the source (for backscatter). The general form of the reflectance is fr = C(u) (Cxuxvx+Cyuyvy+Czuzvz)n 
where u and v are vectors in the incident and reflected directions, Cx,Cy are coefficients determining 
the direction and shape of the lobe, n defines how narrow it is, and C(u) is a normalizing function to 
insure the function conserves energy. Sets of functions of An example of a BRDF that the Lafortune model 
can represent that previous models could not is generalized diffuse reflectance. In general, even surfaces 
that appear matte or diffuse don t reflect radiance evenly in all directions the reflection may peak 
in the direction of the surface normal and fall off at near grazing viewing angles. The effects shown 
here are found using Cx=Cy=0, Cz=1, n equal to zero, 0.5 and 2 respectively.  The Lafortune model, 
unlike Phong or Ward, also provides a mechanism for defining back scatter. In this case a sum of two 
Lafortune lobes is used. With summing functions, there become a large number of parameters Cx,Cy,Cz 
and n to be defined for specifying reflectance. This makes the model inconvenient for user interfaces. 
The Lafortune model is useful though for fitting masses of measured BRDF data into a compact representation. 
 The Ashikhmin-Shirley modification of Phong reflectance (Ashikhmin and Shirley, An Anisotropic Phong 
BRDF Model Journal of Graphic Tools, 5,2, (2000), pp.25-32) has the feature that it includes an explicit 
term for the Fresnel reflectance. The specular reflectance increases as the angle of incidence increases. 
The diffuse component is appropriately reduced at these angles to maintain energy conservation. The formulation 
also maintains reciprocity, and allows for anisotropy. The Fresnel component is computed with Schlick 
s approximation (see Christophe Schlick. A customizable reflectance model for everyday rendering. Rendering 
Techniques 93, pages 73 84. ) In the examples shown, the decrease of the diffuse component with view 
angle relative to the ideal diffuse component used in the Ward model can be observed.  In contrast 
to empirical methods that look for convenient functional forms, first principles methods model the interaction 
with light with a mathematical model of material defined at a microscopic scale. The most frequently 
used first principles models use as a mathematical model a statistical distribution of surface facets 
to describe the details of the boundary between a material and air. The most popular methods model this 
interaction with geometric optics, which requires that the surface being modeled be large with respect 
to the wavelength of light (which is 0.4 to 0.7 microns) Some more complex models use wave optics to 
capture of the effects of diffraction at the surface. First principles models account for the effects 
that facets can have on one another they may block light incident on another facet, making it appear 
darker, or they may block light leaving the facet before it reaches a viewer, again resulting in a darker 
appearance. Even unblocked, the orientation of the facets results in light being scattered in a much 
different directional pattern than from a smooth surface.  Two popular first principles models are 
Blinn, Models of light reflection for computer synthesized pictures, SIGGRAPH 1977, pp. 192-198. and 
Cook- Torrance, Cook and Torrance A reflectance model for computer graphics . ACM Transactionson Graphics 
1, 1 (Jan. 1982), 7 24 They are both based on specular reflections of distributions of facets. The difference 
between them is the distribution of the facets assumed. The principle feature of the Cook-Torrance model 
is the prediction of off specular peaks, that are the consequences of shadowing and masking causing asymmetries. 
The principle feature of the Oren-Nayar model is the prediction of back scattering, that is a consequence 
of facets oriented towards the light source diffusely reflect some light back to the source. The result 
in each case are BRDF functions with lobes in the specular and backscatter directions that have more 
complicated structure than those used in the empirical models. The BRDF for these models is specified 
by giving parameters for the microscopic surface geometry. However, since the microstructure is rarely 
known, the facet distribution parameters are normally treated as parameters similar to n in the Phong 
and Lafortune models for controlling the shape of these complicated distributions.  For nearly smooth 
surfaces specular and/or diffuse reflectance can not be assumed at each facet. The effects of electromagnetic 
waves interfering with each other need to be accounted for. Methods by Kajiya Anisotropic Reflectance 
Models, SIGGRAPH 1985, pp15-21 and He et al. A Comprehensive Physical Model for Light Reflections, SIGGRAPH 
91, pp175-186 account for these effects that are important for nearly smooth surfaces. Accounting for 
wave phenomena on irregular surface makes for a more complicated model   These images showing the dramatic 
effect the He-Torrance model can have on near smooth surfaces were produced at and are copyrighted by 
Westin, Li and Torrance, and appear in the technical report cited . Recent work in optics and computer 
vision have re-examined some assumptions made in many graphics first principles models, in particular 
the form of the shadowing term and the effect of interreflections in rough surfaces. For further reading 
consult: J.J. Koenderink, A.J. Van Doorn, K.J. Dana, and S. Nayar. Bidirectional Reflection Distribution 
Function of Thoroughly Pitted Surfaces. International Journal of Computer Vision, 31(2):129 144, 1999. 
 H. Ragheb and E.R. Hancock. Testing new variants of the Beckmann Kirchhoff model against radiance data. 
Computer Vision and Image Understanding, 102(2):145 168, 2006. Y. Sun. Self shadowing and local illumination 
of randomly rough surfaces. Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of 
the 2004 IEEE Computer Society Conference on, 1. Y. Sun. Statistical ray method for deriving reflection 
models of rough surfaces. Journal of the Optical Society of America A, 24(3):724 744, 2007.  Effects 
that Require Keeping Track of more than just Radiance Polarization  Need to track state of polarization 
There are some optical effects that are important for small classes of materials. One is polarization. 
General references for this include: David C. Tannenbaum, Peter Tannenbaum, and Michael J. Wozny. Polarization 
and  birefringency considerations in rendering. In Proceedings of the 21st annual conference on Computer 
graphics and interactive techniques, pages 221 222. ACM Press, 1994; Alexander Wilkie, Robert F. Tobler, 
and Werner Purgathofer. Combined rendering of polarization and fluorescence effects. In Proceedings of 
the 12th Eurographics Workshop on Rendering, pages 197 204, 2001; Lawrence B. Wolff and David J. Kurlander. 
Ray tracing with polarization parameters. IEEE Comput. Graph. Appl., 10(6):44 55, 1990.   Other Effects 
 Polarization scalar values for radiance and BRDF replaced by matrices Wolff and Kurlander IEEE CGA 1990 
(complete Cook-Torrance model) Tannenbaum SIGGRAPH 1994: birefringent materials  Code available: SCATMECH 
 http://physics.nist.gov/Divisions/Div844 /facilities/scatmech/html/  The index of refraction is a 
function of wavelength, so different wavelengths get refracted differently, causing the separation of 
colors we see.  Another classes of effects is interference and diffraction. General references for 
these phenomena that require modeling the wave nature of light include: Brian E. Smits and Gary W. Meyer. 
Newton s color: Simulating interference phenomena in realistic image synthesis. In Kadi Bouatouch and 
Christian Bouville, editors, Rendering Techniques 90, Eurographics, pages 185 194. Imprimerie de l universit´e 
de Rennes, 1990. Proc. 1st Eurographics Rendering Workshop, Rennes, France, June 11 13, 1990; Yinlong 
Sun, F. David Fracchia, ThomasW. Calvert, and Mark S. Drew. Deriving spectrum from colors and rendering 
light interference. IEEE Comput. Graph. Appl., 19(4):61 67, 1999.; Jos Stam. Diffraction shaders. In 
Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 101 
110. ACM Press/Addison-Wesley Publishing Co., 1999; Yinlong Sun, F. David Fracchia, Mark S. Drew, and 
Thomas W. Calvert. Rendering iridescent colors of optical disks. In Proceedings of the Eurographics Workshop 
on Rendering Techniques 2000, pages 341 352, London, UK, 2000. Springer-Verlag.  A different cause 
of vibrant color is when light reflects and transmits through very thin films. When a layer of transmitting 
material has a thickness on the order of the wavelength of light, wave phenomena have to be accounted 
for. In particular, light waves can reiniforce one another or cancel each other out. Whether light 
waves will cancel or reinforce after traveling some distance through a thin film depends on the wavelength. 
For a given path through the film, some wavelengths will be reinforced and some cancelled, resulting 
in intense colors appearing. Yinlong Sun, F. David Fracchia, Thomas W. Calvert, and Mark S. Drew, "Deriving 
Spectra from Colors and Rendering Light Interference," IEEE Computer Graphics and Application, Vol. 19, 
No. 4, Jul. 1999, pp. 61-67.  This image is from : Sun, Y. 2006. Rendering biological iridescences 
with RGB­based renderers. ACM Trans. Graph. 25, 1 (Jan. 2006), 100-129. In this article a simplified 
model for accurating predicting these reflected colors is presented. As mentioned earlier, when roughness 
is very small, wavelike phenomena need to be accounted for in computing reflectance. When there is regular 
spacing in the small features, there can also be the interference effect similar to the thin film effect. 
The simulated CD image shown here is from: Yinlong Sun, F. David Fracchia, Mark S. Drew, and Thomas W. 
Calvert, "Rendering Iridescent Colors of Optical Disks," the 11th EUROGRAPHICS Workshop on Rendering 
(EGRW), Brno, Czech Republic, June 2000, pp. 341­  Andrew Glassner. A model of phosphorescence and 
fluorescence. In 5th Eurographics Rendering Workshop, pages 57 68, 1994.; Alexander Wilkie, Robert F. 
Tobler, and Werner Purgathofer. Combined rendering of polarization and fluorescence effects. In Proceedings 
of the 12th Eurographics Workshop on Rendering, pages 197 204, 2001. The difference in these images 
is the volumetric properties of the atmosphere. On the right particles that absorb and scatter light 
obscure anything in the distance.  For volumes the effect of material along a path needs to be considered, 
rather than a reflectance that encodes what happens when a ray hits a particular point on a surface. 
 As a ray travels through a volume, the amount of light may decrease by absorption or by light being 
scattered out of the ray path. The amount of light may increase by light being scattered into the path, 
or   For more detail on input data for participating media, see the SIGGRAPH 95 course notes Input 
for Participating Media , that are appended at the end of this tutorial. Also see the recent paper: D. 
Gutierrez, F. Seron, O. Anson, A. Muñoz. Visualizing underwater ocean optics. Volume effects are also 
visible in materials where the particles are tightly packed into a solid. The result is subsurface scattering 
. The only difference between the images on the left and right is the addition of subsurface scattering 
on the right. The top images are lit from the front, the bottom images from the back.In solids the scattering 
can essentially be considered isotropic in all cases.   Subsurface scattering is characterized by 
the BSSDF that accounts for light entering a material and emerging at a different angle and noticeably 
different distance from where it entered. This was original defined in F. E. Nicodemus, J. C. For densely 
packed solids, model as diffusion process and use dipole approximation. Jensen, H. W., Marschner, S. 
R., Levoy, M., and Hanrahan, P.. A practical model for subsurface light transport. SIGGRAPH '01..  
The same subsurface scattering parameters will result in different appearance depending on the thickness 
of the material. For the same density of material, the ratio of scattering to absorption in the material 
gives a different appearance. The lighting in these three images is the same. Reference for computing 
subsurface scattering: Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. 2001. A practical 
model for subsurface light transport. In Proceedings of the 28th Annual Conference on Computer Graphics 
and interactive Techniques SIGGRAPH '01. ACM Press, New York, NY, 511-518. There have been **many** papers 
that build on this technique to enable rendering of subsurface scattering in real time.   A basic mechanism 
for representing spatial variations that characterize a material is to use images mapped to the surface. 
Each pixel in the map may store simply a diffuse color, or a BRDF, a normal, a displacement, or a BTF 
(a virtual BRDF that includes the effect of small scale geometry). The mapping is done by storing coordinate 
of a location in an image with each vertex used to define a geometric model. Mapping to a geometry requires 
that the geometry be parameterized (i.e. a two dimensional coordinate system must be defined on the surface), 
a topic which is studied extensively in computer aided geometric design. Parameterization is one of the 
topics considered in the course in SIGGRAPH 2005 14. Discrete Differential Geometry: Grinspun Desbrun, 
Schröder ( in ACM Digital Library) These two renderings were made with Radiance with procedural textures 
rather than image maps to define spatial variations on the surface (e.g. see D. Ebert, Ed. Texturing 
and Modeling: A Procedural Approach, Third Edition. Morgan Kaufmann, San Francisco, CA, 2002.) The same 
spatial frequency has different visual impact depending on whether the fraction of light is modulated, 
or the direction of the surface normals. An alternative to analytic models of reflectance, is to create 
the small scale microstructure, and simulate its scattering effects by shooting rays at it and saving 
the results in a data structure designed specifically for BRDF. Two examples of this are: Gondek, Meyer, 
and Newman, Wavelength dependent reflectance functions In Proceedings of the 21st annual conference on 
Computer graphics and interactive techniques (1994), ACM Press, pp. 213 220. And Westin, Arvo and Torrance, 
Predicting reflectance functions from complex Surfaces In Proceedings of the 19th annual conference on 
Computer graphics and interactive techniques (1992), ACM Press, pp. 255 264. An advantage of simulation 
is that it can be used to explore the effects of subsurface structure, and effects of interference in 
thin surface layers. Gondek et al used a simulation of thin titanium dioxide mica flakes to simulate 
an irridescent paint such as   Just a reflectance model combined with a spatially varying texture 
isn t adequate to model all materials. In cases where this approach is adequate, it can be difficult 
to find the right parameters to use. A wide range of models have been developed for specialized materials. 
Some common themes in these models that have evolved are developing small scale geometric models, defining 
layers of materials and using measured or captured data. Many materials are composed of bundles of long 
thin fibers. The appearance of the bulk material is modeled by first account for reflection and transmission 
from individual strands. Hair, textiles and finished wood are all examples of materials modeled based 
on the light interactions of individual fibers.  Either naturally or by design many materials have 
the appearance of sparkles small flecks of material that have a high specular reflectance. A challenge 
is to model where the sparkles appear in a way that is consistent frame to frame in animated sequences. 
Materials in which sparkles appear include automotive paint, man-made carpet fibers and snow. Many 
materials have sense of depth because they are composed of multiple layers of material that transmit 
and reflect light. This effect occurs in materials as diverse as paints and the human eye.   With inexpensive 
digital cameras now widely available,many material models are built around data that can readily acquired. 
We review specialized models that have been developed by organizing them into natural and manufactured 
materials. 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed Materials 
Hair and Fur: Same stuff, hair on people, fur on other animals. People: vellus and terminal hairs. 
 An individual hair consists of a core medulla, the cortex and exterior cuticle. The cuticle consists 
of lapped cells on the outside of the hair. Coloration: granules of the melanin pigment, either eumelanins 
or pheomelanins . Hair with no pigment granules appears white. There are two different types of hair 
 vellus hairs and terminal hairs. Terminal hairs are those typically found on the scalp. Vellus hairs 
are unpigmented narrow (4 micron diameter), short (1mm) hairs that grow nearly all over the body. Some 
stats: Scalp hairs on humans are 50 micron to 90 micron diameter, may be circular or ellipsoidal, with 
curlier hair more ellipsoidal in cross section. Beard and moustache (rather than scalp) hair may have 
triangular cross section. Eyelashes, are 20 to 120 micron in diameter. A person has about 175 to 300 
terminal hairs per cm2, for a total of on the order of 100,000 hairs on the typical human scalp. Foxes 
and rabbits: average diameter 20 to 30 micron, and approximately 4000 hairs per cm2 Goats and badgers 
:average diameter 70 to 80 micron and approximately 100 to 200 hairs per cm2). General References on 
types of hair and fur: James Robertson. Forensic Examination of Human Hair. CRC Press, 1999. A. A. Blazej, 
J. Galatik, Z. Galatik, Z. Krul, and M. Mladek. Atlas of Microscopic Structures of Fur Skins 1. Elsevier, 
New York, 1989. Database of fur: http://www.furskin.cz/ 3. SPECIALIZED MATERIAL MODELS Common themes 
Natural Materials Manufactured/Processed Materials Where the common themes come in: Terminal Hair: Fibers 
Vellus Hair: Layer  A volumetric approach for rendering fur was presented in: J. T. Kajiya and T. L. 
Kay. Rendering fur with three dimensional textures. In Proceedings of the 16th annual conference on Computer 
graphics and interactive techniques, pages 271 280. ACM Press, 1989. This is different from pure volume 
rendering because the structure of individual strands must still be visible.  The projected density 
accounts for the number and size of hairs, the frame bundle accounts for their orientation.   Example 
views from the original Kajiya and Kay paper.  An example of modeling with strands (rather than texels): 
Aslan in Narnia Brad Hiebert, Jubin Dave, Tae-Yong Kim, Ivan Neulander, Hans Rijpkema, and Will Telford. 
The chronicles of Narnia: the lion,  the crowds and rhythm and hues. In SIGGRAPH 06: ACMSIGGRAPH 2006 
Courses, page 1, New York, NY, USA, 2006. ACM Press. A more detailed reflectance mode for individual 
strands was presented in Stephen R. Marschner , Henrik Wann Jensen , Mike Cammarano , Steve Worley , 
Pat Hanrahan, Light scattering from human hair fibers, ACM Transactions on Graphics (TOG), v.22 n.3, 
July 2003 Hair growth results in oriented scales on the hair. The orientation of the scales relative 
to the centerline of the hair changes the reflectance distribution relative to what would be expected 
from a smooth cylinder.    The different possible paths through the strand result in different reflectance 
lobes. A comparison shows that the more detailed model predicts effects like secondary highlights Effects 
of shadows and interreflections are studied in Jonathan T. Moon and Stephen R. Marschner. Simulating 
multiple scattering in hair using a photon mapping approach. In SIGGRAPH 06: ACM SIGGRAPH 2006 Papers, 
pages 1067 1074, New York, NY, Tom Lokovic and Eric Veach. Deep shadow maps. In SIGGRAPH 00: Proceedings 
of the 27th annual conference on Computer graphics and interactive techniques, pages 385 392, New York, 
NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co.  Multiple scattering is particularly critical 
for blond hair. 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed 
Materials Vellus Hairs Machine Vision and Applications (2003) 14: 260 268 The secret of velvety skin 
Jan Koenderink, Sylvia Pont Machine Vision andApplications, 14(4):260 268, 2003. Machine Vision and Applications 
(2003) 14: 260 268 The secret of velvety skin Jan Koenderink, Sylvia Pont Asperity scattering: Thin layer 
(. small) of thin (few particles per unit volume) participating medium formed by tiny hairs or dust. 
 The scattering from short vellus hairs on the rest of the body is similar to the scattering of fuzz 
on a peach. A simple function for simulating this effect, which softens the look of a surface is described 
in J. Koenderink and S. Pont. The secret of velvety skin.  The fuzz on a peach is another example of 
asperity scattering. Rather than seeing individual small fibers, we see the effect of the hazy layer, 
and white edges on the silhouette of the peach when it is backlit. Asperity scattering can be modeled 
with a simple expression, with limits on the values of the angles so that the function doesn t go to 
infinity.  Human skin: -- varies in thickness varying from 0.1 to more than 0.5 cm. -- has three layers 
: The epidermis is the thin outside layer, that includes the exterior layer of dead cells (the stratum 
cornuem). The dermis is thicker, and includes the vessels that carry blood. The hypodermis connects 
the skin to the rest of the body. A general reference: Kenneth A. Walters. Dermatological and Transdermal 
Formulations. Marcel Dekker Incorporated, 2002. 3. SPECIALIZED MATERIAL MODELS Common themes Natural 
Materials Manufactured/Processed Materials Skin Where the common themes come in: --Detailed geometry 
modeling --Layers --Captured data   The need to account for subsurface scattering in skin models was 
first noted in: Pat Hanrahan and Wolfgang Krueger. Reflection from layered surfaces due to subsurface 
scattering. In Proceedings of the 20th annual conference on Computer graphics and interactive techniques, 
pages 165 174. ACM Press, 1993. And more recent methods have estimated a subsurface scattering model 
from measurements: Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd Bickel, Craig Donner, Chien 
Tu, Janet McAndless, Jinho Lee, Addy Ngan, HenrikWann Jensen, andMarkus Gross. Analysis of human faces 
using a measurement-based skin reflectance model. In SIGGRAPH 06: ACM SIGGRAPH 2006 Papers, pages 1013 
1024, New York, NY, USA, 2006. ACM Press. Diagram of scattering in subsurface layers and resulting rendering, 
from scattering.  Surface reflectance models  Skin Model Components --subsurface scattering model 
--surface reflectance model (such as Cook-Torrance) --a model of small scale spatial variations in the 
spectral reflectance ---geometry variations of the skin surface. have been fit to captured data including 
fitting the Lafortune model used in: Stephen R. Marschner, Brian K. Guenter, and Sashi Raghupathy. Modeling 
and rendering for realistic facial animation. In Proceedings of the Eurographics Workshop on Rendering 
Techniques 2000, pages 231 242, London, UK, 2000. Springer-Verlag. And Cook-Torrance model used in: Athinodoros 
S. Georghiades. Recovering 3-d shape and reflectance from a small number of photographs. In EGRW 03: 
Proceedings of the 14th Eurographics workshop on Rendering, pages 230 240, Aire-la-Ville, Switzerland, 
Switzerland, 2003. Eurographics Association.  Domes: Measure skin BRDF and facial shape simultaneously 
"Recovering 3-D Shape and Reflectance From a Small Number of Photographs," Eurographics Symposium on 
Rendering, June 2003, pp. 230-240, 315.  Skin Model Components --subsurface scattering model --surface 
reflectance model (such as Cook-Torrance) --a model of small scale spatial variations in the spectral 
reflectance ---geometry variations of the skin surface. Spatial coloring variations are due to freckles 
and age spots, and temporary effects such as blushing. A full first principles model of skin including 
prediction of color due to detailed composition including blood flow is the BioSpec Model: A. Krishnaswamy 
and G.V.G. Baranoski. A Biophysically-Based Spectral Model of Light Interaction with Human Skin. Computer 
Graphics Forum, 23(3):331 340, 2004.   Plots from Krishnaswamy and Baranoski, 2004 for the spectral 
data for various skin model components. Images from Krishnaswam and Baranoski, 2004  Details of modeling 
skin: Stephen R. Marschner, Brian K. Guenter, and Sashi Raghupathy. Modeling and rendering for realistic 
facial animation. In Proceedings of the Eurographics Workshop on Rendering Techniques 2000, pages 231 
242, London, UK, 2000. Springer-Verlag. Generic wrinkle patterns are modeled in: L. Boissieux, G. Kiss, 
N. Magnenat-Thalmann, and P. Kalra. Simulation of skin aging and wrinkles with cosmetics insight. Computer 
Animation and Simulation 2000, pages 15 27, 2000. Applying wrinkles from scanned data: lovinskiy et al., 
2006] Aleksey Golovinskiy, Wojciech Matusik, Hanspeter Pfister, Szymon Rusinkiewicz, and Thomas Funkhouser. 
A statistical model for synthesis of detailed facial geometry. In SIGGRAPH 06: ACM SIGGRAPH 2006 Papers, 
pages 1025 1034, New York, NY, USA, 2006. ACM Press.  Using a molding compound to capture skin geometry 
Real-time, Photo-realistic, Physically Based Rendering of Fine Scale Human Skin Structure A. Haro, B. 
Guenter, and I. Essa, Proceedings 12th Eurographics Workshop on Rendering, London, England, June 2001 
  Using multiple dipoles In thin layers: Light Diffusion in Multi-Layered Translucent Materials - Craig 
Donner Henrik Wann Jensen Proceedings of ACM SIGGRAPH 2005, An example of combining multiple layers to 
form a model for skin. Source of excellent discussion of detailed modeling and parameter tuning for 
skin http://developer.download.nvidia.com/presentations/2007/gdc/Advanced_Skin.pdf MOST comprehensive 
survey: "The Appearance of Human Skin: A Survey," T. Igarashi, K. Nishino, and S. K. Nayar, Foundations 
and Trends in Computer Graphics and Vision, Vol.3, No.1, pp.1-95, 2007. 3. SPECIALIZED MATERIAL MODELS 
Common themes Natural Materials Manufactured/Processed Materials  Eyes Common theme: Layers Eyes have 
a complex appearance due to a complex layered structure. An approximation of this structure based on 
the manufacture of artificial eyes is given by: Aaron Lefohn, Brian Budge, Peter Shirley, Richard Caruso, 
and Erik Reinhard. An ocularist s approach to human iris synthesis. IEEE Comput Graphics Appl, 23(6):70 
75, November/ December 2003. A detailed biological model is described in: Michael W.Y. Lam and Gladimir 
V.G. Baranoski. A predictive light transport model for the human iris. Computer Graphics Forum, 25(3):359 
368, 2006.   Simple geometric model of eye. Textures for various layers that would be combined in 
artificial eye that together give eyes depth.   Lam and Baranoski present a biologically detailed 
model of the eye.  The biologically accurate model can predict eye appearance.  Physically detailed 
models of leaves are given in: G. V. G. Baranoski and J. G. Rokne. An algorithmic reflectance and transmittance 
model for plant tissue. Computer Graphics Forum, 16(3):141 150, August 1997. ISSN 1067-7055. of light 
by leaves. The Visual Computer, 17(8):491 505, 2001. Lifeng Wang, Wenle Wang, Julie Dorsey, Xu Yang, 
Baining Guo, and Heung-Yeung Shum. Real-time rendering of plant leaves. In SIGGRAPH 05: ACM SIGGRAPH 
2005 Papers, pages 712 719, New York, NY, USA, 2005. ACM Press. The detailed layers of plant tissue 
can be modeled as thin layers of scattering participating media The observations can be encoded in terms 
of models for reflectance and transmittance.         Sample output from appearance modeling 
 Another feature of plants is that they may be covered by systems of small hairs. Fuhrer et al. present 
a method for placing and rendering such hairs.  While it is a single material, wood can have a wide 
variety of appearance, as noted in these photographs of natural and finished wood. Lefebre and Poulin 
presented a general procedural model for fitting the structure of color variation in wood.  Results 
from Lefebvre and Poulin s work, synthesizing a new piece of wood using parameters estimated for the 
model from a real piece of wood. Light incident on porous materials undergoes interreflections within 
pores causing darkening:S.Merillou, J.- M. Dischler, and D. Ghazanfarpour. A BRDF postprocess to integrate 
porosity on rendered surface. IEEE Transactions on Visualization and Computer Graphics, 6(4):306 318, 
October 2000.  Pores may not be visible, but can be seen in a microscopic view  This figure from Merillou 
et al. shows the multiple reflections/absorptions that result in darkening.  From Merillou et al., 
comparisons to physical images. 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed 
Materials Wet materials Common Theme:layers captured data  A top smooth layer results in specular 
reflection, the index of refraction of water results in total internal reflection, and so more absorption 
and material darkening. More scattering within the material causes more saturated color.   Forward 
scattering and index of refraction of water result in coherent transmission through a thin material. 
  Observations of geometric variation of wet materials.  Increased saturation of material captured 
with digital photography.   Evaporation and flow in the material contribute to drying. This means ambient 
occlusion (for evaporation) and distance to edge (flow in material) are significant parameters for estimating 
the drying pattern. A wetness map is estimated for an object photographed at different times as it dries. 
 Experimental Procedure: Data Analysis  Functions can be fit to the drying time as a function of distance 
to edge and the ambient occlusion (or the accessibility)  The parameters for these functions are stored 
as look up maps.  Examples of using the model compared to ground truth.  Applying the model to a synthetic 
example.   Nishita et al. presented a model of snow as a participating medium with embedded particles 
for sparkle Modelling rocks requires modeling macroscopic 3D structure.   2D textures are inconvenient 
for modeling truly 3D materials such as rock/stone.  Solid textures allow the separate modeling of 
material geometry and object geometry. Volume 26 (2007), number 1 pp. 66 79 COMPUTER GRAPHICS forum Modeling 
and Rendering of Heterogeneous Granular Materials: Granite Application Romain Souli´e, St´ephane M´erillou, 
Olivier Romain, Terraz Djamchid and Romain Ghazanfarpour Modeling process of heterogeneous granular materials 
using 3DVorono¨i diagrams to obtain a full representation of their structure. Rendering process based 
on photon mapping to simulate subsurface scattering inside these materials, presented in  Figures from 
Soulie et al. modeling granite: the geometry is modelled using a 3D Voronoi diagram, and subsurface scattering 
is modeled for the individual granite components of mica, feldspar and quarts.  Figure from Soulie et 
al., modeling the material 3D structure allows modeling of geometry that is consistent with the material. 
  Texture synthesis such as Heeger and Bergen s spectral analysis fail for materials made of discrete 
particles embedded in a matrix. Stereological Techniques for Solid Textures Jagnow, Dorsey, Rushmeier,SIGGRAPH 
2004 Stereology is used in the construction industry to estimate rock distributions.   Stereology is 
also used in biology to estimate 3D distributions fom 2D slices The distribution of 2D diameters in a 
slice is related to , but not the same as the 3D distribution.  Recent research Volumetric materials 
Recovering Sphere Distributions How many profiles of the smallest size? NA (1) K11 NV (1) K12 NV (2) 
K13 NV (3) K14NV (4) Kij = Probability that particle NV(j) exhibits profile NA(i) Recovering Sphere 
Distributions Putting it all together  For particles that aren t spheres, the relation between 2D and 
3D needs to be calculated Recent research Volumetric materials  Other Particle Types We cannot classify 
arbitrary particles by d/dmax Instead, we choose to use A / Amax Algorithm inputs: Approach: Collect 
statistics for 2D profiles and 3D particle Recent research Volumetric materials Profile Statistics Segment 
input image to obtain profile densities NA. Bin profiles according to their area, A/ Amax Recent research 
Volumetric materials Recovering Particle Distributions Just like before, NA = HKNV 1 -1 K NA Solving 
for the particle densities, NV = H Use NV to populate a synthetic volume. Color using mean particle 
colors from the input image Recent research Volumetric materials  Recovering Noise How can we replicate 
the noisy appearance of the input? Some noise is needed, or results look too pristine The noise residual 
is less structured and responds well to Heeger &#38; Bergen s method     Jagnow et al., ACM Transactions 
on Applied Perception, 2008 There is no single way to estimate particle shape from a single slice. A 
psychophysical test is needed to find which estimation technique is best. Recent research Volumetric 
materials Applied Perception: Do Psychophysical Experiment Which is the slice through the same material 
shown in the center?  An example of one screen view from the psychophysical test. Results of test across 
different particle shapes. 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed 
Materials Finished Wood Textiles Fibers Threads Knitted Woven Automotive Paint Artistic Paint Gems 
 Finished wood can have a lustrous appearance that results from the internal orientation of the wood 
fibers, combined with the reflection and transmission from the smoothed finished top surface. Marschner 
et al. modeled this reflectance as a Gaussian function g that depends on parameters Psi that depend 
on the orientation of the wood fibers and the surface normal. Stephen R. Marschner, Stephen H. Westin, 
Adam Arbree, and Jonathan T. Moon. Measuring and modeling the appearance of finished wood. ACM Trans. 
Graph., 24(3):727 734, 2005.   Figure from Marschner et al 2005, similar to wet surfaces, smooth surface 
from finish layer produces specular refection. Similar to hair analysis, orientation of wood fibers influences 
orientation of reflection.  From Marschner et al.2005, full model for finished wood. 3. SPECIALIZED 
MATERIAL MODELS Common themes Natural Materials Manufactured/Processed Materials Textiles: Individual 
threads Knitted Materials Woven Materials Common theme: Detailed geometry  3. SPECIALIZED MATERIAL 
MODELS Common themes Natural Materials Manufactured/Processed Materials Textiles: Individual threads 
Knitted Materials Woven Materials 36:6388 6392, 1997. The structure of individual threads may be designed 
to give particular optical effects, such as looking a different color from different view angles. B. 
Rubin, H. Kobsa, and S. M. Shearer. Prediction and verification of an iridescent synthetic fiber. Appl. 
Opt., 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed Materials 
Textiles: Individual threads Knitted Materials  Woven Materials Specialized volumetric structures have 
been proposed to model the fuzzy nature of knitwear: Eduard Groeller, Rene T. Rau, and Wolfgang Strasser. 
Modeling and visualization of knitwear. IEEE Transactions on Visualization and Computer Graphics, 1(4):302 
310, 1995. Yarn is loose, fluffy grouping of fibers: Ying-Qing Xu, Yanyun Chen, Stephen Lin, Hua Zhong, 
Enhua Wu, Baining Guo, and Heung-Yeung Shum. Photorealistic rendering of knitwear using the lumislice. 
In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 391 
398. ACM Press, 2001. Xu et al. model yarn as volume of fibers formed by twisting a scatter diagram in 
2D along a 3D axis.  From Xu et al. 2001, a full knitted fabric is formed from the volumetric model 
applied along a pattern of knitted stiches.  The reflectance of individual threads, effect of light 
going through threads and weaving patterns need to be accounted for in realistically rendering woven 
materials Neeharika Adabala, Guangzheng Fei, and Nadia Magnenat-Thalmann. Visualization of woven cloth. 
Proceedings of the 14th Eurographics workshop on Rendering, Leuven, Belgium, 2003. pages 178 185.  
Figure from Adabla et al. showing complete process for modeling woven cloth. Neeharika Adabala, Guangzheng 
Fei, and Nadia Magnenat-Thalmann. Visualization of woven cloth. Proceedings of the 14th Eurographics 
workshop on Rendering, Leuven, Belgium, 2003. pages 178 185. Figure showing side by side images of photographs 
of real cloth and the simulations performed by Adabala et al.  Slide 14 3. SPECIALIZED MATERIAL MODELS 
Common themes Natural Materials Manufactured/Processed Materials  Automotive Paint A first principles 
model for automotive paint, including the depth effect given by multiple layers and sparkles caused by 
reflections off small particles is given in: Sergey Ershov, Konstantin Kolchin, and Karol Myszkowski. 
Rendering pearlescent appearance based on paint-composition modelling. Computer Graphics Forum, 20(3), 
2001. Sergey Ershov, Roman Durikovic, Konstantin Kolchin, and Karol Myszkowski. Reverse engineering approach 
to appearance-based design of metallic and pearlescent paints. Vis. Comput., 20(8-9):586 600, 2004. Roman 
Durikovic and William L. Martens. Simulation of sparkling and depth effect in paints. In SCCG 03: Proceedings 
of the 19th spring conference on Computer graphics, pages 193 198, New York, NY, USA, 2003. ACM Press 
The change of color with angle for metallic car paints is modeled in a system for automotive finish design 
in: Gary Meyer, Clement Shimizu, Alan Eggly, David Fischer, Jim King, and Allan Rodriguez.Computer aided 
design of automotive finishes. In Proceedings of 10th Congress of the International Colour Association, 
pages 685 688, 2005.  ¨ -Johannes G¨unther, Tongbo Chen, Michael Goesele, Ingo Wald, and Hans-Peter 
Seidel. Efficient acquisition and realistic rendering of car paint. In G¨unther Greiner, Joachim Hornegger, 
Heinrich Niemann, and Marc Stamminger, editors, Proceedings of 10th International Fall Workshop -Vision, 
Modeling, and Visualization (VMV) 2005, pages 487 494. Using captured data to fit a multilobed Cook Torrance 
Model An interface for designing the change in paint color with view. Figure from Guenther et all using 
a paint model based on data capture.   Figure from Ershov showing the detailed paint structure used 
in a first principles model. Figures from Ershov et al. 2001 showing results  Artistic paints, particularly 
water colors, are modeled as particulates carried by a fluid modeled with either a 3. SPECIALIZED MATERIAL 
MODELS shallow fluid model: Common themes Natural Materials Cassidy J. Curtis, Sean E. Manufactured/Processed 
Materials Anderson, Joshua E. Seims, KurtW. Fleischer,  Artistic Paints  and David H. Salesin. Computer-generated 
watercolor. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques, 
pages 421 430. ACM Press/Addison- Wesley Publishing Co., 1997. Or a Lattice-Boltzmann model: Nelson 
S.-H. Chu and Chiew-Lan Tai. Moxi: real-time ink dispersion in absorbent paper. ACM Trans. Graph., 24(3):504 
511, 2005. 3. SPECIALIZED MATERIAL MODELS Common themes Natural Materials Manufactured/Processed Materials 
  GEMS Some geometric detailed modeling, Make use of advanced optics effects  Rendering by Yinlong 
Sun showing colors in a diamond produced by accounting for dispersion. Rendering gems with aterism or 
chatoyancy Shigeki Yokoi, Kosuke Kurashige and Jun-ichiro Toriwaki The Visual Computer, Volume 2, Number 
5 / September, 1986 Bright patterns in gems like cat s eyes and sapphires These are caused by ellipsoidal 
inclusions of material in the gem, and are accounted for using a volume rendering method.   Figure 
from from Guy and Soler. For many types of gems polarization effects must be taken into account. IEEE 
TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 3, NO. 4, OCTOBER-DECEMBER 1997 307 Modeling 
and Visualization for a Pearl-Quality Evaluation Simulator Noriko Nagata, Toshimasa Dobashi, Yoshitsugu 
Manabe, Teruo Usami, and Seiji Inokuchi Figures from Nagata et al. 1997, showing the results of modeling 
pearls using different layers to model diffuse reflectance, interference effects, specular reflection, 
and spatial variations. 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches 
 The area of aging and weathering processes is relatively new in material modeling. Many very detailed 
models have been developed. A general view of the area is given by organizing the detailed models into 
three general categories. Taxonomy  Mechanical Processes Biological Growth  -Chemical Reactions like 
rusting or patination -Mechanical Processes like paint crackling and peeling -Biological Growth like 
algae, moss or mold growing  Summary table from Jianye Lu, Athinodoros S. Georghiades, Andreas Glaser, 
Hongzhi Wu, Li-YiWei, Baining Guo, Julie Dorsey, and Holly Rushmeier. Context-aware textures. ACM Trans. 
Graph., 26(1):3, 2007.   Metals often develop a characteristic patina over time. The patination process, 
which develops in a series of thin surface layers, is due to the chemical alteration of a surface and 
results in changes in color. Patination may be the result of deliberately applied craft processes or 
natural corrosion.  To represent surface patinas, we have developed a layered-surface model. We have 
also developed a simple scripting language, consisting of operators such as coat and erode, which can 
be used to specify the evolution of a surface over time. Layered Surface Model Representation Layers 
 Operators and scripting language   Rendering Surface and subsurface scattering  In our work, we 
ve introduced a model for layered surfaces and applied it to a classic example of the aging of a surface: 
the development of metallic patinas. This model allows us to program how the surface will change over 
time in addition to rendering the complex interaction of light with the surface and subsurface of the 
model. We have also developed a simple scripting language, consisting of operators such as coat and erode, 
which can be used to specify the evolution of a surface over time.  Here, you re seeing the build up 
a patina on a small polygonal statue of a buddha. Our model allows us to control the composition and 
thickness of the layers in the model and to vary various position-dependent parameters according to the 
geometry and other factors Julie Dorsey and Pat Hanrahan. Modeling and rendering of metallic patinas. 
In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 387 
396. ACM Press, 1996.  The flow of water is one of the most pervasive and important natural forces 
involved in the weathering of materials producing a distinctive set of patterns of washes and stains. 
These slides show photographs showing the weathering of various buildings. Looking carefully at these 
slides, you can see that many of the complex patterns of the surfaces are due to the flow of water. Water 
may wash dirt from some areas and clean them; in other areas dirt and other materials are deposited, 
creating stains. The result is a visually rich set of patterns that are difficult to model with current 
texturing techniques.  Washing and staining effects: The key factors are the amount of water that incident 
on a surface, the geometry of the surface, and the absorption of water by the surface.  Simulation on 
a Complex Model  Examples: Simulated Patterning  Rendering w/out flows Rendering with flows  Julie 
Dorsey, Hans Kohling Pedersen, and Pat Hanrahan. Flow and changes in appearance. In Proceedings of the 
23rd annual conference on Computer graphics and interactive techniques, pages 411 420. ACM Press, 1996. 
 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches Rust  Stephane Merillou, 
Jean-Michel Dischler, and Djamchid Ghazanfarpour. Corrosion: Simulating and rendering. In GI 2001, pages 
167 174, June 2001. ,  4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches 
 Erosion New stone Oxides &#38; water Crust Erosion Julie Dorsey, Alan Edelman, Henrik Wann Jensen, 
Justin Legakis, and Hans Kohling Pedersen. Modeling and rendering of weathered stone. In Proceedings 
of the 26th annual conference on Computer graphics and interactive techniques, pages 225 234. ACM Press/Addison-Wesley 
Publishing Co., 1999.  Corestone &#38; yellowing Yellowing  Volumetric Surface   Local volumetric 
representation  Combines benefits of surfaces and volumes     4. AGING AND WEATHERING PROCESSES 
Taxonomy Simulation Methods Capture Approaches Cracking  Koichi Hirota, Yasuyuki Tanoue, and Toyohisa 
Kaneko. Generation of crack patterns with a physical model. The Visual Computer, 14(3):126 137, 1998. 
Koichi Hirota, Yasuyuki Tanoue, and Toyohisa Kaneko. Simulation of threedimensional cracks. The Visual 
Computer, 16(7):371 378, 2000. 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture 
Approaches Peeling  S. Gobron and N. Chiba. Simulation of peeling using 3d-surface cellular automata. 
In 9th Pacific Graphics Conference on Computer Graphics and Applications, pages 338 347, Tokyo Japan, 
Oct 2001. IEEE. Eric Paquette, Pierre Poulin, and George Drettakis. The simulation of paint cracking 
and peeling. In Graphics Interface 2002, pages 59 68, May 2002. 4. AGING AND WEATHERING PROCESSES Taxonomy 
Simulation Methods Capture Approaches Scratching  S. Merillou, J.M. Dischler, and D. Ghazanfarpour. 
Surface scratches: measuring, modeling and rendering. The Visual Computer, 17(1):30 45, 2001. C. Bosch, 
X. Pueyo, S. M´erillou, and D. Ghazanfarpour. A physically-based model for rendering realistic scratches. 
Computer Graphics Forum, 23(3):361 370, 2004. 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation 
Methods Capture Approaches Denting and Impacts  George Drettakis Eric Paquette, Pierre Poulin. Surface 
aging by impacts. In Graphics Interface 2001, pages 175 182, June 2001. 4. AGING AND WEATHERING PROCESSES 
 Taxonomy Simulation Methods Capture Approaches Dust Accumulation  Siu-Chi Hsu and Tien-Tsin Wong. 
Simulating dust accumulation. IEEE Comput Graphics Appl, 15(1):18 22, January 1995. 4. AGING AND WEATHERING 
PROCESSES Taxonomy Simulation Methods Capture Approaches Lichen Growth  Brett Desbenoit, Eric Galin, 
and Samir Akkouche. Simulating and modeling lichen growth. Computer Graphics Forum, 23(3):341 350, 2004. 
 4. AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches  Capture in Context 
--appearance ---agent causing change --geometry Observing and Transferring Material Histories  First-principles 
simulations are time-consuming or impossible  New approach:  Capture time variations from real shapes, 
transfer them to generate synthetic objects  Shapes can be rendered at different times in their histories 
   While simulating aging effects produces good results, first-principles simulations are time consuming 
or, in some cases, impossible because the underlying physics and chemistry are not completely understood. 
We have explored a new approach to producing weathering effects. Rather than simulating aging effects, 
we instead capture time variations from real shapes to generate synthetic objects. Using this approach, 
shapes can be rendered at different times in their histories. For example, we applied an artificial patination 
treatment to the bowl over a two week period and captured the shape and texture variations a frequent 
time intervals.    Jianye Lu, Athinodoros S. Georghiades, Andreas Glaser, Hongzhi Wu, Li-YiWei, Baining 
Guo, Julie Dorsey, and Holly Rushmeier. Context-aware textures. ACM Trans. Graph., 26(1):3, 2007. 4. 
AGING AND WEATHERING PROCESSES Taxonomy Simulation Methods Capture Approaches  Time Varying Additional 
efforts have capture time series of BRDF data Additional time varying capture: Jinwei Gu, Chien-I Tu, 
Ravi Ramamoorthi, Peter Belhumeur, Wojciech Matusik, and Shree Nayar. Time-varying surface appearance: 
acquisition, modeling and rendering. ACM Trans. Graph., 25(3):762 771, 2006. 4. AGING AND WEATHERING 
PROCESSES Taxonomy Simulation Methods  Capture Approaches From Single Example  In some cases weathering 
effects can be captured by taking data from a single image of a material that has been marked with fully 
aged and fully new regions. Jiaping Wang, Xin Tong, Stephen Lin, Minghao Pan, Chao Wang, Hujun Bao, 
Baining Guo, and Heung-Yeung Shum. Appearance manifolds for modeling time-variant appearance of materials. 
ACM Trans. Graph., 25(3):754 761, 2006.  5. Future Trends and Resources --Captured data and models --Material 
and Shape --Databases --Specification --Captured data and models --Material and Shape --Databases --Specification 
Following Lefebvre and Poulin ore general models to leverage data?  --Captured data and models --Material 
and Shape --Databases --Specification Following work on 3D solid textures for materials, an we find more 
relationships between material properties and the shapes it takes naturally?  See discussion in supplemental 
documents on light and material in virtual cities. See supplemental document on materials and perception. 
--Captured data and models --Material and Shape --Databases --Specification Parameter setting Navigation 
Retrieval How do we locate materials: oDescriptions journal papers in different disciplines oCode stand 
alone/package specific plugins oData texture images/BRDF/BTF pigment, index of refraction Unlike images 
and objects can t locate by format.   Bibliography Advanced Material Appearance Models General BRDF 
s, BSSRDF s, and Textures [Phong,1975],[Ward,1992],[Lafortuneet al.,1997],[Ashikhmin andShirley,2000],[Blinn,1977],[Cook 
andTorrance,1982],[Oren andNayar,1994],[Jensen et al., 2001] Specialized Material Models Natural Materials 
 Organic  * skin: wrinkles and pores, pigmentation, subsurface scattering, surface oils (references 
for follow-up) [Walters, 2002], [Marschner et al., 2000],[Hanrahan and Krueger, 1993] [Stam, 2001][KrishnaswamyandBaranoski,2004],[Debevec 
et al., 2000],[Georghiades,2003][Fuchs et al., 2005][Weyrich et al., 2006][Boissieux et al.,2000],[Koenderink 
andPont,2003] * hairandfur: scatteringfromindividual strands,interre.ections,self-shadowing,(references 
for follow-up) [Robertson, 1999],[Blazej et al., 1989],[Kajiya and Kay, 1989],[Hiebert et al., 2006],[Marschneret 
al., 2003],[Zinke andWeber,2007],[Lokovic andVeach,2000],[Moon and Marschner, 2006] * eyes: pigmentation, 
creating sense ofdepth,(referencesforfollow-up) [Lefohnet al.,2003],[Lam and Baranoski, 2006] * plants: 
internal structure, pigments, hairy features, growth patterns, including leaves, bark, wood,(referencesforfollow-up) 
[Bloomenthal,1985],[Prusinkiewicz et al., 1988],[Vogelmann, 1993;Franzke andDeussen,2003],[BaranoskiandRokne,1997],[BaranoskiandRokne,2001], 
[Franzke andDeussen,2003],[Wang et al., 2005],[Fowler et al., 1989; 1992],[Fuhrer et al.,], [Lefebvre 
andNeyret,2002],[Buchanan,1998],[Lefebvre andPoulin,2000] * birds: structureoffeathers,forming coats,(referencesforfollow-up) 
[Dai et al., 1995],[Streit and Heidrich, 2002][Chen et al., 2002] * insects: iridescence,(reference) 
[Sun,2006]   Natural Materials Inorganic  * porous: modi.cationof re.ectance,(reference) [Merillou 
et al., 2000] * wet: darkening and transparency e.ectsfor water or oilpermeated materials,(referencesfor 
follow-up) [Jensen et al., 1999],[Nakamae et al., 1990] * snow:internal scattering, visualparticles,(reference) 
[Nishita et al., 1997]   Manufactured/Processed Materials  * .nished wood: senseofdepth,(reference) 
[Marschner et al., 2005] * textiles:individual .bers,threadsandformationoffabrics,(referencesforfollowup) 
[Rubin, 1998],[Groeller et al., 1995],[Xu et al., 2001],[Adabala et al., 2003] * automotivepaint: color 
.op,depth,sparkle,orangepeel,(referencesforfollowup) [Takagi et al., 1990],[G¨unther et al., 2005],[Dumont-B`ecle 
et al., 2001],[Shimizu et al., 2003],[Ershov et al., 2001] * artisticpaintsandinks:pigmentationand .owe.ects, 
[Cockshott et al., 1992],[1997],[Chu and Tai, 2005] * gems: lightphenomena requiring simulation of wave 
e.ects,(referencesforfollowup) [Yuan et al., 1988],[Sun et al., 2001],[Guy and Soler, 2004],[Nagata et 
al., 1997]   1  Aging and Weathering Processes Simulation Methods: Methods that have appeared in the 
graphics literature, with example refer­ences for each. Comparisons of techniques for the same phenomena 
will be presented. * patination: [Dorsey and Hanrahan, 1996],[Chang and Shih, 2000] * rust: [Merillou 
et al.,2001b],[Chang andShih,2003] * erosion:[Dorsey et al., 1999] * cracking: [Hirota et al., 1998],[Hirota 
et al.,2000],[Gobronand Chiba,2001b],[Aoki et al., 2002][Paquette et al., 2002] * peeling: [Gobron and 
Chiba, 2001a] * scratching:[Merillou et al., 2001a],[Bosch et al., 2004] * denting and impacts: [Eric 
Paquette, 2001] * dust accumulation: [chi Hsu and tsin Wong, 1995] * lichen growth: [Desbenoit et al., 
2004] CaptureApproaches:Methodsthat usephotography and/or3D scanning to captureinstances of aging e.ects 
that can be generalized * capturing context [Lu et al., 2005],[Lu et al., 2007] * temporal variation 
ofBTF s andBRDF s [Koudelka,2004][Gu et al., 2006] * models from single examples of materials: [Wang 
et al., 2006] 2  [Adabala et al., 2003] Neeharika Adabala, Guangzheng Fei, and Nadia Magnenat-Thalmann. 
Visual­ization of woven cloth. In Philip Dutr´e, Frank Suykens, Per H. Christensen, and Daniel Cohen-Or, 
editors, Proceedings of the 14th Eurographics workshop on Rendering, pages 178 185, Leuven, Bel­gium, 
2003. Eurographics Association. [Aoki et al., 2002] K. Aoki, Ngo Hai Dong, T. Kaneko, and S. Kuriyama. 
Physically-based simulation of cracks on drying 3d solid. In 10th Paci.c Graphics Conference on Computer 
Graphics and Applications, pages 467 468, Beijing China, Oct 2002. IEEE. [Ashikhmin and Shirley, 2000] 
Michael Ashikhmin and Peter Shirley. An anisotropic Phong BRDF model. Journal of Graphic Tools, 5(2):25 
32, 2000. [Baranoski and Rokne, 1997] G. V. G. Baranoski and J. G. Rokne. An algorithmic re.ectance and 
transmittance model for plant tissue. Computer Graphics Forum, 16(3):141 150, August 1997. ISSN 1067-7055. 
[Baranoski and Rokne, 2001] Gladimir V. G. Baranoski and Jon G. Rokne. E.ciently simulating scattering 
of light by leaves. The Visual Computer, 17(8):491 505, 2001. [Blazej et al.,1989] A.A.Blazej,J.Galatik,Z.Galatik,Z.Krul,andM.Mladek. 
Atlas of Microscopic Structures of Fur Skins 1. Elsevier, New York, 1989. [Blinn,1977]JamesF.Blinn.Modelsoflightre.ectionforcomputersynthesizedpictures. 
In Proceed­ings of the 4th annual conference on Computer graphics and interactive techniques, pages 192 
198. ACM Press, 1977. [Bloomenthal,1985]JulesBloomenthal.Modelingthemighty maple. In Proceedings of the 
12th annual conference on Computer graphics and interactive techniques, pages 305 311. ACM Press, 1985. 
[Boissieux et al., 2000] L. Boissieux, G. Kiss, N. Magnenat-Thalmann, and P. Kalra. Simulation of skin 
aging and wrinkles with cosmetics insight. Computer Animation and Simulation 2000, pages 15 27, 2000. 
[Bosch et al.,2004] C.Bosch,X.Pueyo,S.M´erillou,andD.Ghazanfarpour.Aphysically-based model for rendering 
realistic scratches. Computer Graphics Forum, 23(3):361 370,2004. [Buchanan, 1998] John W. Buchanan. 
Simulating wood using a voxel approach. Computer Graphics Forum,17(3):105 112,1998. ISSN1067-7055. [Chang 
andShih,2000]Yao-Xun Chang andZen-Chung Shih. Physically-basedpatinationforunder­ground objects. Computer 
Graphics Forum, 19(3), August 2000. ISSN 1067-7055. [Chang and Shih, 2003] Yao-Xun Chang and Zen-Chung 
Shih. The synthesis of rust in seawater. The Visual Computer, 19(1):50 66, 2003. [Chen et al., 2002] 
Yanyun Chen, Yingqing Xu, Baining Guo, and Heung-Yeung Shum. Modeling and rendering of realistic feathers. 
In Proceedings of the 29th annual conference on Computer graphics and interactive techniques, pages 630 
636. ACM Press, 2002. [chi Hsu and tsin Wong, 1995] Siu chi Hsu and Tien tsin Wong. Simulating dust accumulation. 
IEEE Comput. Graph. Appl., 15(1):18 22, 1995. [ChuandTai,2005]NelsonS.-H.ChuandChiew-LanTai.Moxi: real-timeinkdispersioninabsorbent 
paper. ACM Trans. Graph., 24(3):504 511, 2005. [Cockshott et al., 1992] T. Cockshott, J. Patterson, and 
D. England. Modelling the texture of paint. Computer Graphics Forum, 11(3):C217 C226, C476, ???? 1992. 
[Cook andTorrance,1982] R.L. CookandK.E.Torrance.A re.ectancemodelforcomputergraphics. ACM Transactions 
on Graphics, 1(1):7 24, January 1982. [Curtis et al., 1997] Cassidy J. Curtis, Sean E. Anderson, Joshua 
E. Seims, Kurt W. Fleischer, and David H. Salesin. Computer-generated watercolor. In Proceedings of the 
24th annual conference on Computer graphics and interactive techniques,pages421 430.ACM Press/Addison-WesleyPublish­ing 
Co., 1997. [Dai et al., 1995] Wen-Kai Dai, Zen-Chung Shih, and Ruei-Chuan Chang. Synthesizing feather 
tex­tures in galliformes. Computer Graphics Forum,14(3):407 420,August1995. ISSN1067-7055. 3  [Debevec 
et al., 2000] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark 
Sagar. Acquiring the re.ectance .eld of a human face. In Proceedings of the 27th annual conference on 
Computer graphics and interactive techniques, pages 145 156. ACM Press/Addison-Wesley Publishing Co., 
2000. [Desbenoit et al., 2004] Brett Desbenoit, Eric Galin, and Samir Akkouche. Simulating and modeling 
lichen growth. Computer Graphics Forum, 23(3):341 350, 2004. [Dorsey and Hanrahan, 1996] Julie Dorsey 
and Pat Hanrahan. Modeling and rendering of metallic patinas. In Proceedings of the 23rd annual conference 
on Computer graphics and interactive tech­niques, pages 387 396. ACM Press, 1996. [Dorsey et al., 1999] 
Julie Dorsey, Alan Edelman, Henrik Wann Jensen, Justin Legakis, and Hans Kohling Pedersen. Modeling and 
rendering of weathered stone. In Proceedings of the 26th annual conference on Computer graphics and interactive 
techniques, pages 225 234. ACM Press/Addison-Wesley Publishing Co., 1999. [Dumont-B`ecle et al.,2001] 
P.Dumont-B`ecle,A.Kemeny,S.Michelin,andD.Arqu`es.Multi-texturing approachforpaint appearancesimulation 
on virtual vehicles. Proceedings of the Driving Simulation Conference 2001, 213, 2001. [Eric Paquette, 
2001] George Drettakis Eric Paquette, Pierre Poulin. Surface aging by impacts. In Graphics Interface 
2001, pages 175 182, June 2001. [Ershov et al., 2001] Sergey Ershov, Konstantin Kolchin, and Karol Myszkowski. 
Rendering pearles­cent appearance based on paint-composition modelling. Computer Graphics Forum, 20(3), 
2001. ISSN 1067-7055. [Fowler et al., 1989] Deborah R. Fowler, James Hanan, and Przemyslaw Prusinkiewicz. 
Modelling spiral phyllotaxis. Computers and Graphics, 13(3):291 296,1989. [Fowler et al.,1992] DeborahR.Fowler,PrzemyslawPrusinkiewicz,andJohannesBattjes.Acollision­basedmodel 
of spiralphyllotaxis. In Proceedings of the 19th annual conference onComputergraphics and interactive 
techniques, pages 361 368. ACM Press, 1992. [FranzkeandDeussen,2003]OliverFranzkeandOliverDeussen. Renderingplantleavesfaithfully. 
In Proceedings of the SIGGRAPH 2003 conference on Sketches &#38; applications, pages 1 1. ACM Press, 
2003. [Fuchs et al.,2005] MartinFuchs,HendrikLensch,andHans-PeterSeidel.Re.ectancefromimages:A model-based 
approachforhumanfaces. IEEETransactions onVisualization andComputerGraphics, 11(3):296 305, 2005. Member-Volker 
Blanz. [Fuhrer et al.,] Martin Fuhrer, Henrik Wann Jensen, and Przemyslaw Prusinkiewicz. Modeling hairy 
plants. Graphical Models, 68(4), July. [Georghiades, 2003] Athinodoros S. Georghiades. Recovering 3-d 
shape and re.ectance from a small numberofphotographs. In EGRW 03:Proceedingsof the14thEurographicsworkshop 
onRendering, pages 230 240, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association. 
[Gobron and Chiba, 2001a] S. Gobron and N. Chiba. Simulation of peeling using 3d-surface cellular automata. 
In 9th Paci.c Graphics Conference on Computer Graphics and Applications, pages 338 347, Tokyo Japan, 
Oct 2001. IEEE. [Gobron and Chiba, 2001b] St´ephane Gobron and Norishige Chiba. Crack pattern simulation 
based on 3d surface cellular automata. The Visual Computer, 17(5):287 309, 2001. [Groeller et al., 1995] 
Eduard Groeller, Rene T. Rau, and Wolfgang Strasser. Modeling and visual­ization of knitwear. IEEE Transactions 
on Visualization and Computer Graphics, 1(4):302 310, 1995. [Gu et al.,2006]JinweiGu, Chien-ITu,RaviRamamoorthi,PeterBelhumeur,WojciechMatusik,and 
Shree Nayar. Time-varying surface appearance: acquisition, modeling and rendering. ACM Trans. Graph., 
25(3):762 771,2006. 4  [G¨unther et al., 2005] Johannes G¨unther, Tongbo Chen, Michael Goesele, Ingo 
Wald, and Hans-Peter Seidel. E.cient acquisition and realistic rendering of car paint. In G¨unther Greiner, 
Joachim Hornegger, Heinrich Niemann, and Marc Stamminger, editors, Proceedings of 10th International 
Fall Workshop -Vision, Modeling, and Visualization (VMV) 2005, pages 487 494. Akademische Verlagsgesellschaft 
Aka GmbH, November 2005. [Guy and Soler, 2004] Stephane Guy and Cyril Soler. Graphics gems revisited: 
fast and physically­based rendering ofgemstones. ACM Trans. Graph., 23(3):231 238,2004. [Hanrahan and 
Krueger, 1993] Pat Hanrahan and Wolfgang Krueger. Re.ection from layered surfaces due to subsurface scattering. 
In Proceedings of the 20th annual conference on Computer graphics and interactive techniques, pages 165 
174. ACM Press, 1993. [Hiebert et al., 2006] Brad Hiebert, Jubin Dave, Tae-Yong Kim, Ivan Neulander, 
Hans Rijpkema, and Will Telford. The chronicles of Narnia: the lion, the crowds and rhythm and hues. 
In SIGGRAPH 06: ACM SIGGRAPH 2006 Courses, page 1, New York, NY, USA, 2006. ACM Press. [Hirota et al.,1998] 
KoichiHirota,YasuyukiTanoue, andToyohisaKaneko. Generation of crackpat­terns with a physical model. The 
Visual Computer, 14(3):126 137, 1998. [Hirota et al., 2000] Koichi Hirota, Yasuyuki Tanoue, and Toyohisa 
Kaneko. Simulation of three­dimensional cracks. The Visual Computer, 16(7):371 378, 2000. [Jensen et 
al.,1999] HenrikWannJensen,JustinLegakis,andJulieDorsey.Renderingof wet material. In Dani Lischinski 
and Greg Ward Larson, editors, Rendering Techniques 99, Eurographics, pages 273 282. Springer-Verlag 
Wien New York, 1999. Proc. 10th Eurographics Rendering Workshop, Granada, Spain, June 21 23, 1999. [Jensen 
et al., 2001] Henrik Wann Jensen, Stephen R. Marschner, Marc Levoy, and Pat Hanrahan. A practical model 
for subsurface light transport. In Proceedings of the 28th annual conference on Computer graphics and 
interactive techniques, pages 511 518. ACM Press, 2001. [KajiyaandKay,1989]J.T.KajiyaandT.L.Kay.Renderingfurwith 
threedimensional textures. In Proceedings of the 16th annual conference on Computer graphics and interactive 
techniques, pages 271 280. ACM Press, 1989. [Koenderink and Pont, 2003] J. Koenderink and S. Pont. The 
secret of velvety skin. Machine Vision and Applications, 14(4):260 268,2003. [Koudelka, 2004] M.L. Koudelka. 
Capture, analysis and synthesis of textured surfaces with variation in illumination, viewpoint, and time. 
Yale University New Haven, CT, USA, 2004. [Krishnaswamy andBaranoski,2004] A.KrishnaswamyandG.V.G.Baranoski.ABiophysically-Based 
Spectral Model of Light Interaction with Human Skin. Computer Graphics Forum, 23(3):331 340, 2004. [Lafortune 
et al., 1997] Eric P. F. Lafortune, Sing-Choong Foo, Kenneth E. Torrance, and Donald P. Greenberg. Non-linear 
approximation of re.ectance functions. In Proceedings of the 24th annual conference on Computer graphics 
and interactive techniques, pages 117 126. ACM Press/Addison-Wesley Publishing Co., 1997. [Lam andBaranoski,2006] 
MichaelW.Y.LamandGladimirV.G.Baranoski.Apredictivelighttrans­port model for the human iris. Computer 
Graphics Forum, 25(3):359 368, 2006. [Lefebvre and Neyret, 2002] Sylvain Lefebvre and Fabrice Neyret. 
Synthesizing bark. In P. Debevec and S. Gibson, editors, 13th Eurographics Workshop on Rendering, Pisa, 
Italy, 2002. Eurographics Association. [Lefebvre and Poulin, 2000] Laurent Lefebvre and Pierre Poulin. 
Analysis and synthesis of structural textures. In Graphics Interface 2000, pages 77 86, May 2000. [Lefohn 
et al., 2003] Aaron Lefohn, Brian Budge, Peter Shirley, Richard Caruso, and Erik Reinhard. An ocularist 
sapproach tohumaniris synthesis. IEEE Comput Graphics Appl, 23(6):70 75,Novem­ber/December 2003. 5  
[Lokovic and Veach, 2000] Tom Lokovic and Eric Veach. Deep shadow maps. In SIGGRAPH 00: Proceedings of 
the 27th annual conference on Computer graphics and interactive techniques, pages 385 392, New York, 
NY, USA, 2000. ACM Press/Addison-Wesley Publishing Co. [Lu et al., 2005] Jianye Lu, Athinodoros S. Georghiades, 
Holly Rushmeier, Julie Dorsey, and Chen Xu. Synthesis of material drying history: Phenomenon modeling, 
transferring and rendering. In proceedings of Eurographics Workshop on Natural Phenomena, pages 7 16, 
2005. [Lu et al., 2007] Jianye Lu, Athinodoros S. Georghiades, Andreas Glaser, Hongzhi Wu, Li-Yi Wei, 
Baining Guo, Julie Dorsey, and Holly Rushmeier. Context-aware textures. ACM Trans. Graph., 26(1):3, 2007. 
[Marschner et al., 2000] Stephen R. Marschner, Brian K. Guenter, and Sashi Raghupathy. Model­ing and 
rendering for realistic facial animation. In Proceedings of the Eurographics Workshop on Rendering Techniques 
2000, pages 231 242, London, UK, 2000. Springer-Verlag. [Marschner et al.,2003] StephenR.Marschner,HenrikWannJensen,Mike 
Cammarano,SteveWorley, and Pat Hanrahan. Light scattering from human hair .bers. ACM Trans. Graph., 22(3):780 
791, 2003. [Marschner et al., 2005] Stephen R. Marschner, Stephen H. Westin, Adam Arbree, and Jonathan 
T. Moon. Measuring and modeling the appearance of .nished wood. ACM Trans. Graph., 24(3):727 734, 2005. 
[Merillou et al., 2000] S. Merillou, J.-M. Dischler, and D. Ghazanfarpour. A BRDF postprocess to integrateporosityon 
rendered surface. IEEE Transactions on Visualization and Computer Graphics, 6(4):306 318, October 2000. 
[Merillou et al., 2001a] S. Merillou, J.M. Dischler, and D. Ghazanfarpour. Surface scratches: measur­ing, 
modeling and rendering. The Visual Computer, 17(1):30 45, 2001. [Merillou et al., 2001b] Stephane Merillou, 
Jean-Michel Dischler, and Djamchid Ghazanfarpour. Cor­rosion: Simulating and rendering. In GI 2001, pages 
167 174, June 2001. [Moon andMarschner,2006] Jonathan T.MoonandStephenR.Marschner.Simulating multiplescat­tering 
in hair using a photon mapping approach. In SIGGRAPH 06: ACM SIGGRAPH 2006 Papers, pages 1067 1074, New 
York, NY, USA, 2006. ACM Press. [Nagata et al.,1997] NorikoNagata,ToshimasaDobashi,YoshitsuguManabe,TeruoUsami, 
andSeiji Inokuchi.Modeling andVisualizationfor aPearl-QualityEvaluationSimulator. IEEE Transactions on 
Visualization and Computer Graphics, 3(4):307 315, October 1997. [Nakamae et al.,1990] E.Nakamae,K.Kaneda, 
T.Okamoto,and T.Nishita.Alighting model aiming at drive simulators. In Proceedings of Siggraph 1990, 
pages 395 404. ACM SIGGRAPH, 1990. [Nishita et al., 1997] T. Nishita, H. Iwasaki, Y. Dobashi, and E. 
Nakamae. A modeling and rendering method for snowby using metaballs. Computer Graphics Forum,16(3):357 
364,August1997. ISSN 1067-7055. [Oren and Nayar, 1994] Michael Oren and Shree K. Nayar. Generalization 
of lambert s re.ectance model. In Proceedings of the21st annual conference onComputergraphics andinteractive 
techniques, pages 239 246. ACM Press, 1994. [Paquette et al., 2002] Eric Paquette, Pierre Poulin, and 
George Drettakis. The simulation of paint cracking and peeling. In Graphics Interface 2002, pages 59 
68, May 2002. [Phong, 1975] Bui Tuong Phong. Illumination for computer generated pictures. Commun. ACM, 
18(6):311 317, 1975. [Prusinkiewicz et al., 1988] Przemyslaw Prusinkiewicz, Aristid Lindenmayer, and 
James Hanan. De­velopment modelsofherbaceousplantsforcomputerimagerypurposes. In SIGGRAPH 88: Proceed­ings 
of the 15th annual conference on Computer graphics and interactive techniques,pages 141 150, New York, 
NY, USA, 1988. ACM Press. [Robertson, 1999] James Robertson. Forensic Examination of Human Hair. CRC 
Press, 1999. 6  [Rubin, 1998] Barry Rubin. Tailored Fiber Cross Sections. Advanced Materials, 10(15):1225 
1227, 1998. [Shimizu et al., 2003] Clement Shimizu, Gary W. Meyer, and Joseph P. Wingard. Interactive 
go­niochromatic color design. In Eleventh Color Imaging Conference, pages 16 22, 2003. [Stam,2001]JosStam.Anilluminationmodelforaskinlayerboundedby 
rough surfaces. In Proceed­ings of the 12th Eurographics Workshop on Rendering Techniques, pages 39 52, 
London, UK, 2001. Springer-Verlag. [Streit and Heidrich, 2002] L. Streit and W. Heidrich. A biologically-parameterized 
feather model. Computer Graphics Forum, 21(3):565 565, 2002. [Sun et al.,2001] YinlongSun,F.DavidFracchia,MarkS.Drew,andThomasW. 
Calvert.Aspectrally based framework for realistic image synthesis. The Visual Computer, 17(7):429 444,2001. 
[Sun, 2006] Yinlong Sun. Rendering biological iridescences with rgb-based renderers. ACM Trans. Graph., 
25(1):100 129,2006. [Takagi et al.,1990] AtsushiTakagi,HitoshiTakaoka,TetsuyaOshima,andYoshinoriOgata.Accurate 
rendering technique based on colorimetric conception. In Proceedings of the 17th annual conference on 
Computer graphics and interactive techniques, pages 263 272. ACM Press, 1990. [Vogelmann, 1993] C. Vogelmann. 
Plant tissue optics. Annual review of plant physiol. plant mol. biol., 44:231 251, 1993. [Walters, 2002] 
Kenneth A. Walters. Dermatological and Transdermal Formulations. Marcel Dekker Incorporated, 2002. [Wang 
et al.,2005] LifengWang,WenleWang,JulieDorsey,XuYang,BainingGuo,andHeung-Yeung Shum. Real-time rendering 
of plant leaves. In SIGGRAPH 05: ACM SIGGRAPH 2005 Papers, pages 712 719, New York, NY, USA, 2005. ACM 
Press. [Wang et al., 2006] Jiaping Wang, Xin Tong, Stephen Lin, Minghao Pan, Chao Wang, Hujun Bao, BainingGuo, 
andHeung-Yeung Shum. Appearance manifoldsfor modeling time-variant appearance of materials. ACM Trans. 
Graph., 25(3):754 761, 2006. [Ward, 1992] Gregory J. Ward. Measuring and modeling anisotropic re.ection. 
In Proceedings of the 19th annual conference onComputergraphics andinteractive techniques,pages265 272.ACMPress, 
1992. [Weyrich et al.,2006] TimWeyrich,WojciechMatusik,HanspeterP.ster,BerndBickel, CraigDonner, Chien 
Tu, Janet McAndless, Jinho Lee, Addy Ngan, Henrik Wann Jensen, and Markus Gross. Analysis of human faces 
using a measurement-based skin re.ectance model. In SIGGRAPH 06: ACM SIGGRAPH 2006 Papers, pages 1013 
1024, New York, NY, USA, 2006. ACM Press. [Xu et al., 2001] Ying-Qing Xu, Yanyun Chen, Stephen Lin, Hua 
Zhong, Enhua Wu, Baining Guo, and Heung-Yeung Shum. Photorealistic rendering of knitwear using the lumislice. 
In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 391 
398. ACM Press, 2001. [Yuan et al., 1988] Ying Yuan, Tosiyasu L. Kunii, Naota Inamato, and Lining Sun. 
Gemstone .re: Adaptive dispersive ray tracing ofpolyhedrons. The Visual Computer, 4(5):259 70,November1988. 
[Zinke and Weber, 2007] Arno Zinke and Andreas Weber. Light scattering from .laments. IEEE Trans. on 
Vis. and Comp. Graphics, 13(2):342 356,March/April 2007. 7    
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401141</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>24</pages>
		<display_no>6</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Input for participating media]]></title>
		<page_from>1</page_from>
		<page_to>24</page_to>
		<doi_number>10.1145/1401132.1401141</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401141</url>
		<abstract>
			<par><![CDATA[<p>Images of radiatively participating media are aesthetically appealing -- curls of smoke, sunsets, fires and clouds. Generating physically accurate, rather than artistic, images of participating media is an extremely challenging computational problem. In computer graphics, significant effort has gone into developing computational methods to account for attenuation and multiple scattering in participating media (e.g. [27], [4], [32],[18],[5],[24],[3],[31], [34],[33]). While such methods are still extremely time consuming, the problem is well understood. However, far less attention has been given into obtaining and/or modeling appropriate input for rendering participating media. In many cases, getting realistic input data is much more difficult than computing the light scattering. In this section we will consider what data is needed and some possible approaches for getting it.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098610</person_id>
				<author_profile_id><![CDATA[81100255828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Holly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rushmeier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[G. K. S. Batchelor. <i>An Introduction to Fluid Dynamics</i>. Cambridge University Press, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H. R. Baum, K. B. McGrattan, and R. G. Rehm. Simulation of smoke plumes from large pool fires. In <i>The Proceedings of the Twenty-fifth International Symposium on Combustion</i>. The Combustion Institute, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[N. Bhate. Application of rapid hierarchical radiosity to participating media. In <i>Proceedings of AATRV-93: Advanced Techniques in Animation, Rendering and Visualization</i>, pages 43--53, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P. Blasi, B. Le Sa&#235;c, and C. Schlick. A rendering algorithm for discrete volume density objects. In <i>Proceedings of Eurographics 1993</i>, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. F. Blinn. Light reflection functions for simulation of clouds and dusty surfaces. In <i>Proceedings of Siggraph 1982</i>, pages 21--29. ACM SIGGRAPH, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[C. Bohren. <i>Clouds in a Glass of Beer</i>. John Wiley and Sons, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[C. F. Bohren and D. R. Huffman. <i>Absorption and Scattering of Light by Small Particles</i>. Wiley, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[B. Collins. Visibility of exit signs in clear and smoky conditions. <i>Journal of the Illumination Engineering Society</i>, pages 69--83, Winter 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357293</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[R. Cook and K. E. Torrance. A reflectance model for computer graphics. <i>ACM Transactions on Graphics</i>, 1:7--24, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[R. A. Dobbins, G. W. Mulholland, and N. P. Bryner. Comparison of a fractal smoke optics model with light extinction measurements. <i>Atmospheric Environment</i>, 28(5):889--897, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[E. Braun et al. Comparison of full scale fire tests and a computer fire model of several smoke ejection experiments. NIST Internal Report 4961, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Faraday. <i>The Chemical History of a Candle</i>. Larlin, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R. J. Ferek, P. V. Hobbs, J. A. Herring, K. K. Laursen, and R. E. Weiss. Chemical composition of emissions from the kuwait oil fires. <i>Journal of Geophysical Research</i>, 97(D13):14483--14489, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[R. W. Fox, and A. T. McDonald. <i>Introduction to Fluid Mechanics</i>. Wiley, 1973.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325248</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[G. Y. Gardner. Visual simulation of clouds. In <i>Proceedings of Siggraph 1985</i>, pages 297--303. ACM SIGGRAPH, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. G. Gaydon and H. G. Wolfhard. <i>Flames, Their Structure, Radiation and Temperature</i>. Chapman and Hall, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[H. E. Gerber and E. E. Hindman. <i>Light Absorption by Aerosol Particles</i>. Spectrum Press, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[S. Haas and G. Sakas. Methods for efficient sampling of arbitrary distributed volume densities. In <i>Proceedings of the Eurographics Workshop on Photosimulation, Realism and Physics in Computer Graphics</i>, pages 215--227. INRIA-IRISA, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122738</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[X. D. He, K. E. Torrance, F. X. Sillion, and D. P. Greenberg. A comprehensive physical model for light reflection. In <i>Proceedings of Siggraph 1991</i>, pages 175--186. ACM SIGGRAPH, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[D. W. Hoock. Modeling time-dependent obscuration for simulated imaging of dust and smoke clouds. In <i>Characterization, Propagation and Simulation of Sources and Backgrounds</i>, pages 164--175. SPIE, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[W. R. Cofer III and et al. Kuwaiti oil fires: Compositions of source smoke. <i>Journal of Geophysical Research</i>, 97(D13):14521--14525, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>90972</ref_obj_id>
				<ref_obj_pid>90967</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. Inakage. A simple model of flames. In <i>Proceedings of Computer Graphics International</i>, pages 71--81, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[F. P. Incropera and D. P. DeWitt. <i>Fundamentals of Heat Transfer</i>. Wiley, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[J. T. Kajiya and B. P. Von Herzen. Ray tracing volume densities. In <i>Proceedings of Siggraph 1984</i>, pages 165--174. ACM SIGGRAPH, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[F. X. Kneizys, E. P. Shettle, G. P. Anderson, L. W. Abreu, J. H. Chetwynd, J. E. A. Selby, S. A. Cloug, and W. O. Gallery. <i>LOWTRAN 7 COMPUTER CODE: USER'S MANUAL AFGL-TR-88-0177</i>. Air Force Geophysics Laboratory, Hanscom AFB, MA, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[L. D. Landau and E. M. Lifshitz. <i>Fluid Mechanics</i>. Pergamon Press, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[E. Langu&#233;nou, K. Bouatouch, and M. Chelle. Global illumination in presence of participating media with general properties. In <i>Proceedings of the 5th Eurographics Workshop on Rendering</i>, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[M. Lesieur. <i>Turbulence in Fluids</i>. Kluwer, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[C. S. Li, F. T. Jenq, and W. H. Lin. Field characterization of submicron aerosols from indoor combustion sources. <i>Journal of Aerosol Science</i>, 23(S1):S547--S550, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[K. N. Liou. <i>Radiation and Cloud Processes in the Atmosphere</i>. The Oxford University Press, New York, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[N. Max. Efficient light propagation for multiple anisotropic volume scattering. In <i>Proceedings of 5th Eurographics Workshop on Rendering</i>, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37437</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, Y. Miyawaki, and E. Nakamae. A shading model for atmospheric scattering considering luminous intensity distribution of light sources. In <i>Proceedings of Siggraph 1987</i>, pages 303--308. ACM SIGGRAPH, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>914720</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[H. Rushmeier. <i>Realistic Image Synthesis for Scenes with Radiatively Participating Media</i>. PhD thesis, The Sibley School of Mechanical and Aerospace Engineering, Cornell University, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[H. Rushmeier and K. E. Torrance. The zonal method for calculating light intensities in the presence of a participating medium. In <i>Proceedings of Siggraph 1987</i>, pages 293--302. ACM SIGGRAPH, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[D. G. Shepherd. <i>Elements of Fluid Mechanics</i>. Harcourt, Brace and World, 1965.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[R. Siegel and J. Howell. <i>Thermal Radiation Heat Transfer</i>. Hemisphere Publishing Corporation, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166163</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[J. Stam and E. Fiume. Turbulent wind fields for gaseous phenomena. In <i>Proceedings of Siggraph 1993</i>, pages 369--376. ACM SIGGRAPH, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[H. Tennekes and J. L. Lumley. <i>A First Course in Turbulence</i>. The MIT Press, Cambridge, MA, 1972.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[H. C. van de Hulst. <i>Light Scattering by Small Particles</i>. Dover, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[W. H. White, D. J. Moore, and J. P. Lodge, editors. <i>Proceedings of the Symposium on Plumes and Visibility: Measurement and Model Components</i>, 1980. in a special issue of Atmospheric Environment, vol. 15, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Input for Participating Media Holly Rushmeier This originally appeared in the SIGGRAPH 95 course notes 
on input for global illumi­nation solutions 1 Introduction Images of radiatively participating media 
are aesthetically appealing curls of smoke, sunsets, .res and clouds. Generating physically accurate, 
rather than artistic, images of participating media is an extremely challenging computational problem. 
In computer graphics, signi.cant effort has gone into developing computational methods to account for 
attenuation and multiple scattering in participating media (e.g. [27], [4], [32],[18],[5],[24], [3],[31], 
[34],[33]). While such methods are still extremely time consuming, the problem is well understood. However, 
far less attention has been given into obtaining and/or modeling appropriate input for rendering participating 
media. In many cases, getting realistic input data is much more dif.cult than computing the light scattering. 
In this section we will consider what data is needed and some possible approaches for getting it. 2 
De.ning The Problem A reasonable place to begin is to de.ne the problem of physically accurate rendering 
of participating media. The geometry of rendering a scene containing a participating medium is shown 
in Fig. 1. As in rendering any realistic scene, the image is computed by .nding the radiance c o m (energy 
per unit time, solid angle and projected area)which would pass through an image pixel to the eye. To 
form a .nal image, a weighted average of this value must be found across the pixel (for antialiasing) 
and the spectral radiance distribution must be mapped to the gamut of the display. Unlike the surface 
problem, in which it is adequate to .nd the radiance of the closest visible surface, in the presence 
of a participating medium an integral along the line of sight must be evaluated. Along the line of sight, 
four processes may occur, absorption, out-scattering, in-scattering and emission. 2.1 Absorption  sca 
at the same wavelength Figure 3: Scattering out of participating medium. Some of the the incident light 
energy leaves the path as light traveling in a different direction. Figure 2 shows absorption some fraction 
of the beam of light is absorbed by the medium. The light energy does not disappear, it is converted 
into another form. The energy transferred to the medium causes it to increase in temperature, or the 
energy is conducted or convected away. The ability of the medium to absorb light is expressed as the 
absorption coef.cient d , the fraction by which the beam of light is reduced by absorption per unit length 
traveled along the line of sight. c e ((m  c (m (1) 2.2 Out-Scattering Figure 3 shows out-scattering 
 some fraction of the beam of light is scattered by the medium. This light is absorbed by the medium 
and immediately reradiated, but in directions that are different from the original path. The ability 
of the medium to scatter light out of the path is expressed as the scattering coef.cient , the fraction 
by which the beam light is reduced by scattering per unit length traveled along the line of sight. c 
c ((m  c (t (2) Bohren gives an example of a simple experiment that illustrates the difference between 
attenuation due to absorption and attenuation due to scattering. Referring to Fig. 4, place Figure 4: 
An experiment described by Bohren, illustrating attenuation by absorption and by out-scattering  two 
glass dishes of water on an overhead projector. Add ink to one dish, and milk to the other. Its possible 
to add ink and milk at rates such that the projection through the two dishes is the same on the screen 
-they have each attenuated the beam from the projector by the same fraction. However, the dish of ink 
will look much darker than the dish of milk. The ink has attenuated the beam by absorption, the milk 
has attenuated the beam by scattering. Bohren s book Clouds in a Glass of Beer[6] describes many other 
simple experiments that help develop a physical understanding of the interaction of visible light with 
participating media. Because they both attenuate the radiance of a beam of light, the absorption and 
scattering coef.cients are frequently combined into the extinction coef.cient, o: ot hc i c (3) c c o(m 
c o(m c( (4) The effect of scattering relative to the effect of outscattering is expressed as the single 
Figure 5: Scattering into the participating medium. Some incident light is scattered into the path. 
 scatter albedo of a medium: (5) Referring back to the milk and ink experiment, the two media have similar 
extinction coef.cients. The milk has a high albedo relative to the ink. 2.3 In-Scattering Scattering 
can also result in augmentation of the beam of light, as diagrammed in Fig. 5. In-scattering from beams 
of light from other directions can increase the radiance along a line of sight. When discussing out-scattering, 
the directionality of scattering was unimportant all that mattered was that light left the path. For 
in-scattering, the directionality of scattering is important to understand to what extent light from 
other directions is scattered into the path. The directionality of scattering is expressed by the scattering 
phase function o o c , where is the angle between the direction of scattering and the original path, 
as shown in Fig. 6. That is, forward scattering is in the direction for which is nearly zero. The phase 
function is a dimensionless quantity which is equal to the ratio of the radiance scattered in a particular 
direction c c s q to the radiance that would be scattered if the medium were .=0, forward .=p, backward 
 . incident direction Figure 6: De.nition of the angle in the scattering phase function. Figure 7: 
Rayleigh scattering phase function (isotropic shown with very light dotted line). isotropic (i.e. if 
the medium scattered equally in all .m directions ): o(o c c (o q c (s m)  (6) Two things to note about 
the phase function are that: o(o c the value ofis not bounded o(o q the function ofis normalized: .m 
 o(o q  (7) Scattering phase functions are shown in polar plots in Figs. 7 and 8.  Figure 8: Mie scattering 
for a 525 nanometer radius sphere with index of refraction 1.5 (not normalized.) dV ! ! ! ! ! ! ! ! ! 
! ! ! ! L(s+ds)=L(s)+dL em other forms of energy Figure 9: Emission in the participating medium. Energy 
in a form other than visible light enters the volume and causes the emission of visible light into the 
path. The increase in radiance along a direction Lthen, due to scattering from a beam of radiance b c 
o q from direction to path Lis tt no h d h o. Adding up all of the contributions from all directions 
gives the increase in direction s as: dd m c o c o o o c d (8)Lt 2.4 Emission Finally, radiance may 
increase in a path due to emission within a volume, as shown in Fig. 9. If the emission is due to thermal 
agitation of the medium, the increase is given by the product of the absorption coef.cient and the blackbody 
temperature of the medium b : dd L b (9) The reason the absorption coef.cient appears in both absorption 
and emission terms is based on thermodynamics. Brie.y, suppose a volume of medium at temperature is in 
an black (totally absorbing) environment . Both the volume and the environment emit radiation at a rate 
governed by . If the volume didn t emit radiation at temperature at the same rate it absorbed, it would 
spontaneously change temperature violating the laws of thermodynamics. This basic idea underlies the 
various reciprocity relationships in radiation(e.g. form factor reciprocity and reciprocity of the BRDF, 
see [36]).  Thermal emission is not the only type of emission that we see day to day. A notable exception 
are the .uorescent gases in .uorescent light .xtures. The emission can be expressed in the same form 
as Eq. 9, but the expression for obtaining is not the same. Putting together the four contributions 
to change in radiance along a path, the equation of transfer in a participating medium is: L c s(t L 
c o(m c o(m t m c o q s o o q L (10) (= In terms of extinction coef.cient and albedo, this can also 
be written: L c o(m L L(= c o(m t  o o m t c o c o o o c (11) The productL(is a dimensionless length 
in the medium called the optical differential oL thickness. Setting the function equal to this dimensionless 
length, Eq. 11 can also be written: L c o(m m L L  c t o  fj c s q o o s q (12) The optical thickness 
s(m (also called optical depth or opacity) of a path through the medium is just the integral of the optical 
differential thickness: o(m  oL(t (13) The extinction coef.cient expresses the effect a differential 
volume has on the incident light. The optical thickness of a medium expresses the effect of the entire 
extent of the medium. The optical thickness of a medium is a dimensionless length that can be used to 
compare the effects of volumes of medium. For example, a glass of milk of diameter 5 cm will attenuate 
a beam of light much more than the same glass .lled with cigarette smoke at a density typically found 
in a restaurant. However, a volume of milk with optical thickness 1 will attenuate a beam of light exactly 
as much as a volume of cigarette smoke with optical thickness 1. Looking at attenuation only, the radiance 
after traveling along a path s in a medium from a starting point at is: c s(t c s qtco c (14) The quantity 
 represents the fraction of light that emergences after traveling through a .nite extent of a medium, 
and is generally referred to as the transmittance . Note that is a function of a .nite extent of a medium, 
it is not a function of a differential volume at a point in the medium.  2.5 Summary of the Input Needed 
In addition to the input data required for a surface-only problem, the de.nition of a problem containing 
a participating medium requires the de.nitions of , o s q ,, and as functions of position in the medium. 
Unlike the surface problem in which geometry and re.ectance properties are treated entirely separately, 
the de.nition of the geometry of a participating medium and its properties are closely coupled. If ois 
given directly as a function of location, the geometry of the medium is implied. The distribution of 
the medium may also be speci.ed by giving partial pressure, volume fraction, or the density of the medium 
as a function of location. The values of oare computed by converting these quantities to densities, and 
using the mass coef.cients of extinction (i.e. (fraction extinction/length)/(mass density)). The spatial 
distribution of scattering particles an gases may be constructed (e.g. by thoroughly mixing milk into 
water), but more often in environments of interest in graphics, they are determined by complex natural 
processes. For most media, the spatial modeling problem is closer to the complexity of modeling plants 
and animals than it is to the complexity of modeling a chair or a desk. We will now look into determining 
input for participating media properties of and emission from a differential volume of medium, and the 
spatial distribution of participating media.  3 Properties and Emission for a Differential Volume Essentially 
there are two types of quantities we need at a differential volume at some point in space the properties 
of the medium, o, and o o q , and the emission . We will discuss properties .rst, and then turn to emission. 
Similar to the study of surface re.ectance, measured values of gas or particulate ab­sorption and scattering 
properties may be used directly, or analytical models may be used to calculate them from more fundamental 
measurements of optical properties and microscopic geometry. We begin with the analytical approaches. 
3.1 Analytical Models for Properties Similar to approximations of re.ectance at surfaces, there are two 
common approaches to modeling the properties of volumetric media geometric optics for particles that 
are large relative to the wavelength of light (e.g. as used in [9] for large surface roughness scales), 
and physical optics for smaller particles (e.g. as used in He [19] for smaller surface roughness scales). 
3.1.1 Geometric Optics Large Specular Spheres The geometry of light intersecting a large specular sphere 
is shown in Fig. 10a. The re.ectance of a specular re.ecting surface as a function of angle of incidenceo 
pis given by the Fresnel equations. Integrating over all incident angles gives  ab Figure 10: A ray 
striking a large specular (a) and diffuse(b) sphere. the hemispherical re.ectance m . Using the Fresnel 
re.ectance, the properties for a cloud large specular spheres, with a size distribution of N(R) spheres 
of radius R per unit volume, are [36]:d too c(15) s m  too c(16) o q R q R t q (17) The scattering and 
absorption coef.cients depend only on the number density of the particles (which gives the cross sectional 
area along the path which is blocked by particles) and the hemispherical re.ectance (which determines 
which fraction of the light which hits particles is absorbed and which is scattered). d Large Diffuse 
SpheresThe geometry of light intersecting a large diffuse sphere is shown in Fig. 10b. The values ofandare 
the given by the same expressions as for the specular case. However, the change in directional variation 
of the re.ectance results in the following the scattering phase function [36]: .(18) s q stcq Rainbows 
Geometric optics can also be used to approximate the scattering that results in rainbows [7]. Raindrops 
have diameters on the order of a millimeter, over 1000 times the wavelength of visible light. Raindrops 
essentially do not absorb the visible light, and scattering occurs as a result of internal re.ections 
and transmissions. In particular consider rays that are refracted, internally re.ectance and refracted 
again as in Fig. 11. Because  in parallel Figure 11: Ray paths that result in a rainbow. of the curved 
surface of the raindrop, and the fact that the index of refraction of water is greater than that of air, 
the rays are concentrated, or a caustic is formed. Because the index of refraction is different for different 
wavelengths, these concentrations are at different positions for different wavelengths, and we see a 
bow of colors, rather than just a bow of bright light. Rainbows occur when the angle of incidence s to 
the surface of the drop is equal to:t m m2 1 (19) The angle of scatter after a single internal re.ection 
is: + (20) Geometric optics cannot predict the correct radiance for a rainbow (the geometric optics 
theory breaks down, and a value of in.nity is obtained). However, Eqs.19 and 20 can be used to determine 
when rainbows can occur, and from which vantage points they will be visible. 3.1.2 Physical Optics For 
particles of arbitrary size, electromagnetic theory must be used to accurately develop an analytical 
expression for absorption and scattering. A solution of Maxwell s equations needs to be found for the 
electric and magnetic .elds inside the particle of interest, and outside of it. The absorption coef.cient, 
scattering coef.cient and scattering phase function can be found from this solution. The most famous 
solution of the problem is for the intersection of a plane wave with a sphere with an arbitrary radius 
and complex index of refraction f+apl , as shown in   hexagonalcubic ice crystal irregularsalt sootcrystal 
 agglomerate  Figure 13: Common particle shapes that are not modeled well by Mie theory. The Mie results 
are stated in this short form here not as a guide to computation, but to show that the solution is completely 
known, and that solutions can be obtained by computing enough terms in the in.nite series which are 
convergent. Code is available from many places to compute the Mie results, such as in the appendix to 
Bohren. Figure 8 shows results computed with this code. Just having a code to compute Mie scattering 
doesn t solve the input problem. The complex index of refraction of the media being modeled is required, 
as is a size distribution of the particles in the medium. Furthermore, although it is quite a detailed 
solution, it does require the assumption of spherical particles. This is probably a good assumption for 
atmospheric clouds composed of water droplets. Water has a complex index of refraction of y1.op1p3+ai 
y Og- . Reference ([30], p. 187) gives values for the size and number density of droplets, with radii 
of 4 f m and number densities of 300 per come-a3 being typical for atmospheric clouds composed of liquid 
water droplets (as opposed to clouds composed of ice crystals). Frequently, input for Mie calculations 
is not given directly, but must be extracted from reports. For example, a special issue of The Journal 
of Geophysical Research had several papers recording measurements of the smoke plumes from the Kuwaiti 
oil .res, eg. [13], [21]. Overall black plumes were found to be composed of elemental carbon particles, 
with a typical diameter of Oh.. efsm and density of 1000 1 :.::'; . This must be coupled with the information 
that elemental carbon has a complex index of refraction of y1.o 1p3+ .o 1 .i , and the mass density of 
solid carbon is 2ng /ncom3 . White smokes were found to be composed primarily of salts, with particles 
of 0.2 f m diameter and density of 1000 1o: : This must :; . be coupled with a typical complex index 
of refraction of salt of y1.o o+1O.i and mass density of solid salt of 2h..2 g /.c m3 . Mie theory doesn 
t give good results for some particles of interest in rendering, such as those shown in Fig. 13. Clouds 
composed of ice crystals are not well modeled with Mie theory [30]. Dobbins et al. [10] show that for 
irregularly shaped soot agglomerates, Mie scattering theory gives results for cross sections that can 
err by as much as a factor of two. A special case of Mie scattering theory is scattering from very small 
particles, generally known as Rayleigh scattering. For this case the series expansion for the scattering 
cross section of particles is:  1 D21 5D) 4a1( n.+ il.)2'5 1 (12 (27) ( n.+ il.)2+ and the scattering 
phase function is: ( ) (1+ e .O2 ) (28) Cigarette smoke consists of particles with diameters less than 
0.1 f m , and can be modeled as Rayleigh scatterers. Number densities of particulates in a room with 
a couple of smoldering cigarettes is on the order of 50,000 eme-a3 [29]. Since the scattering cross section 
is proportional to so;1 , much more light is scattered at short wavelengths (the blue end of the visible 
spectrum) than at longer wavelengths. As a result, scattered light from cigarette smoke generally looks 
bluish. Molecular scattering has the same phase function. However, rather than modeling a molecule as 
a particle with diameter D, the scattering cross section is given by [30],p. 166: n25a12 1e..0e9 135 
(29) 4 N2 where index of refraction is approximated by: ( n5 1)1 0.. 9 0+ h 150 50e010+ e55 5010(30) 
51935-21 55-2 ( in microns.) A typical value for the number density of molecules in the atmosphere N 
is .515 x1 01..emm- 3 . The attenuation coef.cient for molecular scattering becomes signi.cant only over 
dis­tances of kilometers. In the atmosphere, the s;1 dependence in Eq. 29 is apparent in the blue color 
of the sky. 3.2 Measured Properties Because measuring the shape, size distribution and optical properties 
of particles of common participating media can be extremely dif.cult, it is often easier to rely on measured 
values for scattering and absorption coef.cients. For example [17] describes a workshop on measuring 
the interaction of light with aerosol particles. Measurements of absorption coef.cients, mass of particles 
per unit volume of air, and albedo are given for various test cases using soot, methylene blue, salt 
and Arizona road dust. For example the samples of Arizona road dust had typical values of about x1 0-a6 
me-1 for absorption coef.cient, and 05. for albedo. To describe measured scattering distributions, .tting 
Mie parameters would be very tedious. Instead the Heyney-Greenstein function is generally used: 1 5g2 
( 5 g)(31) (1+ g25 ng e .O. ) z   The parameter 9 indicates the asymmetry of the distribution. Reference 
[30] gives typical values of r u , and 9 for cirrus clouds composed of ice crystals. For example, for 
cirrus uncinus, these values are 2.61 kame-h , .9999, and 0.84 respectively. 3.3 Emission Emission from 
volumetric media is generally rarer in rendering problems than absorption and scattering. One of the 
most prominent examples of emission is .ames. Most visible light from .ames comes from emission due to 
thermal agitation from soot. The blackbody radiance is given by Planck s equation: ' g0 a a2s e a.. 
.') 1.) (32) where 0is approximately 0.59544x1..-h 6 Wmf2 , and 0 is 14,388 smm . 2 A feature of the 
Planck distribution is that the product of the wavelength of peak emission aan c e and the temperature, 
is a constant. This is known as Wien s displacement law. It means that the higher the temperature the 
lower the peak wavelength. For low temperatures, aan'c like room temperature, is in the long, infrared 
wavelengths. The spectrum of sunlight is approximately the same as a blackbody at 5600 K, with a peak 
around blue in the visible spectrum. Obviously, the temperatures in .res differ. A typical pool .re temperature 
is on the order of 1000 K. In this temperature range the .ame will tend to look orange or yellow. Other 
types of particles may also have emission due to thermal agitation. Siegel and Howell [36] cite an example 
in rocket design in which aluminum oxide particles are introduced into exhaust, and contribute to the 
luminosity of the plume. Generally other colors in common .ames the blue color of a methane .ame do 
not come from thermal emission, but from electron transitions.  4 Spatial Distribution of Absorbing/Scattering 
Media Similar to properties and emission, the spatial distribution of a medium can be computed from a 
model, or can be obtained from measurements. 4.1 Fluid Mechanics In general, the distribution of participating 
media can be modeled analytically using the principles of .uid mechanics. There is neither space nor 
time to discuss particular methods for solving problems in .uid mechanics, and in this section we simply 
present some basic ideas and vocabulary for understanding literature in this area. A .uid is any substance 
that moves continuously under a shear stress. Both gases and liquids are .uids. A .uid is said to be 
Newtonian if this shear stress is linearly proportional to the velocity gradient in the medium. The constant 
of proportionality is the viscosity. The  ab Figure 14: Laminar (a) and turbulent (b) .ow over a sphere. 
viscosity indicates how thick the .uid is in the sense that maple syrup is much thicker than water. 
The motion of a .uid is governed by the equations of conservation of mass and energy and the Navier-Stokes 
equations. The conservation of mass equation is frequently referred to as the continuity equation. The 
Navier-Stokes equations express conservation of momentum in the .uid. Derivations of these non-linear 
differential equations can be found in any standard .uid mechanics or heat transfer undergraduate textbook 
(e.g. [14], [23]), or in more advanced texts such as [26] and [1]. Full solutions of the Navier-Stokes 
and mass and energy equations are rarely required for practical problems. For example some problems are 
isothermal, so the energy equation is not needed. In some problems viscous forces are very small compared 
to inertial forces, so inviscid equations can be used. When viscous forces are high relative to inertial 
forces, creep .ow equations can be used. Generally the .uids literature refers to two regimes of .ow 
 laminar and turbulent, diagrammed in Fig. 14. Laminar .ow is orderly and layered, while turbulent .ow 
is characterized by rapid .uctuations.Many .ows of interest in rendering such as the smoke plume from 
a large .re are turbulent. Flows are characterized by the Reynolds number, Re, which quanti.es the importance 
of inertial to viscous forces. Re is de.ned as . l where is the mass density, v is velocity is a characteristic 
length of the .ow and f is the viscosity. The characteristic length is measured differently for different 
types of .ows, as shown in Fig. 15. The transition from laminar to turbulent .ow occurs at a critical 
value of Re. In .ows below this value, perturbations are damped out before the .ow becomes unstable. 
In .ows with Re larger than the critical value,the perturbations grow. The Re number for which the transition 
occurs depends on the particular .ow geometry it takes on much different values for pipe .ows than for 
.ows over a .at plate. Futhermore, the Re for transition is not a sharp cut off transitions are experimentally 
observed over a range of numbers. Because of the complicated nature of the Navier-Stokes equations, critical 
values of Re have not been derived analytically.  fluid D x   Figure 15: De.nition of characteristic 
lengths for the Reynold s number for different types of .ow. Turbulence is the result of perturbations 
introduced into the .ow when the Reynold s number is high enough such as irregularities caused by surface 
roughness in a pipe. Generally pipe .ow becomes turbulent at about Re equal to 2000, but when conditions 
are carefully controlled, laminar .ow has been observed at Re up to 40,000 [35]. In fact there is no 
known upper limit to the Re at which laminar .ow could be observed, if no perturbations were introduced 
to the .ow. Interesting laminar .ows can be computed by direct solution of the Navier-Stokes equations. 
Mathematically, there are two ways that a .ow can be characterized. One way, referred to as the Lagrangian 
method, is to follow .uid particles through time. The other way, the Eulerian method, is to solve for 
the velocity at each point in space as a function of time. Generally, a solution for the main .uid (typically 
water or air) is computed with the Eulerian approach. A grid with velocities as a function of time is 
computed. The distribution of particulates (e.g. water droplets, soot, dust) which scatter and absorb 
light can then be found by following them as Lagrangian particles in this .ow .eld. In principle, solutions 
for turbulent .ow, like laminar .ow, could be computed by direct numerical solution of the Navier-Stokes 
equations. The problem is that the non-linearity of the equations requires that the numerical grid be 
capable of capturing the .uid .ow at a extremely wide range of length scales, to capture both the large 
scales of the .ow (e.g. the entire length of the .uid being studied) to the small scales at which .uctuations 
are .nally damped out by viscous dissipation. The range of length scales required grows with Re. Reference 
[28] gives the example of a small wind tunnel problem, where the length scale ranges from 50 mm for the 
size of the tunnel to 0.1 mm for the dissipation length scales. A solution would be needed at (.f10g/ 
1. .. or approximately 0 h points. For atmospheric phenomenon, solutions would be required on grids of 
on the order of 0 points. One alternative to direct numerical simulation is referred to as Large Eddy 
Simulations (LES)[2]. In this case an additional model is introduced to account for the effects of turbulence 
at subgrid length scales. The results of an LES calculation is show in Fig. 16. The use of subgrid models 
is limited, because the non-linearity of the .uid equations prevents a complete decoupling of the various 
length scales. This approach is only useful for the range of .ows for which the additional model of subgrid 
turbulence has been validated.  Because of the length scale problem in computing direct solutions, 
alternative ap­proaches have been developed for modeling turbulence. In particular, statistical methods 
and dimensional arguments have been used [38]. For example a .ow can be viewed as being composed of eddies 
of various lengths ranging from a characteristic length in the problem, to the length scale of viscous 
dissipation. One model is that energy is transferred from the largest scale eddies to the smallest, without 
loss. This process is referred to as the energy cascade. Using statistical arguments for the special 
case of homogeneous turbulence, the energy of eddies in this cascade scale according to the wavenumber 
of the eddy raised to a power. This power law can be used to determine a realistic spectrum of spatial 
and temporal variations to simulate homogeneous turbulence. A complete example of successfully using 
the energy cascade approach is given by Hoock [20] to a Gaussian plume which has an overall shape (centerline 
and width) shown in Fig. 17. The basic plume centerline is modeled with the plume height equal to downwind 
distance raised to a power with the coef.cient and power based on experimental observa­tions of various 
plumes. The basic centerline is then perturbed according to a model of wind conditions. The basic particulate 
concentration of the plume is a Gaussian distribution from the centerline. The width of the distribution 
increases along the centerline, and depends on an estimated rate of entrainment of ambient .uid into 
the plume. The base distribution is then perturbed by sinosoidal .uctuations in concentration. These 
.uctuations simulate turbulent eddies of various length scales. The amplitude of these .uctuations is 
inversely related to their spatial frequency to emulate the observed energy cascade in turbulent .ows. 
 In computer graphics, Stam and Fiume [37] have applied the approach of using a power law relationship 
between energy and length scales to compute realistic looking particulate distributions. Of course, as 
in the case of electromagnetic solutions for scattering, being able to solve a .uid mechanics problem 
doesn t solve the input problem for participating media. If a .uids model is to be used, the appropriate 
input for that computation has to be found i.e. initial and boundary conditions for the the velocities 
and pressures in the .eld. 4.2 Measured Density Distributions Because of the dif.culties in .nding solutions 
for the .uid .ows that frequently of interest in rendering, an alternative is to use measured distributions. 
As illustrated in the case of battle .eld plumes, experimental data can be found to model at least the 
overall spatial distribution of the participating media. Numerous studies in the .re science literature 
are available giving the mass distribution of smoke particles as crude (i.e. not well spatially resolved) 
functions of height and time [8], or the optical thickness in an enclosure as a function of height and 
time [11]. Liou [30] gives data for overall size distribution for atmospheric clouds, as well as data 
for the number density and composition droplet/crystal in the cloud. For example the size si to distribution 
of cumulus clouds per surface area observed from satellite photographs is given. This type of bulk data 
coupled with mathematical functions which mimic observed cloud shape, as presented by Gardner [15], could 
be used to rendering physically realistic clouds. There is no one combined source for data on spatial 
distributions of participating media. However, both particle characteristics and spatial distributions 
for particular types of .ows can be assembled for a particular problem from data presented in journals 
such as the The Journal of Geophysical Research(e.g. [13]), Atmospheric Environment (e.g. [40]), and 
Journal of Aerosol Science (e.g. [29]). 4.3 Fire The complexity of computing the spatial distribution 
of emitting, absorbing and scattering media is compounded in the case of .res. Not only are most .res 
turbulent, but the chemistry of the combustion process must be included in any type of physical simulation. 
 To an even greater extent, input for accurate models of .res needs to be obtained from observations 
and measurement. In computer graphics, this approach has been used by Inagake [22], using descriptions 
of .ame structure from Gaydon and Wolfard [16], and Faraday [12].  5 Existing Codes LOWTRAN Because 
attenuation and scattering through the atmosphere is so important in remote sensing applications, there 
is an extensive body of literature on this topic. For computation, many of the models for the transport 
of radiation have been included in the program LOWTRAN [25]. LOWTRAN is one of the most-used large scale 
scienti.c programs, and is cited widely in the remote sensing literature. Typical, kilometers long, lines 
of sight for which LOWTRAN is used to compute transmittance and radiance through the atmosphere are shown 
in Fig. 18. LOWTRAN was developed over decades at the Air Force Geophysics Laboratory (AFGL) to include 
a wide range of phenomena. A variety of model atmospheres can be selected, e.g. tropical, subartic summer, 
etc. Many different types of aerosols can be included such as fog, volcanic dust and typical desert aerosols. 
Clouds of different types can be speci.ed. Different models for rain can be used. The various models 
used for the properties of various atmospheric components are detailed in a long series of technical 
reports from AFGL. The name LOWTRAN comes from the relatively low spectral sampling for many atmospheric 
applications. The sampling is at 5 eemm-- , which is a low rate at the far infrared (wavenumber 20 eemm-- 
at .of 500 m). In the visible range (wave numbers on the order of 20,000 e mm-- ), it is a relatively 
high sampling rate for graphics researchers accustomed to sampling 3 wavelengths. LOWTRAN covers many 
phenomena and wavelengths (into the ultraviolet and out into the infrared) which are not of interest 
in visible image synthesis. And, as a FORTRAN program which has evolved over many years, the code itself 
is unwieldy to work with. However, for building a renderer for atmospheric effects, the LOWTRAN documentation 
is a good starting point for understanding the important effects to model,and the LOWTRAN code could 
be used to check the accuracy of line intergration of a visible image renderer.  6 Summary For most 
problems of interest in rendering, it is essentially impossible to obtain completely accurate input data 
for the properties, emission and spatial distribution of participating media. While there are detailed 
analytical solutions, such as the Mie scattering theory, for some aspects of the problem, these solutions 
require restrictive assumptions and input data that may also be dif.cult to obtain. Obtaining a physically 
accurate set of input data requires using a mix of analysis and measured data that are appropriate for 
the particular rendering problem at hand. In the introduction to A First Course in Turbulence[38], Tennekes 
and Lumley write: In turbulence the equations do not give the entire story. One must be willing to use 
(and capable of using) simple physical concepts based on experience to bridge the gap between the equations 
and the actual .ows. We do not want to imply that the equations are of little use; we merely want to 
make it unmistakably clear that turbulence needs spirited inventors just as badly as dedicated analysts. 
Similarly, for the entire problem of modeling input for participating media, invention based on the simple 
physical concepts is required as well as detailed mathematical analysis. References [1] G.K.S. Batchelor. 
An Introduction to Fluid Dynamics. Cambridge University Press, 1967. [2] H.R. Baum, K.B. McGrattan, and 
R.G. Rehm. Simulation of smoke plumes from large pool .res. In The Proceedings of the Twenty-.fth International 
Symposium on Combustion. The Combustion Institute, 1994. [3] N. Bhate. Application of rapid hierarchical 
radiosity to participating media. In Proceedings of AATRV-93: Advanced Techniques in Animation, Rendering 
and Visu­alization, pages 43 53, 1993. [4] P. Blasi, B. Le Sa¨ ec, and C. Schlick. A rendering algorithm 
for discrete volume density objects. In Proceedings of Eurographics 1993, 1993. [5] J.F. Blinn. Light 
re.ection functions for simulation of clouds and dusty surfaces. In Proceedings of Siggraph 1982, pages 
21 29. ACM SIGGRAPH, 1982. [6] C. Bohren. Clouds in a Glass of Beer. John Wiley and Sons, 1987. [7] 
C. F. Bohren and D. R. Huffman. Absorption and Scattering of Light by Small Particles. Wiley, 1983. [8] 
B. Collins. Visibility of exit signs in clear and smoky conditions. Journal of the Illumination Engineering 
Society, pages 69 83, Winter 1992. [9] R. Cook and K.E. Torrance. A re.ectance model for computer graphics. 
ACM Transactions on Graphics, 1:7 24, 1982. [10] R.A. Dobbins, G.W. Mulholland, and N.P. Bryner. Comparison 
of a fractal smoke op­tics model with light extinction measurements. Atmospheric Environment, 28(5):889 
897, 1994. [11] E. Braun et al. Comparison of full scale .re tests and a computer .re model of several 
smoke ejection experiments. NIST Internal Report 4961, 1992. [12] M. Faraday. The Chemical History of 
a Candle. Larlin, 1978. [13] R.J. Ferek, P. V. Hobbs, J.A. Herring, K.K. Laursen, and R.E. Weiss. Chemical 
composition of emissions from the kuwait oil .res. Journal of Geophysical Research, 97(D13):14483 14489, 
1992. [14] R.W. Fox, , and A.T. McDonald. Introduction to Fluid Mechanics. Wiley, 1973. [15] G.Y. Gardner. 
Visual simulation of clouds. In Proceedings of Siggraph 1985, pages 297 303. ACM SIGGRAPH, 1985. [16] 
A.G. Gaydon and H.G. Wolfhard. Flames, Their Structure, Radiation and Tempera­ture. Chapman and Hall, 
1979. [17] H.E. Gerber and E.E. Hindman. Light Absorption by Aerosol Particles. Spectrum Press, 1982. 
[18] S. Haas and G. Sakas. Methods for ef.cient sampling of arbitrary distributed volume densities. In 
Proceedings of the Eurographics Workshop on Photosimulation, Realism and Physics in Computer Graphics, 
pages 215 227. INRIA-IRISA, 1990. [19] X.D. He, K.E. Torrance, F.X. Sillion, and D.P. Greenberg. A comprehensive 
physical model for light re.ection. In Proceedings of Siggraph 1991, pages 175 186. ACM SIGGRAPH, 1991. 
[20] D.W. Hoock. Modeling time-dependent obscuration for simulated imaging of dust and smoke clouds. 
In Characterization, Propagation and Simulation of Sources and Backgrounds, pages 164 175. SPIE, 1991. 
[21] W.R. Cofer III and et al. Kuwaiti oil .res: Compositions of source smoke. Journal of Geophysical 
Research, 97(D13):14521 14525, 1992. [22] M. Inakage. A simple model of .ames. In Proceedings of Computer 
Graphics International, pages 71 81, 1989. [23] F.P. Incropera and D.P. DeWitt. Fundamentals of Heat 
Transfer. Wiley, 1981. [24] J.T. Kajiya and B.P. Von Herzen. Ray tracing volume densities. In Proceedings 
of Siggraph 1984, pages 165 174. ACM SIGGRAPH, 1984. [25] F.X. Kneizys, E.P. Shettle, G.P. Anderson, 
L.W. Abreu, J.H. Chetwynd, J.E.A. Selby, S.A. Cloug, and W.O. Gallery. LOWTRAN 7 COMPUTER CODE : USER 
S MANUAL AFGL-TR-88-0177. Air Force Geophysics Laboratory, Hanscom AFB, MA, 1988. [26] L.D. Landau and 
E.M. Lifshitz. Fluid Mechanics. Pergamon Press, 1989. [27] E. Langu´ Global illumination in presence 
of enou, K. Bouatouch, and M. Chelle. participating media with general properties. In Proceedings of 
the 5th Eurographics Workshop on Rendering, 1994. [28] M. Lesieur. Turbulence in Fluids. Kluwer, 1990. 
[29] C.S. Li, F.T. Jenq, and W.H. Lin. Field characterization of submicron aerosols from indoor combustion 
sources. Journal of Aerosol Science, 23(S1):S547 S550, 1992. [30] K.N. Liou. Radiation and Cloud Processes 
in the Atmosphere. The Oxford University Press, New York, 1992. [31] N. Max. Ef.cient light propagation 
for multiple anisotropic volume scattering. In Proceedings of 5th Eurographics Workshop on Rendering, 
1994. [32] T. Nishita, Y. Miyawaki, and E. Nakamae. A shading model for atmospheric scat­tering considering 
luminous intensity distribution of light sources. In Proceedings of Siggraph 1987, pages 303 308. ACM 
SIGGRAPH, 1987. [33] H. Rushmeier. Realistic Image Synthesis for Scenes with Radiatively Participating 
Media. PhD thesis, The Sibley School of Mechanical and Aerospace Engineering, Cornell University, 1988. 
[34] H. Rushmeier and K.E. Torrance. The zonal method for calculating light intensities in the presence 
of a participating medium. In Proceedings of Siggraph 1987, pages 293 302. ACM SIGGRAPH, 1987. [35] D.G. 
Shepherd. Elements of Fluid Mechanics. Harcourt, Brace and World, 1965. [36] R. Siegel and J. Howell. 
Thermal Radiation Heat Transfer. Hemisphere Publishing Corporation, 1981. [37] J. Stam and E. Fiume. 
Turbulent wind .elds for gaseous phenomena. In Proceedings of Siggraph 1993, pages 369 376. ACM SIGGRAPH, 
1993. [38] H. Tennekes and J.L. Lumley. A First Course in Turbulence. The MIT Press, Cambridge, MA, 
1972. [39] H.C. van de Hulst. Light Scattering by Small Particles. Dover, 1981. [40] W.H. White, D.J. 
Moore, and J.P. Lodge, editors. Proceedings of the Symposium on Plumes and Visibility: Measurement and 
Model Components, 1980. in a special issue of Atmospheric Environment, vol. 15,1981.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401142</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>12</pages>
		<display_no>7</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[The perception of simulated materials]]></title>
		<page_from>1</page_from>
		<page_to>12</page_to>
		<doi_number>10.1145/1401132.1401142</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401142</url>
		<abstract>
			<par><![CDATA[<p>Numerically modeling the interaction of light with materials is an essential step in generating realistic synthetic images. While there have been many studies of how people perceive physical materials, very little work has been done that facilitates efficient numerical modeling. Perceptual experiments and guidelines are needed for material measurement, specification and rendering. For measurement, many devices and methods have been developed for capturing spectral, directional and spatial variations of light/material interactions, but no guidelines exist for the accuracy required. For specification, only very preliminary work has been done to find meaningful parameters for users to search for and to select materials in software systems. For rendering, insight is needed on the perceptual impact of material models when combined with global illumination methods.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computer graphics]]></kw>
			<kw><![CDATA[materials]]></kw>
			<kw><![CDATA[realistic image synthesis]]></kw>
			<kw><![CDATA[reflectance]]></kw>
			<kw><![CDATA[textures]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Radiometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098611</person_id>
				<author_profile_id><![CDATA[81100255828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Holly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rushmeier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University, New Haven, CT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Adelson, "On seeing stuff: The perception of materials by humans and machines," <i>Proceedings of the SPIE</i> &#60;b&#62;4299&#60;/b&#62;, pp. 1--12, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[B. Hartung and D. Kersten, "Distinguishing shiny from matte," <i>Journal of Vision</i> &#60;b&#62;2&#60;/b&#62;(7), p. 551, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. Fleming, R. Dror, and E. Edelson, "Real-world illumination and the perception of surface reflectance properties," <i>Journal of Vision</i> &#60;b&#62;3&#60;/b&#62;(5), pp. 347--368, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1077409</ref_obj_id>
				<ref_obj_pid>1077399</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R. W. Fleming and H. H. B&#252;lthoff, "Low-level image cues in the perception of translucent materials," <i>ACM Trans. Appl. Percept</i>. &#60;b&#62;2&#60;/b&#62;(3), pp. 346--382, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1110654</ref_obj_id>
				<ref_obj_pid>1110637</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Colbert, S. Pattanaik, and J. Krivanek, "BRDF-Shop: creating physically correct bidirectional reflectance distribution functions," <i>IEEE Computer Graphics and Applications</i> &#60;b&#62;26&#60;/b&#62;(1), pp. 30--36, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141937</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E. A. Khan, E. Reinhard, R. W. Fleming, and H. H. B&#252;lthoff, "Image-based material editing," <i>ACM Trans. Graph.</i> &#60;b&#62;25&#60;/b&#62;(3), pp. 654--663, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311543</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. Ramasubramanian, S. N. Pattanaik, and D. P. Greenberg, "A perceptually based physical error metric for realistic image synthesis," in <i>SIGGRAPH '99: Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, pp. 73--82, ACM Press/Addison-Wesley Publishing Co., (New York, NY, USA), 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>343611</ref_obj_id>
				<ref_obj_pid>343593</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[V. Volevich, K. Myszkowski, A. Khodulev, and E. A. Kopylov, "Using the visual differences predictor to improve performance of progressive global illumination computation," <i>ACM Trans. Graph.</i> &#60;b&#62;19&#60;/b&#62;(2), pp. 122--161, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1077406</ref_obj_id>
				<ref_obj_pid>1077399</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. Howlett, J. Hamill, and C. O'Sullivan, "Predicting and evaluating saliency for simplified polygonal models," <i>ACM Trans. Appl. Percept.</i> &#60;b&#62;2&#60;/b&#62;(3), pp. 286--308, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073244</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[C. H. Lee, A. Varshney, and D. W. Jacobs, "Mesh saliency," in <i>SIGGRAPH '05: ACM SIGGRAPH 2005</i> Papers, pp. 659--666, ACM, (New York, NY, USA), 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732138</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Y. Sun, F. Fracchia, M. Drew, and T. Calvert, "Rendering Iridescent Colors of Optical Disks," <i>11th EUROGRAPHICS Workshop on Rendering (EGRW)</i>, pp. 341--352, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1122506</ref_obj_id>
				<ref_obj_pid>1122501</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Y. Sun, "Rendering biological iridescences with RGB-based renderers," <i>ACM Transactions on Graphics (TOG)</i> &#60;b&#62;25&#60;/b&#62;(1), pp. 100--129, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[F. Nicodemus, J. Richmond, J. Hsia, I. Ginsberg, and T. Limperis, "Geometric considerations and nomenclature for reflectance. Monograph 160, National Bureau of Standards (US)," October 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732297</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[A. Wilkie and W. Purgathofer, "Combined Rendering of Polarization and Fluorescence Effects," <i>Rendering Techniques 2001: Proceedings of the Eurographics Workshop in London, United Kingdom, June 25--27, 2001</i>, pp. 197--204, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[A. Glassner, "A model of fluorescence and phosphorescence," <i>Proc. of the Fifth Eurographics Workshop on Rendering</i>, pp. 57--68, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015708</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[S. Guy and C. Soler, "Graphics gems revisited: fast and physically-based rendering of gemstones," <i>International Conference on Computer Graphics and Interactive Techniques</i>, pp. 231--238, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357293</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[R. Cook and K. Torrance, "A Reflection Model for Computer Graphics," <i>ACM Transactions on Graphics</i> &#60;b&#62;1&#60;/b&#62;(1), pp. 7--24, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122738</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[X. He, K. Torrance, F. Sillion, and D. Greenberg, "A comprehensive physical model for light reflection," <i>SIGGRAPH 1991: Proceedings of the 18th annual conference on Computer graphics and interactive techniques</i>, pp. 175--186, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192213</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[M. Oren and S. Nayar, "Generalization of Lambert's reflectance model," <i>SIGGRAPH 1994: Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, pp. 239--246, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[S. Ershov, K. Kolchin, K., and Myszkowski, "Rendering Pearlescent Appearance Based On Paint-Composition Modelling," <i>Computer Graphics Forum</i> &#60;b&#62;20&#60;/b&#62;(3), pp. 227--238, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[S. Marschner, H. Jensen, M. Cammarano, S. Worley, and P. Hanrahan, "Light scattering from human hair fibers," <i>ACM Transactions on Graphics</i> &#60;b&#62;22&#60;/b&#62;(3), pp. 780--791, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[P. Irawan and S. Marschner, "A simple, accurate texture model for woven cotton cloth," Tech. Rep. PCG-06-01, Cornell University, Department of Computer Science, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1415107</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[P. Irawan, <i>Appearance of Woven Cloth</i>. PhD thesis, Cornell University, Ithaca, NY, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[T. A. Germer and C. C. Asmail, "Scattering and surface roughness," in <i>Proc. SPIE 3141</i>, Z.-H. Gu and A. A. Maradudin, eds., pp. 220--237, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927098</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[S. R. Marschner, <i>Inverse Rendering in Computer Graphics</i>. PhD thesis, Program of Computer Graphics, Cornell University, Ithaca, NY, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>636891</ref_obj_id>
				<ref_obj_pid>636886</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[H. P. A. Lensch, J. Kautz, M. Goesele, W. Heidrich, and H.-P. Seidel, "Image-based reconstruction of spatial appearance and geometric detail," <i>ACM Transactions on Graphics</i> &#60;b&#62;22&#60;/b&#62;, pp. 234--257, Apr. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[K. J. Dana, B. van Ginneken, S. K. Nayar, and J. J. Koenderinck, "Reflectance and texture of real-world surfaces," <i>ACM Transactions on Graphics</i> &#60;b&#62;18&#60;/b&#62;, pp. 1--34, Jan. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[R. S. Hunter and R. W. Harold, <i>The Measurement of Appearance</i>, John Wiley and Sons, New York, NY, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383318</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[H. Westlund and G. Meyer, "Applying appearance standards to light reflection models," <i>SIGGRAPH 2001:Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pp. 501--51, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[D. J. Heeger and J. R. Bergen, "Pyramid-based texture analysis/synthesis," in <i>SIGGRAPH '95: Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</i>, pp. 229--238, ACM, (New York, NY, USA), 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015724</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[R. Jagnow, J. Dorsey, and H. Rushmeier, "Stereological techniques for solid textures," in <i>SIGGRAPH '04: ACM SIGGRAPH 2004 Papers</i>, pp. 329--335, ACM, (New York, NY, USA), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278765</ref_obj_id>
				<ref_obj_pid>1278760</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[R. Jagnow, J. Dorsey, and H. Rushmeier, "Evaluation of methods for approximating shapes used to synthesize 3d solid textures," <i>ACM Transactions on Applied Perception</i> &#60;b&#62;4&#60;/b&#62;(4), pp. 24:1--24:27, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1189765</ref_obj_id>
				<ref_obj_pid>1189762</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[J. Lu, A. S. Georghiades, A. Glaser, H. Wu, L.-Y. Wei, B. Guo, J. Dorsey, and H. Rushmeier, "Context-aware textures," <i>ACM Trans. Graph.</i> &#60;b&#62;26&#60;/b&#62;(1), pp. 3:1--3:22, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141952</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[J. Gu, C.-I. Tu, R. Ramamoorthi, P. Belhumeur, W. Matusik, and S. Nayar, "Time-varying surface appearance: acquisition, modeling and rendering," in <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Papers</i>, pp. 762--771, ACM, (New York, NY, USA), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[K. Perlin, "An image synthesizer," in <i>SIGGRAPH '85: Proceedings of the 12th annual conference on Computer graphics and interactive techniques</i>, pp. 287--296, ACM, (New York, NY, USA), 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237267</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[S. Worley, "A cellular texture basis function," in <i>SIGGRAPH '96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</i>, pp. 291--294, ACM, (New York, NY, USA), 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[E. N. Wiebe, "Transfer of learning between 3d modeling systems," <i>Engineering Design Graphics Journal</i> &#60;b&#62;67&#60;/b&#62;(3), pp. 15--28, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1198694</ref_obj_id>
				<ref_obj_pid>1198555</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[J. Dorsey, H. Rushmeier, and F. Sillion, "Digital modeling of the appearance of materials," in <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Courses</i>, p. 1, ACM, (New York, NY, USA), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1557600</ref_obj_id>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[J. Dorsey, H. Rushmeier, and F. Sillion, <i>Digital Modeling of Material Appearance</i>, Morgan Kaufmann/Elsevier, Boston, MA, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[R. L. Cook, "Shade trees," in <i>SIGGRAPH '84: Proceedings of the 11th annual conference on Computer graphics and interactive techniques</i>, pp. 223--231, ACM, (New York, NY, USA), 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111425</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[M. McGuire, G. Stathis, H. Pfister, and S. Krishnamurthi, "Abstract shade trees," in <i>I3D '06: Proceedings of the 2006 symposium on Interactive 3D graphics and games</i>, pp. 79--86, ACM, (New York, NY, USA), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[R. R. Lewis, "Making shaders more physically plausible," <i>Computer Graphics Forum</i> &#60;b&#62;13&#60;/b&#62;, pp. 109--120, June 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1012568</ref_obj_id>
				<ref_obj_pid>1012551</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[E. Reinhard, P. Shirley, M. Ashikhmin, and T. Troscianko, "Second order image statistics in computer graphics," <i>Proceedings of the 1st Symposium on Applied perception in graphics and visualization</i>, pp. 99--106, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344812</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[F. Pellacini, J. Ferwerda, and D. Greenberg, "Toward a psychophysically-based light reflection model for image synthesis," <i>SIGGRAPH 2000: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pp. 55--64, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882343</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[W. Matusik, H. Pfister, M. Brand, and L. McMillan, "A data-driven reflectance model," <i>ACM Trans. Graph.</i> &#60;b&#62;22&#60;/b&#62;(3), pp. 759--769, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383945</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[A. Ngan, F. Durand, and W. Matusik, "Image driven navigation of analytical BRDF models," <i>Proceedings of the Eurographics Symposium on Rendering</i>, pp. 399--407, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[C. Shimizu, G. W. Meyer, and J. P. Wingard, "Interactive goniochromatic color design," in <i>Eleventh Color Imaging Conference: Color Science and Engineering Systems, Technologies, Applications Scottsdale</i>, Arizona, pp. 16--22, November 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1029165</ref_obj_id>
				<ref_obj_pid>1029164</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[S. Ershov, R. &#270;urikovi&#269;, K. Kolchin, and K. Myszkowski, "Reverse engineering approach to appearance-based design of metallic and pearlescent paints," <i>The Visual Computer</i> &#60;b&#62;20&#60;/b&#62;(8), pp. 586--600, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276473</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[P. Vangorp, J. Laurijssen, and P. Dutr&#233;, "The influence of shape on the perception of material reflectance," <i>ACM Trans. Graph.</i> &#60;b&#62;26&#60;/b&#62;(3), pp. 77:1--77:10, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276472</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[G. Ramanarayanan, J. Ferwerda, B. Walter, and K. Bala, "Visual equivalence: towards a new standard for image fidelity," <i>ACM Trans. Graph.</i> &#60;b&#62;26&#60;/b&#62;(3), pp. 76:1--76:12, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Perception of Simulated Materials Holly Rushmeier Yale University, PO Box 208285, New Haven, CT 
06520, USA ABSTRACT Numerically modeling the interaction of light with materials is an essential step 
in generating realistic synthetic images. While therehavebeen many studiesofhowpeopleperceivephysical 
materials,very little work hasbeen done that facilitates e.cientnumerical modeling. Perceptual experiments 
and guidelines are needed for material measurement,speci.cation and rendering. For measurement, manydevices 
and methodshavebeen developed for capturing spectral, directional and spatial variations of light/material 
interactions, but no guidelines exist for the accuracy required. Forspeci.cation,onlyvery preliminaryworkhasbeen 
doneto .nd meaningful parametersfor users to search for and to select materials in software systems. 
For rendering, insight is needed on theperceptual impact of material models when combined with global 
illumination methods. Keywords: Computer Graphics, Realistic Image Synthesis, Materials, Re.ectance, 
Textures 1. INTRODUCTION Generating realistic images is one of the goals of computer graphics image synthesis. 
Plausibly realistic images may be needed for applications such as feature .lms or games, or predictive 
realistic images may be needed in product design or training simulations. For either class of application, 
de.nitions of the materials that objects are madeof are needed. Numerous models for materialshavebeen 
developed, and databases of measured data for various materials have been compiled. Despite decades of 
research, the basic issues of what is required of a model to make a simulated material look plausible, 
or the accuracy required of a model for a simulation to faithfully reproduce appearance arepoorly understood. 
The purpose of this paper istobetter de.ne these issues in computer graphics with the goal of inspiring 
new lines of research inperception of materials. While theperception of shape has received much more 
attention,1 there is a growing body of research in human and computer vision on theperception of materials. 
Vision research starts at the opposite end of the imaging problem from where computer graphics starts. 
Broadly speaking vision starts with theperson or sensor, and considers what makes one material look di.erent 
from another. Examples of vision research that inform material simulation in computer graphics include 
Hartung and Kersten s work on distinguishing shiny from matte objects,2 Fleming et al. s on illumination 
and material perception3 , and Fleming andB¨ultho. s study of translucent materials.4 The work on matte 
versus specular explains the e.ectiveness of re.ection maps in displaying specular objects, even when 
the re.ected scene is not accurate. The work on illumination on material perception has led to the development 
of improved material selection interfaces such as BRDF-shop.5 The work on translucent materials resulted 
in a list of reliable heuristics that can be used to enhance the impression of translucency in a rendering. 
This further informed the work by Khan et al. that allows the substitution of materials in images and 
videos.6 However,because vision approaches start at thehuman/sensor end of the process, it canbe di.cult 
to trace the insights they obtain to improved methods for de.ning materials. Vision approaches often 
end with the analysis of images. The analysis of image statistics can be useful in constructing models 
for how humans classify materials, and in developing machine vision methods for the same task. In material 
models for graphics though we seek de.nitions that will apply to the generation of an in.nite number 
of di.erent images. Therehavebeen many successful applicationof insights fromperceptionto topicsrelatedto 
imagesynthesis such as global illumination calculations and geometric simpli.cation. In global illumination, 
successful work exploits the fact that illumination approximations can be computed progressively and 
the results monitored in image space where it ispossible to applyperceptual metrics.7, 8 In geometric 
simpli.cation metrics such as E-mail: holly@acm.org, Web: http://graphics.cs.yale.edu/holly  Figure1.A 
synthetic imageof an objectis formedby computing the visible lightreaching theeye as aresult of the illumination 
environment, and the shape and material of the object. Figure2.A material de.nition describeshow lightisredirected 
asa functionofwavelength, direction andposition. salience9, 10 can exploit the existence of high resolution 
model to compare its visual quality to a lower resolution model. More work is needed to apply similar 
approaches to modeling materials. There are three main areas where we need a characterization of theperception 
of materials for computer graphics: measurement,speci.cation and rendering. Therehavebeen recentlybeen 
promising e.ortsin tackling each of these areas with eitherperceptual experiments or principlesby computer 
graphics researchers. We will examine each area in turn, describe existing work and the open problems. 
We begin by describing the physical principlesand models that are used in de.ning computer graphics materials. 
2. UNDERLYING PHYSICS AND FIRST PRINCIPLES MODELS An image is formed, as illustrated in Figure 1, by 
computing the visible light reaching the eye through the image plane. This light depends on the shape 
of an object, the illumination environment, and the object material. A material description needs to 
de.ne how light incident on the object is redirected either scattered or absorbed. The redirection of 
light has three di.erent aspects as shown in Figure2 spectral, directional, and spatial. Thesetypes 
ofvariations correspond to di.erentwayswe describe our perception of materials wavelength variations 
resulting in color, directional variations resulting in matte, glossy and/or translucent appearance, 
and spatialvariations resultingin texture. The spatialvariations maybepointtopointvariationsinwavelengthand 
directionality, or may be small scale geometric variations such as pores, bumps or wrinkles. Modelingoflightand 
material couldbe doneatthe scaleof photonsand atoms. However,in practice geometric optics is almost always 
assumed in graphics (except for objects that clearly display di.raction and interference  Figure 3. 
The BSSRDF expresses how light at a particular wavelength is redirected as a function of the incident 
and exitantpositions and directions (Figurereproducedfrom NBS 160.13 ) Figure 4. The de.nition of what 
is a material and what is a collection of objects depends on scale. From far away (a) the beach appears 
to be covered by some material, on closer view in (b) and (c) it appears to be covered by small pebbles. 
e.ects such as compact discs11 or butter.y wings.12 ) Ageneral function for characterizing a material 
s interaction with light is the the bidirectional scattering-surface re.ectance distribution function 
(BSSRDF) S: S(., (xi ,Ti ). (xr ,Tr )) = dLr (.,xr . Tr )/Li (., xi . Ti )cos.i d.i , (1) The distribution 
function describes how light at a wavelength . entering the material at a point xi from a directionTi 
=(.i ,fi )is scattered and re-emerges atapointxr ina directionTr . Thepositions and directions are shown 
in Figure 3, which is reproduced from the National Bureau of Standards publication where the BSSRDF distribution 
was .rst de.ned in this manner.13 Whether something quali.es as a material speci.ed by a BSSRDF, or a 
material with BSSRDF and .ne geometric structure, ora collection ofobjects each composedofa material 
de.nedby a di.erent BSSRDFisa functionof scale. In Figure4a)people are standingonabeach coveredby somematerial 
thatappears tohave primarilywavelength variation withposition, at closer rangeinb) there are small geometric 
variations in the material, and even closer still in c), rather than appearing to be a material this 
looks like a pile of individual pebbles each composed of a somewhat di.erent material. Even with the 
geometric optics assumption the complicated form of the BSSRDF is not completely general it doesn t include 
behavior where the wavelength of light leaving the surface is di.erent from the incident (.uorescence), 
there is a time lag between the incident and leaving light (phosphorescence), or where there are polarization 
e.ects. All of these e.ects have been studied in computer graphics,14, 15 but are generally regarded 
16 as special cases that are only important for speci.c materials such as gems. For materials that do 
not have signi.cant subsurface scattering, for example metals, the bidirectional re­.ectance distribution 
function (BRDF) fr is used, which is just the BSSRDF with xi = xr : fr (., x,Ti . Tr )= dLr (., x . Tr 
)/Li (.,x . Ti )cos.i d.i , (2)  (a) (b) Figure 5. Holding a light next to a material can be used for 
a quick assessment of the e.ect of subsurface scattering. The wood on the left (a) doesn t appear to 
have subsurface scattering, while the rock on the right (b) does. The full BSSRDF has receiveda lotof 
attention recentlybecause of itsimportancein realistically rendering human skin, an important material 
in feature .lm and games. A full BSSRDFis not needed for all materials. However there is no speci.c de.nition 
of when it is adequate to use a BRDF rather than BSSRDF. Examples ofwhenitdoesanddoesnotappear tobe necessary 
areshowninFigure5.Theimageof rock ontherightof Figure 5 illustrates that not only may it be necessary, 
but it the BSSRF can depend on complicated macroscopic volumetric structure of the material. There are 
general physical constraints on the values that BSSRDF and BRDF. The fraction of light scattered can 
not be greater than one, so the integral of the re.ectance function over all possible leaving positions 
and angles is bounded. Light obeys reciprocity, so the value of the function is the same when the incident 
and re.ected directions (and positions in the case of BSSRDF) are reversed. There are also physical characteristics 
of the re.ectance/scattering functions for di.erent material types derived from electromagnetic theory. 
Metals conduct electricity, and so light is absorbed within metals within molecular length scales, causing 
metals to appear opaque. Solving Maxwell s equations for smooth surfaces results in the mirror re.ection 
law, Snell s law, and theFresnel re.ectance that increases with incident angle. Working from these .rst 
principles, a lot of work in graphics has gone into creating analytical physical BRDF (and to some extent 
BSSRDF) models based on detailed descriptions of surface microstructure. Examples include surface models 
such as the Cook-Torrance,17 He-Torrance18 and Oren-Nayar19 models, and models including substructure 
such as pigment particles and .akes as in Ershov et al. s paint model.20 First principle models in general 
areveri.ed by comparison tophysical measurements, but the most successful demonstrate perceptual e.ects. 
In the case of Cook and Torrance s work,17 they analyzed theFresnel re.ectances forspeci.c materialtypes. 
They note that most plastics consistent of a pigment particles in a clear binder material. The top smooth 
surface is composed of the binder which re.ects all wavelengths to approximately the degree. The wavelength 
re.ection from the smooth surface of metals however depends on the wavelength variation of the index 
of refraction of the metals. As a result they reasoned that a copper colored or gold colored plastic 
object looks like plastic because it has white highlights, while copper or gold metal look like metal 
because of their colored highlights. In another example OrenandNayar swork explainedthe abscenceofvery 
darkedgeson some matteobjects. They demonstrated that the orientation of small scale facets on the material 
results in back scatter, so that when viewed from near the light source direction materials can look 
relatively bright at edges where matte objects are dark. The appearance of metals versus dielectricobjects(howdowe 
tella anobjectis silver?) andimportance of backscatter (how noticeable is this under realistic illumination) 
remain issues to explore further. Therehas alsobeenwork recentlytomodelthe e.ectof bundlesor groupsof 
macroscopicgeomety on features of material appearance. An example is Marschner et al. s work on the bundles 
of .bers that form hair21 that predicts a hair re.ectance with secondary highlights. Another example 
is Irawan and Marschner s work on woven fabric that predicts unique highlight shapes depending on the 
.ber type and weave of the fabric.22, 23 3. MEASUREMENT Measurementis neededto reproducetheappearanceof 
existingphysical materials. Reasonsformodeling existing physical materials include using an existing 
material in a new design and documenting an existing object for studies in history or archaeology. Traditional 
BRDF measurement for applications in physics and engineering have been developed with high precision, 
measuring the light re.ected inavery narrow direction froma precisely directed incidentbeam. An example 
of such a precision device is the NIST STARR apparatus.24 Recognizing that the same precision is not 
economically feasible, and probably not required, relatively in­expensive image based re.ectance measurement 
systems have been developed for graphics applications. For example Marschner described an approach that 
coats a sphere with the material to be measured and captures an image with the re.ected light for many 
exitant directions at once.25 Lensch et al. developed an image based method for estimating the spatially 
varying re.ectance functions for materials on an existing object.26 Dana et al. invented the concept 
of a bidirectional texture function , which is captured by imaging large areas of a material for multiple 
incident light directions and view directions.27 While image based measurements have been successful, 
there are still only very small databases of such measurements available for use in graphics since they 
are tedious to make. Furthermore, unlike traditional engineering measurements they rarely include speci.c 
estimates of error. Such errors maybe high in absolute terms, but perhaps acceptable in terms of the 
appearance of the images the models are used in. The required accuracy for BRDF measurements in graphics 
has not been studied with any rigor. While the accuracy necessary obviously depends on the application, 
guidelines for a maximum accuracy required are needed for measurements that are done for applications 
such as widely used material databases and for documentation ofmuseum artifacts. Itis unclear how the 
accuracy shouldbe stated a certain percentage regardless of the raw value? What is the minimum sampling 
required? Are di.erent standards needed for di.erent classes of materials? Are di.erent angular sampling 
rates needed for di.erent spatial variations? Simply recommending that you should measure as much as 
you can a.ord doesn t adequately constrain the problem. What trade-o.s should be made between wavelength, 
spatial and angular samples? An alternative to the exhaustive NIST style high precision measurements, 
are practical measurements made for industrial appearance applications. Hunter and Harald28 de.ne numerous 
visual properties and describe devices designed to evaluate them by measuring scattering at limited numbers 
of incident and exitant angles. Westlund and Meyer29 explored deriving computer graphics re.ectance models 
from these industrial appearance measurement devices. Additional work is needed to .nd how well this 
approach works over a range of materials, and if the existing devices couldbe modi.ed or augmented to 
give acceptable material data for computer graphics applications. For textures, a simple method of measurement 
is to take an evenly lit image of a spatially varying surface. Perceptually informed methodshavebeendevelopedtocharacterized 
these measured datatoproduce unlimited expanses of the the same texture. For example, Heeger and Bergen30 
use the observation that textures appear the same when image statistics are matched. Two dimensional 
samples of three dimensional textures have alsobeen used to estimate the three dimensional texture usingaphysical 
model from stereologybyJagnow et al.31 In their work, materials composed of embedded large particles 
are considered, and the statistical distribution of size and density of praticles is estimated. In this 
case,perceptual experimentsweren tperformed, butaphysical comparison to simulationwas presented for the 
user to make their own judgment. It was assumed that viewing Figure 6 would lead to the conclusion that 
the new method producedbetter visual results for the class of materials studied. The stereology method 
proposed by Jagnow et al. requires estimating the shape of the three dimensional particles suspended 
in the materials. Choosing between di.erent methods for estimating shape from a two dimensional slices 
is not as clear cut a problem as determining whether a method that explicitly models particles producessuperior 
resultsfor materials consistingof particles. In subsequent workbyJagnowetal.32 methods for estimating 
particle shape were evaluated using psychophysical experiments comparing textures from a ground truth 
synthetic volume, and textures generated using di.erent particle estimate techniques. Since textures 
 Figure 6. An example showingperceptual di.erences, where it was assumed that extensive experiments 
weren t needed to show the advantage of one method over the other for a particular class of solid textures, 
from.31 Image (a) shows a sample 2D slice of material, and (b) an object made from the material. Images 
(c) and(d) show simulations using the method from stereology, and (e) a simulation using a previous method. 
 Figure 7. An example screen shot from a study described by Jagnow et al.32 to compare techniques for 
estimating embedded particle shape. The center image is a slice from a ground truth synthetic material. 
The observer is asked to select whether the left or right image came from the same material. One of the 
images was generated from the ground truth material, one from a texture generated with one of the particle 
estimation techniques being studied. aren texpectedtolookthe same pixelbypixel,theexperimentasked observers 
toselectwhichoftwo textures looked like it had come from the same material as the center texture, as 
shown in Figure 7 Recently researchershave attemptedto measure boththe spatialandtemporalvariationsin 
materialappear­ance due to weathering.33, 34 Since these variations depend on object geometry and environment, 
making use of the measured data ischallenging. What measurementsbesides BSSRDF need tobe made to completely 
docu­menttheweatheredappearance? Figure8showsa comparisonofanobject thatwasphysicallyweathered,anda synthetic 
image of the same object with weathering e.ects transferred from a measured object using re.ectance data 
augmented with geometric data. Such comparison cases are di.cult and time consuming to prepare. While 
there are similarities in the images, it isn t clear whether the observer gets the same impression of 
the weathered material what visual variations would there be if the same physical process were repeated 
many times? What sort of judgments is the observer meant to make from the image? Better de.nition of 
the weathering problem and design ofperceptual experiments are needed to gain more insight to move thisarea 
of measurement forward. 4. SPECIFICATION In graphics systems the user needs to specify the materials 
to attach to digital shapes. This may mean selecting from a large number of existing materials, or editing 
an existing material. 4.1 Current Interfaces Current interfaces used to specify materials from some popular 
graphics systems are shown in Figure 9. Each interface is complicated and brings up multiple sub-menus, 
and not all are shown here. No standards are followed. However, all of the interfaces are built around 
the computer graphics data structures used to de.ne components of material appearance rather thanperceptual 
features. Spectral characteristics arespeci.ed withRGB color,  Figure8.An example comparisonofthe same 
objectphysically(right)andsynthetically (left)weatheredfromLuetal.33 Perceptual tests would be useful 
to evaluate the synthetic results, but would be di.cult to conduct. Figure9.Interfaces for materialspeci.cation 
from Blender (top),Maya(middle) andCinema4D(bottom). directionality is speci.ed with common graphics 
re.ectance models and their parameters(e.g. Oren-Nayar or Cook-Torrance),and spatial variations aregivenby 
texture mapsorprocedural functions. Procedural functions that de.ne spatial variations in terms of basis 
functions spanning subsets of all variations possible arePerlin noise35 and Worley cellular textures.36 
Systems such as Blender http://www.blender.org/, Maya http://www.autodesk.com/, and Cinema4D http://www.maxon.com 
provide the capability to de.ne a wide range of materials. In a proof by marketplace these interfaces 
are e.ective in the sense that users invest a lot of time and/or money in using them to de.ne materials 
that are used to generate compelling realistic images. However, the learning curve for each system is 
high, and being an expert in one system does not transfer into learning another quickly In the domain 
of geometry creation in computer aided design (CAD), there is evidence that techniques based on fundamental 
design principles allow users to learn new packages quickly. Weibe37 measured the ability of engineering 
students to learn and use a new commercial CAD modeling software package after learning a .rst package. 
While this wasa limited and early study, an insight gained was that studentswere apparentlybetter able 
to transfer skillsbetween packages thatbetter incorporated basic design principles -in this case the 
use of constraint-based modeling. By rethinking the material speci.cation process in terms of material 
appearance rather than computer graphics data structures and models, it maybepossible tovastly improve 
the interfaces over those shown in Figure 9 and make them easier to learn. How natural cana material 
interface be? Howmuch should aperson need to learn tospec.y materials? One approach is to learn to observe 
materials to decompose them into spectral, directional and spatial e.ects, without reference tospeci.c 
computer graphics models. Examplesof thistypeof approachhavebeen givenby Dorsey et al.38, 39 This follows 
the tradition of teaching observation in art, although with a di.erent view of how appearance shouldbe 
analyzed. Perhaps alternative approaches to observation and understanding the features of appearance 
that can be transferred into numerical speci.cations can be developed. Di.erent interfaces havebeen proposed 
that appear quite di.erent from the sliders and radio buttons common in current systems. In BRDF-Shop 
the user paints the appearance desired, however the system is limited to selecting spatially uniform 
BRDF s. Painting avoids having to decompose the appearance, or to even think of terms to describe it. 
A hazard of painting, even if generalized for spatially varying descriptions, is that it is always tied 
to a speci.c illumination, shape and scale. From a coding point of view, a complete material speci.cation 
is expressed in software referred to as a shader. Elaborate and e.cient shaders canbe writtenby programmers 
withknowledge of graphics hardware. Cook40 developed the idea of a shade tree that describes how various 
textures and re.ectance models are combined to form a particular material behavior. To allow artist/designers 
to generate e.cient shader code McGuire et al.41 developed a visual programming interface for users to 
specify a material as an abstract shade tree that can thenbe e.ciently encoded forvarious graphics hardware 
systems. While the visual interface gives the user a di.erent view of how the various graphics models 
are applied, it doesn t represent a di.erent set of structures and parameters for the user to express 
the appearance of the material they want to use. 4.2 Creation: Plausibility and Feasibility Current 
graphics systems and shader programming languages allow the de.nition of materials that could not physically 
exist. For some applications,being able tophysically produce thematerial may notbe important, but generally 
it is important that the material look plausible. A material should somehow relate to our experience 
of viewing materials in the real world. In most cases a large fraction of the parameter space for di.erent 
types of materials in graphics systems produce materials that look unnatural. Energy conservation and 
reciprocity, discussed in Section 2, are the only constraints that have been proposed for making a plausible 
material.42 However, it isn t clear that these constraints are necessary or su.cient for a material to 
appear plausible. More study is needed to understand to what extent conservation and reciprocity are 
required, and whether there are other constraints on the nature of the directionality of re.ection. There 
is also more study needed in the area of plausible textures. Perlin noise functions are used to simulate 
natural irregularities in materials. Reinhard et al.43 compared commonly recommendedPerlin model parameters, 
and found that theyvaried from commonly accepted second order image statistics. BothPerlin noise andWorley 
cellular textureshavebeen used for many natural materials, without aspeci.c studyof the natural appearance 
for various combinations of parameters. 4.3 Navigation One problem in existing systems is that even 
when a material de.nition is found that is in some sense close to the desired material, it is di.cult 
to .nd the adjustments to move closer. Several researchers have begun looking at developingperceptual 
dimensions for graphics controls to allow navigation in material space. Work by Pellacini et al.44 establishedperceptual 
dimensions for gloss to use in adjusting the parameters of a simple re.ectance model. Matusik et al.45 
expanded the dimensionality of this approach by using a collection of measured BRDF s and asking a user 
to classify their appearance according to a predetermined set of traits such as roughness and greasiness 
. Using these traits, they were able to organize the measured data in a manner that would let the user 
navigate between the data set according to these traits. Expanding this to larger numbers of traits, 
material types and users has not been attempted. Determining meaningful parameters andperceptually equal 
sized stepsin parameter space requiresnumerous perceptual experiments. Ngan et al.46 propose simplifying 
the problem by representing the materials as images. Steps in any parameter direction can be represented 
by images around the current material de.nition, and equal sized steps canbe determined using image metrics. 
This converts the problem from running experiments identifying perceptual traits to identifying suitable 
illumination, shapes andposes to use in the selection of material. 4.4 Material Classi.cation There 
are clearly di.erent classes of materials in the world, and it isn t clear that there is a continuous 
space of all reasonable possibilities. From a practical point of view it isn t likely that someone trying 
to specify a material correspondingto irridiscentbird feathers would startoutwithapieceofgraniteandadjustindependent 
parameters until the result looked correct. It is more likely that aperson would takea catalog approach, 
and select a material type. This approach is used in the LightWorks rendering system http://www.lightworks-user.com/ 
which o.ers catalogs of materials from the manufacturers of architectural products such as Sherwin-Williams 
paints, Westbond Carpets, and MoldTech plastics. Third party materials are authored in a proprietary 
LWA format through software sold by LightWorks. In this case, the material accuracy may not be veri.ed 
by any psychophysical testing, but the interest of manufacturers in selling product over a long term 
provides motivation for the material models to be reliable. However, how can the catalog approachbe extended 
to materials that don t existyet and need tobe speci.ed? After selecting a class of materials, and a 
typical instance, how can the user adjust it? Are there some universal knobs that are appropriate (e.g. 
shinier), or are they always material speci.c? For a very narrow range of materialsina particular editor 
canbecomposed such asthe metallic carpaint color editorby Shimizu etal.47 It shouldbe noted that this 
editor is primarily for experimenting with particular aspects of appearance deemed importantfor one design 
approach.A more complex editorisproposedby Ershovetal.48 The complexity is the result of designing a 
material in terms of a physical recipe for actually producing the paint. The granite and leather examples 
shown in Figure 10 from the Maya rendering package are examples for a more general catalog. Maya o.ers 
options for material types such as the leather and snow shown here. However the parameters are in terms 
ofa cellular texturing procedure (for leather) anda subsurface scattering model (for snow), rather than 
the grain and patina terms people might use for leather appearance, or the size of crystals that might 
be used for snow. A basic question is whether given a type of material if named dimensions that are recognized 
are needed, or whether it is adequate to determine the number of dimensions and let the user experiment 
with varying the parameters. 4.5 Search and Retrieval With the high level of activity in graphics around 
the world, rather than building geometries or textures an alternativehasbecome touse anengine such asGoogleto 
searchforthe desireddata. Awidevariety ofimage search engines have been developed, and a geometry search 
engine is available from the Princeton Graphics Group at http://shape.cs.princeton.edu/search.html. There 
are two obstacles to the development of a materials search engine: A standardized format for material 
descriptions equivalent to the VRML, OBJ formats for geometry or JPG and TIF for images has not emerged. 
 A reliable method to make signatures for materials to run queries against has notbeen developed.  
 Even if the full material model isn t expressed in a standard form, some standard representations of 
the mate­rial such as imagesonspeci.cobjects underspeci.c illumination(as proposedbyNganfor material 
navigation) could be added as metadata to any material de.nition. Textual tags could also be added. Research 
is need to .nd the representations and tags that could reliably be used for material retrieval. 5. RENDERING 
The purpose of de.ning materials is to use them in rendering images. Anyrendering processis limitedbymemory 
or time. Regardless of the available resources, it is always advantageous to use the most compact de.nition 
of any graphics input that is adequate for the current rendering. Finding simpli.ed material models for 
speci.c renderings is the area that has the greatestpotential for immediate improvements, since it can 
be most easily related to image metrics. In this area recent work by Vangorp et al.49 studied the interaction 
of object shape and material re.ectance in appearance. They found that an object s material can look 
quite di.erent depending on the properties of the shape of the object. Ramanarayanan et al.50 introduced 
the idea of visual equivalence to describe objects in images that have the same appearance, even though 
the images have noticeable di.erences when examined in detail. They de.ne a visual equivalence predictor 
(VEP) that they can apply to a shape, simple re.ectance model, and illumination description. Using the 
VEP allows the calculation of an image that is visually equivalent to an exact rendering using reduced 
resources. These results from both Vangorp et al. and Ramanarayanan et al. indicate the possibility of 
substantially adjustingthe resources devotedtoa material descriptionobjectby object,and scene by scene. 
REFERENCES 1. E. Adelson, On seeing stu.: Theperception of materials byhumans and machines, Proceedings 
of the SPIE 4299, pp. 1 12, 2001. 2. B. Hartung and D. Kersten, Distinguishing shiny from matte, Journal 
of Vision 2(7), p. 551, 2002. 3. R. Fleming, R. Dror, and E. Edelson, Real-world illumination and the 
perception of surface re.ectance properties, Journal of Vision 3(5), pp. 347 368, 2003. 4. R. W. Fleming 
and H. H.B¨ultho., Low-level image cues in theperception of translucent materials, ACM Trans. Appl. Percept. 
2(3), pp. 346 382, 2005. 5. M. Colbert, S. Pattanaik, and J. Krivanek, BRDF-Shop: creating physically 
correct bidirectional re­.ectance distribution functions, IEEE Computer Graphics and Applications 26(1), 
pp. 30 36, 2006. 6. E. A. Khan, E. Reinhard, R. W. Fleming, and H. H. B¨ultho., Image-based material 
editing, ACM Trans. Graph. 25(3), pp. 654 663, 2006. 7. M. Ramasubramanian, S. N. Pattanaik, and D. 
P. Greenberg, A perceptually based physical error metric for realistic image synthesis, in SIGGRAPH 99: 
Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pp. 73 82, 
ACM Press/Addison-Wesley Publishing Co., (New York, NY, USA), 1999. 8. V. Volevich, K. Myszkowski, A. 
Khodulev, and E. A. Kopylov, Using the visual di.erences predictor to improveperformance of progressive 
global illumination computation, ACM Trans. Graph. 19(2), pp. 122 161, 2000. 9. S. Howlett, J. Hamill, 
and C. O Sullivan, Predicting and evaluating saliency for simpli.edpolygonal mod­els, ACM Trans. Appl. 
Percept. 2(3), pp. 286 308, 2005. 10. C. H. Lee, A. Varshney, and D. W. Jacobs, Mesh saliency, in SIGGRAPH 
05: ACM SIGGRAPH 2005 Papers, pp. 659 666, ACM, (New York, NY, USA), 2005. 11. Y. Sun, F. Fracchia, 
M. Drew, and T. Calvert, Rendering Iridescent Colors of Optical Disks, 11th EU-ROGRAPHICS Workshop on 
Rendering (EGRW) , pp. 341 352, 2000. 12. Y. Sun, Rendering biological iridescences with RGB-based renderers, 
ACM Transactions on Graphics (TOG) 25(1), pp. 100 129, 2006. 13. F. Nicodemus, J. Richmond, J. Hsia, 
I. Ginsberg, and T. Limperis, Geometric considerations and nomen­clature for re.ectance. Monograph 160, 
National Bureau of Standards (US), October1977.  14. A. Wilkie and W. Purgathofer, Combined Rendering 
of Polarization and Fluorescence E.ects, Rendering Techniques 2001: Proceedings of the Eurographics Workshop 
in London, United Kingdom, June 25-27, 2001 , pp. 197 204, 2001. 15. A. Glassner, A model of .uorescence 
and phosphorescence, Proc. of the Fifth Eurographics Workshop on Rendering , pp. 57 68, 1994. 16. S. 
Guy and C. Soler, Graphics gems revisited: fast and physically-based rendering of gemstones, Interna­tional 
Conference on Computer Graphics and Interactive Techniques , pp. 231 238, 2004. 17. R. Cook and K. Torrance, 
A Re.ection Model for Computer Graphics, ACM Transactions on Graph­ics 1(1), pp. 7 24, 1982.  18. X. 
He, K. Torrance, F. Sillion, and D. Greenberg, A comprehensive physical model for light re.ection, SIGGRAPH 
1991: Proceedings of the 18th annual conference on Computer graphics and interactive tech­niques , pp. 
175 186, 1991. 19. M. Oren and S. Nayar, Generalization of Lambert s re.ectance model, SIGGRAPH 1994: 
Proceedings of the 21st annual conference on Computer graphics and interactive techniques , pp. 239 246, 
1994. 20. S. Ershov, K. Kolchin, K., and Myszkowski, Rendering Pearlescent Appearance Based On Paint-Composition 
Modelling, Computer Graphics Forum 20(3), pp. 227 238, 2001. 21. S. Marschner, H. Jensen, M. Cammarano, 
S. Worley, and P. Hanrahan, Light scattering from human hair .bers, ACM Transactions on Graphics 22(3), 
pp. 780 791, 2003. 22. P. Irawan and S. Marschner, A simple, accurate texture model for woven cotton 
cloth, Tech. Rep. PCG­06-01, Cornell University, Department of Computer Science, 2006. 23. P. Irawan, 
Appearance of Woven Cloth. PhD thesis, Cornell University, Ithaca, NY, 2007. 24. T. A. Germer and C. 
C. Asmail, Scattering and surface roughness, in Proc. SPIE 3141, Z.-H. Gu and A. A. Maradudin, eds., 
pp. 220 237, 1997. 25. S. R. Marschner, Inverse Rendering in Computer Graphics. PhD thesis, Program 
of Computer Graphics, Cornell University, Ithaca, NY, 1998. 26. H.P.A. Lensch,J. Kautz,M.Goesele,W. 
Heidrich, and H.-P. Seidel, Image-basedreconstructionof spatial appearance and geometric detail, ACM 
Transactions on Graphics 22, pp. 234 257, Apr. 2003. 27. K.J. Dana,B.van Ginneken,S.K.Nayar, andJ.J.Koenderinck, 
Re.ectance and texture of real-world surfaces, ACM Transactions on Graphics 18, pp. 1 34, Jan. 1999. 
 28. R. S. Hunter and R. W. Harold, The Measurement of Appearance, John Wiley and Sons, New York, NY, 
1987. 29. H. Westlund and G. Meyer, Applying appearance standards to light re.ection models, SIGGRAPH 
2001:Proceedings of the 28th annual conference on Computer graphics and interactive techniques , pp. 
501 51, 2001.  30. D. J. Heeger and J. R. Bergen, Pyramid-based texture analysis/synthesis, in SIGGRAPH 
95: Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, pp. 229 
238, ACM, (New York, NY, USA), 1995. 31. R. Jagnow, J. Dorsey, and H. Rushmeier, Stereological techniques 
for solid textures, in SIGGRAPH 04: ACM SIGGRAPH 2004 Papers, pp. 329 335, ACM, (New York, NY, USA), 
2004. 32. R. Jagnow, J. Dorsey, and H. Rushmeier, Evaluation of methods for approximating shapes used 
to synthe­size 3d solid textures, ACM Transactions on Applied Perception 4(4), pp. 24:1 24:27, 2008. 
 33. J.Lu,A.S. Georghiades,A. Glaser,H.Wu, L.-Y.Wei,B.Guo,J. Dorsey,andH. Rushmeier, Context-aware textures, 
ACM Trans. Graph. 26(1), pp. 3:1 3:22, 2007. 34. J. Gu, C.-I.Tu, R. Ramamoorthi,P. Belhumeur, W. Matusik, 
and S. Nayar, Time-varying surface appear­ance: acquisition, modeling and rendering, in SIGGRAPH 06: 
ACM SIGGRAPH 2006 Papers, pp. 762 771, ACM, (New York, NY, USA), 2006. 35. K. Perlin, An image synthesizer, 
in SIGGRAPH 85: Proceedings of the 12th annual conference on Com­puter graphics and interactive techniques, 
pp. 287 296, ACM, (New York, NY, USA), 1985. 36. S.Worley, A cellular texture basis function, in SIGGRAPH 
96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pp. 291 
294, ACM, (New York, NY, USA), 1996. 37. E. N. Wiebe, Transfer of learning between 3d modeling systems, 
Engineering Design Graphics Jour­nal 67(3), pp. 15 28, 2003. 38. J. Dorsey, H. Rushmeier, and F. Sillion, 
Digital modeling of the appearance of materials, in SIGGRAPH 06: ACM SIGGRAPH 2006 Courses, p. 1, ACM, 
(New York, NY, USA), 2006. 39. J. Dorsey, H. Rushmeier, and F. Sillion, Digital Modeling of Material 
Appearance, Morgan Kauf­mann/Elsevier, Boston, MA, 2008. 40. R. L. Cook, Shade trees, in SIGGRAPH 84: 
Proceedings of the 11th annual conference on Computer graphics and interactive techniques, pp. 223 231, 
ACM, (New York, NY, USA), 1984. 41. M. McGuire, G. Stathis, H. P.ster, and S. Krishnamurthi, Abstract 
shade trees, in I3D 06: Proceedings of the 2006 symposium on Interactive 3D graphics and games, pp. 79 
86, ACM, (New York, NY, USA), 2006. 42. R. R. Lewis, Making shaders more physically plausible, Computer 
Graphics Forum 13, pp. 109 120, June 1994. 43. E. Reinhard, P. Shirley, M. Ashikhmin, and T. Troscianko, 
Second order image statistics in computer graphics, Proceedings of the 1st Symposium on Applied perception 
in graphics and visualization , pp. 99 106, 2004. 44. F. Pellacini, J. Ferwerda, and D. Greenberg, Toward 
a psychophysically-based light re.ection model for image synthesis, SIGGRAPH 2000: Proceedings of the 
27th annual conference on Computer graphics and interactive techniques , pp. 55 64, 2000. 45. W. Matusik, 
H. P.ster, M. Brand, and L. McMillan, A data-driven re.ectance model, ACM Trans. Graph. 22(3), pp. 759 
769, 2003. 46. A. Ngan, F. Durand, and W. Matusik, Image driven navigation of analytical BRDF models, 
Proceedings of the Eurographics Symposium on Rendering , pp. 399 407, 2006. 47. C. Shimizu, G. W. Meyer, 
and J. P. Wingard, Interactive goniochromatic color design, in Eleventh Color Imaging Conference: Color 
Science and Engineering Systems, Technologies, Applications Scottsdale, Ari­zona, pp. 16 22, November 
2003. 48. S. Ershov, R. .c, K. Kolchin, and K. Myszkowski, Reverse engineering approach to appearance­ 
 Durikovi. based design of metallic andpearlescent paints, The Visual Computer 20(8), pp. 586 600, 2004. 
 49. P.Vangorp,J. Laurijssen,andP. Dutr´e, The in.uenceofshapeontheperceptionof material re.ectance, 
ACM Trans. Graph. 26(3), pp. 77:1 77:10, 2007. 50. G. Ramanarayanan, J. Ferwerda, B. Walter, and K. Bala, 
Visual equivalence: towards a new standard for image .delity, ACM Trans. Graph. 26(3), pp. 76:1 76:12, 
2007.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401143</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>4</pages>
		<display_no>8</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Light and materials in virtual cities]]></title>
		<page_from>1</page_from>
		<page_to>4</page_to>
		<doi_number>10.1145/1401132.1401143</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401143</url>
		<abstract>
			<par><![CDATA[<p>The level of accuracy required for physical models of materials and their interaction with light in virtual city models is an open question. We consider work that has been done to date on modeling architectural materials, the measurement of such materials in situ, and modeling of natural materials found in urban environments (e.g. snow, ice, and mud). We identify gaps in currently available data, in current technology for specifying materials on large sets of buildings and rendering complex materials on a large scale. We also consider illumination of cities -- both day lighting and the natural and manmade illumination at night. We identify issues related to night illumination, where assumptions about adapting to light source spectra and sensitivity to color that are made for daylight may not hold.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098612</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorsey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098613</person_id>
				<author_profile_id><![CDATA[81100255828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Holly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rushmeier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166141</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. G. Becker and N. L. Max. Smooth transitions between bump rendering algorithms. In <i>SIGGRAPH '93: Proceedings of the 20th annual conference on Computer graphics and interactive techniques</i>, pages 183--190, New York, NY, USA, 1993. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[I. Cheng and P. Boulanger. Scale-space 3D TexMesh simplification. <i>Multimedia and Expo, 2004. ICME'04. 2004 IEEE International Conference on</i>, 1, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[K. J. Dana, B. van Ginneken, S. K. Nayar, and J. J. Koenderinck. Reflectance and texture of real-world surfaces. <i>ACM Transactions on Graphics</i>, 18(1):1--34, Jan. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P. Debevec, C. Tchou, A. Gardner, T. Hawkins, C. Poullis, J. Stumpfel, A. Jones, N. Yun, P. Einarsson, T. Lundgren, M. Fajardo, and P. Martinez. Estimating surface reflectance properties of a complex scene under captured natural illumination. Technical report, USC ICT, December 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1557600</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Dorsey, H. Rushmeier, and F. Sillion. <i>Digital Modeling of Material Appearance</i>. Morgan Kaufmann/Elsevier, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732137</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[F. Durand and J. Dorsey. Interactive tone mapping. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques 2000</i>, pages 219--230, London, UK, 2000. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1196364</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[P. Dutr&#233;, P. Bekaert, and K. Bala. <i>Advanced Global Illumination, Second Edition</i>. AK Peters Limited, Wellesley, MA, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[A. Efros and T. Leung. Texture synthesis by non-parametric sampling. <i>International Conference on Computer Vision</i>, 2(9):1033--1038, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344809</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[P. Fearing. Computer modelling of fallen snow. In <i>Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 37--46, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141985</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[K. Garg and S. Nayar. Photorealistic rendering of rain streaks. <i>International Conference on Computer Graphics and Interactive Techniques</i>, pages 996--1002, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1285529</ref_obj_id>
				<ref_obj_pid>1285519</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[K. Garg and S. Nayar. Vision and Rain. <i>International Journal of Computer Vision</i>, 75(1):3--27, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1133793</ref_obj_id>
				<ref_obj_pid>1133767</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[G. Haro, M. Bertalm&#237;o, and V. Caselles. Visual acuity in day for night. <i>Int. J. Comput. Vision</i>, 69(1):109--117, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218088</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[H. Iben and J. O'Brien. Generating surface crack patterns. <i>Proceedings of the 2006 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, pages 177--185, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383306</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[H. Jensen, F. Durand, J. Dorsey, M. Stark, P. Shirley, and S. Premo&#382;e. A physically-based night sky model. <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 399--408, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286090</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[G. W. Larson and R. Shakespeare. <i>Rendering with Radiance: The Art and Science of Lighting Visualization</i>. Morgan Kaufmann, San Francisco, CA, 1998. ISBN 1-55860-499-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[L. Lefebvre and P. Poulin. Analysis and synthesis of structural textures. In <i>Graphics Interface 2000</i>, pages 77--86, May 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383293</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Legakis, J. Dorsey, and S. Gortler. Feature-based cellular texturing for architectural models. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 309--316. ACM Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>636891</ref_obj_id>
				<ref_obj_pid>636886</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[H. P. A. Lensch, J. Kautz, M. Goesele, W. Heidrich, and H.-P. Seidel. Image-based reconstruction of spatial appearance and geometric detail. <i>ACM Transactions on Graphics</i>, 22(2):234--257, Apr. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927098</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[S. R. Marschner. <i>Inverse Rendering in Computer Graphics</i>. PhD thesis, Program of Computer Graphics, Cornell University, Ithaca, NY, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141931</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[P. M&#252;ller, P. Wonka, S. Haegler, A. Ulmer, and L. Van Gool. Procedural modeling of buildings. <i>ACM Transactions on Graphics (TOG)</i>, 25(3):614--623, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>113031</ref_obj_id>
				<ref_obj_pid>113023</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[E. Nakamae, K. Kaneda, K. Harada, T. Miwa, T. Nishita, and R. Saiki. Reliability of computer graphic images for visual assessment. <i>The Visual Computer</i>, 7(2):138--148, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97922</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[E. Nakamae, K. Kaneda, T. Okamoto, and T. Nishita. A lighting model aiming at drive simulators. In <i>Proceedings of Siggraph 1990</i>, pages 395--404. ACM SIGGRAPH, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[T. Nishita, H. Iwasaki, Y. Dobashi, and E. Nakamae. A modeling and rendering method for snow by using metaballs. <i>Computer Graphics Forum</i>, 16(3):357--364, August 1997. ISSN 1067--7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844176</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[M. Olano, B. Kuehne, and M. Simmons. Automatic shader level of detail. In <i>HWWS '03: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</i>, pages 7--14, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280922</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[S. N. Pattanaik, J. A. Ferwerda, M. D. Fairchild, and D. P. Greenberg. A multiscale model of adaptation and spatial vision for realistic image display. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, pages 287--298, New York, NY, USA, 1998. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276472</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[G. Ramanarayanan, J. Ferwerda, B. Walter, and K. Bala. Visual equivalence: towards a new standard for image fidelity. <i>ACM Trans. Graph.</i>, 26(3):76:1--76:12, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1208706</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[E. Reinhard, G. Ward, S. Pattanaik, and P. Debevec. <i>High Dynamic Range Imaging: Acquisition, Display and Image-Based Lighting</i>. Morgan Kaufmann, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>561383</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[F. X. Sillion and C. Puech. <i>Radiosity and Global Illumination</i>. Morgan Kaufmann Publishers Inc., San Franciso, CA, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[P. Sloan, J. Kautz, and J. Snyder. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. <i>Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, pages 527--536, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>637641</ref_obj_id>
				<ref_obj_pid>637640</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[W. Thompson, P. Shirley, and J. Ferwerda. A Spatial Post-Processing Algorithm for Images of Night Scenes. <i>Graphics Tools: The JGT Editors' Choice</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276473</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[P. Vangorp, J. Laurijssen, and P. Dutr&#233;. The influence of shape on the perception of material reflectance. <i>ACM Trans. Graph.</i>, 26(3):77:1--77:10, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383585</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[A. Wilkie, R. Tobler, C. Ulbricht, G. Zotti, and W. Purgathofer. An analytical model for skylight polarisation. <i>Proceedings of the EUROGRAPHICS Symposium on Rendering (EGSR 2004), Norrk&#246;ping, Sweden</i>, pages 387--99, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1249378</ref_obj_id>
				<ref_obj_pid>1249243</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[C. Xu, A. Georghiades, H. Rushmeier, and J. Dorsey. A system for reconstructing integrated texture maps for large structures. In <i>Third International Symposium on 3D Data Processing, Visualization and Transmission</i>, pages 822--829, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280874</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Y. Yu and J. Malik. Recovering photometric properties of architectural scenes from photographs. In <i>Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, pages 207--217. ACM Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Light and Materials in Virtual Cities Julie Dorsey* Holly Rushmeier Yale University ABSTRACT The level 
of accuracy required for physical models of materials and their interaction with light in virtual city 
models is an open question. We consider work that has been done to date on mod­eling architectural materials, 
the measurement of such materials in situ, and modeling of natural materials found in urban environments 
(e.g. snow, ice, and mud). We identify gaps in currently available data, in current technology for specifying 
materials on large sets of buildings and rendering complex materials on a large scale. We also consider 
illumination of cities both day lighting and the nat­ural and manmade illumination at night. We identify 
issues related to night illumination, where assumptions about adapting to light source spectra and sensitivity 
to color that are made for daylight may not hold. Index Terms: I.3.7 [Three-Dimensional Graphics and 
Realism]: Color, shading, shadowing and texture ; 1 INTRODUCTION Realistic real time rendering of urban 
environments has applica­tion in architectural design, urban planning, emergency planning, driving simulators, 
defense, historical reconstruction and entertain­ment. While some applications depend on visibility line 
of sight calculations that only require geometry, applications that depend on visual detection or aesthetic 
judgments require accurate simulation of materials and lighting as well. Examples of applications where 
visual detection is important include simulating how well a driver can see traf.c signs or pedestrians, 
how well an eye witness could have seen an event at night, or how an ancient building may have been con.gured 
to facilitate day-to-day activities. Examples where aesthetics are critical include evaluation of new 
building designs and generation of virtual sets for .lm. All of these are existing ap­plications that 
could be enhanced and be more widely accessible by advancing the methods used for simulating light and 
materials on the scale of cities. A considerable body of relevant research exists for simulating the 
illumination [7, 15, 28] and materials [5] used in man-made structures. Gaps exist though in data and 
models, methods for spec­ifying materials, and techniques for ef.ciently rendering physically accurate 
illumination simulations. Currently available models for materials are incomplete, and the capture of 
existing materials in place is problematic. There is little data available for natural ma­terials commonly 
found in urban environments. Current methods for specifying and applying materials are inef.cient for 
large scale scenes. The rendering accuracy required for many applications has yet to be established. 
In particular, many applications require ren­dering conditions at night, where the characteristics of 
human vi­sion are much different than in normal daylight. * e-mail: dorsey@cs.yale.edu e-mail: holly@acm.org 
 Figure 1: An example of the wide variations in color, texture and geometry of brick materials. 2 MATERIALS 
A material de.nition in computer graphics describes how the mate­rial redirects light as a function of 
wavelength, direction and posi­tion. Adequate data for describing the varieties of materials appear­ing 
in an urban environment is dif.cult to obtain. How to compactly represent materials and apply them to 
geometric structures is also an open issue.  2.1 Data from Material Samples Initially it seems that 
there are only a few materials required for simulating the buildings and pavement that dominate urban 
scenes stone, brick, concrete, wood, glass and metal. It also is often assumed that these are generally 
matte materials so that they can easily be simulated with texture maps. However, even disregarding the 
wide variety of complex materials needed to populate a city with people, automobiles and vegetation, 
just simulating architectural features requires much more than a library of a few hundred texture maps. 
Figure 1 shows a small set of examples illustrating the vari­ety of brick materials. Clearly size, layout 
and color vary. Most graphics systems allow varying these parameters (e.g. LightWorks www.lightworks-user.com 
brick plug-in allows 2100 vari­ations to be speci.ed). However there are also variations in small scale 
geometry and within-brick color variation. The regularity of the laying of the bricks also varies. These 
additional variations con­tribute to the appearance of a real, versus an arti.cially simulated, structure. 
Furthermore, brick is not a simple matte material. Variations in the small scale geometry of bricks and 
their .nish result in direc­tional variations in their appearance. Figure 2 shows a brick wall photographed 
under two different lighting conditions. The bricks look different in each of the photographs, and in 
different locations within the photographs. The variations occur both because of the .nish on the individual 
bricks, and because of the degree of inden­tation of the grout between them. The variation in bricks 
is just an example of one small class of material. Figure 3 shows examples of the variations that oc­cur 
in metals used in architecture. Materials that are often used are brushed metals with anisotropic re.ectance, 
metals that are crimped into shape, treated to have small scale bumps or subtle variations in  Figure 
2: An example of how the appearance of individual bricks, as well as a set of bricks changes with view 
and lighting. Figure 3: A variety of metals used in architecture. color. In addition to the variation 
in materials deliberately introduced by builders, natural materials, such as snow, dirt and mud, need 
to be introduced as well. While there have been models of snow [9, 23] developed, they have focused on 
snow as it would be seen freshly fallen in a more rural environment (Figure 4(a)), rather than as it 
appears in urban scenes (Figure 4(b)). Some aspects of mud have been simulated [13], but not a wide variety 
of packed dirt found in cities (Figure 5). Many methods exits for measuring the data needed for city 
modeling either as bidirectional re.ectance distribution functions (BRDF s) of spatial uniform materials 
[19], spatially varying BRDF [18] or bidirectional texture functions [3]. Some collections of def­initions 
provided by vendors are available, such as Tostem roof­ing and siding materials available in the propriety 
LWA format for the LightWorks system. Partial data is available for some mate­rials, such as the solar 
re.ectance for various roo.ng materials at http://eetd.lbl.gov/coolroof/. Collecting the large quantities 
of material de.nitions required for even a small city model is generally not feasible for one team to 
measure for a single project. Existing data is either incomplete or widely scattered at different sites 
in different forms. Unlike im­ages and geometry, there are no widely used openly documented formats such 
as TIF and JPG or VRML and OBJ for materials that can be used by web crawlers for searches. Methods for 
querying for materials have not been developed such as sets of common text descriptors or visual interfaces. 
While it is unlikely that the graph­ics community can agree on a speci.c format for even a subset of 
 Figure 4: Newly fallen snow (a), and snow in an urban setting (b). Figure 5: Examples of packed dirt. 
 Figure 6: An example of varying incident light color. materials, one possibility is develop a .le extension 
for some meta­data description of materials that could be retrieved in searches.  2.2 Acquiring Material 
Data in Situ Acquiring material data from real buildings is necessary to include particular important 
buildings in simulations, and to decompose into components that can be used in new building designs. 
Acquir­ing material properties for small opaque objects, where lighting can be controlled, is relatively 
straightforward [18]. However acquiring materials for objects on the scale of buildings remains problematic. 
A small number of efforts have attempted to acquire material properties. Yu and Malik proposed a photometric 
method for build­ings [34] that uses estimates of the directional illumination envi­ronment. This gives 
approximate solutions, because of the approx­imate estimates of variation of the illumination on the 
structure. In generating a model of the Parthenon [4], Debevec et al. took the approach of .rst measuring 
a collection of material samples, and then mapping the BRDF of the large structure by essentially iden­tifying 
the material at each point as a combination of the samples. The identi.cation proceeds by iteratively 
updating material de.­nitions used to generating synthetic images of the structure with photographs augmented 
with light probe data to estimate incident illumination conditions. This method relies on the structure 
includ­ing a small set of materials, and that samples of the materials can be accessed easily for detailed 
measurement. Xu et al. [33] used the laser return value to estimate the re­.ectance in a single channel 
to correct color images. This approach assumes that the incident light spectrum is spatially uniform. 
As shown in the .gures of an outdoor sculpture in Figure 6, this is a poor assumption out of doors. Areas 
illuminated by direct sunlight, skylight and re.ections from other structures and the ground are re.ecting 
different incident spectra. An extension of the approach attempts to segment the color images into areas 
illuminated by dif­ferent sources to appropriately apply different corrections. The dis­advantages of 
this method are that it relies on the laser return, and assumes diffuse re.ection only. The development 
of ef.cient methods of estimating materials for existing buildings, in particular methods that don t 
require expen­sive laser time-of-.ight scanner, remains an open problem.  2.3 Representations While 
there are some materials that can be speci.ed as a single BRDF, most of the materials in urban environments 
shown in Sec­tion 2.1 include spatial variations. Repeated use of the same sample of a BRDF/displacement 
map or BTF is easily spotted as arti.cial in a city simulation. Popular texture synthesis techniques 
based on sampling (e.g. methods inspired by Efros and Leung [8]) are far too slow to cover a city. Ideally, 
procedural textures should be used so that they can be generated rapidly, and with the appropriate vari­ability 
as observed even in the regular brick patterns in Figure 1. Lefebvre and Poulin [16] proposed techniques 
for creating proce­dural models of bricks, tile and wood from sample data. These techniques need to be 
tested and extended to a wider range of ma­terials.  The level of detail shown in the .gures in Section 
2.1 are only required for a small (several foot diameter) region around a pedes­trian in a city environment. 
Level of detail management is needed for procedural textures. Becker and Max [1] developed a method for 
transitioning from displacement maps, to bump maps to BRDF s depending on the level of detail needed 
for a material in a partic­ular view. Olano et al. [24] built on this idea to generate shaders that automatically 
adjust level of detail. These concepts need to be applied to the procedural methods generated for architectural 
ma­terials. Further, they need to be combined with the management of geometric level of detail, exploiting 
the interaction of material and geometry that has been explored by several groups [2, 26, 31].  2.4 
Speci.cation Assigning materials to individual structures is impractical on the city scale. To some extent 
this can be simpli.ed by assigning ma­terials to geometries that are assembled into structures by grammar 
rules [20]. However, to avoid generating masses of similar struc­tures, methods are needed to assign 
different types of cellular pat­terns to structures in a plausible way. Initial work to perform this 
assignment was present by Legakis et al. [17]. This approach needs to be tested on a larger scale. 3 
ILLUMINATION Clearly direct illumination is most important in outdoor environ­ments buildings cast shadows 
on one another and structures look much different in cloudy conditions than they do in sunshine. In the 
late eighties and early nineties, researchers began producing methods for ef.cient daylight simulations 
for urban modeling [21]. These methods continue to be improved with studies of effects such as polarization 
[32]. Most rendering packages have the ability to adjust the environment and specify the sun direction 
based on time and geographic location. While skylight and direct sun dominate day lighting simulations, 
the effect of structure-to-structure interre.ections can not always be ignored. Figure 7 illustrates 
the effects of such interre.ections. In the feature in the top left of the .gure, the left side of the 
indentation looks redder than the right, despite the fact that there is no change in material. The color 
difference is caused by illumination from the red brick building shown on the right that faces the left 
side of the indentation. The dappled effect on the building facade on the lower left is not due to material 
variations. The irregular illumination of the facade is due to specular re.ections from the mirrored 
building it faces, shown in the lower right. The length scale of the effect of interre.ections is relatively 
small compared to the scale of a city. Precomputed radiance trans­fer (PRT) [29] has been used extensively 
on small objects to encode the effects of interre.ections of objects on themselves. Methods to adapt 
PRT ef.ciently to structures and to individual building com­ponents that can be reused are needed to 
realistically light large scale scenes in real time. Simulations of scenes are needed for conditions 
other than clear daylight. Many years ago, Nishita et al. demonstrated a driving simulator for rainy 
night scenes [22]. Improved methods for vari­ous weather conditions and for night continue to be developed. 
Re-  Figure 7: The effect of interre.ections between buildings. searchers at Columbia have recently 
focused on simulating vision under different weather conditions, such as rain [10, 11]. An accu­rate 
model of the night time sky and illumination was presented in [14]. Night scenes in urban environments 
represent problems both in computing illumination and in appropriately displaying the results. Figure 
8 illustrates some of the problems. At twilight (upper image) both skylight and arti.cial light need 
to be accounted for. Twilight is a particularly critical time for evaluating road safety conditions, 
such as designing the round blue traf.c signs with arrows to pop out in the cluttered visual environments. 
The range of very high to very low illumination levels bring into question either the need for specialized 
tone mapping algorithms or high dynamic range dis­plays [27]. The lower two images in Figure 8 show the 
importance of cor­rectly modeling light sources. The lower left image shows how large numbers of light 
sources dominate the scene. Some can just be treated as isolated emitters, but the effect of others illuminat­ing 
facades and specularly re.ecting off glass and metal need to be taken into account. The lower right image 
shows that more atten­tion needs to be paid to the spectral properties of light sources. In daylight 
and indoors we adapt to the color of the dominant light source, and it appears white. At night, when 
there may be no sin­gle dominant source, we become sensitive to the variations in light source colors. 
Early work in graphics in adaptation in different light levels is considered in [25]. A post processing 
algorithm to alter pho­tographs to appear as though they are a night scene is given in [30]. Changes 
in visual acuity with change in light level is given in [12]. Temporal variations are described in [6]. 
Work is needed to validate these approaches for simulating vision in a complex night scene in an urban 
environment.  4 SUMMARY Existing work in materials and illumination needs to be tested and expanded 
for the successful simulation of urban scenes for many applications. Speci.c areas where work is needed 
include: Searchable urban material databases.  Improved methods for acquiring building material in 
situ.  Methods for deriving procedural material models from data.  Level of detail management for materials 
with geometry.  Assigning materials that adapt to building structures.  Precomputed radiance transfer 
for large scale structures.   Figure 8: Twilight (top) and night (bottom). Improved perceptual models 
for twilight and night scenes.  REFERENCES [1] B. G. Becker and N. L. Max. Smooth transitions between 
bump ren­dering algorithms. In SIGGRAPH 93: Proceedings of the 20th annual conference on Computer graphics 
and interactive techniques, pages 183 190, New York, NY, USA, 1993. ACM. [2] I. Cheng and P. Boulanger. 
Scale-space 3D TexMesh simpli.cation. Multimedia and Expo, 2004. ICME 04. 2004 IEEE International Con­ference 
on, 1, 2004. [3] K. J. Dana, B. van Ginneken, S. K. Nayar, and J. J. Koenderinck. Re.ectance and texture 
of real-world surfaces. ACM Transactions on Graphics, 18(1):1 34, Jan. 1999. [4] P. Debevec, C. Tchou, 
A. Gardner, T. Hawkins, C. Poullis, J. Stumpfel, A. Jones, N. Yun, P. Einarsson, T. Lundgren, M. Fajardo, 
and P. Mar­tinez. Estimating surface re.ectance properties of a complex scene under captured natural 
illumination. Technical report, USC ICT, De­cember 2004. [5] J. Dorsey, H. Rushmeier, and F. Sillion. 
Digital Modeling of Material Appearance. Morgan Kaufmann/Elsevier, 2008. [6] F. Durand and J. Dorsey. 
Interactive tone mapping. In Proceedings of the Eurographics Workshop on Rendering Techniques 2000, pages 
219 230, London, UK, 2000. Springer-Verlag. [7] P. Dutr´ e, P. Bekaert, and K. Bala. Advanced Global 
Illumination, Sec­ond Edition. AK Peters Limited, Wellesley, MA, 2006. [8] A. Efros and T. Leung. Texture 
synthesis by non-parametric sampling. International Conference on Computer Vision, 2(9):1033 1038, 1999. 
[9] P. Fearing. Computer modelling of fallen snow. In Proceedings of the 27th annual conference on Computer 
graphics and interactive tech­niques, pages 37 46, 2000. [10] K. Garg and S. Nayar. Photorealistic rendering 
of rain streaks. In­ternational Conference on Computer Graphics and Interactive Tech­niques, pages 996 
1002, 2006. [11] K. Garg and S. Nayar. Vision and Rain. International Journal of Computer Vision, 75(1):3 
27, 2007. [12] G. Haro, M. Bertalm´io, and V. Caselles. Visual acuity in day for night. Int. J. Comput. 
Vision, 69(1):109 117, 2006. [13] H. Iben and J. O Brien. Generating surface crack patterns. Proceed­ings 
of the 2006 ACM SIGGRAPH/Eurographics symposium on Com­puter animation, pages 177 185, 2006. [14] H. 
Jensen, F. Durand, J. Dorsey, M. Stark, P. Shirley, and S. Premo.ze. A physically-based night sky model. 
Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 399 
408, 2001. [15] G. W. Larson and R. Shakespeare. Rendering with Radiance: The Art and Science of Lighting 
Visualization. Morgan Kaufmann, San Francisco, CA, 1998. ISBN 1-55860-499-5. [16] L. Lefebvre and P. 
Poulin. Analysis and synthesis of structural tex­tures. In Graphics Interface 2000, pages 77 86, May 
2000. [17] J. Legakis, J. Dorsey, and S. Gortler. Feature-based cellular texturing for architectural 
models. In Proceedings of the 28th annual confer­ence on Computer graphics and interactive techniques, 
pages 309 316. ACM Press, 2001. [18] H. P. A. Lensch, J. Kautz, M. Goesele, W. Heidrich, and H.-P. Sei­del. 
Image-based reconstruction of spatial appearance and geometric detail. ACM Transactions on Graphics, 
22(2):234 257, Apr. 2003. [19] S. R. Marschner. Inverse Rendering in Computer Graphics. PhD the­sis, 
Program of Computer Graphics, Cornell University, Ithaca, NY, 1998. [20] P. M¨uller, P. Wonka, S. Haegler, 
A. Ulmer, and L. Van Gool. Proce­dural modeling of buildings. ACM Transactions on Graphics (TOG), 25(3):614 
623, 2006. [21] E. Nakamae, K. Kaneda, K. Harada, T. Miwa, T. Nishita, and R. Saiki. Reliability of computer 
graphic images for visual assessment. The Visual Computer, 7(2):138 148, 1991. [22] E. Nakamae, K. Kaneda, 
T. Okamoto, and T. Nishita. A lighting model aiming at drive simulators. In Proceedings of Siggraph 1990, 
pages 395 404. ACM SIGGRAPH, 1990. [23] T. Nishita, H. Iwasaki, Y. Dobashi, and E. Nakamae. A modeling 
and rendering method for snow by using metaballs. Computer Graphics Forum, 16(3):357 364, August 1997. 
ISSN 1067-7055. [24] M. Olano, B. Kuehne, and M. Simmons. Automatic shader level of detail. In HWWS 03: 
Proceedings of the ACM SIG-GRAPH/EUROGRAPHICS conference on Graphics hardware, pages 7 14, Aire-la-Ville, 
Switzerland, Switzerland, 2003. Eurographics Association. [25] S. N. Pattanaik, J. A. Ferwerda, M. D. 
Fairchild, and D. P. Green­berg. A multiscale model of adaptation and spatial vision for realistic image 
display. In SIGGRAPH 98: Proceedings of the 25th annual conference on Computer graphics and interactive 
techniques, pages 287 298, New York, NY, USA, 1998. ACM. [26] G. Ramanarayanan, J. Ferwerda, B. Walter, 
and K. Bala. Visual equiv­alence: towards a new standard for image .delity. ACM Trans. Graph., 26(3):76:1 
76:12, 2007. [27] E. Reinhard, G. Ward, S. Pattanaik, and P. Debevec. High Dy­namic Range Imaging: Acquisition, 
Display and Image-Based Light­ing. Morgan Kaufmann, 2005. [28] F. X. Sillion and C. Puech. Radiosity 
and Global Illumination. Morgan Kaufmann Publishers Inc., San Franciso, CA, 1994. [29] P. Sloan, J. Kautz, 
and J. Snyder. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting 
environments. Proceedings of the 29th annual conference on Computer graphics and interactive techniques, 
pages 527 536, 2002. [30] W. Thompson, P. Shirley, and J. Ferwerda. A Spatial Post-Processing Algorithm 
for Images of Night Scenes. Graphics Tools: The JGT Editors Choice, 2005. [31] P. Vangorp, J. Laurijssen, 
and P. Dutr´e. The in.uence of shape on the perception of material re.ectance. ACM Trans. Graph., 26(3):77:1 
77:10, 2007. [32] A. Wilkie, R. Tobler, C. Ulbricht, G. Zotti, and W. Purgathofer. An analytical model 
for skylight polarisation. Proceedings of the EU-ROGRAPHICS Symposium on Rendering (EGSR 2004), Norrk¨oping, 
Sweden, pages 387 99, 2004. [33] C. Xu, A. Georghiades, H. Rushmeier, and J. Dorsey. A system for reconstructing 
integrated texture maps for large structures. In Third International Symposium on 3D Data Processing, 
Visualization and Transmission, pages 822 829, 2006. [34] Y. Yu and J. Malik. Recovering photometric 
properties of architec­tural scenes from photographs. In Proceedings of the 25th annual conference on 
Computer graphics and interactive techniques, pages 207 217. ACM Press, 1998.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401144</section_id>
		<sort_key>120</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Beyond programmable shading: fundamentals]]></section_title>
		<section_page_from>4</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098614</person_id>
				<author_profile_id><![CDATA[81100460149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefohn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098615</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098616</person_id>
				<author_profile_id><![CDATA[81100112038]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chas.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098617</person_id>
				<author_profile_id><![CDATA[81100626902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kayvon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fatahalian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098618</person_id>
				<author_profile_id><![CDATA[81100501844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Forsyth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098619</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098620</person_id>
				<author_profile_id><![CDATA[81100458295]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Owens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401145</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>21</pages>
		<display_no>9</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Beyond programmable shading]]></title>
		<subtitle><![CDATA[fundamentals]]></subtitle>
		<page_from>1</page_from>
		<page_to>21</page_to>
		<doi_number>10.1145/1401132.1401145</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401145</url>
		<abstract>
			<par><![CDATA[<p>This first course in a series gives an introduction to parallel programming architectures and environments for interactive graphics. There are strong indications that the future of interactive graphics involves a programming model more flexible than today's OpenGL/Direct3D pipelines. As such, graphics developers need to have a basic understanding of how to combine emerging parallel programming techniques with the traditional interactive rendering pipeline. This course gives an introduction to several parallel graphics architectures, programming environments, and an introduction to the new types of graphics algorithms that will be possible.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.3</cat_node>
				<descriptor>Parallel programming</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169.10010175</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010175</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098621</person_id>
				<author_profile_id><![CDATA[81100460149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefohn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098622</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AMD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098623</person_id>
				<author_profile_id><![CDATA[81100112038]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098624</person_id>
				<author_profile_id><![CDATA[81100626902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kayvon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fatahalian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098625</person_id>
				<author_profile_id><![CDATA[81100501844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Forsyth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098626</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098627</person_id>
				<author_profile_id><![CDATA[81100458295]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Owens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Beyond Programmable Shading: Fundamentals SIGGRAPH 2008 5/1/2008 Course Organizers: Aaron Lefohn, Intel 
Mike Houston, AMD Course Speakers: Aaron Lefohn Mike Houston Chas. Boyd Kayvon Fatahalian Tom Forsyth 
David Luebke John Owens Intel AMD Microsoft Stanford University Intel NVIDIA University of California, 
Davis Speaker Contact Info: Chas. Boyd, Microsoft 1 Microsoft Way Bldg 84/1438 Redmond, WA 98052­6399 
chasb@microsoft.com Kayvon Fatahalian Gates Building Rm 381 353 Serra Mall Stanford University Stanford, 
CA 94301 kayvonf@graphics.stanford.edu Tom Forsyth 6841 NE 137th Street Kirkland WA 98034 tomf@radgametools.com 
Mike Houston, AMD 4555 Great America Parkway Suite 501 Santa Clara, CA 95054 Michael.Houston@amd.com 
Aaron Lefohn, Intel 2700 156th Ave NE, Suite 300 Bellevue, WA 98007 aaron.lefohn@intel.com David Luebke 
1912 Lynchburg Dr. Charlottesville, VA 22903 dlebke@nvidia.com John Owens Electrical and Computer Engineering 
University of California, Davis One Shields Avenue Davis, CA 95616 jowens@ece.ucdavis.edu Beyond Programmable 
Shading: Fundamentals Course Description: This first course in a series gives an introduction to parallel 
programming architectures and environments for interactive graphics. There are strong indications that 
the future of interactive graphics involves a programming model more flexible than today s OpenGL/Direct3D 
pipelines. As such, graphics developers need to have a basic understanding of how to combine emerging 
parallel programming techniques with the traditional interactive rendering pipeline. This course gives 
an introduction to several parallel graphics architectures, programming environments, and an introduction 
to the new types of graphics algorithms that will be possible. Intended Audience: We are targeting researchers 
and engineers interested in investigating advanced graphics techniques using parallel programming techniques 
on many­core GPU and CPU architectures, as well as graphics and game developers interested in integrating 
these techniques into their applications. Prerequisites: Attendees are expected to have experience with 
a modern graphics API (OpenGL or Direct3D), including basic experience with shaders, textures, and framebuffers 
and/or background with parallel programming languages. Some background with parallel programming on CPUs 
or GPUs is useful but not required as an overview of will be provided in the course. Level of difficulty: 
Advanced Special presentation requirements: Several speakers will bring their own demo machines for use 
in the course. Speakers: Beyond Programmable Shading: Fundamentals ­ Aaron Lefohn, Intel ­ Mike Houston, 
AMD ­ Chas Boyd, Microsoft ­ Kayvon Fatahalian, Stanford ­ Tom Forsyth, Intel ­ David Luebke, NVIDIA 
­ John Owens, UC Davis ­ Introduction: o Why and how is interactive graphics programming changing? Lefohn: 
10 min Very high throughput parallel hardware increases flexibility and complexity of algorithms that 
will run at > 30 fps  The transition from programmable shading to programmable graphics  What does 
all of this programmability mean for graphics?  ­ Parallel Architectures for Graphics: o Overview of 
graphics architectures: Fatahalian: 15 min o NVIDIA architecture: Luebke: 20 min o AMD/ATI architecture: 
Houston: 20 min o Intel architecture: Forsyth: 20 min  ­ Parallel Programming Models Overview: Owens: 
20 min o What to look for in the coming talks ­ 15 minute break ­ Parallel Programming for Interactive 
Graphics: o Brook+ / CAL: Mike Houston: 20 min  The Brook+ computing platform  Combining Brook+ and 
DX/OGL together for graphics  Example graphics algorithms enabled by Brook+  o CUDA: David Luebke: 
20 min  The CUDA GPU Computing platform  Combining CUDA + DX/OGL together for graphics  Example graphics 
algorithms enabled by CUDA  o Future Direct3D: Chas Boyd: 20 min  Traditional DX graphics pipeline 
 General GPU computation in Direct3D  Combining GPU­Compute + DX together for graphics  o TBA: A 
new programming model: TBA: 20 min o Intel: Aaron Lefohn: (20 min)  Content to be announced at SIGGRAPH 
­ Wrap­Up, Q&#38;A: All speakers: 5+ minutes Speaker Biographies Aaron Lefohn, Ph.D. Aaron Lefohn is 
a Senior Graphics Architect at Intel on the Larrabee project. Previously, he designed parallel programming 
models for graphics as a Principal Engineer at Neoptica, a computer graphics startup that was acquired 
by Intel in October 2007. Aaron's Ph.D. in Computer Science from the University of California Davis focused 
on data structure abstractions for graphics processors and data­parallel algorithms for rendering. From 
2003 ­2006, he was a researcher and graphics software engineer at Pixar Animation Studios, focusing on 
interactive rendering tools for artists and GPU acceleration of RenderMan. Aaron was formerly a theoretical 
chemist and was an NSF graduate fellow in computer science. Aaron Lefohn 2700 156th Ave NE, Suite 300 
Bellevue, WA 98007 425­881­4891 aaron.lefohn@intel.com Mike Houston, Ph.D. Mike Houston is a System 
Architect in the Advanced Technology Development group at AMD in Santa Clara working in architecture 
design and programming models for parallel architectures. He received his Ph.D. in Computer Science from 
Stanford University in 2008 focusing on research in programming models, algorithms, and runtime systems 
for parallel architectures including GPUs, Cell, multi­core, and clusters. His dissertation includes 
the Sequoia runtime system, a system for programming hierarchical memory machines. He received his BS 
in Computer Science from UCSD in 2001 and is a recipient of the Intel Graduate Fellowship. Mike Houston 
4555 Great America Parkway Suite 501 Santa Clara, CA 95054 408­572­6010 Michael.Houston@amd.com, Chas 
Boyd Chas. is a software architect at Microsoft. Chas. joined the Direct3D team in 1995 and has contributed 
to releases since DirectX 3. Over that time he has worked closely with hardware and software developers 
to drive the adoption of features like programmable hardware shaders and float pixel processing. He has 
developed and demonstrated initial hardware­accelerated versions of techniques like hardware soft skinning, 
and hemispheric lighting with ambient occlusion. He is currently working on the design of future DirectX 
releases and related components. Chas. Boyd 1 Microsoft Way Bldg 84/1438 Redmond WA 98052­6399 425­922­7859 
 chasb@microsoft.com Kayvon Fatahalian Kayvon Fatahalian is a Ph.D. candidate in computer science in 
the Computer Graphics Laboratory at Stanford University. His research interests include programming systems 
for commodity parallel architectures and computer graphics/animation systems for the interactive and 
film domains. His thesis research seeks to enable execution of more flexible rendering pipelines on future 
GPUs and multi­core PCs. Gates Building Rm 381 353 Serra Mall Stanford University Stanford, CA 94301 
 kayvonf@graphics.stanford.edu Tom Forsyth Tom Forsyth has been rendering Cobra MkIIIs on everything 
he's ever used. In rough chronological order he has worked on the ZX Spectrum, Atari ST, 386, Virge, 
Voodoo, 32X, Saturn, Pentium1, Permedia2, Permedia3, Dreamcast, Xbox1, PS2, 360, PS3 and now Larrabee. 
Past jobs include writing utilities for Microprose, curved­surface libraries for Sega, DirectX drivers 
for 3Dlabs, three shipped games for Muckyfoot Productions, and Granny3D and Pixomatic for RAD Game Tools. 
He is currently working for Intel as a software and hardware architect on the Larrabee project. Tom Forsyth 
6841 NE 137th Street Kirkland WA 98034 425­522­4499 tomf@radgametools.com David Luebke, Ph.D. David 
Luebke is a Research Scientist at NVIDIA Corporation, which he joined after eight years on the faculty 
of the University of Virginia. He has a Ph.D. in Computer Science from the University of North Carolina 
and a B.S. in Chemistry from the Colorado College. Luebke's research interests are GPU computing and 
realistic real­time computer graphics. Recent projects include advanced reflectance and illumination 
models for real­time rendering, image­based acquisition of real­world environments, temperature­aware 
graphics architecture, and scientific computation on GPUs. Past projects include leading the book "Level 
of Detail for 3D Graphics" and the Virtual Monticello museum exhibit at the New Orleans Museum of Art. 
David Luebke 1912 Lynchburg Dr. Charlottesville, VA 22903 434­409­1892 dlebke@nvidia.com John Owens, 
Ph.D. John Owens is an assistant professor of electrical and computer engineering at the University of 
California, Davis. His research interests are in commodity parallel hardware and programming models, 
including GPU computing. At UC Davis, he received the Department of Energy Early Career Principal Investigator 
Award and an NVIDIA Teaching Fellowship. John earned his Ph.D. in electrical engineering in 2003 from 
Stanford University and his B.S. in electrical engineering and computer sciences in 1995 from the University 
of California, Berkeley. John Owens Electrical and Computer Engineering University of California, Davis 
One Shields Avenue Davis, CA 95616 530­754­4289 jowens@ece.ucdavis.edu http://www.ece.ucdavis.edu/~jowens/ 
 Beyond Programmable Shading: Fundamentals Aaron Lefohn Intel Beyond Programmable Shading: Fundamentals 
 Disclaimer about these Course Notes  The material in this course is bleeding edge  Unfortunately, 
that means we can t share most of the details with you until SIGGRAPH 2008  Most talks are missing from 
the submitted notes  The talks that are included will change substantially   To address this inconvenience 
 We will post all course notes/slides on a permanent web page, available the first day of SIGGRAPH 2008 
 We have included in the notes a number of related recently published articles that provide key background 
material for the course   Future interactive rendering techniques will be an inseparable mix of data-and 
task-parallel algorithms and graphics pipelines How do we write new interactive 3D rendering algorithms? 
 Fixed - Function Graphics Pipeline Writing new rendering algorithms means Tricks with stencil buffer, 
depth buffer, blending,  Examples  Shadow volumes  Hidden line removal    Programmable Shading 
 Writing new rendering algorithms means Tricks with stencil buffer, depth buffer, blending,  Plus: 
Writing shaders  Examples  Parallax mapping  Shadow-mapped spot light    Beyond Programmable Shading 
 Writing new rendering algorithms means Tricks with stencil buffer, depth buffer, blending,  Plus: 
Writing shaders  Plus: Writing data-and task-parallel algorithms  Analyze results of rendering pipeline 
 Create data structures used in rendering pipeline   Examples  Dynamic summed area table  Dynamic 
quadtree adaptive shadow map  Dynamic ambient occlusion   Fast Summed-Area Table Generation and its 
Applications, Hensley et al., Eurographics 2005 Dynamic Ambient Occlusion and Indirect Lighting, Bunnell, 
GPU Gems II, 2005 Resolution Matched Shadow Maps, Lefohn et al., ACM Transactions on Graphics 2007 Beyond 
Programmable Shading Writing new rendering algorithms means Tricks with stencil buffer, depth buffer, 
blending,  Plus: Writing shaders  Plus: Writing data-and task-parallel algorithms  Analyze results 
of rendering pipeline  Create data structures used in rendering pipeline   Plus: Extending, modifying, 
or creating graphics pipelines  Examples PlayStation 3 developers creating hybrid Cell/GPU graphics 
pipelines See afternoon talk from Jon Olick (Id Software)  Active area of research  Why Beyond Programmable 
Shading?  Short answer: The parallel processors in your desktop machine or game console are now flexible 
and powerful enough to execute both User-defined parallel programs and  Graphics pipelines  All within 
1/30th of a second   The Point Is  Interactive graphics programming is changing  This course gives 
you:  Introduction to the HW causing/enabling this change  Programming tools used to explore this new 
world  A little bit about what developers/researchers can do with these new capabilities   And the 
afternoon course This Afternoon Course Beyond Programmable Shading: In Action Case studies from game 
developers, academics, and industry This Afternoon s Course Beyond Programmable Shading: In Action 
 Show-casing new interactive rendering algorithms that result in more realistic imagery than is possible 
using only the pre-defined DX/OpenGL graphics pipeline by Combining task-, data-, and/or graphics pipeline 
parallelism,  Analyzing intermediate data produced by graphics pipeline,  Building and using complex 
data structures every frame, or  Modifing/extending the graphics pipelines  Speakers (in order of 
appearance)  Aaron Lefohn, Intel  Kayvon Fatahalian, Stanford  Dave Luebke, NVIDIA  Mike Houston, 
AMD  Tom Forsyth, Intel  John Owens, UC Davis  Chas Boyd, Microsoft  TBA, TBA  Schedule Intro 
8:30 8:40 Lefohn GPU Architectures  Overview 8:40 8:55 Fatahalian  NVIDIA 8:55 9:15 Luebke  AMD 
9:15 9:35 Houston  Intel 9:35 9:55 Forsyth   GPU Programming Models  Overview 9:55 10:15 Owens 
<Break> 10:15 10:30  Brook+ 10:30 10:50 Houston  CUDA 10:50 11:10 Luebke  DirectX 11:10 11:30 
Boyd  TBA 11:30 11:50 TBA  Intel 11:50 12:10 Lefohn   Q &#38; A 12:10 12:15+ 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401146</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>13</pages>
		<display_no>10</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Overview]]></title>
		<subtitle><![CDATA[making sense of GPU architectures]]></subtitle>
		<page_from>1</page_from>
		<page_to>13</page_to>
		<doi_number>10.1145/1401132.1401146</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401146</url>
		<abstract>
			<par><![CDATA[<p>GPUs are high throughput multi-core processors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>C.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010528.10010531</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Parallel architectures->Multiple instruction, multiple data</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003033.10003079</concept_id>
				<concept_desc>CCS->Networks->Network performance evaluation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011123.10011674</concept_id>
				<concept_desc>CCS->General and reference->Cross-computing tools and techniques->Performance</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010575</concept_id>
				<concept_desc>CCS->Computer systems organization->Dependable and fault-tolerant systems and networks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098628</person_id>
				<author_profile_id><![CDATA[81100626902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kayvon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fatahalian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Overview: Making Sense of GPU Architectures Kayvon Fatahalian Stanford University  A GPU Rasterization 
 Core Core Core Core Blend Core Core Core Core Texture Filtering Compression Core Core Core Core 
 Scheduling/Dispatch  A thread of execution A sequence of instructions executing within a processor 
context Program counter Inst ructi o n Decode Execution Registers Context ALU Memory mappings Processing 
Core  Multi-core increases throughput Replicate resources and execute in parallel Dec ode Dec ode Decode 
Decode Exec Exec Exec Exec Context Context Context Context ALU ALU ALU ALU Core 1 Core 2 Core 3 Core 
4  SIMD processing Share instruction stream control logic across ALUs Execution Context ALU Inst ructi 
o n Decode ALU ALU ALU Processing Core  Example: 4 cores, 4-wide SIMD Decode Dec ode Exec Exec 
 AL U AL U AL U AL U Context Context AL U AL U AL U AL U Core 1 Core 2 Dec ode Decode Exec Exec 
 AL U AL U AL U AL U Context Context AL U AL U AL U AL U Core 3 Core 4  Multi-threading  Each 
core maintains more thread execution contexts than it can simultaneously execute  Upon thread stall, 
core chooses another thread to execute  Exec Cxt Inst ructi o n Decode Exec Cxt ALU ALU ALU ALU 
Exec Cxt Processing Core   Questions to ask about GPUs!  How does architecture organize itself into 
multi­core, multi-threaded, and SIMD processing?  How do kernels/shaders map to SIMD execution and 
multiple threads?  How are instructions streams shared across kernels/threads?  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401147</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>11</pages>
		<display_no>11</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[GPUs a closer look]]></title>
		<page_from>1</page_from>
		<page_to>11</page_to>
		<doi_number>10.1145/1401132.1401147</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401147</url>
		<abstract>
			<par><![CDATA[<p>A gamer wanders through a virtual world rendered in nearcinematic detail. Seconds later, the screen fills with a 3D explosion, the result of unseen enemies hiding in physically accurate shadows. Disappointed, the user exits the game and returns to a computer desktop that exhibits the stylish 3D look-and-feel of a modern window manager. Both of these visual experiences require hundreds of gigaflops of computing performance, a demand met by the GPU (graphics processing unit) present in every consumer PC.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098629</person_id>
				<author_profile_id><![CDATA[81100626902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kayvon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fatahalian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098630</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A gamer wanders through a virtual world rendered in near­cinematic detail. Seconds later, the screen 
fills with a 3D explosion, the result of unseen enemies hiding in physically accurate shadows. Disappointed, 
the user exits the game and returns to a computer desktop that exhibits the stylish 3D look­and-feel 
of a modern window manager. Both of these visual experiences require hundreds of gigaflops of computing 
perfor­mance, a demand met by the GPU (graphics processing unit) present in every consumer PC. GPUs 
a closer look As the line between GPUs and CPUs begins to blur, it s important to understand what makes 
GPUs tick. KAYVON FATAHALIAN and MIKE HOUSTON, STANFORD UNIVERSITY   resources busy. GPU designs 
go to great lengths to obtain high ef.ciency, conveniently reducing the dif.culty pro­grammers face when 
programming graphics applications. As a result, GPUs deliver high performance and expose an expressive 
but simple programming interface. This interface remains largely devoid of explicit parallelism or asynchronous 
execution and has proven to be portable across vendor implementations and generations of GPU designs. 
At a time when the shift toward throughput-oriented CPU platforms is prompting alarm about the complexity 
of parallel programming, understanding key ideas behind the success of GPU computing is valuable not 
only for developers targeting software for GPU execution, but also for informing the design of new architectures 
and programming systems for other domains. In this article, we dive under the hood of a modern GPU to 
look at why interactive rendering is challenging and to explore the solutions GPU architects have devised 
to meet these challenges. 4(% A graphics system generates images that represent views of a virtual scene. 
This scene is de.ned by the geometry, orientation, and material properties of object surfaces and the 
position and characteristics of light sources. A scene view is described by the location of a virtual 
camera. Graphics systems seek to .nd the appropriate balance between con.icting goals of enabling maximum 
performance and maintaining an expressive but simple interface for describing graphics computations. 
Realtime graphics APIs such as Direct3D and OpenGL strike this balance by representing the rendering 
compu­tation as a graphics processing pipeline that performs opera­tions on four fundamental entities: 
vertices, primitives, fragments, and pixels. Figure 1 provides a block diagram of a simpli.ed seven-stage 
graphics pipeline. Data .ows between stages in streams of entities. This pipeline con­tains .xed-function 
stages (tan) implementing API-speci­.ed operations and three programmable stages (brown) whose behavior 
is de.ned by application code. Figure 2 illustrates the operation of key pipeline stages. VG (vertex 
generation). Realtime graphics APIs repre­sent surfaces as collections of simple geometric primitives 
(points, lines, or triangles). Each primitive is de.ned by a set of vertices. To initiate rendering, 
the application provides the pipeline s VG stage with a list of vertex descriptors. From this list, VG 
prefetches vertex data from memory and constructs a stream of vertex data records for subsequent processing. 
In practice, each record contains the 3D (x,y,z) scene position of the vertex plus additional application-de.ned 
parameters such as surface color and normal vector orientation. VP (vertex processing). The behavior 
of VP is applica­tion programmable. VP operates on each vertex indepen­dently and produces exactly one 
output vertex record from each input record. One of the most important operations of VP execution is 
computing the 2D output image (screen) projection of the 3D vertex position. PG (primitive generation). 
PG uses vertex topology data provided by the application to group vertices from VP into an ordered stream 
of primitives (each primitive record is the concatenation of several VP output vertex records). Vertex 
topology also de.nes the order of primi­tives in the output stream. PP (primitive processing). PP operates 
independently on each input primitive to produce zero or more output primitives. Thus, the output of 
PP is a new (potentially longer or shorter) ordered stream of primitives. Like VP, PP operation is application 
programmable. FG (fragment generation). FG samples each primitive densely in screen space (this process 
is called rasteriza­tion). Each sample is manifest as a fragment record in the FG output stream. Fragment 
records contain the output image position of the surface sample, its distance from more queue: www.acmqueue.com 
!#-15%5% March/April 2008 21  the virtual camera, as well as values computed via inter­polation of 
the source primitive s vertex parameters. FP (fragment processing). FP simulates the interaction of light 
with scene surfaces to determine surface color and opacity at each fragment s sample point. To give surfaces 
realistic appearances, FP computations make heavy use of filtered lookups into large, parameterized 1D, 
2D, or 3D arrays called textures. FP is an application-programmable stage. PO (pixel operations). PO 
uses each fragment s screen position to calculate and apply the fragment s contribu­tion to output image 
pixel values. PO accounts for a sample s distance from the virtual camera and discards fragments that 
are blocked from view by surfaces closer to the camera. When fragments from multiple primi­tives contribute 
to the value of a single pixel, as is often the case when semi-transparent surfaces overlap, many rendering 
techniques rely on PO to perform pixel updates in the order defined by the primitives positions in the 
PP output stream. All graphics APIs guarantee this behavior, and PO is the only stage where the order 
of entity pro­cessing is specified by the pipeline s definition.  SHADER pROGRAMMING The behavior of 
application-programmable pipeline stages (VP, PP, FP) is defined by shader functions (or shad­ers). Graphics 
programmers express vertex, primitive, and fragment shader functions in high-level shading languages 
such as NVIDIA s Cg, OpenGL s GLSL, or Microsoft s HLSL. Shader source is compiled into bytecode offline, 
then transformed into a GPU-specific binary by the graphics driver at runtime. Shading languages support 
complex data types and a rich set of control-flow constructs, but they do not contain primitives related 
to explicit parallel execution. Thus, a shader definition is a C-like function that serially computes 
output-entity data records from a single input entity. Each function invocation is abstracted as an inde­pendent 
sequence of control that executes in complete isolation from the processing of other stream entities. 
As a convenience, in addition to data records from stage input and output streams, shader functions may 
access (but not modify) large, globally shared data buffers. Prior to pipeline execution, these buffers 
are initialized to contain shader-specific parameters and textures by the application.  CHARACTERISTICS 
AND CHALLENGES Graphics pipeline execution is characterized by the fol­lowing key properties. Opportunities 
for parallel processing. Graphics presents opportunities for both task (across pipeline stages) and data 
(stages operate independently on stream entities) parallelism, making parallel processing a viable strategy 
for increasing throughput. Despite abundant potential parallelism, however, constraints on the order 
of PO stage processing introduce dynamic, fine-grained dependencies that complicate parallel implementation 
throughout the pipeline. Although output image contri­butions from most fragments can be applied in parallel, 
those that contribute to the same pixel cannot. Fixed-function stages encapsulate difficult-to-paral­lelize 
work. Each shader function invocation executes serially; programmable stages, however, are trivially 
paral­lelizable by executing shader functions simultaneously on multiple stream entities. In contrast, 
the pipeline s non­programmable stages involve multiple entity interactions (such as ordering dependencies 
in PO or vertex grouping in PG) and stateful processing. Isolating this non-data­parallel work into fixed 
stages keeps the shader program­ming model simple and allows the GPU s programmable processing components 
to be highly specialized for data-parallel execution. In addition, the separation enables difficult aspects 
of the graphics computation to be encapsulated in optimized, fixed-function hardware components. Extreme 
variations in pipeline load. Although the number of stages and data flows of the graphics pipeline is 
fixed, the computational and bandwidth requirements of all stages vary significantly depending on the 
behavior of shader functions and properties of scenes. For example, primitives that cover large regions 
of the screen gener­ate many more fragments than vertices. In contrast, many small primitives result 
in high vertex-processing demands. Applications frequently reconfigure the pipe­line to use different 
shader functions that vary from tens of instructions to a few hundred. For these reasons, over the duration 
of processing for a single frame, different stages will dominate overall execution, often resulting in 
bandwidth- and compute-intensive phases of execu­tion. Maintaining an efficient mapping of the graphics 
pipeline to a GPU s resources in the face of this variability is a significant challenge, as it requires 
processing and on-chip storage resources to be dynamically reallocated to pipeline stages, depending 
on current load.  Mixture of predictable and unpredictable data access. The graphics pipeline rigidly 
defines inter-stage data flows using streams of entities. This predictability presents opportunities 
for aggregate prefetching of stream data records and highly specialized hardware management on-chip storage 
resources. In contrast, buffer and texture accesses performed by shaders are fine-grained memory operations 
on dynamically computed addresses, making prefetch difficult. As both forms of data access are critical 
to maintaining high throughput, shader programming models explicitly differentiate stream from buffer/texture 
memory accesses, permitting specialized hardware solu­tions for both types of accesses. Opportunities 
for instruction stream sharing. While the shader programming model permits each shader invocation to 
follow a unique stream of control, in practice, shader execution on nearby stream elements often results 
in the same dynamic control-flow decisions. As a result, multiple shader invocations can likely share 
an instruction stream. Although GPUs must accom­modate situations where this is not the case, instruction 
stream sharing across multiple shader invocations is a key optimization in the design of GPU processing 
cores and is accounted for in algorithms for pipeline scheduling. processing. As shown in table 1, these 
throughput-com­puting techniques are not unique to GPUs (top two rows). In comparison with CPUs, however, 
GPU designs push these ideas to extreme scales. Multicore + SIMD Processing = Lots of ALUs. A thread 
of control is realized by a stream of processor instructions that execute within a processor-managed 
environment, called an execution (or thread) context. This context con­sists of states such as a program 
counter, a stack pointer, general-purpose registers, and virtual memory mappings. A multicore processor 
replicates processing resources (both ALUs and execution contexts) and organizes them into independent 
cores. When an application features multiple threads of control, multicore architectures pro­vide increased 
throughput by executing these instruction streams on each core in parallel. For example, an Intel Core 
2 Quad contains four cores and can execute four instruction streams simultaneously. As significant paral­lelism 
exists across shader invocations, GPU designs easily push core counts higher. High-end models contain 
up to 16 cores per chip. Even higher performance is possible by populating each core with multiple floating-point 
ALUs. This is done efficiently with SIMD processing, which uses each ALU to perform the same operation 
on a different piece of data. The most common implementation of SIMD processing is via explicit short-vector 
instructions, similar to those provided by the x86 SSE or PowerPC Altivec ISA exten­sions. These extensions 
provide a SIMD width of four, with instructions that control the operation of four ALUs. Alternative 
implementations, such as NVIDIA s 8-series architecture, perform SIMD execution by implicitly shar- 
OGRAMMAbLEpR OCESSING RESOURCES A large fraction of a GPU s resources exist within programmable processing 
cores responsible for exe­cuting shader functions. While substantial imple­mentation differences exist 
across vendors and product lines, all modern GPUs maintain high efficiency through the use of multi­core 
designs that employ both hardware multi­threading and SIMD (single instruction, multiple data)  TABLE 
1 Tale of the Tape: 1SSE processing only, does not account for x86 FPU. 2Stream processing (SPE) cores 
only, does not account for PPU cores. 332-bit, floating point (all ALUs are multiply-add except the Intel 
Core 2 Quad) 4The ratio of core thread contexts to simultaneously executable threads. We use the ratio 
T (rather than the total number of per-core thread contexts) to describe the extent to which processor 
cores automatically hide thread stalls via hardware multithreading. more queue: www.acmqueue.com ACM 
QUEUE March/April 2008 23  ing an instruction across multiple threads with identical PCs. In either 
SIMD implementation, the complexity of processing an instruction stream and the cost of circuits and 
structures to control ALUs are amortized across mul­tiple ALUs. The result is both power-and area-efficient 
chip execution. CPU designs have converged on a SIMD width of four as a balance between providing increased 
throughput and retaining high single-threaded performance. Characteris­tics of the shading workload make 
it beneficial for GPUs to employ significantly wider SIMD processing (widths ranging from 32 to 64) and 
to support a rich set of opera­tions. It is common for GPUs to support SIMD implemen­tations of reciprocal 
square root, trigonometric functions, and memory gather/scatter operations. The efficiency of wide SIMD 
processing allows GPUs to pack many cores densely with ALUs. For example, the NVIDIA GeForce 8800 Ultra 
GPU contains 128 single­precision ALUs operating at 1.5 GHz. These ALUs are organized into 16 processing 
cores and yield a peak rate of 384 Gflops (each ALU retires one 32-bit multiply-add per clock). In comparison, 
a high-end 3-GHz Intel Core 2 CPU contains four cores, each with eight SIMD floating­point ALUs (two 
4-width vector instructions per clock), and is capable of, at most, 96 Gflops of peak performance. GPUs 
execute groups of shader invocations in par­allel to take advantage of SIMD processing. Dynamic per-entity 
control flow is implemented by executing all control paths taken by the shader invocations. SIMD operations 
that do not apply to all invocations, such as those within shader code conditional or loop blocks, are 
partially nullified using write-masks. In this implemen­tation, when shader control flow diverges, fewer 
SIMD ALUs do useful work. Thus, on a chip with width-S SIMD processing, worst-case behavior yields performance 
equal­ing 1/S the chip s peak rate. Fortunately, shader workloads exhibit sufficient levels of instruction 
stream sharing to justify wide SIMD implementations. Additionally, GPU ISAs contain special instructions 
that make it possible for shader compilers to transform per-entity control flow into efficient sequences 
of SIMD operations. Hardware Multithreading = High ALU Utilization. Thread stalls pose an additional 
challenge to high-perfor­mance shader execution. Threads stall (or block) when the processor cannot dispatch 
the next instruction in an instruction stream because of a dependency on an outstanding instruction. 
High-latency off-chip memory accesses, most notably those generated by fragment shader texturing operations, 
cause thread stalls lasting hundreds of cycles (recall that while shader input and output records lend 
themselves to streaming prefetch, texture accesses do not). Allowing ALUs to remain idle during the period 
while a thread is stalled is inefficient. Instead, GPUs maintain more execution contexts on chip than 
they can simul­taneously execute, and they perform instructions from runnable threads when others are 
stalled. Hardware scheduling logic determines which context(s) to execute in each processor cycle. This 
technique of overprovision­ing cores with thread contexts to hide the latency of thread stalls is called 
hardware multithreading. GPUs use multithreading to hide both memory access and instruc­tion pipeline 
latencies. The latency-hiding ability of GPU multithreading is dependent on the ratio of hardware thread 
contexts to the number of threads that can be simultaneously exe­cuted in a clock (value T from table 
1). Support for more thread contexts allows the GPU to hide longer or more frequent stalls. All modern 
GPUs maintain large num­bers of execution contexts on chip to provide maximal memory latency-hiding ability 
(T ranges from 16 to 96). This represents a significant departure from CPU designs, which attempt to 
avoid or minimize stalls using large, low-latency data caches and complicated out-of-order execution 
logic. Current Intel Core 2 and AMD Phenom processors maintain one thread per core, and even high­end 
models of Sun s multithreaded UltraSPARC T2 proces­sor manage only four times the number of threads they 
can simultaneously execute. Note that in the absence of stalls, the throughput of single- and multithreaded 
processors is equivalent. Multi­threading does not increase the number of processing resources on a chip. 
Rather, it is a strategy that interleaves execution of multiple threads in order to use existing resources 
more efficiently (improve throughput). On aver­age, a multithreaded core operating at its peak rate runs 
each thread 1/T of the time. Large-scale multithreading requires execution contexts to be compact in 
order to fit many contexts within on­chip memories. The number of thread contexts supported by a GPU 
core is shader-program dependent and typi­cally limited by the size of on-chip storage. GPUs require 
compiled shader binaries to declare input and output entity sizes, as well as bounds on temporary storage 
and scratch registers required for execution. At runtime, GPUs use these bounds to partition unspillable 
on-chip storage (including data registers) dynamically among execution contexts. Thus, GPUs support many 
thread contexts (up to an architecture-specific bound) and, correspondingly, provide maximal latency-hiding 
ability when shaders use fewer resources. When shaders require large amounts of storage, the number of 
execution contexts provided by a GPU drops. (The accompanying sidebar details an example of the efficient 
execution of a fragment shader on a GPU core.) FIxED-FUNCTION pROCESSING RESOURCES A GPU s programmable 
cores interoperate with a collec­tion of specialized fixed-function processing units that provide high-performance, 
power-efficient implementa­tions of nonshader stages. These components do not simply augment programmable 
processing; they perform sophisticated operations and constitute an additional hundreds of gigaflops 
of processing power. Two of the most important operations performed via fixed-function hardware are texture 
filtering and rasterization (fragment generation). Texturing is handled almost entirely by fixed-function 
logic. A texturing operation samples a contiguous 1D, 2D, or 3D signal (a texture) that is discretely 
represented by a multidimensional array of color values (2D texture data is simply an image). A GPU texture-filtering 
unit accepts a point within the texture s parameterization (represented by a floating-point tuple, such 
as {.5,.75}) and loads array values surrounding the coordinate from memory. The val­ues are then filtered 
to yield a single result that represents the texture s value at the specified coordinate. This value 
is returned to the calling shader function. Sophisticated texture filtering is required for generating 
high-quality images. As graphics APIs provide a finite set of filtering kernels, and because filtering 
kernels are computationally expensive, texture filtering is well suited for fixed-func­tion processing. 
Primitive rasterization in the FG stage is another key pipeline operation implemented by fixed-function 
com­ponents. Rasterization involves densely sampling a primi­tive (at least once per output image pixel) 
to determine which pixels the primitive overlaps. This process involves interpolating the location of 
the surface at each sample point and then generating fragments for all sample points covered by the primitive. 
Bounding-box computations and hierarchical techniques optimize the rasterization process. Nonetheless, 
rasterization involves significant computation. In addition to the components for texturing and ras­terization, 
GPUs contain dedicated hardware components for operations such as surface visibility determination, output 
pixel compositing, and data compression/decom­pression. THE MEMORY SYSTEM Parallel-processing resources 
place extreme load on a GPU s memory system, which services memory requests from both fixed-function 
and programmable compo- nents. These requests include a mixture of fine-granular­ity and bulk prefetch 
operations and may even require realtime guarantees (such as display scan out). Recall that a GPU s programmable 
cores tolerate large memory latencies via hardware multithreading and that interstage stream data accesses 
can be prefetched. As a result, GPU memory systems are architected to deliver high-bandwidth, rather 
than low-latency, data access. High throughput is obtained through the use of wide more queue: www.acmqueue.com 
ACM QUEUE March/April 2008 25  memory buses and specialized GDDR (graphics double data rate) memories 
that operate most efficiently when memory access granularities are large. Thus, GPU memory controllers 
must buffer, reorder, and then coalesce large numbers of memory requests to synthesize large opera­tions 
that make efficient use of the memory system. As an example, the ATI HD 2700XT memory controller manipulates 
thousands of outstanding requests to deliver 105 GB per second of bandwidth from GDDR3 memories attached 
to a 512-bit bus. GPU data caches meet different needs from CPU caches. GPUs employ relatively small, 
read-only caches (no cache coherence) that filter requests destined for the memory controller and reduce 
bandwidth requirements placed on main memory. Thus, GPU caches typically serve to amplify total bandwidth 
to processing units rather than decrease latency of memory accesses. Inter­leaved execution of many threads 
renders large read-write caches inefficient because of severe cache thrashing. GPUs benefit from small 
caches that capture spatial locality across simultaneously executed shader invocations. This situation 
is common, as texture accesses performed while processing fragments in close screen proximity are likely 
to have overlapping texture-filter support regions. Although most GPU caches are small, this does not 
imply that GPUs contain little on-chip storage. Signifi­cant amounts of on-chip storage are used to hold 
entity streams, execution contexts, and thread scratch data.  pIpELINE SCHEDULING AND CONTROL Mapping 
the entire graphics pipeline efficiently onto GPU resources is a challenging problem that requires dynamic 
and adaptive techniques. A unique aspect of GPU computing is that hardware logic assumes a major role 
in mapping and scheduling computation onto chip resources. GPU hardware scheduling logic extends beyond 
the thread-scheduling responsibilities discussed in previous sections. GPUs automatically assign computa­tions 
to threads, clean up after threads complete, size and manage buffers that hold stream data, guarantee 
ordered processing when needed, and identify and discard unnec­essary pipeline work. This logic relies 
heavily on specific upfront knowledge of graphics workload characteristics. Conventional thread programming 
uses operating­system or threading API mechanisms for thread creation, completion, and synchronization 
on shared structures. Large-scale multithreading coupled with the brevity of shader function execution 
(at most a few hundred instructions), however, means GPU thread management must be performed entirely 
by hardware logic. GPUs minimize thread launch costs by preconfigur­ing execution contexts to run one 
of the pipeline s three types of shader functions and reusing the configuration multiple times for shaders 
of the same type. GPUs launch threads when a shader stage s input stream contains a sufficient number 
of entities, and then they automati­cally provide threads access to shader input records. Similar hardware 
logic commits records to the output stream buffer upon thread completion. The distribution of execution 
contexts to shader stages is reprovisioned periodically as pipeline needs change and stream buffers drain 
or approach capacity. GPUs leverage upfront knowledge of pipeline enti­ties to identify and skip unnecessary 
computation. For example, vertices shared by multiple primitives are identified and VP results cached 
to avoid duplicate vertex processing. GPUs also discard fragments prior to FP when the fragment will 
not alter the value of any image pixel. Early fragment discard is triggered when a fragment s sample 
point is occluded by a previously processed sur­face located closer to the camera. Another class of hardware 
optimizations reorganizes fine-grained operations for more efficient processing. For example, rasterization 
orders fragment generation to maximize screen proximity of samples. This ordering improves texture cache 
hit rates, as well as instruction stream sharing across shader invocations. The GPU mem­ory controller 
also performs automatic reorganization when it reorders memory requests to optimize memory bus and DRAM 
utilization. GPUs ensure inter-fragment PO ordering dependen­cies using hardware logic. Implementations 
use structures such as post-FP reorder buffers or scoreboards that delay fragment thread launch until 
the processing of overlap­ping fragments is complete. GPU hardware can take responsibility for sophisticated 
scheduling decisions because semantics and invariants of  Running a Fragment Shader on a GPU Core Shader 
compilation to SIMD (single instruction, multiple data) instruction sequences coupled with dynamic hardware 
thread scheduling leads to ef.cient execution of a fragment shader on the simpli.ed single-core GPU shown 
in .gure A. UÊÊ/.iÊV.ÀiÊiÝiVÕÌiÃÊ>.Ê..ÃÌÀÕVÌ...ÊvÀ..Ê>ÌÊ..ÃÌÊ..iÊÌ.Ài>`Ê each processor clock, but maintains 
state for four threads on-chip simultaneously (T=4). UÊÊ .ÀiÊÌ.Ài>`ÃÊ.ÃÃÕiÊiÝ« .V.ÌÊÜ.`Ì..ÎÓÊ- ÊÛiVÌ.ÀÊ..ÃÌÀÕV­tions; 
32 ALUs simultaneously execute a vector instruction in a single clock. UÊÊ/.iÊV.ÀiÊ.>ÃÊ>Ê«.. Ê.vÊ£ÈÊ}i.iÀ> 
.«ÕÀ«.ÃiÊÛiVÌ.ÀÊÀi}.ÃÌiÀÃÊ (R0 to R15) that are partitioned among thread contexts. The elements of each 
length-32 vector are 32-bit values. UÊÊ/.iÊ.. ÞÊÃ.ÕÀViÊ.vÊÌ.Ài>`ÊÃÌ> ÃÊ.ÃÊÌiÝÌÕÀiÊ>VViÃÃÆÊÌ.iÞÊ.>ÛiÊ 
a maximum latency of 50 cycles. Shader compilation by the graphics driver produces a GPU binary from 
a high-level fragment shader source. The resulting vector instruction sequence performs 32 invoca­tions 
of the fragment shader simultaneously by carrying out each invocation in a single lane of the width-32 
vectors. The compiled binary requires four vector registers for temporary results and contains 20 arithmetic 
instructions between each texture access operation. At runtime, the GPU executes a copy of the shader 
binary on each of its four thread contexts, as illustrated in .gure B. The core executes T0 (thread 0) 
until it detects a stall resulting from texture access in cycle 20. While T0 waits for the result of 
the texturing operation, the core continues to execute its remaining three threads. The result of T0 
s texture access becomes available in cycle 70. Upon T3 s stall in cycle 80, the core immediately resumes 
T0. Thus, at no point dur­ing execution are ALUs left idle. When executing the shader program for this 
example, a minimum of four threads is needed to keep core ALUs busy. Each thread operates simultaneously 
on 32 fragments; thus, 4*32=128 fragments are required for the chip to achieve peak performance. As memory 
latencies on real GPUs involve hundreds of cycles, modern GPUs must contain support for signi.cantly 
more threads to sustain high utilization. If we extend our simple GPU to a more realistic size of eight 
processing cores and provision each core with storage for 16 execution contexts, then simultaneous processing 
of 4,096 fragments is needed to approach peak processing rates. Clearly, GPU performance relies heavily 
on the abundance of parallel shading work.   the graphics pipeline are known a priori. Hardware imple­mentation 
enables fine-granularity logic that is informed by precise knowledge of both the graphics pipeline and 
the underlying GPU implementation. As a result, GPUs are highly efficient at using all available resources. 
The drawback of this approach is that GPUs execute only those computations for which these invariants 
and struc­tures are known. Graphics programming is becoming increasingly versatile. Developers constantly 
seek to incorporate more sophisticated algorithms and leverage more configurable graphics pipelines. 
Simultaneously, the growing popular­ity of GPGPU (general-purpose computing using GPU platforms) has 
led to new interfaces for accessing GPU resources. Given both of these trends, the extent to which GPU 
designers can embed a priori knowledge of com­putations into hardware scheduling logic will inevitably 
decrease over time. A major challenge in the evolution of GPU program­ming involves preserving GPU performance 
levels while increasing the generality and expressiveness of applica­tion interfaces. The designs of 
GPGPU interfaces, such as NVIDIA s CUDA and AMD s CAL, are evidence of how difficult this challenge is. 
These frameworks abstract computation as large batch operations that involve many invocations of a kernel 
function operating in parallel. The resulting computations execute on GPUs efficiently only under conditions 
of massive data parallelism. Programs that attempt to implement non-data-parallel algorithms perform 
poorly. GPGPU programming models are simple to use and permit well-written programs to make good use 
of both GPU programmable cores and (if needed) texturing resources. Programs using these interfaces, 
however, can­not use powerful fixed-function components of the chip, such as those related to compression, 
image compositing, or rasterization. Also, when these interfaces are enabled, much of the logic specific 
to graphics-pipeline scheduling is simply turned off. Thus, current GPGPU programming frameworks restrict 
computations so that their structure, as well as their use of chip resources, remains sufficiently simple 
for GPUs to run these programs in parallel.  GpU AND CpU CONVERGENCE The modern graphics processor is 
a powerful computing platform that resides at the extreme end of the design space of throughput-oriented 
architectures. A GPU s pro­cessing resources and accompanying memory system are heavily optimized to 
execute large numbers of operations in parallel. In addition, specialization to the graphics domain has 
enabled the use of fixed-function processing and allowed hardware scheduling of a parallel computa­tion 
to be practical. With this design, GPUs deliver unsur­passed levels of performance to challenging workloads 
while maintaining a simple and convenient programming interface for developers. Today, commodity CPU 
designs are adopting features common in GPU computing, such as increased core counts and hardware multithreading. 
At the same time, each generation of GPU evolution adds flexibility to pre­vious high-throughput GPU 
designs. Given these trends, software developers in many fields are likely to take interest in the extent 
to which CPU and GPU architec­tures and, correspondingly, CPU and GPU programming systems, ultimately 
converge. Q LOVE IT, HATE IT? LET US KNOW feedback@acmqueue.com or www.acmqueue.com/forums KAYVON FATAHALIAN 
is a Ph.D. candidate in computer science in the Computer Graphics Laboratory at Stanford University. 
His research interests include programming systems for commodity parallel architectures and computer 
graphics/animation systems for the interactive and film domains. His thesis research seeks to enable 
execution of more flexible rendering pipelines on future GPUs and multi­core PCs. He will soon be looking 
for a job. MIKE HOUSTON is a Ph.D. candidate in computer science in the Computer Graphics Laboratory 
at Stanford University. His research interests include programming models, algo­rithms, and runtime systems 
for parallel architectures includ­ing GPUs, Cell, multicore CPUs, and clusters. His dissertation includes 
the Sequoia runtime system, a system for program­ming hierarchical memory machines. He received his B.S. 
in computer science from UCSD in 2001 and is a recipient of the Intel Graduate Fellowship. &#38;#169; 
2008 ACM 1542-7730/08/0300 $5.00   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401148</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>11</pages>
		<display_no>12</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Future graphics architectures]]></title>
		<page_from>1</page_from>
		<page_to>11</page_to>
		<doi_number>10.1145/1401132.1401148</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401148</url>
		<abstract>
			<par><![CDATA[<p>GPUs continue to evolve rapidly, but toward what?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10011014</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Concurrent programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10011777.10011014</concept_id>
				<concept_desc>CCS->Computing methodologies->Concurrent computing methodologies->Concurrent programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098631</person_id>
				<author_profile_id><![CDATA[81547494456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel and University of Texas, Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141947</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blythe, D. 2006. The Direct3D 10 system. In <i>ACM SIGGRAPH 2006 Papers:</i> 724--734.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L., Carpenter, L., Catmull, E. 1987. The Reyes image rendering architecture. <i>Computer Graphics (Proceedings of ACM SIGGRAPH):</i> 95--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>195576</ref_obj_id>
				<ref_obj_pid>195473</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Laudon, J., Gupta, A., Horowitz, M. 1994. Interleaving: a multithreading technique targeting multiprocessors and workstations. In <i>Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems:</i> 308--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Mark, W., Fussell, D. 2005. Real-time rendering systems in 2010. Technical Report 05-18, University of Texas.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>363368</ref_obj_id>
				<ref_obj_pid>363347</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Myer, T. H., Sutherland, I. E. 1968. On the design of display processors. <i>Communications of the ACM</i>, 11(6): 410--414.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
  FOCUS GPUs  GPUs continue to evolve rapidly, but toward what? raphics architectures are in the midst 
of a major transition. In the past, these were specialized architectures designed to support a single 
rendering algorithm: the standard Z buffer. Realtime 3D graphics has now advanced to the point where 
the Z-buf­fer algorithm has serious shortcomings for generating the next generation of higher­quality 
visual effects demanded by games and other interactive 3D applications. There is also a desire to use 
the high computa­tional capability of graphics architectures to support collision detection, approximate 
physics simulations, scene management, and simple artificial intelligence. In response to these forces, 
graphics architectures are evolving toward a general-purpose parallel-programming  FUTURE GRAPHICS 
ARCHITECTURES model that will support a variety of image-synthesis algo­rithms, as well as nongraphics 
tasks. This architectural transformation presents both opportunities and challenges. For hardware designers, 
the primary challenge is to balance the demand for greater programmability with the need to continue 
deliver­ing high performance on traditional image-synthesis algorithms. Software developers have an opportunity 
to escape from the constraints of hardware-dictated image­synthesis algorithms so that almost any desired 
algorithm can be implemented, even those that have nothing to do with graphics. With this opportunity, 
however, comes the challenge of writing ef.cient, high-performance parallel software to run on the new 
graphics architectures. Writ­ing such software is substantially more dif.cult than writ­ing the single-threaded 
software that most developers are accustomed to, and it requires that programmers address challenges 
such as algorithm parallelization, load balanc­ing, synchronization, and management of data locality. 
The transformation of graphics hardware from a spe­cialized architecture to a .exible high-throughput 
parallel architecture will have an impact far beyond the domain of computer graphics. For a variety of 
technical and busi­ness reasons, graphics architectures are likely to evolve into the dominant high-throughput 
manycore architec­tures of the future. This article begins by describing the high-level forces that drive 
the evolution of realtime graphics systems, then moves on to some of the detailed technical trends in 
realtime graphics algorithms that are emerging in response to these high-level forces. Finally, it considers 
how future graphics architectures are expected to evolve to accommodate these changes in graphics algorithms 
and discusses the challenges that these architectures will present for software developers. !00,)#!4)/.3/,54)/.'2!0()#3#()4%#452%3 
To understand what form future graphics architectures are likely to take, we need to examine the forces 
that are driving the evolution of these architectures. As with any engineered artifact, graphics architectures 
are designed to deliver the maximum bene.t to the end user within the fundamental technology constraints 
that determine what is affordable at a particular point in time. As VLSI (very large-scale integration) 
fabrication technology advances, the boundary of what is affordable changes, so that each generation 
of graphics architecture can provide additional capabilities at the same cost as the previous generation. 
Thus, the key high-level question is: What do we want these new capabilities to be? Roughly speaking, 
graphics hardware is used for three purposes: 3D graphics, particularly entertainment applica­tions (i.e., 
games); 2D desktop display, which used to be strictly 2D but now uses 3D capabilities for compositing 
desktops such as those found in Microsoft s Vista and Apple s Mac OS X; and video playback (i.e., decompres­sion 
and display of streaming video and DVDs). Although for most users desktop display and video playback 
are more important than 3D graphics, this article focuses on the needs of 3D graphics because these applications, 
with their signi.cant demands for perfor­mance and functionality, have been the strongest force driving 
the evolution of graphics architectures. Designing a graphics system for future 3D entertain­ment applications 
is particularly tricky because at a technical level the goals are ill de.ned. It is currently not possible 
to compute an image of the ideal quality at real­time frame rates, as evidenced by the fact that the 
images in computer-generated movies are of higher quality than those in computer games. Thus, designers 
must make approximations to the ideal computation. There are an enormous variety of possible approximations 
to choose from, each of which introduces a different kind of visual artifact in the image, and each of 
which uses different algorithms that may in turn run best on different archi­tectures. In essence, the 
system design problem is reduced to the ill-speci.ed problem of which system (software and hardware) 
produces the best-quality game images for a speci.c cost. Figure 1 illustrates this problem. In prac­tice, 
there are also other constraints, such as backward compatibility and a desire to build systems that facilitate 
content creation.  As VLSI technology advances with time, the system designer is provided with more 
transistors. If we assume that the frame rate is .xed at 60 Hz, the additional computational capability 
provided by these transistors can be used in three fundamental ways: increasing the screen resolution; 
increasing the scene detail (polygon count or material shader complexity); and changing the overall approximations, 
by changing the basic rendering algorithm or speci.c components of it. Looking back at the past six years, 
we can see these forces at work. Games have adopted program­mable shaders that allow sophisticated modeling 
of materials and multipass techniques that approxi­mate shadows, re.ections, and other effects. Graphics 
architectures have enabled these changes through the addition of programmable vertex and fragment units, 
as well as more .exibil­ity in how data moves between stages in the graphics pipeline. Current graphics 
proces­sors use the programming model illustrated in .gure 2a. This model supports the traditional Z-buffer 
algorithm and is organized around a prede.ned pipe­line structure that is only partially recon.gurable 
by the application.1 The prede.ned pipeline struc­ture employs specialized hardware for the Z-buffer 
algorithm (in particular for polygon rasterization and Z-buffer read-modify-write operations), as well 
as for other operations such as the thread scheduling needed by the programmable stages. Many of the 
individual pipeline stages are program­mable (to support programmable material shading computations in 
particular), with all of the program­mable stages multiplexed onto a single set of homoge­neous programmable 
hardware processors. The programs executing within these pipeline stages, however, are heavily restricted 
in how they can communicate with each other and in how if at all they can access the global shared memory. 
This programming model provides high performance for the computations it is designed to support, but 
makes it dif.cult to support other computa­tions ef.ciently. It is important to realize that modern game 
applica­tions fundamentally require programmability in the graphics hardware. This is because the real 
world contains an enormous variety of materials (wood, metal, glass, skin, fur, ...), and the only reasonable 
way to specify the  FUTURE GRAPHICS ARCHITECTURES interactions of these materials with light is to 
use a differ­ent program for each material. This situation is very different from that found for other 
high-performance tasks, such as video decode, which does not inherently require programmable hardware; 
one could design fixed-function hardware sufficient to support the standard video formats without any 
programmability at all. As a practical matter most video-decode hardware does include some program­mable 
units, but this is an implementation choice, not a fundamental requirement. This need for programmability 
by 3D graphics applications makes graphics architectures uniquely well positioned to evolve into more 
general high-throughput parallel computer architectures that handle tasks beyond graphics. LiMits of 
the trAditionAL Z-buffer grAphics pipeLine The Z-buffer graphics pipeline with programmable shad­ing 
that is used as the basis of today s graphics architec­tures makes certain fundamental approximations 
and assumptions that impose a practical upper limit on the image quality. For example, a Z buffer cannot 
efficiently determine if two arbitrarily chosen points are visible from each other, as is needed for 
many advanced visual effects. A ray tracer, on the other hand, can efficiently make this determination. 
For this reason, computer-generated mov­ies use rendering techniques such as ray-tracing algo­rithms 
and the Reyes (renders everything you ever saw) algorithm2 that are more sophisticated than the standard 
Z-buffer graphics pipeline. Over the past few years, it has become clear that the next frontier for improved 
visual quality in realtime 3D graphics will involve modeling lighting and complex illumination effects 
more realistically (but not necessarily photo-realistically) so as to produce images that are closer 
in quality to those of computer-generated movies. These effects include hard-edged shadows (from small 
lights), soft-edged shadows (from large lights), reflections from water, and approximations to more complex 
effects such as diffuse lighting interactions that dominate most inte­rior environments. There is also 
a desire to model effects such as motion blur and to use higher-quality anti-alias­ing techniques. Most 
of these effects are challenging to produce with the traditional Z-buffer graphics pipeline. Modern game 
engines (e.g., Unreal Engine 3, CryEn­gine 2) have begun to support some of these effects using today 
s graphics hardware, but with significant limita­tions. For example, Unreal Engine 3 uses four different 
shadow algorithms, because no one algorithm provides an acceptable combination of performance and image 
quality in all situations. This problem is a result of limita­tions on the visibility queries that are 
supported by the traditional Z-buffer pipeline. Furthermore, it is common for different effects such 
as shadows and partial transpar­ency to be mutually incompatible (e.g., partially trans­parent objects 
cast shadows as if they were fully opaque objects). This lack of algorithmic robustness and gener­ality 
is a problem for both game-engine programmers and for the artists who create the game content. These 
limitations can also be viewed as violations of impor­tant principles of good system design such as abstrac­tion 
(a capability should work for all relevant cases) and orthogonality (different capabilities should not 
interact in unexpected ways). The underlying problem is that the traditional Z-buffer graphics pipeline 
was designed to compute visibility (i.e., the first surface hit) for regularly spaced rays originat­ing 
at a single point (see figure 3a), but effects such as hard-edged shadows, soft-edged shadows, reflections, 
and diffuse lighting interactions all require more general vis­ibility computations. In particular, reflections 
and diffuse lighting interactions require the ability to compute visible surfaces efficiently along rays 
with a variety of origins and directions (figure 3d). These types of visibility queries cannot be performed 
efficiently with the traditional graphics pipeline, but VLSI technology now provides enough transistors 
to support more sophisticated realtime visibility algorithms that can perform these queries efficiently. 
These transistors, however, must be organized into an architecture that can efficiently support the more 
sophisticated visibility algorithms. Since the Z-buffer graphics pipeline is ill suited for producing 
the desired effects, the natural solution is to design graphics systems around more powerful visibil­ity 
algorithms. Figure 3 provides an overview of some of these algorithms. I believe that these more powerful 
visibility algorithms will be gradually adopted over the next few years in response to the inadequacies 
of the standard Z buffer, although there is substantial debate in the graphics community as to how rapidly 
this change will occur. In particular, algorithms such as ray tracing are likely to be adopted much more 
rapidly in realtime graphics than they were in movie rendering, because real­time graphics does not permit 
the hand-tweaking of light­ing for every shot that is common in movie rendering. THE ARGUMENT FOR GENERAL-PURPOSE 
GRAPHICS HARDWARE Given the desire to support more powerful visibility algo­rithms, graphics architects 
could take several approaches. Should the new visibility techniques be implemented in some kind of specialized 
hardware (like today s Z-buffer visibility computations), or should they be implemented in software on 
a .exible parallel architecture? I believe that a .exible parallel architecture is the best choice, because 
it supports the following software capabilities: Mixing visibility techniques. Flexible hardware sup­ports 
multiple visibility algorithms, ranging from the traditional Z buffer to ray tracing and beam tracing. 
Each application can choose the best algorithm(s) for its needs. The more sophisticated of these visibility 
algo­rithms require the ability to build and traverse irregular data structures such as KD-trees, which 
demands a more .exible parallel programming model than that used by today s GPUs. Application-tailored 
approximations. Rendering images at realtime frame rates requires making math­ematical approximations 
(e.g., for particular lighting effects), but the variety of possible approximations is enormous. Often, 
different approximations use very different overall rendering algorithms and have very different performance 
characteristics. Since the best approximation and algorithm vary from application to application and 
sometimes even within an application, an architecture that allows the application to choose its approximations 
can provide far greater ef.ciency for the overall rendering task than an architecture that lacks this 
.exibility. Integration of rendering with scene management. Traditionally, realtime graphics systems 
have used one set of data structures to represent the persistent state of the scene (e.g., object positions, 
velocities, and groupings) and a different set of data structures to compute visibility. The two sets 
of data structures are on opposite sides of an intervening API such as DirectX or OpenGL. For every frame, 
all of the visible geometry is transferred across this API. In a Z-buffer system this approach works 
because it is relatively straightforward to determine which geometry might be visible. In a ray-tracing 
system, however, this approach does not work very well, and it is desirable to integrate the two sets 
of data structures more tightly, with both residing on the graphics processor (.gure 4). It is also desirable 
to change the traditional layering of APIs so that the game engine takes over most of the low-level rendering 
tasks currently handled by graphics hardware (.gure 5). A highly programmable architecture makes it much 
easier to do this integration while still preserving .exibility for the application to maintain the persistent 
  FUTURE GRAPHICS ARCHITECTURES data structures in the most ef.cient manner. It also allows scene management 
computations to be performed on the high-performance graphics hardware, eliminating a bottleneck on the 
CPU. Support for game physics and AI. A .exible parallel architecture can easily support computations 
such as collision detection, .uid dynamics simulations (e.g., for explosions), and arti.cial intelligence 
for game play. It also allows these computations to be tightly integrated with the rendering computation. 
Rapid innovation. Software can be changed more rap­idly than hardware, so a .exible parallel architecture 
that uses software to express its graphics algorithms enables more rapid innovation than traditional 
designs. The best choice for the system as a whole is to use .ex­ible parallel hardware that permits 
software to use aggres-sive algorithmic specialization and optimization, rather than to use specialized 
parallel hardware that mandates a particular algorithm.  02/'2!--).'/$%, When I say that future graphics 
architectures are likely to support an extremely .exible parallel programming model, what do I mean? 
There is considerable debate within the graphics hardware community as to the speci.c programming model 
that graphics architectures should adopt in the near future. I expect that in the short term each of 
the major graphics hardware companies will take a somewhat different path. There are a variety of reasons 
for this diversity: different emphasis placed on adding new capabilities versus improving performance 
of the old programming models; fundamental philosophical differences in tackling the parallel programming 
prob­lem; and the desire by some companies to evolve existing designs incrementally. In the longer term 
(.ve years or so), the program­ming models will probably converge, but there is not yet a consensus on 
what such a converged programming model would look like. This section presents some of the key issues 
that today s graphics architects face, as well as thoughts on what a converged future program­ming model 
could look like and the challenges that it will present for programmers. Most of the programming challenges 
discussed here will be applicable to all future graphics architectures, even those that are somewhat 
dif­ferent from the one I am expecting.  end of the hArdwAre-defined pipeLine Graphics processors will 
evolve toward a programming model similar to that illustrated in figure 2b. User-written software specifies 
the overall structure of the computa­tion, expressed in an extremely flexible parallel program­ming model 
similar to that used to program today s multicore CPUs. The user-written software may option­ally use 
specialized hardware to accelerate specific tasks such as texture mapping. The specialized hardware may 
be accessed via a combination of instructions in the ISA (instruction set architecture), special memory-mapped 
registers, and special inter-processor messages. The latest generation of GPUs (graphics process­ing 
units) from NVIDIA and AMD have already taken a significant step toward this future graphics programming 
model by supporting a separate programming model for nongraphics computations that is more flexible than 
the programming model used for graphics. This second programming model is an assembly-level parallel-pro­gramming 
model with some capabilities for fine-grained synchronization and data sharing across hardware threads. 
NVIDIA calls its model PTX (Parallel Thread Execution), and AMD s is known as CTM (Close to Metal). Note 
that NVIDIA s C-like CUDA language (see Scalable Parallel Programming with CUDA in this issue) is a layer 
on top of the assembly-level PTX. It is important to real­ize, however, that PTX and CTM have some significant 
limitations compared with traditional general-purpose parallel programming models. PTX and CTM are still 
fairly restrictive, especially in their memory and concur­rency models. These limitations become obvious 
when comparing PTX and CTM with the programming models supported by other single-chip highly parallel 
processors, such as Sun s Niagara server chips. I believe that the program­ming model of future graphics 
architectures will be substantially more flexible than PTX and CTM. tAsk pArALLeLisM And MuLtithreAding 
The parallelism supported by current GPUs primarily takes the form of data parallelism that is, the GPU 
oper­ates simultaneously on many data elements (such as ver­tices or pixels or elements in an array). 
In contrast, task parallelism is not supported well, except for the specific case of concurrent processing 
of pixels and vertices. Since better support for task parallelism is necessary to support user-defined 
rendering pipelines efficiently, I expect that future GPUs will support task parallelism much more aggressively. 
In particular, multiple tasks will be able to execute asynchronously from each other and from the CPU, 
and will be able to communicate and synchronize with each other. These changes will require a substantially 
more sophisticated software runtime environment than the one used for today s GPUs and will introduce 
signifi­cant complexity into the hardware/software interactions for thread management. As with today 
s GPUs and Sun s Niagara processor, each core will use hardware multithreading,3 possibly aug­mented 
by additional software multithreading along the lines of that used by programmers of the Cell architec­ture. 
This multithreading serves two purposes: First, it allows the core to remain fully utilized even if each 
individual instruction has a pipeline latency of several cycles the core just executes an instruction 
from another thread. Second, it allows the core to remain fully utilized even if one or more of the 
threads on the core stalls because of an off-chip DRAM access such as those that occur when fetching 
data from a texture. Programmers will face the challenge of exposing parallelism for multiple cores and 
for multiple threads on each core. This challenge is already starting to appear with programming models 
such as NVIDIA s CUDA. siMd execution within eAch core An important concern in the design of graphics 
hardware is obtaining the maximum possible performance using a fixed number of transistors on a chip. 
If one instruction cache/fetch/decode unit can be shared among several arithmetic units, the die area 
and power requirements of the hardware are reduced, as compared with a design that has one instruction 
unit per arithmetic unit. That  FUTURE GRAPHICS ARCHITECTURES is, a SIMD (single instruction, multiple 
data) execution model increases efficiency as long as most of the elements in the SIMD vectors are kept 
active most of the time. A SIMD execution model also provides a simple form of fine-grained synchronization 
that helps to ensure that memory accesses have good locality. Current graphics hardware uses a SIMD execu­tion 
model, although it is sometimes hidden from the programmer behind a scalar programming interface as in 
NVIDIA s hardware. One area of ongoing debate and change is likely to be in the underlying hardware SIMD 
width; there is a tension between the efficiency gained for regular computations as SIMD width increases 
and the efficiency gained for irregular computations as SIMD width decreases. NVIDIA GPUs (GeForce 8000 
and 9000 series) have an effective SIMD width of 32, but the trend has been for the SIMD width of GPUs 
to decrease to improve the efficiency of algorithms with irregular con­trol flow. There is also debate 
about how to expose the SIMD execution model. It can be directly exposed to the pro­grammer with register-SIMD 
instructions, as is done with x86 SSE instructions, or it may be nominally hidden from the programmer 
behind a scalar programming model, as is the case with NVIDIA s GeForce 9000 series. If the SIMD execution 
model is hidden, the conversion from the scalar programming model to the SIMD hardware may be performed 
by either the hardware (as in the GeForce 9000 series) or a compiler or some combination of the two. 
Regardless of which strategy is used, programmers who are concerned with performance will need to be 
aware of the underlying SIMD execution model and width. sMALL AMounts of LocAL storAge One of the most 
important differences between GPUs and CPUs is that GPUs devote a greater fraction of their tran­sistors 
to arithmetic units, whereas CPUs devote a greater fraction of their transistors to cache. This difference 
is one of the primary reasons that the peak performance of a GPU is much higher than that of a CPU. I 
expect that this difference will continue in the future. The impact on programmers will be significant: 
although the overall programming model of future GPUs will become much closer to that of today s CPUs, 
pro­grammers will need to manage data locality much more carefully on future GPUs than they do on today 
s CPUs. This problem is made even more challenging by multithreading; if there are N threads on each 
core, the amount of local storage per thread per core is effectively 1/N of the core s total local storage. 
This issue can be mitigated if the N threads on a core are sharing a working set, but to do this the 
programmer must think of the N threads as being closely coupled to each other. Similarly, programmers 
will have to think about how to share a working set across threads on different cores. These considerations 
are already becoming apparent with CUDA. The constraints are likely to be frustrating to programmers 
who are accustomed to the large caches of CPUs, but they need to realize that extra local storage would 
come at the cost of fewer ALUs (arithmetic logic units), and they will need to work closely with hardware 
designers to determine the optimum balance between cache and ALUs. cAche-coherent shAred MeMory The most 
important aspect of any parallel architecture is its overall memory and communication model. To illus­trate 
the importance of this aspect of the design, consider four (of many) possible alternatives (of course, 
hybrids and enhancements of these models are possible): A message-passing architecture, in which each 
processor core has its own memory space and all communication occurs through explicit message passing. 
Most large­scale supercomputers (those with 100-plus processors) use this model. An architecture such 
as the Sony/Toshiba/IBM Cell with a noncached, noncoherent shared memory. In such an architecture, all 
transfers of data between a core s small private memory and the global memory must be orches­trated through 
explicit memory-transfer commands. An architecture such as NVIDIA s GeForce 8800 with what amounts to 
a minimally cached, noncoherent shared memory, with support for load/store to this memory. An architecture 
such as modern multicore CPUs, with cached, coherent shared memory. In such architectures, hardware mechanisms 
manage transfer of data between cache and main memory and ensure that data in caches of different processors 
remains consistent. There is considerable debate within the graphics archi­tecture community as to which 
memory and communi­cation model would be best for future architectures, and in the near term different 
hardware vendors are taking different approaches. Software programmers should think carefully about these 
issues so that they are prepared to influence the debate. Which approach is most likely to dominate 
in the medium to long term? I have previously argued that the trend in rendering algorithms is toward 
those that build and traverse irregular data structures. These irregular data structures allow algorithms 
to adapt to the scene geom­etry and the current viewpoint. Explicitly managing all data locality for 
these algorithms is painful, especially if multiple cores share a read/write data structure. In my experience, 
it is easier to develop these algorithms on a cache-coherent architecture, even if achieving optimal 
 performance often still requires thinking very carefully about the communication and memory-access patterns 
of the performance-critical kernels. For these and other reasons too detailed to discuss here, I believe 
that future graphics architectures will efficiently support a cache-coherent memory model, and that any 
architecture lacking these capabilities will be a second choice at best for programmers who are develop­ing 
innovative rendering techniques. Sun s Niagara archi­tecture provides a good preview of the kind of memory 
and threading model that I anticipate for future GPUs. I also expect, however, that cache-coherent graphics 
archi­tectures will include a variety of mechanisms that provide the programmer with explicit control 
over communica­tion and memory access, such as streaming loads that bypass the cache. fine-grAined speciALiZAtion 
The desire to support greater algorithmic diversity will drive future graphics architectures toward greater 
flex­ibility and generality, but specialization will still be used where it provides a sufficiently large 
benefit for the major­ity of applications. Most of this specialization will be at a fine granularity, 
used to accelerate specific operations, in contrast to the coarse, monolithic granularity used to dictate 
the overall structure of the algorithms executed on the hardware in the past. In particular, I expect 
the following specialization will continue to exist for graphics architectures: Texture hardware. Texture 
addressing and filtering operations use low-precision (typically 16-bit) values that are decompressed 
on the fly from a compressed represen­tation stored in memory. The amount of data accessed is large and 
requires multithreading to deal effectively with cache misses. These operations are a significant fraction 
of the overall rendering cost and benefit enormously from specialized hardware. Specialized floating-point 
operations. Rendering makes heavy use of floating-point square-root and recip­rocal operations. Current 
graphics hardware provides high-performance instructions for these operations, as well as other operations 
used for shading such as swiz­zling and trigonometric functions. Future graphics hard­ware will need 
to do the same. Video playback and desktop compositing. Video play­back and 2D and 2.5D desktop window 
operations benefit significantly from specialized hardware. Specialization of these operations is especially 
important for power effi­ciency. I anticipate that much of this hardware will follow the traditional 
coarse-grained monolithic fixed-function model and thus will not be useful for user-written 3D graphics 
programs. Current graphics hardware also includes specialized hardware to assist with triangle rasterization, 
but I expect that this task will be taken over by software within a few years. The reason is that rasterization 
is gradually becoming a smaller fraction of total rendering costs, so the penalty for implementing it 
in software is decreasing. This trend will accelerate as more sophisticated visibility algorithms supplement 
or replace the Z buffer. As graphics software switches to more powerful vis­ibility algorithms such as 
ray tracing, it may become clear that certain operations represent a sufficiently large por­tion of the 
total computation cost that hardware accelera­tion would be justified. For example, future architectures 
could include specialized instructions to accelerate the data-structure traversal operations used by 
ray tracing. the chALLenge for grAphics Architects At a high level, the key challenge facing future graphics 
architectures is to strike the best balance between the desire to provide high performance on existing 
graph­ics algorithms and the desire to provide the flexibility needed to support new algorithms with 
high perfor­  FUTURE GRAPHICS ARCHITECTURES mance, including nongraphics algorithms and the next generation 
of more capable and sophisticated graphics algorithms. I believe that the opportunity for improved visual 
quality and robustness provided by more sophis­ticated graphics algorithms will cause the transition 
to more flexible architectures to happen relatively rapidly, an opinion that remains a matter of debate 
within the graphics architecture community.  THE FUTURE OF GRAPHICS ARCHITECTURES In the past, graphics 
architectures defined the algorithms used for rendering and their performance. In the future, graphics 
architectures will cease to define the render­ing algorithms and will simply set the performance and 
power efficiency limits within which software developers may do whatever they want. For the programmer, 
future graphics architectures are likely to be very similar to today s multicore CPU archi­tectures, 
but with greater SIMD instruction widths and the availability of specialized instructions and processing 
units for some operations. Like today s Niagara processor, however, the amount of cache per processor 
core will be relatively small. To achieve peak performance, program­mers will have to think more carefully 
about memory­access patterns and data-structure sizes than they have been accustomed to with the large 
caches of modern CPUs. Future graphics architectures will enable a golden age of innovation in graphics; 
I expect that over the next few years we will see the development of a variety of new rendering algorithms 
that are more efficient and more capable than the ones used in the past. For computer games, these architectures 
will allow game logic, phys­ics simulation, and AI to be more tightly integrated with rendering than 
before. For data-visualization applications, these architectures will allow tight integration of domain­specific 
data analysis with the rendering computations used to display the results of this analysis. The general­purpose 
nature of these architectures combined with the low cost enabled by their high-volume market will also 
cause them to become the preferred platform for almost all high-performance floating-point computations. 
Q AcknowLedgMents And further reAding Don Fussell, Kurt Akeley, Matt Pharr, Pat Hanrahan, Mark Horowitz, 
Stephen Junkins, and several graphics hardware architects contributed directly and indirectly to the 
ideas in this article through many fun and productive discus­sions. More details about many of the ideas 
discussed in this article can be found in another article I wrote with Don Fussell in 2005.4 The tendency 
of graphics hardware to become increasingly general until the temptation emerges to incorporate new specialized 
units has existed for a long time and was described in 1968 as the wheel of reincarnation by Myer and 
Sutherland.5 The funda­mental need for programmability in realtime graphics hardware, however, is much 
more important now than it was then. references 1. Blythe, D. 2006. The Direct3D 10 system. In ACM SIG-GRAPH 
2006 Papers: 724 734. 2. Cook, R.L., Carpenter, L., Catmull, E. 1987. The Reyes image rendering architecture. 
Computer Graphics (Pro­ceedings of ACM SIGGRAPH): 95 102. 3. Laudon, J., Gupta, A., Horowitz, M. 1994. 
Interleaving: a multithreading technique targeting multiprocessors and workstations. In Proceedings of 
the Sixth Interna­tional Conference on Architectural Support for Programming Languages and Operating 
Systems: 308 318. 4. Mark, W., Fussell, D. 2005. Real-time rendering systems in 2010. Technical Report 
05-18, University of Texas. 5. Myer, T.H., Sutherland, I.E. 1968. On the design of display processors. 
Communications of the ACM, 11(6): 410 414.  LOVE IT, HATE IT? LET US KNOW feedback@acmqueue.com or 
www.acmqueue.com/forums BILL MARK leads intel s advanced graphics research lab. he is on leave from the 
university of texas at Austin, where until January 2008 he led a research group that investigated future 
graphics algorithms and architectures. in 2001-2002 he was the technical leader of the team at nVidiA 
that co­designed (with Microsoft) the cg language for programma­ble graphics hardware and developed the 
first release of the nVidiA cg compiler. his research interests focus on systems and hardware architectures 
for realtime computer graphics and on the opportunity to extend these systems to support more general 
parallel computation and a broader range of graphics algorithms, including interactive ray tracing. &#38;#169; 
2008 AcM 1542-7730/08/0300 $5.00  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401149</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>15</pages>
		<display_no>13</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Parallel programming models overview]]></title>
		<page_from>1</page_from>
		<page_to>15</page_to>
		<doi_number>10.1145/1401132.1401149</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401149</url>
		<categories>
			<primary_category>
				<cat_node>D.1.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Concurrent, distributed, and parallel languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.3.3</cat_node>
				<descriptor>Concurrent programming structures</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011024.10011034</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language features->Concurrent programming structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010177</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10011014</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Concurrent programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010175</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10011014</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Concurrent programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10011777.10011014</concept_id>
				<concept_desc>CCS->Computing methodologies->Concurrent computing methodologies->Concurrent programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098632</person_id>
				<author_profile_id><![CDATA[81100458295]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Owens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1406956</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mattson/Sanders/Massingill, <i>Patterns for Parallel Programming</i>, Addison Wesley 2004]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015800</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Buck et al., "Brook for GPUs: Stream Computing on Graphics Hardware", Siggraph 2004]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383274</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lindholm et al., "A User-Programmable Vertex Engine", Siggraph 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401150</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>10</pages>
		<display_no>14</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Data-parallel computing]]></title>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/1401132.1401150</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401150</url>
		<abstract>
			<par><![CDATA[<p>Users always care about performance. Although often it's just a matter of making sure the software is doing only what it should, there are many cases where it is vital to get down to the metal and leverage the fundamental characteristics of the processor.</p> <p>Until recently, performance improvement was not difficult. Processors just kept getting faster. Waiting a year for the customer's hardware to be upgraded was a valid optimization strategy. Nowadays, however, individual processors don't get much faster; systems just get more of them.</p> <p>Much comment has been made on coding paradigms to target multiple-processor cores, but the data-parallel paradigm is a newer approach that may just turn out to be easier to code to, and easier for processor manufacturers to implement.</p> <p>This article provides a high-level description of data-parallel computing and some practical information on how and where to use it. It also covers data-parallel programming environments, paying particular attention to those based on programmable graphics processors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.3</cat_node>
				<descriptor>Parallel programming</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169.10010175</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010175</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098633</person_id>
				<author_profile_id><![CDATA[81100112038]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1142511</ref_obj_id>
				<ref_obj_pid>1142473</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Govindaraju, N. K., Gray, J., Kumar, R., Manocha, D. 2006. GPUTeraSort: High-performance graphics coprocessor sorting for large database management. <i>Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data</i>; http://research.microsoft.com/research/pubs/view.aspx?msr_tr_id=MSR-TR-2005-183).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882363</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kr&#252;ger, J., Westermann, R. 2003. Linear algebra operators for GPU implementation of numerical algorithms. <i>ACM Transactions on Graphics</i> 22(3).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blythe, D. 2008. The Rise of the GPU. <i>Proceedings of the IEEE</i> 96(5).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Shubhabrata, S., Lefohn, A. E., Owens, J. D. 2006. A work-efficient step-efficient prefix sum algorithm. <i>Proceedings of the Workshop on Edge Computing Using New Commodity Architectures:</i> D-26-27.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1122505</ref_obj_id>
				<ref_obj_pid>1122501</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lefohn, A. E., Kniss, J., Strzodka, R., Sengupta, S., Owens, J. D. 2006. Glift: Generic, efficient, randomaccess GPU data structures. <i>ACM Transactions on Graphics 25(1)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[See reference 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GPUs FOCUS  CHAS. BOYD, MICROSOFT  Data parallelism is a key concept in leveraging the power of today 
s manycore GPUs. sers always care about performance. U Although often it s just a matter of making sure 
the software is doing only what it should, there are many cases where it is vital to get down to the 
metal and leverage the fundamental characteristics of the processor. Until recently, performance improvement 
was not dif­ ficult. Processors just kept getting faster. Waiting a year for the customer s hardware 
to be upgraded was a valid opti­ mization strategy. Nowadays, however, individual proces­ sors don t 
get much faster; systems just get more of them. Much comment has been made on coding paradigms to target 
multiple-processor cores, but the data-parallel paradigm is a newer approach that may just turn out to 
be easier to code to, and easier for processor manufactur­ ers to implement. This article provides a 
high-level description of data­parallel computing and some practical information on how and where to 
use it. It also covers data-parallel pro­gramming environments, paying particular attention to those 
based on programmable graphics processors. A BIT OF BACkgROUnD Although the rate of processor-performance 
growth seems almost magical, it is gated by fundamental laws of phys­ics. For the entire decade of the 
90s, these laws enabled processors to grow exponentially in performance as a result of improvements in 
gates-per-die, clock speed, and instruction-level parallelism. Beginning in 2003, though, the laws of 
physics (power and heat) put an end to growth in clock speed. Then the silicon area requirements for 
increasingly sophisticated ILP (instruction-level paral­lelism) schemes (branch prediction, speculative 
execu­tion, etc.) became prohibitive. Today the only remaining basis for performance improvement is gate 
count.  Recognizing this, manufacturers have restructured to stop pushing clock rate and focus on gate 
count. Forecasts project that gates-per-die can double every two years for the next six to eight years 
at least. What do you do with all those gates? You make more cores. The number of cores per die will 
therefore double every two years, resulting in four times today s core counts (up to 32 cores) by 2012. 
Customers will appreciate that growth rate, but they will benefit only if software becomes capable of 
scal­ing across all those new cores. This is the challenge that performance software faces in the next 
five to ten years. For the next decade, the limiting factor in software performance will be the ability 
of software developers to restructure code to scale at a rate that keeps up with the rate of core-count 
growth. PARAllEl PROgRAMMIng Parallel programming is difficult. We deprecate the use of GOTO statements 
in most languages, but parallel execu­tion is like having them randomly sprinkled throughout the code 
during execution. The assumptions about order of execution that programmers have made since their early 
education no longer apply. The single-threaded von Neumann model is com­prehensible because it is deterministic. 
Parallel code is subject to errors such as deadlock and livelock, race conditions, etc. that can be extremely 
subtle and difficult to identify, often because the bug is nonrepeatable. These issues are so severe 
that despite decades of effort and dozens of different approaches, none has really gained significant 
adoption or even agreement that it is the best solution to the problem. An equally subtle challenge is 
performance scaling. Amdahl s law states that the maximum speedup attain­able by parallelism is the reciprocal 
of the proportion of code that is not parallelizable. If 10 percent of a given code base is not parallel, 
even on an infinite number of processors it cannot attain more than a tenfold speedup. Although this 
is a useful guideline, determining how much of the code ends up running in parallel fashion is very difficult. 
Serialization can arise unexpectedly as a result of contention for a shared resource or requirements 
to access too many distant memory locations. The traditional methods of parallel programming (thread 
control via locks, message-passing interface, etc.) often have limited scaling ability because these 
mechanisms can require serialization phases that actually increase with core count. If each core has 
to synchronize with a single core, that produces a linear growth in serial code, but if each core has 
to synchronize with all other cores, there can be a combinatoric increase in serializa­tion. After all, 
any code that serializes is four times slower on a four-core machine, but 40 times slower on a 40-core 
machine. Another issue with performance scaling is more fundamental. A common approach in multicore paral­lel 
programming for games is to start with a top-down breakdown. Relatively isolated subsystems are assigned 
to separate cores, but what happens once the number of subsystems in the code base is reached? Since 
restructur­ing code at this level can be pervasive, it often requires a major rewrite to break out subsystems 
at the next finer level, and again for each hardware generation. For all these reasons, transitioning 
a major code base to parallel paradigms is time consuming. Getting all the subtle effects of nondeterminism 
down to an accept­able level can take years. It is likely that by that time, core-count growth will have 
already exceeded the level of parallelism that the new code structure can scale to. Unfortunately, the 
rate of core-count growth may be outstripping our ability to adapt to it. Thus, the time has come to 
look for a new paradigm ideally one that scales with core count but without requiring restructuring of 
the application architecture every time a new core count is targeted. After all, it s not about choosing 
a paradigm that operates well at a fixed core count; it s about choosing one that continues to scale 
with an increasing number of cores without requir­ing code changes. We need to identify a finer level 
of granularity for parallelism.  DATA-PARAllEl PROgRAMMIng Given the difficulty of finding enough subsystem 
tasks to assign to dozens of cores the only elements of which there are a comparable number are data 
elements the data-parallel approach is simply to assign an individual data element to a separate logical 
core for processing. Instead of breaking code down by subsystems, we look for fine-grained inner loops 
within each subsystem and parallelize those. For some tasks, there may be thousands to millions of data 
elements, enabling assignment to thousands of cores. (Although this may turn out to be a limitation in 
the future, it should enable code to scale for another decade or so.) For example, a modern GPU can support 
hundreds of ALUs (arithmetic logic units) with hundreds of threads per ALU for nearly 10,000 data elements 
on the die at once. The history of data-parallel processors began with the efforts to create wider and 
wider vector machines. Much of the early work on both hardware and data-parallel algorithms was pioneered 
at companies such as MasPar, Tera, and Cray. Today, a variety of fine-grained or data-parallel pro­gramming 
environments are available. Many of these have achieved recent visibility by supporting GPUs. They can 
be categorized as follows: Older languages (C*, MPL, Co-Array Fortran, Cilk, etc.). Several languages 
have been developed for fine­grained parallel programming and vector processing. Many add only a very 
small difference in syntax from well-known languages. Few of them support a variety of platforms and 
they may not be available commercially or be supported long term as far as updates, documentation, and 
materials. Newer languages (XMT-C, CUDA, CAL, etc.). These languages are being developed by the hardware 
com­pany involved and therefore are well supported. They are also very close to current C++ programming 
models syntactically; however, this can cause problems because the language then provides no explicit 
representation of the unique aspects of data-parallel programming or the processor hardware. Although 
this can reduce the changes required for an initial port, the resulting code hides the parallel behavior, 
making it harder to compre­hend, debug, and optimize. Simplifying the initial port of serial code through 
syntax is not that useful to begin with, since for best performance it is often an entire algo­rithm 
that must be replaced with a data-parallel version. Further, in the interest of simplicity, these APIs 
may not expose the full features of the graphics-specific silicon, which implies an underutilized silicon 
area. Array-based languages (RapidMind, Acceleware, Microsoft Accelerator, Ct, etc.). These languages 
are based on array data types and specific intrinsics that operate on them. Algorithms converted to these 
languages often result in code that is shorter, clearer, and very likely faster than before. The challenge 
of restructuring design con­cepts into array paradigms, however, remains a barrier to adoption of these 
languages because of the high level of abstraction at which it must be done. Graphics APIs (OpenGL, 
Direct3D). Recent research in GPGPU (general-purpose computing on graphics pro­cessing units) has found 
that while the initial ramp-up of using graphics APIs can be difficult, they do provide a direct mapping 
to hardware that enables very specific optimizations, as well as access to hardware features that other 
approaches may not allow. For example, work by Naga Govindaraju1 and Jens Krüger2 relies on access to 
fixed-function triangle interpolators and blending units that the newer languages mentioned here often 
do not expose. Further, there is good commercial support and a large and experienced community of developers 
already using them. gPUS AS DATA-PARAllEl MACHInES The GPU is the second-most-heavily used processor 
in a typical PC. It has evolved rapidly over the past decade to reach performance levels that can exceed 
the CPU by a large factor, at least on appropriate workloads.3 GPU evolution has been driven by 3D rendering, 
an embar­rassingly data-parallel problem, which makes the GPU an excellent target for data-parallel code. 
As a result of this signi.cantly different workload design point (processing model, I/O patterns, and 
locality of reference), the GPU has a substantially different processor architecture and memory subsystem 
design, typically featuring a broader SIMD (single instruction, multiple data) width and a higher-latency, 
higher-bandwidth streaming memory sys­tem. The processing model exposed via a graphics API is a task-serial 
pipeline made up of a few data-parallel stages that use no interthread communication mechanisms at all. 
While separate stages appear for processing vertices or pixels, the actual architecture is somewhat simpler. 
 As shown in .gure 1, a modern DirectX10-class GPU has a single array of processors that perform the 
computa­tional work of each stage in conjunction with specialized hardware. After polygon-vertex processing, 
a specialized hardware interpolator unit is used to turn each polygon into pixels for the pixel-processing 
stage. This unit can be thought of as an address generator. At the end of the pipeline, another specialized 
unit blends completed pixels into the image buffer. This hardware is often useful in accumulating results 
into a destination array. Further, all processing stages have access to a dedicated texture-sam­pling 
unit that performs linearly interpolated reads on 1D, 2D, or 3D source arrays in a variety of data-element 
formats. Shaped by these special workload requirements, the modern GPU has: UÊÊ/i.ÊÌ..iÃÊÌ.iÊ "*-Ê.vÊ 
*1ÊV..«ÃÊv.ÀÊÃ... >ÀÊ«À.ViÊ and power consumption UÊÊ/..ÕÃ>.`ÃÊ.vÊÌ.Ài>`ÃÊ`.ÃÌÀ.LÕÌi`Ê.ÛiÀÊ.Õ.`Ài`ÃÊ.vÊ 
single-precision .oating-point ALUs UÊÊ Ê`i`.V>Ìi`ÊÃÌÀi>...}..i..ÀÞÊÃÞÃÌi.ÊÜ.Ì.Ê£äÊÌ..iÃÊ the memory 
bandwidth of a CPU UÊÊ i`.V>Ìi`Ê.i..ÀÞÊV>«>V.ÌÞÊÃ... >ÀÊÌ.ÊÌ.iÊ *1ÊÃÞÃÌi.Ê memory capacity UÊÊ-«iV.> 
.âi`ÊV.ÀiÃÊv.ÀÊw ÌiÀ..}]ÊL i.`..}]ÊÀ>ÃÌiÀ.â..}]Ê>.`Ê video processing A GPU s memory subsystem is designed 
for higher I/O latency to achieve increased throughput. It assumes only very limited data reuse (locality 
in read/write access), featuring small input and output caches designed more as FIFO (.rst in, .rst out) 
buffers than as mechanisms to avoid round-trips to memory. Recent research has looked into applying these 
proces­sors to other algorithms beyond 3D rendering. There have been applications that have shown signi.cant 
bene.ts over CPU code. In general, those that most closely match the original design workload of 3D graphics 
(such as image processing) and can .nd a way to leverage either the tenfold compute advantage or the 
tenfold bandwidth advantage have done well. (Much of this work is cata­loged on the Web at http://www.gpgpu.org.) 
This research has identi.ed interesting algorithms. For example, compacting an array of variable-length 
records is a task that has a data-parallel implementation on the parallel pre.x sum or scan. The pre.x-sum 
algorithm computes the sum of all previous array elements (i.e., the first output element in a row r 
is r0 , while the second is o = r + r, and the nth output element is o = r + r 101n01 + + rn). Using 
this, a list of record sizes can be accu­mulated to compute the absolute addresses where each record 
element is to be written. Then the writes can occur completely in parallel. Note that if the writes are 
done in order, the memory-access pattern is still completely sequential.4 MAkIng CODE DATA-PARAllEl 
Before starting to write your code, check for tasks that are known data-parallel cases. Often you can 
find library routines already available for accelerating common tasks using data-parallel hardware. Most 
data-parallel program­ming environments include such libraries as a convenient way for users to begin 
adopting their technology. If you need to write custom data-parallel code, the process is similar to 
a localized optimization effort. You can adopt data-parallel programming incrementally, since you can 
identify and optimize the key inner loops one at a time, without perturbing the larger-scale structure 
of the code base. Here are the basic steps for converting code to the data-parallel model: 1. Identify 
a key task that looks data-parallel. 2. Identify a data-parallel algorithm for this task. 3. Select 
a data-parallel programming environment. 4. Implement code. 5. Evaluate performance scaling rate. 
6. Go to step 1.  STeP 1: IDeNTIFy A key TASk ThAT LOOkS DATA-PARALLeL Look for a segment of code that 
doesn t rely greatly on cross communication between data elements, or con­versely, a set of data elements 
that can be processed without requiring too much knowledge of each other. Look for data-access patterns 
that can be regularized, as opposed to arbitrary/random (such as linear arrays versus sparse-tree data 
structures). While searching for candidates to parallelize, you can evaluate performance potential via 
Amdahl s law: just comment out this candidate task (simulate infinite parallelism) and check to see total 
performance change. If there isn t a significant improvement, going through the effort of parallelizing 
won t pay off. STeP 2: IDeNTIFy A DATA-PARALLeL ALGORIThM FOR ThIS TASk Often a good place to look is 
in the history books (math) or in routines developed by Tera/Cray for its vector processors. For example, 
bitonic sorts were identified as interesting before computers were developed, but fell out of favor during 
the rise of current cache-based machines. Other examples are radix sorts, and prefix sum (scan) operations 
used for packing sparse data. STeP 3: SeLeCT A DATA-PARALLeL PROGRAMMING eNvIRONMeNT Many data-parallel 
programming environments are avail­able today. Many of the criteria to use in evaluation are the same 
as for any development environment. The areas to look for are: Abstraction level. Do you need a library, 
a set of data­abstraction utilities, or a language?  Syntax clarity. Are limitations of the implementation 
explicit in the syntax or hidden by it?  Maintainability. Would the resulting code complexity be manageable? 
 Support. Are there user groups or support services?  Availability. How broadly distributed is the 
environment or any hardware that it requires?  Compatibility. Is the environment compatible with a broad 
range of systems or only a specific subset?  Lifespan. Is the environment compatible with future hardware, 
even from the same vendor?  Documentation. Do the docs make sense? Are the samples useful?  Cost. How 
much will it cost users of your product to get any required hardware or software?  STeP 4: IMPLeMeNT 
CODe Code it up, at least at the pseudocode level. If implemen­tation turns out to require more than 
one or two places where interthread communication is required, then this may not be a sufficiently data-parallel 
algorithm. In that case, it may be necessary to look for another algorithm (step 2) or another task to 
parallelize (step 1). STeP 5: evALUATe PeRFORMANCe SCALING Performance at a given core count is interesting 
but not the key point. (If you are going to check that, be sure to compare using a realistic before case.) 
A more impor­tant metric to check is how the new code scales with increasing core count. If there is 
no sign of a performance plateau, the system will have some scaling headroom. After all, absolute performance 
relative to a single core is not as relevant as how it scales with core-count growth over time.  In 
summary:  Understand the paradigm. What are data-parallel com­putation and the streaming-memory model? 
 Understand your code. Which portions operate at which level of granularity?  Understand the environment. 
How does it help solve the problem?   gPU PERFORMAnCE HInTS If targeting a GPU, are there operations 
that can lever­age the existing graphics-related hardware? Are your data types small enough? GPUs are 
designed to operate on small data elements so media data (image/video pixels or audio samples) is a good 
fit. Or when sorting on the GPU, working with key-index pairs separately is often a win. Then the actual 
movement of data records can be done on the CPU, or on the GPU as a separate pass. GPUs are optimized 
for work with 1D, 2D, or 3D arrays of similar data elements. Array operations are often faster using 
GPU hardware because it can transparently opti­mize them for spatially coherent access. When reading 
such arrays, the GPU can easily linearly interpolate regular array data. This effectively enables a floating-point 
(fuzzy) array index. Many mathemati­cal algorithms use either a simple linear interpolation of array 
elements or slightly higher-order schemes that can be implemented as a few linear interpolations. GPU 
hardware has a significant proportion of silicon allocated to optimizing the performance of these operations. 
Algorithms that involve an accumulation or summa­tion of values into a set of results (instead of just 
a write/ copy) can leverage yet another large chunk of special silicon on GPUs: the blender is designed 
for efficiently compositing or accumulating values into an array. Some matrix math algorithms and reduction 
operations have shown benefits here. RegisteR PRessuRe Some architectures (such as GPUs) are flexible 
in that they can assign variable numbers of threads to a core based on how many registers each thread 
uses. This enables more threads to be used when fewer temporary registers are needed, but reduces the 
threads available (and the paral­lelism) for algorithms that need more registers. The key is to break 
tasks into simpler steps that can be executed across even more parallel threads. This is the essence 
of data-parallel programming. For example, a standard 8x8 image DCT (discrete cosine transform) algorithm 
operates on transposed data for its second half. The transpose can takes dozens of reg­isters to execute 
in place, but breaking it into two passes so that the transpose happens in the intervening I/O results 
in only a handful of registers needed for each half. This approach improved performance from far slower 
than a CPU to three times that of a highly optimized SSE assembly routine. Hints foR Reductions Reductions 
are common operations: find the total, aver­age, min, max, or histogram of a set of data. The com­putations 
are easily data-parallel, but the output write is an example of cross-thread communication that must 
be managed carefully Initial implementations allocated a single shared loca­tion for all the threads 
to write into, but execution was completely serialized by write contention to that loca­tion. Allocating 
multiple copies of the reduction destina­tion and then reducing these down in a separate step was found 
to be much faster. The key is to allocate enough intermediate locations to cover the number of cores 
(hun­dreds) and, therefore, performance level that you want to scale to. PRogRAMMing tHe MeMoRy subsysteM 
The data-parallel paradigm extends to the memory subsystem as well. A full data-parallel machine is able 
not only to process individual data elements separately, but also to read and write those elements in 
parallel. This characteristic of the memory subsystem is as important to performance as the execution 
model. For example, I/O ports are a shared resource, and performance is improved if multiple threads 
are not contending for the same one. Data structures manipulated imply memory-access pat­terns. We have 
seen cases where switching from pointer­based data structures such as linked lists or sparse trees to 
data-parallel-friendly ones (regular arrays, grids, packed streams, etc.) allows code to become compute-bound 
instead of memory-bound (which can be as much as 10 times faster on GPUs). This is because memory is 
typi­cally organized into pages, and there is some overhead in switching between pages. Grouping data 
elements and threads so that many results can be read from (or written to) the same page helps with performance. 
 Many types of trees and other sparse-data structures have data-parallel-friendly array-based implementations. 
Although using these structures is quite conventional, their implementations are nonintuitive to developers 
trained on pointer-based schemes.5 The most important characteristic of the GPU memory subsystem is the 
cache architecture. Unlike a CPU, the GPU has hardly any read/write cache. It is assumed that so much 
data will be streaming through the processor that it will over.ow just about any cache. As a result, 
the only caches present are separate read-through and write-through buffers that smooth out the data 
.ow. Therefore, it is critical to select algorithms that do not rely on reuse of data at scales larger 
than the few local registers available. For example, histogram computation requires more read/write storage 
to contain the histogram bins than typical register allocation supports. Upcoming GPU architectures are 
beginning to add read/write caches so that more algorithms will work, including reasonably sized histograms, 
but since these caches are still 10 to 100 times smaller than those on the CPU, this will remain a key 
criterion when choosing an algorithm. GPUS AS DATA-PARALLEL HARDWARE GPU systems are cheap and widely 
available, and many programmers (such as game developers) have identi.ed key approaches to programming 
them ef.ciently. First, it can be important to leverage all the silicon on the die. Applications that 
don t light up the graph­ics-speci.c gates are already at a disadvantage compared with a CPU. For example, 
Govindaraju s sort implementa­tions show signi.cant bene.ts from using the blending hardware.6 Another 
way to ensure programming ef.ciency is to keep the data elements small. This extra hardware is assuming 
graphics data types that are optimal when they are 16 or fewer bytes in size, and ideally four bytes. 
If you can make your data look like what a GPU usually processes, you will get large bene.ts. Unfortunately, 
the GPU s high-speed memory system (10 times faster throughput than the CPU front side bus) is typically 
connected to the CPU by a link that is 10 times slower than CPU memory. Minimizing data and control traf.c 
through this link is vital to GPU perfor­mance in low-latency scenarios. The secret is to keep data in 
the GPU s memory as long as possible, bringing it back to the CPU only for persistent storage. Sometimes 
this may involve executing a small non-data-parallel task on the GPU because the cost of sending the 
required data across to the CPU, synchronizing it, and sending it back may be even greater. GPU GENERALITY 
With shorter design cycles, GPUs have been evolving more rapidly than CPUs. This evolution has typically 
been in the direction of increased generality. Now we are seeing GPU generality growing beyond the needs 
of basic rendering to more general applications. For example, in the past year new GPU environments have 
become avail­able that expose features that the graphics APIs do not. Some now support sharing of data 
among threads and more .exible memory-access options. This enables entirely new classes of algorithms 
on GPUs. Most obviously, more general approaches to 3D processing are becoming feasible, including manipulation 
of acceleration data structures for ray tracing, radiosity, or collision detection. Other obvious applications 
are in media processing (photo, video, and audio data) where the data types are similar to those of 3D 
rendering. Other domains using similar data types are seismic and medical analysis.   FUTURE HARDwARE 
EvOlUTIOn:CPU/gPU COnvERgEnCE? Processor features such as instruction formats will likely converge as 
a result of pressure for a consistent program­ming model. GPUs may migrate to narrower SIMD widths to 
increase performance on branching code, while CPUs move to broader SIMD width to improve instruction 
efficiency. The fact remains, however, that some tasks can be executed more efficiently using data-parallel 
algorithms. Since efficiency is so critical in this era of constrained power consumption, a two-point 
design that enables the optimal mapping of tasks to each processor model may persist for some time to 
come. Further, if the hardware continues to lead the soft­ware, it is likely that systems will have more 
cores than the application can deal with at a given point in time, so providing a choice of processor 
types increases the chance of more of them being used. Conceivably, a data-parallel system could support 
the entire feature set of a modern serial CPU core, including a rich set of interthread communications 
and synchroniza­tion mechanisms. The presence of such features, however, may not matter in the longer 
term because the more such traditional synchronization features are used, the worse performance will 
scale to high core counts. The fastest apps are not those that port their existing single-threaded or 
even dual-threaded code across, but those that switch to a different parallel algorithm that scales better 
because it relies less on general synchronization capabilities. Figure 2 shows a list of algorithms that 
have been implemented using data-parallel paradigms with varying degrees of success. They are sorted 
roughly in order of how well they match the data-parallel model. Data-parallel processors are becoming 
more broadly available, especially now that consumer GPUs support data-parallel programming environments. 
This paradigm shift presents a new opportunity for programmers who adapt in time. The data-parallel industry 
is evolving without much guidance from software developers. The first to arrive will have the best chance 
to drive and shape upcoming data­parallel hardware architectures and development environ­ments to meet 
the needs of their particular application space. When programmed effectively, GPUs can be faster than 
current PC CPUs. The time has come to take advan­tage of this new processor type by making sure each 
task in your code base is assigned to the processor and memory model that is optimal for that task. Q 
RefeRences 1. Govindaraju, N.K., Gray, J., Kumar, R., Manocha, D. 2006. GPUTeraSort: High-performance 
graphics copro­cessor sorting for large database management. Proceed­ings of the 2006 ACM SIGMOD International 
Conference on Management of Data; http://research.microsoft.com/ research/pubs/view.aspx?msr_tr_id=MSR-TR-2005-183). 
 2. Krüger, J., Westermann, R. 2003. Linear algebra opera­tors for GPU implementation of numerical algorithms. 
ACM Transactions on Graphics 22(3). 3. Blythe, D. 2008. The Rise of the GPU. Proceedings of the IEEE 
96(5). 4. Shubhabrata, S., Lefohn, A.E., Owens, J.D. 2006. A work-efficient step-efficient prefix sum 
algorithm. Pro­ceedings of the Workshop on Edge Computing Using New Commodity Architectures: D-26-27. 
 5. Lefohn, A.E., Kniss, J., Strzodka, R., Sengupta, S., Owens, J.D. 2006. Glift: Generic, efficient, 
random­access GPU data structures. ACM Transactions on Graph­ics 25(1). 6. See reference 1.  suggested 
fuRtHeR ReAding GPU Gems 2: http://developer.nvidia.com/object/ gpu_gems_2_home.html  GPU Gems 3: http://developer.nvidia.com/object/ 
gpu-gems-3.html Ch 39 on prefix sum Glift data structures: http://graphics.cs.ucdavis.edu/~lefohn/work/glift/ 
Rapidmind: http://www.rapidmind.net/index.php  Intel Ct: http://www.intel.com/research/platform/terascale/ 
TeraScale_whitepaper.pdf Microsoft DirectX SDK: http://msdn2.microsoft.com/en-us/library/aa139763.aspx 
 Direct3D HLSL: http://msdn2.microsoft.com/en-us/library/bb509561.aspx Nvidia CUDA SDK: http://developer.nvidia.com/object/cuda.html 
 AMD Firestream SDK: http://ati.amd.com/technology/streamcomputing/ stream-computing.pdf Microsoft Research 
s Accelerator: http://research.microsoft.com/research/pubs/view. aspx?type=technical%20report&#38;id=1040&#38;0sr=a 
http://research.microsoft.com/research/downloads/ Details/25e1bea3-142e-4694-bde5-f0d44f9d8709/Details. 
aspx LOVE IT, HATE IT? LET US KNOW feedback@acmqueue.com or www.acmqueue.com/forums CHAS. BOYD is a software 
architect at Microsoft. He joined the Direct3D team in 1995 and has contributed to releases since DirectX 
3. During that time he has worked closely with hardware and software developers to drive the adoption 
of features such as programmable hardware shaders and .oat pixel processing into consumer graphics. Recently 
he has been investigating new processing architectures and applica­tions for mass-market consumer systems. 
&#38;#169; 2008 ACM 1542-7730/08/0300 $5.00   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401151</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>37</pages>
		<display_no>15</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Stream computing]]></title>
		<page_from>1</page_from>
		<page_to>37</page_to>
		<doi_number>10.1145/1401132.1401151</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401151</url>
		<categories>
			<primary_category>
				<cat_node>E.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.1</cat_node>
				<descriptor>Information theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003712</concept_id>
				<concept_desc>CCS->Mathematics of computing->Information theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003712</concept_id>
				<concept_desc>CCS->Mathematics of computing->Information theory</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003712.10003713</concept_id>
				<concept_desc>CCS->Mathematics of computing->Information theory->Coding theory</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002978.10002979.10002985</concept_id>
				<concept_desc>CCS->Security and privacy->Cryptography->Mathematical foundations of cryptography</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098634</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AMD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401152</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>14</pages>
		<display_no>16</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Scalable parallel programming with CUDA]]></title>
		<page_from>1</page_from>
		<page_to>14</page_to>
		<doi_number>10.1145/1401132.1401152</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401152</url>
		<abstract>
			<par><![CDATA[<p>Is CUDA the parallel programming model that application developers have been waiting for?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>D.1.3</cat_node>
				<descriptor>Parallel programming</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>D.3.2</cat_node>
				<descriptor>Concurrent, distributed, and parallel languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.3.3</cat_node>
				<descriptor>Concurrent programming structures</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010177</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10011014</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Concurrent programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011024.10011034</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language features->Concurrent programming structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010175</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010175</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Parallel programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169.10010175</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies->Parallel programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098635</person_id>
				<author_profile_id><![CDATA[81100365054]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nickolls]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nvidia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098636</person_id>
				<author_profile_id><![CDATA[81100248942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nvidia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098637</person_id>
				<author_profile_id><![CDATA[81100516743]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nvidia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098638</person_id>
				<author_profile_id><![CDATA[81100290998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Skadron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[NVIDIA. 2007. CUDA Technology; http://www.nvidia.com/CUDA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[NVIDIA. 2007. CUDA Programming Guide 1.1; http://developer.download.nvidia.com/compute/cuda/1_1/NVIDIA_CUDA_Programming_Guide_1.1.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Stratton, J. A., Stone, S. S., Hwu, W. W. 2008. M-CUDA: An efficient implementation of CUDA kernels on multicores. IMPACT Technical Report 08-01, University of Illinois at Urbana-Champaign, (February).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[See reference 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015800</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Buck, I., Foley, T., Horn, D., Sugerman, J., Fatahalian, K., Houston, M., Hanrahan, P. Brook for GPUs: Stream computing on graphics hardware. 2004. <i>Proceedings of SIGGRAPH</i> (August): 777--786; http://doi.acm.org/10.1145/1186562.1015800.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Stone, S. S., Yi, H., Hwu, W. W., Haldar, J. P., Sutton, B. P., Liang, Z.-P. 2007. How GPUs can improve the quality of magnetic resonance imaging. The First Workshop on General-Purpose Processing on Graphics Processing Units (October).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Stone, J. E., Phillips, J. C., Freddolino, P. L., Hardy, D. J., Trabuco, L. G., Schulten, K. 2007. Accelerating molecular modeling applications with graphics processors. <i>Journal of Computational Chemistry</i> 28(16): 2618--2640; http://dx.doi.org/10.1002/jcc.20829.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Nyland, L., Harris, M., Prins, J. 2007. Fast n-body simulation with CUDA. In <i>GPU Gems 3</i>. H. Nguyen, ed. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248979</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Golub, G. H., and Van Loan, C. F. 1996. <i>Matrix Computations</i>, 3&#60;sup&#62;rd&#60;/sup&#62; edition. Johns Hopkins University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2401989</ref_obj_id>
				<ref_obj_pid>2401945</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Buatois, L., Caumon, G., L&#233;vy, B. 2007. Concurrent number cruncher: An efficient sparse linear solver on the GPU. <i>Proceedings of the High-Performance Computation Conference (HPCC)</i>, Springer LNCS.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1280110</ref_obj_id>
				<ref_obj_pid>1280094</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Sengupta, S., Harris, M., Zhang, Y., Owens, J. D. 2007. Scan primitives for GPU computing. In <i>Proceedings of Graphics Hardware</i> (August): 97--106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[See Reference 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
  FOCUS GPUs The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are 
now parallel systems. Furthermore, their parallelism continues to scale with Moore s law. The challenge 
is to develop mainstream application software that transparently scales its parallel­ism to leverage 
the increasing number of processor cores, much as 3D graphics applications transparently scale their 
parallelism to manycore GPUs with widely varying numbers of cores. According to conventional wisdom, 
parallel program­ming is difficult. Early experience with the CUDA1,2 scalable parallel programming model 
and C language, however, shows that many sophisticated programs can be readily expressed with a few easily 
understood abstrac­tions. Since NVIDIA released CUDA in 2007, developers have rapidly developed scalable 
parallel programs for a wide range of applications, including computational chemistry, sparse matrix 
solvers, sorting, searching, and physics models. These applications scale transparently to hundreds of 
processor cores and thousands of concurrent threads. NVIDIA GPUs with the new Tesla unified graph­ics 
and computing architecture (described in the GPU sidebar) run CUDA C programs and are widely available 
in laptops, PCs, workstations, and servers. The CUDA  with CUDA Is CUDA the parallel programming model 
that application developers have been waiting for? FOCUS  Scalable Parallel PROGRAMMING with CUDA 
model is also applicable to other shared-memory parallel processing architectures, including multicore 
CPUs.3 CUDA provides three key abstractions a hierarchy of thread groups, shared memories, and barrier 
syn­chronization that provide a clear parallel structure to conventional C code for one thread of the 
hierarchy. Multiple levels of threads, memory, and synchronization provide fine-grained data parallelism 
and thread paral­lelism, nested within coarse-grained data parallelism and task parallelism. The abstractions 
guide the programmer to partition the problem into coarse sub-problems that can be solved independently 
in parallel, and then into  UNIFIED GRAPHICS AND COMPUTING GPUS D riven by the insatiable market demand 
for realtime, high-definition 3D graphics, the programmable GPU (graphics processing unit) has evolved 
into a highly parallel, multithreaded, manycore processor. It is designed to efficiently support the 
graphics shader programming model, in which a program for one thread draws one vertex or shades one pixel 
fragment. The GPU excels at fine-grained, data-parallel workloads consisting of thousands of indepen­dent 
threads executing vertex, geometry, and pixel-shader program threads concurrently. The tremendous raw 
performance of modern GPUs has led researchers to explore mapping more general non-graph­ics computations 
onto them. These GPGPU (general-purpose computation on GPUs) systems have produced some impres­sive results, 
but the limitations and difficulties of doing this via graphics APIs are legend. This desire to use the 
GPU as a more general parallel computing device motivated NVIDIA to develop a new unified graphics and 
computing GPU architecture and the CUDA programming model. GPU COMPUTING ARCHITECTURE Introduced by 
NVIDIA in November 2006, the Tesla unified graphics and computing architecture1,2 significantly extends 
the GPU beyond graphics its massively multithreaded processor array becomes a highly efficient unified 
platform for both graphics and general-purpose parallel computing applications. By scaling the number 
of processors and mem­ory partitions, the Tesla architecture spans a wide market range from the high-performance 
enthusiast GeForce 8800 GPU and professional Quadro and Tesla computing products to a variety of inexpensive, 
mainstream GeForce GPUs. Its computing features enable straightforward programming of the GPU cores in 
C with CUDA. Wide availability in laptops, desktops, workstations, and servers, coupled with C pro­grammability 
and CUDA software, make the Tesla architec­ture the first ubiquitous supercomputing platform. The Tesla 
architecture is built around a scalable array of multithreaded SMs (streaming multiprocessors). Current 
GPU implementations range from 768 to 12,288 concur­rently executing threads. Transparent scaling across 
this wide range of available parallelism is a key design goal of both the GPU architecture and the CUDA 
programming model. Figure A shows a GPU with 14 SMs a total of 112 SP (streaming processor) cores interconnected 
with four external DRAM partitions. When a CUDA program on the host CPU invokes a kernel grid, the CWD 
(compute work distribution) unit enumerates the blocks of the grid and begins distributing them to SMs 
with available execution capacity. The threads of a thread block execute concurrently on one SM. As thread 
blocks terminate, the CWD unit launches new blocks on the vacated multiprocessors. An SM consists of 
eight scalar SP cores, two SFUs (special function units) for transcendentals, an MT IU (multithreaded 
instruction unit), and on-chip shared memory. The SM cre­ates, manages, and executes up to 768 concurrent 
threads in hardware with zero scheduling overhead. It can execute as many as eight CUDA thread blocks 
concurrently, limited by thread and memory resources. The SM implements the CUDA __syncthreads() barrier 
synchronization intrinsic with a single instruction. Fast barrier synchronization together with lightweight 
thread creation and zero-overhead thread scheduling efficiently support very fine-grained parallelism, 
allowing a new thread to be created to compute each vertex, pixel, and data point. To manage hundreds 
of threads running several different programs, the Tesla SM employs a new architecture we call finer 
pieces that can be solved cooperatively in parallel. The programming model scales transparently to large 
numbers of processor cores: a compiled CUDA program executes on any number of processors, and only the 
run­time system needs to know the physical processor count. THE CUDA PARADIGM CUDA is a minimal extension 
of the C and C++ program­ming languages. The programmer writes a serial program that calls parallel kernels, 
which may be simple functions or full programs. A kernel executes in parallel across a set of parallel 
threads. The programmer organizes these threads into a hierarchy of grids of thread blocks. A thread 
block is a set of concurrent threads that can cooperate among themselves through barrier synchronization 
and shared access to a memory space private to the block. A grid is a set of thread blocks that may each 
be executed independently and thus may execute in parallel. When invoking a kernel, the programmer specifies 
the number of threads per block and the number of blocks  FOCUS  Scalable Parallel PROGRAMMING with 
CUDA making up the grid. Each thread is given a unique thread ID number threadIdx within its thread 
block, numbered 0, 1, 2, ..., blockDim 1, and each thread block is given a unique block ID number blockIdx 
within its grid. CUDA supports thread blocks containing up to 512 threads. For convenience, thread blocks 
and grids may have one, two, or three dimensions, accessed via .x, .y, and .z index fields. As a very 
simple example of parallel programming, suppose that we are given two vectors x and y of n float­ing-point 
numbers each and that we wish to compute the result of y.ax + y, for some scalar value a. This is the 
SIMT warp start together at the same program address but are otherwise free to branch and execute independently. 
Each SM manages a pool of 24 warps of 32 threads per warp, a total of 768 threads. Every instruction 
issue time, the SIMT unit selects a warp that is ready to execute and issues the next instruction to 
the active threads of the warp. A warp executes one common instruction at a time, so full efficiency 
is realized when all 32 threads of a warp agree on their execution path. If threads of a warp diverge 
via a data-dependent conditional branch, the warp serially executes each branch path taken, dis­abling 
threads that are not on that path, and when all paths complete, the threads converge back to the same 
execution path. Branch divergence occurs only within a warp; different warps execute independently regardless 
of whether they are executing common or disjointed code paths. As a result, the Tesla-architecture GPUs 
are dramatically more efficient and flexible on branching code than previous-generation GPUs, as their 
32-thread warps are much narrower than the SIMD (single-instruction multiple-data) width of prior GPUs. 
SIMT architecture is akin to SIMD vector organizations in that a single instruction controls multiple 
processing elements. A key difference is that SIMD vector organiza­tions expose the SIMD width to the 
software, whereas SIMT instructions specify the execution and branching behavior of a single thread. 
In contrast with SIMD vector machines, SIMT enables programmers to write thread-level parallel code for 
independent, scalar threads, as well as data-parallel code for coordinated threads. For the purposes 
of cor­rectness, the programmer can essentially ignore the SIMT behavior; however, substantial performance 
improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. 
In practice, this is analogous to the role of cache lines in traditional code: cache line size can be 
safely ignored when designing for correctness but must be considered in the code structure when designing 
for peak performance. Vector architectures, on the other hand, require the software to coalesce loads 
into vectors and man­age divergence manually. A thread s variables typically reside in live registers. 
The 16KB SM shared memory has very low access latency and high bandwidth similar to an L1 cache; it holds 
CUDA per-block __shared__ variables for the active thread blocks. The SM provides load/store instructions 
to access CUDA __device__ variables in GPU external DRAM. It coalesces indi­vidual accesses of parallel 
threads in the same warp into fewer memory-block accesses when the addresses fall in the same block and 
meet alignment criteria. Because global memory latency can be hundreds of processor clocks, CUDA programs 
copy data to shared memory when it must be accessed multiple times by a thread block. Tesla load/store 
memory instructions use integer byte addressing to facilitate conven­tional compiler code optimizations. 
The large thread count in each SM, together with support for many outstanding load requests, helps to 
cover load-to-use latency to the external DRAM. The latest Tesla-architecture GPUs also provide atomic 
read-modify-write memory instructions, facilitating parallel reductions and parallel-data structure management. 
CUDA applications perform well on Tesla-architecture GPUs because CUDA s parallelism, synchronization, 
shared memories, and hierarchy of thread groups map efficiently to features of the GPU architecture, 
and because CUDA expresses application parallelism well. REFERENCES 1. Lindholm, E., Nickolls, J., Oberman, 
S., Montrym, J. 2008. NVIDIA Tesla: A unified graphics and computing architec­ture. IEEE Micro 28(2). 
 2. Nickolls, J. 2007. NVIDIA GPU parallel computing archi­tecture. In IEEE Hot Chips 19 (August 20), 
Stanford, CA; http://www.hotchips.org/archives/hc19/. 3. See reference 1.  so-called saxpy kernel 
defined by the BLAS (basic linear algebra subprograms) library. The code for performing this computation 
on both a serial processor and in paral­lel using CUDA is shown in figure 1. The __global__ declaration 
specifier indicates that the procedure is a kernel entry point. CUDA programs launch parallel kernels 
with the extended function-call syntax kernel<<<dimGrid, dimBlock>>>(... parameter list ...); where dimGrid 
and dimBlock are three-element vectors of type dim3 that specify the dimensions of the grid in blocks 
and the dimensions of the blocks in threads, respectively. Unspecified dimensions default to 1. In the 
example, we launch a grid that assigns one thread to each element of the vectors and puts 256 threads 
in each block. Each thread computes an element index from its thread and block IDs and then performs 
the desired calculation on the corresponding vector elements. The serial and parallel versions of this 
code are strikingly similar. This represents a fairly common pat- Computing y . ax + y with a Serial 
Loop void saxpy_serial(int n, float alpha, float *x, float *y) { for(int i = 0; i<n; ++i) y[i] = alpha*x[i] 
+ y[i]; } // Invoke serial SAXPY kernel saxpy_serial(n, 2.0, x, y); Computing y . ax + y in parallel 
using CUDA __global__ void saxpy_parallel(int n, float alpha, float *x, float *y) { int i = blockIdx.x*blockDim.x 
+ threadIdx.x; if( i<n ) y[i] = alpha*x[i] + y[i]; } // Invoke parallel SAXPY kernel (256 threads per 
block) int nblocks = (n + 255) / 256; saxpy_parallel<<<nblocks, 256>>>(n, 2.0, x, y);  FIG 1  tern. 
The serial code consists of a loop where each itera­tion is independent of all the others. Such loops 
can be mechanically transformed into parallel kernels: each loop iteration becomes an independent thread. 
By assigning a single thread to each output element, we avoid the need for any synchronization among 
threads when writing results to memory. The text of a CUDA kernel is simply a C function for one sequential 
thread. Thus, it is generally straightfor­ward to write and is typically simpler than writing paral­lel 
code for vector operations. Parallelism is determined clearly and explicitly by specifying the dimensions 
of a grid and its thread blocks when launching a kernel. Parallel execution and thread management are 
auto­matic. All thread creation, scheduling, and termination are handled for the programmer by the underlying 
sys­tem. Indeed, a Tesla-architecture GPU performs all thread management directly in hardware. The threads 
of a block execute concurrently and may synchronize at a barrier by calling the __syncthreads() intrinsic. 
This guarantees that no thread participating in the barrier can proceed until all participating threads 
have reached the barrier. After pass­ing the barrier, these threads are also guaranteed to see all writes 
to memory performed by participating threads before the barrier. Thus, threads in a block may commu­nicate 
with each other by writing and reading per-block shared memory at a synchronization barrier. Since threads 
in a block may share local memory and synchronize via barriers, they will reside on the same physical 
processor or multiprocessor. The number of thread blocks can, however, greatly exceed the number of processors. 
This virtualizes the processing elements and gives the programmer the flexibility to parallelize at what­ever 
granularity is most convenient. This allows intuitive problem decompositions, as the number of blocks 
can be dictated by the size of the data being processed rather than by the number of processors in the 
system. This also allows the same CUDA program to scale to widely varying numbers of processor cores. 
To manage this processing element virtualization and provide scalability, CUDA requires that thread blocks 
exe­cute independently. It must be possible to execute blocks in any order, in parallel or in series. 
Different blocks have no means of direct communication, although they may coordinate their activities 
using atomic memory operations on the global memory visible to all threads by atomi­cally incrementing 
queue pointers, for example. This independence requirement allows thread blocks to be scheduled in any 
order across any number of cores, making the CUDA model scalable across an arbitrary FOCUS  Scalable 
Parallel PROGRAMMING with CUDA number of cores, as well as across a variety of parallel architectures. 
It also helps to avoid the possibility of dead­lock. An application may execute multiple grids either 
independently or dependently. Independent grids may execute concurrently given sufficient hardware resources. 
Dependent grids execute sequentially, with an implicit inter-kernel barrier between them, thus guaranteeing 
that all blocks of the first grid will complete before any block of the second dependent grid is launched. 
Threads may access data from multiple memory spaces during their execution. Each thread has a private 
local memory. CUDA uses this memory for thread-private vari­ables that do not fit in the thread s registers, 
as well as for stack frames and register spilling. Each thread block has a shared memory visible to all 
threads of the block that has the same lifetime as the block. Finally, all threads have access to the 
same global memory. Programs declare vari­ables in shared and global memory with the __shared__ and __device__ 
type qualifiers. On a Tesla-architecture GPU, these memory spaces correspond to physically sepa­rate 
memories: per-block shared memory is a low-latency on-chip RAM, while global memory resides in the fast 
DRAM on the graphics board.  Shared memory is expected to be a low-latency mem­ory near each processor, 
much like an L1 cache. It can, therefore, provide for high-performance communication and data sharing 
among the threads of a thread block. Since it has the same lifetime as its corresponding thread block, 
kernel code will typically initialize data in shared variables, compute using shared variables, and copy 
shared memory results to global memory. Thread blocks of sequentially dependent grids communicate via 
global memory, using it to read input and write results. Figure 2 diagrams the nested levels of threads, 
thread blocks, and grids of thread blocks. It shows the corre­sponding levels of memory sharing: local, 
shared, and global memories for per-thread, per-thread-block, and per-application data shar­ing. A program 
manages the global memory space visible to kernels through calls to the CUDA runtime, such as cudaMalloc() 
and cudaFree(). Kernels may execute on a physically separate device, as is the case when running kernels 
on the GPU. Consequently, the application must use cudaMemcpy() to copy data between the allocated space 
and the host system memory. The CUDA program­ming model is similar in style to the familiar SPMD (single-program 
multiple­data) model it expresses parallelism explicitly, and each kernel executes on a fixed number 
of threads. CUDA, however, is more flexible than most real­izations of SPMD, because each kernel call 
dynamically creates a new grid with the right number of thread blocks and threads for that application 
step. The program­mer can use a convenient degree of parallelism for each kernel, rather than having 
to design all phases of the computation to use the same number of threads. Figure 3 shows an example 
of a SPMD-like CUDA code sequence. It first instantiates kernelF on a 2D grid of 3×2 blocks where each 
2D thread block consists of 5×3 threads. It then instantiates kernelG on a 1D grid of four 1D thread 
blocks with six threads each. Because kernelG depends on the results of kernelF, they are separated by 
an inter­kernel synchronization barrier. The concurrent threads of a thread block express fine-grained 
data and thread parallelism. The independent thread blocks of a grid express coarse­grained data parallelism. 
Independent grids express coarse-grained task paral­lelism. A kernel is simply C code for one thread 
of the hierarchy. RESTRICTIONS When developing CUDA programs, it is important to understand the ways 
in which the CUDA model is restricted, largely for reasons of efficiency. Threads and thread blocks may 
be created only by invoking a parallel kernel, not from within a parallel kernel. Together with the required 
independence of thread blocks, this makes it possible to execute CUDA programs with a simple scheduler 
that introduces minimal runtime over­head. In fact, the Tesla architecture implements hardware management 
and scheduling of threads and thread blocks. Task parallelism can be expressed at the thread-block level, 
but blockwide barriers are not well suited for supporting task parallelism among threads in a block. 
To enable CUDA programs to run on any number of processors, communication between thread blocks within 
the same kernel grid is not allowed they must execute independently. Since CUDA requires that thread 
blocks be independent and allows blocks to be executed in any  FOCUS  Scalable Parallel PROGRAMMING 
with CUDA order, combining results generated by multiple blocks must in general be done by launching 
a second kernel on a new grid of thread blocks. However, multiple thread blocks can coordinate their 
work using atomic operations on global memory (e.g., to manage a data structure). Recursive function 
calls are not allowed in CUDA kernels. Recursion is unattractive in a massively paral­lel kernel because 
providing stack space for the tens of thousands of threads that may be active would require substantial 
amounts of memory. Serial algorithms that are normally expressed using recursion, such as quicksort, 
are typically best implemented using nested data parallelism rather than explicit recursion. To support 
a heterogeneous system architecture combining a CPU and a GPU, each with its own memory system, CUDA 
programs must copy data and results between host memory and device memory. The overhead of CPU GPU interaction 
and data transfers is minimized by using DMA block-transfer engines and fast intercon­nects. Of course, 
problems large enough to need a GPU performance boost amortize the overhead better than small problems. 
 RELATED WORK Although the first CUDA implementation targets NVIDIA GPUs, the CUDA abstractions are 
general and useful for programming multicore CPUs and scalable parallel systems. Coarse-grained thread 
blocks map naturally to separate processor cores, while fine-grained threads map to multiple-thread contexts, 
vector operations, and pipe­lined loops in each core. Stratton et al. have developed a prototype source-to-source 
translation framework that compiles CUDA programs for multicore CPUs by map­ping a thread block to loops 
within a single CPU thread. They found that CUDA kernels compiled in this way perform and scale well.4 
CUDA uses parallel kernels similar to recent GPGPU programming models, but differs by providing flex­ible 
thread creation, thread blocks, shared memory, global memory, and explicit synchronization. Stream­ing 
languages apply parallel kernels to data records from a stream. Applying a stream kernel to one record 
is analogous to executing a single CUDA kernel thread, but stream programs do not allow dependencies 
among kernel threads, and kernels communicate only via FIFO (first-in, first-out) streams. Brook for 
GPUs differentiates between FIFO input/output streams and random-access gather streams, and it supports 
parallel reductions. Brook is a good fit for earlier-generation GPUs with random access texture units 
and raster pixel operation units.5 Pthreads and Java provide fork-join parallelism but are not particularly 
convenient for data-parallel applica­tions. OpenMP targets shared memory architectures with parallel 
execution constructs, including parallel for and teams of coarse-grained threads. Intel s C++ Thread­ing 
Building Blocks provide similar features for multicore CPUs. MPI targets distributed memory systems and 
uses message passing rather than shared memory. CUDA APPLICATION ExPERIENCE The CUDA programming model 
extends the C language with a small number of additional parallel abstractions. Programmers who are comfortable 
developing in C can quickly begin writing CUDA programs. In the relatively short period since the introduction 
of CUDA, a number of real-world parallel application codes have been developed using the CUDA model. 
These include FHD-spiral MRI reconstruction,6 molecular dynamics,7 and n-body astrophysics simulation.8 
Running on Tesla-architecture GPUs, these applications were able to achieve substantial speedups over 
alternative imple­mentations running on serial CPUs: the MRI reconstruc­tion was 263 times faster; the 
molecular dynamics code was 10 100 times faster; and the n-body simulation was 50 250 times faster. These 
large speedups are a result of the highly parallel nature of the Tesla architecture and its high memory 
bandwidth.  ExAMPLE: SPARSE MATRIx-VECTOR PRODUCT A variety of parallel algo­rithms can be written 
in CUDA in a fairly straight­forward manner, even when the data structures involved are not simple regular 
grids. SpMV (sparse matrix-vector multiplica­tion) is a good example of an important numerical building 
block that can be parallelized quite directly using the abstractions provided by CUDA. The kernels we 
discuss here, when combined with the provided CUBLAS vector routines, make writing iterative solvers 
such as the conjugate gradient9 method straightforward. A sparse n × n matrix is one in which the number 
of nonzero entries m is only a small fraction of the total. Sparse matrix rep­resentations seek to store 
only the nonzero elements of a matrix. Since it is fairly typical that a sparse n × n matrix will contain 
only m=O(n) nonzero elements, this represents a substan­tial savings in storage space and processing 
time. One of the most com­mon representations for general unstructured sparse matrices is the CSR (compressed 
sparse row) representation. The m nonzero elements of the matrix A are stored in row­major order in an 
array Av. A second array Aj records the corresponding column index for each entry of Av. Finally, an 
array Ap of n+1 float multiply_row(unsigned int rowsize, unsigned int *Aj, // column indices for row 
float *Av, // non-zero entries for row float *x) // the RHS vector { float sum = 0; for(unsigned int 
column=0; column<rowsize; ++column) sum += Av[column] * x[Aj[column]]; return sum; }   FIG 5 void 
csrmul_serial(unsigned int *Ap, unsigned int *Aj, float *Av, unsigned int num_rows, float *x, float *y) 
{ for(unsigned int row=0; row<num_rows; ++row) { unsigned int row_begin = Ap[row]; unsigned int row_end 
= Ap[row+1]; y[row] = multiply_row(row_end-row_begin, Aj+row_begin, Av+row_begin, x); }  } FIG 6 __global__ 
 void csrmul_kernel(unsigned int *Ap, unsigned int *Aj, float *Av, unsigned int num_rows, float *x, float 
*y) { unsigned int row = blockIdx.x*blockDim.x + threadIdx.x; if( row<num_rows ) { unsigned int row_begin 
= Ap[row]; unsigned int row_end = Ap[row+1]; y[row] = multiply_row(row_end-row_begin, Aj+row_begin, 
Av+row_begin, x); } FIG 7 }  FOCUS  Scalable Parallel PROGRAMMING with CUDA elements records the 
extent unsigned int blocksize = 128; // or any size up to 512of each row in the previous unsigned int 
nblocks = (num_rows + blocksize - 1) / blocksize; arrays; the entries for row  csrmul_kernel<<<nblocks,blocksize>>>(Ap, 
Aj, Av, num_rows, x, y); i in Aj and Av extend from index Ap[i] up to, but not including, index Ap[i+1]. 
 FIG 8 This implies that Ap[0] will always be 0 and Ap[n] will always be the number of nonzero elements 
in the matrix. Figure 4 shows an example of the CSR representation of a simple matrix. Given a matrix 
A in CSR form, we can compute a single row of the product y = Ax using the multiply_row() procedure shown 
in figure 5. Computing the full product is then simply a matter of looping over all rows and computing 
the result for that row using multiply_row(), as shown in figure 6. This algorithm can be translated 
into a parallel CUDA kernel quite easily. We simply spread the loop in csrmul_serial() over many parallel 
threads. Each thread will compute exactly one row of the output vector y. Figure 7 shows the code for 
this kernel. Note that it looks extremely similar to the serial loop used in the csrmul_serial() procedure. 
There are really only two points of difference. First, the row index is computed from the block and thread 
indices assigned to each thread. Second, we have a conditional that evaluates a row product only if the 
row index is within the bounds of the matrix (this is necessary since the number of rows n need not be 
a mul­tiple of the block size used in launching the kernel). Assuming that the matrix data structures 
have already been copied to the GPU device memory, launching this kernel will look like the code in figure 
8. The pattern that we see here is a common one. The original serial algorithm is a loop whose iterations 
are independent of each other. Such loops can be parallelized quite easily by simply assigning one or 
more iterations of the loop to each parallel thread. The programming model provided by CUDA makes expressing 
this type of parallel­ism particularly straightforward. This general strategy of decomposing computations 
into blocks of independent work, and more specifically __global__ void csrmul_cached(unsigned int *Ap, 
unsigned int *Aj, float *Av, unsigned int num_rows, const float *x, float *y) { // Cache the rows of 
x[] corresponding to this block. __shared__ float cache[blocksize]; unsigned int block_begin = blockIdx.x 
* blockDim.x; unsigned int block_end = block_begin + blockDim.x; unsigned int row = block_begin + threadIdx.x; 
// Fetch and cache our window of x[]. if( row<num_rows) cache[threadIdx.x] = x[row]; __syncthreads(); 
 if( row<num_rows ) { unsigned int row_begin = Ap[row]; unsigned int row_end = Ap[row+1]; float sum 
= 0, x_j; for(unsigned int col=row_begin; col<row_end; ++col) { unsigned int j = Aj[col]; // Fetch x_j 
from our cache when possible if( j>=block_begin &#38;&#38; j<block_end ) x_j = cache[j-block_begin]; 
else x_j = x[j]; sum += Av[col] * x_j; } breaking up independent loop iterations, is not unique to y[row] 
= sum; CUDA. This is a common approach used in one form or } another by various parallel programming 
systems, includ­ }  FIG 9 ing OpenMP and Intel s Threading Building Blocks. example shown in figure 
9, even this fairly simple use of CACHING IN SHARED MEMORY The SpMV algorithms outlined here are fairly 
simplistic. shared memory returns a roughly 20 percent performance We can make a number of optimizations 
in both the CPU improvement on representative matrices derived from 3D and GPU codes that can improve 
performance, including surface meshes. The availability of an explicitly managed loop unrolling, matrix 
reordering, and register blocking.10 memory in lieu of an implicit cache also has the advan-The parallel 
kernels can also be reimplemented in terms tage that caching and prefetching policies can be specifi­of 
data-parallel scan operations.11 cally tailored to the application needs. One of the important architectural 
features exposed by CUDA is the presence of the per-block shared memory, ExAMPLE: PARALLEL REDUCTION 
a small on-chip memory with very low latency. Taking Suppose that we are given a sequence of N integers 
that advantage of this memory can deliver substantial perfor-must be combined in some fashion (e.g., 
a sum). This mance improvements. One common way of doing this occurs in a variety of algorithms, linear 
algebra being is to use shared memory as a software-managed cache to a common example. On a serial processor, 
we would hold frequently reused data, shown in figure 9. write a simple loop with a single accumulator 
variable In the context of sparse matrix multiplication, we to construct the sum of all elements in sequence. 
On a observe that several rows of A may use a particular array parallel machine, using a single accumulator 
variable element x[i]. In many common cases, and particularly would create a global serialization point 
and lead to very when the matrix has been reordered, the rows using x[i] poor performance. A well-known 
solution to this problem will be rows near row i. We can therefore implement is the so-called parallel 
reduction algorithm. Each paral­a simple caching scheme and expect to achieve some lel thread sums a 
fixed-length subsequence of the input. performance benefit. The block of threads processing We then collect 
these partial sums together, by summing rows i through j will load x[i] through x[j] into its __global__ 
shared memory. We will void plus_reduce(int *input, unsigned int N, int *total) unroll the multiply_row() 
{loop and fetch elements of unsigned int tid = threadIdx.x;x from the cache whenever unsigned int i = 
blockIdx.x*blockDim.x + threadIdx.x;possible. The resulting code is shown in figure 9.  // Each block 
loads its elements into shared memory, paddingShared memory can also // with 0 if N is not a multiple 
of blocksizebe used to make other __shared__ int x[blocksize];optimizations, such as x[tid] = (i<N) ? 
input[i] : 0;fetching Ap[row+1] from an __syncthreads();adjacent thread rather than refetching it from 
memory. // Every thread now holds 1 input value in x[] Because the Tesla //architecture provides // Build 
summation tree over elements. See attached figure.an explicitly managed for(int s=blockDim.x/2; s>0; 
s=s/2)on-chip shared memory {rather than an implicitly if(tid < s) x[tid] += x[tid + s];active hardware 
cache, it __syncthreads();is fairly common to add }this sort of optimization. Although this can impose 
 // Thread 0 now holds the sum of all input valuessome additional devel-// to this block. Have it add 
that sum to the running totalopment burden on the if( tid == 0 ) atomicAdd(total, x[tid]); programmer, 
it is relatively } minor, and the potential performance benefits can be substantial. In the  FIG10 
 FOCUS  Scalable Parallel PROGRAMMING with CUDA pairs of partial sums in parallel. Each step of this 
pair-wise summation cuts the number of partial sums in half and ultimately produces the final sum after 
log2 N steps. Note that this implicitly builds a tree structure over the initial partial sums. In the 
example shown in figure 10, each thread simply loads one element of the input sequence (i.e., it initially 
sums a subse­quence of length one). At the end of the reduction, we want thread 0 to hold the sum of 
all elements initially loaded by the threads of its block. We can achieve this in parallel by summing 
values in a tree-like pattern. The loop in this kernel implicitly builds a summation tree over the input 
elements. The action of this loop for the simple case of a block of eight threads is illustrated in figure 
11. The steps of the loop are shown as successive levels of the diagram and edges indicate from where 
partial sums are being read. At the end of this loop, thread 0 holds the sum of all the values loaded 
by this block. If we want the final value of the location pointed to by total to contain the total of 
all elements in the array, we must combine the partial sums of all the blocks in the grid. One strategy 
would be to have each block write its partial sum into a second array and then launch the reduction kernel 
again, repeating the process until we had reduced the sequence to a single value. A more attractive alternative 
supported by the Tesla architecture is to use atomicAdd(), an efficient atomic read-modify-write primitive 
supported by the memory subsystem. This eliminates the need for addi­tional temporary arrays and repeated 
kernel launches. Parallel reduction is an essential primitive for parallel programming and highlights 
the importance of per-block shared memory and low-cost barriers in making coopera­tion among threads 
efficient. This degree of data shuffling among threads would be prohibitively expensive if done in off-chip 
global memory.  THE DEMOCRATIzATION OFPARALLEL PROGRAMMING CUDA is a model for parallel programming 
that pro­vides a few easily understood abstractions that allow the programmer to focus on algorithmic 
efficiency and develop scalable parallel applications. In fact, CUDA is an excellent programming environment 
for teaching paral­lel programming. The University of Virginia has used it as just a short, three-week 
module in an undergraduate computer architecture course, and students were able to write a correct k-means 
clustering program after just three lectures. The University of Illinois has successfully taught a semester-long 
parallel programming course using CUDA to a mix of computer science and non-computer science majors, 
with students obtaining impressive speedups on a variety of real applications, including the previously 
mentioned MRI reconstruction example. CUDA is supported on NVIDIA GPUs with the Tesla unified graphics 
and computing architecture of the GeForce 8-series, recent Quadro, Tesla, and future GPUs. The programming 
paradigm provided by CUDA has allowed developers to harness the power of these scal­able parallel processors 
with relative ease, enabling them to achieve speedups of 100 times or more on a variety of sophisticated 
applications. The CUDA abstractions, however, are general and provide an excellent programming environment 
for mul­ticore CPU chips. A prototype source-to-source translation framework developed at the University 
of Illinois com­piles CUDA programs for multicore CPUs by mapping a parallel thread block to loops within 
a single physical thread. CUDA kernels compiled in this way exhibit excel­lent performance and scalability.12 
Although CUDA was released less than a year ago, it is already the target of massive development activity 
there are tens of thousands of CUDA developers. The combi­nation of massive speedups, an intuitive programming 
environment, and affordable, ubiquitous hardware is rare in today s market. In short, CUDA represents 
a democrati­zation of parallel programming. Q REFERENCES 1. NVIDIA. 2007. CUDA Technology; http://www.nvidia. 
com/CUDA. 2. NVIDIA. 2007. CUDA Programming Guide 1.1; http:// developer.download.nvidia.com/compute/cuda/1_1/ 
NVIDIA_CUDA_Programming_Guide_1.1.pdf. 3. Stratton, J.A., Stone, S. S., Hwu, W. W. 2008. M-CUDA: An 
efficient implementation of CUDA kernels on mul­ticores. IMPACT Technical Report 08-01, University of 
Illinois at Urbana-Champaign, (February). 4. See reference 3. 5. Buck, I., Foley, T., Horn, D., Sugerman, 
J., Fataha­lian, K., Houston, M., Hanrahan, P. Brook for GPUs: Stream computing on graphics hardware. 
2004. Proceedings of SIGGRAPH (August): 777-786; http://doi. acm.org/10.1145/1186562.1015800. 6. Stone, 
S.S., Yi, H., Hwu, W.W., Haldar, J.P., Sutton, B.P., Liang, Z.-P. 2007. How GPUs can improve the quality 
of magnetic resonance imaging. The First Workshop on General-Purpose Processing on Graphics Processing 
Units (October). 7. Stone, J.E., Phillips, J.C., Freddolino, P.L., Hardy, D.J., Trabuco, L.G., Schulten, 
K. 2007. Accelerating molecu­lar modeling applications with graphics processors. Journal of Computational 
Chemistry 28(16): 2618 2640; http://dx.doi.org/10.1002/jcc.20829. 8. Nyland, L., Harris, M., Prins, 
J. 2007. Fast n-body simulation with CUDA. In GPU Gems 3. H. Nguyen, ed. Addison-Wesley.  9. Golub, 
G.H., and Van Loan, C.F. 1996. Matrix Compu­tations, 3rd edition. Johns Hopkins University Press. 10. 
Buatois, L., Caumon, G., Lévy, B. 2007. Concurrent number cruncher: An efficient sparse linear solver 
on the GPU. Proceedings of the High-Performance Computa­tion Conference (HPCC), Springer LNCS. 11. Sengupta, 
S., Harris, M., Zhang, Y., Owens, J.D. 2007. Scan primitives for GPU computing. In Proceedings of Graphics 
Hardware (August): 97 106. 12. See Reference 3.  Links to the latest version of the CUDA development 
tools, documentation, code samples, and user discussion forums can be found at: http://www.nvidia.com/CUDA. 
LOVE IT, HATE IT? LET US KNOW feedback@acmqueue.com or www.acmqueue.com/forums JOHN NICKOLLS is director 
of architecture at NVIDIA for GPU computing. He was previously with Broadcom, Silicon Spice, Sun Microsystems, 
and was a cofounder of MasPar Computer. His interests include parallel processing systems, languages, 
and architectures. He has a B.S. in electrical engi­neering and computer science from the University 
of Illinois, and M.S. and Ph.D. degrees in electrical engineering from Stanford University. IAN BUCK 
works for NVIDIA as the GPU-Compute software manager. He completed his Ph.D. at the Stanford Graph­ics 
Lab in 2004. His thesis was titled Stream Computing on Graphics Hardware, researching programming models 
and computing strategies for using graphics hardware as a general-purpose computing platform. His work 
included developing the Brook software tool chain for abstracting the GPU as a general-purpose streaming 
coprocessor. MICHAEL GARLAND is a research scientist with NVIDIA Research. Prior to joining NVIDIA, he 
was an assistant profes­sor in the department of computer science at the University of Illinois at Urbana-Champaign. 
He received Ph.D. and B.S. degrees from Carnegie Mellon University. His research interests include computer 
graphics and visualization, geo­metric algorithms, and parallel algorithms and programming models. KEVIN 
SKADRON is an associate professor in the depart­ment of computer science at the University of Virginia 
and is currently on sabbatical with NVIDIA Research. He received his Ph.D. from Princeton University 
and B.S. from Rice University. His research interests include power- and temperature-aware design, and 
manycore architecture and programming models. He is a senior member of the ACM. &#38;#169; 2008 ACM 1542-7730/08/0300 
$5.00  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401153</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>6</pages>
		<display_no>17</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Programmable graphics]]></title>
		<subtitle><![CDATA[the future of interactive rendering]]></subtitle>
		<page_from>1</page_from>
		<page_to>6</page_to>
		<doi_number>10.1145/1401132.1401153</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401153</url>
		<abstract>
			<par><![CDATA[<p>Recent innovations in computer hardware architecture---the arrival of multi-core CPUs, the generalization of graphics processing units (GPUs), and the imminent increase in bandwidth available between CPU and GPU cores---make a new era of interactive graphics possible. As a result of these changes, game consoles, PCs and laptops will have the potential to provide unprecedented levels of visual richness, realism, and immersiveness, making interactive graphics a compelling killer app for these modern computer systems. However, current graphics programming models and APIs, which were conceived of and developed for the previous generation of GPU-only rendering pipelines, severely hamper the type and quality of imagery that can be produced on these systems. Fulfilling the promise of <i>programmable graphics</i>---the new era of cooperatively using the CPU, GPU, and complex, dynamic data structures to efficiently synthesize images---requires new programming models, tools, and rendering systems that are designed to take full advantage of these new parallel heterogeneous architectures.</p> <p>Neoptica is developing the next-generation interactive graphics programming model for these architectures, as well as new graphics techniques, algorithms, and rendering engines that showcase the unprecedented visual quality that they make possible.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098639</person_id>
				<author_profile_id><![CDATA[81100088896]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pharr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098640</person_id>
				<author_profile_id><![CDATA[81100460149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefohn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098641</person_id>
				<author_profile_id><![CDATA[81100640935]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kolb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098642</person_id>
				<author_profile_id><![CDATA[81100065581]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lalonde]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098643</person_id>
				<author_profile_id><![CDATA[81332498769]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098644</person_id>
				<author_profile_id><![CDATA[81100006255]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Geoff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Programmable Graphics The Future of Interactive Rendering Matt Pharr, Aaron Lefohn, Craig Kolb, Paul 
Lalonde, Tim Foley, and Geoff Berry Neoptica Technical Report, March 2007 Overview Recent innovations 
in computer hardware architecture the arrival of multi-core CPUs, the generalization of graphics processing 
units (GPUs), and the imminent increase in bandwidth available between CPU and GPU cores make a new era 
of interactive graphics possible. As a result of these changes, game consoles, PCs and laptops will have 
the potential to provide unprecedented levels of visual richness, realism, and immersiveness, making 
interactive graphics a compelling killer app for these modern computer systems. However, current graphics 
programming models and APIs, which were conceived of and developed for the previous generation of GPU-only 
rendering pipelines, severely hamper the type and quality of imagery that can be produced on these systems. 
Ful.lling the promise of programmable graphics the new era of cooperatively using the CPU, GPU, and complex, 
dynamic data structures to ef.ciently synthesize images requires new programming models, tools, and rendering 
systems that are designed to take full advantage of these new parallel heterogeneous architectures. Neoptica 
is developing the next-generation interactive graphics programming model for these architectures, as 
well as new graphics techniques, algorithms, and rendering engines that showcase the unprecedented visual 
quality that they make possible. Introduction Computer system architecture is amidst a revolution. The 
single-processor computer is being supplanted by parallel heterogeneous systems comprised of processors 
supporting multiple styles of computation. CPU architects are no longer able to improve computational 
performance of the traditional heart of the computer system, the CPU, by increasing the clock speed of 
a single processor; instead, they are now providing a rapidly-increasing number of parallel coarse-grained 
cores, currently capable of delivering approximately 90 GFLOPS. Simultaneously, graphics processing units 
have evolved to be ef.cient .ne-grained data-parallel coprocessors that deliver much greater raw .oating-point 
horsepower than today s multi-core CPUs; the latest graphics processors from NVIDIA and AMD provide on 
the order of 400 GFLOPS of peak performance via hundreds of computational units working in parallel. 
In addition, although CPUs and GPUs have traditionally been separated by low-bandwidth and high-latency 
communication pathways (e.g. AGP and PCI-Express), rapidly-improving interconnect technology (e.g. AMD 
Torrenza and Intel Geneseo) and the promise of integrating CPUs and GPUs on a single chip (e.g. AMD Fusion) 
allow CPUs and GPUs to share data much more ef.ciently, thereby enabling graphics applications to intermix 
computation styles to optimally use the system's computational resources. The success of these new heterogeneous 
parallel architectures hinges upon consumer applications taking full advantage of their computational 
power. In order for this to happen, programmers must be presented with intuitive and ef.cient parallel 
programming models for these systems. However, decades of work on parallel programming solutions have 
shown that low-level primitives such as mutexes, semaphores, threads, and message passing are not amenable 
to creating reliable, complex software systems. Furthermore, existing higher-level parallel programming 
abstractions have not proven widely successful; these models typically limit developers to a single type 
of parallelism (i.e., exclusively data-parallel or exclusively task-parallel), which unnecessarily constrains 
developer .exibility and makes poor use of the mixed computational resources in heterogeneous systems. 
Without a higher-level, easy-to-use parallel programming model that allows developers to take full advantage 
of the entire system, the new parallel architectures may not deliver compelling bene.t to users, thus 
reducing consumer demand for new PCs. Interactive 3-D computer graphics is now the most computationally 
demanding consumer application. The economic force of the computer gaming industry and its appetite for 
computational power have driven the rapid development of current GPUs. In addition, the GPU programming 
model represents perhaps the only widely-adopted parallel programming model to date. Unfortunately, this 
model assumes a GPU-only, unidirectional, .xed graphics pipeline. Creating a new programming model for 
interactive graphics that fully exposes the computational and communication abilities of these new architectures 
is necessary to enable a revolution in the quality and ef.ciency of interactive graphics and to provide 
a killer app for these new platforms. Neoptica is developing the next-generation interactive graphics 
programming model for heterogeneous parallel architectures, as well as a broad suite of new graphics 
techniques, algorithms, and renderers that showcase the unprecedented visual quality possible with these 
systems. Neoptica's solution makes possible the new era of programmable graphics: parallel CPU and GPU 
tasks cooperatively executing graphics algorithms while sharing complex, dynamic data structures. With 
Neoptica's technology, graphics programmers are able to: treat all processors in the system as .rst-class 
participants in graphics computation;  easily express concurrent computations that are deadlock-free, 
composable, and intuitive to debug;  design custom graphics software pipelines, rather than being limited 
to the single pipeline exposed by current GPUs and graphics APIs; and  design rendering algorithms that 
use dynamic, complex user-de.ned data structures for sparse and adaptive computations.  By enabling 
graphics programmers to fully leverage these new architectures and freeing them from the constraints 
of the prede.ned, one-way graphics pipeline, Neoptica's system spurs the next generation of graphics 
algorithm innovation, with potential impact far greater than that of the programmable shading revolution 
of the last .ve years. Trends in Interactive Graphics The last .ve years have seen signi.cant innovation 
in interactive graphics software and hardware. GPUs have progressed from being con.gurable .xed-function 
processors to highly-programmable data-parallel coprocessors, while CPUs have evolved from single-core 
to task-parallel multi-core processors. These changes have brought about three stages of interactive 
graphics programming: Fixed function: the GPU was con.gurable, but not programmable. Certain specialized 
states could be set to achieve simple visual effects (e.g. bump mapping) using multi-pass rendering techniques. 
The life-span of this stage was short due to its lack of .exibility and relatively high bandwidth demands. 
 Programmable shading: the vertex and fragment processing stages of the GPU rendering pipeline could 
be programmed using small data-parallel programs called shaders. Using shaders, procedural techniques 
such as vertex skinning, complex texturing techniques, and advanced lighting models could be implemented 
ef.ciently on the GPU. This approach spurred a great deal of graphics algorithm innovation. However, 
existing graphics APIs and programming models limit developers to a .xed rendering pipeline and a small 
set of prede.ned data structures. Implementing custom rendering techniques that exploited more complex 
data structures was possible only with heroic developer effort, greatly increased code complexity, and 
high development costs.  Programmable graphics: today, developers are on the threshold of being able 
to de.ne custom interactive graphics pipelines, using a heterogeneous mix of task- and data-parallel 
computations to de.ne the renderer. This approach enables complex data structures and adaptive algorithms 
for techniques such as dynamic ambient occlusion, displacement mapping, complex volumetric effects, and 
real-time global illumination that were previously only possible in of.ine rendering. However, current 
graphics programming models and tools are preventing the widespread transition to this era.  The promise 
of programmable graphics illustrates the fact that GPU programmability has implications for computer 
graphics far beyond simple programmable shaders. User-de.ned data structures and algorithms bring tremendous 
.exibility, ef.ciency, and image quality improvements to interactive rendering. Indeed, programmable 
graphics can be seen as completing the circle of GPGPU (general purpose computing on GPUs). Much of the 
recent innovation in using data structures and algorithms on GPUs has been driven by the application 
of GPUs to computational problems outside of graphics. In the era of programmable graphics, techniques 
developed for GPGPU are applied to the computational problems of advanced interactive graphics. By giving 
graphics programmers the ability to de.ne their own rendering pipelines with custom data structures, 
programmable graphics brings far greater .exibility to interactive graphics programmers than is afforded 
even to users of today s of.ine rendering systems. A renderer's ability to ef.ciently build and use dynamic, 
complex data structures relies on a mix of task- and data-parallel computation. The GPU's data-parallel 
computation model, where the same operation is performed on a large number of data elements using many 
hardware-managed threads, is ideal for using data structures and for generating large amounts of new 
data. In contrast, the task-parallel compute model used by CPU-like processors provides an ideal environment 
in which to build data structures, perform global data analysis, and perform other more irregular computations. 
While it is possible to use only one processor type for all rendering computations, heterogeneous renderers 
that leverage the strengths of each use available hardware resources much more ef.ciently, and make interactive 
many techniques that would otherwise be limited to use in of.ine rendering alone. The transition to programmable 
graphics is hampered by current graphics programming models and tools. Seemingly simple operations such 
as sharing data between the CPU and GPU, building pointer-based data structures on one processor for 
use on the other, and using complex application data in graphics computation currently require esoteric 
expertise. The specialized knowledge required severely limits the number of developers who are able to 
creatively explore the capabilities of hardware systems, which has historically been the key driver of 
advancing the state of the art in interactive graphics. The New Era Of Programmable Graphics Neoptica 
has built a new system that moves beyond current GPU-only graphics APIs like OpenGL and Direct3D and 
presents a new programming model designed for programmable graphics. The system enables graphics programmers 
to build their own heterogeneous rendering pipelines and algorithms, making ef.cient use of all CPU and 
GPU computational resources for interactive rendering. With Neoptica's technology and a mixture of heterogeneous 
processing styles available for graphics, software developers have the opportunity to reinvent interactive 
graphics. Many rendering algorithms are currently intractable for interactive rendering with GPUs alone 
because they require sophisticated per-frame analysis and dynamic data structures. The advent of programmable 
graphics makes many of these approaches possible in real-time. New opportunities from the era of programmable 
graphics include: Feedback loops between GPU and CPU cores: with the ability to perform many round-trip 
per-pixel communications per frame, users can implement per-frame, global scene analyses that guide adaptive 
geometry, shading, and lighting calculations to substantially reduce unnecessary GPU computation.  Complex 
user-de.ned data structures that are built and used during rendering: these data structures enable demand-driven 
adaptive algorithms that deliver higher-quality images more ef.ciently than today s brute-force, one-way 
graphics pipeline.  Custom, heterogeneous rendering pipelines that span all processor resources: for 
example, neither a pure ray-tracing approach nor a pure rasterization approach is the most ef.cient way 
to render complex visual effects like shadows, re.ections, and global lighting effects; heterogeneous 
systems and programmable graphics will make it possible to easily select the most appropriate algorithm 
for various parts of the graphics rendering computation. During the past year, Neoptica has built a 
suite of high-level programming tools that enable programmable graphics by making it easy for developers 
to write applications that perform sophisticated graphics computation across multiple CPUs and GPUs, 
while insulating them from the dif.cult problems of parallel programming. The system:  uses a C-derived 
language for coordinating rendering tasks while using languages such as Cg and HLSL for GPU programming 
and C and C++ for CPU programming, integrating seamlessly with existing development practices and environments 
and providing for easy adoption;  treats all processors in the system as .rst-class participants in 
graphics computation and enables users to easily share data structures between processors;  presents 
a deadlock-free, composable parallel programming abstraction that embraces both data-parallel and task-parallel 
workloads;  provides intuitive, source-level debugging and integrated performance measurement tools. 
 This system has enabled in the rapid development of new programmable graphics rendering algorithms 
and pipelines. Developers are able to design custom rendering algorithms and systems that deliver imagery 
that is impossible using the traditional hardware rendering pipeline, and deliver 10x to 50x speedups 
of existing GPU-only approaches. Summary We are at the threshold of a new era of interactive computer 
graphics. No longer limited to today s brute­force, unidirectional rendering pipeline, developers will 
soon be able to design adaptive, demand-driven renderers that ef.ciently and easily leverage all processors 
in new heterogeneous parallel systems. New rendering algorithms that tightly couple the distinct capabilities 
of the CPU and the GPU will generate far richer and more realistic imagery, use processor resources more 
ef.ciently, and scale to hundreds of both CPU and GPU cores. Neoptica's technology ushers in this new 
era of interactive graphics and makes it accessible to a large number of developers. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401154</section_id>
		<sort_key>220</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Beyond programmable shading: in action]]></section_title>
		<section_page_from>5</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098645</person_id>
				<author_profile_id><![CDATA[81100460149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefohn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098646</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098647</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098648</person_id>
				<author_profile_id><![CDATA[81421592647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098649</person_id>
				<author_profile_id><![CDATA[81100112064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pellacini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401155</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>20</pages>
		<display_no>18</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Beyond programmable shading]]></title>
		<subtitle><![CDATA[in action]]></subtitle>
		<page_from>1</page_from>
		<page_to>20</page_to>
		<doi_number>10.1145/1401132.1401155</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401155</url>
		<abstract>
			<par><![CDATA[<p>This second course in a series will demonstrate case studies of combining traditional rendering API usage with advanced parallel computation from game developers, researchers, and graphics hardware vendors. There are strong indications that the future of interactive graphics programming is a model more flexible than today's OpenGL/Direct3D pipelines. As such, graphics developers need to have a basic understanding of how to combine emerging parallel programming techniques and more flexible graphics processors with the traditional interactive rendering pipeline. Each case study includes a live demo and discusses the mix of parallel programming constructs used, details of the graphics algorithm, and how the rendering pipeline and computation interact to achieve the technical goals.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>D.1.3</cat_node>
				<descriptor>Parallel programming</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169.10010175</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010175</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098650</person_id>
				<author_profile_id><![CDATA[81100460149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefohn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098651</person_id>
				<author_profile_id><![CDATA[81100541080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AMD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098652</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098653</person_id>
				<author_profile_id><![CDATA[81421592647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Id Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098654</person_id>
				<author_profile_id><![CDATA[81100112064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pellacini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Beyond Programmable Shading: In Action SIGGRAPH 2008 5/19/2008 Course Organizers: Aaron Lefohn, Intel 
Mike Houston, AMD Course Speakers: Aaron Lefohn Intel Mike Houston AMD David Luebke NVIDIA Jon Olick 
 Id Software Fabio Pellacini Dartmouth College Speaker Contact Info: Aaron Lefohn 2700 156th Ave NE, 
Suite 300 Bellevue, WA 98007 aaron.lefohn@intel.com Mike Houston 4555 Great America Parkway Suite 501 
Santa Clara, CA 95054 Michael.Houston@amd.com  David Luebke 1912 Lynchburg Dr. Charlottesville, VA 22903 
dlebke@nvidia.com  Jon Olick 3819 Towne Crossing #222 Mesquite, TX 75150 zelexi@gmail.com Fabio Pellacini 
6211 Sudikoff Lab Hanover, NH 03755 fabio@cs.dartmouth.edu          Beyond Programmable Shading: 
In Action Course Description: This second course in a series will demonstrate case studies of combining 
traditional rendering API usage with advanced parallel computation from game developers, researchers, 
and graphics hardware vendors. There are strong indications that the future of interactive graphics programming 
is a model more flexible than today s OpenGL/Direct3D pipelines. As such, graphics developers need to 
have a basic understanding of how to combine emerging parallel programming techniques and more flexible 
graphics processors with the traditional interactive rendering pipeline. Each case study includes a live 
demo and discusses the mix of parallel programming constructs used, details of the graphics algorithm, 
and how the rendering pipeline and computation interact to achieve the technical goals. Intended Audience: 
We are targeting researchers and engineers interested in investigating advanced graphics techniques using 
parallel programming techniques on many-core GPU and CPU architectures, as well as graphics and game 
developers interested in integrating these techniques into their applications. Prerequisites: Attendees 
are expected to have experience with a modern graphics API (OpenGL or Direct3D), including basic experience 
with shaders, textures, and framebuffers and/or background with parallel programming languages. Some 
background with parallel programming on CPUs or GPUs is useful but not required as an overview of will 
be provided in the course. Attendees are strongly encouraged to attend the first course in this series, 
Beyond Programmable Shading: Fundamentals. Level of difficulty: Advanced Special presentation requirements: 
Several speakers will bring their own demo machines for use in the course. Speakers: - Aaron Lefohn, 
Intel  - Mike Houston, AMD  - David Luebke, NVIDIA  - Jon Olick, Id Software  - Fabio Pellacini, 
Dartmouth College  Beyond Programmable Shading: In Action - Introduction: Mike Houston: 15 min  o 
Brief recap of course 1  From programmable shading to fully programmable graphics  List of programming 
environment possibilities  Recap of benefits over using the traditional graphics pipeline  Teaser 
pictures for case studies    (NOTE: All case studies will discuss the graphics algorithms, the mix 
of parallel computation being used to achieve the effect(s), as well as performance and of course pretty 
pictures) - Research Case Study  o Interactive Cinematic Lighting: Fabio Pellacini: 30 min  The fake: 
pre-computing your way to interactive global illumination on a GPU  The real deal: no-pre-computation 
global illumination in 5 seconds with CPUs + GPU  What it will take to do this 500 times faster  
  - Games Case Studies  o Current Generation Parallelism in Games: Jon Olick (Sony/Id): 30 min  
Offloading work from the GPU to many CPU cores  Triangle culling, progressive meshes, displacement 
maps  Geometry compression / decompression: indices, vertices  Tightly-coupled CPU-GPU synchronization 
techniques  o Next Generation Parallelism in Games: Jon Olick (Id): 30 min  Why ray casting? What 
are the advantages over rasterization?  Does it add end-user value? Risks?  Efficient implementations, 
control flow, data structures    - Break, 15 minutes   - Graphics Hardware Vendor Case Studies 
 o Each of the hardware vendor talks will give a case study covering several advanced graphics algorithms 
that require a mix of data- and/or task-parallel computation as well as the traditional graphics pipeline. 
Example topics may include, but are not limited to:  Interactive global illumination  Building/using 
irregular data structures during interactive rendering  Combining real-time physics and rendering 
 Combining ray tracing and rasterization  o Each case study will describe the graphics algorithms, 
the data structures used/built, the parallel algorithms and programming tools used, as well as give live 
demos.  o The details of the case studies cannot be revealed at this time due to intellectual property 
limitations; however, each of the vendors and speakers has a strong reputation of delivering cutting-edge 
content and algorithm design to the SIGGRAPH community.  NVIDIA case study: 30 min  AMD case study: 
30 min  Intel case study: 30 min   - Conclusions: Lefohn + All Speakers: 15 min  o Summary of state-of-the-world 
 o Guesses at the future  o Open Q &#38; A with all speakers    Speaker Biographies Aaron Lefohn, 
Ph.D. Aaron Lefohn is a Senior Graphics Architect at Intel on the Larrabee project. Previously, he designed 
parallel programming models for graphics as a Principal Engineer at Neoptica, a computer graphics startup 
that was acquired by Intel in October 2007. Aaron's Ph.D. in Computer Science from the University of 
California Davis focused on data structure abstractions for graphics processors and data-parallel algorithms 
for rendering. From 2003 - 2006, he was a researcher and graphics software engineer at Pixar Animation 
Studios, focusing on interactive rendering tools for artists and GPU acceleration of RenderMan. Aaron 
was formerly a theoretical chemist and was an NSF graduate fellow in computer science. Aaron Lefohn 2700 
156th Ave NE, Suite 300 Bellevue, WA 98007 425-881-4891 aaron.lefohn@intel.com Mike Houston, Ph.D. Mike 
Houston is a System Architect in the Advanced Technology Development group at AMD in Santa Clara working 
in architecture design and programming models for parallel architectures. He received his Ph.D. in Computer 
Science from Stanford University in 2008 focusing on research in programming models, algorithms, and 
runtime systems for parallel architectures including GPUs, Cell, multi-core, and clusters. His dissertation 
includes the Sequoia runtime system, a system for programming hierarchical memory machines. He received 
his BS in Computer Science from UCSD in 2001 and is a recipient of the Intel Graduate Fellowship. Mike 
Houston 4555 Great America Parkway Suite 501 Santa Clara, CA 95054 408-572-6010 Michael.Houston@amd.com, 
 David Luebke, Ph.D. David Luebke is a Research Scientist at NVIDIA Corporation, which he joined after 
eight years on the faculty of the University of Virginia. He has a Ph.D. in Computer Science from the 
University of North Carolina and a B.S. in Chemistry from the Colorado College. Luebke's research interests 
are GPU computing and realistic real-time computer graphics. Recent projects include advanced reflectance 
and illumination models for real-time rendering, image-based acquisition of real-world environments, 
temperature-aware graphics architecture, and scientific computation on GPUs. Past projects include leading 
the book "Level of Detail for 3D Graphics" and the Virtual Monticello museum exhibit at the New Orleans 
Museum of Art. David Luebke 1912 Lynchburg Dr. Charlottesville, VA 22903 434-409-1892 dlebke@nvidia.com 
 Jon Olick Jon Olick has been working on games and games technology professionally for the past 9 years, 
helping to create multi-million selling games such as Medal of Honor: Allied Assault. Currently, Jon 
is a Programmer and Engineer at id Software. Previously, on the ICE team (a technology group based at 
Naughty Dog) he played key architecture roles in the design and development of many Sony first and third 
party tools and technologies for PlayStation 3. This work has already been used in game titles such as 
Uncharted: Drakes Fortune, Resistance: Fall of Man, Heavenly Sword, NBA 07, Warhawk, MotorStorm and MLB 
2007. He is a published author in the book GPU Gems 2. Jon Olick 3819 Towne Crossing #222 Mesquite, 
TX 75150 918-407-3744 zelexi@gmail.com  Fabio Pellacini, Ph.D. Fabio Pellacini is an assistant professor 
in computer science at Dartmouth College. His research focuses on algorithms for interactive, high-quality 
rendering of complex environments and for artist-friendly material and lighting design to support more 
effective content creation. Prior to joining academia, Pellacini worked at Pixar Animation Studios on 
lighting algorithms, where he received credits on various movie productions. Pellacini received his Laurea 
degree in physics from the University of Parma (Italy), and his M.S. and Ph.D. in computer science from 
Cornell University. Fabio Pellacini 6211 Sudikoff Lab Hanover, NH 03755 603-646-8710 fabio@cs.dartmouth.edu 
  Beyond Programmable Shading: In Action Mike Houston AMD  Disclaimer about these Course Notes  The 
material in this course is bleeding edge Unfortunately, that means we can t share most of the details 
with you until SIGGRAPH 2008  Several talks are missing from the submitted notes  The talks that are 
included may change substantially   To address this inconvenience We will post all course notes/slides 
on a permanent web page, available the first day of SIGGRAPH 2008    Review from course 1 (Fundamentals) 
 Future interactive rendering techniques will be an inseparable mix of data- and task-parallel algorithms 
and graphics pipelines  How do we write new interactive 3D rendering algorithms?  Fixed-Function Graphics 
Pipeline  Writing new rendering algorithms means Tricks with stencil buffer, depth buffer, blending, 
 Examples Shadow volumes  Hidden line removal       Programmable Shading  Writing new rendering 
algorithms means Tricks with stencil buffer, depth buffer, blending,  Plus: Writing shaders  Examples 
Parallax mapping  Shadow-mapped spot light       Beyond Programmable Shading  Writing new rendering 
algorithms means Tricks with stencil buffer, depth buffer, blending,  Plus: Writing shaders  Plus: 
Writing data- and task-parallel algorithms Analyze results of rendering pipeline  Create data structures 
used in rendering pipeline   Examples Dynamic summed area table  Dynamic quadtree adaptive shadow 
map  Dynamic ambient occlusion        Fast Summed-Area Table Generation and its Applications, 
Hensley et al., Eurographics 2005  Resolution Matched Shadow Maps, Lefohn et al., ACM Transactions on 
Graphics 2007  Dynamic Ambient Occlusion and Indirect Lighting, Bunnell, GPU Gems II, 2005  Beyond 
Programmable Shading  Writing new rendering algorithms means Tricks with stencil buffer, depth buffer, 
blending,  Plus: Writing shaders  Plus: Writing data- and task-parallel algorithms Analyze results 
of rendering pipeline  Create data structures used in rendering pipeline   Plus: Extending, modifying, 
or creating graphics pipelines  Examples PlayStation 3 developers creating hybrid Cell/GPU graphics 
pipelines See upcoming talk from Jon Olick (Id Software)   Active area of research      Why Beyond 
Programmable Shading?  Short answer: The parallel processors in your desktop machine or game console 
are now flexible and powerful enough to execute both User-defined parallel programs and  Graphics pipelines 
 All within 1/30th of a second      This Course  Case studies from game developers, academics, 
and industry   This Course  Show-casing new interactive rendering algorithms that result in more realistic 
imagery than is possible using only the pre- defined DX/OpenGL graphics pipeline by  Combining task-, 
data-, and/or graphics pipeline parallelism,  Analyzing intermediate data produced by graphics pipeline, 
 Building and using complex data structures every frame, or  Modifying/extending the graphics pipelines 
  Speakers (in order of appearance)  Mike Houston, AMD  Fabio Pellacini, Dartmouth College  Jon 
Olick, Id Software  Dave Luebke, NVIDIA  Aaron Lefohn, Intel   Schedule  Intro 1:45 2:00 Houston 
 Research Case Study Interactive Cinematic Lighting 2:00 2:30 Pellacini    Games Case Studies Current 
Generation Parallelism in Games 2:30 3:00 Olick  Next Generation Parallelism in Games 3:00 3:30 Olick 
   <Break> 3:30 3:45 Graphics Hardware Vendor Case Studies NVIDIA 3:45 4:15 Luebke  AMD 4:15 4:45 
Houston  Intel 4:45 5:15 Lefohn   Q &#38; A 5:15+     
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401156</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>47</pages>
		<display_no>19</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Interactive cinematic lighting]]></title>
		<page_from>1</page_from>
		<page_to>47</page_to>
		<doi_number>10.1145/1401132.1401156</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401156</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098655</person_id>
				<author_profile_id><![CDATA[81100112064]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pellacini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 interactive cinematic lighting fabiopellacini joint work with M. Hasan (Cornell), K. Bala (Cornell), 
K. Vidimce et al. (Pixar)  disclaimer: will skip details ask questions if interested  digital lighting 
design CornellBox-light CornellBox-final  user repeatedly set lighting parameters feedback: run the 
rendering algorithm   digital lighting design CornellBox-light CornellBox-final  user repeatedly 
set lighting parameters feedback: run the rendering algorithm goal: interactive feedback   digital 
lighting design CornellBox-light CornellBox-final  user repeatedly set lighting parameters feedback: 
run the rendering algorithm goal: intuitive interfaces   I have started to work in this field more 
than 15 years ago. I do not understand why, while computers get faster, it takes us more time than before 
to make an image - jc kalache, dp, pixar  cinematic rendering  high geometric complexity  105 smooth 
primitives high shading complexity  103shaders with 105 ops with 5 GBs textures high quality antialiasing 
 no artifacts with depth-of-field, motion blur complex lighting  indirect illumination, subsurface 
scattering, environment lighting, ambient occlusion  cinematic rendering  high geometric complexity 
 105 smooth primitives high shading complexity  103shaders with 105 ops with 5 GBs textures high 
quality antialiasing  no artifacts with depth-of-field, motion blur complex lighting  indirect illumination, 
subsurface scattering, environment lighting, ambient occlusion not interactive  goal: interactive feedback 
 in complex environments with high accuracy  cinematic rendering  high geometric complexity  105 smooth 
primitives high shading complexity  103shaders with 105 ops with 5 GBs textures high quality antialiasing 
 no artifacts with depth-of-field, motion blur complex lighting  indirect illumination, subsurface 
scattering, environment lighting, ambient occlusion  box00 box1 box2  direct indirect first indirect 
bounce other indirect bounces direct bounce    kitchen-lc  [image courtesy of B. Walter]   kitchen-lc 
complex lighting as many-lights lighting simulation converted to virtual point lights [image courtesy 
of B. Walter]  kitchenh-ref k4 k1 k3 = S hair-ref b5 b2 b4 = S temple-300-900-16-9 = S l4 l1 sky 
  ~1M pixels ~100k lights temple-300-900-16-9   l4 l1   ~ 0.1 s ~ 30 m realtime offline 
 computing one column [SIGGRAPH 2005b] [SIGGRAPH 2005a] lpics images  method: cache approx. data-parallel 
hardware lpics images  results: artist at work one column at a time lpics images  motion blur and 
depth of field [Regan-Kelley et al., SIGGRAPH 2007] lightspeed images  impact: interactivity for cinematic 
lighting changes artists workflow  lessons learned  interactive cinematic lighting is possible  interactive 
algorithms mostly match offline  hard to match: shader as content is bad!  render-dependent look definition 
  temple-ref hair-ref kitchenh-ref 10 min 13 min 20 min brute force too slow  gather-dir direct 
on virtual lights indirect illumination still-ind cache   [SIGGRAPH 2006]  method: precompute 
(sparsely) matrix compress by signal processing data-parallel sparse multiply    coherence .sparse 
light vector gather-dir flatten 3D virtual lights sparse arrange into image wavelet transform method: 
impose a wavelet basis, by clustering, then lossily compress   coherence .sparse matrix method: impose 
same wavelet basis for rows construct sparse images for each columns fg2 fg3 wl1 wl2 wl3 fg1 after wavelet 
xform original columns lit by wavelet lights lit by original lights  results precomputation: 1.6 
h 11.4 18.7 fps 107k polys /Users/fabio/Documents/Data/Work/docs/talks/Yale07/dti06-bunny.mov results 
 precomputation: 2.5 h 8.5 25.8 fps 2.1M polys /Users/fabio/Documents/Data/Work/docs/talks/Yale07/dti06-temple.mov 
 impact: highest quality interactive illumination for lighting design to date  lessons learned  interactive 
global illumination is possible  interactive algorithms do not match offline  operations: rasterize, 
shade, other kernels  other kernels are CPU or (painfully) GPU   required improvement: dynamic environments 
with no precomputation  cb matrix   900 pixels 700 lights insight: [SIGGRAPH 2007]  cb similar 
columns intuition: coherence .low rank         render representatives final image as weighted 
sum          cluster columns render representatives final image as weighted sum      
 method: cluster similar columns render representatives computed sum            unknown matrix 
 render rows on GPU assemble rows into reduced matrix cluster reduced columns render representative 
columns on GPU final image as weighted sum of representative columns      method: render a few 
rows, cluster reduced columns   sum of column pairs norms of reduced columns  dist of reduced columns 
 cost of all clusters cluster metric clustering as unbiased importance sampling  results temple-ref 
temple-300-900-16-9 300 rows / 900 cols: ~ 17 s reference: ~ 17 m 2.1 M polys / 100 k lights  results 
 hair-100-400-5-0 trees-100-200-2-9 complex geometry and arbitrary materials/lights  comparison to state 
of the art > 30x for same quality insight: data parallel queries                  
                   stochastic pointsampling on CPU stochastic coherent sampling on 
GPU  impact: fastest algorithm for previewing accurate lighting  next: geometric scalability (cities, 
forests, crowds in secs)  take home message  shaders are not content (renderer lock)  complex lighting 
does not map to GL/DX  GL/DX are great for other applications  for us, GL/DX complexity is not useful 
 for us, GL/DX constraints stifle innovation   questions? /Users/fabio/Documents/Data/Work/docs/talks/Yale07/dti06-bunny.mov 
   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401157</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>120</pages>
		<display_no>20</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Current generation parallelism in games]]></title>
		<page_from>1</page_from>
		<page_to>120</page_to>
		<doi_number>10.1145/1401132.1401157</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401157</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.3</cat_node>
				<descriptor>Parallel programming</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169.10010175</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010175</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098656</person_id>
				<author_profile_id><![CDATA[81421592647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[id Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Current Generation Parallelism In Games Jon Olick id Software  Brief History of Parallelism  1 Processor 
The good old days.  Why parallelize? Just wait a little and your programs will get faster.    Brief 
History of Parallelism  2 to 3 Processors Logical splitting of game process into pipelined pieces. Game 
 Rendering  Sound  Loading/Decompression      Brief History of Parallelism  About 6 to 8 Processors 
The transition to a job scheduling type architecture  1st order parallelism Game  Rendering  Sound 
 Physics  Collision  Loading/Decompression  Etc      Brief History of Parallelism  About 8 
to 16 Processors End of CPU history.  Enter 1998 in GPU history. Approx # of processors as average parallel 
scalar operations.   2nd order parallelism  Jobs which create and manage the resources of other jobs. 
GPU Command Processor (DMA engine)      Brief History of Parallelism  About 16+ processors 3rd 
order parallelism  Jobs which create and manage the resources of other jobs which create and manage 
the resources of other jobs GPU Vertex Processors      Brief History of Parallelism    Current 
State of Parallelism  Desktop Processors Intel Core 2 Quad - 4 processors - 3.2 ghz - 51.2 Gflops (102.4 
Gflops theoretical) Soon to be 8 core?     Multimedia Processors Cell Processor - 8 processors - 
3.2 ghz - 134.4 Gflops (192.0 Gflops theoretical) 1 main, 7 co-processors     Graphics Accelerators 
8800 GTX - 1.35 ghz - 345.6 Gflops (518.43 Gflops theoretical) 128 stream processors      THE CELL 
PROCESSOR  PLAYSTATION®3 Cell Processor Overview  Game  Animation  Geometry Processing  Post Processing 
 Occlusion Rasterization  Sorting  Collision Detection  Fourier Transform  (De)Compression  Not 
going to cover all of these   PLAYSTATION®3 Cell Processor Overview  Parallelize ordinarily sequential 
CPU processing  Assist in what is typically considered GPU processing   PLAYSTATION®3 Cell Processor 
Overview  Fitting code and data in the 256k local co- processor memory  Best solutions are ones that 
don't treat the 256k local store as a typical on demand caching architecture Scattered reads bad, sequential 
reads good   Software Pipelining  Only 16 bytes aligned reads/writes  Synchronization   MD6 ANIMATION 
PROCESSING  MD6 Animation Processing  Game Logic  MD6 Animation Processing  Blending Tree Generation 
 Game Logic  MD6 Animation Processing   Low Level Operation List Generation Blending Tree Generation 
 Game Logic  MD6 Animation Processing   Low Level Operation Execution Low Level Operation List Generation 
 Blending Tree Generation Game Logic  MD6 Animation Processing   Low Level Operation Execution 
Low Level Operation List Generation Blending Tree Generation Game Logic  Serial  Parallel  MD6 
Animation Processing  Additive Blending  Subtractive Blending  Animation Algebra Blend Equations Animation 
blending trees in the form of an equation.  Example equation: (animA + animB) animC       Partial 
Animation Blending  Generalized play an animation only on the face, torso, etc  One weight per joint 
per animation  Compute alpha for slerp via following equation: For each joint Let w0 = weight of joint 
in animation A  Let w1 = weight of joint in animation B  If(w1 > w0) Let alpha = (alpha * w1) / w0 
   Else Let alpha = ((w1 w0) + alpha * w0) / w1       Varying parameter treatment     
                   Varying parameter treatment                  
                 Varying parameter treatment                     
                         16k 16k 16k  MD6 Animation Webs  Separates Thinking 
from Representation Game Object says what it wants to look like.  Animation Webs take care of the rest. 
   Unstructured graph Each node has a blend tree   Designed with simplicity in mind Animators should 
animate, not fiddle with nodes.  Extract as much information as possible directly from the animation 
data.    MD6 Animation Webs  MD6 Animation Webs  = play standAnim  MD6 Animation Webs  You Are 
Here  Desired State = Stand  Blend Equation = play standAnim   MD6 Animation Webs  Desired State 
= Stand  Blend Equation = play standAnim   MD6 Animation Webs   Desired State = Stand  Blend Equation 
= play standAnim   MD6 Animation Webs   Desired State = Walk  Blend Equation = play standAnim  
 MD6 Animation Webs   Desired State = Walk  Blend Equation = blend standAnim and walkAnim   MD6 
Animation Webs   Desired State = Walk  Blend Equation = play walkAnim   MD6 Animation Webs   Desired 
State = Stand  Blend Equation = play standAnim   MD6 Animation Webs   Desired State = Stand  Blend 
Equation = play standAnim   MD6 Animation Webs   Desired Traversal = Stand to Walk  Desired Direction 
= ~5 degrees   MD6 Animation Webs   Desired Traversal = Stand to Walk  Desired Direction = ~175 
degrees   MD6 Animation Webs   Desired State = Run   MD6 Animation Webs   Desired Traversal 
= Run to Cover  Mechanical Behavior Not believable   MD6 Animation Webs    Desired State = Run 
  MD6 Animation Webs     Desired State = Run   MD6 Animation Webs  Automatically picks best 
animation for search criteria  Time warps animation to perfectly match criteria  Works in parallel 
with game code   GEOMETRY PROCESSING  Two modes of usage  Primary mode Use offline tools  Partition 
into vertex sets  Use indexed triangles  All features of pipeline can be used         SPU 
                  Two modes of usage (cont)  Secondary mode Data generated by other 
tools  Formats other than indexed triangles  Non-partitioned objects  Subset of pipeline features 
can be used           SPU                        SPU Geometry Pipeline 
Stages   Index Decompress  Blend Shapes  Skinning  Triangle Culling  Compression  Vertex 
Decompress  Output  SPU Pipeline  Progressive Mesh  Vertex Decompression   Index Decompress 
 Blend Shapes  Skinning  Triangle Culling  Compression  Vertex Decompress  Output  SPU Pipeline 
 Progressive Mesh  Vertex Attributes       Unique Vertex Array 0              
  Instance Vertex Array 1    Vertex Decompression                Float Tables  
                           Unique Vertex Array 0 Instance Vertex Array 1 
 24bit Unit Vector  Smallest 2 compression Two smallest components with 10 bits each Encoded from sqrt(2)/2 
to +sqrt(2)/2   Largest component reconstructed via Largest = sqrt(1 smallestA2 smallestB2)  One 
additional bit for sign of largest component.      24bit Unit Vector  Smallest 2 compression Two 
smallest components with 10 bits each Encoded from sqrt(2)/2 to +sqrt(2)/2   Largest component reconstructed 
via Largest = sqrt(1 smallestA2 smallestB2)  One additional bit for sign of largest component.  
   One more bit to represent W as +1 or -1 For constructing bi-normal from normal and tangent.   
  N-bit Fixed Point with integer offsets  Simple n.x fixed point values Per-segment integer offset 
   Bit count may vary from attribute to attribute   Index Decompression   Index Decompress  Blend 
Shapes  Skinning  Triangle Culling  Compression  Vertex Decompress  Output  SPU Pipeline 
 Progressive Mesh  Index Table Construction  Index table is created by a vertex cache optimizer Based 
on K-cache algorithm   Number of vertex program outputs affects Vertex Cache size.  Four vertex mini 
cache most important optimization factor   Strip Example        3 new vertices 3  Strip Example 
        3 1 1 new vertex  Strip Example        3 1 1 1 new vertex  Strip Example 
        3 1 1 1 1 new vertex 2 vertices + 1 per triangle in total  Free Form Example  
      3 new vertices 3  Free Form Example        1 new vertex 3  1  Free Form Example 
       1 new vertex 3  1 1  Free Form Example        1 new vertex 3  1 1 1  Free 
Form Example        1 new vertex 3  1 1 1 1  Free Form Example        0 new vertices! 
 3  1 1 1 1 0  Free Form Example        1 new vertex 3  1 1 1 1 0 1  Free Form Example 
       1 new vertex 3  1 1 1 1 0 1 1  Free Form Example        1 new vertex 3 
 1 1 1 1 0 1 1 1  Free Form Example        0 new vertices 3  1 1 1 1 0 1 1 1 
 0 2 vertices + 3 per 4 triangles in total  Index Cache Optimizer  Our vertex cache optimizer produces 
very regular index data   Index Decompression  Provided vertex cache optimizer produces very regular 
index data   Index Decompression  0 1 2    Triangle Indexes 0 2 1  Index Decompression 
 2 0 1    Triangle Indexes 2 1 0  Index Decompression  Index Decompression  Index Decompression 
 85% compression 6.5 : 1  Blend Shapes   Index Decompress  Blend Shapes  Skinning  Triangle 
Culling  Compression  Vertex Decompress  Output  SPU Pipeline  Progressive Mesh  Blend Shapes 
in MLB 08: The Show nocloth cloth Skinning   Index Decompress  Blend Shapes  Skinning  Triangle 
Culling  Compression  Vertex Decompress  Output  SPU Pipeline  Progressive Mesh  Skinning on 
SPUs void SkinVs(float4 inPosition : ATTR0, float4 weights : ATTR3, float4 matrixIndex : ATTR4, out 
float4 position : POSITION, uniform float4 joints[72], uniform float4x4 modelViewProj) { position = 0; 
for (int i = 0; i < 4; i++) { float idx = matrixIndex[i]; float3x4 joint = float3x4(joints[idx+0], joints[idx+1], 
joints[idx+2]); position += weights[i] * mul(joint, inPosition); } position = mul(modelViewProj, position); 
}  Skinning on SPUs 30% Performance Improvement  Skinning on SPUs 30% Performance Improvement Shadow 
map generation.... 70%!  Discrete Progressive Mesh  Smoothly reduces the triangle count as a model 
moves into the distance  With discrete progressive mesh, the LOD calculation is done once for an entire 
object    Index Decompress  Blend Shapes  Skinning  Triangle Culling  Compression  Vertex 
Decompress  Output  SPU Pipeline  Progressive Mesh  At an LOD there are two types of vertexes 
LOD = 0.0 Parent Vertex Child Vertex  As the LOD level decreases, the children slide towards their 
parents LOD = 0.2 Parent Vertex Child Vertex  The children continue to move towards their parents 
 LOD = 0.7 Parent Vertex Child Vertex  At the next integral LOD, all child vertexes disappear as do 
the triangles                              LOD = 1.0  Parent Vertex Child 
Vertex  Continuous Progressive Mesh  Like discrete progressive mesh, child vertexes move smoothly toward 
their parents  However, the LOD is calculated for each vertex instead of just once for the object  
  Index Decompress  Blend Shapes  Skinning  Triangle Culling  Compression  Vertex Decompress 
  Output  SPU Pipeline  Progressive Mesh  Vertex set about to undergo continuous progressive mesh 
 Parent Vertex Child Vertex, LOD 1 Child Vertex, LOD 0  A single vertex set can straddle several LOD 
ranges LOD = 1.0 Parent Vertex Child Vertex, LOD 1 Child Vertex, LOD 0 LOD = 0.0  Vertexes move 
depending on their distance Parent Vertex Child Vertex, LOD 1 Child Vertex, LOD 0 LOD = 1.0 LOD 
= 0.0  Triangle Culling   Index Decompress  Blend Shapes  Skinning  Triangle Culling  Compression 
  Vertex Decompress  Output  SPU Pipeline  Progressive Mesh  Up to 70% of triangles do not contribute 
to final image.          Off Screen Triangles          Back Facing Triangles   
        Zero Area Triangles          Zero Area Triangles           No Pixel 
Triangles                       Triangle Culling                
       Multisampling adds some complications                         
  Culled                           Triangle Culling 10% to 20% Performance 
Improvement  Compression for Output   Index Decompress  Blend Shapes  Skinning  Triangle Culling 
  Compression  Vertex Decompress  Output  SPU Pipeline  Progressive Mesh           
    Float Tables       When done, the vertex attributes are compressed into one output stream 
                   Output Vertex Array                Float Tables 
      Output Buffering Schemes  Vertex and index data constructed by the SPUs is output from SPU 
local store  Holes in the RSX command buffer are patched with pointers to the vertex and index data 
as well as the draw commands    Index Decompress  Blend Shapes  Skinning  Triangle Culling 
 Compression  Vertex Decompress  Output  SPU Pipeline  Progressive Mesh  Double Buffer  Each 
buffer stores vertex and index data for an entire frame  SPUs atomically access a mutex which is used 
to allocate memory from a buffer  Easy synchronization with the RSX once a frame  Uses lots of memory 
  Vertex and Index Data for Frame 0 Vertex and Index Data for Frame 1   It is possible to completely 
fill a buffer  Can use a callback to allocate new memory (which you may not have)  Don t draw geometry 
that doesn t fit (difficult to pick which geometry not to draw)  SPU Data Vertex and Index Data 
   Double buffering adds a frame of lag Build Jobs on PPU Process Jobs on SPU Render on RSX Scan 
Out Build Jobs on PPU Process Jobs on SPU Render on RSX Scan Out Build Jobs on PPU Process Jobs 
on SPU Render on RSX Scan Out  Single Buffering  Uses only half the memory!  Still possible to completely 
fill the buffer  Vertex and Index Data for Single Frame  Single Buffering has a shorter pipeline 
 Vertex and index data is created just-in-time for the RSX  Draw commands are inserted into the command 
buffer while the RSX is rendering  Requires tight SPU.RSX synchronization  Build Jobs on PPU SPU 
Processing/ RSX Rendering Scan Out Build Jobs on PPU SPU Processing/ RSX Rendering Scan Out Build 
Jobs on PPU SPU Processing/ RSX Rendering Scan Out  SPU.RSX Synchronization Using Local Stalls  Command 
Buffer Draw 17 Local Stall Other Local Stall Local Stall Local Stall Local Stall Local Stall 
 Place local stalls in the command buffer where necessary  RSX will stop processing at a local stall 
until it is overwritten by new commands  SPUs will generally stay ahead of the RSX , so stalls rarely 
occur  Local Stall  SPU will overwrite local stalls when it outputs a set of new commands  No SPU.SPU 
synchronization required!  SPU   Command Buffer Draw 17 Put Pointer Local Stall Other Local 
Stall Local Stall Local Stall Local Stall Local Stall Local Stall New Commands  Ring Buffers 
 Small memory footprint  Will not run out of memory  Can stall the SPUs if buffers become full  Objects 
need to be processed in the same order the RSX renders them to prevent deadlock   Vertex and Index 
 Data  End of Free Area  Start of Free Area  RSX writes a semaphore once a chunk of data has been 
consumed  A command to write a semaphore needs to be added to the command buffer after all commands 
that use the data The value of the semaphore to be written is the new end of free area pointer    
 Data 19 Data 6 Data 14  Start of Free Area Draw 6 Draw 5 Draw 7 Semaphore Semaphore Semaphore 
 Current RSX Execution  New End of Free Area  Each SPU has its own buffer  Data 23 Data 17 Data 
10  Data 22 Data 11 Data 16  Data 19 Data 6 Data 14  Data 20 Data 7 Data 15  Data 21 Data 
12 Data 9  Data 18 Data 8 Data 13  SPU 0 SPU 1 SPU 2 SPU 3 SPU 4 SPU 5     Buffer 0 
Buffer 1 Buffer 3 Buffer 2 Buffer 4 Buffer 5  Geometry Performance  Software Pipelined C with 
SPU Intrinsics do { m1 = in1; in1 = si_lqx(pIn1, offset); m2 = in2; in2 = si_lqx(pIn2, offset); m3 = 
in3; in3 = si_lqx(pIn3, offset); temp2 = si_selb(m3, m1, mask_0X00); si_stqx(out1, pOut1, offset); temp3 
= si_selb(m2, m1, mask_00X0); si_stqx(out2, pOut2, offset); temp1 = si_selb(m1, m2, mask_0X00); si_stqx(out3, 
pOut3, offset); offset = si_ai(offset, 0x30); out2 = si_shufb(m2, temp2, qs_bCaD); out1 = si_selb(temp1, 
m3, mask_00X0); out3 = si_shufb(m3, temp3, qs_caBD); } while(si_to_int(offset) != 0);  Software Pipelined 
C with SPU Intrinsics Up to 20x faster than naive C/C++ do { m1 = in1; in1 = si_lqx(pIn1, offset); 
m2 = in2; in2 = si_lqx(pIn2, offset); m3 = in3; in3 = si_lqx(pIn3, offset); temp2 = si_selb(m3, m1, mask_0X00); 
si_stqx(out1, pOut1, offset); temp3 = si_selb(m2, m1, mask_00X0); si_stqx(out2, pOut2, offset); temp1 
= si_selb(m1, m2, mask_0X00); si_stqx(out3, pOut3, offset); offset = si_ai(offset, 0x30); out2 = si_shufb(m2, 
temp2, qs_bCaD); out1 = si_selb(temp1, m3, mask_00X0); out3 = si_shufb(m3, temp3, qs_caBD); } while(si_to_int(offset) 
!= 0);  1 SPU  1 SPU 800,000+ Triangles Per Frame at 60 Frames per Second  1 SPU 800,000+ Triangles 
Per Frame at 60 Frames per Second 60% of which are culled!    
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401158</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>89</pages>
		<display_no>21</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Next generation parallelism in games]]></title>
		<page_from>1</page_from>
		<page_to>89</page_to>
		<doi_number>10.1145/1401132.1401158</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401158</url>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Parallel processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.1.3</cat_node>
				<descriptor>Parallel programming</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169.10010175</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011009.10010175</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language types->Parallel programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010169</concept_id>
				<concept_desc>CCS->Computing methodologies->Parallel computing methodologies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098657</person_id>
				<author_profile_id><![CDATA[81421592647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[id Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                            Next Generation 
Parallelism In Games Jon Olick id Software  GAME ENTITY PROCESSING  Game Entity Processing  Current 
Generation Serial Processing of entities in a giant for loop.    for(int i = 0; i < numEntities; ++i) 
{ entity[i]->Think(); }  Game Entity Processing  Current Generation Serial Processing of entities in 
a giant for loop.   Next Generation Parallelism via Double Buffering    Game Entity Processing 
 Current Generation Serial Processing of entities in a giant for loop.   Next Generation Parallelism 
via Double Buffering  Each entity can only read from previous frame s results    Game Entity Processing 
 Current Generation Serial Processing of entities in a giant for loop.   Next Generation Parallelism 
via Double Buffering  Each entity can only read from previous frame s results  Each entity can only 
write to itself  Every entity runs in parallel with each other with no dependency stalls.    Game 
Entity Processing  Record the progress of the game and replay it to debug.  Single thread and randomize 
processing of entities to help find bugs.  Can protect memory so that bad accesses cause exceptions 
to enforce double buffering rules.   RAY CASTING  Why Ray Casting?  A good question   ?  ?  Back 
in Quake 1 If you had to make a decision between an additional CPU and a Graphics Card which would you 
choose?    ?  Back in Quake 1 If you had to make a decision between an additional CPU and a Graphics 
Card which would you choose?  Why is this any different today?    ?  Back in Quake 1 If you had 
to make a decision between an additional CPU and a Graphics Card which would you choose?  Why is this 
any different today?  Its not.    Why Ray Casting?  What value does it provide to game developers? 
Scene complexity no longer significantly affects performance or memory      Why Ray Casting?  What 
value does it provide to end users?   Screen shot from RAGE goes here   Screen shot from RAGE goes 
here  (highlight flat regions)   Why Ray Casting?  Current State of Rasterization  Vertex Processing 
  Triangle Setup  Fragment Processing  Command Buffer    Vertex Processing  Fragment Processin 
g    Future of Rasterization         Future of Rasterization          Future of 
Rasterization                    Future of Rasterization             
       Future of Rasterization                        Future of Rasterization 
                               Future of Rasterization        
                                       Future of Rasterization 
 Vertex Processing  Triangle Setup  Fragment Processing  Command Buffer    Vertex Processing 
  Fragment Processing    Triangle Sorting   Future of Rasterization  Vertex Processing  Triangle 
Setup  Fragment Processing  Command Buffer         Multiple Cores  Future of Rasterization 
x2  GPU Triangle Setup  Future of Rasterization x2 150 GPU Triangle Setup   Future of Rasterization 
x2 151 GPU Triangle Setup   Future of Rasterization x2 152 GPU Triangle Setup   Future of Rasterization 
x2 153 GPU Triangle Setup   Future of Rasterization x2 154 GPU Triangle Setup   Why voxels, 
and not triangles?  Unifying Primitive  No Near Phase Collision  Solves Two Problems in One Unique 
Texturing &#38; Unique Geometry   Good for CUDA / Larabee   Why is the control flow efficient? 
          Why is the control flow efficient?           Why is the control flow efficient? 
          Why is the control flow efficient?         Voxel Mip Mapping Thin Walls 
          Voxel Mip Mapping Thin Walls      Voxel Mip Mapping Thin Walls    Voxel 
Mip Mapping Thin Walls   Caveats of Ray-Tracing?  Primary rays cache, secondary rays thrash Importance 
sampling to the rescue!   Ray Tracing != Ray Casting   Results May Vary  Sparse Voxel Oct-trees 
 Oct-trees as collection of maximal blocks. Related to run-length encoding.   Possibly variable splitting 
planes in future. Guided by artist placed hint planes.    Data Structure  Disk Caching with Virtual 
and Physical Pages Start out with a single virtual page.  Render some voxels into the tree until page 
capacity is reached.  Split page into 8 sub-pages and attempt to add the overflow voxel again.  Store 
out virtual pages to disk.  Load/Unload each page s levels as necessary at runtime.     Data Structure 
 Page capacity can be based on... CUDA's shared memory size  Cell's SPU local store size  Optimum 
disk streaming performance  Minimum physical page memory    Data Structure Page Fragmentation 
 Traverse indexing oct-tree Write out pages according to optimal layout (breadth first, depth first, 
etc...)    Data Structure - Page Optimization  Execution time proportional to number of nodes. 
 Number of nodes can be reduced through translation.  Translating by 2n doesn t affect any oct-tree 
level smaller than 2n   Data Structure - Page Optimization  Create scratch page with enlarged region 
2n+1 x 2n+1 x 2n+1   Apply successive translations of magnitude power of 2 in the x, y, &#38; z directions 
and keep a count of the number of nodes required.  Store off total translation so that the ray casting 
can be adjusted appropriately.  O(4n) n is the number of levels in the oct-tree    Data Structure 
- Page Optimization 172          Data Structure - Page Optimization 173         
 Data Structure - Page Optimization  Minimize outside nodes for faster tracing   Data Structure - 
Page Optimization 175          Data Structure - Page Optimization 176          
 Data Structure - Page Optimization 177           Data Structure  Different structures for 
runtime and storage.   Runtime Data Structure  child pointers : 32  diffuse rgb : 3  specular scale/power 
: 1  normal xy : 2  total : 38 bytes per node   Storage Data Structure  children bit mask : 1  
diffuse rgb : 3  specular scale/power : 1  normal xy : 2  total : 7 bytes per node   Data Compression 
 Compressing child bits  Compressing Colors   Compressing Child Bits  (diagram showing statistical 
distribution of child bytes)   Compressing Child Bits  Split by oct-tree level.  Arithmetic Compression 
 (diagram showing statistical distribution of child bytes at each oct-tree level)   Compressing Color 
Data  (Diagram showing each node of the tree as a delta between itself and its parent)   Compressing 
Color Data  Split by oct-tree level.  Quantization  Arithmetic Compression   Data Structure Size 
 1.15 bits of positional data per voxel  Cost savings improves as triangle size decreases.  72 bits 
equivalent per triangle in oct-tree (for next generation)   160 bits per triangle in traditional format 
x,z,y,s,t all 32-bits  2.2:1 compression ratio   80 bits per triangle in compressed format x,y,z,s,t 
all 16-bits  1.1:1 compression ratio    Generating the data  Every surface can enumerate into voxels. 
 3D Scan Conversion  Volume Projection  Subdivision   3D Scan Conversion  Z=0  Z=1  Z=2  
 Z=3  Z=4  =  3D Scan Conversion       3D Scan Conversion            3D Scan Conversion 
                                3D Scan Conversion  Only 8 connectivity 
required.  Flood fill world and remove unnecessary voxels.   Generating the data  Generate geometry 
mip-maps  Geometry mips consume 15% extra data   Generating the data  Save off the un-lit data. 
  Generating the data  Perform ray-tracing to light the voxels.   Using the data  For each pixel 
on the screen Shoot out a ray into the oct-tree and record the color (and depth?)    Need More Detail? 
 Bottom up?  Top Down?  Etc   Rendering Skinned Characters  Ray cast against triangle mesh  Transform 
to base pose  Trace with local oct-tree   Abstracting Skinned Characters  Moving world geometry without 
oct-tree re- organization cost.  Enables instancing.   Hybrid Rendering  Render a coarse hull of 
the geometry into a z-buffer. Automatically calculate from voxel geometry.   Post-process the depth-buffer 
to start the ray casting at the point specified by the depth-buffer instead of at the ray origin. Possibly 
transforming it as well.    Hybrid Rendering  Skips most of the traversal process. 2x to 4x expected 
speed improvement   Allows dynamic geometry  Prevents second and third order ray tracing No z-buffer 
pre-pass to transform ray start for dynamic geometry    Hybrid Rendering with Atmospheric Effects 
 Atmospheric effects in combination with rasterization possible. Rasterize coarse hull into depth buffer 
 Rasterize triangles into color and alternate depth buffer  At ray-collision time, keep track of atmospheric 
collisions along the way.  If hit rasterized depth, stop and modify existing color   Else modify 
voxel color.      Adaptive Sub-Sampling  After rendering the scene, perform a Sobel edge filter 
over the frame buffer to figure out where additional rays would improve the quality of the image.  Cast 
additional rays.  Repeat until 16 ms.   Adaptive Sub-Sampling Problems  Inherently always sampling 
the most divergent parts of the scene  Can manage performance hit by sampling highly aliased to less 
aliased in chunks   Infinite Surface Detail  Oct-tree node's recursively point back in on themselves 
to create an infinite amount of detail  Create detail types octree sub-segments to simulate rough, smooth, 
porous, sharp edges, etc.. Recursive trees are entirely in cache, so very fast calculation.  Requires 
delta compressed colors to be in the runtime format for correct shadowing Not a huge computation burden, 
but more bandwidth required      How much time to innovate?  1 year tools  3 months runtime  
 Expected Runtime Performance  33% of the time rendering characters / etc  66% of the time rendering 
world  Ray-casting the world must complete in ~20ms for 30 FPS  Theoretically possible on today's technology 
(GeForce 8800 Series) Possible on 7800 with additional memory requirements.    How would this affect 
a platform launch?  Generational skip in geometric complexity  Next gen platforms 4 times better at 
least  Completely plausible at 720p and 60 FPS w/ antialaising or 1080p and 60 FPS    ? Questions 
 Jon Olick (jon.olick@gmail.com) id Software  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401159</section_id>
		<sort_key>270</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: CGAL - the computational geometry algorithms library]]></section_title>
		<section_page_from>6</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098658</person_id>
				<author_profile_id><![CDATA[81100223488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alliez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098659</person_id>
				<author_profile_id><![CDATA[81100137321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fabri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098660</person_id>
				<author_profile_id><![CDATA[81100459634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Efi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fogel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401160</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>358</pages>
		<display_no>22</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Computational geometry algorithms library]]></title>
		<page_from>1</page_from>
		<page_to>358</page_to>
		<doi_number>10.1145/1401132.1401160</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401160</url>
		<abstract>
			<par><![CDATA[<p>The CGAL Open Source Project provides <i>easy access to</i> efficient and reliable geometric algorithms in the form of a C++ library, offering geometric data structures and algorithms, which are efficient, robust, easy to use, and easy to integrate in existing software. The usage of de facto standard libraries increases productivity, as it allows software developers to focus on the application layer. This course is targeted at software developers with geometric needs, and course graduates will be able to select and use the appropriate algorithms and data structures provided by CGAL in their current or upcoming projects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.4</cat_node>
				<descriptor>Algorithm design and analysis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.13</cat_node>
				<descriptor>Reusable libraries</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003705</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011072</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Software libraries and repositories</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098661</person_id>
				<author_profile_id><![CDATA[81100223488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alliez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098662</person_id>
				<author_profile_id><![CDATA[81100137321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fabri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GeometryFactory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098663</person_id>
				<author_profile_id><![CDATA[81100459634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Efi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fogel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel-Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computational Geometry Algorithms Library Pierre Alliez Andreas Fabri Efi Fogel INRIA GeometryFactory 
Tel-Aviv University Abstract The CGAL Open Source Project provides easy access to efficient and reliable 
geometric algorithms in the form of a C++ library, offering geometric data structures and algorithms, 
which are efficient, robust, easy to use, and easy to integrate in existing software. The usage of de 
facto standard libraries increases productivity, as it allows software developers to focus on the application 
layer. This course is targeted at software developers with geometric needs, and course graduates will 
be able to select and use the appropriate algorithms and data structures provided by CGAL in their current 
or upcoming projects. Key Facts CGAL 3.3, released in June 2007: 90 software components, 600,000 lines 
of code, 3,500 user and reference manual pages, Cross platform support, Annual release with 12,000 downloads, 
1,000 subscribers on the user mailing list, 40 subscribers on the developer mailing list. CGAL is used 
in many application areas by companies as Total (Oil&#38;gas), British Telecom (Telecom), Cadence (VLSI), 
Leica Geosystems (GIS), Dassault Systèmes (CAD), The Moving Picture Company (Visual effects). CGAL Project 
The project is steered by an Editorial Board, it has a well defined development process, and the infrastructure 
for distributed development. The following research institutes and companies are actively involved or 
made contributions to the library: INRIA-Sophia-Antipolis, Max-Planck Institute for Computer Science, 
Tel-Aviv University, GeometryFactory, ETH Zurich, FU Berlin, University of Groningen, University of Utrecht, 
Stanford University, Athens University, and the Foundation of Research and Technology Hellas. For more 
information on the project see www.cgal.org For an overview on what is in CGAL: http://www.cgal.org/Manual/3.3/doc_html/cgal_manual/packages.html 
Organization of the Course This course starts with an overview followed by three in-depth sessions covering 
central data structures: The overview session presents the CGAL project and the design principles of 
CGAL. CGAL has adopted the exact computing paradigm, which yields robust and at the same time fast algorithms. 
CGAL further has adopted the generic programming paradigm, which makes CGAL particularly easy to customize 
and to integrate. Finally, we show how CGAL fits naturally with the STL, and the Boost graph library. 
The session on polyhedral surfaces presents the underlying halfedge data structure and how it can be 
customized to user needs. We further present algorithms for polyhedral surfaces like parameterization, 
mesh subdivision and simplification, Boolean operations, and intersection detection. The session on arrangements 
presents the arrangement API and several data structures built on top of it. These are 3D Minkowski sums, 
which can be used for collision detection, and 3D lower envelopes, which can be used for visibility map 
computations. The last session covers the 2D and 3D triangulation API as well as the surface and volume 
mesh generators, which are based on Delaunay refinement. The source code of the examples will be made 
available at http://www.cgal.org/Courses/SIGGRAPH2008. Presenters Dr. Andreas Fabri (organizer) Chief 
Officer GeometryFactory 20, Avenue Yves Emmanuel Baudoin 06130 Grasse, FRANCE Email: andreas.fabri@geometryfactory.com 
http://www.geometryfactory.com/ Dr. Efi Fogel Senior Researcher Faculty of Exact Sciences. Tel Aviv 
University Tel Aviv 69978, ISRAEL Email: efifogel@gmail.com http://www.cs.tau.ac.il/~efif/ Dr. Pierre 
Alliez Senior Researcher INRIA Sophia Antipolis Mediterranee 2004 route des Lucioles BP 93 -GEOMETRICA, 
FRANCE Email: pierre.alliez@sophia.inria.fr http://www-sop.inria.fr/geometrica/team/Pierre.Alliez/ Biographies 
 Andreas Fabri, PhD, GeometryFactory As member of the initial development team of the CGAL project, Andreas 
Fabri is one of the architects of the CGAL software. For several years he chaired the CGAL Editorial 
Board. In 2003 Andreas founded the GeometryFactory as spin-off of the CGAL project, offering licenses, 
service and support to commercial users, who cannot comply with the Open Source license of CGAL. Andreas 
received his PhD in computer science in 2004 from Ecole de Mines de Paris while working on geometric 
algorithms for parallel machines at INRIA. Pierre Alliez obtained his PhD from Ecole nationale supérieure 
des Télécommunications, did his postdoc at Caltech, and is researcher at INRIA since 2001. His main research 
interests are on topics commonly referred to as Geometry Processing: geometry compression, surface approximation, 
mesh parameterization, surface remeshing and mesh generation. He is this year co-chair of the EUROGRAPHICS 
Symposium on Geometry Processing. In 2005 Pierre Alliez received the Eurographics Young Researcher Award. 
Efi Fogel, MSc, Tel-Aviv University Efi Fogel is a co-founder of LucidLogix Ltd., a startup company that 
intends to deliver high performance 3D graphics systems. Efi Fogel is completing his Ph.D. studies at 
Tel-Aviv University. 3D Graphics and Computational Geometry are his main areas of interests. He is a 
member of the CGAL Editorial Board, and he is deeply involved with the design and implementation of the 
arrangement package of CGAL and its derivatives. Efi Fogel received his M.Sc. from Stanford University 
in 1989. He worked for Silicon Graphics Inc. (SGI) between 1989-1997 at the Advanced Graphics Devision, 
where he contributed to the specification of OpenGl among the other. After that Efi worked for Immersia 
Ltd, and he served as the CTO of Enbaya Ltd. Course Syllabus Overview Andreas 30 Polyhedron Pierre 40 
Arrangements Efi 40 Break 20 Triangulations &#38; Meshes Andreas, Pierre 80 Wrap-up, Q&#38;A All 15 
 Bibliography  Bibliographic entries for individual chapters of CGAL manuals  CGAL User and Reference 
Manual Bibliography  CGAL Contributors  CGAL Editorial Board Sylvain Pion, Pierre Alliez, Eric Berberich 
, Andreas Fabri, Efi Fogel, Bernd Gärtner, Michael Hemmer, Michael Hoffmann, Menelaos Karavelas, Marc 
Pouget, Monique Teillaud, Ron Wein CGAL Developers Fernando Cacciola, Frédéric Cazals, Raphaëlle Chaine, 
Pavel Emeliyanenko, Marc Glisse, Luc Habert, Peter Hachenberger, Idit Haran, Thomas Herrmann, Samuel 
Hornus, Michael Kerber, Nico Kruithof, Sebastian Limbach, Sébastien Loriot, Pedro Machado Manhães de 
Castro, Andreas Meyer, Michal Meyerovitch, Thanh-Trung Nguyen, Luis Peñaranda, Joachim Reichel, Laurent 
Rineau, Daniel Russel, Laurent Saboret, Ophir Setter, Tel-Aviv University, George Tzoumas Alumni Matthias 
Bäsken, Hervé Brönnimann, Tran Kai Frank Da, Christophe Delage, Olivier Devillers, Katrin Dobrindt, Eti 
Ezra, Kaspar Fischer, Eyal Flato, Julia Flötotto, Wolfgang Freiseisen, Geert-Jan Giezeman, Philippe Guigue, 
Iddo Hanniel, Sariel Har-Peled, Susan Hert, Shai Hirsch, Lutz Kettner, Eran Leiserowitz, Bruno Levy, 
Eugene Lipovetsky, Abdelkrim Mebarki, Naceur Meskini, Oren Nechushtan, Gabriele Neyer, Steve Oudot, Eli 
Packer, Dmitrii Pasechnik, Sigal Raab, François Rebufat, Niv Sabath, Stefan Schirra, Sven Schönherr, 
Michael Seel, Le-Jeng Shiue, Hans Tangelder, Radu Ursu, Carl Van Geem, Remco Veltkamp, Wieger Wesselink, 
Afra Zomorodian, Tali Zvi, Baruch Zukerman EU Project Board Members Helmut Alt, Jean-Daniel Boissonnat, 
Dan Halperin, Kurt Mehlhorn, Stefan Näher, Mark Overmars, Geert Vegter, Emo Welzl, Peter Widmayer  Overview 
 Andreas Fabri GeometryFactory Mission Statement Make the large body of geometric algorithms developed 
in the field of computational geometry available for industrial applications CGAL Project Proposal, 1996 
Algorithms and Datastructures  Bounding Volumes Polyhedral Surface BooleanOperations  Triangulations 
Voronoi Diagrams Mesh Generation   Subdivision Simplification Parametrisation Streamlines Ridge Neighbor 
Kinetic Detection Search Datastructures   Intersection Minkowski Polytope Lower Envelope Arrangement 
PCA QP Sover Detection Sum distance CGAL in Numbers 500,000 lines of C++ code 10,000 downloads/year 
(+ Linux distributions) 3,500 manual pages 3,000 subscribers to cgal-announce 1,000 subscribers to cgal-discuss 
120 packages 60 commercial users 20 active developers 12 months release cycle 2 licenses: Open Source 
and commercial Some Commercial Users  Why They Use CGAL I recommended to the senior management that 
we start a policy of buying-in as much functionality as possible to reduce the quantity of code that 
our development team would have to maintain. This means that we can concentrate on the application layer 
and concentrate on our own problem domain. Senior Development Engineer &#38; Structural Geologist Midland 
Valley Exploration Why They Use CGAL My research group JYAMITI at the Ohio State University uses CGAL 
because it provides an efficient and robust code for Delaunay triangulations and other primitive geometric 
predicates. Delaunay triangulation is the building block for many of the shape related computations that 
we do. [...] Without the robust and efficient codes of CGAL, these codes could not have been developed. 
Tamal Dey Professor, Ohio State University CGAL Open Source Project CGAL Open Source Project Inria, 
MPI, Tel-Aviv U, Utrecht U, Groningen U, ETHZ, GeometryFactory, FU Berlin, Forth, U Athens  Editorial 
Board  Steers and animates the project  Reviews submissions   Development Infrastructure  Gforge: 
svn, tracker,..  Nightly distributed testsuite  Developer manual and mailing list  Two 1-week developer 
meetings per year   Contributions  Submission of specifications to the Editorial Board  Editorial 
Board review and decision  Institutional members make a long term commitment  Value for contributor 
 Integration in developer community  Chapter author  Gain visibility in a mature project   Exact 
Geometric Computing Predicates and Constructions Predicates Constructions  orientation in_circle intersection 
circumcenter Robustness Issues  Naive use of floating-point arithmetic causes geometric algorithms 
to:  Produce [slightly] wrong output  Crash after invariant violation  Infinite loop   There is 
a gap between  Geometry in theory  Geometry with floating-point arithmetic   Geometry in Theory 
 Correctness proofs of algorithms rely on such theorems The Trouble with double orientation( p, q, r) 
= sign((q-p)(r-p) (q-p)(r-p)) xxyyyyxx  Exact Geometric Computing [Yap] Make sure that the control 
flow in the implementation corresponds to the control flow with exact real arithmetic  Filtered Predicates 
 Generic functor adaptor Filtered_predicate<>  Try the predicate instantiated with intervals  In case 
of uncertainty, evaluate the predicate with multiple precision arithmetic   Refinements:  Static error 
analysis  Progressively increase precision   Filtered Constructions Lazy object = approximated object 
and Lazy number = interval and arithmetic geometric operation tree expression tree (3.2 + 1.5) * 13 
 m' ([], []) n' []  [] 13 i' ([ ], [ ] ) p'([], []) 3.2 1.5 s2 l b s1 Test that may trigger an 
exact re-evaluation: if (collinear(a',m',b')) if ( n' < m' ) The User Perspective  Convenience Kernels 
 Exact_predicates_inexact_constructions_kernel  Exact_predicates_exact_constructions_kernel  Exact_predicates_exact_constructions_kernel_with_sqrt 
  Number Types  double, float  CGAL::Gmpq (rational), Core (algebraic)  CGAL::Lazy_exact_nt<ExactNT> 
  Kernels  CGAL::Cartesian<NT>  CGAL::Filtered_kernel<Kernel>  CGAL::Lazy_kernel<NT>    Merits 
and Limitations Ultimate robustness inside the black box  The time penalty is reasonable, e.g. 10% 
for 3D Delauny triangulation of 1M random points  Limitations of Exact Geometric Computing  Topology 
preserving rounding is non-trivial  Construction depth must be reasonable  Cannot handle trigonometric 
functions    Generic Programming STL Genericity template <class Key, class Less> class set { Less 
less; insert(Key k) { if (less(k, treenode.key)) insertLeft(k); else insertRight(k); } };  CGAL Genericity 
template < class Geometry > class Delaunay_triangulation_2 { Geometry::Orientation orientation; Geometry::In_circle 
in_circle; void insert(Geometry::Point t) { ... if(in_circle(p,q,r,t)) {...} ... if(orientation(p,q,r){...} 
 }  }; CGAL Genericity template < class Geometry, class TDS > class Delaunay_triangulation_2 { }; 
 template < class Vertex, class Face > class Triangulation_data_structure_2 { }; Iterators template 
<class Geometry> class Delaunay_triangulation_2 { typedef .. Vertex_iterator; typedef .. Face_iterator; 
 Vertex_iterator vertices_begin(); Vertex_iterator vertices_end(); template <class OutputIterator> 
incident_faces(Vertex_handle v, OutputIterator it); }; std::list<Face_handle> faces; dt.incident_faces(v, 
std::back_inserter(faces)); Iterators template <class Geometry> class Delaunay_triangulation_2 { template 
<class T> void insert(T begin, T end);// typeof(*begin)==Point }; list<Kernel::Point_2> points; Delaunay_triangulation<Kernel> 
dt; dt.insert(points.begin(), points.end()); Boost Graph Library (BGL) Rich collection of graph algorithms: 
shortest paths, minimum spanning tree, flow, etc.  Design that  decouples data structure from algorithm 
 links them through a thin glue layer   BGL and CGAL  Provide glue layer for CGAL  Extension to 
embedded graphs inducing the notion of faces    BGL Glue Layer: Traits Class  template <typename 
Graph > struct boost::graph_traits { typedef ... vertex_descriptor; typedef ... edge_descriptor; typedef 
... vertex_iterator; typedef ... out_edge_iterator; }; BGL Glue Layer: Free Functions vertex_descriptor 
v, w; v edge_descriptor e; G w v = source(e,G); w = target(e,G); std::pair<out_edge_iterator, out_edge_iterator> 
ipair; ipair = out_edges(v,G); BGL Glue Layer for CGAL Users can run: Courtesy: P.Schroeder, Caltech 
boost::kruskal_mst(P); template <typename T> graph_traits<Polyhedron<T>>; template <typename T> Polyhedron<T>::Vertex 
source(Polyhedron<T>::Edge); CGAL provides partial specializations: CGALmanualCGAL manual From A BGL 
Glue Layer for CGAL  To BGL Style CGAL Algorithms  Summary: Overview Huge collection with uniform 
APIs  Modular and not monolithic  Open Source and commercial licenses  Clear focus on geometry  Interfaces 
with de facto standards/leaders: STL, Boost, GMP, Qt, blas  Robust and fast through exact geometric 
computing  Easy to integrate through generic programming   Right away from the start of the CGAL 
Project the focus was on making software. We are on track to accomplish this mission: There are now 
many algorithms and data structures.  A possible interpretation: Many lines of code, decomposed in independent 
packages which are all well documented. A large user base paired with ongoing development CGAL is distributed 
under an Open Source license as well as under commercial licenses. We are neither dogmatic (pure GPL), 
nor a charity organization (BSD style license): Those who give a free ride, get a free ride. Users come 
from extremely different application areas as geometric computing is ubiquitous.  Two testimonials, 
one from a commercial user and the other one from an academic user. The message is the same: Get more 
productive byleveraging existing technology.  Software is more than files and a manual, but has a 
lot of organizational issues. It needs reviewing as well as testing to fulfill quality standards. It 
needs a development process and management. It needs a community of engaged developers.  Besides institutional 
members, the project got contributions from other research labs, like Stanford.  Predicates and constructions 
are the primitives of many geometric algorithms Performing these kind of operations with floating point 
arithmetic is kind of problematic. « If something is close to zero it is zero »  Computing with machine-precision 
floating-pointarithmetic leads to incorrect or inconsistent results in the presence of degenerate, or 
near-degenerate,inputs. In this example we compute the orientation or three points p, q and r that are 
nearly collinear (namely wecheck if we make a left turn at q when going from p to r). The colored diagram 
shows how small perturbationsin the input may affect the result of evaluting this predicate. The flow 
of an algorithm is typically determined by the results of the predicates it evaluates. When using exact 
computing, it is guaranteed that thepredicates always yield correct result, as in the realRAM model, 
and the correctness of the algorithmfollows. A trivial solution is to use arbitrary precison arithmeticpackages, 
but that is too slow in practice. We don t need to use exact computation per se. Note that we emphasized 
the word corresponds on the previous slide. In order to speed up computations, it is possible to use 
filters that try to use fast, inexact arithmetic wherever possible. Only when this fails (in near­degenerate 
situations), we resort to exact arithmetic. Things get more complicated when the algorithmsneed to construct 
new entities, instead of justperforming predicates that involve its input. We maintain approximations 
of numbers or geometric objects, and store their construction history. When a predicate cannot be succesfully 
evaluated forthe approximations, the exact versions of the objectsare computed. CGAL offers default 
kernels, for algorithms that use only predicates, or for algorithms that performconstructions as well. 
Advanced users choose the arithmetic and the kernel which best fits their needs and requirements. If 
the exact geometric computing paradigm is so great why doesn't everybody simply use it? Because it does 
not solve all problems, especially as soon as one interfaces with software not adhering to the paradigm. 
 Generic programming with C++ means to specify concepts which describe interfaces, and to writetemplates 
of algorithms and data structure, where the concept is a template parameter. Users then can instantiate 
the template will models, that is with classes that implement the interface. Concepts will become part 
of the C++ language. Generic programming is used extensively in the C++Standard Template Library. In 
these slides we see the tip of the iceberg. Templates allow to combine the CGAL data structures with 
-the 2D point class of all CGAL kernels, -the 3D point class together with orientation and incirlce predicates 
working on the projection, -with user defined point classes together with the user defined predicates. 
Note: As there are more types involved than just the point and the incircle test, the real templateparameters 
look slightly different: All types andpredicates are regrouped in one traits class. Also thereal triangulation 
has another template argument forthe underlying triangulation data structure.  Iterators in the STL 
decouple algorithms from data structures. Containers provide iterators which allow to enumerate what 
is inside. In our case vertices, and faces of the triangulation, but also faces. Containers can have 
template member functions, in our example a function that writes into an OutputIterator. Container member 
functions can take iterator ranges as arguments. The advantage is that there is no need to duplicate 
the data in a canonical container. The BGL is one library among others in the Boostproject. The BGL 
needs more elaborate design ideas than the STL as graphs are non-linear structures. Everything for that 
a graph_traits exists is a Graph. The graph_traits allows algorithms to find out about the various types 
a graph provides. Refined version: Everything for that a graph_traits and some free functions, for exploring 
the graph incidences, exist, is a Graph. Even more refined version: There does exist a whole taxonomy 
of graph concepts with varying requirements. The fact that the graph_traits is not part of the graph 
class itself, allows to define graph_traits for third party software without the need to modify it. 
From the BGL point of view, CGAL is just third party software, providing the necessary traits class and 
free functions. Schematically, what we described in the previous slides is the following: The glue layer 
is the implementation of the is model of relationship. As the BGL design is powerful, we extend the 
Graph concept to the EmbeddedGraph concept. The EmbeddedGraph requires that the edges incident to a vertex 
are ordered, which in turn defines edge cycles around faces. CGAL provides first algorithms which operate 
on models of the EmbeddedGraph concept. Just as any BGL algorithm can be used by CGAL polyhedral surfaces, 
2D triangulations and arrangements, CGAL users only have to provide the glue layer for their data structure, 
in order to run CGAL algorithms on them.  Polyhedral Surfaces Pierre Alliez INRIA Outline  Halfedge 
Data Structure and Polyhedron  Euler Operators  Customization  Algorithms for Geometric Modelling 
and Geometry Processing  Halfedge Data Structure Represented by vertices, edges, facets and an incidence 
relation on them, restricted to orientable 2-manifolds with boundary.  Polyhedron Building blocks assembled 
with C++ templates  Default Polyhedron   Example  #include <CGAL/Cartesian.h> #include <CGAL/Polyhedron_3.h> 
 typedef CGAL::Simple_cartesian<double> Kernel; typedef CGAL::Polyhedron_3<Kernel> typedef Polyhedron::Vertex_iterator 
Polyhedron; Vertex_iterator; int main() { Polyhedron p; // ... read from file or build Vertex_iterator 
v; for(v = p.vertices_begin(); v != p.vertices_end(); ++v) std::cout << v->point() << std::endl; } 
Flexible Data Structure   Iterate over all Vertices Vertex_iterator v; for( v = polyhedron.vertices_begin(); 
v != polyhedron.vertices_end(); ++v ) { // do something with v }  Circulate around Facet  Halfedge_around_facet_circulator 
he,end; he = end = f->facet_begin(); CGAL_For_all(he,end) { // do something with he }  Circulate 
around Vertex Halfedge_around_vertex_circulator he,end; he = end = v->vertex_begin(); CGAL_For_all(he,end) 
{ // do something with he }  Euler Operators  split_facet create_center_vertex split_vertex join_facet 
erase_center_vertex join_vertex (aka edge collapse)  split_loop add_vertex_and_facet add_facet_to_border 
join_loop _to_border erase_facet erase_facet  Customization  template <class Refs> struct MyFace : 
public CGAL::HalfedgeDS_face_base<Refs> { CGAL::Color color; }; struct MyItems : public CGAL::Polyhedron_items_3 
{ template <class Refs, class Traits> struct Face_wrapper { typedef MyFace<Refs> Face; }; }; typedef 
CGAL::Simple_cartesian<double> Kernel; typedef CGAL::Polyhedron_3<Kernel,MyItems> MyPolyhedron;   Algorithms 
 Intersection detection  Convex hull  Boolean operations  Kernel  Parameterization  Subdivision 
 Principal component analysis  Estimation of curvatures  Extraction of ridges  Simplification  
 Intersection Detection  Efficient algorithm for finding all intersecting pairs for large numbers of 
axis-aligned bounding boxes. Generic programming: Boxes can contain objects of any type Example: Intersecting 
3D Triangles  #include <CGAL/Exact_predicates_inexact_constructions_kernel.h> #include <CGAL/intersections.h> 
#include <CGAL/Bbox_3.h> #include <CGAL/box_intersection_d.h> typedef CGAL::Exact_predicates_inexact_constructions_kernel 
Kernel; typedef Kernel::Triangle_3 Triangle_3; typedef std::list<Triangle_3>::iterator Iterator; typedef 
CGAL::Box_intersection_d::Box_with_handle_d<double,3,Iterator> Box; void callback(const Box&#38; a, 
const Box&#38; b) { Triangle_3 ta = *a.handle(); Triangle_3 tb = *b.handle(); if(CGAL::do_intersect(ta,tb)) 
{ // do something } } std::list<Triangle_3> triangles(...); std::list<Box> boxes; Iterator it; for(it 
= triangles.begin(); it != triangles.end(); ++it) boxes.push_back(Box((*it).bbox(),it)); CGAL::box_self_intersection_d(boxes.begin(), 
boxes.end(), callback); Convex Hull  From point set  Output a polyhedron   Example #include <CGAL/convex_hull_3.h> 
#include <CGAL/ Exact_predicates_inexact_constructions_kernel.h> typedef CGAL::Exact_predicates_inexact_constructions_kernel 
Kernel; typedef CGAL::Convex_hull_traits_3<Kernel> Traits; typedef Traits::Polyhedron_3 Polyhedron; typedef 
Kernel::Point_3 Point; int main() { std::list<Point> points; // fill container Polyhedron polyhedron; 
CGAL::convex_hull_3(points.begin(), points.end(), polyhedron); return 0; } Boolean Operations The 3D 
Nef polyhedron is a B-repdata structure which is closed under Boolean operations  without enforcing 
regularization  Operations: Union  Intersection  Difference  Complement  Interior  Exterior  
  B-A A-B Boundary  Closure  Regularization    Example #include <CGAL/Exact_predicates_exact_constructions_kernel.h> 
#include <CGAL/Polyhedron_3.h> #include <CGAL/Nef_polyhedron_3.h> typedef CGAL::Exact_predicates_exact_constructions_kernel 
Kernel; typedef CGAL::Polyhedron_3<Kernel> Polyhedron; typedef CGAL::Nef_polyhedron_3<Kernel> Nef_polyhedron; 
 Polyhedron p1; Polyhedron p2; Nef_polyhedron n1(p1); Nef_polyhedron n2(p1); n1 += n2; // union Polyhedron 
p3; n1.convert_to_Polyhedron(p3); +  =   Example n1 -= n2; // difference  - =  Example   = 
- Courtesy: The Moving Picture Company  Kernel Intersection of all its interior half-spaces. (2D) 
Kernel  Intersection of all its interior half-spaces.  Using linear programming  [Fischer et al.] 
 input polyhedron kernel Kernel w/ Linear Programming  ? Empty kernel? Kernel w/ Linear Programming 
  Parameterization  Planar Conformal [Eck et al., Levy et al., Desbrun et al.]  Mean value coordinates 
[Floater]  ...   Example  #include <CGAL/Cartesian.h> #include <CGAL/Polyhedron_3.h> #include <CGAL/Parameterization_polyhedron_adaptor_3.h> 
#include <CGAL/parameterize.h> Polyhedron mesh; Mesh_adaptor_polyhedron mesh_adaptor(&#38;mesh); parameterize(&#38;mesh_adaptor); 
Point_2 uv = mesh_adaptor.info(he)->uv();  Parameterization User-provided cut graph for closed or high 
genus surfaces.  (conformal distortion) Parameterization  Subdivision  Designed to work on CGAL 
polyhedron  Catmull-Clark  Loop  Doo-Sabin  Sqrt3    Example #include <CGAL/Cartesian.h> #include 
<CGAL/Polyhedron_3.h> #include <CGAL/Subdivision_method_3.h> typedef CGAL::Cartesian<double> Kernel; 
typedef CGAL::Polyhedron_3<Kernel> Polyhedron; Polyhedron polyhedron; int subdivision_depth = 3; CatmullClark_subdivision(polyhedron, 
subdivision_depth);  Principal Component Analysis  Linear least squares fitting on sets of 3D kernel 
objects: points for triangle meshes triangles  Example #include <CGAL/linear_least_squares_fitting_3.h> 
 Polyhedron mesh; std::list<Triangle_3> triangles; Polyhedron::Facet_iterator f; for(f = mesh.facets_begin(); 
 f != mesh.facets_end(); ++f) { const Point&#38; a = f->halfedge()->vertex()->point(); const Point&#38; 
b = f->halfedge()->next()->vertex()->point(); const Point&#38; c = f->halfedge()->prev()->vertex()->point(); 
triangles.push_back(Triangle_3(a,b,c)); }  Plane_3 plane; CGAL::linear_least_squares_fitting_3( triangles.begin(), 
triangles.end(), plane,CGAL::PCA_dimension_2_tag() ); Same for Boost freaks  #include <CGAL/linear_least_squares_fitting_3.h> 
#include <boost/iterator/transform_iterator.hpp> Polyhedron mesh; class ToTriangle { Triangle_3 operator(Polyhedron::FacetIterator 
f) { return Triangle_3( f->halfedge()->vertex()->point(); f->halfedge()->next()->vertex()->point(); f->halfedge()->prev()->vertex()->point() 
);  } }; Plane_3 plane; CGAL::linear_least_squares_fitting_3( boost::make_transform_iterator(mesh.facets_begin(), 
ToTriangle()), boost::make_transform_iterator(mesh.facets_end(), ToTriangle()), plane, CGAL::PCA_dimension_2_tag()); 
 Estimation of Curvatures  Estimates general differential properties (Monge form) on point sets. min 
curvature directions Through polynomial (d-jet) fitting  max curvature directions Example  #include 
<CGAL/Monge_via_jet_fitting.h> typedef CGAL::Cartesian<double> Kernel; typedef CGAL::Monge_via_jet_fitting<Kernel> 
Monge_fit; typedef Monge_fit::Monge_form Monge_form; Monge_fit monge_fit; Monge_form monge_form = monge_fit( 
points.begin(), points.end(), dim_fitting, dim_monge); Vector_3 kmin = monge_form.minimal_principal_direction(); 
Vector_3 kmax = monge_form.maximal_principal_direction(); Vector_3 normal = monge_form.normal_direction(); 
 Extraction of Ridges Ridge: curve along which one of the principal curvatures has an extremum along 
its curvature line.  Simplification  Implementation of [Lindstrom-Turk] BGL-style allows simplification 
of any model of EmbeddedGraph Simplification: Example  #include <CGAL/Simple_cartesian.h> #include 
<CGAL/Polyhedron_3.h> #include <CGAL/Surface_mesh_simplification/HalfedgeGraph_Polyhedron_3.h> #include 
<CGAL/Surface_mesh_simplification/edge_collapse.h> #include <CGAL/Surface_mesh_simplification/Policies/Edge_collapse/Count_stop_predicate.h> 
typedef CGAL::Simple_cartesian<double> Kernel; typedef CGAL::Polyhedron_3<Kernel> Mesh; namespace SMS 
= CGAL::Surface_mesh_simplification ; Mesh mesh; SMS::Count_stop_predicate< Mesh > stop(1000); // target 
# edges SMS::edge_collapse(mesh, stop, CGAL::vertex_index_map(boost::get(CGAL::vertex_external_index, 
mesh)) .edge_index_map(boost::get(CGAL::edge_external_index, mesh)));  Summary and Outlook The halfedge 
data structure and the polyhedron are highly flexible  CGAL provides algorithms for geometric modeling 
and geometry processing  Polyhedral surface as output of surface mesh generation algorithms (Part IV) 
  Under development:  Remeshing  BGL-ization of existing CGAL algorithms    We are going to talk 
about not only the CGAL polyhedral surfaces, but also about them at work in algorithms for geometric 
modeling and processing. The design choice for the CGAL polyhedral surface data structure is a halfedge 
data structure. Each edge is represented by two oriented halfedges. Each halfedge can access its next 
and previous halfedge bounding the current facet, the opposite halfedge, its end vertex and its mate 
facet. The C++ design clearly separates combinatorial information from geometry. The combinatorial data 
structure stores only incidence relations between the vertex, halfedge and face primitves, which can 
be customized using the C++ derivation mechanism. Each vertex internally stores a 3D point. The geometric 
traits (by default the point coordinates) are specified through a Geometric Kernel. The final polyhedron 
class is assembled from a halfedge data structure and the geometric traits. The default polyhedron contains 
the three primitive types: vertex, halfedge and facet. The vertex provides access to one incident halfedge 
(minimal and sufficient to seed a circulator) and to its point. The halfedge provides access to its 
opposite, next and previous halfedge, and to its incident vertex and facet. The facet provides access 
to one incident halfedge and its plane equation. Of course, the user can add any attribute to each primitive 
such as normal vector per facet, color per facet, etc.  The simplest code example which instantiates 
a polyhedron and iterates over all its vertices. The data structure is very flexible in the sense that 
it is possible to use only the halfedge data structure as a graph, without even the notion of face or 
vertex. Only the next function and opposite functions are provided for each halfedge. Note that the prev 
function is also accessible in practice and automatically emulated by calling enough next functions. 
 An example of iteration over all mesh vertices, very much inspired by the STL library. It is possible 
to iterate over all primitives: Vertices, points, faces, halfedges, edges (skip one halfedge every two). 
 An example of circulation over halfedges around a facet (counterclockwise). The end circulator is required 
to specify an end to the circular sequence which has in essence no predefined end. The function CGAL_For_all 
calls he++ internally. An example of circulation over halfedges incident to a vertex (clockwise). Each 
halfedge points to the vertex. Note that the circulator calls internally he->next()­>opposite(). The 
polyhedron offers a set of atomic functions to modify the mesh. The function split_facet splits a face 
into two by adding an edge incident to the two specified vertices. Its inverse merges two faces into 
one. The function create_center_vertex creates a triangulation of a face with all triangles incident 
to the newly inserted vertex. Its inverse performs vertex removal, turning the set of incident triangles 
into a polygon. The function split_vertex splits a vertex into two vertices. The inverse function performs 
edge collapse, decreasing the degree by one (or even collapsing for a triangle) the two incident faces. 
The function split_loop separates a cycle of three halfedges into two cycles, introducing a boundary. 
The inverse function merges two cycles. The function add_vertex_and_facet_to_border expands a boundary 
by adding one triangle incident to a specified halfedge and a newly inserted vertex. The function add_facet_to_border 
expands a boundary by adding one triangle incident to two specified halfedges. This piece of code shows 
how to customize a polyhedron by adding a color attribute to each facet. The polyhedron is now templated 
with a custom data structure, which wraps the facet type. The class MyItems could as well wrap a vertex 
and a halfedge. We have chosen eight algorithms revolving around geometric modeling and geometry processing, 
which operate on the CGAL Polyhedron. The first seven algorithms are provided by CGAL and operate directly 
on the CGAL Polyhedron. The mesh simplification algorithm follows the BGL-style and can operate on any 
model of an EmbeddedGraph. The intersection detection algorithms come in two flavors. Either we have 
one set of objects and apply a callback to all pairs of intersections. Or we have two sets, and report 
intersections between objects of the two sets. The closeup on the right shows intersecting triangles 
of a 500Kf version of the Lucy statue. Each triangle is boxed first, then the detection algorithm detects 
486 intersecting triangles in 4s. A code example where each box contains a handle to a 3D triangle. 
The callback function reports only intersections between the boxes, it remains to test if the pair of 
triangles intersects. Here we see a function as callback. In C++ it could as well be a function object 
with internal state, e.g., a counter. Convex Hull From point set  Output a polyhedron   Example 
 #include <CGAL/convex_hull_3.h> #include <CGAL/ Exact_predicates_inexact_constructions_kernel.h> typedef 
CGAL::Exact_predicates_inexact_constructions_kernel Kernel; typedef CGAL::Convex_hull_traits_3<Kernel> 
Traits; typedef Traits::Polyhedron_3 Polyhedron; typedef Kernel::Point_3 Point; int main() { std::list<Point> 
points; // fill container Polyhedron polyhedron; CGAL::convex_hull_3(points.begin(), points.end(), polyhedron); 
return 0; } The Boolean operations used for solid modeling are implemented using 3D Nef polyhedra in 
CGAL. Closed under Boolean operations implies that 1D antennas, 2D sheets, or isolated points on faces 
can be represented properly and are not automatically removed by a regularization step. Union operator 
shown in red.  The kernel using exact predicates and exact constructions allows robust operations, even 
when the input mesh (here the turbine) has badly shaped triangles. Another example implemented by the 
Moving Picture Company. Kernel Intersection of all its interior half-spaces.  Using linear programming 
  [Fischer et al.] input polyhedron kernel   The main interface consists of providing a boundary 
as a list of halfedges, so that the parameterization parameterizes the interior of the region bounded 
by this boundary. On the bottom example the specified boundary is a seam on the mesh (no boundary is 
present in the mesh). The mesh adaptor is required to model u,v coordinates either per vertex away from 
the seam lines, or per corner (designated by a halfedge). Bottom right shows two sets of halfedges with 
different u,v coordinates on both sides of the seam. The red lines show the cut graphs for both models. 
The default 32 bit implementation based on TAUCS sparse linear solver scales to 1M vertices. The component 
provides two types of conformal parameterizations: Either with a fixed user-specified boundary or with 
a free boundary (a minimum of two vertices must be constrained). The surface subdivision component provides 
all default subdivision techniques. Its generic implementation using stencils (bottom right) allows the 
user to quickly implement new subdivision schemes. A simple code example for Catmull-Clark subdivision. 
 Suppose you want to fit a plane to a triangle surface mesh in the least squares sense. The CGAL principal 
component analysis component allows fitting either 3D points or 3D triangles. In this example we iterate 
of the mesh triangle faces and fill a container of 3D triangles. The linear least squares fitting function 
is then called so as to fit the surface of the triangles, hence the dimension 2 tag. Internally we are 
using closed form formulas to compute the covariance matrix and no quadrature. An illustration of the 
Boost transform_iterator. Instead of generating a list of triangles, we iterate over the sequence of 
faces and generate the triangle on the fly in the dereference operation. This example shows the difference 
between fitting a set of points and fitting a set of triangles. Fitting the triangles allows us to be 
insensitive to the sampling variation of the mesh, a nice feature when performing geometry processing 
operations. Suppose one wants to estimate principal curvature directions on a polyhedron (example shown). 
The Estimation of Curvature CGAL component allows estimating general differential properties (the Monge 
form) on point sets through polynomial d-dimensional jet fitting. At each vertex of the mesh it is sufficient 
to gather a set of vertices in the vicinity and to call the jet fitting function as shown next. Example 
#include <CGAL/Monge_via_jet_fitting.h> typedef CGAL::Cartesian<double> Kernel; typedef CGAL::Monge_via_jet_fitting<Kernel> 
Monge_fit; typedef Monge_fit::Monge_form Monge_form; Monge_fit monge_fit; Monge_form monge_form = monge_fit( 
points.begin(), points.end(), dim_fitting, dim_monge); Vector_3 kmin = monge_form.minimal_principal_direction(); 
Vector_3 kmax = monge_form.maximal_principal_direction(); Vector_3 normal = monge_form.normal_direction(); 
 Extraction of Ridges Ridge: curve along which one of the principal curvatures has an extremum along 
its curvature line. C G A L m a n u al  The mesh simplification algorithm in CGAL closely follows the 
BGL design, so that it can also be used to simplify any mesh other than the CGAL Polyhedron. The current 
simplification is the volume-preservation technique from Lindstrom and Turk. Simplification: Example 
#include <CGAL/Simple_cartesian.h> #include <CGAL/Polyhedron_3.h> #include <CGAL/Surface_mesh_simplification/HalfedgeGraph_Polyhedron_3.h> 
#include <CGAL/Surface_mesh_simplification/edge_collapse.h> #include <CGAL/Surface_mesh_simplification/Policies/Edge_collapse/Count_stop_predicate.h> 
typedef CGAL::Simple_cartesian<double> Kernel; typedef CGAL::Polyhedron_3<Kernel> Mesh; namespace SMS 
= CGAL::Surface_mesh_simplification ; Mesh mesh; SMS::Count_stop_predicate< Mesh > stop(1000); // target 
# edges SMS::edge_collapse(mesh, stop, CGAL::vertex_index_map(boost::get(CGAL::vertex_external_index, 
mesh)) .edge_index_map(boost::get(CGAL::edge_external_index, mesh))); C G A L m a n u al O n l i n e 
d e m o _b o o s t : : p r o p e r t y m a p _ _b g l n a m e d p ar am s  Besides a call of the simplification 
function, this example shows two other features of Boost, namely property maps and named parameters. 
Property maps serve for mapping properties to vertices. These properties are either stored in the vertex 
objects, or in independent data structures. Named parameters allow to pass parameters in arbitrary order. 
This improves code readability, and at the same time allows to leave out default values in the middle 
of a long parameter list. In the sequel we will see how the polyhedral surface can be the output of 
CGAL surface mesh generation algorithms. Under development: an isotropic surface triangle remeshing algorithm, 
and more CGAL mesh processing algorithms adapted a la BGL so that they can be applied to other mesh data 
structures as well.  Arrangements Efi Fogel Tel Aviv University Outline Arrangements  Algorithms 
based on Arrangements  Boolean Set Operations  Minkowski Sums and Polygon Offset  Envelopes   Arrangements 
on Surfaces  Arrangement Definition Given a collection of curves on a surface, the arrangement is the 
partition of the surface into vertices, edges and faces induced by the curves An arrangementAn arrangement 
of An arrangementof circles in the lines in the plane of geodesic arcsplane on the sphere CGAL::Arrangement_2 
 Constructs, maintains, modifies, traverses, queries, andpresents subdivisions of the plane  Robust 
and exact  All inputs are handled correctly (including degenerate)  Exact number types are used to 
achieve exact results   Efficient  Generic  Easy to interface, extend, and adapt  Notification mechanism 
for change propagation   Modular  Geometric and topological aspects are separated  Code Example 
#include <CGAL/Exact_predicates_exact_constructions_kernel.h> #include <CGAL/Arr_segment_traits_2.h> 
#include <CGAL/Arrangement_2.h> typedef CGAL::Exact_predicates_exact_constructions_kernel Kernel; typedef 
CGAL::Arr_segment_traits_2<Kernel> typedef Traits::Point_2 typedef Traits::X_monotone_curve_2 typedef 
CGAL::Arrangement_2<Traits_2> Traits; Point; Segment; Arrangement; int main() { Point p1(0, 0), p2(1, 
0), p3(0, 1); Segment cv[3] = { Segment(p1,p2), Segment(p2,p3), Segment(p3,p1)}; Arrangement arr; p3 
CGAL::insert(arr, cv, cv+3); return 0; }  Geometric Traits Define the family of curves  Aggregate 
geometric types and operations over the types   Arrangement Traits Classes Curve Family Degree Surface 
Boundness Arithmetic Attribute linear segments 1 plane bounded rational cachingnoncaching linear segments, 
rays, lines 1 plane unbounded rational  piecewise linear curves 8 plane bounded rational cachingnoncaching 
 circular arcs, linear segments = 2 plane bounded rational CK algebraic curves = 2 plane Bounded unbounded 
algebraic CORE CKvA_2 quadric projections = 2 plane unbounded algebraic algebraic curves = 3 plane 
unbounded algebraic algebraic curves = n plane unbounded algebraic planar Bézier curves = n plane unbounded 
algebraic  univariate polynomials = n plane unbounded algebraic RS rational function arcs = n plane 
unbounded algebraic  geodesic arcs on sphere = 2 sphere bounded rational  quadric intersection arcs 
= 2 quadric unbounded algebraic  dupin cyclide intersection. arcs = 2 dupin cyclides bounded algebraic 
  Enrich Vertices, Edges, Faces template <typename Traits, typename Dcel = Arr_default_dcel<Traits_> 
> class Arrangement_2 { ... }; enum Color {BLUE, RED, WHITE}; typedef CGAL::Arr_segment_traits_2<Kernel> 
Traits; typedef Traits::Point_2 Point; typedef Traits::X_monotone_curve_2 Segment; typedef CGAL::Arr_extended_dcel<Traits, 
Color, Color, Color> Dcel; typedef CGAL::Arrangement_2<Traits, Dcel> Arrangement; Point Location Given 
a subdivision A of the space into cells and a query point q, find the cell of A containing q  Fast query 
processing  Reasonably fast preprocessing  Small space data structure  Naive Walk RIC Landmarks 
Preprocessing time none None O(n log n) O(k log k) Memory space none none O(n) O(k) Query time bad reasonable 
good good Code Development simple quite simple complicated quite simple Walk Walk along a line k number 
of landmarks RIC Random Incremental Construction * Can be reduced to O(n)based on trapezoidal decomposition 
Point Location typedef CGAL::Arr_naive_point_location<Arrangement_2> Naive_pl; typedef CGAL::Arr_landmarks_point_location<Arrangement_2> 
Landmarks_pl; int main() { Arrangement arr; construct_arr(arr); Point p(1, 4); Naive_pl naive_pl(arr); 
// Associate arrangement to naïve point location CGAL::Object obj1 = naive_pl.locate(p); Landmarks landmarks_pl; 
landmarks_pl.attach(arr); // Attach landmarks point location to arrangement CGAL::Object obj2 = landmarks_pl.locate(p); 
 return (obj1 == obj2); } Boolean Set Operations Union Intersection Complement Circular arcs Bezier 
Curves Line Segments Code Example int main ( ) { Polygon p, q; p.push_back(Point(0, 0)); p.push_back(Point(2, 
0)); p.push_back(Point(1, 1)); p.push_back(Point(0, 2)); q.push_back(Point(1, 1)); q.push_back(Point(3, 
1)); q.push_back(Point(2, 2)); q.push_back(Point(1, 3)); Polygon_with_holes comp_p, comp_q; CGAL::complement(p, 
comp_p); CGAL::complement(q, comp_q); Polygon_with_holes a; CGAL::join(comp_p, comp_q, a); std::list<Polygon_with_holes> 
l1, l2; CGAL::complement(a, std::back_inserter(l1)); CGAL::intersection(p, q, l2); return std::compare(l1, 
l2); }  CGAL::Boolean_set_operation_2  Supports  regularized Boolean set-operations  intersection 
predicates  point containment predicates   Operands and results are regularized point sets bounded 
by x-monotone curves referred to as general polygons  General polygons may have holes  Extremely efficient 
aggregated operations  Minkowski Sum in Rd and Q are two polytopes in Rd . Q ={p + q | p . P, q . Q} 
Minkowski sum nQ . Ø. Origin . P . (-Q) collision detection  CGAL::Minkowski_sum_2 Supports Minkowski 
sums of two simple polygons  Implemented using either decomposition or convolution  Exact computation 
  Interoperable with Boolean_set_operations_2, e.g., Compute the union of offset polygons  Polygon 
Offsets Minkowski sums of a simple polygon and a disc  CGAL offers inner and outer offset computation 
 Exact computation which needs square roots   Polygon Offsets Minkowski sums of a simple polygon and 
a disc  CGAL offers inner and outer offset computation  Conservative approximation scheme with rationals 
  Motion Planning The input robot and the obstacle are non-convex  Exploits the convolution method 
 The output sum contains four holes, isolated points,and antennas  Minkowski-Sums of Polytopes The 
Gaussian map of a polytope is the decomposition of S2 into maximal connected regions so that the extremal 
point is the same for all directions within one region The overlay of the Gaussian maps of two polytopes 
P and Q is the Gaussian map of the Minkowski sum of P and Q  Envelopes in R2 Given a set of x-monotone 
curves C = {C1,C2, . . . ,Cn}, their lower envelope is the point-wise minimum of all curves  Computer 
Aided Manufacturing  The lower envelope of a set of line segments and hyperbolic arcs obtained by the 
radial projection of a triangulated Utah teapot Envelopes in R3 Given a set of xy-monotone surfaces S 
={S1,S2, .. . ,Sn}, their lower envelope is the point-wise minimum of all surfaces The minimization diagram 
of S is a 2D Arrangement Envelopes in R3 Given a set of xy-monotone surfaces S ={S1,S2, .. . ,Sn}, their 
lower envelope is the point-wise minimum of all surfaces The minimization diagram of S is a 2D Arrangement 
 CGAL::Envelope_3 Constructs lower and upper envelopes of surfaces  Based on the Arrangement_2 package 
 Exploits  Overlay computation (using the sweep line framework)  Isolated points  Zone traversal 
  Surface Family Class Name triangles Env_triangle_traits_3 spheres Env_sphere_traits_3 planes and 
half planes Env_plane_traits_3 quadrics Env_quadric_traits_3  Voronoi Diagrams via Envelopes  Computed 
as upper envelopes of planes  Represented as planar arrangements of unbounded   points along a line 
segment points on a grid inside a square points on a circle points on a square boundary  Arrangements 
on Surfaces in R3 A parametric surface S of two parameters is a surface defined by parametric equations 
involving two parameters u and v: fS(u, v)= (x(u, v), y(u, v), z(u, v)) Thus, fs : P . R3 and S = fs(P), 
where P is a continuous and simply connected two-dimensional parameter space We deal with orientable 
parametric surfaces Voronoi Diagrams on the Sphere  All algorithms supported by the Arrangement_2 package 
can also be used on the sphere  We compute lower envelopes defined over the sphere  We can compute 
Voronoi diagrams on the sphere, the  Voronoi diagram Degenerate VoronoiPower (Laguerreon the sphere 
diagram on the sphere Voronoi) diagram onthe sphere Arrangements on the Sphere The overlay of  An arrangement 
on the sphere induced by  the continents and some of the islands on earth  5 cities  New Orleans 
 Los Angeles  San Antonio  San Diego  Boston    Voronoi diagram of the cities   Diagrams, envelopes, 
etc. are represented as arrangements  Can be passed as input to consecutive operations  Video Arrangements 
of Geodesic Arcs on the Sphere  Was presented at the 24th ACM Symposium on Computational Geometry, College 
Park, Maryland, July 2008  Summary Arrangements are a versatile tool  In CGAL arrangements are used 
as foundation for higher level geometric data structures  Arrangements are not bound to the plane  
   CGAL avoids discretization of curved arcs, which allows to handle large data sets.  A geometric 
traits class has to provide predicates: Compare two points  Determine the relative position of a point 
and an x­monotone curve  Determine the relative position of two x-monotone curves to the left (or right) 
of a point  Subdivide a curve into x-monotone curves  Find all intersections of two x-monotone curves 
 Supports  regularized Boolean set-operations  intersection predicates  point containment predicates 
  Operands and results are regularized point sets boundedby x-monotone curves referred to as general 
polygons General polygons may have holes  Based on the Arrangement_2 package  Extremely efficient 
aggregated operations, which use ablend of a divide and conquer and a plane sweepalgorithm      
   The minimization diagram of C is the subdivision of the x-axis into cells, such that the identity 
of the curves that induce the lower envelope over a specific cell of the subdivision (an edge or a vertex) 
is the same The minimization diagram of 8 line segments Each diagram vertex points to the point associated 
with it  Each diagram edge is marked with segments that induce it       Triangulations and Mesh 
Generation Andreas Fabri GeometryFactory Pierre Alliez INRIA Outline  2D  From triangulations to quality 
meshes  Related components   3D  Triangulations  Mesh generation  Key concepts  Surfaces  Volumes 
 Next release and work in progress    Summary 2D From Triangulations to Quality Meshes  Delaunay 
Triangulation A triangulation is a Delaunay triangulation, if the circumscribing circle of any facet 
of the triangulation contains no vertex in its interior  Code Example #include <CGAL/Exact_predicates_inexact_constructions_kernel.h> 
#include <CGAL/Delaunay_triangulation_2.h> typedef CGAL::Exact_predicates_inexact_constructions_kernel 
Kernel; typedef Kernel::Point_2 Point; typedef CGAL::Delaunay_triangulation_2<Kernel> Delaunay; typedef 
Delaunay::Vertex_handle Vertex_handle; int main() { Delaunay dt; dt.insert( std::istream_iterator<Point>(std::cin), 
std::istream_iterator<Point>() ); Vertex_handle v = dt.nearest_vertex(Point(0.0,0.0)); std::cout << 
Nearest vertex to origin: << v->point() << std::endl; return 0; } Adding Constraints  Code Example 
 #include <CGAL/Exact_predicates_inexact_constructions_kernel.h> #include <CGAL/Constrained_Delaunay_triangulation_2.h> 
 typedef CGAL::Exact_predicates_inexact_constructions_kernel Kernel; typedef Kernel::Point_2 Point; 
typedef CGAL::Constrained_Delaunay_triangulation_2<Kernel> CDT; typedef CDT::Vertex_handle Vertex_handle; 
 int main() { CDT cdt; // from points cdt.insert_constraint(Point(0.0,0.0), Point(1.0,0.0)); // from 
vertices Vertex_handle v1 = cdt.insert(Point(2.0,3.0)); Vertex_handle v2 = cdt.insert(Point(4.0,5.0)); 
 cdt.insert_constraint(v1,v2); return 0; } Conforming Delaunay  Code Example #include <CGAL/Triangulation_conformer_2.h> 
 // constrained Delaunay triangulation CDT cdt; // insert points &#38; constraints CGAL::make_conforming_Delaunay_2(cdt); 
Delaunay Meshing  Code Example #include <CGAL/Exact_predicates_inexact_constructions_kernel.h> #include 
<CGAL/Constrained_Delaunay_triangulation_2.h> #include <CGAL/Delaunay_mesher_2.h> typedef CGAL::Exact_predicates_inexact_constructions_kernel 
Kernel; typedef CGAL::Constrained_Delaunay_triangulation_2<Kernel> CDT; int main() { CDT cdt; // insert 
points and constraints CGAL::refine_Delaunay_mesh_2(cdt); return 0; } Background Delaunay Edge An edge 
is said to be a Delaunay edge, if it is inscribed in an empty circle Gabriel Edge An edge is said to 
be a Gabriel edge, if its diametral circle is empty Conforming Delaunay Triangulation A constrained 
Delaunay triangulation is a conforming Delaunay triangulation, if every constrained edge is a Delaunay 
edge non conforming conforming Conforming Gabriel Triangulation A constrained Delaunay triangulation 
is a conforming Gabriel triangulation, if every constrained edge is a Gabriel edge non conforming conforming 
Gabriel Steiner Vertices Any constrained Delaunay triangulation can be refined into a conforming Delaunay 
or Gabriel triangulation by adding Steiner vertices. non conforming conforming Gabriel Delaunay Refinement 
 Rule #1: break bad elements by inserting circumcenters (Voronoi vertices) bad in terms of size or shape 
(too big or skinny) Picture taken from [Shewchuk] Delaunay Refinement Rule #2: Midpoint vertex insertion 
A constrained segment is said to be encroached, if there is a vertex inside its diametral circle Picture 
taken from [Shewchuk] Delaunay Refinement Encroached subsegments have priority over skinny triangles 
 Picture taken from [Shewchuk] API Rich API Traversal  Rich API Traversal  Localization   Rich 
API  Traversal  Localization  Dynamic: insertion &#38; removal   Rich API  Traversal  Localization 
 Dynamic: insertion &#38; removal  Parameters for mesh generation  Parameters for Mesh Generation 
 Shape Lower bound on triangle angles  Input PLSG 5 deg 20.7 deg  Parameters for Mesh Generation  
Shape Lower bound on triangle angles  Size  No constraint  Uniform sizing  Sizing function  
  Sizing Parameter  No constraint Uniform Sizing function Example Code #include <CGAL/Exact_predicates_inexact_constructions_kernel.h> 
#include <CGAL/Constrained_Delaunay_triangulation_2.h> #include <CGAL/Delaunay_mesher_2.h> #include <CGAL/Delaunay_mesh_size_criteria_2.h> 
typedef CGAL::Exact_predicates_inexact_constructions_kernel Kernel; typedef CGAL::Constrained_Delaunay_triangulation_2<Kernel> 
CDT; typedef CGAL::Delaunay_mesh_size_criteria_2<CDT> Criteria; typedef CGAL::Delaunay_mesher_2<CDT, 
Criteria> Meshing_engine; int main() { CDT cdt; Meshing_engine engine(cdt); engine.refine_mesh(); engine.set_criteria(Criteria(0.125, 
0.5)); // min 20.6 degree, 0.5 for sizing engine.refine_mesh(); // refine once more, etc. return 0; 
} Parameters for Mesh Generation  Shape Lower bound on triangle angles  Size  No constraint  Uniform 
sizing  Sizing function   Seeds  Exclude/include components  Performances  Related Components 
 Voronoi diagram   Related Components Voronoi diagram  Elevation (through traits class)   Courtesy 
IPF,Vienna University of Technology &#38; Inpho GmbH Related Components Voronoi diagram  Elevation 
 Interpolation (natural neighbors)   Related Components  Voronoi diagram  Elevation  Interpolation 
 Placement of streamlines    3D  3D Triangulations  Delaunay  Regular  Rich API  Fully dynamic 
 1M points in 16s   T e t r a h e d r o n Mesh Generation Key concepts: Delaunay filtering  Delaunay 
refinement   Delaunay Filtering Dual Voronoi edge  Delaunay Refinement Steiner point Bad facet = 
big or badly shaped or large approximation error Surface Mesh Generation Algorithm repeat { pick bad 
facet f insert furthest (dual(f) n S) in Delaunay triangulation update Delaunay triangulation restricted 
to S } until all facets are good Isosurface from 3D Grey Level Image #include <CGAL/Surface_mesh_default_triangulation_3.h> 
#include <CGAL/Complex_2_in_triangulation_3.h> #include <CGAL/make_surface_mesh.h> #include <CGAL/Gray_level_image_3.h> 
#include <CGAL/Implicit_surface_3.h> typedef CGAL::Surface_mesh_default_triangulation_3 Tr; typedef CGAL::Complex_2_in_triangulation_3<Tr> 
C2t3; typedef CGAL::Implicit_surface_3<Kernel, Gray_level_image> Surface_3; Tr tr; // 3D-Delaunay triangulation 
C2t3 c2t3 (tr); Gray_level_image image("data/brain",128); Surface_3 surface(image, bounding_sphere, 1e-2); 
 // Criteria: min triangle angles, size, approximation error, CGAL::Surface_mesh_default_criteria_3<Tr> 
criteria(30.,5.,5.); CGAL::make_surface_mesh(c2t3, surface, criteria, CGAL::Manifold_tag());  Output 
Mesh  input  approximating S Output Mesh Properties Well shaped triangles Lower bound on triangle 
angles  Homeomorphic to input surface  Manifold not only combinatorially, i.e., no self-intersection 
 Faithful Approximation of input surface  Hausdorff distance  Normals    Delaunay Refinement vs 
Marching Cubes Delaunay refinement Marching cubes in octree Parameters  Shape of triangles lower 
bound on triangle angles  Size  No constraint  Uniform sizing  Sizing function   Approximation 
error  Mesh Generation Framework   3D image Surface mesh Implicit function  A Versatile Framework 
 3D grey level images  3D multi-domain images  Implicit function: f(x, y, z) = constant  Surface 
mesh  Point set (surface reconstruction)  Next Release Volume Mesh Generation  More Delaunay Filtering 
 tetrahedron circumsphere Dual Voronoi vertex inside domain O ( oracle )  Delaunay Refinement Steiner 
point Bad tetrahedron = big or badly shaped Volume Mesh Generation Algorithm  repeat { pick bad simplex 
if(Steiner point encroaches a facet) refine facet else refine simplex update Delaunay triangulation restricted 
to domain } until all simplices are good Exude slivers Tetrahedron Zoo  sliver Sliver Exudation [Edelsbrunner-Guoy] 
 Delaunay triangulation turned into a regular triangulation with null weights.  Small increase of weights 
triggers edge­facets flips to remove slivers.  Sliver Exudation Process  Try improving all tetrahedra 
with an aspect ratio lower than a given bound  Never flips a boundary facet  distribution of aspect 
ratios 3D Mesh from Multi-Domain Images Tr tr; // 3D Delaunay triangulation C2t3 c2t3(tr); // 2D complex 
in 3D-Delaunay triangulation Image_3 image( segmented_visible_human"); Mesh_traits mesh_traits(image); 
Facets_criteria facets_criteria(5, 1); // facet sizing / approximation error Tets_criteria tets_criteria(5); 
// tet sizing // 0.5 = radius-radius ratio upper bound for the sliver exudation process CGAL::make_mesh_3_for_multivolumes(c2t3, 
mesh_traits, facets_criteria, tets_criteria, 0.5); Output Volume Mesh Visible human Work in Progress 
 Piecewise Smooth Surfaces  Input: Piecewise smooth complex crease corner More Delaunay Filtering 
 primitive Voronoi vertex dual of tetrahedron test inside against domain Voronoi edge Voronoi face facet 
edge intersect intersect domain boundary crease  Delaunay Refinement Steiner points  Summary Summary 
 From triangulation to quality meshes  Mesh generation:  2D: Preserves constraints exactly.  Summary 
 From triangulation to quality meshes  Mesh generation:  2D: Preserves constraints exactly.  3D: 
Interpolates boundary and sharp   creases. Summary  From triangulation to quality meshes  Mesh 
generation:  2D: Preserves constraints exactly.  3D:  Interpolates boundary and sharp creases.  Versatile 
through oracle-based design    See Also DelPSC software Skin surfaces (based on CGAL)  [Dey-Levine] 
   Outline  2D  From triangulations to quality meshes  Related components   3D  Triangulations 
 Mesh generation  Key concepts  Surfaces  Volumes  Next release and work in progress   Summary 
 2D Cliquez pour ajouter un texte Delaunay Triangulation A triangulation is a Delaunay triangulation, 
if the circumscribing circle of any facet of the triangulation contains no vertex in its interior  
 Code Example #include <CGAL/Exact_predicates_inexact_constructions_kernel.h> #include <CGAL/Constrained_Delaunay_triangulation_2.h> 
 typedef CGAL::Exact_predicates_inexact_constructions_kernel Kernel; typedef Kernel::Point_2 Point; 
typedef CGAL::Constrained_Delaunay_triangulation_2<Kernel> CDT; typedef CDT::Vertex_handle Vertex_handle; 
 int main() { CDT cdt; // from points cdt.insert_constraint(Point(0.0,0.0), Point(1.0,0.0)); // from 
vertices Vertex_handle v1 = cdt.insert(Point(2.0,3.0)); Vertex_handle v2 = cdt.insert(Point(4.0,5.0)); 
 cdt.insert_constraint(v1,v2); return 0; }  Consider an interactive application scenario where a user 
adds constraints to a fine triangulation. This creates badly shaped triangles. Delaunay conforming improves 
this by adding Steiner vertices on the input constrained edges. Code Example #include <CGAL/Triangulation_conformer_2.h> 
// constrained Delaunay triangulation CDT cdt; // insert points &#38; constraints CGAL::make_conforming_Delaunay_2(cdt); 
The function make_conforming_Delaunay_2 turns a constrained Delaunay triangulation into a Delaunay conforming 
one. Delaunay meshing adds Steiner points both on the constrained edges and in the domain in order to 
generate well-shape triangles. Code Example #include <CGAL/Exact_predicates_inexact_constructions_kernel.h> 
#include <CGAL/Constrained_Delaunay_triangulation_2.h> #include <CGAL/Delaunay_mesher_2.h> typedef CGAL::Exact_predicates_inexact_constructions_kernel 
Kernel; typedef CGAL::Constrained_Delaunay_triangulation_2<Kernel> CDT; int main() { CDT cdt; // insert 
points and constraints CGAL::refine_Delaunay_mesh_2(cdt); return 0; } Background Let s go back to the 
key concepts behind the algorithms we have just listed.  Conforming Delaunay Triangulation A constrained 
Delaunay triangulation is a conforming Delaunay triangulation, if every constrained edge is a Delaunay 
edge non conforming conforming  Delaunay Refinement Rule #2: Midpoint vertex insertion A constrained 
segment is said to be encroached, if there is a vertex inside its diametral circle Picture taken from 
[Shewchuk] Delaunay Refinement Encroached subsegments have priority over skinny triangles Picture taken 
from [Shewchuk] API Cliquez pour ajouter un texte  Rich API Traversal  Localization  Dynamic: insertion 
&#38; removal   Rich API Traversal  Localization  Dynamic: insertion &#38; removal  Parameters 
for mesh generation  Parameters for Mesh Generation Shape Lower bound on triangle angles  Input PLSG 
5 deg 20.7 deg Parameters for Mesh Generation  Shape Lower bound on triangle angles  Size  No constraint 
 Uniform sizing  Sizing function    Example Code #include <CGAL/Exact_predicates_inexact_constructions_kernel.h> 
#include <CGAL/Constrained_Delaunay_triangulation_2.h> #include <CGAL/Delaunay_mesher_2.h> #include <CGAL/Delaunay_mesh_size_criteria_2.h> 
typedef CGAL::Exact_predicates_inexact_constructions_kernel Kernel; typedef CGAL::Constrained_Delaunay_triangulation_2<Kernel> 
CDT; typedef CGAL::Delaunay_mesh_size_criteria_2<CDT> Criteria; typedef CGAL::Delaunay_mesher_2<CDT, 
Criteria> Meshing_engine; int main() { CDT cdt; Meshing_engine engine(cdt); engine.refine_mesh(); engine.set_criteria(Criteria(0.125, 
0.5)); // min 20.6 degree, 0.5 for sizing engine.refine_mesh(); // refine once more, etc. return 0; 
} Parameters for Mesh Generation  Shape Lower bound on triangle angles  Size  No constraint  Uniform 
sizing  Sizing function   Seeds  Exclude/include components  Performances   Related Components 
 Voronoi diagram  Elevation  Interpolation (natural neighbors)   3D 3D Triangulations  Delaunay 
 Regular  Rich API  Fully dynamic  1M points in 16s   T e t r a h e d r o n Mesh Generation Key 
concepts: Delaunay filtering  Delaunay refinement  Delaunay Filtering Dual Voronoi edge Delaunay 
Refinement Steiner point Bad facet = big or badly shaped or large approximation error Surface Mesh 
Generation Algorithm repeat { pick bad facet f insert furthest (dual(f) n S) in Delaunay triangulation 
update Delaunay triangulation restricted to S } until all facets are good Isosurface from 3D Grey Level 
Image #include <CGAL/Surface_mesh_default_triangulation_3.h> #include <CGAL/Complex_2_in_triangulation_3.h> 
#include <CGAL/make_surface_mesh.h> #include <CGAL/Gray_level_image_3.h> #include <CGAL/Implicit_surface_3.h> 
typedef CGAL::Surface_mesh_default_triangulation_3 Tr; typedef CGAL::Complex_2_in_triangulation_3<Tr> 
C2t3; typedef CGAL::Implicit_surface_3<Kernel, Gray_level_image> Surface_3; Tr tr; // 3D-Delaunay triangulation 
C2t3 c2t3 (tr); Gray_level_image image("data/brain",128); Surface_3 surface(image, bounding_sphere, 1e-2); 
 // Criteria: min triangle angles, size, approximation error, CGAL::Surface_mesh_default_criteria_3<Tr> 
criteria(30.,5.,5.); CGAL::make_surface_mesh(c2t3, surface, criteria, CGAL::Manifold_tag());  Output 
Mesh Properties  Well shaped triangles Lower bound on triangle angles  Homeomorphic to input surface 
 Manifold not only combinatorially, i.e., no self-intersection  Faithful Approximation of input surface 
 Hausdorff distance  Normals    Parameters Shape of triangles lower bound on triangle angles 
 Size  No constraint  Uniform sizing  Sizing function   Approximation error   A Versatile Framework 
 3D grey level images  3D multi-domain images  Implicit function: f(x, y, z) = constant  Surface mesh 
 Point set (surface reconstruction)   Next Release Cliquez pour ajouter un texte More Delaunay Filtering 
 tetrahedron circumsphere Dual Voronoi vertex inside domain O ( oracle ) Volume Mesh Generation Algorithm 
 repeat { pick bad simplex if(Steiner point encroaches a facet) refine facet else refine simplex update 
Delaunay triangulation restricted to domain } until all simplices are good Exude slivers Sliver Exudation 
[Edelsbrunner-Guoy] Delaunay triangulation turned into a regular triangulation with null weights.  
Small increase of weights triggers edge­facets flips to remove slivers.  Sliver exudation is a post-process 
to optimize the shape of tetrahedra. Sliver Exudation Process Try improving all tetrahedra with an 
aspect ratio lower than a given bound  Never flips a boundary facet  distribution of aspect ratios 
 In multi-domain 3D image, each voxel contains the index of a domain. The 3D mesh produced by the algorithm 
is a 3D triangulation, where all tetrahedra are contained in only one domain. For that purpose, the interface 
surfaces between domains are finelly meshed.  Work in Progress Cliquez pour ajouter un texte  More 
Delaunay Filtering primitive dual of test against Voronoi vertex tetrahedron inside domain Voronoi edge 
facet intersect domain boundary Voronoi face edge intersect crease   Summary Cliquez pour ajouter 
un texte  Summary From triangulation to quality meshes  Mesh generation:  2D: Preserves constraints 
exactly.  3D:  Interpolates boundary and sharp creases.  Versatile through oracle-based design  
  DelPSC-related Papers - Delaunay refinement for piecewise smooth complexes. S.-W. Cheng, T. K. Dey, 
and E. A. Ramos. Proc. 18th Annu. ACM-SIAM Sympos. Discrete Algorithms (2007), 1096--1105.  -A practical 
Delaunay meshing algorithm for a large class of domains. S.-W. Cheng, T. K. Dey, and J. A. Levine. Proc. 
16th Internat. Meshing Roundtable (2007), 477--494. -Reducing expensive computations in Delaunay meshing 
of piecewise smooth complexes. T. K. Dey and J. Levine, manuscript. 2008.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401161</section_id>
		<sort_key>290</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Professional Development and Education: Computational photography: advanced topics]]></section_title>
		<section_page_from>7</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098664</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098665</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098666</person_id>
				<author_profile_id><![CDATA[81100216430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tumblin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401162</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>198</pages>
		<display_no>23</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Computational photography]]></title>
		<subtitle><![CDATA[advanced topics]]></subtitle>
		<page_from>1</page_from>
		<page_to>198</page_to>
		<doi_number>10.1145/1401132.1401162</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401162</url>
		<abstract>
			<par><![CDATA[<p>Computational photography combines plentiful computing, digital sensors, modern optics, many varieties of actuators, probes and smart lights to escape the limitations of traditional film cameras and enables novel imaging applications. Unbounded dynamic range, variable focus, resolution, and depth of field, hints about shape, reflectance, and lighting, and new interactive forms of photos that are partly snapshots and partly videos, performance capture and interchangeably relighting real and virtual characters are just some of the new applications emerging in Computational Photography. The computational techniques encompass methods from modification of imaging parameters during capture to sophisticated reconstructions from indirect measurements.</p> <p>We will bypass basic and introductory material presented in earlier versions of this course (Computational Photography 2005, 6, 7) and expand coverage of more recent topics. Emphasizing more recent work in computational photography and related fields (2006 or later) this course will give more attention to advanced topics only briefly touched before, including tomography, heterodyning and Fourier Slice applications, inverse problems, gradient illumination, novel optics, emerging sensors and social impact of computational photography. With this deeper coverage, the course offers a diverse but practical guide to topics in image capture and manipulation methods for generating compelling pictures for computer graphics and for extracting scene properties for computer vision, with several examples.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098667</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098668</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098669</person_id>
				<author_profile_id><![CDATA[81100216430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tumblin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Northwestern University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2008 Course: Computational Photography: Advanced Topics http://computationalphotography.org 
Speakers Paul Debevec (USC, USA) (debevec (at) ict.usc.edu)  Ramesh RASKAR (MIT Media Lab, USA) (raskar 
(at) media.mit.edu)  Jack TUMBLIN (Northwestern University, USA) (jet (at) cs.northwestern.edu)  Course 
Abstract Computational photography combines plentiful computing, digital sensors, modern optics, many 
varieties of actuators, probes and smart lights to escape the limitations of traditional film cameras 
and enables novel imaging applications. Unbounded dynamic range, variable focus, resolution, and depth 
of field, hints about shape, reflectance, and lighting, and new interactive forms of photos that are 
partly snapshots and partly videos, performance capture and interchangeably relighting real and virtual 
characters are just some of the new applications emerging in Computational Photography. The computational 
techniques encompass methods from modification of imaging parameters during capture to sophisticated 
reconstructions from indirect measurements. We will bypass basic and introductory material presented 
in earlier versions of this course (Computational Photography 2005,6,7) and expand coverage of more recent 
topics. Emphasizing more recent work in computational photography and related fields (2006 or later) 
this course will give more attention to advanced topics only briefly touched before, including tomography, 
heterodyning and Fourier Slice applications, inverse problems, gradient illumination, novel optics, emerging 
sensors and social impact of computational photography. With this deeper coverage, the course offers 
a diverse but practical guide to topics in image capture and manipulation methods for generating compelling 
pictures for computer graphics and for extracting scene properties for computer vision, with several 
examples. Speaker Info Paul Debevec Research Associate Professor, USC Paul Debevec is a research associate 
professor at the University of Southern California and the associate director of graphics research at 
USC's Institute for Creative Technologies. Debevec's Ph.D. thesis (UC Berkeley, 1996) presented Façade, 
an image-based modeling and rendering system for creating photoreal architectural models from photographs. 
Using Facade he led the creation of virtual cinematography of the Berkeley campus for his 1997 film The 
Campanile Movie whose techniques were used to create virtual backgrounds in the 1999 film The Matrix. 
Subsequently, Debevec developed techniques for illuminating computer-generated scenes with real-world 
lighting captured through high dynamic range photography, demonstrating new image-based lighting techniques 
in his films Rendering with Natural Light (1998), Fiat Lux (1999), and The Parthenon (2004); he also 
led the design of HDR Shop, the first high dynamic range image editing program. At USC ICT, Debevec has 
led the development of a series of Light Stage devices for capturing and simulating how objects and people 
reflect light, recently used to create realistic digital actors in films such as Spider Man 2 and Superman 
Returns. He is the recipient of ACM SIGGRAPH's first Significant New Researcher Award and a co-author 
of the 2005 book High Dynamic Range Imaging from Morgan Kaufmann. Ramesh Raskar Associated Professor, 
Media Lab, MIT Ramesh Raskar joined the Media Lab in spring 2008 as head of the Camera Culture research 
group. He was a a Senior Research Scientist at MERL. The group focuses on developing tools to help us 
capture and share the visual experience. This research involves developing novel cameras with unusual 
optical elements, programmable illumination, digital wavelength control, and femtosecond analysis of 
light transport, as well as tools to decompose pixels into perceptually meaningful components. He is 
a member of the ACM and IEEE. Jack Tumblin Associate Professor, EECS Dept. Northwestern University Jack 
Tumblin is an Associate Professor of Computer Science at Northwestern University. His interests include 
novel photographic sensors and lighting devices to assist museum curators in historical preservation, 
computer graphics and visual appearance, and image-based modeling and rendering. During his doctoral 
studies at Georgia Tech and post-doc at Cornell, he investigated tone-mapping methods to depict high-contrast 
scenes. His MS in Electrical Engineering (December 1990) and BSEE (1978), also from Georgia Tech, bracketed 
his work as co-founder of IVEX Corp., (>45 people as of 1990) where he designed flight simulators. He 
was co-organizer of Computational Photography courses at Siggraph 2005 and 2006. He was an Associate 
Editor of ACM Transactions on Graphics 2001-2006, and holds 9 patents. Schedule Module 1: 90 minutes 
9:00: A.1 Introduction and Overview (Raskar, 15 minutes) 9:15: A.2 Concepts in Computational Photography 
(Tumblin, 15 minutes) 9:30: A.3 Optics: Computable Extensions (Raskar, 30 minutes) 10:00: A.4 Sensor 
Innovations (Tumblin, 30 minutes) 10:30: Q &#38; A  (5 minutes) 10:35: Break: 25 minutes Module 2: 90 
minutes 11:00: B.1 Illumination As Computing (Debevec, 25 minutes) 11:25: B.2 Scene and Performance Capture 
(Debevec, 20 minutes) 11:45: B.3 Image Aggregation &#38; Sensible Extensions (Tumblin, 20 minutes) 12:05: 
B.4 Community and Social Impact (Raskar, 20 minutes) 12:25: B.4 Summary and Discussion, Q&#38;A (All, 
10 minutes)   Course: Computational Photography Advanced Topics  Course Page : http://computationalphotography.org/ 
 Course 1: Computational Photography  Class: Computational Photography Organisers Ramesh Raskar MIT 
 Media Lab Jack Tumblin Northwestern University What is Photography? PHYSICAL PERCEIVED Expos ure 
 Light &#38; Contro l, Optics tone map  Photo: A Tangible Record Editable, storable as Film or Pixels 
   Ultimate Photographic Goals  PERCEIVED PHYSICAL or UNDERSTOOD   Patented 1903 Array of pinholes 
near image plane   Devices for recording light fields (using geometrical optics) big baseline  small 
baseline   handheld camera  camera gantry  array of cameras   [Buehler 2001] [Stanford 2002] 
[Wilburn 2005] [Ng 2005] [Levoy 2006]   Digital Refocusing using Light Field Camera  High performance 
imaging using large camera arrays Bennett Wilburn, Neel Joshi, Vaibhav Vaish, Eino-Ville Talvala, Emilio 
Antunez, Adam Barth, Andrew Adams, Mark Horowitz, Marc Levoy (Proc. SIGGRAPH 2005)  Coding and Modulation 
in Camera Using Masks  Heterodyne Light Full Resolution Field Camera Digital Refocusing Captured Blurred 
Photo  Refocused on Person  Compound Lens of Dragonfly    Wavefront Coding using Cubic Phase Plate 
  "Wavefront Coding: jointly optimized optical and digital imaging systems , E. Dowski, R. H. Cormack 
and S. D. Sarama , Aerosense Conference, April 25, 2000 Depth Invariant Blur Conventional System Wavefront 
Coded System  The Eye s Lens  Varioptic Liquid Lens: Electrowetting  Varioptic Liquid Lens  (Courtesy 
Varioptic Inc.)  Origami Lens : Thin Folded Optics (2007)  Ultrathin Cameras Using Annular Folded 
Optics, E. J. Tremblay, R. A. Stack, R. L. Morrison, J. E. Ford Applied Optics, 2007 -OSA Origami Lens 
 Conventional Origami Lens Lens Optical Performance Conventional Scene Origami Origami Conventional 
Lens Image Lens Image Single Pixel Camera  Single Pixel Camera  Example Original Compressed Imaging 
  4096 Pixels 65536 Pixels 1600 Measurements 6600 Measurements (40%) (10%)   Edgerton 1930 s  Stroboscope 
Multi-flash (Electronic Flash) Sequential Photography Time Flash Shutter Open   Diffuse optical tomography 
 female breast with absorption scattering sources (red) and (yellow is high) (yellow is high) detectors 
(blue) assumes light propagation by multiple scattering  model as diffusion process  inversion is 
non - linear and ill - posed  solve using optimization with regularization (smoothing)   Optical 
Projection Tomography (OPT)  [Trifonov 2006] [Sharpe 2002] Coded aperture imaging  (from Zand) optics 
cannot bend X - rays, so they cannot be focused  pinhole imaging needs no optics, but collects too little 
light  use multiple pinholes and a single sensor  produces uperimposed shifted opies f   Example 
using 2D images (Paul Carlisle)  * =   Smarter Lighting Equipment   Image - Based Actual Re 
- lighting  Measure incoming light, Matched LA and Milan lighting.    Dots Removed Acquired Image 
 (with Francesc Moreno and Peter Belhumeur 07) Fast Multispectral Imaging  A.2 Concepts in Computational 
Photography (Tumblin, 15 minutes) The Photographic Signal  What is the ideal photograph?  Ray-based 
versus pixel-based concepts  Understanding dimensionality ofrays outside and inside the camera  Course: 
Computational Photography Advanced Topics  Course Page : http://computationalphotography.org/ Focus, 
Click, Print: Film - Like Photogr aphy Angle( . , . ) Position( x , y) 2D Imag e: Instantaneous Intensity 
Map Light + 3D Scene: Illumination, shape, movement, surface BRDF, Center of Projection (P 3 or P 2 
Origin) Rays Rays  We still hang on to the mistaken notion that we re copying the image formed by the 
lens to the image formed by the display, an intrinsically 2D process to approximate the appearance of 
a 3D scene. We ve confused the PROCESS of photography with its PURPOSE and GOALS. At first, it was a 
wonder we could do it at all: Now it s a wonder how easily we take (bad) photos, how many choices and 
adjustments we can make to our cameras to make them better, but even more importantly, how many OTHER 
CHOICES we have besides a lens and a box holding a sensitized plate. We have many other choices for image 
formation (tomography, coded image methods, structured lighting, coded aperture, etc. etc.) for lighting 
(projectors, movable sources, multispectral sources, tuneable lasers, flash, strobe, reflectors, Schlieren 
retro-reflectors), and for display (interactive devices; light-sensitive displays, HDR, etc.) . Yet look 
at how much of high-quality photography is dominated by overcoming device limitations, artful choices 
of lighting, and adjusting the myriad settings our cameras and digital darkrooms offer to us.  Perfect 
Copy : Perfect Photograph? Scene Light Intensities  scene   Pixel values  (sc e n e inten s it y 
? dis p la y intens it y ? Display Light Intensities perc e ived inten s it y ? blac k ness / w h iteness 
? )  displa y Digital Photography is almost entirely a matter of copying---just like film! The underlying 
assumption is that we copy a 2D scene to a 2D display, and if we do it accurately, we re done.  Film 
- Like Photography Ideals, Design Goals:  Instantaneous light meas urement  Of focal plane image 
behind a lens.  Reproduce those amounts of light.  Implied:  What we see is . focal - plane intensities. 
 well, no we see much mor e ! (seeing is deeply cognitive)  A common misconception: Our Definitions 
  Film - like Photography: Displayed image . sensor imagesensor image Computational Photography: Displayed 
image . sensor image . visually meaningful scene contents A more expressive &#38; contro llable display 
e d result, med, merged, decoded data fromtransfor med, merged, decoded data fromcompute - assisted sensors, 
lights, optics, displays  What is Photography? Safe ans w er: A wholly new, expr essive medium (ca. 
1830s)  Manipulated display of what Manipulated display of what we think, feel, want,  Capture a memory, 
a visual ex perience in tangible form  painting with light ; ex press the subj ect s visual essence 
  Exactitude is not the truth.  Henri Matisse  It s Um, er. This isn t What is Photography? Display 
RGB(x,y,t n ) Image I(x,y, . ,t) Light &#38; Optics3D Scene light sources, BRDFs, shapes, positions, 
movements, Eyepoint position, movement, projection, PHYSICAL PERCEIVED Exposure Co ntrol, tone map 
Scene light sources, BRDFs, shapes, positions, movements, Eyepoint position, movement, projection, 
Vision Photo: A Tangible Record Editable, storable as Film or Pixels  Humans see basic, partial information 
about boundaries, shape, occlusion, lighting, shadows and texture, with few discernible difficulties 
with high dynamic range, resolution, or noise, lighting, or exposure. This basic data is usually difficult 
or impossible to reliably extract from pixels. But why require extraction? Instead, we should encode 
this information as part of the image itself. Towards this goal, Bixels offer a straightforward way to 
represent intensity and gradient discontinuities within images with subpixel precision, at a fixed cost 
an additional 8 bits per pixel. BLACKEST OF BLACK BOXES Ultimate Photographic Goals 3D Scene? light 
sources, BRDFs, shapes, positions, movements, Eyepoint ? position, movement, projection, Meaning Visual 
Stimul us 3D Scene light sources, BRDFs, shapes, positions, movements, Eyepoint position, movement, 
projection, PHYSICAL PERCEIVED or UNDERSTOOD Vision Sensor(sSensor(s) Computing Light &#38; Optics Photo: 
A Tangible Record Scene estimates w e can capture, edit, store, display  What we would like is something 
that more directly describes the visual experience, --something that, with some computing, would allow 
a computer-equipped display to construct a display image, one that, based on the viewing conditions, 
has the best chance of evoking the desired perceptions of the original scene.  Core i d eas ar e anci 
e nt, simple, seem obvious:  Lighting: ray sources  Optics: ray bending/folding devices  Sensor: 
measur e light  Processing: assess it  Displa y : reproduce it   Ancient Greeks: ey e rays wipe 
the wor l d to feel its contents h t tp :/ / www. ml ah anas.de / Greeks / Op tics. ht m  GREEKS: Photog. 
SEEMS obvious because what we gather can be described by ray geometry if we think of our retina as a 
sensory organ, we WIPE it across the scene, as if light let our retina reach out and touch what is around 
us. So let s look further into that:; lets consider light as a way of exploring our surroundings without 
contact, a magical way of transporting the the perceivable properties of our surroundings into our brain. 
EVEN THE GREEKS knew this idea well they used RAYS in exploration of vision, and described how rays going 
through a small aperture mapped angle to position The Photographic Signal Path Claim: C o mputing can 
impr ove ev er y step Light Sources Sensors Data Types, Processing Display Rays Optics Optics Scene Rays 
Ey es  We tend to think of photography as capturing light, not visual impressions. BUT VISUAL IMPRESSIONS 
DEPEND ON EVERY STAGE OF The Photographic Signal Path If we REPLACE 2D PIXELS WITH NOTIONS OF MEANINGFUL 
CHANGES IN SETS OF RAYS, then .. remember LIGHT IS LINEAR Review: How many Rays in a 3 - D Scene? A 
4 - D set of infinitesimal members. (Le v o y et al. SI GG 96) ( Gortler et al. 96) Imagine:  Convex 
Enclosur e of a 3D scene  Inwar d - facing ray camera at every surface point  Pick the rays you need 
fo r ANY camer a out s ide.  2D surface of cameras, + 2D ray set for each camera,  4 - D Light 
Field / Lumigraph Measur e all the outgoing light rays.    4D x 4D = 8 - D Reflectance Field Ratio: 
R ij = (outgoi ng ray i ) / (i ncomi n g ra y j ) Because Ray Changes Convey Appearance  These r a 
ys + all these r a ys give m e   MANY more useful details I can examine    Missing: Viewpoint Freedom 
 MultipleMultiple- Ce nt er - of - Projec tio n I m a ges Rad e m a ch er , P , Bishop, G.,Bishop, 
G., SIGGRAPHSIGGRAPH '98'98  Occlusion often hides visually important features that help us understand 
what we see. Missing: Interaction Adjust everything: lighting, pose, viewpoint, focus, FOV, Winnemoller 
EG 2005: after Malzbender, SIGG2001 Mild Viewing &#38; Lighting Changes; (is true 3D shape necessary?) 
 Convicing visual appearance: Is Accurate Depth really necessary? a few good 2 - D images may be enough 
  Image jets , Le v e l Sets , and Silhouette s  Lance Williams, talk at Stanford, 1998. THERE ARE 
AT LEAST 4 blocks that we can generalize and improve: lighting, optics, sensors, processing, (display: 
light sensitive display) The Ideal Photographic Signal I CLAIM IT IS: All Ra y s ? Some Ra y s ? Changes 
in Some Ra ys Photographic ra y s p ace is vast and redundant >8 dimensions: 4D vie w , 4D light, time, 
. , ? Gather only vis u ally signif icant ray changes ? ? What ra y s should we measure ? ? How should 
we combine them ? ? How should we display them ?  Rays are an infinitesimal discrete, computed abstraction 
they match what we perceive (an infinitely sharp world of disjoint objects), and they also escape a great 
deal inconvenient physics that entangles photography in practical difficulties They ignore rarely-perceived 
effects (diffraction, noise, fluorescence) that are computationally MUCH more difficult. ASIDE: Rays 
largely abandoned in modern optics &#38; lens design replaced by `Fourier Optics methods that properly 
account for diffraction effects, coherent (laser) light and nearly all wave propagation effects (see 
the classic textbook by Goodman, 1968). WHY USE Rays? They are ENOUGH Up until the time of machine-assisted 
image making, none of these efx of physics were a problem human perception guided image making instead. 
 Beyond Film - Like Photography Call it Computational Photograph y : To ma ke meaningful r a y changes 
tangible,  Optics can do more  Sensors can do more  Light Sources can do more  Processing can 
do more  can do more b y appl y i ng low - cost sto r age, computation, and control. Illumination as 
Computing with appl ications t o  Scene &#38; Performance Capture   Paul Debevec i University of 
S o uthe rn C a lifornia I n stitute for Creativ e Technologies G r a p h ic s Lab o ra t o ry  SIGGR 
A PH 2 0 08 Class o n Comput atio nal Ph ot ogra phy Los Angeles, Aug u st 20 0 8 www.de bevec.org / 
gl.ict .usc.edu  In this presentation I ll be speaking about some techniques that use Computational 
Photography to measure aspects of the lighting and reflectance of real scenes. There s been a lot of 
recent work in this area, and I ll only have a chance to give an overview of some of the projects, but 
hopefully what I have to say will give a reasonably clear path through a significant variety of material 
which will serve as a good primer to explore this area further. The most traditional 3D scanners use 
a laser stripe which scans over the object. That s why we traditionally 3D scene capture as scanning, 
even if nothing actually scans across the scene. The laser hits the object and is imaged back onto a 
sensor, forming a triangle. The optics of the sensor are calibrated so that triangulation allows an entire 
line of scene points to be constructed in 3D. The sensors (such as this one custom-made by Cyberware) 
are usually designed so that the laser peak is detected for each pixel column in hardware, so that the 
images do not need to be processed for each laser stripe position. Without such peak detection in hardware, 
this isn t very practical since you have to take a whole image every time the laser moves. What if you 
would prefer to build your own scanner with just a video projector and a video camera? It turns out 
this isn t that difficult, and you don t even need to take all that many pictures! Many computational 
illumination techniques make use of video projectors to emit various types of coded illumination. A classic 
application of coded illumination is for 3D scanning using structured light patterns. Now, as we all 
know scenes don t just consist of geometry, they also consist of reflectance properties and illumination. 
Gray code patterns  Binary (on/off) pattern  Unique f o r eve r y c o lumn Single Pixel: Column 5 
 Chr i s T chou. Image-Based Models: Geo m etry and Reflectance Acquisiti o n Systems . Master' s Th 
esi s , U n ive r sit y of Calif orn i a at Berkeley, Dec e mber 2002.  Gray code patterns  Binary 
(on/off) pattern  Unique f o r eve r y c o lumn  Project inverse patte r ns to neglec t indirec t 
illumination  Chr i s T chou. Image-Based Models: Geo m etry and Reflectance Acquisiti o n Systems 
. Master' s Th esi s , U n ive r sit y of Calif orn i a at Berkeley, Dec e mber 2002.  Gray code patterns 
 Binary (on/off) pattern  Unique f o r eve r y c o lumn  Project inverse patte r ns to neglec t indirec 
t illumination  Chr i s T chou. Image-Based Models: Geo m etry and Reflectance Acquisiti o n Systems 
. Master' s Th esi s , U n ive r sit y of Calif orn i a at Berkeley, Dec e mber 2002.  Gray code patterns 
 Binary (on/off) pattern  Unique f o r eve r y c o lumn  Project inverse patte r ns to neglec t indirec 
t illumination  Robust to blur Chr i s T chou. Image-Based Models: Geo m etry and Reflectance Acquisiti 
o n Systems . Master' s Th esi s , U n ive r sit y of Calif orn i a at Berkeley, Dec e mber 2002.  
Finding out which camera pixels correspond to which projector pixels produces a correspondence map, which 
can be turned into a 3D point cloud or geometric mesh using triangulation. Unfortunately, the geometry 
can appear aliased to the discretization of pixel coordinates. Much smoother geometry can be obtained 
by slightly blurring the projector and analyzing the grey levels at pixel boundaries, as described in 
Chris Tchou s Master s thesis. Depth fr o m project o r de focu s: Moreno-Noguer, Belh u m eur, and 
Nayar. Active Refo cusing of Image s and Videos . SIGGRAPH 20 07.  far medium near pattern with dots 
depth at dots removed dots  Here s a computational illumination technique for obtaining depth using 
just one video projector pattern. This SIGGRAPH 2007 paper from EPFL and Columbia aligns a video projector 
and a video camera using a beam splitter. They then project a grey pattern into the scene with a grid 
of white dots. The projector is focused behind everything, so the dots are the sharpest (and smallest) 
when they hit further away objects and larger and appear as larger out-of-focus circles when they hit 
nearer objects. This gives a depth estimate at each dot position, which can be turned into a depth estimate 
at each camera pixel based on region segmentation. The dots can also be removed from the digitally projected 
image since their locations are known. This computed depth map does not have a great deal of depth fidelity 
(the person s face reads as a flat card), but it s enough to actively refocus the otherwise in-focus 
camera image. The Bidirectional Reflectance Distribution Function (BRDF) Nicodemus et al 1977, Geometric 
considerations and nomenclature for reflectance .  In 3D using bv .(.i, fi, .r, fr)  The BRDF is the 
ratio of reflected light to incident light for any incident and radiant light directions.  But scanning 
3D geometry with computational illumination techniques is not the main topic today. Instead, we re more 
interested capturing the reflectance properties of objects. When we traditionally think of reflectance, 
we think of diffuse and specular components and the various reflectance models which have been proposed 
for them, all of which generalize to what are known as Bidirectional Reflectance Distribution Functions, 
or BRDFs. These say for any incident direction of illumination on the hemisphere, what the outgoing distribution 
of reflected light over the hemisphere is. Mirrors, which simply reflect rays, and diffuse Lambertian 
surfaces, have particularly simple forms of the BRDF. Here s a nice graph of how a BRDF is typically 
parameterized courtesy of Steve Marschner. Gonioref lectometry for BRDF Measurem ent Li, Foo, Torrance, 
and Westin. Automated three-axis gonioreflectometer for computer graphics applications. Proc. SPIE 5878, 
Aug. 2005. Stanford Spherical Gantry  Measuring BRDF s of real materials traditionally requires complex 
equipment with well-calibrated moving parts and lots and lots of measurements to capture the 4D BRDF 
of a reflectance sample. Here are a few successful examples of from academia and government. This project 
from UBC captures BRDFs using a small video projector, a video camera, and custom reflective optics to 
illuminate and image a material sample over (most of) the hemisphere with no moving parts. That makes 
measurement potentially much faster. More importantly, the authors do not just project point samples 
of incident illumination onto the scene. Instead, they project basis illumination functions, which directly 
measure the surface s response to basis illumination conditions. This allows the full BRDF, as projected 
onto a set of Zonal basis functions, to be captured in far fewer images than exhaustive BRDF measurement. 
From: http://www.cs.ubc.ca/labs/imager/tr/2007/BRDFAcquisition/ : The distinguishing characteristic of 
our BRDF measurement approach is that it captures the response of the surface to illumination in the 
form of smooth basis functions, while existing methods measure impulse response using thin pencils of 
light that approximate Dirac peaks. For this concept to be practical, we require an optical setup that 
allows us to simultaneously project light onto the sample from a large range of directions, and likewise 
to measure the reflected light distribution over a similarly large range of directions. Developing such 
optics also has the advantage that no moving parts are required, which is one reason for the speed of 
our acquisition. In this work, we choose a spherical zone of directions as the acquisition region for 
both incident and exitant light directions. Spherical zones have several advantages over regions of other 
shape. First, they allow us to develop basis functions that align nicely with the symmetries present 
in many BRDFs, thus minimizing the number of basis functions required to represent a given BRDF. Alignment 
also simplifies extrapolation of data into missing regions. Second, a zonal setup allows us to design 
optics that could, in principle, cover over 98% of the hemisphere, with only a small hole near the zenith, 
where BRDF values are usually smoother compared to more tangential directions. This technique leverages 
the fact that BRDF s can be represented as a sum of relatively simple basis functions. The projector 
emits a set of Zonal Basis Function Illumination conditions, and the camera picks up the result of this 
light when it is reflected. As a result, BRDF models can be fit to the data. Here are some of the BRDFs 
which were captured with relatively few measurements. Now, suppose that we want to capture how a whole 
object reflects light, instead of just a material sample. Objects, photometrically, are simply volumes 
of space which transform a field of incident illumination into a field radiant illumination, reflected 
back from the object. We know that incident illumination can be parameterized as a 4D incident light 
field. To do this we conceptually enclose the object within a convex surface such as a sphere, and we 
use (u,v) to indicate the position on the surface where the light enters, and (theta,phi) to indicate 
the direction in which it enters. The radiant light can be described similarly as a radiant light field. 
It can be parameterized the same way, except we look at how light is leaving the surface that surrounds 
the object. We can thus characterize how an object reflects light as an eight-dimensional function called 
the reflectance field. For any incident ray of light, it encodes the 4D radiant light distribution resulting 
from the object being illuminated by that ray. The reflectance field thus contains the information necessary 
to rendering the object under any illumination condition, from environmental to spatially-varying lighting, 
and seen from any viewpoint. The reflectance field s form is similar to that of the BRDF, and it s almost 
as if we have promoted the BRDF from characterizing light reflection at a point to characterizing light 
transport into and out of a region of space. The reflectance field in fact has the same basic form as 
the BSSRDF, which represents how light diffuses through an inhomogeneous translucent surface such as 
skin. However, the surface upon which light impinges is not assumed to be coincident with the actual 
surface of the material. Since light is linear, the radiant distribution of any two simultaneous incident 
rays is the sum of the distributions of the individual rays. This means that the reflectance field is 
linear, and thus its transport of light can be represented as a matrix operation from a vector representing 
the incident light field to a vector representing the radiant light field. This is sometimes called the 
transport matrix. Reflectance Field Storage Requirements   R( u i , v i , . i , f i ;u;ur , v r , 
. r , f r ) 36 0 x 180 x 180 x 180 x 360 x 180 x 180 x 18 0 = 4.4e18 measurements  x 6 bytes/pixel 
(in RGB 16-bit ) = 26 exabytes (billion GB) = 82 millio n 300GB hard drive s (41 million if we exploit 
Helmholz Reciprocity)  Compared to BRDF s, the reflectance field is even more daunting to capture and 
store exhaustively. The reflectance field can even be considered more generally. Parameters (on both 
the incident and radiant rays) for time, wavelength, and 3D position yield a 14D function. Adding Stokes 
parameters for the incident and radiant rays to characterize polarization would expand the dimensionality 
even further. More often, we actually want to simplify reflectance the consideration of the reflectance 
field. It is often reduced to a 4D function wherein the viewpoint is fixed at a particular camera location, 
and rays of light are assumed to emanate from far away from the object. This precludes recording the 
effects of spatially-varying illumination, such as dappled light or partial shadow. In this form, the 
coordinates on the surface of the reflectance field has a one-to-one relationship with camera pixels, 
so (u,v) is usually thought of as the particular camera pixel viewing the radiant illumination. Since 
it is easier to capture, this 4D version is often the preferred form of reflectance field capture for 
human subjects. Of course, people move, so recording a time-varying 4D reflectance field is of interest. 
 Light Stage 1 was designed to capture 4D reflectance fields of human faces in a tractable amount of 
time with low-cost equipment, with relatively high lighting resolution. Spinning the light around in 
a spiral over the course of minute yields images of the face illuminated in nearly 2,000 lighting conditions. 
Here we see a sub-sampled version of such a dataset about 1/16th of the total number of images acquired. 
The data shows the face lit from every direction that light can come from. Technically, the light is 
always just 5 feet away, but since the head is small we assume that this represents the response to a 
distant lighting environment. It shows what the fact would look lik with a unit intensity white light 
source from every (theta, phi) direction. If we want to show the face under a different lighting environment, 
we first need to resample the lighting environment to be in the same coordinate space and lighting resolution 
as the facial reflectance field dataset. You can see that at the bottom of this slide. By multiplying 
the lighting and reflectance datasets together, we get a mosaic of images where each face has been tinted 
to be the color and intensity of the illumination coming from that direction in the environment. For 
example, the faces in the center left of the mosaic are bright and yellow since there is bright and yellow 
light coming from that direction in the environment. Essentially, we have lit the face by the HDR lighting 
environment one piece of the environment at a time. To show the face in the entire environment at once, 
we simply need to add all of these images together. Adding all the images together, since light is additive, 
yields an image of the person in the novel lighting environment. It s even easy to change the lighting 
environment! Here are four other lighting environments from various light probe images. It is informative 
to also look at the transpose of the 4D data which we just saw as a 2D grid of faces. If you pick a particular 
pixel on the face, we have recorded about 2000 pixel values for it according to the incident lighting 
direction. These can be shown in a latitude-longitude 2D image representation. We call these 2D pixel 
maps reflectance functions, because they encode how a given reflects light from any possible incident 
direction. Reflectance functions begin to look like slices of the facial pixel BRDFs, since they include 
specular lobes and diffuse lobes of reflectance. But they also include non­local reflectance effects 
such as indirect illumination, self-shadowing, and subsurface scattering. These particular reflectance 
functions also include some shadowing from the phi-bar and glare from the light source in the back. 
We can perform the same lighting calculations in this transposed reflectance function space. The spherical 
map of incident illumination and the reflectance function yields the color of that pixel illuminated 
by that lighting environment. Reflectance functions, even more than regular images, tend to be compressible. 
In the bottom row you can see here that the reflectance function projected onto the DCT basis concentrates 
energy in relatively few lighting coefficients. The HDR lighting environment, in contrast, has a lot 
more frequency content in comparison. What s particularly cool and useful is that you can still do the 
relighting process directly on the transformed coefficients and arrive at the same rendered pixel values. 
That s because the particular transform we are using the DCT is an orthonormal transform. The techniques 
of Precomputed Radiance Transfer (e.g. Sloan et al SIGGRAPH 2002) all leverage this fact to perform real-time 
relighting of CG objects essentially based on pre-rendered light stage datasets of the objects. Performing 
the relighting in frequency space allows high-resolution datasets to be re­illuminated in real time, 
such as seen in the real-time face Facial Reflectance Field Demo at http://gl.ict.usc.edu/Data/FaceDemo/ 
 A number of publicly-available light stage datasets taken with Light Stage 6 are available on the graphics 
lab web site. So let s think about how we can do better than what we ve seen so far. Can we improve 
on these techniques to allow for: Faster capture?  Higher lighting resolution?  Better image quality? 
 Spatially-varying illumination?  Faster capture can be done through hardware techniques as it turns 
out. You just need a light stage with a whole sphere of rapidly-controllable bright LED lights, and 
a high-speed camera, like this Vision Research Phantom v7.1. It can capture images at 800x600 pixel resolution 
at up to 4800 frames per second. The Light Stage 5 apparatus shown in Figure 3 is a 2m sphere of 156 
white LED light sources that surround an actor. The LED lights are controlled by a microcontroller that 
can change the lighting direction thousands of times per second, fast enough that the illumination appears 
as a fluttering sphere of light rather than sequential lighting directions. Filming the actor with a 
synchronized high-speed video camera yields a stream of images of the actor under the repeating sequence 
of 156 lighting directions, with each complete sequence taking as little as 1/24th of a second of capture 
time. With this data, the images can be recombined with image-based relighting to show the actor s performance, 
in motion, in any new lighting environment. To achieve the sharpest results, some motion warping through 
optical flow is required to give the appearance that each set of 156 images was taken all at the same 
time. Later in this talk, we ll discuss other ways of achieving more time-efficient capture, including 
using gradient illumination patterns, and compressed sensing. One problem with this kind of high-speed 
capture is that you become very limited by the amount of light available given that there are such short 
exposures. The individual lighting direction images from the high-speed camera can actually look quite 
noisy. Let s now ask ourselves if we could capture our datasets in a different way which could alleviate 
this problem, and we ll look to some work from Yoav Schechner and his colleagues for this. When you capture 
a light stage dataset, there is nothing requiring you to just turn on one light at a time (as seen for 
three lights in the first row). If you instead turn different sets of lights which are linearly independent 
and thus span the same space as the set of single light sources (as seen in the second row, when two 
lights at a time are turned on). You just need to run the resulting images through an inverse matrix 
to get back to the images illuminated by single light sources, as seen at the bottom for this small example. 
Why would you want to do this, other than some fun with linear algebra and a higher electric bill? Well, 
as it turns out, the images you get by demultiplexing will generally have a different signal-to­noise 
ratio than the single-light-source images. Suppose there is additive noise of variance sigma^2 in each 
pixel of every image taken. Then, the demultiplexed images will have a sigma of (3/4)sigma^2, which is 
less than the variance in single light source images. (For example, the variance of a1,2 a2,3 + a1,3 
is three times the variance of the original images since Var(a+b)=Var(a)+Var(b) if a and b are uncorrelated, 
and the variance of half this quantity is one quarter that value since Var(k*a)= k^2 Var(a). Schechner 
et al tried this approach with a larger number of light soruces by projecting patterns of rectangles 
of light onto a wall of a room to act as a set of light sources reflecting back onto a subject (the pumpkin, 
in this diagram) over a subset of the incident lighting sphere. In their work they used Hadamard patterns, 
formed using an S-matrix, to illuminate the scene. There are the same number of Hadamard patterns as 
there are individual lights, but each Hadamard has just over half of the lights turned on. The top two 
images show actual images taken under the Hadamard patterns. The have nice noise characteristics, but 
they are not the final images we are interested in. Instead, we can . The images are certainly noisier, 
but still look reasonable. The bottom two images show images taken under single light sources. Since 
a single light source is pretty dim, the images are quite dark. Here they have brightened considerably 
in order to show the image, and it s clear the signal is so small that the quantization noise of the 
camera has considerably degraded the signal. Since quantization noise is additive noise, it can be reduced 
using Hadamard multiplexing. Noi s e cu rves f o r th ree typi cal ca mera s, s h owi n g cl ose fi 
ts to an addi ti ve-plus-photon-noise m o del as U Pa Debevec. Performance And r eas Wen g er, Ch ri 
s Tch ou , And r ew G a rdn e r, Ti m Haw k in s, Jon as Un g e r, Pau l Debevec. Performance Rel i g 
h t in g an d Reflect a n c e Tran sform a t i o n w i th Ti m e -Mul t i pl ex ed I l lu m i n a t i 
on , SI G G R AP H 20 05  There is an issue which arises when applying Hadamard multiplexing with real 
images, which is that typical cameras have both additive noise (due to quantization and dark current) 
plus some amount of photon noise whose variance is proportional to the signal. These three curves show 
noise response curves for three cameras we ve used in our laboratory. We shot 100 images of uniformly 
lit patches at various brightness levels, and graphed the variance for a pixel in each patch against 
the mean pixel value for that patch. The cameras include the high speed camera from the Light Stage 5 
project, a Canon D30 still camera, and a cooled QICam machine vision camera. Each curve was well described 
as a constant amount of additive noise plus photon noise with a standard deviation proportional to the 
square root of the signal, i.e. sideways parabolas. The cooled QICam had the lease dark current noise 
of all the camera, almost negligible. The problem is that when photon noise dominates, there can actually 
be a multiplexing disadvantage, as bad in theory as doubling the variance of the demultiplexed signals 
in the worst case. Another problem is that demultiplexed Hadamard images can have visible noise in shadow 
regions, since all areas of the images tend to have an equal amount of noise. (Some) Multiplexing advantages 
and disadvantages  If ad ditive noi s e d o minates, t h ere is an SNR advantag e If photon noise dominates, 
there can × If photon noise dominates, there can be a SNR disdvantage  Scene dynamic range is compressed 
 Dark areas in the demultiplexed × Dark areas in the demultiplexed i mages have as much noi s e as bri 
g ht regi ons, which can be visible  Human perception of the patterns can be improved Latest results: 
Nenanel Ratner and Yoav Y. Schechner, Illumination multiplexing within fundamental limits. CVPR 2007 
  Hadamard patterns are a clear win when single-light images are very underexposed, and quantization 
or dark current noise dominates. If you are able to expose your images properly, single-lit images may 
give the most pleasing results due to the photon noise effect shadow noise issue. When we tried Hadamard 
patterns in Light Stage 5, we actually found that the flashing Hadamard patterns were more comfortable 
for the subjects than the single-light patterns since the lights blinked well above the rate of perception, 
bathing the actor in a relatively constant glow. Hadamard patterns also have distinct advantages when 
a scene includes both diffuse and sharp specular reflections, since the wide-area patterns bring the 
brightness of the reflections more in line with each other, alleviating problems in capturing the full 
dynamic range of the scene. For now, though, let s think about achieving higher lighting resolution. 
Objects with shiny reflections or translucency can be difficult to capture with light stage techniques, 
since specular reflections can be very sharp. Obtaining better lighting resolution would be great for 
these objects. One approach to continuous illumination uses video projectors. We saw how Schechner used 
a video projector to light up a wall in front of the object. The Reflective Light Stage of Peers et al. 
lights up a whole hemisphere surrounding an object using an Elumens fisheye video projector. A rough 
specular painted surface on the inside of the dome increases light efficiency. These two projects built 
light stages out of a single computer-controlled Disco light which projected a spot of illumination onto 
a projection surface surrounding the object. Mohan et al. At the expense of needed to take longer exposures, 
Fuchs et al projected onto a room of black felt, which greatly reduced the indirect illumination on the 
scene. They used the width of the beam to acquire adaptively sampled resolution patterns, and found ways 
of interpolating between different lighting conditions to given the appearance of super-resolution reflectance 
fields. Another technique for achieving high-resolution lighting capture leverages Helmholtz Reciprocity 
 the condition that light rays are reversible, in that if you switch a sensor and a light emitter in 
a scene, the same amount of light will still get from one to the other, no matter the complexities of 
the light path(s). With the Dual Light Stage, the object is surrounded in a diffusely painted grey sphere. 
A very bright laser sweeps across the object, and at each point, the laser reflects, refracts, and scatters 
to form images of each pixel s reflectance function on the inside of the sphere. These complete-sphere 
images are then recorded by a camera with a fisheye lens at the top of the sphere. The photographed reflectance 
functions have hundreds of thousands of pixels enough to see clear reflections in still liquid, and sharp 
. However, the spatial image resolution is not optimized the images themselves are relatively low resolution 
(200x200). Another project which made use of Helmholz Reciprocity was the Dual Photography project from 
Stanford. This project showed that an image of an object could be obtained from the position of a video 
projector just as well as from a camera. Sen et all used a set of adaptive patterns to greatly increase 
the speed at which the light transport matrix could be measured, and achieved spatially-varying relighting 
from up to sixteen points of view. However, since the patterns were adaptive, the images could not be 
taken particularly quickly since processing in between the images was required. 65536 Pixels 65536 Pixels 
original 1300 3300 Measurements Measurements (2%) (5%) .,M a rc o F. D u arte , Mar k A. D a ven p ort, 
Dh arm p al Ta kh ar, J a so n N . La s k a , Ti ng Sun, Kevi n F. K e ll y an d Ri char d G. Bar a ni 
uk, Si ngl e Pi xel Im agi n g vi a i Compressi ve Sa mpl in g, I EEE Si gnal Proce ssi ng Maga zi ne, 
Marc h 20 08.  We ve heard in the last talk a little about compressive sensing (CS) for novel imaging 
application. With CS, a compressed full-resolution version of a signal (such as this image of the letter 
R from Rice University s single-pixel camera project) can be inferred from a much smaller number of non-adaptive 
measurements than from exhaustive capture, as long as it is sparse in some projectable basis. Since CS 
uses non-adaptive input signals, no online processing is required to obtain the reconstructed results, 
and the patterns are scene-independent. Can we apply CS to capturing object reflectance as well s regular 
images? Of course! The time required to capture a high resolution reflectance field is directly proportional 
to the number of photographs that need to be acquired. The number of photographs is in turn directly 
proportional to the lighting resolution. Thus, for high resolution reflectance fields, an impractically 
large number of photographs need to be recorded. We will now look at specific properties of reflectance 
fields, that can help speed up the acquisition process. For this purpose, consider the following scene. 
 Two randomly selected reflectance functions might look something like this. Both functions are very 
similar in appearance, and are both relatively simple in content. In order to exploit this apparent simplicity, 
reflectance functions have often been transformed into a different basis to express this simplicity in 
a more formal way. For example, if we convert these functions into a wavelet basis, we could get functions 
that look like this. First thing to note is that these functions contain many zero or near zero elements. 
The simplicity in appearance of before is now quantitatively expressed by just a few important (non-zero) 
coefficient in the new basis. Note, that if we set the near-zero elements to zero, an approximation of 
the original function is obtained, that is not exactly the same, but very similar. This method, is for 
example used in compressing images. So what does it mean to have just a few non-zero coefficient of reflectance 
function in a specific basis. Well, this means that we only need to measure these coefficients to obtain 
a good approximation of the reflectance functions. As we have seen before, measuring the response of 
a specific coefficient of a basis is equal to emitting that basis function onto the scene and observing 
its response. Thus, by only emitting the basis functions that correspond to non-zero coefficient, we 
can potentially measure a reflectance field much faster. However, there is s light problem: we are measuring 
not just a single reflectance function, but a whole reflectance field, which is a collection of many 
reflectance functions. Each reflectance function might have just a few non-zero coefficients, which is 
good, but also that the set of non-zero coefficients for each function is different. So in the worst 
case, we still have to measure all coefficients, and thus obtain no speed up. For example, in our example 
here, the green coefficients are shared, and thus emitting the corresponding basis functions yields information 
gain for both pixels. However, the red marked coefficients are not shared. So when you measure on of 
these red coefficients, you will only measure additional information for a single pixel, and not gain 
any information for the other. From this it is clear that in order to have a fast acquisition, we need 
to somehow find a way to maximize the information gain for all pixels. The solution is to emit multiple 
basis functions at the same time, trying to make sure that we know for each functions which basis functions 
has an effect and which ones don t. Exploi ting Compressibili ty for Acquisition  There a two possible 
solutions. The first one, tries to explicitly coordinate the parallelism of measuring coefficient during 
the acquisition phase. This are called an adaptive methods. An adaptive method, uses the information 
it has of the reflectance field, to schedule new measurements such that information gain is maximized. 
This requires some processing during acquisition, but usually no after acquisition. Also, the illumination 
patterns will differ when measuring different scenes. The second method is called non-adaptive. This 
method always uses the same illumination patterns, which are specially designed to maximize the information 
gain per measurement. In order words, these measure responses for every basis function (but with different 
weights) in each measurement. During post-processing, each reflectance functions needs to be inferred 
in an adaptive fashion from the measurement. In other words, there is an implicit parallelism during 
acquisition. Non-adaptive methods move the complexity of the system from the measurements to the post-processing. 
Both methods have their advantages and disadvantages, and are capable of measuring a reflectance fields 
in a number of measurements that is proportional to their compressed size. This is to some degree independent 
of the illumination resolution. Non-adaptive Methods   We will now discuss three non-adaptive methods 
briefly. The first method was presented by Matusik et al. In 2004. This method uses natural illumination 
as illumination patterns (photographs). These photographs are emitted from a CRT monitor onto the scene. 
They represent the reflectance field as a sum a box kernel functions which different sizes and positions. 
During post­processing they split the kernel in each reflectance function that explains the observed 
responses under the known natural illumination best. Furthermore, to improve the results, a spatial correction 
is performed after post-processing to ensure that neighboring pixels have similar reflectance functions. 
Non-adaptive Methods  Here you can see a result of their method. On the right you see a reference photograph 
of the scene, and on the left the same scene relit using the same illumination and using the computed 
reflectance field. Each reflectance function is the result of 24 subdivisions. Here you see a different 
scene. Again, 24 subdivisions were performed to obtain these results. The number of measurements was 
approximately 200. Non-adaptive Methods   In 2005, Peers and Dutré presented a system that uses Wavelet 
noise as illumination patterns. Here you can see such a wavelet noise illumination pattern. This allowed 
them to use a Haar wavelet basis instead of the weighted sum of box kernel functions of Matusik et al. 
A Haar wavelet basis has the advantage that it is a real basis, and has been study really well in image-compression, 
and thus a lot of mathematical properties are known. Their algorithm builds a list of candidate coefficients, 
and at each step adds the best candidate. When adding a coefficient to the solution, it children s coefficients 
are estimated are added to the list of candidates. No spatial coherence is enforced in their algorithm. 
 Here you see a scene lit by a photograph of a stone bridge. On the right a reference photograph, and 
on the left a relit image. Each reflectance function was reconstructed from 250 photographs, and contained 
64 Haar wavelet coefficients. Here a different scene is shown. This time inferred from 500 measurements, 
and each reflectance function is reconstructed using 128 Haar wavelet coefficients. Non-adaptive Methods 
  This year, Peers et al., presented a technique that utilizes the mathematical theory of compressive 
sensing (as explained before). However, directly applying compressive sensing does not work well. For 
this they made a number of enhancements that make it better suited for measuring reflectance fields. 
 First they use newly designed measurement patterns called segregated binary patterns. These patterns 
are binary (black and white), and are segregated in two ways. First there is scale. As you can see on 
the three patterns on the right. All three look somewhat similar, but have different scales. Second, 
each scale is further segregated into three groups, shown on the three patterns on the left. As you can 
see, the first pattern exhibits more horizontal structures, the middle one more vertical, while the third 
one does not prefer any direction. By capturing a balanced mix of these patterns, segregated in scale 
and direction, compressive sensing can be used to reconstruct reflectance fields. Additionally, they 
exploit the spatial coherence of the reflectance field by using a hierarchical algorithm to infer the 
reflectance functions. Non-adaptive Methods  Authors  Patterns  Basis (Amplitude Normalized) Haar 
Wavelets [2005] Segregated Binary Peers et al. [2008] Haar Wavelets Patterns  Spatial Algorithm Coherence 
 List of Candidates Compressive HierarchicalSensing  Relit (128 coeff.) Reference Photograph Here you 
can see a scene, illuminated by a natural illumination condition (emitted from a CRT monitor on the right). 
On the right a reference photograph, on the left a relit image. The reflectance field contains 128 Haar 
wavelet coefficients per reflectance function. Approximately 1000 measurement were performed to obtain 
this result. Non-adaptive Methods Spatial Authors  Patterns Basis Algorithm Coherence Haar Wavelets 
(Amplitude Normalized) List of Candidates [2005] Segregated Binary Compressive Hierarchical Peers et 
al. [2008] Haar Wavelets Patterns Sensing  Relit (128 coeff.) Reference Photograph Here is a final 
example. In this case the illumination is emitted from the hemispherical Reflective Light Stage shown 
earlier. Again, 1000 measurements were performed, and 128 coefficients reconstructed per reflectance 
functions. As we will see, it can be easier to capture and analyze reflectance signals if it is possible 
to separate various reflectance behaviors in the reflectance functions, such as diffuse and specular 
reflections. "Fast Separa tion of Direct and Global Components of a S c ene using High Frequency Illumina 
tion," ,. S.K. Nayar , G. Krishnan, M. D . Grossberg, R. Rask a r, ACM T r ans. on Graphics (a lso Proc. 
of ACM S I GGRAPH), Jul, 20 06 .. Nayar et al. used high-frequency illumination patterns to quickly 
separate direct and global components. Basically, the global components stay the same as you phase-shift 
high-frequency illumination on the scene, while the direct components appear and disappear at different 
pixels. Taking the minimum value over a sequence of phase shifts yields the global component, multiplied 
by the fill ratio of the patterns; the maximum minus the minimum yields the direct component. This project 
uses a similar technique to Nayar et al 2006 to separate diffuse and specular reflections, rather than 
direct and global components. It makes use of the Reflective Light Stage. Here is the object whose components 
we will separate a marble ball, with both specular and diffuse reflections. The image on the left is 
an image we will consider projecting into the dome and observing its reflections back from the object. 
We won t project the pattern itself, since that would yield both diffuse and specular reflections mixed 
together. Instead, we will modulate the patter by four phase­shifted high-frequency illumination patterns 
and project those instead. A detail of the reflection of one of these patterns is shown at the right. 
 Taking the mimimum pixel value over the four-pattern sequence yields the left image showing the diffuse 
reflectance of the subject under the image-based illumination environment. Finding the difference between 
the maximum and minimum values for each pixel over the sequence yields an estimate of the specular component 
of the object s reflectance, shown at the right. For this object, this component reveals a clear image 
of the image-based illumination environment. Another useful way to separate diffuse (or subsurface) 
and specular reflections is through polarization. Placing perpendicular polarizers on a light source 
and the camera removes specular reflections, which maintain polarization. The subsurface and diffuse 
components remain since they depolarize the light. Turning one of the polarizers so that it becomes parallel 
to the other tunes the specular reflections back in. The difference between two such images shows only 
the light which maintains polarization, which is primarily the specular component of the illumination. 
 This polarization separation process can be combined with a gradient illumination technique to obtain 
useful estimates of facial surface normals. Let s return to looking at a canonical reflectance function, 
with a diffuse and a specular component. If we wanted to develop a shorthand for the reflectance function, 
we could say a lot about it if we knew the color and centroid of the diffuse lobe, and the intensity 
and centroid of the specular lobe. If there were a way to separate the reflectance function into its 
diffuse and specular components, and then have each function available, this is relatively straightforward. 
Debevec et al 2000 used a colorspace separation technique to separate the reflectance function seen above. 
The surface normal as determined from the diffuse reflection is essentially equivalent to the centroid 
of the diffuse normal. The centroid of the specular reflection actually is an estimate of the reflection 
vector; the corresponding surface normal the lies halfway back to the view vector. Note that this does 
not necessarily yield the same normal estimate. This pattern of linear polarizers from [Ma et al 2007] 
allows the entire sphere of illumination to be cross-polarized from a particular camera position at the 
front of the stage. Ignoring a few complications regarding the Fresnel equations and in particular the 
Brewster angle, this allows spherical reflectance functions to be separated into diffuse and specular 
components through polarization difference imaging. Let s now build a process for measuring the centroid 
of reflectance functions (based on their diffuse or specular component) using a small number of incident 
illumination patterns. First let s simplify the problem to 1D. We can compute the centroid of a 1D function 
by integrating it against a linear function, or gradient, as seen in this slide. We ll see in a second 
how we can perform the same process for a spherical reflectance function to estimate a surface normal 
or a reflection vector. To find the centroid of a spherical reflectance function, such as the diffuse 
one shown above, we can integrate it against gradients of spherical illumination across the three coordinate 
axes. As before, we also integrate the total energy of the function in order to normalize the coordinate 
estimates. Instead of measuring these functions exhaustively, we note that what we really need to compute 
the surface normal is the dot product of the reflectance function with each gradient illumination condition. 
These we can actually measure directly by lighting the subject with the gradient illumination conditions 
directly, instead of measuring the reflectance functions first. In this way, we will use the illumination 
to compute the surface normals in the scene! Four images of the scene under the gradient illumination 
patterns yield estimates of the surface normals from either the diffuse or specular components of the 
subject s reflectance. Gradient Illumination techniques are efficient enough that surface normal maps 
for a face can be recorded in real time with relatively standard camera equipment. Surface normal maps 
can be used for a variety of normal map relighting tricks. The specular normal map in particular provides 
a great deal of high-resolution geometric information about the face, since this measures the reflection 
of light from . This high­resolution facial scan was created from a lower-resolution structured light 
scan (accurate to a millimeter or two) increased in resolution (to perhaps 0.1mm) using the specular 
normal map, as in Ma et al. 2007. The specular and diffuse surface normals can also be used for hybrid 
normal map rendering. This rendering of a face on the left, seen from a novel viewpoint and illumination 
condition, uses surface normals computed from the face s diffuse component to shade the diffuse reflection, 
and normals computed from the face s specular component to shade the specular reflection. The real-time 
rendering closely resembled the validation photograph to its right. The skin-like appearance results 
from differences in the surface normals due to the subsurface scattering properties of the skin. The 
last two projects I want to mention make clever use of wavelength-altering reflectance properties use 
of for scene capture, which we haven t yet seen. MOVA s Coutour system leverages fluorescence to capture 
3D facial geometry. It uses stereo correspondence from multiple cameras to record the 3D geometry of 
the face, but it avoids a common problem of lacking sufficient facial texture to match points between 
the viewpoints. They do this by applying a random pattern of glow-in-the-dark makeup to the performer, 
and illuminated the subject with a combination of ultraviolet and visible lights which turn on approximately 
thirty times per second. The cameras record a visible texture image when the lights are on, and during 
this time the ultraviolet lights charge the phosphorescent makeup. Then, a second image is acquired when 
the lights are off, observing the makeup glowing on its own. This also alleviates the problem of specular 
facial reflectance in geometry matching, since the emmissive surface has a relatively even light output 
over the hemisphere. In this way, natural facial shapes can be captured. Hullin et al s work being presented 
at this year s SIGGRAPH conference presents an ingenious technique for 3D scanning otherwise difficult-to-scan 
objects using Fluorescent Immersion Range Scanning. In this technique, a laser stripe moves over the 
object, but object is placed in a liquid with fluorescent dye which glows orange when the green laser 
stripe travels through it. When the stripe reaches the object, the volume no longer fluoresces, and there 
is a dark contour which indicates the intersection of that plane of the laser with the object, easily 
triangulated into 3D. The fluorescence (and a filter on the camera blocking the wavelength of the laser 
light) removes the effect of multiple scattering, which would be a problem if the laser were traveling 
through a cloudy liquid. Thanks li Pi eter Peers s lid es on E x pl oitin g Com p re ssib ility for 
Acquisiti o n and Non-Adaptive Methods i i, ICT G r aph ics Lab: Abh ijeet G h osh , Pi eter Peers, 
Andrew Jones, Charl e s-F e lix Chabert, Per Einarsson, Alex Ma, Aimee Dozois, Jay B u sch, , Tom Pereira, 
Naho Inam oto , Brian Emers o n, Marc Br ownlow, Tim Hawki n s, Andrea s Wenger , , Andrew Gardner, Chris 
Tchou, Jonas U n ger, F r ed e r ik Gorranson, John L a i, T o m P e re ira, David Price  Steve Mars 
chner, Cornell Aut h ors of all t h e work covere d in t h is t a lk ICT Sponsor s : USC O ffice of 
the Provost, RDECOM, TOPP A N Pri n tin g Co, Ltd. lBil l Swartout, R a nd al Hill, USC ICT Randal 
Hall, Max Nikias, US C Ramesh Rask ar and Jack Tumblin, class organizers SIGGRAPH 20 08 Teach-Learn Commi 
ttee Ste p he n Spe n ce r www.debevec.org / gl.ict.usc.edu   Thank you! Optics: Computatable Extensions 
Ramesh Raskar MIT Media Lab Optics: Computable Extensions Unusual Optics  Wavefront Coding  Folded 
Optics  Nonlinear Optics   Material (Refractive Index)  Graded Index (GRIN) materials  Photonics 
crystals  Negative Index   Imaging  Schlieren Imaging  Agile Spectrum Imaging  Random Lens Imaging 
  Animal Eyes  What can we learn Conventional lenses have a limited depth of field. One can increase 
the depth of field and reduce the blur by stopping down the aperture. However, this leads to noisy images. 
Wavefront Coding using Cubic Phase Plate  "Wavefront Coding: jointly optimized optical and digital 
imaging systems , E. Dowski, R. H. Cormack and S. D. Sarama , Aerosense Conference, April 25, 2000 Slides 
by Shree Nayar A solution proposed by authors and now commercialized by CDM optics uses a cubic phase 
plate. The effect of cubic phase plate is equivalent so summing images due to lens position was different 
planes of focus. Note that the cubic phase plate can be made up of glass of varying thickness OR glass 
of varying refractive index. The example here shows a total phase difference of just 8 periods. Unlike 
traditional systems, where you see a conical profile of lightfield for a point in focus, for CDM the 
profile is more like a twisted cylinder of straws. This makes the point spread function somewhat depth 
independent. The point spread function is not a point even when the point is in sharp focus. One can 
say it is equally worse over a large range of focus. Example Conventional System Wavefront Coded System 
 Stopped Down After Processing Slides by Shree Nayar After software decoding one can recover sharp images. 
These images show good tradeoff between depth of field and image noise. Traditional lenses are long, 
the length is close to the focal length in mm. Origami Lens : Thin Folded Optics (2007)  Ultrathin Cameras 
Using Annular Folded Optics, E. J. Tremblay, R. A. Stack, R. L. Morrison, J. E. Ford Applied Optics, 
2007 -OSA Slides by Shree Nayar New techniques are trying decrease this distance using a folded optics 
approach. The origami lens uses multiple total internal reflection to propagate the bundle of rays. 
 Nonlinear Optics Linear Optics  Light is deflected or delayed without wavelength change  (a linear 
system )   High intensity light in non-linear media  Think of it as self-modifying programs  Mainly 
lasers and crystals   Nonlinear optics  Change the color of a light beam  Change beam s shape in 
space and time  Light intensity refractive index  Self-focussing via gaussien high intensity beam 
 Switch and gate telecommunications systems  Create shortest events (in femtoseconds, 10-15).   www.physics.gatech.edu/gcuo/UltrafastOptics/3803/OpticsI23NonlinearOptics.ppt 
Nonlinear optics has been a rapidly growing field in recent decades. It is based on the study of effects 
and phenomena related to the interaction of intense coherent light radiation with matter. Nonlinear optics 
(NLO) is the branch of optics that describes the behaviour of light in nonlinear media, that is, media 
in which the dielectric polarization P responds nonlinearly to the electric field E of the light. This 
nonlinearity is typically only observed at very high light intensities such as those provided by pulsed 
lasers.  Why do nonlinear effects occur, in general? Imagine playing music through a cheap amplifier 
that just can t quite put out the power necessary to hit the loud notes. The sharp edges correspond 
to higher frequencies harmonics! www.physics.gatech.edu/gcuo/UltrafastOptics/3803/OpticsI23NonlinearOptics.ppt 
Non-linear effects are always present but usually barely noticeable. However, when the input signal intensity 
is high, the system does not behave as a linear system. The higher harmonics have progressively smaller 
amplitude. Nonlinear Optics Interesting Applications Change in refractive index (RI),  Original RI: 
n0  Nonlinear RI: n2  Incident light intensity, I  Modified refractive index, n = n0 + n2*I   Self-focusing 
 N2 is positive,  RI is higher for higher intensity, usually center of beam  Create a convex lens, 
 (But collapsing beam on itself till material is damaged)   www.physics.gatech.edu/gcuo/UltrafastOptics/3803/OpticsI23NonlinearOptics.ppt 
For Computational Photography, an interesting future direction could be programmable refractive index. 
We will see more on the next slide. Self-focusing is induced by the change in refractive index. A medium 
whose refractive index increases with the electric field intensity acts as a focusing lens for a laser 
beam. The peak intensity of the self-focused region keeps increasing as the wave travels through the 
medium, until defocusing effects or medium damage interrupt this process. Since n2 is positive in most 
materials, the refractive index becomes larger in the areas where the intensity is higher, usually at 
the centre of a beam, creating afocusing density profile which potentially leads to the collapse of a 
beam on itself.[ Self-focusing is often observed when radiation generated by femtosecond lasers propagates 
through many solids, liquids and gases. Depending on the type of material and on the intensity of the 
radiation, several mechanisms produce variations in the refractive index which result in self-focusing: 
a well known example is Kerr-induced self-focusing. Optical Kerr Effect: Intensity dependent Refractive 
Index The refractive index including higher order terms 2 (1) (3) n= 1+. +. E Now, the usual refractive 
index (which we ll call n0) is: n0 = 1+.(1) So: 2 (3) (3) 2 n= n0 +. E 2 = n0 1+. E 2/ n0 Assume that 
the nonlinear term << n0 : So: 1 (3) 2 (3) n n.1+. E 2/ n. n+. E 2 /2 n00 . 2 0 . 0 Usually, we define 
a Nonlinear Refractive Index : n2 ..(3) /2n0 2 nn n 0 + 2I since I. E Before 1960, optics mathematical 
equations manifested a linear system of equations, using usual refractive index n0. The optical Kerr 
effect, or AC Kerr effect is the case in which the electric field is due to the light itself. This causes 
a variation in index of refraction which is proportional to the local irradiance of the light. This refractive 
index variation is responsible for the nonlinear optical effects of self-focusing and self-phase modulation, 
and is the basis for Kerr-lens modelocking. This effect only becomes significant with very intense beams 
such as those from lasers. Nonlinear materials like quartz crystal create a second harmonic (twice the 
frequency, i.e. half the wavelength) when a high intensity laser is incident. This figure is borrowed 
from Margaret Murnane s HHG talk. Optics: Computable Extensions Unusual Optics  Wavefront Coding 
 Folded Optics  Nonlinear Optics   Material (Refractive Index)  Graded Index (GRIN) materials  Photonics 
crystals  Negative Index   Imaging  Schlieren Imaging  Agile Spectrum Imaging  Random Lens Imaging 
  Animal Eyes  What can we learn Gradient Index (GRIN) Optics Refractive Index along width Gradient 
Index Lens Conventional Convex Lens Continuous change of the refractive index Constant refractive index 
but carefully within the optical material designed geometric shape Change in RI is very small, 0.1 or 
0.2 Consider a conventional lens: An incoming light ray is first refracted when it enters the shaped 
lens surface because of the abrupt change of the refractive index from air to the homogeneous material. 
It passes the lens material in a direct way until it emerges through the exit surface of the lens where 
it is refracted again because of the abrupt index change from the lens material to air (see Fig. 1, right). 
A well-defined surface shape of the lens causes the rays to be focussed on a spot and to create the image. 
The high precision required for the fabrication of the surfaces of conventional lenses aggrevates the 
miniaturization of the lenses and raises the costs of production. GRIN lenses represent an interesting 
alternative since the lens performance depends on a continuous change of the refractive index within 
the lens material. Instead of complicated shaped surfaces plane optical surfaces are used. The light 
rays are continuously bent within the lens until finally they are focussed on a spot. Miniaturized lenses 
are fabricated down to 0.2 mm in thickness or diameter. The simple geometry allows a very cost-effective 
production and simplifies the assembly. Varying the lens length implies an enormous flexibility at hand 
to fit the lens parameters as, e.g., the focal length and working distance. For example, appropriately 
choosing the lens length causes the image plane to lie directly on the surface plane of the lens so that 
sources such as optical fibers can be glued directly onto the lens surface. Photonic Crystals Photonic 
Crystal  Nanostructure material with ordered array of holes  A lattice of high-RI material embedded 
within a lower RI  High index contrast  2D or 3D periodic structure   Photonic band gap  Highly 
periodic structures that blocks certain wavelengths  (creates a gap or notch in wavelength)   Applications 
 Semiconductors for light : mimics silicon band gap for electrons  Highly selective/rejecting narrow 
wavelength filters (Bayer Mosaic?)  Light efficient LEDs  Optical fibers with extreme bandwidth (wavelength 
multiplexing)  Hype: future terahertz CPUs via optical communication on chip   Current explosion in 
information technology has been derived from our ability to control the flow of electrons in a semiconductor 
in the most intricate ways. Photonic crystals promise to give us similar control over photons -with even 
greater flexibility because we have far more control over the properties of photonic crystals than we 
do over the electronic properties of semiconductors. Negative Refractive Index  Left-handed material 
 Refractive index N = +/-sqrt(eµ)  Usual matter, both permittivity e and permeability µ are positive 
 NIM: both e and µ are negative Usually over a limited wavelength band  Incident and refracted waves 
are on the same side of the normal to the boundary  The electric field, magnetic field and wave vector 
follow a left-hand rule   Applications  Perfect Lens  A planar slab of NIM,  Imaging beyond diffraction 
limits  No glare (lens reflection)   Sub-wavelength imaging   Implementations  Metamaterials  
So far only in lower frequencies, microwave  Array of wires and spring-ring resonators  Carefully designed 
photonic crystal (glass rods in air)   All transparent or translucent materials that we know of possess 
positive refractive index-­a refractive index that is greater than zero. However, is there any fundamental 
reason that there should not be materials with negative refractive index? Perfect Lens* using Photonic 
Crystal Slab  Rods in Air http://www.ee.duke.edu/~drsmith/nim_pubs/pubs_2004/bliokh_physics_uspekhi_2004.pdf 
http://www.ee.duke.edu/~drsmith/nim_pubs/pubs_2004/he_prb_2004.pdf Refraction of a plane wave at the 
interface of a left-handed medium and a right-handed one looks quite unusual. The fact that the incident 
and refracted waves are on the same side of the normal to the boundary enables one to manufacture quite 
unusual optic elements of left-handed media. For instance, a plane-parallel plate made of a left-handed 
material works as a collecting lens. Optics: Computable Extensions Unusual Optics  Wavefront Coding 
 Folded Optics  Nonlinear Optics   Material (Refractive Index)  Graded Index (GRIN) materials  
Photonics crystals  Negative Index   Imaging  Schlieren Imaging  Agile Spectrum Imaging  Random 
Lens Imaging   Animal Eyes  What can we learn Schlieren Photography  Image of small index of refraction 
gradients in a gas  Invisible to human eye (subtle mirage effect)  Collimated Light Knife edge blocks 
half the light unless distorted beam focuses imperfectly  Changes in the index of refraction of air 
are made visible by Schlieren Optics. This special optics technique is extremely sensitive to deviations 
of any kind that cause the light to travel a different path. In schlieren photography, the collimated 
light is focused with a lens, and aknife-edge is placed at the focal point, positioned to block about 
half the light. In flow of uniform density this will simply make the photograph half as bright. However 
in flow with density variations the distorted beam focuses imperfectly, and parts which have focussed 
in an area covered by the knife-edge are blocked. The result is a set of lighter and darker patches corresponding 
to positive and negative fluid density gradients in the direction normal to the knife-edge. Clearest 
results are obtained from flows which are largely two-dimensional and not volumetric. Full-Scale Schlieren 
Image Reveals The Heat Coming off of a Space Heater, Lamp and Person Full-Scale Schlieren Image Reveals 
A Gas Leak. For amateur photography, the setup is surprisingly simple and one can record stunning images. 
 Rainbow plane (t=tR)  C Control the spectral sensitivity of the B sensor by placing an appropriate 
grayscaleA masks in the R-plane.    What if the input to output relationship of the light rays were 
randomized? How could you use such a camera? What would be the benets and the drawbacks of such an approach? 
The ability to use a camera with those characteristics would open new regions of the space of possible 
optical designs, since in many cases it is easier to randomize light ray directions than to precisely 
direct them. Authors show applications in super-resolution and depth estimation. Community and Social 
Impact Ramesh Raskar Community and Social Impact Crowdsourcing  Object Recognition  CMU's captcha-like 
games  MIT s LabelMe  Distributed Search   Cross-sources Image Visualization  Google/Virtual Earth 
problems  From street maps to street-level photos to 3D models   Trust and Privacy  Verification 
and Forensics  Privacy-preserving Computation   Mobile Phones  ZoneSurger: social tagging of photos 
 Govt forms in developing counties   Social/Political Goals  Crowd Sourcing Get Help from Crowd 
for Tasks That Tax Computers Object Recognition/Labeling  Image-based Applications  Fight Spam using 
CAPTCHAs  Completely Automated Turing Test To Tell Computers and Humans Apart  reCAPTCHAs For OCR 
of old text  LabelMe Segmentation and object recognition  Distributed Search   Crowdsourcing is 
a new online strategy for converting a task difficult for computers and too expensive for a team of dedicated 
humans, and outsourcing it to an undefined, generally large group of people, in the form of an open call. 
Thanks to Web2.0 online technologies make participation of thousands of synchronous or asynchronous users 
possible to solve a talk. Example: Digitizing Old Books  Optical Character Recognition for Old Text 
 reCAPTCHA project at CMU  Human solves CAPTCHA and solves difficult OCR problems http://recaptcha.net/ 
http://en.wikipedia.org/wiki/Captcha http://news.bbc.co.uk/2/hi/technology/7023627.stm The CMU team is 
involved in digitising old books and manuscripts supplied by a non-profit organisation called the Internet 
Archive, and uses Optical Character Recognition (OCR) software to examine scanned images of texts and 
turn them into digital text files which can be stored and searched by computers. But the OCR software 
is unable to read about one in 10 words, due to the poor quality of the original documents. Computers 
cannot read words as easily as humans The only reliable way to decode them is for a human to examine 
them individually -a mammoth task since CMU processes thousands of pages of text every month. To solve 
this problem the team takes images of the words which the OCR software can't read, and uses them as CAPTCHAs. 
These CAPTCHAs, known as reCAPTCHAS, are then distributed to websites around the world to be used in 
place of conventional CAPTCHAs. When visitors decipher the reCAPTCHAs to gain access to the web site, 
the answers -the results of humans examining the images -are sent back to CMU. Every time an Internet 
user deciphers a reCAPTCHA, another word from an old book or manuscript is digitised. LabelMe Bryan 
Russell, Antonio Torralba and William T. Freeman at MIT Recognition performance increases dramatically 
when more labeled training data is made available  Goal: Create a massive high quality database for 
research on object recognition.  Multiple users label as many objects and regions as they can  within 
the same image. http://labelme.csail.mit.edu/ The authors seek to build a large collection of images 
with ground truth labels to be used for object detection and recognition research. Such data is useful 
for supervised learning and quantitative evaluation. To achieve this, they developed a web-based tool 
that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, 
they have collected a large dataset that spans many object categories, often containing multiple instances 
over a wide variety of images. They quantify the contents of the dataset and compare against existing 
state of the art datasets used for object recognition and detection. Also, they show how to extend the 
dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering 
of objects in a scene, and increase the number of labels using minimal user supervision and images from 
the web. Distributed Patch-wise Image Search  Example: Steve Fossett s plane, 2007  Divide and Conquer 
 Hires imagery from DigitalGlobe  Amazon s Mechanical Turk splits into small patches  Volunteers each 
review individual patches  Report back and aggregate info for professionals   http://www.wired.com/software/webservices/news/2007/09/distributed_search 
The search for aviator Steve Fossett, whose plane went missing in Nevada in 2007, in which up to 50,000 
people examined high-resolution satellite imagery from DigitalGlobe that was made available via Amazon 
Mechanical Turk. The helpers are issued squares that represent 278-foot-square pieces of the search area. 
If they see something worth closer study, participants flag it. Since each square is issued to 10 different 
people, squares that are flagged by several volunteers are given greater scrutiny. User Generated Content 
Visualization http://phototour.cs.washington.edu/ Google Map overlayed with geo-tagged photos  Image-based 
Mashups  Microsoft Photosynth and U-Washington s Phototourism software takes a large collection of 
photos of a place or an object, analyzes them for similarities, and then displays the photos in a reconstructed 
three-dimensional space, showing you how each one relates to the next. New options on Google Maps allows 
users to post and view populated map with geo­tagged photos provided by Panoramio. Mobile Photography 
 ZoneSurfer  (Yahoo Research)  Spatial -social -topical mobile photo browser  Mobile window to the 
world of multimedia  Social interface based on Flickr   Mobile phone-based entrepreneurship  Developing 
countries  Many examples: http://nextbillion.mit.edu/    Zurfer from Yahoo Research uses channel 
metaphor to give users contextual access to media of interest according to key dimensions: spatial, social, 
and topical. Elements of social interaction and communication aroundcthe photographs are built into the 
mobile application, to increasecuser engagement. The application utilizes Flickr.comc as an image and 
social-network data source (From http://yahooresearchberkeley.com/blog/wp­content/uploads/2007/09/sp50a-hwang.pdf) 
Developing Countries: CAMForms Paper forms with barcodes  83-bit 2D codes (including seven bits of 
error correction)   Parikh (2005) The camera phone provides an easy interface to fill in and verify 
government forms. The paper form is printed with 2D bar-codes which are decoded by camera phone and info 
is transmitted to a central location. Israeli Information Center for Human Rights in the Occupied Territories 
captures photos of events. From their website: Goal is to Document and educate Israeli public and policymakers 
about human rights violations in Occupied Territories. Second goal is to combat phenomenon of denial 
prevalent among Israeli public. They hope to create human rights culture in Israel    Course: Computational 
Photography Advanced Topics  Course Page : http://computationalphotography.org/ 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401163</section_id>
		<sort_key>310</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Don't be a WIMP]]></section_title>
		<section_page_from>8</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098670</person_id>
				<author_profile_id><![CDATA[81100323279]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Johannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Behr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098671</person_id>
				<author_profile_id><![CDATA[81100095238]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dirk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reiners]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401164</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>170</pages>
		<display_no>24</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Class notes: don't be a WIMP]]></title>
		<subtitle><![CDATA[(http://www.not-for-wimps.org)]]></subtitle>
		<page_from>1</page_from>
		<page_to>170</page_to>
		<doi_number>10.1145/1401132.1401164</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401164</url>
		<abstract>
			<par><![CDATA[<p>Virtual and augmented reality have been around for a long time, but for most people they are movie fantasies. Very few people outside a few research labs have worked with or experienced these systems for themselves. On the other hand, interactive 3D graphics applications are ubiquitous, mostly in the form of games. More and more people are working in animation and games, creating models and programs for interactive 3D applications on standard monitors.</p> <p>The goal of this class is to demonstrate that the leap to actual immersive or augmented environments is not as big as you might think. It explains how high-powered 3D graphics cards, mainstream applications of stereoscopic displays in 3D TV and movies, and webcams that achieve TV-quality images have significantly lowered the barriers to entry. And how, in combination with those hardware advances, freely available software based on open standards like X3D provides all the tools you need to access the elusive world of virtual and augmented reality applications. Following a summary of the basic principles of stereo displays, tracking systems and post-WIMP interaction metaphors, the main part of the course is a practical introduction to creating and running your own interactive and immersive applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098672</person_id>
				<author_profile_id><![CDATA[81100323279]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Johannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Behr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunofer IGD, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098673</person_id>
				<author_profile_id><![CDATA[81100095238]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dirk]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reiners]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Louisiana at Lafayette]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>601673</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin, and C. T. Silva. Point set surfaces. <i>IEEE Visualization 2001</i>, pages 21--28, October 2001. ISBN 0-7803-7200-x.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Marc Alexa. Linear geometry interpolation in opensg, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Marc Alexa and Johannes Behr. Cooperative VR enviroment. <i>brasil</i>, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Marc Alexa and Johannes Behr. Volume Rendering in VRML. <i>Web3D - VRML 2001 Proceedings</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Marc Alexa and Johannes Behr. Fast and Effective Striping. <i>1. OpenSG Symposium OpenSG, 2002, Darmstadt</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Marc Alexa and Johannes Behr. Linear Geometry Interpolation in OpenSG. <i>1. OpenSG Symposium OpenSG</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Marc Alexa, Johannes Behr, Daniel Cohen-Or, Shachar Fleishman, David Levin, and Claudio T. Silva. Computing and rendering point set surfaces.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>330172</ref_obj_id>
				<ref_obj_pid>330160</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Marc Alexa, Johannes Behr, and Wolfgang M&#252;ller. The morph node. <i>Web3D - VRML 2000 Proceedings</i>, pages 29--34, 2000. ISBN 1-58113-211-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>504514</ref_obj_id>
				<ref_obj_pid>504502</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Althoff, Stocker, and McGlaun. A Generic Approach for Interfacing VRML Browsers to Various Input Devices and Creating Customizable 3D Applications. <i>Web3D</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274369</ref_obj_id>
				<ref_obj_pid>271897</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Y. Araki. A High-level Multi-user Extension Library For Interactive VRML Worlds. <i>VRML</i>, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Avalon. Avalon. http://www.zgdv.de/avalon, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. Behr, SM. Choi, S. Gro&#223;kopf, MH, and G. Sakas. Modelling, visualization, and interaction techniques for diagnosis and treatment planning in cardiology. <i>Computers &amp; Graphics</i>, Vol 24.5:741--753, 2000. ISSN 0097--8493.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Johannes Behr and Marc Alexa. Fast and effective striping. 1. OpenSG Symposium, Darmstadt, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Johannes Behr and Patrik D&#228;hne. AVALON: Ein komponentenorientiertes Rahmensystem f&#252;r dynamische Mixed-Reality Anwendungen. <i>TUD thema Forschung</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Johannes Behr, Jorge A. Diz, and Marcelo G. Malheiros. An Extensible Interactive Image Synthesis Environment. <i>echnical report DCA-006/97 - DCA, FEEC, Unicamp</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Johannes Behr and Andreas Froehlich. AVALON, an Open VRML VR/AR system for Dynamic Application. <i>Topics</i>, 1(1):28, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Johannes Behr, Torsten Froehlich, Christian Knoepfle, Bernd Lutz, Dirk Reiners, Frank Schoeffel, and Wolfram Kresse. The Digital Cathedral of Siena - Innovative Concepts for Interactive and Immersive Presentation of Cultural Heritage Sites. <i>ICCHIM Conference Proceedings, Milan</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Johannes Behr and Axel Hildebrand. Sanare - VR Med enviroment. <i>Topics</i>, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Johannes Behr and Marc Niemann. Interactive Volume Data Rendering for Medical VR Applications. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Johannes Behr, Choi Soo-Mi, and Stefan Gro&#223;kopf. 3D Modellierung zur Diagnose und Behandlungsplanung in der Kardiologie. <i>Der Radiologe</i>, 40(3):256--261, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Allan Bierbaum, Albert Baker, Carolina Cruz-Neira, Patrick Hartling, Christopher Just, and Kevin Meinert. VR Juggler: A Virtual Platform for Virtual Reality Application Development. Master's thesis, Iowa State University, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Roland Blach, Juergen Landauer, Angela Roesch, and Andreas Simon. A flexible prototyping tool for 3d realtime user-interaction. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>296488</ref_obj_id>
				<ref_obj_pid>296477</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Roland Blach, J&#252;rgen Landauer, Angela R&#246;sch, and Andreas Simon. A Highly FlexibleVirtual Reality System. <i>Future Generation Computer Systems</i>, 14(3--4):167--178, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[X3D Consortium. X3d standard. http://www.web3d.org/x3d/, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Matthew Conway, Randy Pausch, Rich Gossweiler, and Tommy Burnette. Alice: A rapid prototyping system for building virtual environments. 2:295--296, April 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166134</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Carolina Cruz-Neira and Daniel J. Sandin. Surround-Screen Projection-Based Virtual Reality: The Design and Implementation of the CAVE. <i>ACM Computer Graphics, SIGGRAPH 93</i>, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274368</ref_obj_id>
				<ref_obj_pid>271897</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Paul J. Diefenbach, Daniel Hunt, and Prakash Mahesh. Building openworlds. <i>Web3D - VRML 1998 Proceedings</i>, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[N. I. Durlach and A. S. Mavor. Virtual Reality: Scientific and Technological Challenges. <i>National Academy Press.</i>, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794878</ref_obj_id>
				<ref_obj_pid>794192</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Thorsten Fr&#246;hlich, Johannes Behr, and Peter Eschler. Cybernarium Days 2002 - A Public Experience of Virtual and Augmented Worlds. <i>First International Symposium on Cyber Worlds 2002</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Philippe Coiffet Grigore C. Burdea. <i>Designing Virtual Reality Systems: The Structured Approach</i>. Wiley-IEEE Press, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[H-Anim. ISO/IEC FCD 19774; Humanoid animation Specification. http://www.h-anim.org, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Roger Hubbold, Jon Cook, Martin Keates, Simon Gibson, Toby Howard, Alan Murta, and Adrian West. Gnu/maverik a micro-kernel for large-scale virtual environments, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835898</ref_obj_id>
				<ref_obj_pid>580130</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[John Kelso, Lance E. Arsenault, Ronald D. Kriz, and Steven G. Satterfield. DIVERSE: A Framework for Building Extensible and Reconfigurable Device Independent Virtual Environments. <i>IEEE Virtual Reality Conference</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1088852</ref_obj_id>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Gerard Kim. <i>Designing Virtual Reality Systems: The Structured Approach</i>. SpringerVerlag, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Blair MacIntyre. A Touring Machine: Prototyping 3D Mobile Augmented Reality Systems for Exploring the Urban Environment. <i>ISWC</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897872</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Steve Molnar, Michael Cox, David Ellsworth, and Henry Fuchs. A sorting classification of parallel rendering. Technical Report TR94-023, 8, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>182469</ref_obj_id>
				<ref_obj_pid>182466</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Steve Molnar, Michael Cox, David Ellsworth, and Henry Fuchs. A Sorting Classification of Parallel Rendering. <i>IEEE Computer Graphics and Applications</i>, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[OpenAL. OpenAL Specification and Documentation. http://www.openal.org/, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[OpenSceneGraph. OpenSceneGraph documenten. http://www.openscenegraph.org, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Wayne Piekarski, Bruce Thomas, David Hepworth, Bernard Gunther, and Victor Demczuk. An architecture for outdoor wearable computers to support augmented reality and multimedia applications. 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Dirk Reiners. Opensg: Basic concepts.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569679</ref_obj_id>
				<ref_obj_pid>569673</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Dirk Reiners, Gerrit Voss, and Johannes Behr. A Multi-thread Safe Foundation for Scene Graphs and its Extension to Clusters. <i>Eurographics Workshop on Parallel Graphics and Visualisation 2002. Proceedings</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Dirk Reiners, Gerrit Voss, and Johannes Behr. OpenSG - Basic Concepts. <i>First OpenSG Symposium OpenSG, 2002, Darmstadt</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Patrick Reuter, Johannes Behr, and Marc Alexa. An improved adjacency data structure for efficient trianglestripping. <i>accepted for publication in the Journal of Graphics Tools</i>, To appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[J. Rohlf and J. Helman. IRIS Performer: A high performance toolkid for real-time 3D graphics. <i>ACM Computer Graphics, SIGGRAPH 94</i>, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Mark Segal, Akeley Kurt, Chris Frazier, and Jon Leech. Opengl Specification. http://www.opengl.org/, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581839</ref_obj_id>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[William R. Sherman and Alan B. Craig. <i>Understanding Virtual Reality: Interface, Application, and Design</i>. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[R. Stiles, S. Tewari, and M. Mehta. Adapting VRML For Free-form Immersed Manipulation. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253462</ref_obj_id>
				<ref_obj_pid>253437</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[R. Stiles, S. Tewari, and M. Metha. Adapting VRML 2.0 for Immersive Use. <i>VRML 97, Second Symposium on the Virtual Reality Modeling language</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134089</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[P. S. Strauss and R. Carey. An object-oriented 3D graphics toolkit. <i>ACM Computer Graphics</i>, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>273030</ref_obj_id>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[C. Szyperski. Component Software, Beyond Objekt-Oriented Programming. <i>ACM Press.</i>, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[H. Tramberend. Avocado - a distributed virtual environment framework. http://www.ercim.org/publication/Ercim_News/enw38/tramberend.htm, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>971169</ref_obj_id>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[John Vince. <i>Introduction to Virtual Reality</i>. SpringerVerlag, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[W3C. Xml Protocol Working Group, sOAP Version 1.2 Specification. http://www.w3.org/2000/xp/Group/, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Wikipedia. Virtual reality. http://en.wikipedia.org/wiki/Virtual_reality, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Wikipedia. Virtual reality. http://en.wikipedia.org/wiki/Augmented_reality, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[WorldToolKit. Sense8 Corporation; WorldToolKit: Virtual Reality Support Software. 4000 Bridgeway Suite 101, Sausalito, CA 94965, telephone: (415) 331--6318., 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[S. Ting Wu and Johannes Behr. An Extensible Interactive Image Synthesis Environment. <i>XXIV Semish proceedings</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2008 Class Notes: Don t be a WIMP (http://www.not-for-wimps.org) Johannes Behr1 Dirk Reiners2 
Fraunofer IGD University of Louisiana at Lafayette Germany USA June 3, 2008 1johannes.behr@igd.fraunhofer.de 
2dirk@lite3d.com Class Description Virtual and augmented reality have been around for a long time, but 
for most people they are movie fantasies. Very few people outside a few research labs have worked with 
or experienced these systems for themselves. On the other hand, interactive 3D graphics applications 
are ubiquitous, mostly in the form of games. More and more people are working in animation and games, 
creating models and programs for interactive 3D applications on standard monitors. The goal of this class 
is to demonstrate that the leap to actual immersive or augmented environ­ments is not as big as you might 
think. It explains how high-powered 3D graphics cards, mainstream applications of stereoscopic displays 
in 3D TV and movies, and webcams that achieve TV-quality images have signi.cantly lowered the barriers 
to entry. And how, in combination with those hardware advances, freely available software based on open 
standards like X3D provides all the tools you need to access the elusive world of virtual and augmented 
reality applications. Following a summary of the basic principles of stereo displays, tracking systems 
and post-WIMP interaction metaphors, the main part of the course is a practical introduction to creating 
and running your own interactive and immersive applications. Prerequisites Basic knowledge of computer 
graphics. Understanding of what polygons, lights, and cameras are. Helpful but not required: graphics 
programming or 3D animation experience. This class is intended for attendees who are interested in interactive 
3D graphics and might want to move beyond the WIMP (Window, Icon, Menu, Pointer) environment. Instructors 
Johannes Behr, Fraunhofer IGD, Germany Johannes Behr leads the VR group at the Fraunhofer Institut für 
Graphische Datenverarbeitung in Darmstadt, Germany. His areas of interest focus on virtual reality, computer 
graphics and 3D inter­action techniques. Most of the results of his recent work are available as part 
of the InstantReality Framework. He has an MS from the University of Wolverhampton and received his PhD 
from the Technische Universität Darmstadt. Dirk Reiners, University of Louisiana at Lafayette, US Dirk 
Reiners is a faculty member in the Center for Advanced Computer Studies (CACS) at the Uni­versity of 
Louisiana at Lafayette. His research interests are in interactive 3D graphics and software systems to 
make building 3D applications easier. He has an MS and a PhD from the Technical Technische Universität 
Darmstadt and is the project lead for the OpenSG Open Source scenegraph. Contents 1 Introduction 5 1.1 
WhatisVR/AR ........................................ 5 1.2 BuildingVR/ARApplications ................................ 
6  2 X3D as a Basic Technolgy 7 2.1 UtilizingX3DforVR/ARDevelopment ........................... 7 2.2 
RelatedWork ......................................... 8 2.3 HelloworldinX3D ...................................... 
8 2.3.1 Whatweneed .................................... 9 2.3.2 Howwedoit ..................................... 
9 2.3.2.1 Understandingthecode .......................... 9 2.3.3 Showthewords ................................... 
9 2.4 CreatingX3DApplications .................................. 10 2.4.1 RelationtoX3D .................................... 
10 2.4.2 X3DSourcestoread ................................. 10 2.4.3 X3DConformance .................................. 
10 2.4.4 GrowingnumberofNodesandComponents . . . . . . . . . . . . . . . . . . . 10 2.4.5 Mimetypesandencodings 
.............................. 11 2.5 Getyourengineright ..................................... 12 2.5.1 
Scene......................................... 12 2.5.2 Engine......................................... 
12 2.5.3 ContextSetup ..................................... 12 2.5.4 TheScenenodeinclassicencoding 
........................ 14  3 MultipleViews and Stereo 16 3.1 Rendering .......................................... 
16 3.2 StereoBasics ......................................... 17 3.2.1 Depthperceptionofhumaneyes .......................... 
17 3.2.1.1 Monoculardepthperception ....................... 17 3.2.1.2 Binoculardepthperception ........................ 
17 3.2.2 DepthgenerationinVR ............................... 18 3.2.3 Eyeseparation .................................... 
19 3.3 MultipleWindowsandViews ................................ 21 3.3.1 Enginecon.guration ................................. 
21 3.3.2 Multipleviewareas .................................. 22 3.3.3 Multiplewindows ................................... 
23 3.4 ActiveStereo ......................................... 24 3.4.1 Hardware ....................................... 
24 3.4.2 Stereomodi.er .................................... 24 3.4.3 QuadBufferStereo ................................. 
25 3.5 PassiveStereo ........................................ 26 3.5.1 Stereomodi.er .................................... 
27 3.5.2 Stereobyoverlappingviewareas .......................... 27 3.5.3 Stereobyseparateviewareas 
........................... 28  4 Interaction and Devices 31 4.1 Interaction ........................................... 
31 4.1.1 Low-LevelSensors .................................. 31 4.1.2 High-LevelSensors ................................. 
33 4.2 Input/Outputstreams ..................................... 34 4.2.1 Introduction...................................... 
34 4.2.2 IOSensorbasics ................................... 34 4.2.3 ImplicitServicede.nition .............................. 
34 4.2.4 ExplicitServicede.nitions .............................. 34 4.2.4.1 ParameterofServices ........................... 
35 4.2.4.2 IOSlotsofServices ............................ 35 4.2.5 Conclusion ...................................... 
35 4.3 ConnectingDevices ..................................... 36 4.3.1 Introduction...................................... 
36 4.3.2 Findingyourslots .................................. 36 4.3.2.1 Connectyourdevicetothelocalmachine 
. . . . . . . . . . . . . . . . 36 4.3.2.2 StartthecorrectDevice-handler ..................... 37 4.3.2.3 
Getalistofallout-slots .......................... 38 4.3.3 UsetheinformationtostartanIOSensor . . . 
. . . . . . . . . . . . . . . . . . 38 4.3.4 Conclusion ...................................... 42 4.4 
Space-Mouse/Navigator/PilotDevice ............................ 42 4.4.1 Introduction...................................... 
42 4.4.2 StartthecorrectdeviceHandler .......................... 42 4.4.2.1 USBDeviceonWindows ......................... 
43 4.4.2.2 USBDeviceonOSX ........................... 43 4.4.2.3 Serial-DeviceonallSystems ....................... 
44 4.4.3 ControllingtheApplication .............................. 45 4.5 Navigator ........................................... 
45 4.5.1 Introduction...................................... 45 4.5.2 Gettingthedata-theIOSensor ........................... 
45 4.5.3 Movingaround-theNavigator3D .......................... 45 4.6 ImmersivePointingSensorInteraction 
........................... 47 4.6.1 Desktopbasedinteraction .............................. 47 4.6.2 
Fullyimmersiveinteraction ............................. 47 4.7 VisionMarkerTracking .................................... 
49 4.7.1 Introduction...................................... 49 4.7.2 IOSensor ....................................... 
49 4.8 VisionTrackingDevice .................................... 51 4.8.1 Introduction...................................... 
51 4.8.2 TrackinginGeneral ................................. 51 4.8.3 TheExample ..................................... 
51 4.8.4 Modi.cations ..................................... 54 4.8.4.1 VideoSource................................ 
54 4.8.4.2 Marker ................................... 55 4.9 AppleSuddenMotionSensor ................................ 
56 4.9.1 Introduction...................................... 56 4.9.2 Shaking........................................ 
56 4.9.3 Tilt........................................... 56 4.10SerialCommunication .................................... 
57 4.10.1Introduction ...................................... 57 4.10.2Settinguptheserialport .............................. 
57 4.10.3SendingDatatotheSerialPort ........................... 58 4.10.4ReceivingDatafromtheSerialPort 
........................ 58 4.10.5ExampleScene .................................... 58  5 Clustering 
61 5.1 ClusterBasicsandLoadBalancing ............................. 61 5.1.1 Topologies ...................................... 
61 5.1.2 Loadbalancing .................................... 62 5.1.2.1 Imagespacebalancing(Sort-First) 
. . . . . . . . . . . . . . . . . . . 62 5.1.2.2 Geometry based balancing (Sort-Last) . . . . . . . . 
. . . . . . . . . 63 5.1.3 Networkcon.guration ................................ 64 5.1.4 InstantPlayerandInstantCluster 
.......................... 64 5.1.5 TechnicalDetails ................................... 64 5.2 CAVEcluster 
......................................... 64 5.2.1 AspectsofaCAVE .................................. 
64 5.2.2 Assumptions ..................................... 65 5.2.3 Settinguptheviews ................................. 
65 5.2.4 Loadbalancing .................................... 71 5.2.5 Headtracking ..................................... 
71 5.3 Singledisplaycluster ..................................... 73 5.3.1 SingleDisplaycluster ................................ 
73 5.4 Multipledisplaycluster .................................... 74 5.4.1 Multidisplaycluster ................................. 
74 5.4.1.1 Differentconcepts ............................. 74 5.4.1.2 UsingClusterWindow ........................... 
75 5.4.1.3 UsingTiledClusterWindow ........................ 75 5.4.2 Loadbalancing .................................... 
77 5.4.3 Multidisplaystereocon.guration .......................... 77  6 Scripting 80 6.1 Scripting:Java 
........................................ 80 6.1.1 Introduction...................................... 
80 6.1.2 SettinguptheSceneandtheScript ........................ 80 6.1.2.1 SettingupaJavaClass .......................... 
80 6.1.2.2 GettingValuesfromtheScene ...................... 81 6.1.2.3 WritingbackValuestotheScene 
.................... 81  7 Animation 83 7.1 Followers ........................................... 83 
 7.1.1 Introduction...................................... 83 7.1.2 PositionChaser3D .................................. 
83 7.2 Steeringbehaviourbasics .................................. 84 7.2.1 Whataresteeringbehaviours? 
........................... 84 7.2.2 Thesteeringsytem .................................. 85 7.2.3 Addingvehicles 
.................................... 85 7.2.3.1 Parameterizingthevehicle ........................ 85 
 7.2.4 Addingbehaviourstothevehicles ......................... 86 7.2.5 Updatingthevehicles ................................ 
86 7.2.6 Idon tseeanything!? ................................ 87 7.2.7 Movingsomeboxes ................................. 
87 7.2.8 Debuggingvehiclesandbehaviours ........................ 87 7.2.8.1 Vehicle soutputOnly.elds 
........................ 87 7.3 Humanoidanimation ..................................... 87 7.3.1 Overview 
....................................... 88 7.3.2 Animation....................................... 88 
 7.3.3 Morphing ....................................... 92 8 Conclusion 94 Chapter 1  Introduction 
This document contains additional material and tutorials to support and simplify different aspects of 
he VR/AR application development process. It s not about basic visualisation or tracking methods but 
about building applications based on available results, standards and systems. The target audience are 
people interested in using the technology rather than developing it. 1.1 What is VR/AR We give some basic 
de.nitions for VR/AR but these are very limited and de.ning this topics is not the goal of the course, 
partially because even though the topics have been around for a long time there is no commonly agreed 
de.nition. The goal of the course is to give people the ability to start building interactive, immersive 
applications. Basic de.nitions of AR and VR can be found in the literature ([47, 53, 34, 30]) or online 
([55, 56]). Most of the literature de.nes some basic elements which are critical but variable for all 
VR/AR applications: Virtual Content The content of the medium de.nes the virtual world. This imaginary 
space, man­ifested through a medium, de.ning any collection or number of objects in a space and the relations 
and rules in the corresponding simulation. Immersion Being mentally and physically immersive are important 
criteria for VR applications. Whereby physical immersion is a de.ning characteristic of virtual reality 
and mental immersion is proba­bly the goal of most media creators. Therefore it is important that there 
is a synthetic stimulus of the body s senses via the use of technology. This does not imply all senses 
or that the entire body is immersed/engulfed. Most systems focus on vision and sound. Some include touch 
and haptics, generally known as force feedback. Other senses are much harder to stimulate by a computer 
and are only touched in very few research environments. Sensory feedback Sensory feadback is an ingredient 
essential to virtual reality. The VR system provides direct sensory feedback to the participants based 
on their physical position. In most cases it is the visual sense that receives the most feedback, as 
it is also the sense that brings most of the information from the environment into the human system anyway. 
Interactivity For VR to seem authentic, it must respond to user actions, namely, be interactive. The 
system must produce sensory feedbacks according to the user action. Most systems give visual feedback 
with an updaterate from at least 30 times per second. More is desirable, and immersion gets lost when 
the time lag between user actions and sensable reactions exceeds 100 ms. There is not a single interaction 
and navigation method or device that would de.ne VR nor AR. There is no Window, Icon, Menu, 2D-Pointer 
de.ning a single methoper but every application de­signer is free to choose whatever is most attractive 
or appropriate for the current set of goals. This gives the developer on the one hand a lot of freedom 
but on the other hand asks for a new set of development tools, standards and methods. 1.2 Building VR/AR 
Applications Early Virtual Reality (VR) applications where mainly focusing on "virtual prototypes", "ergonomic 
evaluation", "assembly-disassembly" and "design review". The automotive industry was one of the driving 
forces of Virtual and Augmented Reality and today this technology is used in many application areas. 
The automotive industry learned how to use VR successfully, saving time and costs by building and evaluating 
virtual prototypes. But in fact, only large companies could take advantage of virtual reality, because 
of the high initial hardware and software costs. Consequently small companies were not using VR/AR and 
also for many application domains, using VR/AR was not pro.table. Things have changed signi.cantly, and 
today the initial costs of the main expense factors, the computer graphics hardware and projection systems, 
are at least an order of magnitude lower than 5-10 years ago. Nearly every modern PC is equipped with 
a 3D-graphics card (e.g. nvidia Geforce, ATI Radeon), which is able to deliver graphics performance easily 
outperforming the high end graph­ics systems from last decade (e.g. Silicon Graphics Onyx systems), which 
were built to run VR/AR applications. Furthermore VR/AR systems have evolved a lot: Abstract device management 
is com­monplace today, there are some well accepted base technologies, e.g. X3D, and the introduction 
of component models offer great .exibility, allowing application designers to tackle a wide variety of 
application domains with a single software system (all-purpose), including Augmented Reality (AR) based 
applications. Chapter 2  X3D as a Basic Technolgy There is no standard method or tool for VR/AR application 
development. This course material and tutorials are based on X3D as one of the few widely accepted industry 
standards in the .eld and the InstantReality framework, because it is freely available and supports the 
wide variety of applications and technologies that are covered in this course. 2.1 Utilizing X3D for 
VR/AR Development The InstantReality toolkit (IR) [16, 11] is an open/free environment for VR/AR applications 
developed at the Institute for Computer Graphics (IGD) in Darmstadt, Germany, which utilizes the X3D 
ISO standard as application description language. One of the main reasons for starting the project was 
the ever present need for a faster and more ef.cient application development and teaching tool. Therefore, 
the goal was to de.ne abstractions for interaction and behaviour descriptions, which work well for VR 
and AR applications but which is also suitable for beginnes Like most traditional toolkits, IR uses a 
scene-graph to organize the data, for spatial as well as logical relations. In addition to the scene 
description, a VR/AR application needs to deal with dynamic behaviour of objects, and the user interaction 
via the available input devices. The major drawback of traditional VR/AR-toolkits is the fact that the 
application developer has to implement a controller to handle all aspects of both behaviour and interaction 
for most applications anew. To overcome this drawback, modern component design models providing techniques 
like connection oriented programming [51] were employed for IR. The basic idea is that the system should 
provide a component-framework which allows the user to load and wire different components to ful.ll the 
widest possible variety of different application requests. Instead of designing another application model, 
we adopted the basic idears of the X3D ISO standard [24]. The X3D node is not just a static graph-element 
but de.nes a state, state-changes, and in­put/output slots for every node. Application development is 
done by instantiating and wiring nodes, which are provided by the system. The use of X3D as an application 
programming language leads to a number of advantages over a proprietary language: It is integral to 
an ef.cient development cycle to have access to simple yet powerful scene development tools. With X3D, 
the application developer can use a wide range of systems for modelling, optimizing and converting, as 
the X3D standard is supported by most major modelc reation tools.  The interface is well de.ned by a 
company-independent ISO standard.  Due to platform independence, development and testing can even be 
done on standard desktop computers.  X3D and the used scripting language based on the widely used ECMAScript 
(aka JavaScript) are much easier to learn and teach than the low level interfaces often provided by traditional 
VR/AR toolkits.  There is a growing number of CAD and simulation packages which export static and dynamic 
X3D worlds.  There is a great number of books and tutorials available.  From an application developer 
s point of view, the IR runtime environment acts and feels very much like a web-client based browser. 
However, since our goal was not to build another X3D web client but a full feature VR environment, the 
system includes some additional aspects not found in traditional X3D clients: The X3D speci.cation includes 
only high level sensors. The X3D sensors recognize that the user has touched or turned some part of the 
world but do not de.ne how a speci.c input device or user action has to be handled. The X3D speci.cation 
of high level sensors has to be reinterpreted for immersive environments. Additionally, the system must 
provide low level sensors that stream data (e.g. stream of .oat values) from the real to the virtual 
world and vice versa.  The system utilizes the concept of nodes and routes not only for the scene itself 
but also for the browser con.guration and any other dynamic aspect. Every dynamic component (e.g. Job, 
Window) in IR is a node with .elds, communicating with slots living in a speci.c namespace and hierarchy. 
The namespace type de.nes the use and the types of nodes that can live in the namespace. The route mechanism 
not only de.nes the communication channels directly, but also the thread paths used to parallelize the 
system execution indirectly.  The VRML standard allows the developer to create prototypes with behaviour 
scripting in Java or JavaScript/ECMAScript. In order to achieve maximum performance, we need the ability 
to extend the node pool with native C++ implementations and fetch these nodes without recom­piling or 
linking the toolkit.  We should provide standard interfaces (e.g. HTTP, SOAP) so that other software 
packages (e.g. simulation packages) can utilize IR just like a visualisation and interaction service 
provider.  We tried to address these different requirements in the design and development of the IR 
system and present and discuss some of our results in this paper. 2.2 Related Work These days, there 
are very few immersive VR applications which are really built from scratch, just using a hardware abstraction 
library for graphics (e.g. OpenGL [46]) and sound (e.g. OpenAL [38]). Most systems utilize at least a 
scene graph library like OpenSG [43], OpenSceneGraph [39], Per­former [45] or Inventor [50]. Most VR 
frameworks include abstractions for input devices and multi-screen and stereo setups [21, 32, 33]. But 
there are only very few, which really provide abstractions for behaviour and ani­ mation description 
[25, 57]. However, there are some VR Systems which even provide connection oriented programming techniques 
as part of the application development model: the Avocado [52] and Lightning [23] systems are VR systems 
using the concepts of routes for internode communica­ tion to de.ne the behaviour graph, but are not 
related to VRML or X3D. Both systems are based on the Performer [45] library and de.ne a scene-graph 
behaviour graph, which is very closely related to the Performer rendering system. The OpenWorld [27] 
system is a VRML based development sys­ tem, which also provides support for stereo views, but is not 
a full feature VR-System with various abstractions for input and output systems. Stiles et al. [49] adapted 
VRML for immersive use and was especially concerned about sensor and viewpoint handling in immersive 
environments. Our work on low level and high level sensors is partly based on his results. 2.3 Hello 
world in X3D This section shows you how to create a Hello world! in X3D 2.3.1 What we need This section 
will show you how to write the famous Hello world! program in X3D. All you need is a text-editor and 
a X3D-browser like the IR viewer. 2.3.2 How we do it First start up the text-editor and enter the following 
code: <?xml version="1.0" encoding="UTF-8"?> <X3D profile='Immersive'> <Scene> </Scene> </X3D> Save the 
.le and call it simple.x3d Sweet! You just created the most simplest X3D world possible -an empty one. 
You can check that it is empty by opening it with your favourite X3D browser and watch if the browser 
gives any warnings. If he does consult the browser documentation if it supports the X3D XML encoding 
-some browser might only support the X3D Classic VRML encoding. 2.3.2.1 Understanding the code Let s 
have a look at the code: <?xml version="1.0" encoding="UTF-8"?> This is the XML .le declaration and it 
s used for easy identi.cation. The .le declaration should be present in every valid XML .le so just copy-and-paste 
it there. The .le declaration is followed by the X3D document root element which speci.es a pro.le : 
<X3D profile='Immersive'> A complete overview of the pro.les concept can be found in the X3D speci.cation 
. Simply put it tells the browser which kind of nodes the world uses so that the browser can check if 
he supports the pro.le (and the nodes associated to that pro.le). The Immersive pro.le used here is targeted 
at implementing immersive virtual worlds with complete navigational and environmental sensor control 
Next comes the empty Scene element: <Scene></Scene> In the following we will put some text into our scene. 
 2.3.3 Show the words What is missing in our world is the content. Since we want the two words Hello 
world! in fully blown 3D we add the following Text element to the Scene so that the code now looks like 
this: <?xml version="1.0" encoding="UTF-8"?> <X3D profile='Immersive'> <Scene> <Shape> <Text string="Hello 
world!" /> </Shape> </Scene> </X3D> We added a Shape which contains a Text geometry -and that s it! Save 
the .le to helloworld.x3d and start it up. The words Hello World should be shown by your X3D browser. 
You could now continue to play around with the Text , e.g. changing the depth or the fontStyle . Files: 
 simple.x3d  helloworld.x3d   2.4 Creating X3D Applications This section is the .rst in a group of 
sections which tries to teach how to develop VR/AR applications with the InstantReality Framework. This 
section tries to explain some basics but is mainly a starting point to get you pushed into the right 
direction. These sections are no sections on VR/AR in general or developer sections for framework internals. 
2.4.1 Relation to X3D One goal of the IR design was to make it really easy to write immersive VR and 
AR applications. One basic idea was, not to use a single GUI-Tool or method but a software abstraction 
layer which would allow us to de.ne rich, dynamic and highly interactive content. Since there is no ISO/ANSI/whatever 
standard for such an VR/AR application-interface, we tried to adapt something which is well known, practical 
and very close to our domain: X3D. The X3D standard is a royalty-free ISO standard .le format and run-time 
architecture to repre­sent and communicate 3D scenes and objects using XML. The standard itself is a 
successor of the VRML97 ISO standard. We utilized and extended this standard to .t the requirements we 
had from the VR/AR domain. This way our nodes and features are really a superset of X3D/VRML, and every 
X3D application is a valid Avalon application. 2.4.2 X3D Sources to read To get started you have at 
least to understand the basic concepts of VRML/X3D. The of.cial web­page has the X3D spec online which 
is not what you would like to read in the beginning. The developer section on the Web3d page holds some 
interesting links to sections and software tools. If you prefer some text books you should check out 
the X3D Book from Don Brutzman and Leonard Daly. Sometimes you can .nd some interesting, possibly used 
and really cheap VRML books, like the VRML Handbook or e.g. VRML -3D-Welten im Internet . 2.4.3 X3D 
Conformance Most VRML/X3D .les should work right away. If you have some .les that do not perform right, 
please visit the forum or write us a mail including the .le and/or a short description. However, there 
are some known bugs and not yet implemented VRML/X3D features. The online documentation should list the 
state of every implementation. 2.4.4 Growing number of Nodes and Components VRML was a static ISO standard 
which only de.ned a .xed set of 54 nodes. X3D almost doubled this number in the .rst version and indroduced 
Components and Pro.les to group and organize all nodes. X3D, in contrast to VRML, was designed to be 
extensible and therefore people keep revising the standard or build application speci.c node-sets like 
we did. If you look at the documentation you can see how the node-sets were growing over time from VRML 
2.0, X3D 3.0, X3D 3.1 up to X3D 3.2. In addition to the nodes new components (e.g. for shaders) where 
also introduced. 2.4.5 Mimetypes and encodings X3D supports not just a single data encoding but three 
in order to ful.ll different application require­ments. First of all there is an XML based encoding, 
which is easy to read and write for humans as well as for computers. <?xml version="1.0" encoding="UTF-8"?> 
<X3D profile='Immersive'> <Scene> <Shape> <Text string="Hello world!" /> </Shape> </Scene> </X3D> The 
VRML syntax has some disadvantages concerning parsing and the like. However for histori­cal reasons the 
VRML encoding is still supported in X3D as the so-called classic encoding: #X3D 3.0 utf8 Shape { geometry 
Text { string "Hello, World" } } The binary format can be loaded very ef.ciently but is is not readable 
by human beings at all (well I know somebody that can read parts of it but this is another story). 0000000 
00e0 0100 7800 02cf 7378 2864 7468 7074 0000010 2f3a 772f 7777 772e 2e33 726f 2f67 3032 0000020 3130 
582f 4c4d 6353 6568 616d 692d 736e 0000030 6174 636e f065 023c 3358 7844 7006 6f72 0000040 6966 656c 
4643 6c75 786c 7606 7265 6973 0000050 6e6f 3342 302e 817b 1881 6f6e 614e 656d 0000060 7073 6361 5365 
6863 6d65 4c61 636f 7461 0000070 6f69 086e 6826 7474 3a70 2f2f 7777 2e77 0000080 6577 3362 2e64 726f 
2f67 7073 6365 6669 0000090 6369 7461 6f69 736e 782f 6433 332d 302e 00000a0 782e 6473 3cf0 5304 6563 
656e 043c 6853 00000b0 7061 7c65 5403 7865 7874 7305 7274 6e69 00000c0 0867 2205 6548 6c6c 206f 6f77 
6c72 2164 00000d0 ff22 ffff All those data .les can be gzip-compressed and the system loader can handle 
the compression automatically. The loader/writer framework supports all three encodings and VRML 2.0 
equally well and in addition tools (e.g. aopt) to convert between these formats. You can not mix different 
en­codings in the same .le but you are free to mix the encodings in a single application (e.g. having 
a foo.wrl inline in you bar.x3d world) . Right now we are using the XML and VRML encoding through-out 
the sections. But we will include some translation mechanisms later on. Files: helloworld.x3d  helloworld.x3db 
 helloworld.x3dv   2.5 Get your engine right This section shows you how to build up different elements 
of a more complex application by providing a scene, engine and context-setup description. The different 
parts allow the author to cluster and specify various aspects of your .nal application. The Scene contains 
the dynamic spatial content, the Engine contains device and environment speci.c parts (e.g. window or 
cluster setup) and the ContextSetup de.nes the application independent parts like the path of your java 
installation. The ContextSetup parameter are normally stored in the system preferences (e.g. plist-.les 
on Mac or registry entries on Windows) but this sections shows how to overwrite the settings per application 
2.5.1 Scene The Scene de.nes the default SAI execution context and the root node of your spatial world. 
As shown in the following example it is only valid as X3D element. There is only one Scene per execution 
context or .le allowed. However you can Inline extra X3D-.les which must include a Scene element. <X3D> 
<Scene> <Shape> <Box size='1 2 9'/> </Shape> </Scene> </X3D> 2.5.2 Engine The scene describes the content 
of and interaction with the world -they do not describe on which kind of output device or how the content 
should be displayed. This is where the engine .le comes in. Imagine you want to display your world in 
red/green stereo, or you want it to be displayed on a three-sided power-wall, etc. This can all be done 
via an engine setting without even touching your world. This separation between the world and the engine 
.le allows you to visualize your world on a variety of output devices without changing the world itself. 
The next example shows a very simple engine setting as an example of a basic engine .le, which only contains 
a RenderJob . The RenderJob includes a WindowGroup, an abstraction used to combine render threads, which 
itself includes two windows. Therefore the engine will start two windows on the desktop showing the same 
scene and camera. The Engine sub-tree is a own X3D-namespace and can include any nodes which are not 
SceneBaseN­odes. For example scripts and event-.lter are very usefull as part of your engine. The Engine 
also does not have to be complete. Missing jobs will automatically be inserted by using the En-gine.requiredJobList 
values. More complex engine settings can be found in the cluster section. <X3D> <Engine> <RenderJob DEF='render'/> 
<WindowGroup> <Window DEF='win1'/> <Window DEF='win2'/> </WindowGroup> </RenderJob> </Engine> <Scene> 
<Inline url='theWorld.X3D'/> <Scene> </X3D> 2.5.3 ContextSetup The ContextSetup stores all setup and 
preferences per execution context which are related to speci.c nodes or components but independent of 
a spec.c instance of a node. You can set different limits, e.g. texture size and system pathes (like 
the Java classpath and various other properties which are usual stored in the player plist or registry 
entry). If and only if you really have to overwrite it in the application .le you can create an extra 
Con­textSetup child as part of the X3D element. <X3D> <ContextSetup maxTexSize='2048'> <Engine> ... 
</Engine> <Scene> ... <Scene> </X3D> The attributes and the values are not .xed and dynamically build 
up from the loaded component. Use the WebInterface access point in order to see the con.guration of your 
running system. The following section includes a list of attributes for the beta3 player including all 
components. cdfPath /Users/jbehr/src/Avalon/tmp/cdfFolder () defines the for the cdf data multiContext 
FALSE () Run more than one context appThreadCount 16 () The number of application threads forbidFieldFree 
FALSE () forbid Field free optimize forbidNodeFree FALSE () forbid Node free optimize rayIntersectMode 
auto (auto,lowMem,fast,fastest) defines the ray intersection mode forceSingleDataTypeRoute FALSE () force 
routes to connect only slots with same type explicitGZIPTest FALSE () use the explicit gzip test forbidReindex 
FALSE () forbid geos to reindex to share properties forbidStripFan FALSE () forbid geos to strip/fan 
the mesh data forbidVertexResort FALSE () forbid vertex resort to improve cache usage forbidSingleIndex 
FALSE () forbid singe-index opt. for static/unshared obj. forbidIndexResize FALSE () forbid optimized-index 
opt. for static/unshared obj. forbidDList FALSE () forbid DisplayLists for static obj forbidVBO FALSE 
() forbid VertexBufferObjects for static obj forbidNormalUpdate FALSE () forbid normal updates for dynamic 
obj. geoPropertyUpdateMode memcpy (none,memcpy,stl-assign,stl-swap) mode used to update backend geoProps 
maxTexSize 4096 () max texture u,v,w size, 0 means hardware limit forceTexCompress FALSE () force textures 
to use compressed internal types frontCollision TRUE () do front-collision check while navigating zRatio 
20000 () 13 ratio to specify the z-near limit defaultJobList TimerJob,InteractionJob,ExternalInterfaceJob,WebServiceJob,CollisionJob,SoundJ 
Defines the default jobs showStatusMessage true () show the status message in render view infoScreenAnimationTime 
0.5 () info screen animation time in seconds logoMode auto (auto,on,off) logo mode forceSingleThread 
FALSE () force single thread app/render cycle keyAssignMode ctrlMod (autoSwitch,app,sys,ctrlMod,altMod,shiftMod) 
defines how key-events are assigned to sys/app binSearchKeyIndex FALSE () use binSearch to find the key 
value tessellationFactor 1 () tessellationFactor (from 0 to 1) ecmaScriptShareRunTime FALSE () ecmascript 
share-runTime ecmaScriptGlobalMem 8388608 () ecmascript global system mem size ecmaScriptLocalMem 8192 
() ecmascript local script mem size ecmaGCPerFrame none (none,auto,force) set GarbageCollection mode 
per frame javaVMPath () The path to the Java virtual machine javaClassPath /Users/jbehr/src/Avalon/java/instantreality.jar 
() The Java class path javaOptions () Options that are transfered to the Java virtual machine cgVertexProfile 
auto (auto,arbvp1,vp20,vp30,optimal) Cg vertex shader profile cgFragmentProfile auto (auto,arbfp1,fp20,fp30,optimal) 
Cg fragment shader profile rigidBodyTrigger Collision () name of the rigid body physics trigger 2.5.4 
The Scene node in classic encoding The classic encoding normally does not include an explicit Scene node. 
All root nodes of a single .le are added to the scene as children. Here we extent the spec by allowing 
to explicitly set context root nodes like ContextSetup, Engine or Scene nodes. #X3D 3.0 utf8 Engine { 
} Scene { children [ Transform { ... } ] } This section just tries to explain the Context root nodes, 
like Scene, Engine and ContextSetup­nodes, and how to build up an application using only one or none 
of them. More useful examples can be found in the cluster and rendering sections . 14 An execution context 
will always have an Engine and ContextSetup node. In most cases there is only an explicit (=X3D encoding) 
or implicit Scene (=classic encoding) and the runtime system will automatically create the object. Chapter 
3  MultipleViews and Stereo In this section we will outline some of the modi.cations and extensions 
for running X3D applications in immersive environments with multiple views and stereo setups: which nodes 
and techniques we have to adopt and which additional nodes are useful and necessary. The X3D standard 
only supports a single Camera with the bound Viewpoint. The Viewpoint is a node in the scengraph de.ning 
a position, orientation and .eldOfView. There is no Window, Viewport or clipping area de.ned. For different 
VR/AR scenarios it is essential to drive Multi-screen and Multi­pipe setups. Therefore we have to de.ne 
some extensions and re.nements to ful.ll the requirements we have. 3.1 Rendering The proposed X3D Speci.cation 
Revision 1 [24] includes two new components, Layering and Layout, which provide nodes and functionality 
to render and layout different scene-parts in different layers. The Layer and LayerSet nodes de.ne the 
sub-trees and rendering order but do not de.ne what kind of composition method is used. Layer nodes are 
intended to create special 2D-/3D-interaction elements such as heads-up displays or non-transforming 
control elements. With the new Viewport node additional clip boundaries can be de.ned, but they only 
refer to a single render window. There is no notion of how this information shall be treated in a multi 
screen cluster setup. This gets even worse when having a closer look at the Layout component. Because 
of nodes like the ScreenFontStyle and its pixel-speci.c addressing it is mainly designed as a means to 
provide some additional information and with desktop applications and interaction metaphors in mind, 
but is not applicable for immersive systems. Augmented (AR) or Mixed Reality (MR) applications in general 
require methods that render dif­ferent layers of information -at least some sort of video-stream as background 
and 2D-and 3D­annotations. Even more complex layering techniques with certain compositing methods, including 
mechanisms for general multi-pass techniques as proposed in [?] with the RenderedTexture node, are needed 
to implement image-based rendering techniques like for instance dynamic HDR glow or motion-blur. For 
such advanced rendering effects which are -depending of the type of applica­tion -quite essential for 
evoking the sensation of presence and immersion, the possibility to render window-sized and view-aligned 
quads and additionally some way to control the composition method is needed. Besides this especially 
in immersive environments real time shadows are needed for depth cues and correct perception. For desktop 
applications and simple mono projector systems image-based rendering can be achieved by using an extended 
LayoutLayer node from the Rev1 speci.cation, and additionally in­troducing novel X3DAppearanceChildNode 
types for advanced render state control for image com­positing. This approach can be extended for stereo 
systems by introducing special shader uniform variables denoting such information like left/right eye 
(named StereoLeftEye in the following pixel shader code fragment) or even a cluster window id. This way 
creating stereo textures can be easily accomplished. Another challenge is how to treat these different 
kinds of layers in really immersive environments like e.g. the CAVE. Here, the correct positioning of 
user interface elements can be consistently handled with our special Viewspace node, which transforms 
its children to the coordinate system of the current active viewpoint. But there still exists no continuous 
conceptual model for handling pure effects layer nodes for mono/stereo and all display types ranging 
from a desktop PC to a tiled display cluster system. Another important issue are multi resolution surfaces 
(e.g. LOD or adaptively tessellated NURBS) on the one hand and simulation systems (like physics and particle 
systems) on the other hand on multi screen setups. The latter can especially lead to problems when the 
simulation is non-deterministic and distributed across a cluster for all render windows. If the simulation 
calculations are not bound to the application thread but run on different client PCs, the result is not 
necessarily the same and might therefore lead to rendering errors. Similar problems can occur with multi 
resolution meshes. If the view dependent tessellation is different on neighboring windows of tiled display 
systems this can also lead to artifacts. 3.2 Stereo Basics This chapter is not a section at all but 
a brief excursion into optics. It gives an overview over the human eye as a tool to perceive our world 
stereoscopically. It will also introduce those factors which are responsible for depth perception and 
which are further used to generate an arti.cial stereoscopic view in virtual reality environments. As 
the trick is to create and present a different image for each of your eyes, the last section will discuss 
the most popular technologies to get this done in a more or less simple way. There are no preconditions. 
3.2.1 Depth perception of human eyes There are two categories of depth perception, the monocular perception 
related to only one eye and the binocular perception related to both eyes. 3.2.1.1 Monocular depth perception 
Occlusion Farther objects are being occluded by nearer objects. Perspective Objects of the same size 
are bigger if they are nearer to the viewer. Depth of .eld Adaption of the eye lens warping, so called 
accomodation, focuses objects in a spe­ci.c distance to the viewer. Other objects with a different distance 
to the viewer appear blurred. Movement parallax If objects with a different distance to the viewer move 
with the same speed, nearer objects appear to be faster than farther objects.  3.2.1.2 Binocular depth 
perception Parallax (Each of our eyes sees a slightly different image of the world. This is because each 
eye is a separate camera which) grabs the environment from a different position. In the image below you 
see two eyes fo­cusing the black point. The optical axes show the alignment of the eyes and the projected 
point on the retinas. There is another point in green which is also projected. Those points have the 
same distance L and R to the optical axes on both retinas, they are correspond­ing points . Each point 
on the horopter , let s say a focus circle, has these characteristics. Points which are in front or behind 
the horopter have different distances, L and R , to the optical axes on the retina. The difference between 
R and L is the so called disparity , which is positive or negative dependent on the position in front 
or behind the horopter. Points on the horopter have a disparity of 0. Convergence (The distance between 
our eyes is .xed but the angle depends on the distance of a focused object. If we watch) the clouds in 
the sky the eye orientation is nearly the same but when we look at a .y which sits on our nose, the left 
eye looks more to the right and the right eye to the left, so we are squiting. This angle between the 
optical axes of both eyes, the convergence, gives our brain a hint about the distance to an object. 
 3.2.2 Depth generation in VR Generation of stereoscopic viewing in virtual reality environments depends 
mainly on the above de­scribed parallax and convergence, which go hand in hand. On a projection plane 
two separate im­ages are displayed, one for the left and one for the right eye. Therefore the scene must 
be rendered from two different cameras, next to each other like the human eyes. A separation technique 
has to make sure that one eye receives only one image but this issue is discussed in the next chapter. 
As each eye receives its appropriate image, their optical axes have a speci.c angle which yields to the 
convergence. The brain recognizes both points as the same point and interprets it as it was behind the 
projection plane or in front of it. Parameters which affect the convergence are the distance to the projection 
plane which is called zero parallax distance and the eye distance . With these parameters depth impression 
can be adjusted.  3.2.3 Eye separation Different images for both eyes have to be generated, presented 
and redivided to the eyes. How these three steps are performed depends on the chosen technology. Those 
are categorized into active and passive approaches. Active eye separation means a presentation of left 
and right images one after each other with high frequency while the eyes are alternately being shut in 
the same frequency by shutter glasses (see image below). The advantage is to see the original images 
without reduction of colors or color spectrums. On the other hand you need hardware (graphics card, beamer) 
which is able to display images in a high frequency of about 120 Hz as well as synchronisation with shutter 
glasses. With passive stereo, images for the left and right eye are displayed simultaneously. A common 
approach is to separate color channels in order to show the blue channel for the left eye and and the 
red channel for the right eye. A user wears glasses that .lter the red or blue channel away as you can 
see in the left image below. The big disadvantage is the corruption of proper colors. A better approach 
in this direction is the technology of In.tec , which doesn t split whole color channels but color spectrums 
in each channel. For each eye three different bands -for red, green and blue wavelenths -are .ltered, 
with the result to get a total of six bands: right eye red, right eye green, right eye blue, left eye 
red, left eye green and left eye blue. The desired colors are not completely modi.ed like in the color 
channel separation case, but only a few frequency regions are lost instead of a whole color channel. 
Another concept of passive stereo is the separation by polarization .lters. Light oscillates in different 
directions and a polarization .lter just .lters light in a way to let only parts of the light in a speci.c 
direction come through. So, for one eye, vertical oscillating and for the other, horizontal oscillating 
light is permitted. It s a simple solution as the .lters are not very expensive and can be easily put 
in front of a video beamer and into cheap paper glasses. The downside is a loss of brightness and interesting 
effects when inclining the head. This effect can be reduced with radial polarization approaches instead 
of horizontal and vertical .    3.3 Multiple Windows and Views This section describes the con.guration 
of multiple windows and multiple view areas per window. It is a precondition for the later parts of this 
section and the Clustering section. Please read the Engine section if you are not yet familiar with the 
concept of different Context base elements. 3.3.1 Engine con.guration First we try to get some basic 
window settings right. Therefore we have to setup the RenderJob. In the engine section that is used for 
local rendering, the de.nition for the rendering in most cases looks like the following line: DEF render 
RenderJob {} Instant Reality automatically adds the missing con.guration to produce an image on a local 
win­dow. To be able to understand more complex rendering con.gurations, we ll have a short look at the 
automatically generated con.guration for a local window. ... DEF render RenderJob { windowGroups [ WindowGroup 
{ windows [ LocalWindow { size 512 512 views [ Viewarea { lowerLeft 0 0 upperRight 1 1 } ] } ] } ] } 
... With this con.guration we have one local window with one Viewarea that covers the whole window. 
3.3.2 Multiple view areas To create a second view area we just have to add a Viewarea node and de.ne 
the region where it should appear in the window. 0 means the left or bottom side and 1 is the right or 
top. If you set values greater than 1, they are interpreted as pixel values. In each view area, the complete 
scene is rendered. The code for putting two view areas next to each other will look like this: ... DEF 
render RenderJob { windowGroups [ WindowGroup { windows [ LocalWindow { size 800 400 views [ Viewarea 
{ lowerLeft 0 0 upperRight 0.5 1 } Viewarea { lowerLeft 0.5 0 upperRight 1 1 } ] } ] } ] } ...  View 
areas can be modi.ed in a way to change the camera position and orientation by ViewModi­ .er nodes. 
This is especially used for stereo con.gurations and CAVE environments (see appropriate sections or nodetype 
tree documentation). 3.3.3 Multiple windows Multiple windows can be con.gured by adding LocalWindow 
nodes into the RenderJob section: ... DEF render RenderJob { windowGroups [ WindowGroup { windows [ 
LocalWindow { size 800 400 position 0 0 views [ Viewarea { lowerLeft 0 0 upperRight 0.5 1 } Viewarea 
{ lowerLeft 0.5 0 upperRight 1 1 } ] } LocalWindow { size 300 300 position 500 500 views [ Viewarea 
{ lowerLeft 0 0 upperRight 1 1 } ] } ] } ] } ... Files: MultipleViewareas.wrl  MultipleWindows.wrl 
 tie.wrl   3.4 Active Stereo This section describes the con.guration of the engine to achieve an active 
stereoscopic view of scenes by using synchronized shutter glasses. Please read the sections Multiple 
Windows and Views as well as Stereo Basics in this category to get a good overview about stereo approaches 
and basic con.guration issues regarding multiple views in Instant Reality. 3.4.1 Hardware A normal graphics 
card uses a double buffer approach, a back buffer to write into and a front buffer to display in the 
meantime to avoid .ickering. To use active stereo you should take a graphics card with quad buffer, i.e. 
four buffers. That means it uses a front and a back buffer for each eye. As display you can either use 
a monitor or a video beamer which is able to display active stereo images interleaved in time. Now you 
just need some shutter glasses which let you see the correct image for the appropriate eye. It is synchronized 
with the graphics card, mostly using infrared as you can see in the image below.  3.4.2 Stereo modi.er 
If we want to do stereo, then we need two view areas. One for the left eye and one for the right eye. 
For stereo it is neccessary to modify viewing parameters. For this kind of modi.cation there exists a 
number of modi.ers in Instant Reality. For a simple stereo projection we have to use the ShearedStereoViewModi.er 
: ... Viewarea { modifier [ ShearedStereoViewModifier { leftEye TRUE rightEye FALSE eyeSeparation 0.08 
 zeroParallaxDistance 1 } ] } ... Depending on the eye which should be represented by this modi.er, 
leftEye and rightEye has to be set to TRUE or FALSE. zeroParallaxDistance and eyeSeparation values are 
in metres, so they have good default values, if your scene is also modeled in metres. Otherwise you could 
either adapt the values or as a better approach, you should use a NavigationInfo node in the Scene namespace 
and set the sceneScale .eld to 0.01 if the scene is modeled in centimetres or 0.001 if the scene is modeled 
in millimetres and so on. The advantage is you can keep the stereo con.guration .x for your setup and 
each scene and just need to change one value. ... Scene { children [ NavigationInfo { sceneScale 0.01 
 } ... ] } 3.4.3 Quad Buffer Stereo Active stereo con.guration is simple. First you have to tell the 
LocalWindow to use four instead of two buffers, the default setting. ... LocalWindow { buffer 4 ... 
 } ... In the window we need two view areas, one for the left and one for the right eye. These areas 
are overlapping as we don t set speci.c regions for them. For each Viewarea we de.ne a ShearedStere­ 
oViewModi.er which is responsible for the camera modi.cation of the left or right eye respectively. It 
has also to be de.ned which buffers on the graphics card should be used by which view area. Therefore 
we set ... Viewarea { leftBuffer TRUE rightBuffer FALSE ... } ... for the left eye view area and ... 
Viewarea { leftBuffer FALSE rightBuffer TRUE ... } ... for the right eye view area. After all the 
con.guration looks like the one below: RenderJob { windowGroups [ WindowGroup { windows [ LocalWindow 
{ buffer 4 size 1024 768 views [ Viewarea { leftBuffer TRUE rightBuffer FALSE modifier [ ShearedStereoViewModifier 
{ leftEye TRUE rightEye FALSE } ] } Viewarea { leftBuffer FALSE rightBuffer TRUE modifier [ ShearedStereoViewModifier 
{ leftEye FALSE rightEye TRUE } ] } ] } ] } ] } Files: activeStereo.wrl  tie.wrl (test model)   
3.5 Passive Stereo This section describes the con.guration of the engine to achieve a passive stereoscopic 
view of scenes. It will distinguish between overlapped view areas and separated side by side view areas. 
The .rst one is performed by splitting color channels and using red/blue glasses. The latter can be used 
to achieve a stereo setup with two video beamers and appropriate .lters for example. You should also 
check the preconditions for this section. Please read the sections Multiple Windows and Views as well 
as Stereo Basics in this category to get a good overview about stereo approaches and basic con.guration 
issues regarding multiple views in Instant Reality. Notice that this section is just about passive stereo. 
3.5.1 Stereo modi.er If we want to do stereo, then we need two view areas. One for the left eye and one 
for the right eye. For stereo it is neccessary to modify viewing parameters. For this kind of modi.cation 
there exists a number of modi.ers in Instant Reality. For a simple stereo projection we have to use the 
ShearedStereoViewModi.er : ... Viewarea { modifier [ ShearedStereoViewModifier { leftEye TRUE rightEye 
FALSE eyeSeparation 0.08 zeroParallaxDistance 1 } ] } ... Depending on the eye which should be represented 
by this modi.er, leftEye and rightEye has to be set to TRUE or FALSE. zeroParallaxDistance and eyeSeparation 
values are in metres, so they have good default values, if your scene is also modeled in metres. Otherwise 
you could either adapt the values or as a better approach, you should use a NavigationInfo node in the 
Scene namespace and set the sceneScale .eld to 0.01 if the scene is modeled in centimetres or 0.001 if 
the scene is modeled in millimetres and so on. The advantage is you can keep the stereo con.guration 
.x for your setup and each scene and just need to change one value. ... Scene { children [ NavigationInfo 
{ sceneScale 0.01 } ... ] }  3.5.2 Stereo by overlapping view areas To receive a simple red/blue stereoscopic 
view, we have to overlap two view areas, display only one color channel per area (red or blue) and put 
a ShearedStereoViewModi.er into both areas. After all the code looks like this: DEF render RenderJob 
{ windowGroups [ WindowGroup { windows [ LocalWindow { views [ Viewarea { red TRUE green FALSE blue 
FALSE lowerLeft 0 0 upperRight 1 1 modifier [ ShearedStereoViewModifier { leftEye TRUE rightEye FALSE 
} ] } Viewarea { red FALSE green FALSE blue TRUE lowerLeft 0 0 upperRight 1 1 modifier [ ShearedStereoViewModifier 
{ leftEye FALSE rightEye TRUE } ] } ] } ] } ] } The result will look like this. Everything you need 
now are some glasses with a red foil for the left eye and a blue foil for the right eye. 3.5.3 Stereo 
by separate view areas Splitting color channels is the fastest variant of stereoscopic viewing. But we 
get much better results if we use the full color of left and right images. That s the reason why we render 
both images side­by-side, choose our output device, let s say a graphics card with two outputs and a 
video beamer on each output. The beamer images are then superposed. To receive only one image per eye 
we use polarization or color spectrum .lters (see Stereo Basics section) in front of the beamers and 
in front of our eyes. To be able to show the image for the left eye on the left side of our window and 
the image for the right eye on the right side, we use two view areas again. The .rst is located from 
0 -0.5 and the second from 0.5 to 1. ... LocalWindow { size 600 300 views [ Viewarea { lowerLeft 0 0 
upperRight 0.5 1 modifier [ ShearedStereoViewModifier { leftEye TRUE rightEye FALSE } ] } Viewarea 
{ lowerLeft 0.5 0 upperRight 1 1 modifier [ ShearedStereoViewModifier { leftEye FALSE rightEye TRUE 
 } ] } ] } ...  Your graphics card has to be con.gured so that the left side of the desktop is visible 
on the left output and the right side of the desktop is visible on the right graphics output. Additionally 
the size of the window must cover the whole desktop. This can be done by the following code. ... LocalWindow 
{ fullScreen TRUE } ... Now we have a simple stereo setup. The disadvantage is that a single PC is responsible 
for the simulation and the rendering of two images per frame. We can get a much better performance, if 
we are using 3 (or more) hosts where one is responsible for the simulation, one to calculate the right 
eye image and one to calculate the left eye image. To .nd out more about this, read the sections in the 
Cluster section . Files: stereoOverlap.wrl (color channel split)  stereoSeparated.wrl  tie.wrl (test 
model)  Chapter 4  Interaction and Devices 4.1 Interaction One major drawback of the X3D speci.cation 
is the extremely limited support for Input/Output (IO) devices. The X3D speci.cation does not mention 
devices at all -it only speci.es some very high­level nodes that allow to control the way the user navigates 
in the scene (NavigationInfo node) and interacts with objects (PointingDeviceSensor nodes). The actual 
mapping between these nodes and the concrete devices connected to the computer is up to the browser. 
While this interaction model is suf.cient for web-based 3D applications consisting of simple walk-through 
scenarios running on a desktop machine, it is much too limited for immersive VR and AR applications. 
For example, consider a driving simulator with a mock-up of the dashboard. The simulator (written in 
X3D) should be able to get the status of the switches on the dashboard e.g. used to switch the headlights 
on or off. Or consider a video see-through AR application where the X3D scene needs to get the video 
images from a camera attached to the system to put them into the background of the virtual scene. This 
demonstrates that there are much more usage scenarios for IO devices than the simple navigation and point-and-click 
scenarios currently supported by the X3D speci.cation. Our proposal, and the one implemented in IR, is 
to use a layered approach to integrate sup­port for IO devices into X3D. On the basic layer, we propose 
a set of low-level sensors that allow the application to receive data streams from or to send data streams 
to devices or external software components. On top of this layer is a set of high-level sensors consisting 
of the traditional PointingDe­viceSensor nodes mentioned in the X3D speci.cation. This approach is similar 
to that of traditional 2D user interfaces where we have a low-level layer consisting of simple mouse 
and keyboard events and higher-level layers consisting of user interface elements like buttons, text 
.elds and menus. 4.1.1 Low-Level Sensors The purpose of the low-level sensors is to send or receive raw 
data streams without imposing any interpretation of these streams by the X3D browser. It is the sole 
responsibility of the X3D applica­tion to handle these data streams in a use-and meaningful way. Recently, 
there have been some competing proposals for low-level sensors [9, ?, ?]. These proposals suffer from 
two design .aws: It is generally not a good idea to specify nodes like JoystickSensor , MidiSensor or 
Track­erSensor . An approach like this means that we have to specify nodes for all kinds of devices available, 
which is obviously not possible. There will always be devices that do not .t into the set of device classes 
available in the X3D speci.cation. As a result, this approach does not reliably and convincingly solve 
the problem.  Even worse, these types of nodes require that the X3D application developer has to foresee 
what kinds of devices are available to the users of his application. This is obviously not possible and 
con.icts with the typical use-case of X3D applications -downloading them from a web server and running 
them on any kind of hardware available.  For these reasons, we proposed another solution [?] which does 
not have these drawbacks. The idea is to treat devices as entities consisting of typed input and output 
data streams. We admit that this is not as straightforward as using special nodes for each kind of device, 
but the bene.ts far DEF cam Viewpoint { ... } DEF headPos SFVec3fSensor { label "Head Position" } ROUTE 
headPos.value changed TO cam.set position DEF headRot SFRotationSensor { label "Head Orientation" } ROUTE 
headRot.value changed TO cam.set orientation Shape { appearance Appearance { DEF videoTex PixelTexture 
{} } geometry IndexedFaceSet { ... } } DEF frame SFImageSensor { label "Video Frames" } ROUTE frame.value 
changed TO videoTex.set image Figure 4.1: Template of an AR application in X3D. outweigh this disadvantage. 
We get maximum .exibility, we do not bloat the X3D standard, and we get a stable basis for higher layers 
of abstraction. So we propose a set of sensors, one for each X3D .eld type. The interface of these nodes 
looks like this ( x is just a placeholder for the concrete X3D .eld types SFBool, SFFloat, ..., MFBool, 
MFFloat, ...): xSensor : X3DDirectSensorNode { [in,out] value SFBool [] out FALSE SFString [] label "" 
} The value exposed .eld is used to send or receive data values. The out .eld speci.es whether the node 
is used to receive data values ( FALSE ) or to send data values ( TRUE ). Finally, the label provides 
means to map the sensor node to a concrete data stream of a device. The important point here is that 
we do not specify where the data comes from (e.g Joystick 1/X-Axis ). Instead, in the label .eld we specify 
what the data values are used for (e.g. Move left/right ). When the user loads an X3D scene that contains 
sensors that are not mapped to devices, a dialog window opens that lists the labels of the sensors. Next 
to each label is a drop-down menu that contains all devices that are currently connected to the machine 
and that have a matching type and direction. This is just the same procedure that we are used to when 
we start a game, e.g. a .rst-person-shooter, for the .rst time. Before we can start playing the game, 
we have to go into a Con.guration dialog to specify which joystick to use and which of the joystick s 
buttons is the .re button and so on. We propose to do the same for X3D scenes. After the user speci.ed 
a mapping for the sensors, the X3D browser can save the mapping in a database (using the URL of the X3D 
scene as a key), so the user does not have to do this con.guration each time he starts the X3D scene 
later on. It is also possible to de.ne some kind of default mapping, e.g. we could specify that an SFFloat 
input sensor with the label Move left/right by default gets mapped to the x-axis of the .rst joystick. 
Figure 4.1 shows a simpli.ed template of a video see-through AR application written in X3D that demonstrates 
how to use low-level sensors. There is a SFVec3fSensor that provides the current head position from the 
tracking system, and a SFRotationSensor that provides the orientation. We simply route both values into 
a Viewpoint node. Furthermore, there is a SFImageSensor that provides video images from a camera. These 
images are routed into a PixelTexture node that is mapped onto an IndexedFaceSet in the background of 
the virtual scene. DEF pointerTransform Transform { children DEF userBody UserBody { children Shape { 
... } } } DEF handPos SFVec3fSensor { label "Hand Position" } ROUTE handPos.value changed TO pointerTransform.set 
position DEF handRot SFRotationSensor { label "Hand Orientation" } ROUTE handRot.value changed TO pointerTransform.set 
orientation DEF handHot SFBoolSensor { label "Hand Active" } ROUTE handHot.value changed TO userBody.set 
hot Figure 4.2: Using the UserBody. 4.1.2 High-Level Sensors High-level sensors are sensors that are 
built on top of low-level sensors. They provide a more abstract interface to devices. Examples for high-level 
sensors are the X3D PointingDeviceSensor nodes as well as all means for navigating in the virtual scene. 
The PointingDeviceSensor nodes allow to interact with objects in the scene. The user can choose which 
object to manipulate by locating a pointing device over the object. In the case of 2D projections on 
desktop clients, over is de.ned by moving the mouse pointer over the object. But unfortunately, the X3D 
speci.cation does not give any hints about how to interpret over in immersive, stereo projections using 
3D or even 6D devices. Stiles et al. [49] describe possible solutions. In our system, we use a special 
node called UserBody that de.nes a 3D pointer. Its interface looks like this: UserBody : Group { SFBool 
[in,out] hot FALSE } It is simply a Group node that has one additional .eld, hot . The children of this 
group node con­sist of geometries that form the shape of the 3D pointer. The hot .elds speci.es whether 
the pointer is active (i.e. clicked ) or not. There can be an arbitrary number of UserBodies, e.g. for 
multiuser applications. The pointer gets transformed in the 3D scene the usual way by putting Transform 
nodes in the transformation hierarchy above the UserBody and by routing position and orientation values 
into these transform nodes. We usually get the position and orientation values via low-level sensors 
from a tracking system, e.g. when using a stylus to interact with a scene, but it is possible to use 
arbitrary sources for these values, e.g. Script nodes. This is similar to the proposal made by Polys 
et al. in[?]. Figure 4.2 shows an example that demonstrates how to connect the UserBody to a tracking 
system. To interact with PointingDeviceSensors, our systems provides three different kinds of interaction 
modes, project , intersect and collision , which have speci.c advantages depending on the kind of application 
and interaction device. The user can select one of these interaction modes by using the user interface 
of our system. project is useful for interaction devices that only provide position values (3 degrees 
of freedom). We shoot a ray from the Viewpoint through the center of origin of the UserBody node. The 
.rst object that gets hit by this ray is the object our pointer is currently over . intersect and collision 
are useful for interaction devices that provide position values as well as rotation values (6 degrees 
of freedom). When using intersect , we shoot a ray from the origin of the UserBody node along the negative 
z axis. Again, the .rst object that gets hit is the object we are over . When using collision , the user 
actually has to collide the geometry of the UserBody with another object.  4.2 Input/Output streams 
This section shows you how to use an IOSensor to connect input/output streams; as example we will use 
two joystick-axes to control the diffuse-color of a box. 4.2.1 Introduction IR supports various ways 
to get the device data in/out of your application/scene to handle different classes of applications and 
scenarios. For web-applications it s desirable to have device-independent setups. On the other hand the 
system must provide concrete and low-level access to services to handle complex and extensive application-/device-setups 
which are part of a .xed and controlled desktop or even immersive environments. For device-independent 
setups, you can just de.ne labeled streams inside of a scene and map this streams outside (e.g. interactive 
or as part of your engine) to a logical devices. To access the device directly you have to specify a 
concrete service-type. This section shows how to utilize IOSensor-nodes to handle both setups. 4.2.2 
IOSensor basics The IOSensor allows you to connect a local or network-device or service. (Side node: 
To be more precise: it abstracts a backend or namespace of the device-subsystem which is in most cases 
a single device) There are other X3D extensions and systems which also provide low-level device access 
but most of them provide one node per logical device-class. We followed a different approach: We have 
a single IOSensor node type. Every IOSensor node instance can be used to connect to a service/namespace 
of the supported device-subsystems. Interfaces and parameters of a concrete device, if any, will be mapped 
to dynamic .elds, similar to the Script node. Therefore the node has, more or less, only two .elds. IOSensor 
: X3DSensorNode { SFString [] type [auto] [joystick,video,...] SFString [] name [] } The type speci.es 
the type of device/namespace/backend which should be used or can be set to auto . This switches two essential 
modes: The implicit or explicit naming of services which leads in most cases to device independent respectively 
dependent setups. 4.2.3 Implicit Service de.nition If your application needs IO-data-streams but would 
not like to specify which device or service should be used the type .eld should be set to auto . In this 
case, the system and runtime environment will try to map the user-given IO-slots to concrete devices 
automatically or by asking the user. DEF camNav IOSensor { eventOut SFFloat speed eventOut SFFloat direction 
} 4.2.4 Explicit Service de.nitions If the type .eld is not auto it should de.ne one of the supported 
device/backend/namespace types. Ths standard device-abstraction system supports more than 30 backends. 
All de.nitions can be accessed by using the device management system provided in the Help-Menu. Which 
devices are available depends on the system and environment. The name-.eld is used as an identi.er in 
the device-subsystem. It is not used inside of your X3D-application. 4.2.4.1 Parameter of Services Every 
device type provides a list of dynamic SFString .elds which can be used as parameters. Most backend types 
provide e.g. a device .eld which takes a number (e.g. 0,1,2) or even a description substring of the description: 
DEF myStick IOSensor { type "joystick" device "microsoft" } If the type joystick does not provide a dynamic 
device -.eld you will get a warning and a list of valid .elds in the application log. You can also lookup 
valid backend-.elds using the online interface which is available through the help menu. The parameter-.elds 
only depend on the device type but not on a single service instance. For example all joystick-backends 
provide a device -.eld but every joystick instance provides a different number of buttons and axes. 
4.2.4.2 IO Slots of Services The IOSensors use user-provided dynamic in/out-slots to connect to these 
devices: e.g. the x/y axis of a joystick could be accessed in the following way DEF myStick IOSensor 
{ type "joystick" eventOut SFFloat *x*axis* eventOut SFFloat *y*axis* } DEF myScript Script { eventIn 
SFFloat speed } ROUTE myStick.*x*axis TO myScript.speed These slots can be used together with ROUTES 
to process and transmit the events from or to the device. The names of the slots are used to map the 
device interfaces to the node interface. To increase the usability .le-system like wild-cards (* and 
?) are supported. Again: Use the backend­interfaces which can be accessed in your Help-menu to check 
the provided slots. The last example shows how to map the image data of a local camera onto a box. DEF 
video IOSensor { type "video" eventOut SFImage frame* } Shape { appearance Appearance { texture DEF tex 
PixelTexture { } } geometry Box { } } ROUTE video.frame* TO tex.image  4.2.5 Conclusion This section 
only shows how to get your data in. It does not show how to use the data for interaction and navigation 
purpose. If you would like to use the data together with high level PointingSensors look at the section 
Immersive PointingSensor Interaction. For a section on abstract 3D-Navigators please have a look at the 
Navigator section . Files: joystickToColorTutorial.x3d  videoCameraTutorial.x3d   4.3 Connecting 
Devices This section shows you how to connect a device to the framework and how to .nd all in/out slots 
of the device interactively. This is necessary if you would like to write a application which automatically 
connects a device using e.g. a IOSensor (See Input/Output streams section). It only shows how to start 
a IO-Node and how to .nd the coresponding input/output slots. You still need to connect the streams to 
the application (e.g. using a IOSensor) and some code to process the input data (e.g. SteeringNavigator); 
The section uses a simple joystick but the steps are in general the same for every device 4.3.1 Introduction 
The framework does not classify a speci.c interface for a device-type but dynamically creates in­put/output 
slots for every device instance. This has the advantage that we can connect any kind of device providing 
any kind of data channels. But the design has also one major drawback: You always have to test a speci.c 
physical device to see what kind of in/out slots it provides. There is for e.g. only a single Joystick 
-Type which supports any number of buttons and axis. Therefore the same code can be used to connect a 
12-Button/6-axis joypad or an 5-Button/0-axis apple-remote (which is also registered as joystick in OSX). 
You can connect the in/out slots interactively which gives you the most .exibility e.g. using the Web-Interface. 
However, if you really know what you are doing and would like to write a application which uses for e.g. 
an IOSensor to start and connect a speci.c device you have to know what kind of slots there are. This 
.rst part of the section shows you to .nd those slots. The second part shows how to use this information 
with an IOSensor. 4.3.2 Finding your slots 4.3.2.1 Connect your device to the local machine Connect 
the device to the same machine where you would like to run the InstantPlayer system (You can use devices 
remotely easily using the InstantIO-Server but this is not the topic of this section). Here I connect 
a logitech joypad to my notebook.  4.3.2.2 Start the correct Device-handler Start the InstantPlayer 
and open the Web-Interface of the Device-Managment system by clicking on Help->Web Interface Device Managment 
in the menu bar entry. This opens your preferred web­browser showing the Web Interface of the device 
managment system called InstantIO. This page may look like a static page but this is actual the dynamic 
User-Interface for the device managment system. The Web interface allows you to control the system remotely 
which is very handy for Immersive or mobile-AR applications.  Click further on Nodes to open the page 
to add and remove device handler to the current names­pace. In the lower side of the page you .nd the 
Create new node section which allows you to create a new device handler. Select the correct type for 
the connected device. In our case we select Joystick and push the Create Button. The following page 
allows you to set some parameter for the device. In our case it is the name and the device identi.er. 
More information can be found on the speci.c page This should start the device handler and bring you 
back to the list of active devices. There is now a Operating Logitech Dual Action which is sleeping since 
no Sensor is connected. Click on the Root link at the top of the page to get back to the namespace page. 
 4.3.2.3 Get a list of all out-slots Now since we have an active device handler for the joystick we can 
see what kind of dynamic slots are available. Now click on OutSlots to get a list of all available OutSlots 
for the new device This list shows 12 Buttons and 6 Axis which the joystick provides. These are all slots 
of the physical device.  4.3.3 Use the information to start an IOSensor The IOSensor allows you to start 
a InstantIO type directly from your Scene. Use dynamic outputOnly slots, the same way you use them in 
Script nodes, to connect the slots. The Name of the slots have to be the same as in the above list. There 
is one aspect you have to keep in mind. The InstantIO slots sometimes contain spaces and you have to 
replace those with wildcards (* and ?) to create a single token. DEF myStick IOSensor { type "joystick" 
outputOnly SFFloat *Hat*x*axis* outputOnly SFFloat *Hat*y*axis* } If you use the xml encoding you can 
use the full name including any kind of spaces: <IOSensor DEF='myStick' type='joystick'> <field accessType='outputOnly' 
name='Hatswitch X-Axis' type='SFFloat'/> <field accessType='outputOnly' name='Hatswitch Y-Axis' type='SFFloat'/> 
</IOSensor>   You can connect those slots to any kind of other node (e.g Navigator or Script) to process 
the incoming values. Look at the next parts in this section to get further information. Look at the Input-Output 
stream section to get further information about the IOSensor. 4.3.4 Conclusion This section shows how 
to connect external devices and how to .nd their provided slots. This is the information you need to 
connect the service to the framework. You can do it interatively or as part of your scene useing an IOSensor 
or similiar services.  4.4 Space-Mouse/Navigator/Pilot Device This tutorial demonstrates how to connect 
a 3Dconnexion (former LogiCad3D) SpaceMouse, Space-Navigator, SpacePilot or compatible device to Instant 
Player. 4.4.1 Introduction This tutorial demonstrates how to connect a 3Dconnexion (former LogiCad3D) 
SpaceMouse, Space-Navigator, SpacePilot or compatible device to Instant Player. There are three different 
approaches, depending on the type of device and the operating system you are using. In general all the 
devices provide 6 degrees of input. Three axes for translation and three axes for rotation. In addition 
the devices support a varying number of buttons, ranging from 2 to more than 20. The main difference 
is the type of connection. New devices, e.g. SpaceNavigator and SpacePilot, use a USB connector. Older 
devices, like the classic DLR SpaceMouse are usually connected via a serial slot. These are becoming 
rare, so in most cases a USB slot will be .ne. 4.4.2 Start the correct device Handler The correct InstantIO 
Handler depends on the physical device and opperating system. Look for the Input/Output streams tutorials 
to get more background information about the IOSensor. 4.4.2.1 USB Device on Windows The SpaceNavigator 
backend is the recommended way to connect SpaceNavigator and SpacePilot devices on Windows. It does not 
exist on Mac OS X or Linux, and it does not work out of the box for classic SpaceMice connected to the 
serial port. To get the backend working, .rst install the appropriate driver from the 3Dconnexion web 
site (it is actually the same driver for all USB devices, 3DxSoftware 3.x ). Then, integrate an IOSensor 
node into the scene whose type is SpaceNavigator : DEF ios IOSensor { type "SpaceNavigator" eventOut 
SFFloat X?translation eventOut SFFloat Y?translation eventOut SFFloat Z?translation eventOut SFRotation 
Rotation eventOut SFBool Button??1 eventOut SFBool Button??2 } The SpaceNavigator backend has three .oat 
outslots that provide values between -1 and 1 for the x, y and z translation of the cap, one rotation 
outslot for the rotation of the cap, and two boolean outslots for the buttons 1 and 2 of the SpaceNavigator 
(we currently do not support the other buttons available on other 3Dconnexion devices like the SpacePilot). 
 4.4.2.2 USB Device on OSX The Joystick backend is the recommended way to connect SpaceNavigator and 
SpacePilot devices on Mac OS X. It does not work on Windows or Linux, and it does not work for classic 
SpaceMice connected to the serial port. To get the backend working, do not install any drivers -just 
integrate an IOSensor node into the scene whose type is Joystick . In the device SFString .eld, you can 
either specify the name of the device or its index (0 is the .rst joystick device in the system, 1 the 
second, and so on): DEF ios IOSensor { type "Joystick" device "0" eventOut SFFloat X-Axis eventOut SFFloat 
Y-Axis eventOut SFFloat Z-Axis eventOut SFFloat X-Rotation eventOut SFFloat Y-Rotation eventOut SFFloat 
Z-Rotation eventOut SFBool Button??1 eventOut SFBool Button??2 } The Joystick backend has three .oat 
outslots that provide values between 0 and 1 for the x, y and z translation of the cap, three .oat outslots 
that provide values between 0 and 1 for the x, y and z rotation of the cap, and boolean outslots for 
each button of the device. 4.4.2.3 Serial-Device on all Systems The SpaceMouse backend is the recommended 
way to connect classic (serial) SpaceMouse devices on all operating systems. To get the backend working, 
do not install any drivers -just integrate an IOSensor node into the scene whose type is SpaceMouse . 
In the device SFString .eld, you have to specify the serial port the SpaceMouse is connected to (0 is 
COM1, 1 is COM2, and so on). DEF ios IOSensor { type "SpaceMouse" device "0" eventOut SFFloat X?translation 
eventOut SFFloat Y?translation eventOut SFFloat Z?translation eventOut SFFloat X?rotation eventOut SFFloat 
Y?rotation eventOut SFFloat Z?rotation eventOut SFBool Button?1 eventOut SFBool Button?2 eventOut SFBool 
Button?3 eventOut SFBool Button?4 eventOut SFBool Button?5 eventOut SFBool Button?6 eventOut SFBool Button?7 
eventOut SFBool Button?8 } The SpaceMouse backend has three .oat outslots that provide values between 
0 and 1 for the x, y and z translation of the cap, three .oat outslots that provide values between 0 
and 1 for the x, y and z rotation of the cap, and eight boolean outslots for the buttons 1 - 8 of the 
device. It is currently not possible to access the * button of the device via the IOSensor node. Side 
Node: It is also possible to get older (serial) SpaceMouse devices working with the Space-Navigator backend 
on Windows, but that involves a little bit of hacking. You have to install the appro­priate driver from 
the 3Dconnexion web site ( 3DxSoftware 2.x ). Additionally you have to get and register a library (TDxInput.dll). 
This library only comes with the drivers for newer (USB) devices ( 3DxSoftware 3.x ). So you have to 
perform the following steps to get older (serial) devices working: Get and install the driver for newer 
(USB) devices ( 3DxSoftware 3.x ). As I already said, this driver is the same for all USB devices, so 
it does not matter whether you choose the driver for the SpaceNavigator, the SpacePilot or any other 
USB device.  Locate the library TDxInput.dll . On an english version of Windows, when you installed 
the driver into the default location, the location of that library is C:Program Files3Dconnexion3Dconnexion 
3DxSoftware3DxWarewin32 . Copy that library into a safe location.  Uninstall the 3DxSoftware 3.x driver. 
 Get and install the driver for older (serial) devices ( 3DxSoftware 2.x ).  Register the library TDxInput.dll 
. To to that, you have to log in as an administrator, open the command line, go into the directory that 
contains the library, and enter the following command: regsvr32 TDxInput.dll . Do not move the library 
to another location or remove it from the hard disk -it is registered at that speci.c location.   4.4.3 
Controlling the Application The IOSensor nodes give you the raw datastreams of the devices. You, as application 
developer, are totally free to use it to change various application states. You can e.g. navigate the 
camera, transform BodyPart nodes to trigger PointSensors or change the color of an object according to 
the current rotation state. You can use Scripts to code this behavior or use helper Nodes like an SteeringNavigator. 
Check the Navigator and Immersive PointingSensor Interaction tutorial for more details. Attached to this 
tutorial you .nd a simple example which shows most usual case. Using a Space­Navigator/SpacePilot on 
Windows to control a Navigator while walking/.ying in a Virtual Environment Files: space-nav.x3d  4.5 
Navigator This sections shows how to use 2D and 3D navigators together with device inputs to move the 
user camera. 4.5.1 Introduction For desktop applications navigation is simply accomplished by using the 
mouse. Internally a so-called Navigator2D node, which is part of the engine, especially the Viewarea 
node, is used to navigate with a mouse through the 3D scene. Thus it has three input .elds, mousePress 
, mouseRelease , and mouseMove . Actually they were designed for reacting to mouse events, but as other 
devices may produce similiar events, for the purpose of generality those events may be routed to the 
2D Navigator as well. But generally the user doesn t need to worry about that. This is different for 
the 3D Navigators, which were especially developed for joysticks or VR­devices. They also inherit from 
the abstract Navigator base node, but for convenience they are part of the scene and therefore have to 
be suitably parameterized, which will be explained in later sections. 4.5.2 Getting the data -the IOSensor 
If you want to navigate or interact with your scenes using a joystick, spacemouse or a similiar external 
device you .rst need an IOSensor in your scene, for retrieving the input values. Below is an example 
for addressing a joystick. Usage of e.g. a spacemouse would be quite similiar, with the type .eld set 
to type spacemouse . DEF ios IOSensor { type "joyStick" eventOut SFFloat *x*axis eventOut SFFloat *z*rot* 
eventOut SFBool *button*8 } 4.5.3 Moving around -the Navigator3D Now that you have the values of your 
device, there are basically two options for navigating. On the one hand you can route the translational 
and rotational values to a Script node, calculate the corresponding transformations and route the results 
to your Viewpoint. Because this might be quite cumbersome on the other hand you can alternatively use 
a Navigator3D node. Currently there are three types: the PointingNavigator , the SpaceNavigator , and 
the SteerNavigator . The PointingNavigator is especially useful for fully immersive navigation in combination 
with a Pen. Via point and click you can .y to the chosen location, which is conceptually similiar to 
the lookat navigation type. The calculated movement is generated from relative movements of the device. 
 The SpaceNavigator allows navigation regarding all six degrees of freedom by directly manip­ulating 
the camera position in 3D space, but therefore usually a lot of practice is needed.  The SteerNavigator 
tries to alleviate this by providing a simpler interface for walk-through and .y modes with devices like 
joystick and spacemouse. In the following the latter navigation type will be further explained exemplarily. 
 After having outlined what type of navigators exist, it will now be explained, how they are used. One 
possibility is to instantiate a navigator as a child of a Viewpoint , which is shown in the following 
code fragment. This has the great advantage that the navigator is automatically connected to the currently 
active Viewpoint. Viewpoint { position 45.15 0.42 5.11813 orientation -0.21 0.97 0.0644 0.59193 navigator 
[ DEF nav SteerNavigator { inputRange [0 1] rotationSpeed -0.2 -0.2 -0.2 translationSpeed 10 10 10 } 
] } ROUTE ios.*x*axis TO nav.set yRotation ROUTE ios.*z*rot* TO nav.set zTranslation As can be seen in 
the next code fragment, despite .elds for the type of navigation etc., the NavigationInfo also contains 
a MFNode .eld navigator for holding the 3D navigator, which will be called for the currently bound ViewBindable 
node. NavigationInfo { type "walk" navigator [ DEF nav SteerNavigator { inputRange [0 1] rotationSpeed 
-0.2 -0.2 -0.2 translationSpeed 10 10 10 } ] } ROUTE ios.*x*axis TO nav.set yRotation ROUTE ios.*z*rot* 
TO nav.set zTranslation Now there remains one question. How do the navigators update their internal state? 
The Steer-Navigator for instance has six input .elds for [x|y|z]Rotation as well as for [x|y|z]Translation, 
which de.ne a rotation around the corresponding axis or a translation in the appropriate direction respec­tively. 
For updating camera movement, you only need to route the corresponding values from your device sensor 
node to the navigator node as shown above. Furthermore there exist some interesting .elds for .ne-tuning 
your navigator. The inputRange .eld speci.es the input value range e.g. [-1;1] or [0;1]. It is possible 
to specify one value for all inputs or a single range for all 6 input values. The rotationSpeed .eld 
de.nes the rotations per second for each axis; the values can also be negative for inverting the direction 
of rotation. The translationSpeed .eld de.nes the speed of the translation in meters per second for each 
axis. In order to avoid drift when not interacting with the input device the SteerNavigator has two SFVec3f 
.elds for de.ning the values of zero de.ection for each axis (meaning the control sticks, after initially 
having moved them already, are now at rest): zeroDe.ectionTrans and zeroDe.ectionRot . Last but not least 
the SteerNavigator node has an SFBool eventIn slot called updateRotationCenter . If this slot is triggered 
a point along the viewing ray, usually the point of intersection with the scene geometry, is set as the 
new center of rotation (default is the origin), which is used in examine mode as the center point around 
which the viewpoint is rotated. The example .le shows a simple walk-through world using an IOSensor for 
joystick movement and a SteerNavigator for showing the previously explained .elds in action. Files: walk_through.wrl 
 4.6 Immersive PointingSensor Interaction This sections shows how to use a UserBody together with immersive 
interaction devices in order to trigger pointing sensors. 4.6.1 Desktop based interaction For desktop 
applications object manipulation is simply accomplished by using the mouse or similiar devices. The X3D 
PointingDeviceSensor nodes therefore allow to interact with objects in the scene. The user can choose 
which object to manipulate by locating the mouse pointer over the object. Here the interaction concepts 
directly follow the way they are described in the pointing device sensor component of the X3D speci.cation. 
Hint: This concept can easily be generalized for any screen-space based input data. Internally the so-called 
Navigator2D node, which is part of the engine, is used to handle navigation and interaction with a mouse 
within a 3D scene. But other devices like e.g. optical tracking may produce similiar events, which can 
also be used. Because those concepts were already explained in the context of 2D/3D navigation, the interested 
reader may refer to the corresponding navigation section . 4.6.2 Fully immersive interaction Within 
X3D a pointing-device sensor is activated when the user locates the pointing device over geometry that 
is in.uenced by that speci.c pointing-device sensor. For desktop clients with a 2D mouse this is just 
de.ned by the mouse pointer. In immersive environments (e.g. a CAVE using a 6DOF interaction device) 
it is not so straightforward how over should be understood. Therefore one additional node to generalize 
the immersive implementation is provided. The User-Body derived from the Group node de.nes a sub-graph 
as so-called user body. The UserBody has only one extra SFBool .eld hot . The hot-.eld is analogous to 
a mouse button for 2D interaction and corresponds to the button pressed state. If the UserBody is instantiated 
as child of a Transform node it can be transformed by external interaction devices like a spacemouse 
or a pen (whose values can be accessed by means of the IOSensor node), and can be used for direct visual 
feedback of pointing tasks as well as for colliding with real scene geometry, equivalent to a 3D mouse 
cursor. The type of interaction is set in the NavigationInfo node. Currently the following interaction 
types are possible: none -no interaction  ray -handles ray selection in 3D; the ray origin is the position 
of the user body, and the ray points into the negative z direction (typically an array, by grouping a 
Cone and a Cylinder, is used for representing the proxy geometry, in this case don t forget to add an 
additional rotation of 1 0 0 -1.5707963 for correct adjustment to the parent Transform)  nearest -also 
ray based, but uses the nearest sensor, because sometimes it might be quite dif.cult to really hit an 
object by means of a ray intersect  projection -like ray this type also handles ray selection in 3D, 
but this time the ray points from the camera through the origin of the user body s coordinate system, 
what is especially useful for desktop applications. Be careful not to mix up the origin (which might 
not be visible) with the real position of your object. Hint: When using the Viewspace a Geometry2D node 
works best as user body.  collision -here the notion of being over is modelled by means of a collision 
of the user body geometry with the sensor geometry  With the help of the following code fragment (the 
complete version can be found in the example) a typical usage scenary will .nally be exemplarily discussed. 
DEF script Script { eventIn SFTime update eventIn SFFloat set xRotation eventIn SFFloat set yRotation 
eventIn SFFloat set zRotation eventIn SFFloat set xTranslation eventIn SFFloat set yTranslation eventIn 
SFFloat set zTranslation eventOut SFRotation rotation changed eventOut SFVec3f translation changed url 
"javascript: ..." } DEF timeSensor TimeSensor { loop TRUE } ROUTE timeSensor.time TO script.update 
DEF ios IOSensor { type "spacemouse" eventOut SFFloat X*Rotation eventOut SFFloat Y*Rotation eventOut 
SFFloat Z*Rotation eventOut SFFloat X*Translation eventOut SFFloat Y*Translation eventOut SFFloat Z*Translation 
eventOut SFBool Button* } DEF navInfo NavigationInfo { interactionType "ray" sceneScale 0.01 } Viewspace 
{ scaleToScene TRUE children [ DEF userBodyTrans Transform { children [ DEF userBody UserBody { ... 
} ] } ] } ROUTE ios.X*Rotation TO script.set xRotation ROUTE ios.Y*Rotation TO script.set yRotation 
ROUTE ios.Z*Rotation TO script.set zRotation ROUTE ios.X*Translation TO script.set xTranslation ROUTE 
ios.Y*Translation TO script.set yTranslation ROUTE ios.Z*Translation TO script.set zTranslation 48 ROUTE 
ios.Button* TO userBody.hot ROUTE script.rotation changed TO userBodyTrans.set rotation ROUTE script.translation 
changed TO userBodyTrans.set translation Because a UserBody can only have an effect when being moved 
around, you .rst have to update its position and orientation to determine which 3D objects are to be 
hit. This can be done with the help of an IOSensor for receiving the input data of your desired interaction 
device. In this example a spacemouse was chosen. Because a spacemouse has six SFFloat eventOut slots, 
three for translation along the x, y, and z axis, and three for rotation about these axes, the .nal translation 
(of type SFVec3f) and rotation (of type SFRotation) have to be assembled in a script. After that the 
results are routed to the parent transform of the UserBody node, which contains the pointer geometry. 
In this example the user body is also a child of a Viewspace node. This is due to the fact, that usually 
the pointer geometry is not really considered as being part of the scene but rather a tool for interacting 
in immersive environments. In this context two .elds are quite important: If scaleToScene is true, the 
Viewspace is scaled to the same size as de.ned in sceneScale of the NavigationInfo. This is very useful 
in case the scene wasn t modelled in meters; hence if the scene was modelled e.g. in centimeters, the 
sceneScale .eld should be set to 0.01. Warning Please note, that currently only the .rst UserBody can 
activate pointing device sensors in ray, nearest and collision mode; whereas the projection mode may 
not work in multi­viewport/ highly immersive environments. Files: Ray intersect  Projective intersect 
 Projective intersect with HUD   4.7 Vision Marker Tracking This section shows you how to use instant 
reality s vision module for marker tracking. 4.7.1 Introduction instant vision is a set of visual tracking 
systems starting with simple marker tracking going to mark­erless tracking like line trackers and KLT. 
The true power of the system lies in the ability to combine several such tracking procedures, for instance 
using a line tracker for initialisation with an absolute pose and KLT for frame to frame tracking. In 
this example we will focuson a simple marker tracking example using the VisionLib backend. 4.7.2 IOSensor 
The marker tracking is loaded like any other HID device via an IOSensor. These are instant vision s .elds: 
 VideoSourceImage (SFImage) : Camera image  TrackedObjectCamera_ModelView (SFMatrix) : Camera s modelview 
matrix  TrackedObjectCamera_Projection (SFMatrix) : Camera projection matrix  TrackedObjectCamera_Position 
(SFVec3f) : Camera s position  TrackedObjectCamera_Orientation (SFRotation) : Camera s orientation 
 <IOSensor DEF='VisionLib' type='VisionLib' configFile='visionlib.pm'> <field accessType='outputOnly' 
name='VideoSourceImage' type='SFImage'/> <field accessType='outputOnly' name='TrackedObjectCamera ModelView' 
type='SFMatrix4f'/> <field accessType='outputOnly' name='TrackedObjectCamera Projection' type='SFMatrix4f'/> 
<field accessType='outputOnly' name='TrackedObjectCamera Position' type='SFVec3f'/> <field accessType='outputOnly' 
name='TrackedObjectCamera Orientation' type='SFRotation'/> </IOSensor> In order to use the camera s correct 
modelview and projection in the scene we are using a Viewfrustrum instead of a standard Viewpoint. By 
routing the IOSensor s TrackedObjectCamera_ModelView and TrackedObjectCamera_Projection to the ViewFrustrum 
s modelview and projection the virtual camera matches the real camera s position and orientation relative 
to the marker. <Viewfrustum DEF='vf' /> <ROUTE fromNode='VisionLib' fromField='TrackedObjectCamera ModelView' 
toNode='vf' toField='modelview'/> <ROUTE fromNode='VisionLib' fromField='TrackedObjectCamera Projection' 
toNode='vf' toField='projection'/ With the camera s image in the background we are creating an Augmented 
Reality scenario. <PolygonBackground> <Appearance positions='0 0, 1 0, 1 1, 0 1' > <TextureTransform 
rotation='0' scale='1 -1'/> <PixelTexture2D DEF='tex' autoScale='false'/> </Appearance> </PolygonBackground> 
 <ROUTE fromNode='VisionLib' fromField='VideoSourceImage' toNode='tex' toField='image'/> This example 
works with standard webcams with vga resolution. An example how to manipulate instant vision s con.guration 
.le and to create and use different markers will follow. Files: visionlib.x3d (Example)  visionlib.pm 
(Con.guration File)  visionlib.pdf (Marker)  50  4.8 Vision Tracking Device This tutorial shows you 
how to vision based tracking; As example we will create a marker tracker. 4.8.1 Introduction 4.8.2 Tracking 
in General World The desciption of the world how the tracking system sees it. It contains one ore more 
Tracke­dObjects. TrackedObject Describes the objects to be tracked, thus this node contains all information 
about an object which should be tracked Marker One way to track things is the use of a marker. To describe 
a TrackedObject with a marker this node is added to the TrackedObject Camera A camera is used in every 
vision tracking system. The node contains one Intrinsic and one Extrinsic data node. ExtrinsicData Part 
of the camera description, contains the parameters descibing the position and orientation of the camera 
in the world. IntrinsicData Second part of the camera description, describes the internal parameters 
of a camera like resolution, focal length or distortion. ActionPipe The execution units (Actions) in 
InstantVision are arranged in an execution pipe which is called an ActionPipe. DataSet All data items 
used in InstantVision are placed in the DataSet and have a key(name) to refere to them. 4.8.3 The Example 
Two .les will be needed to setup an InstantReality scene with a vision tracking device. The .rst is a 
VisionLib con.guration .le, which describes the tracking setup, the second is a scene .le for IR. The 
VisionLib con.g (visionlib.pm). All images and cameras used in the VisionLib con.g are exported to InstantPlayer, 
so you can use the images as textures or backgrounds and the cameras as transformations for Viewpoint, 
Viewfrustum or ComponentTransform. The names of the Images are the same as in the VisionLib con.g, the 
cameras are split into 4 names where the .rst part names the TrackedObject from which the camera is derived 
and the post.x names the output type. In the Example these names are TrackedObjectCamera_ModelView , 
TrackedObjectCamera_Projection , TrackedObjectCamera_Position , TrackedObjectCamera_Orientation . The 
camera here is derived from World.TrackedObject which gave the name. Tracking multiple markers can be 
achieved by duplicating the TrackedObject sections and give the TrackedObjects distinctive keys and marker 
codes. As described above, you will get the camera (inverted object) transformations named like the TrackedObject 
key + (e.g.) Camera_ModelView . <?xml version="1.0" encoding="UTF-8" standalone="no" ?> <VisionLib2 Version="2.0"> 
<Plugins size="0"> </Plugins> <ActionPipe category="Action" name="AbstractApplication-AP"> <VideoSourceAction 
category="Action" name="VideoSourceAction"> <Keys size="2"> <key val="VideoSourceImage"/> <key val=""/> 
</Keys> <ActionConfig preferred height="480" preferred width="640" shutter="-1" source url="ds"/> </VideoSourceAction> 
<ImageConvertActionT ImageT RGB FrameImageT GREY Frame category="Action" name="ImageConvertAc <Keys 
size="2"> <key val="VideoSourceImage"/> <key val="ConvertedImage"/> </Keys> </ImageConvertActionT ImageT 
RGB FrameImageT GREY Frame> <MarkerTrackerAction category="Action"> <Keys size="5"> <key val="ConvertedImage"/> 
<key val="IntrinsicData"/> <key val="World"/> <key val="MarkerTrackerInternalContour"/> <key val="MarkerTrackerInternalSquares"/> 
 </Keys> <ActionConfig MTAThresh="140" MTAcontrast="0" MTAlogbase="10" WithKalman="0" WithPoseNll </MarkerTrackerAction> 
<TrackedObject2CameraAction category="Action" name="TrackedObject2Camera"> <Keys size="3"> <key val="World"/> 
<key val="IntrinsicData"/> <key val="Camera"/> </Keys> </TrackedObject2CameraAction> </ActionPipe> 
<DataSet key=""> <IntrinsicDataPerspective calibrated="1" key="IntrinsicData"> <!--Image resolution (application-dependant)--> 
<Image Resolution h="480" w="640"/> <!--Normalized principal point (invariant for a given camera)--> 
<Normalized Principal Point cx="5.0037218855e-01" cy="5.0014036507e-01"/> <!--Normalized focal length 
and skew (invariant for a given camera)--> <Normalized Focal Length and Skew fx="1.6826109287e+00" fy="2.2557202465e+00" 
s="-5.7349563803e-04"/> <!--Radial and tangential lens distortion (invariant for a given camera)--> <Lens 
Distortion k1="-1.6826758076e-01" k2="2.5034542035e-01" k3="-1.1740904370e-03" k4="-4.8766380599e­ </IntrinsicDataPerspective> 
<World key="World"> <TrackedObject key="TrackedObject"> <ExtrinsicData calibrated="0"> <R rotation="1 
0 0 &#38;#xA;"/> <t translation="0 0 0 &#38;#xA;"/> <Cov covariance="0 0 0 0 0 0 &#38;#xA;0 0 0 0 0 0 
&#38;#xA;0 0 0 0 0 0 &#38;#xA;0 </ExtrinsicData> <Marker BitSamples="2" MarkerSamples="6" NBPoints="4" 
key="Marker1"> <Code Line1="1100" Line2="1100" Line3="0100" Line4="0000"/> <Points3D nb="4"> <HomgPoint3Covd 
Cov3x3="0 0 0 &#38;#xA;0 0 0 &#38;#xA;0 0 0 &#38;#xA;" w="1" x="0" y="6" <HomgPoint3Covd Cov3x3="0 0 
0 &#38;#xA;0 0 0 &#38;#xA;0 0 0 &#38;#xA;" w="1" x="6" y="6" <HomgPoint3Covd Cov3x3="0 0 0 &#38;#xA;0 
0 0 &#38;#xA;0 0 0 &#38;#xA;" w="1" x="6" y="0" <HomgPoint3Covd Cov3x3="0 0 0 &#38;#xA;0 0 0 &#38;#xA;0 
0 0 &#38;#xA;" w="1" x="0" y="0" </Points3D> </Marker> </TrackedObject> </World> </DataSet> </VisionLib2> 
52 The scene .le instantiates an IOSensor based on the VisionLib con.g .le. The output of this IO sensor 
is then routed to a texture and a Viewfrustum node. <?xml version="1.0" encoding="UTF-8"?> <X3D> <Engine 
DEF='engine'> <TimerJob DEF='timer'/> <SynchronizeJob DEF='synchronize'/> <RenderJob DEF='render'> <WindowGroup> 
<Window position='10 50' size='640,480' fullScreen='false' /> </WindowGroup> </RenderJob> </Engine> <Scene 
DEF='scene'> <IOSensor DEF='VisionLib' type='VisionLib' configFile='visionlib.pm'> <field accessType='outputOnly' 
name='VideoSourceImage' type='SFImage'/> <field accessType='outputOnly' name='TrackedObjectCamera ModelView' 
type='SFMatr <field accessType='outputOnly' name='TrackedObjectCamera Projection' type='SFMat <field 
accessType='outputOnly' name='TrackedObjectCamera Position' type='SFVec3f <field accessType='outputOnly' 
name='TrackedObjectCamera Orientation' type='SFRo </IOSensor> <Viewfrustum DEF='vf' /> <PolygonBackground> 
<Appearance positions='0 0, 1 0, 1 1, 0 1' > <TextureTransform rotation='0' scale='1 -1'/> <PixelTexture2D 
DEF='tex' autoScale='false'/> </Appearance> </PolygonBackground> <Transform translation='0 0 0'> <Shape 
DEF='geo2'> <Appearance> <Material emissiveColor='1 0.5 0' /> </Appearance> <Teapot size='5 5 5' /> 
</Shape> </Transform> <ROUTE fromNode='VisionLib' fromField='VideoSourceImage' toNode='tex' toField='image 
<ROUTE fromNode='VisionLib' fromField='TrackedObjectCamera ModelView' toNode='vf' <ROUTE fromNode='VisionLib' 
fromField='TrackedObjectCamera Projection' toNode='vf' </Scene> </X3D> <Viewpoint DEF='vf' fieldOfView='0.5' 
/> <ROUTE fromNode='VisionLib' fromField='VideoSourceImage' toNode='tex' toField='image'/> <ROUTE fromNode='VisionLib' 
fromField='TrackedObjectCamera Position' toNode='vf' toField='position'/> <ROUTE fromNode='VisionLib' 
fromField='TrackedObjectCamera Orientation' toNode='vf' toField='orientation #VRML V2.0 utf8 DEF trackingSensor 
IOSensor { type "VisionLib" configFile "visionlib.pm" 53 eventOut SFImage VideoSourceImage eventOut 
SFMatrix4f TrackedObjectCamera ModelView eventOut SFMatrix4f TrackedObjectCamera Projection eventOut 
SFVec3f TrackedObjectCamera Position eventOut SFRotation TrackedObjectCamera Orientation } DEF trans 
Transform { children [ Shape { appearance Appearance { texture DEF tex PixelTexture2D { } } geometry 
Box { } } ] } ROUTE trackingSensor.VideoSourceImage TO tex.image ROUTE trackingSensor.TrackedObjectCamera 
Orientation TO trans.rotation 4.8.4 Modi.cations This section gives you some clues what to change to 
get your setup running. 4.8.4.1 VideoSource The example above uses DirectShow (or QT on the Mac) to access 
a camera. This should work for all cameras which support it, these will usually have a WDM driver to 
be installed. To use other cameras you need to change the VideoSource:ActionCon.g:source_url .eld in 
the .pm .le. There are also a number of arguments which can be passed to the video source driver. The 
arguments are added to the source url like this: driver://parameter1=value;parameter2=value . Some drivers 
and their parameters are (available on platform in parentheses): ds (win32, darwin) Windows driver as 
mentioned above, on a Mac this is the same as qtvd Param­eters are: device -string name of the camera, 
mode -string name of the mode, framerate ­integer. The driver compares the given parameters to whatever 
DS reports about the camera, if there is a mach the maching parameters are used, other values are ignored. 
vfw (win32) Old VideoForWindows driver. That is a good luck driver, no parameters implemented. v4l (linux) 
Works with video4linux (old version 1). No parameter support yet but you can pass something like v4l:///dev/myvideodev 
to select a device and it reads environment variables VIDEO_SIZE which is an integer value [0-10] which 
selects a video size between 160x120 and 768x576 ieee1394 (win32) FireWire DC cameras which run with 
the CMU driver http://www.cs.cmu.edu/~iwan/1394/index.html on windows. No parameters available for now. 
ieee1394 (linux) FireWire DC cameras which run with the video1394 kernel module and libdc1394 (coriander), 
which includes PGR devices. Parameters are: unit -integer value for selecting a camera at the bus, trigger 
-boolean [0,1] 1 switches on external trigger, downsample -boolean [0,1] downsamples a bayer coded image 
to half size, device -string like /dev/video1394/0 the device .le to use. ieee1394pgr (win32) PointGreyResearch 
cameras, license needed. Parameters are: unit -integer value for selecting a camera at the bus, trigger 
-boolean [0,1] 1 switches on external trigger, downsample -boolean [0,1] downsamples a bayer coded image 
to half size, mode -string value to select a mode, when passing mode=320 some mode with a resolution 
of 320x240 is selected. ueye (win32, linux) IDS imaging uEye cameras, license needed (more adjustments 
then the ds drivers) Parameters: downsample -boolean [0,1] downsamples a bayer coded image to half size, 
vrmc (win32) VRmagic cameras, license needed, No parameters supported yet. qtvd (darwin) Mac QuickTimeVideoDigitizer, 
no parameters yet Some of these drivers require additional libs/dlls which must be installed on your 
system and in the path. 4.8.4.2 Marker A marker in IV is described by a 4x4 code mask and four corner 
points. You can easily change the marker code by editing the .elds DataSet:World:TrackedObject:Marker:Code:LineX. 
The marker is made of 4 lines Line1 = 1100 Line2 = 1100 Line3 = 0100 Line4 = 0000 where 0 = black and 
1 = white, e.g. the real marker must have a black square around this and another white square around. 
It will look like One way to create and print whose things is to go into word and create a 8x8 table, 
make the rows and cols the same size and color the cell background with black n white. You can also change 
the position of the marker in the world by changing ...Marker:Points3D:HomgPoint3Covd:[xyz] values, make 
sure the marker stays rectangular and planar. The points describe the outer black bor­der (6x6 .eld) 
not the white surrounding, they corrospond to upper left, upper right, lower right, lower left corners 
of the image. You also use multiple markers in one TrackedObject, just duplicate DataSet:World:TrackedObject:Marker 
and change one of them to re.ect its physical position on the object you want to track.  4.9 Apple 
Sudden Motion Sensor This section shows you how to use the Apple Sudden Motion Sensor inside a 3d scene. 
4.9.1 Introduction In 2005 Apple introduced the Sudden Motion Sensor for its portable computers in order 
to protect the hardrive. This example only works on Apple Powerbooks, iBooks, MacBooks and MacBooks Pro 
build after 2005. The sensor is a 3-axis accelerometer. The AppleMotionSensor backend delivers a Vec3f 
with the three acceleration values. 4.9.2 Shaking In the .rst example we create an IOSensor of the type 
AppleMotionSensor and route its values to a Transform node. The values are getting smoothed by a PositionDamper. 
The AppleMotionSensor is loaded like any other HID device via an IOSensor. The acceleration values are 
stored in the .eld Motion : Motion (SFVec3f) : Acceleration values (x, y, z) <IOSensor DEF='AppleMotionSensor' 
type='AppleMotionSensor'> <field accessType='outputOnly' name='Motion' type='SFVec3f'/> </IOSensor> The 
acceleration values could be routed to a Transform node. But in order to smooth the values we are putting 
a PositionDamper inbetween. <Transform DEF='tr'> <Shape> <Appearance> <Material diffuseColor='1 1 1' 
/> </Appearance> <Box/> </Shape> </Transform> <PositionDamper DEF='pd' tau='0.1' /> <ROUTE fromNode='AppleMotionSensor' 
fromField='Motion' toNode='pd' toField='set destination'/> <ROUTE fromNode='pd' fromField='value changed' 
toNode='tr' toField='set translation'/> Files: test_suddenMotion.x3d (Example) 4.9.3 Tilt In this second 
example we are mapping the acceleration on the orientation of an object. Calling the SFRotation() constructor 
with the acceleration vector and a vector SFVec3f(0,1,0) calculates the sensor s orientation. By routing 
that value on a Transform s rotation the object seems to keep its position while rotating the notebook. 
<Script DEF='script'> <field accessType='inputOnly' name='set motion' type='SFVec3f'/> <field accessType='outputOnly' 
name='rotation changed' type='SFRotation'/> javascript: var rotation changed; var vector = new SFVec3f(0,1,0); 
 function set motion(motion) { rotation changed = new SFRotation(motion, vector); } ] ]> </Script> 
<ROUTE fromNode='AppleMotionSensor' fromField='Motion' toNode='script' toField='set motion'/> <ROUTE 
fromNode='script' fromField='rotation changed' toNode='tr' toField='set rotation'/> Files: test_suddenMotion_02.x3d 
(Example)  4.10 Serial Communication This section shows you how to communicate with a serial device. 
4.10.1 Introduction This section shows how to communicate with a serial device within a 3D scene. Possible 
devices are microcontroller boards like Arduino or Wiring , but also for example Wacom tablets, GPS devices 
and rotary encoders. 4.10.2 Setting up the serial port The serial port is set up with an IOSensor node 
at the beginning of the scene. The following param­eters are available: Device : A number starting at 
0 specifying the serial interface to use. 0 for COM1 or the .rst tty serial device. (default: 0)  BaudRate 
: Baud rate of the serial port. 9600, 19200, ... (default: 9600)  DataBits : Number of data bits used 
for the communication on the serial port. Possible values are 7 or 8. (default: 8)  Parity : Type of 
parity used for the communication on the serial port. Possible values are even, odd or none. (default: 
none)  StopBits : Number of stop bits used for the communication on the serial port. Possible values 
are 1 or 2. (default: 1)  Handshake : Type of handshake (.ow control). Possible values are none, hardware 
or software. (default: none)  DTR : The status of the DTR line.  RTS : The status of the RTS line. 
 Init String : An initialisation string that is send to the serial device to start operation.  Deinit 
String : A deinitialisation string that is send to the serial device to stop operation.  Delimiter : 
Ascii value of the character that splits the serial message (default: no delimiter)  MaxBytes : The 
maximum number of bytes a message consists of. A value of -1 means that there is no maximum number of 
bytes. (default: -1)  <IOSensor DEF='serial' type='serial' Device='0' Delimiter='10' BaudRate='9600'> 
<field accessType='outputOnly' name='Data out' type='SFString'/> <field accessType='inputOnly' name='Data 
in' type='SFString'/> </IOSensor> Here we are seeting up a serial device with 9600 baud rate at COM1 
or tty.usbserial-00001. The delimiter is set to a line break (ASCII value: 10). All other parameters 
have the default values. There are two .elds for the incoming and outgoing data: Data out : Data from 
the serial device to the scene  Data in : Data from the scene to the serial device  4.10.3 Sending 
Data to the Serial Port By routing the KeySensors keyPress .eld to the serial devices Data in .eld we 
are sending each keystroke (SFString) to the serial port. We also specify a name for the device handler 
via the name .eld. <KeySensor DEF='keysensor' /> <ROUTE fromNode='keysensor' fromField='keyPress' toNode='serial' 
toField='Data in'/>  4.10.4 Receiving Data from the Serial Port In order to get the data from the serial 
port and to show it in the scene we are routing the values from the serial devices Data out .eld to a 
Text nodes string .eld. <Transform> <Shape> <Text DEF='text' string='' solid='true'> </Shape> </Transform> 
<ROUTE fromNode='serial' fromField='Data out' toNode='text' toField='string'/> 4.10.5 Example Scene 
This is a simple example for the communication between an Arduino microcontroller and instant viewer 
. We are sending keystrokes from the scene to the controller. The software on the Arduino board switches 
an LED on when 1 is sent and switches it off when 2 is sent. It sends the Strings On and Off back to 
the scene where it is routed on a Text node s string. <X3D> <Scene DEF='scene'> <IOSensor DEF='serial' 
type='serial' Device='0' Delimiter='10' BaudRate='9600'> <field accessType='outputOnly' name='Data out' 
type='SFString'/> <field accessType='inputOnly' name='Data in' type='SFString'/> </IOSensor> <Viewpoint 
position='0.625 0.3 1.9' /> <Transform> <Shape> <Appearance> <Material diffuseColor='1 1 1' /> </Appearance> 
<Text DEF='text' string='/../' solid='true'> <FontStyle justify='BEGIN' family='SANS' /> </Text> </Shape> 
</Transform> <ROUTE fromNode='serial' fromField='Data out' toNode='text' toField='string'/> <KeySensor 
DEF='keysensor' /> <ROUTE fromNode='keysensor' fromField='keyPress' toNode='serial' toField='Data in'/> 
 </Scene> </X3D> void setup() { Serial.begin(9600); pinMode(13, OUTPUT); } void loop() { if (Serial.available() 
> 0) { int incoming = Serial.read(); if ((char)incoming == '1') { digitalWrite(13, HIGH); Serial.println("On"); 
 } else if ((char)incoming == '2') { digitalWrite(13, LOW); Serial.println("Off"); } 59 } } Files: 
 serialTutorial.x3d  serialExample.pde  Chapter 5  Clustering 5.1 Cluster Basics and Load Balancing 
This section gives an overview over cluster topologies in computer graphics. It continues with the often 
ignored topic of load balancing, as clusters are mostly used to set up multi display environments without 
a real distribution of load. In InstantReality the activation of load balancing is very easy. You will 
also get a basic introduction on how to setup your network and start cluster servers to be ready to use 
for your cluster. Different setups will be explained in the following sections of this Clustering section. 
5.1.1 Topologies To make expensive computations faster, a common approach nowadays is establishing a 
cluster of more or less convenient hardware, let s say PCs. In computer graphics we can imagine several 
con.gurations using a cluster of PCs as you can see in following scenarios. Most common systems look 
like the illustration below, several PCs are connected over a network to render and display a virtual 
scene together on multiple displays. The advantage is a high resolution but as one PC is responsible 
for exactly one display the system is only as fast as the PC which has the highest render load. To render 
complex geometries a con.guration as shown below is possible. The scene has to be distributed to PCs 
in a cluster, they compute their task and send the results back to the PC which composes and displays 
the .nal image. In this case we use a cluster to render complex scenes but we lose the high resolution 
of a multi display system. If we like to bene.t of both advantages, i.e. high resolution and effective 
rendering of complex models, we need a .exible system. In this system we have several PCs in a network 
where some are playing a role as display and others that are not. This topology can also be used as stereo 
setup for instance. The InstantReality system provides the concept of using an arbitrary number of displays 
in a speci.ed alignment together with an arbitrary number of PCs. Those can be connected to a display 
or not. This .exible concept allows each of the above basic setups. But it can also be used to create 
more complex con.gurations like cave environments with displays which are orthogonal to each other or 
even displays in an arbitrary angle. To see how simple a cluster is con.gured please read the following 
sections in the Clustering section. But you should .nish this one for load balancing aspects and basic 
IR cluster setup information. 5.1.2 Load balancing In a cluster, PCs should share the overall load equally 
between each other to be effective and to get the best performance. In computer graphics all of them 
generate an equally cost intensive part of the scene. A proper and fast precomputation of the scene takes 
place before distributing parts of it to the PCs. Finally every rendered section which belongs to another 
display PC is copied over the network to its target. This approach offers arbitrary setups like single 
display and multi display systems, both as mono or stereo solution with an effective balancing of the 
upcoming load. There are several approaches to balance the load of 3D scenes. Two big categories are 
image space distribution and geometry distribution. 5.1.2.1 Image space balancing (Sort-First) In image 
space distributed balancing, also called Sort-First , a precomputation takes place which transforms only 
bounding boxes of objects into camera space to get the approximate position of geometries on the displays. 
By this information a cost function is estimated. This is based on transformation costs as well as the 
rasterization costs and therefore takes the number of vertices and size of rendered bounding boxes into 
account. Now that each PC has it s estimated costs, parts of viewports are distributed to render on 
an­other PC which has only low costs. The resulting rendered parts are sent back to the display PC over 
network. If you want to learn more about the implemented cost estimation and load balancing algorithms, 
please check the Technical Details section at the end of this section.  5.1.2.2 Geometry based balancing 
(Sort-Last) In this kind of load balancing, parts of the scenegraph are distributed. That means geometries 
are distributed between PCs and after rendering the pixel data including the depth information is copied 
back to the display PC. On the display PC all received images are composed to one image again by involving 
the depth. This approach shouldn t be used on multi display systems for one reason. Geometries can be 
bigger than a single display resolution, so it can t be rendered by only one graphics card as it doesn 
t .t to the framebuffer. But on a single display cluster system it is very fast and a better choice than 
Sort-First . Important To use load balancing effectively it is very important to have 1000Mbit network, 
because pixel data has to be sent over network. Otherwise you won t have the advantages of load balancing. 
  5.1.3 Network con.guration Setup your network. Each host must be reachable by name from each other 
host. Try this with ping host . If you have a local network (no access to the internet), you have to 
de.ne a dummy gateway eg. 192.168.1.254 if your network uses the IP-Range 192.168.1.0 -192.168.1.253. 
On Linux, if your /etc/hosts .le contains a line like 127.0.0.1 myHostname where myHostname is not localhost, 
then remove this line. 5.1.4 InstantPlayer and InstantCluster Doing cluster rendering with the InstantReality 
framework you have one instance of the InstantPlayer running your X3D application. This application provides 
the user interface, loads the x3d .le including the engine con.guration and does all the simulation for 
your scene. To be able to produce a graphics output on another host, you have to run an InstantCluster 
on this host. Start the InstantCluster entry in your menu or application directory or use an autostart 
mechanism to run it all the time. It only needs resources while rendering and otherwise sleeps and waits 
for connections. 5.1.5 Technical Details For technical details about the algorithms of the load balancing 
and some benchmarks I suggest to have a look into the paper Load Balancing on Cluster-Based Multi Projector 
Display Systems . With sort-.rst approach, i.e. image space based distribution, we got a speedup of 3 
to 6 for a single display setup and 16 PCs in a cluster. On multi display systems with 48 PCs (24 PCs 
for displaying in 6 x 4 alignment) animations were 3 to 4 times faster. Sort-last balancing with a model 
of Standford s David Statue and 56 millions of polygons achieved a speedup of 10 with 16 PCs in the cluster 
and even over 20 with more than 32 PCs. For the composition of image parts it uses a new pipeline approach. 
 5.2 CAVE cluster This section demonstrates how to setup a CAVE environment with three projection walls. 
It will also take stereo functionality into account which is essential in a CAVE as well as load balancing 
between cluster PCs. To get knowledge of multi display con.gurations and stereo setups, I suggest to 
work through the other Clustering sections and Multiple Views and Stereo . After that you will know everything 
about a ClusterWindow and Viewarea s. Those nodes will be used in this section and extended by a view 
modi.er named ProjectionViewModi.er . 5.2.1 Aspects of a CAVE A CAVE consists of up to six projection 
walls which are usually aligned orthogonal to each other to build a cubic room or a part of a cubic room. 
That s the main reason why we have to use Cluster-Window instead of the precon.gured node TiledClusterWindow 
(see section Multiple display cluster ). Each wall shows a different view of the scene, i.e. the camera 
looks at different directions. So we have to con.gure each view manually with a ProjectionViewModi.er 
which modi.es the camera orientation. Another attribute of a CAVE is it s immersive character, due to 
stereo projection.  5.2.2 Assumptions In this section we assume to have a CAVE with 3 walls. A bottom 
plane, a front plane and one side wall. Each wall has the size of 2.4 x 2.4 meters. We want the camera 
of our scene to be in the middle of the CAVE. Let s also assume, our scene is modeled in meters, so the 
side wall is -1.2 to the left of the camera, the bottom plane -1.2 below and the front plane -1.2 to 
the front (negative z-axis). Each projection should also have a square resolution of 1024 x 1024 pixels, 
generated by PCs with a standard framebuffer resolution of 1280 x 1024 pixels. 5.2.3 Setting up the 
views Now we want to con.gure a different view for each wall. Therefore a concept exists which allows 
to de.ne a plane in space with four points. This plane acts as a projection plane for the scene relative 
to the camera. The projection plane is con.gured via the node ProjectionViewModi.er , a modi.er for the 
Viewarea node like ShearedStereoViewModi.er . It also inherits the .elds leftEye , rightEye and eyeSeparation 
from the stereo modi.er. The projection view modi.er for the left wall and the left eye in our CAVE setup 
will look like this: ... modifier [ DEF mod front left ProjectionViewModifier { surface [ -1.2 -1.2 -1.2, 
1.2 -1.2 -1.2, 1.2 1.2 -1.2, -1.2 1.2 -1.2 ] leftEye TRUE rightEye FALSE eyeSeparation 0.08  } ] ... 
 The surface points have to be counterclockwise, starting with the lower left corner. For the right eye 
on the same wall you just have to set leftEye to FALSE and rightEye to TRUE . Respectively the front 
wall has to be set up like this: ... modifier [ DEF mod left left ProjectionViewModifier { surface [ 
-1.2 -1.2 1.2, -1.2 -1.2 -1.2, -1.2 1.2 -1.2, -1.2 1.2 1.2 ] leftEye TRUE rightEye FALSE eyeSeparation 
0.08 } ] ... And .nally for the .oor: ... modifier [ DEF mod bottom left ProjectionViewModifier { surface 
[ -1.2 -1.2 1.2,  1.2 -1.2 1.2, 1.2 -1.2 -1.2, -1.2 -1.2 -1.2 ] leftEye TRUE rightEye FALSE eyeSeparation 
0.08 } ] ... Important Important: Use the same unit (e.g. metres, millimetres) for the projection surfaces 
like your scene is modeled in. Otherwise you will get interesting .eld of views. With stereo con.guration 
we now have 6 different views. Each view should be rendered by one PC. Let s call them front_leftEye 
, front_rightEye , left_leftEye , left_rightEye , bottom_leftEye and bottom_rightEye . As mentioned in 
the Assumptions section, each PC has a resolution of 1280 x 1024 pixels. So we will need a window with 
enough space for each PC (better said framebuffer of PC), which results in a ClusterWindow of a size 
of 7680 x 1024 pixels. The ClusterWindow con.guration now looks like this: ... ClusterWindow { servers 
[ "front leftEye" "front rightEye" "left leftEye" "left rightEye" "bottom leftEye" "bottom rightEye" 
] size 7680 1024 hServers 6 vServers 1 ... } ... This con.guration results in the following partitioning 
of the window: Figure 5.9: Partitioning of the cluster window The last missing issue is the setup of 
view areas on the cluster window, because CAVE walls are square (1024 x 1024) but framebuffers of the 
PCs are not (1280 x 1024). We will de.ne a square Viewarea per non-square PC region on the cluster window 
and put one of the above projection view modi.ers into each. Then we will obtain the .nal con.guration: 
... DEF render RenderJob { windowGroups [ WindowGroup { windows [ LocalWindow { #This window is just 
for interaction enabled FALSE } ClusterWindow { servers [ "front leftEye" "front rightEye" "left leftEye" 
"left rightEye" "bottom leftEye" "bottom rightEye" ] size 7680 1024 hServers 6 vServers 1 views [ #Front 
wall, left eye Viewarea { lowerLeft 0 0 upperRight 1023 1023 modifier [ DEF mod front left ProjectionViewModifier 
{ surface [ -1.2 -1.2 -1.2, 1.2 -1.2 -1.2, 1.2 1.2 -1.2, -1.2 1.2 -1.2 ] leftEye TRUE rightEye FALSE 
eyeSeparation 0.08 } ] } #Front wall, right eye Viewarea { lowerLeft 1280 0 upperRight 2303 1023 modifier 
[ DEF mod front right ProjectionViewModifier { surface [ -1.2 -1.2 -1.2, 1.2 -1.2 -1.2, 1.2 1.2 -1.2, 
-1.2 1.2 -1.2 ] leftEye FALSE rightEye TRUE eyeSeparation 0.08 } ] } #Left wall, left eye Viewarea { 
lowerLeft 2560 0 upperRight 3583 1023 modifier [ DEF mod left left ProjectionViewModifier { surface 
[ -1.2 -1.2 1.2, -1.2 -1.2 -1.2, -1.2 1.2 -1.2, -1.2 1.2 1.2 ] leftEye TRUE rightEye FALSE eyeSeparation 
0.08 } ] } #Left wall, right eye Viewarea { lowerLeft 3840 0 upperRight 4863 1023 modifier [ 69 DEF 
mod left right ProjectionViewModifier { surface [ -1.2 -1.2 1.2, -1.2 -1.2 -1.2, -1.2 1.2 -1.2, -1.2 
1.2 1.2 ] leftEye FALSE rightEye TRUE eyeSeparation 0.08 } ] } #Bottom, left eye Viewarea { lowerLeft 
5120 0 upperRight 6143 1023 modifier [ DEF mod bottom left ProjectionViewModifier { surface [ -1.2 -1.2 
1.2, 1.2 -1.2 1.2, 1.2 -1.2 -1.2, -1.2 -1.2 -1.2 ] leftEye TRUE rightEye FALSE eyeSeparation 0.08 } 
] } #Bottom, right eye Viewarea { lowerLeft 6400 0 upperRight 7423 1023 modifier [ DEF mod bottom right 
ProjectionViewModifier { surface [ -1.2 -1.2 1.2, 1.2 -1.2 1.2, 1.2 -1.2 -1.2, -1.2 -1.2 -1.2 ] leftEye 
FALSE rightEye TRUE eyeSeparation 0.08 } ] } ] } ] } ] } ... View areas are set in pixels instead 
of relative window coordinates for following reasons: Readability: Pixel coordinates are much better 
to associate to a region than something like 70 0.633205... Accuracy: Relative coordinates like 0.633333 
are not as accurate than de.ned pixel coordi­nates  Calibration: You don t have to calibrate projectors 
with pixel accuracy, just calibrate the view areas  Finally the result are 6 views you need for the 
CAVE. The image below shows only three views because it doesn t take stereo into account. It shows the 
Dome of Siena with front, left and bottom views. For a better visualization in the top left corner these 
views are texturing a virtual 3-sided CAVE.  5.2.4 Load balancing If you ve read the other Clustering 
sections, you will know how to switch load balancing on. Just add these two lines into the ClusterWindow 
node. The second one is just for debugging purpose to see how load balancing works. Be sure to use Gigabit 
LAN to obtain an effective balancing. ... balance TRUE showBalancing TRUE ... 5.2.5 Head tracking One 
important issue has not been taken into account yet. As a user is moving around in a CAVE, the eye position 
is not the same as the camera position which is in the middle due to our setup of the projection planes. 
So the viewing frustums for each wall are not correct for the users .eld of view. They have to be adapted 
to the users position like the image below illustrates. You see a projection plane with a red and a 
blue tree. The tree images belong to the red and blue viewing frustums. A users head is represented by 
coordinate systems which lie at the end of the frustums. When the head moves from blue to red position 
the accordant frustum is signi.cant to show the correct view on the projection plane. This additional 
modi.cation of the camera is also done in the ProjectionViewModi.er . A head tracking device is needed 
which returns a 4 x 4 transformation matrix for position and orientation of a head in a CAVE. The hardware 
is mostly an infrared or a magnetic device attached to stereo glasses. The device handling is not in 
scope of this section, but .nally if you have a node which produces a transformation matrix from the 
device, you have to route it to the set_eyeTransform .eld of all ProjectionViewModi.er nodes like this: 
... DEF render RenderJob { ... } ROUTE headSensor.value changed TO mod front left.set eyeTransform ROUTE 
headSensor.value changed TO mod front right.set eyeTransform ROUTE headSensor.value changed TO mod left 
left.set eyeTransform ROUTE headSensor.value changed TO mod left right.set eyeTransform ROUTE headSensor.value 
changed TO mod bottom left.set eyeTransform ROUTE headSensor.value changed TO mod bottom right.set eyeTransform 
... Files: CaveStereo.wrl  tie.wrl (test model)   5.3 Single display cluster This section shows how 
to setup a cluster for a single display con.guration. It also describes how to use real load balancing 
with this setup. Please read the section Cluster Basics in the Clustering category to get an overview 
of how to con.gure a PC cluster using InstantCluster. 5.3.1 Single Display cluster In this section we 
want to use three PCs of our cluster to render an image on a single display. That means one PC is designated 
to display the .nal scene while the others support it by rendering parts of the scene on their local 
framebuffer and sending the results (pixels of the image region) back. The appropriate RenderJob section 
will look like this: ... DEF render RenderJob { windowGroups [ WindowGroup { windows [ LocalWindow { 
#This window is just for interaction enabled FALSE } ClusterWindow { servers [ "displaypc" "clusterpc1" 
"clusterpc2" ] hServers 1 vServers 1 size 1024 768 balance TRUE showBalancing TRUE } ] } ] } ... We see 
two windows here, the .rst one is the LocalWindow , which only exists for user interaction. We disable 
rendering here to gain a real speedup for the cluster window. Otherwise the PC with the local window 
would have to render the whole scene itself. The important part of the con.guration is the ClusterWindow 
. This line servers [ "displaypc" "clusterpc1" "clusterpc2" ] lists the hostnames of PCs, which should 
take part in the cluster. It is followed by the speci.cation of the display area by setting the number 
of horizontal and vertical displays as well as its resolution: hServers 1 vServers 1 size 1024 768 In 
this example we just use a single display with a resolution of 1024 x 768 pixels. The server which is 
responsible for displaying is displaypc , because it is the .rst server in the servers list. The next 
two lines set up the load balancing, where the .eld showBalancing is just for debugging purpose and pigments 
those areas of the rendered image which are generated by other servers and copied over the network. balance 
TRUE showBalancing TRUE The image shows framebuffers of three PCs, where the left one is dedicated to 
display the whole scene (single display constellation). The other ones generate rectangular parts of 
the scene and copy the pixel data to the .rst PC over a fast network. You can see the copied parts on 
the left image as coloured rectangles if showBalancing is set to TRUE. It is very important for an effective 
load balancing to use a Gigabit network. Let s say we have a display PC and one additional server with 
a resolution of 1280 x 1024 pixels each. In the worst case the server has to deliver half or more of 
the the screen to the display PC. This is 1280 x 1024 / 2 = 655.360 pixels and for each pixel three color 
components (RGB), which results in almost 2 MB of data per frame. In a 100Mbit network we can send about 
10 MB per second, so we would get a framerate of 5 fps! Files: SingleDisplayLoadBalancing.wrl  SingleDisplayLoadBalancing.x3d 
(same as above but in X3D syntax)  tie.wrl (test model)   5.4 Multiple display cluster This section 
shows how to setup a cluster for a multi display con.guration. It will also implement real load balancing 
with this setup and extend it to a stereo con.guration with .ve PCs. Please read the Cluster Basics section 
in the Clustering category to get an overview of how to con.gure a PC cluster using InstantCluster. For 
information about stereo con.gurations you should take a look into the Multiple Views and Stereo section 
category (especially Multiple Windows and Views , Stereo Basics and Passive Stereo sections). 5.4.1 Multi 
display cluster In this chapter we want to use three PCs of our cluster to render a scene over two displays. 
On one PC, just the application will run in a local window to provide interaction. Two other PCs (displaypc1 
and displaypc2) are designated to display the scene over two screens. 5.4.1.1 Different concepts There 
are two concepts of doing this. The .rst one is by using the known ClusterWindow node and the second 
is the TiledClusterWindow which is especially created for n*m displays arrangements and provides overlapping 
features. The latter is based on the .rst one internally and just simpli.es the usage on some setups. 
So the question is, when to use which node. The following list should get you on the right way. You should 
use a TiledClusterWindow if: you use multiple homogeneous displays to act as one big display  all displays 
are in one plane  above points apply and you want to use stereo  you don t want to see borders between 
your displays (overlapping projections) ClusterWindow  you use a single display cluster  you setup 
a CAVE, i.e. multiple displays, but not in the same plane  you want to con.gure view areas manually 
(e.g. for a CAVE), otherwise TiledClusterWindow is better to use  The main difference is the reduction 
of work when you have to con.gure stereo setups in a multi display cluster, because TiledClusterWindow 
con.gures view areas and different projection parameters for each area by itself. Another difference 
is the ability of this node to take overlapping into account. For CAVE setups you have to con.gure view 
areas manually, so you are free to arrange CAVE walls as you want. In this section both approaches will 
be explained and you will soon realize the advantage of the TiledClusterWindow . 5.4.1.2 Using ClusterWindow 
The appropriate RenderJob section will look like this: ... DEF render RenderJob { windowGroups [ WindowGroup 
{ windows [ LocalWindow { #This window is just for interaction enabled FALSE } ClusterWindow { servers 
[ "displaypc1" "displaypc2" ] hServers 2 vServers 1 } ] } ] } ... We see two windows here, the .rst one 
is the LocalWindow , which only exists for user interaction. We disable rendering here, otherwise the 
PC with the local window would have to render the whole scene itself. This can be a problem with large 
models, especially when using load balancing for the cluster later. The important part of the con.guration 
is the ClusterWindow . This line servers [ "displaypc1" "displaypc2" ] lists the hostnames of PCs, which 
should take part in the cluster. It is followed by the speci.cation of the display area by setting the 
number of horizontal and vertical displays: hServers 2 vServers 1 By setting hServers to 2 and vServers 
to 1, the whole window consists of two horizontal aligned displays. As there are just two displays (hServers 
* vServers), the .rst two servers (displaypc1 and displaypc2) are responsible for displaying the window 
area which is as large as the accumulated native resolutions of the PCs displays. The local window is 
opened on the machine from where the VRML .le is loaded. 5.4.1.3 Using TiledClusterWindow The appropriate 
RenderJob section will look like this: ... DEF render RenderJob { windowGroups [ WindowGroup { windows 
[ LocalWindow { #This window is just for interaction enabled FALSE } TiledClusterWindow { servers [ 
"displaypc1" "displaypc2" ] columns 2 rows 1 } ] } ] } ... In this con.guration the tiled window has 
2 columns and 1 row as in the ClusterWindow case. When using the TiledClusterWindow, there are a few 
additional options, like overlapping: overlapX 20 overlapY 0 These lines result in a region between 
the two displays which is rendered twice. So when using two video beamers, you can adjust these by taking 
the overlapping into account and an intersection will not be as noticable as without overlaps.   5.4.2 
Load balancing The next two lines set up the load balancing, where the .eld showBalancing is just for 
debugging purpose and pigments those areas of the rendered image which are generated by other servers 
and copied over the network. You just have to add these into the ClusterWindow node or TiledCluster-Window 
respectively. balance TRUE showBalancing TRUE That means both cluster PCs support each other by rendering 
parts of the scene on their local framebuffer and sending the results (pixels of the image region) back. 
The role changes depending on which PC has the higher load. Additionally the resolution of the window 
has to be de.ned when using load balancing. In ClusterWindow you have to set the overall size of the 
window: size 2048 768 You may adjust it to 2560 x 1024, if your single display resolution is 1280 x 1024. 
In TiledCluster-Window the size of the window is de.ned by setting the width and height of one tile. 
Together with columns and rows .elds the window size is calculated internally: tileWidth 1024 tileHeight 
768 Tile sizes have usually to be adapted to the native resolution of one single display. We will also 
attach an additional PC which is only used as support for the display PCs. We write the PC s name at 
the end of the servers list, because the .rst (de.ned through hServer/vServers or rows/columns ) servers 
are automatically used as display. servers [ "displaypc1" "displaypc2" "supportpc" ]  5.4.3 Multi display 
stereo con.guration In a stereo con.guration the existance of TiledClusterWindow will become clear. Imagine 
a stereo setup of two displays, i.e. two PCs for the displays of the left eye and two PCs for the displays 
of the right eye. One display (one tile) has the resolution of 1280 x 1024 pixels, so the whole window 
will have 2560 x 1024. To foreclose the solution with a TiledClusterWindow here is the simple con.guration: 
... DEF render RenderJob { windowGroups [ WindowGroup { windows [ LocalWindow { #This window is just 
for interaction enabled FALSE } TiledClusterWindow { servers [ "display leftSide leftEye" "display 
leftSide rightEye" "display rightSide leftEye" "display rightSide rightEye" ] tileWidth 1280 tileHeight 
1024 stereo TRUE eyeSeparation 0.08 zeroParallaxDistance 1 columns 2 rows 1 } ] } ] } ... The role 
of a server in the servers list is well de.ned here. If stereo is set to TRUE, the .rst server (display_leftSide_leftEye) 
will render the left eye camera of the .rst display, second server (display_leftSide_rightEye) will render 
the right eye camera of the .rst display. With third and fourth servers it s the same but for the right 
side. In a grid of m columns and n rows of displays the .rst one is always lower left and the last one 
upper right. Additional servers are only used by load balancing if it is switched on. The ClusterWindow 
approach is more .exible as you can setup view areas which can be stereo or not. You can try to setup 
the above scenario to get the same result on the displays using Cluster-Window , but you might not want 
to. A hint: You ll need a window with four displays .tting in and four viewports each modi.ed by a ProjectionViewModi.er 
. How this is done and where you will need this is discussed in the next section for setting up a CAVE 
environment. Files: MultiDisplayLoadBalancing.wrl  TiledDisplayLoadBalancing.wrl  TiledDisplayStereo.wrl 
 tie.wrl (test model)  Chapter 6  Scripting 6.1 Scripting: Java This section shows you how to use 
Java in script nodes for making your scene dynamic. 6.1.1 Introduction 6.1.2 Setting up the Scene and 
the Script Java script nodes are exactly looking like Ecmascript nodes in X3D. Only the url .eld is linking 
to a .class .le instead of a .js .le. <Script DEF='javanode' directOutput='true' url='InstantJava.class'> 
<field name='set touchtime' type='SFTime' accessType='inputOnly'/> <field name='get newcolor' type='SFColor' 
accessType='outputOnly'/> </Script> The two .elds of the script node are de.ning the incoming and outgoing 
values. set_touchtime will route an SFTime value into Java. get_newcolor s value will get .lled by Java 
and routed on a Material node in the Scene. 6.1.2.1 Setting up a Java Class Java Classes extend vrml.node.Script 
. Just like Ecmascripts Java Scripts have the common initial­ize() , shutdown() and processEvent() functions. 
This is an example how a basic Java Class looks like: import vrml.*; import vrml.field.*; import vrml.node.*; 
import vrml.Event; public class InstantJava extends Script { public void initialize() { System.out.println("initializing 
java.."); } public void processEvent( Event e ) { } public void shutdown() { System.out.println("bye!"); 
 } }  6.1.2.2 Getting Values from the Scene Incoming events are processed in the processEvent() function. 
In this example the touchTime .eld of a TouchSensor is routed on the script and catched by an if condition. 
The event has to be casted into the right type. public void processEvent( Event e ) { if (e.getName().equals("set 
touchtime")) { ConstSFTime time = (ConstSFTime)e.getValue(); System.out.println( "touched at " + time.getValue()); 
} } 6.1.2.3 Writing back Values to the Scene In order to send values to the scene we have to get the 
eventOut in the initialize() function and cast it into the right type. With the function setValue(value) 
we are sending the values to the script node s .eld in the scene. public SFColor get newcolor; public 
void initialize() { get newcolor = (SFColor)getEventOut("get newcolor"); get newcolor.setValue(1, 0.5, 
0); } Files: InstantJava.x3d  InstantJava.java  InstantJava.class  Chapter 7   Animation 7.1 
Followers This section shows you how to use damper and chaser nodes for animation. 7.1.1 Introduction 
Followers divide in Dampers and Chasers. They are an easy to use alternative for common X3D interpolators. 
You only have to de.ne a destination value and the duration of the interpolation in order to create a 
tween. Instant Reality provides the following dampers and chasers: ColorChaser  ColorDamper  CoordinateChaser 
 CoordinateDamper  OrientationChaser  OrientationDamper  PositionChaser2D  PositionChaser3D  PositionDamper2D 
 PositionDamper3D  ScalarChaser  ScalarDamper  TexCoordChaser  TexCoordDamper  7.1.2 PositionChaser3D 
This is an example about a PositionChaser3D that lets an object follow the mouse pointer. Other damper 
and chaser nodes follow the same logic. Initially only the PositionChaser s duration -the time it takes 
to get to the destination value -has to be de.ned. <PositionChaser3D DEF='pc' duration='5' /> The mouse 
pointer s position on a plane is recognized by a TouchSensor and routed to the Po­sitionChaser3D s destination 
value. By routing the PositionChaser3D s value_changed .eld to a Transform s set_translation the objects 
seems to follow the mouse. <PositionChaser DEF='pc' duration='5' /> <Transform DEF='trans box'> <Shape> 
<Appearance> <Material diffuseColor='0 0.329 0.706' /> </Appearance> <Sphere radius='0.25'/> </Shape> 
</Transform> <Transform DEF='trans plane' translation='0 -0.25 0'> <TouchSensor DEF='ts' /> <Shape> 
 <Appearance> <Material diffuseColor='1 1 1' emissiveColor='1 1 1'/> </Appearance> <Box size='10 0.1 
10'/> </Shape> </Transform> <ROUTE fromNode='ts' fromField='hitPoint changed' toNode='pc' toField='set 
destination'/> <ROUTE fromNode='pc' fromField='value changed' toNode='trans box' toField='set translation'/> 
 Files: PositionChaser3D.x3d  7.2 Steering behaviour basics This section shows you how to use steering 
behaviours to add some life to your world. 7.2.1 What are steering behaviours? Citing Craig Reynolds 
GDC 1999 paper: Steering behaviours are a solution for one requirement of autonomous characters in ani­mation 
and games: the ability to navigate around their world in a life-like and improvisa­tional manner. By 
combining prede.ned behaviours a variety of autonomous systems can be simulated. The basics of steering 
behaviours are described in Craig Reynolds paper and there are plenty of other resources out on the web 
(e.g. www.steeringbehaviors.de ) -just google for steering behaviours . Make sure you have read and understood 
the basic principles (vehicles with behaviours) as the rest of this section focuses on how to use them 
with Avalon. 7.2.2 The steering sytem We start with creating a SteeringSystem node and giving it a nice 
name: <?xml version="1.0" encoding="UTF-8"?> <X3D profile='Immersive'> <Scene> <SteeringSystem DEF='steerSystem'> 
</SteeringSystem> </Scene> </X3D> The parameters of the steering system will be discussed later. First 
we ll insert some vehicles into our system. 7.2.3 Adding vehicles Within a steering system one or more 
vehicles represent autonomous agent(s) parameterized with behaviours. The vehicle class used in Avalon 
is based on a point-mass approximation which allows for a simple physically-based model (for example, 
a point mass has velocity (linear momentum) but no moment of inertia (rotational momentum)). Adding a 
vehicle to the steering system looks like this: <SteeringSystem DEF='steerSystem'> <SteeringVehicle DEF='vehicle1' 
maxSpeed='2' maxForce='4' /> <SteeringVehicle DEF='vehicle2' maxSpeed='6' maxForce='12' /> </SteeringSystem> 
Warning A SteeringVehicle can only be a child of exactly one SteeringSystem at a time. Re-USEing a vehicle 
in another system is not supported and will lead to unde.ned results. 7.2.3.1 Parameterizing the vehicle 
A vehicle has a few attributes, which can be read and set at any time: mass the mass of the vehicle radius 
the radius of the vehicle (used for obstacle and neighbour avoidance) maxSpeed the maximum speed of the 
vehicle maxForce the maximum force of the vehicle There two additional inputOutput .elds named useFixedY 
and fixedY . If useFixedY is true the value of fixedY is used as the y-component of the vehicles position. 
By setting a .xed value the vehicle can be constrained to stay on a .xed plane. If you dynamically route 
values to the .eld more interesting effects are possible.(e.g. terrain following). If useFixedY is false 
the value of fixedY is ignored.  7.2.4 Adding behaviours to the vehicles After our steering system is 
equipped with some vehicles we need to add behaviour(s) to them, otherwise they won t do anything (which 
isn t very interesting). The available behaviours are: SeekBehaviour A Seek behaviour acts to steer the 
character towards a speci.ed position in global space. FleeBehaviour Flee is the inverse of seek and 
acts to steer the character away from the target. AvoidNeighborBehaviour Tries to keep characters which 
are moving in arbitrary directions from running into each other. AvoidObstaclesBehaviour Gives a character 
the ability to avoid obstacles. EvasionBehaviour Evasion is similar to Flee except that the menace (target) 
is another moving character. PursuitBehaviour Pursuit is similar to Seek except that the quarry (target) 
is another moving char­acter. WanderBehaviour Wander is a type of random steering. <SteeringSystem DEF='steerSystem'> 
<SteeringVehicle DEF='vehicle1' maxSpeed='2' maxForce='4' mass='1.4'> <SeekBehaviour DEF='seekBehaviour' 
factor='1.0' containerField='behaviours' /> <WanderBehaviour DEF='wanderBehaviour' factor='0.2' containerField='behaviours' 
/> </SteeringVehicle> <SteeringVehicle DEF='vehicle2' maxSpeed='2' maxForce='4' mass='1.1' > <PursuitBehaviour 
DEF='pursuitBehaviour' containerField='behaviours' > <SteeringVehicle USE='vehicle1' containerField='quarry' 
/> </PursuitBehaviour> </SteeringVehicle> </SteeringSystem> The example above shows a SteeringSystem 
that contains two vehicles. The vehicles contain different behaviours. The .rst vehicle is called vehicle1 
and contains two behaviours: a SeekBehaviour and a Wan­ derBehaviour . That way vehicle1 is seeking the 
target while wandering a little (seek has a factor of 1.0 while wander has 0.1). The target of the SeekBehaviour 
is not speci.ed explicitly so the default value (0,0,0) will be used. This results in vehicle1 seeking 
around the origin. The vehicle2 only contains a PursuitBehaviour which is parameterized to pursue vehicle1 
. This results in vehicle2 following vehicle1 . 7.2.5 Updating the vehicles In order to run the steering 
behaviour simulation the vehicle s update .eld has to be called con­tinously. This could be achieved 
by connecting a TimeSensor .time .eld to the update .eld. In practice the SteeringSystem does this job 
for you. Instead of connecting the TimeSensor to every single vehicle, you simply connect it to the SteeringSystem 
.time .eld which calls the update .eld on all it s child vehicles. <TimeSensor DEF='timeSensor' loop='true' 
/> <ROUTE fromNode='timeSensor' fromField='time' toNode='steerSystem' toField='time' /> 7.2.6 I don 
t see anything!? Right. Until now we have setup a SteeringSystem , inserted some vehicles and added behaviours 
to them. But there is nothing to see! That s because the SteeringSystem is a simulation node, that is: 
it has no visual output. All it does is simulating an autonomous behaviour by calculating a new position 
and orientation for every vehicle. So after each simulation step triggered by the TimeSensor connected 
to a SteeringSystem s time .eld the vehicles of the system contain a new position and orientation in 
their translation and rotation .elds. It s up to you what to do with these values. The most common practice 
is connecting the simulated position and rotation to a ComponentTransform which is the parent of a subgraph 
containing the geometry which visually represents the vehicle. 7.2.7 Moving some boxes As an example 
we will add two boxes representing our vehicles. <ComponentTransform DEF='trans1'> <Shape> <Box containerField='geometry' 
size='0.1 0.1 0.1' /> <Appearance><Material diffuseColor='1 0 0' /></Appearance> </Shape> </ComponentTransform> 
<ComponentTransform DEF='trans2'> <Shape> <Box containerField='geometry' size='0.1 0.1 0.1' /> <Appearance><Material 
diffuseColor='0 0 1' /></Appearance> </Shape> </ComponentTransform> <ROUTE fromNode='vehicle1' fromField='translation 
changed' toNode='trans1' toField='translation' <ROUTE fromNode='vehicle2' fromField='translation changed' 
toNode='trans2' toField='translation' <ROUTE fromNode='vehicle1' fromField='rotation changed' toNode='trans1' 
toField='rotation' /> <ROUTE fromNode='vehicle2' fromField='rotation changed' toNode='trans2' toField='rotation' 
/> 7.2.8 Debugging vehicles and behaviours A vehicle offers more attributes which are read-only and 
can be used for displaying the internal state of the vehicle in the case of debugging. 7.2.8.1 Vehicle 
s outputOnly .elds speed The current speed of the vehicle (which is the length of the velocity vector). 
velocity The current velocity vector of the vehicle. forward The current forward vector of the vehicle. 
seekForce The current seek force of the vehicle (a null vector if no SeekBehaviour is used). avoidObstaclesForce 
The current avoid obstacle force of the vehicle (a null vector if no AvoidOb­ stacles behaviour is used). 
Files: steeringBasics.x3d  7.3 Humanoid animation This section shows how to animate virtual characters 
with H-Anim. 7.3.1 Overview H-Anim .gures are articulated 3D representations that depict animated characters. 
A single H-Anim .gure is called a humanoid. While H-Anim .gures are intended to represent human-like 
characters, they are a general concept that is not limited to human beings. Below two links on H-Anim 
are listed. The .rst one holds a good introducery overview on the concepts of H-Anim in general, and 
the second one contains the X3D speci.cation for H-Anim nodes. Description of humanoid animation component 
 X3D H-Anim component speci.cation  Currently there exist two types of H-Anim .gures: Skeletal body 
geometry describes the body as separate geometric pieces and therefore can lead to artifacts. Skinned 
body geometry in contrast regards the body as a continuous piece of geometry. Therefore all point and 
normal vector data sets are de.ned in one place, in the skinCoord and skinNormal .elds of the HAnimHumanoid 
, for allowing smooth mesh animations. In this section only the latter, more natural looking type is 
described. The skin .eld of the HAnimHumanoid node contains the real mesh information, i.e. the Shape 
nodes, which de.ne appearance and geometry of certain body parts like face or legs. As can be seen in 
the next code fragment, the Geometry s coord and normal .elds only hold references to the Coordinates 
and Normals already de.ned in the skinCoord and skinNormal .elds of the HAnimHumanoid. This way a seemless 
animation is achieved both for the vertices and the normals without the need to recalculate the latter. 
 7.3.2 Animation The HAnimJoint node is used to describe the articulations of the humanoid .gure. Each 
articulation is represented by an HAnimJoint node. These joints are organized into a hierarchy of transformations 
that describes the parent-child relationship of joints of the skeleton and provides a container for information 
that is speci.c to each joint. This transformation hierarchy is listed in the skeleton .eld of the HAnimHumanoid 
node. An additional .eld joints holds references to all used HAnimJoint nodes. An HAnimJoint has two 
.elds that allow it to manipulate individual vertices de.ned within the skin-Coord .eld of the HAnimHumanoid 
node. Incoming rotation or translation events of the joint affect the vertices indicated by the skinCoordIndex 
.eld by a factor that is described by the corresponding values within the skinCoordWeight .eld. The MFFloat 
.eld skinCoordWeight contains a list of val­ues that describe the amount of weighting to be used to affect 
the appropriate vertices, as indicated by the skinCoordIndex .eld, of the humanoid s skinCoord and skinNormal 
.elds. DEF HUMANOID HAnimHumanoid { name "Charles" skeleton [ DEF hanim HumanoidRoot HAnimJoint { name 
"HumanoidRoot" center 0 .9723 -.0728 skinCoordIndex [ 0 1 2 3456 7 8 9 10 11 ] skinCoordWeight [ 111111111111 
] children [ DEF hanim l hip HAnimJoint { name "l hip" center .0956 .9364 0 skinCoordIndex [ #... ] skinCoordWeight 
[ #... ] children [ #... ] } DEF hanim r hip HAnimJoint { name "r hip" #... } #... ] } ] joints [ USE 
hanim HumanoidRoot USE hanim r hip USE hanim l hip #... ] skinCoord DEF hanim skin coord Coordinate 
{ point [ #... ] } skinNormal DEF hanim skin normal Normal { vector [ #... ] } skin [ DEF faceShape 
Shape { appearance Appearance { texture ImageTexture { url "headTexture.jpg" } } geometry IndexedFaceSet 
{ coord USE hanim skin coord normal USE hanim skin normal 89 normalUpdateMode "none" coordIndex [ #... 
] normalIndex [ #... ] } } #... ] } DEF TIMER TimeSensor { loop TRUE cycleInterval 5 } DEF HUMANOIDROOT 
POS ANIMATOR PositionInterpolator { key [] keyValue [] } DEF HUMANOIDROOT ANIMATOR OrientationInterpolator 
{ key [] keyValue [] } DEF L HIP ANIMATOR OrientationInterpolator { key [] keyValue [] } DEF R HIP 
ANIMATOR OrientationInterpolator { key [] keyValue [] } #... ROUTE TIMER.fraction changed TO HUMANOIDROOT 
POSITION ANIMATOR.set fraction ROUTE TIMER.fraction changed TO HUMANOIDROOT ANIMATOR.set fraction ROUTE 
TIMER.fraction changed TO L HIP ANIMATOR.set fraction ROUTE TIMER.fraction changed TO R HIP ANIMATOR.set 
fraction #... ROUTE HUMANOIDROOT POS ANIMATOR.value changed TO hanim HumanoidRoot.set translation ROUTE 
HUMANOIDROOT ANIMATOR.value changed TO hanim HumanoidRoot.set rotation ROUTE L HIP ANIMATOR.value changed 
TO hanim l hip.set rotation ROUTE R HIP ANIMATOR.value changed TO hanim r hip.set rotation #... The HAnimSegment 
node is a specialized grouping node that can only be de.ned as a child of an HAnimJoint node. It represents 
body parts of the humanoid .gure and is organized in the skeletal hierarchy of the humanoid. The HAnimSite 
node can be used to de.ne an attachment point for accessories such as jewelry and clothing on the one 
hand and an end effecter location for an inverse kinematics system on the other hand. Both nodes usually 
are not needed for skinned body animation. The HAnimDisplacer nodes are usually used to control the shape 
of the face. Each HAnimDis­placer node speci.es a location, called a morph target, that can be used to 
modify the displacement properties of the corresponding vertices de.ned by the coordIndex .eld. The scalar 
magnitude of the displacement is given by the weight .eld and can be dynamically driven by an interpolator 
or a script. The next code fragment shows an example. The mesh therefore can be morphed smoothly us­ing 
the base mesh and a linear combination of all sets of displacement vectors, given by the MFVec3f displacements 
.eld of the HAnimDisplacer nodes. DEF Head HAnimJoint { name "Head" center 0 1.58 0.03 skinCoordIndex 
[ 0 1 2 3 4 5 6 7 8 9 10#... ] skinCoordWeight [ 1 111 1 1 1 111#... ] displacers [ DEF Phon AShape 
HAnimDisplacer { name "Phon AShape" weight 0.0 coordIndex [ 0 1 2 3456 7 8 9 10 #... ] displacements 
[ 0.000000 0.000000 0.000500, -0.002130 -0.002270 0.006110, #... ] } DEF Idle Blink bothShape HAnimDisplacer 
{ #... } ] } DEF Timer TimeSensor { loop TRUE cycleInterval 5 } DEF Interpol ScalarInterpolator { key 
[ 0.0, 0.25, 0.5, 0.75, 1.0 ] keyValue [ 0.0, 0.25, 0.5, 0.25, 0.0 ] } ROUTE Timer.fraction changed 
TO Interpol.set fraction 91 ROUTE Interpol.value changed TO Idle Blink bothShape.weight ROUTE Interpol.value 
changed TO Phon AShape.weight  7.3.3 Morphing Quite similar to the already described Displacer node 
is the CoordinateMorpher node. Assume you want to animate a face, and you have given, say n , target 
states of your modelled face, a neutral one, and n-1 other ones, e.g. a smiling one, one with open eyes, 
one with closed eyes, one with raised eyebrows, one saying a , and so on. The Morpher node regards each 
of these states as a base vector of an n dimensional space spanning all possible combinations of point 
sets. In order to get valid linear combinations be careful that the coef.cients (weights) of your data 
points (i.e. sets of expressions, which are also called morph targets) sum up to 1 (which is called a 
convex combination). In the code fragment shown below we want to interpolate between a neutral state 
(the .rst one or keyValue No 0 respectively) and state No 10. Therefore additionally a VectorInterpolator 
is needed. For each key time a vector of n keyValues is needed, de.ning the maximum weight for all morph 
targets (please note, that all lines sum up to 1). Another important thing to keep in mind, is that the 
sequence of points must not change, because they all belong to the same index .eld. Shape { appearance 
Appearance {} geometry IndexedFaceSet { coord DEF coords Coordinate { point [ 0.086, 0.050, 0.431, 0.089, 
0.044, 0.434, #... ] } coordIndex [ 0, 1, 2,-1, 3,4,5, -1, #... ] } } DEF morph CoordinateMorpher { keyValue 
[ # 15 sets of coordinates; one set for each state: 0.086, 0.050, 0.431, 0.089, 0.044, 0.434, #... 
] } DEF vipol VectorInterpolator { key [ 0.0, 0.1, 0.75, 1.0 ] keyValue [ 1.00 0 0 0 0 0 0 0 0 0 0 
0 0 0, 0.40 0 0 0 0 0 0 0 0 0.60 0 0 0, 0.60 0 0 0 0 0 0 0 0 0.40 0 0 0, 1.00 0 0 0 0 0 0 0 0 0 0 0 
0 0,  ] } DEF ts TimeSensor { loop TRUE cycleInterval 5 } ROUTE ts.fraction changed TO vipol.set fraction 
ROUTE vipol.value changed TO morph.set weights ROUTE morph.value changed TO coords.set point The .rst 
of the attached .les shows a simple but skinned walking character whereas the second .le shows the morpher 
in action for doing simple facial animation. Files: The famous boxman  A morphed face  93 Chapter 
8 Conclusion The goal of this course was to demonstarte howeasy it is to go beyond simple Windows and 
Mouse/Pointer interaction. Using freely available tools and COTS hardware it is possible to create and 
interact with compelling Virtual and Augmented Environments without blowing the budget of a small lab 
or even an interested individual. So don t be a WIMP! Go to the http://www.not-for-wimps.org to .nd the 
latest updates and links to further tutorals and code. Check out the http://www.instantreality.org and 
http://www.opensg.org projects to get more information and help to start your project. Bibliography [1] 
M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin, and C. T. Silva. Point set surfaces. IEEE Visualization 
2001, pages 21 28, October 2001. ISBN 0-7803-7200-x. [2] Marc Alexa. Linear geometry interpolation in 
opensg, 2002. [3] Marc Alexa and Johannes Behr. Cooperative VR enviroment. brasil, 2000. [4] Marc Alexa 
and Johannes Behr. Volume Rendering in VRML. Web3D -VRML 2001 Proceed­ings, 2001. [5] Marc Alexa and 
Johannes Behr. Fast and Effective Striping. 1. OpenSG Symposium OpenSG, 2002, Darmstadt, 2002. [6] Marc 
Alexa and Johannes Behr. Linear Geometry Interpolation in OpenSG. 1. OpenSG Sym­posium OpenSG, 2002. 
[7] Marc Alexa, Johannes Behr, Daniel Cohen-Or, Shachar Fleishman, David Levin, and Claudio T. Silva. 
Computing and rendering point set surfaces. [8] Marc Alexa, Johannes Behr, and Wolfgang Müller. The morph 
node. Web3D -VRML 2000 Proceedings, pages 29 34, 2000. ISBN 1-58113-211-5. [9] Althoff, Stocker, and 
McGlaun. A Generic Approach for Interfacing VRML Browsers to Various Input Devices and Creating Customizable 
3D Applications. Web3D, 2002. [10] Y. Araki. A High-level Multi-user Extension Library For Interactive 
VRML Worlds. VRML, 1998. [11] Avalon. Avalon. http://www.zgdv.de/avalon, 1998. [12] J. Behr, SM. Choi, 
S. Großkopf, MH, and G. Sakas. Modelling, visualization, and interaction techniques for diagnosis and 
treatment planning in cardiology. Computers &#38; Graphics, Vol 24.5:741 753, 2000. ISSN 0097-8493. [13] 
Johannes Behr and Marc Alexa. Fast and effective striping. 1. OpenSG Symposium, Darmstadt, 2002. [14] 
Johannes Behr and Patrik Dähne. AVALON: Ein komponentenorientiertes Rahmensystem für dynamische Mixed-Reality 
Anwendungen. TUD thema Forschung, 2003. [15] Johannes Behr, Jorge A. Diz, and Marcelo G. Malheiros. An 
Extensible Interactive Image Syn­thesis Environment. echnical report DCA-006/97 -DCA, FEEC, Unicamp, 
1997. [16] Johannes Behr and Andreas Froehlich. AVALON, an Open VRML VR/AR system for Dynamic Application. 
Topics, 1(1):28, 1998. [17] Johannes Behr, Torsten Froehlich, Christian Knoep.e, Bernd Lutz, Dirk Reiners, 
Frank Schoef­fel, and Wolfram Kresse. The Digital Cathedral of Siena -Innovative Concepts for Interactive 
and Immersive Presentation of Cultural Heritage Sites. ICCHIM Conference Proceedings, Milan, 2001. [18] 
Johannes Behr and Axel Hildebrand. Sanare VR Med enviroment. Topics, 1998. [19] Johannes Behr and Marc 
Niemann. Interactive Volume Data Rendering for Medical VR Appli­cations. 1998. [20] Johannes Behr, Choi 
Soo-Mi, and Stefan Großkopf. 3D Modellierung zur Diagnose und Be­handlungsplanung in der Kardiologie. 
Der Radiologe, 40(3):256 261, 2000. [21] Allan Bierbaum, Albert Baker, Carolina Cruz-Neira, Patrick Hartling, 
Christopher Just, and Kevin Meinert. VR Juggler: A Virtual Platform for Virtual Reality Application Development. 
Master s thesis, Iowa State University, 2000. [22] Roland Blach, Juergen Landauer, Angela Roesch, and 
Andreas Simon. A .exible prototyping tool for 3d realtime user-interaction. 1998. [23] Roland Blach, 
Jürgen Landauer, Angela Rösch, and Andreas Simon. A Highly FlexibleVirtual Reality System. Future Generation 
Computer Systems, 14(3 4):167 178, 1998. [24] X3D Consortium. X3d standard. http://www.web3d.org/x3d/, 
2008. [25] Matthew Conway, Randy Pausch, Rich Gossweiler, and Tommy Burnette. Alice: A rapid proto­typing 
system for building virtual environments. 2:295 296, April 1994. [26] Carolina Cruz-Neira and Daniel 
J. Sandin. Surround-Screen Projection-Based Virtual Reality: The Design and Implementation of the CAVE. 
ACM Computer Graphics, SIGGRAPH 93, 1993. [27] Paul J. Diefenbach, Daniel Hunt, and Prakash Mahesh. Building 
openworlds. Web3D -VRML 1998 Proceedings, 1998. [28] N.I. Durlach and .A.S. Mavor. Virtual Reality: Scienti.c 
and Technological Challenges. National Academy Press., 1995. [29] Thorsten Fröhlich, Johannes Behr, and 
Peter Eschler. Cybernarium Days 2002 -A Public Expe­rience of Virtual and Augmented Worlds. First International 
Symposium on Cyber Worlds 2002, 2002. [30] Philippe Coiffet Grigore C. Burdea. Designing Virtual Reality 
Systems: The Structured Ap­proach. Wiley-IEEE Press, 2003. [31] H-Anim. ISO/IEC FCD 19774; Humanoid animation 
Speci.cation. http://www.h-anim.org, 2001. [32] Roger Hubbold, Jon Cook, Martin Keates, Simon Gibson, 
Toby Howard, Alan Murta, and Adrian West. Gnu/maverik a micro-kernel for large-scale virtual environments, 
1999. [33] John Kelso, Lance E. Arsenault, Ronald D. Kriz, and Steven G. Satter.eld. DIVERSE: A Frame­work 
for Building Extensible and Recon.gurable Device Independent Virtual Environments. IEEE Virtual Reality 
Conference, 2002. [34] Gerard Kim. Designing Virtual Reality Systems: The Structured Approach. SpringerVerlag, 
2005. [35] Blair MacIntyre. A Touring Machine: Prototyping 3D Mobile Augmented Reality Systems for Exploring 
the Urban Environment. ISWC, 1997. [36] Steve Molnar, Michael Cox, David Ellsworth, and Henry Fuchs. 
A sorting classi.cation of parallel rendering. Technical Report TR94-023, 8, 1994. [37] Steve Molnar, 
Michael Cox, David Ellsworth, and Henry Fuchs. A Sorting Classi.cation of Parallel Rendering. IEEE Computer 
Graphics and Applications, 1994. [38] OpenAL. OpenAL Speci.cation and Documentation. http://www.openal.org/, 
1999. [39] OpenSceneGraph. OpenSceneGraph documenten. http://www.openscenegraph.org, 2003. [40] Wayne 
Piekarski, Bruce Thomas, David Hepworth, Bernard Gunther, and Victor Demczuk. An architecture for outdoor 
wearable computers to support augmented reality and multimedia ap­plications. 2000. [41] Dirk Reiners. 
Opensg: Basic concepts. [42] Dirk Reiners, Gerrit Voss, and Johannes Behr. A Multi-thread Safe Foundation 
for Scene Graphs and its Extension to Clusters. Eurographics Workshop on Parallel Graphics and Visualisation 
2002. Proceedings, 2002. [43] Dirk Reiners, Gerrit Voss, and Johannes Behr. OpenSG -Basic Concepts. First 
OpenSG Symposium OpenSG, 2002, Darmstadt, 2002. [44] Patrick Reuter, Johannes Behr, and Marc Alexa. An 
improved adjacency data structure for ef.cient trianglestripping. accepted for publication in the Journal 
of Graphics Tools, To appear. [45] J. Rohlf and J. Helman. IRIS Performer: A high performance toolkid 
for real-time 3D graphics. ACM Computer Graphics, SIGGRAPH 94, 1994. [46] Mark Segal, Akeley Kurt, Chris 
Frazier, and Jon Leech. Opengl Speci.cation. http://www.opengl.org/, 2003. [47] William R. Sherman and 
Alan B. Craig. Understanding Virtual Reality: Interface, Application, and Design. Morgan Kaufmann Publishers 
Inc., San Francisco, CA, USA, 2002. [48] R. Stiles, S. Tewari, and M. Mehta. Adapting VRML For Free-form 
Immersed Manipulation. 1998. [49] R. Stiles, S. Tewari, and M. Metha. Adapting VRML 2.0 for Immersive 
Use. VRML 97, Second Symposium on the Virtual Reality Modeling language, 1997. [50] P.S. Strauss and 
R. Carey. An object-oriented 3D graphics toolkit. ACM Computer Graphics, 1992. [51] C. Szyperski. Component 
Software, Beyond Objekt-Oriented Programming. ACM Press., 1998. [52] H. Tramberend. Avocado a distributed 
virtual environment framework. http://www.ercim.org/publication/Ercim_News/enw38/tramberen d.htm, 1999. 
[53] John Vince. Introduction to Virtual Reality. SpringerVerlag, 2004. [54] W3C. Xml Protocol Working 
Group, sOAP Version 1.2 Speci.cation. http://www.w3.org/2000/xp/Group/, 2000. [55] Wikipedia. Virtual 
reality. http://en.wikipedia.org/wiki/Virtual_reality, 2008. [56] Wikipedia. Virtual reality. http://en.wikipedia.org/wiki/Augmented_reality, 
2008. [57] WorldToolKit. Sense8 Corporation; WorldToolKit: Virtual Reality Support Software. 4000 Bridge­way 
Suite 101, Sausalito, CA 94965, telephone : (415) 331-6318., 1994. [58] S. Ting Wu and Johannes Behr. 
An Extensible Interactive Image Synthesis Environment. XXIV Semish proceedings, 1997. Don t be a WIM 
P A 60 - Second Introdu cti on to Aug mente d and Virtual Real ity  (w ith 124 M inutes Cred it s) 
 Get the latest version http://www.not-for-wimps.org Johannes.Behr@igd.fraunhofer.de dirk@lite3d.com 
Overview  Virtual Reality as an HCI challenge  Virtual Reality History  Efficient Development of VR/AR 
Applications  X3D as a basis  Interaction and Navigation  Stereo and Multiview  Clustering   Traditional 
computer expe rien ce Bill Buxton Test: Draw a computer in 15 secs 80 % draw screen, keyboard &#38; 
mouse Users perceive Computer primarily through in-and output technology Increasing Reso ur ces  Moore´s 
Law Doubles Performance every 18 month  It really means that computer will be faster, more powerfull 
  and cheaper Brain overload  The capacity of the human brain stays constant  => Need to improve 
the Standard User Interface  Standard User Interf ace  Standard User Interface has only changed little 
in 25 years WIMP (Windows, Icons Menus and Pointer)  Things that have  changed (somewhat) Resolution 
Standard User Interf ace Tech. for Humans with One eye  One ear  Without mouth  Without body  One 
hand  One finger  VR as an HCI challenge  Techniques and Methods to improve the man-machine-Interface 
 Not bound to a WIMP (Window, Icon, Menu, Pointer) environment  Tries to improve the experience by 
incorporating more senses to immerse the user  What i s V irt ual Reality  The primary defining characteristic 
of VR is inclusion; being surrounded by an environment. VR places the participant inside information. 
 W. Bricken, HIT Lab  Indeed, in the ultimate display one will not look at that world through a window, 
but will be immersed in it.  Ivan Sutherland, 1966 If I turn my head and nothing happens, it ain´t VR! 
Steve Bryson, NASA Ames Every computer graphics program written after 1994. David Mizell, Cray What i 
s V irt ual Reality Characteristics Virtual Content  Immersion  Sensory Feedback in real-time  
Interactivity  VR Re ference Model Semantics of the data Dynamic Semantics Static Semantics VR No 
semantics Interaction Single Action immersive Action Sequnece Real Time Presentation Why V irt ual 
Reality I hear I forget I see I remember I experience I understand Chinese Proverb Variations of 
Virt ual Reality  Immersive Virtual Reality: Synthetic stimulus of the body s senses via the use of 
technology.  Augmented Reality: Real-life imagery that is augmented by synthetic objects  Cyberworlds: 
Metaphor for the real-world global telecom grid which provides a consensual hallucination experience 
 Beyo nd Virtual: Augmented Augmented Reality: Don t replace, enhance the real world Either through 
special see-through Head-Mounted Displays  Or through adding video as a background  Much cheaper, easier 
and more flexible Has all the problems of VR, but harder But some cases can be made to work farirly easily 
Overview  Virtual Reality as HCI challenge  Virtual Reality History  Efficient Development of VR Application 
  X3D as basis  Interaction and Navigation  Stereo and Multiview  Cluster   VR History: 1. Generation 
1960 - 1985  Morton Heilig Sensorama (1962) Ivan Sutherland, Ultimate Display (1970) VR History: 2. 
Generation 1985 - 1995 First commercial Hardware Setups Scott Fisher,VIEW (1985)  People start to 
explore application areas  Great visions start the all-media hype  VR History: 2. Generation 1985 
- 1995  VR History: 2. Generation 1985 - 1995  VR History: 2. Generation 1985 - 1995  VR History: 
2. Generation 1985 - 1995  VR History: 2. Generation 1985 - 1995  VR History: 2. Generation 1985 - 
1995  VR History: 2. Generation 1985 - 1995  VR History: 2. Generation 1985 - 1995  VR History: 2. 
Generation 1985 - 1995  HW components very expensive and limited  HMDs support only minimal FOV and 
Res.  10-20 FPS with hundreds of polygons  National R&#38;D Labs   VR History: 3. Generation 1995 
- 2000  SGI Graphics Supercomputers  IPTs Immersive Projections System  Industrial Investment   
VR History: 3. Generation 1995 - 2000  Stereo projection CAVEWorkbench VR History: 3. Generation 1995 
- 2000  VR History: 3. Generation 1995 - 2000 Marker based AR Systems  VR History: 4. Generation 2000 
- today  Cheap Render HW  PC-Cluster  VR History: 4. Generation 2000 - today  Inexpensive sensors 
  Consumer devices  Cameras  Accelerometer  IR-Tracker  Haptics   VR History: 4. Generation 2000 
- today  Non-Marker AR Systems  Mobile AR Systems  VR Tod ay  Inexpensive render and sensor HW 
 Inexpensive stereo systems  Costly application development  Large number of technologies and tools 
 No Standards available  No Standard tool chain   Overview  Virtual Reality as HCI challenge  Virtual 
Reality History  Efficient Development of VR Application  X3D as basis  Interaction and Navigation 
 Stereo and Multiview  Cluster   Effici ent VR Application deve lopm ent  Problem: Wide variety 
of topics  Image synthesis, Image analysis  Behaviour description, Collision and dynamics  Interaction 
and Navigation, 3D User Interfaces  Cluster management and synchronisation   => Utilize 3D-Standard 
which is close to the VR/AR domain: X3D (VRML successor)  Overview  Virtual Reality as HCI challenge 
 Virtual Reality History  Efficient Development of VR Application  X3D as basis  Interaction and 
Navigation  Stereo and Multiview  Cluster   X3D Standard as Basis ISO Standard which describes an 
abstract functional behaviour of time-based, interactive 3D, multimedia information Multi-parent Scene-graph 
 Behaviour-graph  IO Device Independent  Portable  Multiple encodings (XML, Inventor-ASCII, Binary) 
  Sce negraph structure  Nodes build a scene hierarchy  Heterogeneous node types  Directed (parent/child) 
graph  Traditional image synthesis data structure  Eve nt flow grap h  Connects Fields with Routes 
 Transport Event/Values between slots/fliels  First introduced in VRML2.0 (1997)  Common Model in 
modern VR Systems (Avalon, Avango, Lightning, Virtools, Eon)  X3D Node  Type (e.g. Group, Transform) 
 Name (e.g. house)  Interface  Fields (type,name)  In/Out Slots (type,name)  0-N Relegation parent/route 
per Slot/Field (fan-in und fan­out)   Traditional X3D nodes  Hierarchy (Transform, Group, LOD, Switch) 
 Geometry (IndexedFaceSet, IndexedTriStripSet, Box, Nurbs)  Material (Shading-Parameter, Texture, MultiTexture, 
GLSL/CG-Shader  Lights (Directional, Point, Spot)  <Transform translation='0 10 20'> <Shape> <Appearance> 
<Material diffuseColor '1 0 0'/> </Appearance> <Box size='1 4 9'/> </Shape> </Transform> X3D Int erpolator 
and Follower Supports key-frame and chaser animation Hierarchy Animation (e.g OrientationInterpolator, 
PositionChaser) Animation per Objekt (e.g. CoordinateInterpolator, ColorChaser) X3D Sound and AudioS 
ource  Sound  3D Sound emitter with spatial hull  Controls the sound synthesis   AudioSource  Audio 
data (e.g. wav-file)  Could be used by multiple Sound nodes   X3D Body Animation  H-Anim ISO Standard 
 Focus on interchange of character animation  Supports skin &#38; bone animation  Bone transformation 
 Skin updates   X3D Logic elements Allows do describe behavior without programming Filter  Sequencer 
 Toggle  Trigger  X3D Scripts Allows to script the behavior <Script DEF='foo'> of a single node 
instance <field accessType='inputOnly' name='set_time' Declaration of interface type='SFTime'/> elements 
per instance ecmascript: function set_time(value) { Support for multiple ... scripting languges }  
Ecmascript > >  JAVA  X3D /VR runtime environment  Web/Desktop Browser  WIMP environment with mouse 
&#38; keyboard  Single screen per application  Application interfaces only with browser   Immersive 
VR environment  Wide range of IO devices  Multi Screen/Pipe and Cluster setups  Distributed application 
  Utili zing X3D for VR/AR  X3D as Application description language  Extension for VR/AR requirements 
 Immersive Interaction and Navigation  Low-level Sensors  High-Level Sensors   Stereo and Multiview 
 Cluster   Overview  Virtual Reality as HCI challenge  Virtual Reality History  Efficient Development 
of VR Application  X3D as basis  Interaction and Navigation  Stereo and Multiview  Cluster   X3D 
and immersive interaction  Run Standard X3D application immersively  X3D extensions to support novel 
devices and techniques as part of the application  Leve l of abstractio n for VR sens ors  Low-Level 
Device-System  VR-System (e.g. VRPN, VRJuggler, OpenTracker)  X3D-Ext (e.g xj3d, Blaxxun, Stilles) 
 Classification for every physical-device  Interaction model depends on device class   High-Level 
Interaction-Model  Abstract interaction (snapping, touching, ...)  No device streams   Standard High 
- Level X3D Sensors  TimeSensor  Environment Sensor (Visibility, Proximity, Transformation)  Pointing 
Sensor (TouchSensor, PlaneSensor, CylinderSensor, SphereSensor)  Picking Sensors (LinePicker, PointPicker, 
PrimtivePicker, VolumePicker)  Devices ?!?  Data-Streams (e.g. image stream)  Low - level Stream Sens 
ors  Device and device-class Independent  One IOSensor-node maps in/out slots from any stream space 
namespace  Name/type used to map the sensor to the stream  Low - level Stream Sens ors Stream Space 
Local Streams Network Streams Backends provide multiple data-streams in local namespace UI to manage 
backends Low - level Stream Sens ors One physical device can be mapped to multiple sensors IOSensor 
{ eventOut SFFloat brake   } High Leve l Pointing Sens or  Using unchanged X3D PointingSensor nodes 
in immersive environments  n BodyPart nodes to trigger PointingSenosr transitions instead of a single 
mouse  BodyPart is a special Group-node to define a named Transformation and visible representation 
 Desk top PointingSensor  Touch/Sphere/Cylinder Sensor  2D/Mouse Interaction (distance interaction) 
 IsOver notion (ray hits object) interacti Trigger notion (user clicks left mouse button) on ray intersecti 
poin ont Immersive Pointi ngSe nsor Wand/Ray/Collision Interaction IsOver notion (ray hits object) 
 Trigger notion (body part is hot)  body part Example : Low - leve l Stream Sen sors Controlling 
material color by joystick <IOSensor DEF= sensor type= joystick />  <Script DEF= script url= javascript: 
 />  <Shape />  <ROUTE sensor.axis TO script.axisIn >  <ROUTE script.colorOut TO mat.diffuseColor 
>  Practical Example s  Moving Objects  Joystick Navigation  Wiimote tracker  AR Tracker  Overview 
 Virtual Reality as HCI challenge  Virtual Reality History  Efficient Development of VR Application 
  X3D as basis  Interaction and Navigation  Stereo and Multiview  Cluster  To get a real 3D impression 
need different images for left and right eye From slightly different viewpoints and perspectives  Need 
to be delivered to the eyes individually  Head-mounted display: one screen per eye  Monitor? Projector? 
  Stereo? Mult iview ?  Large or high-resolution images need more than one video signal  Similar 
problem to stereo, but not the same  Needs to display part of the whole image  Need part specification 
 Stereo Deli very  Mechanical  Time  After each other  Separated by active glasses  Needs very 
fast monitor/projector   Expensive! Stereo Deli very  Polarization  Linear or Circular  Separated 
by passive glasses  Needs special filters for projectors or special monitor (Zalman or iZ3D)   Color 
 Red/Green or Infitec  Passive glasses  Needs special projector (expensive)   How to get ster eo 
out of the system?  Active: needs driver support  Passive: many different options:  Left/Right  Top/Bottom 
 Multiple Outputs  Line or Column Interleaved   X3D?  There is no window or viewport in X3d  Only 
a Viewpoint  Need to be added as extensions  See examples Immersive X3D scen es  What about Steve 
Bryson?  Head Tracking  Tracking Data -> additional transformation on top of the Viewpoint transformation 
 Objects which are bound to the camera   Viewpoint.children, Viewspace -Group Exa mples and Results 
 The End  VR and AR are possible  Even at home, even on a budget  All it takes are some free tools 
 And some creativity  So go out and virtualize    
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401165</section_id>
		<sort_key>330</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Flow simulations using particles: bridging computer graphics and CFD]]></section_title>
		<section_page_from>9</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098674</person_id>
				<author_profile_id><![CDATA[81100305532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Petros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koumoutsakos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098675</person_id>
				<author_profile_id><![CDATA[81100104931]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Georges-Henri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cottet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098676</person_id>
				<author_profile_id><![CDATA[81365598217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rossinelli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401166</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<pages>73</pages>
		<display_no>25</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Flow simulations using particles]]></title>
		<subtitle><![CDATA[bridging computer graphics and CFD]]></subtitle>
		<page_from>1</page_from>
		<page_to>73</page_to>
		<doi_number>10.1145/1401132.1401166</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401166</url>
		<abstract>
			<par><![CDATA[<p>The simulation of the motion of interacting particles is a deceivingly simple, yet powerful and natural method for exploring and animating flows in physical systems as diverse as planetary dark accretion and sea waves, unsteady aerodynamics and nanofluidics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.2</cat_node>
				<descriptor>Physics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010441</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Physics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098677</person_id>
				<author_profile_id><![CDATA[81100305532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Petros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koumoutsakos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computational Science and Engineering Laboratory, ETH Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098678</person_id>
				<author_profile_id><![CDATA[81100104931]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Georges-Henri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cottet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#233; de Grenoble, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098679</person_id>
				<author_profile_id><![CDATA[81365598217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rossinelli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computational Science and Engineering Laboratory, ETH Z&#252;rich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073380</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Angelidis and F. Neyret. Simulation of smoke based on vortex filament primitives. In <i>ACM-SIGGRAPH/EG Symposium on Computer Animation (SCA)</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[P. Angot, C. H. Bruneau, and P. Fabrie. A penalization method to take into account obstacles in incompressible viscous flows. <i>NUMERISCHE MATHEMATIK</i>, 81(4):497--520, Feb 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>20806</ref_obj_id>
				<ref_obj_pid>20802</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. T. Beale. A convergent 3-D vortex method with grid-free stretching. 46:401--424, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[M. Bergdorf, G. H. Cottet, and P. Koumoutsakos. Multilevel adaptive particle methods for convection-diffusion equations. <i>Multiscale Modeling and Simulation</i>, 4(1):328--357, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Bergdorf and P. Koumoutsakos. A lagrangian particle-wavelet method. <i>MULTISCALE MODELING AND SIMULATION</i>, 5(3):980--995, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Bergdorf, I. F. Sbalzarini, and P. Koumoutsakos. Particle simulations of growth. <i>J. Computational Physics</i>, 2008 (submitted).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. J. BERGER and J. OLIGER. Adaptive mesh refinement for hyperbolic partial-differential equations. <i>JOURNAL OF COMPUTATIONAL PHYSICS</i>, 53(3):484--512, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>347185</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[W. L. Briggs, V. E. Henson, and S. F. McCormick. <i>A multigrid tutorial: second edition</i>. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>942319</ref_obj_id>
				<ref_obj_pid>942275</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. Bryson and D. Levy. High-order central WENO schemes for multidimensional Hamilton-Jacobi equations. 41(4):1339--1369, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. CARLSON, P. MUCHA, and G. TURK. Rigid fluid: Animating the interplay between rigid bodies and fluid, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>959043</ref_obj_id>
				<ref_obj_pid>959042</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. K. Chaniotis, C. E. Frouzakis, J. C. Lee, A. G. Tomboulides, D. Poulikakos, and K. Boulouchos. Remeshed smoothed particle hydrodynamics for the simulation of laminar chemically reactive flows. 191(1):1--17, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[P. Chatelain, A. Curioni, M. Bergdorf, D. Rossinelli, W. Andreoni, and P. Koumoutsakos. Billion vortex particle direct numerical simulations of aircraft wakes. <i>Computer Methods in Applied Mechanics and Engineering</i>, 197(13--16):1296--1304, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[C. G. Chatelain P. and K. P. Particle mesh hydrodynamics for astrophysics simulations. <i>Int. J. Modern Physics C</i>, 18(4):610--618, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D. Chopp and J. Sethian. Flow under curvature: Singularity formation, minimal surfaces, and geodesics. 2(4):235--255, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>158507</ref_obj_id>
				<ref_obj_pid>158500</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D. L. Chopp. Computing minimal-surfaces via level set curvature flow. 106(1):77--91, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. J. Chorin. Numerical study of slightly viscous flow. 57(4):785--796, 1973.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1412954</ref_obj_id>
				<ref_obj_pid>1412756</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[M. Coquerelle and G.-H. Cottet. A vortex level set method for the two-way coupling of an incompressible fluid with colliding rigid bodies. <i>J. Comput. Phys.</i>, (in print), 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[G. Cottet. A particle model for fluid-structure interaction. <i>C. R. Acad. Sci. Paris</i>, Ser. I(335):833--838, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>90845</ref_obj_id>
				<ref_obj_pid>90831</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[G. H. Cottet. A particle-grid superposition method for the Navier-Stokes equations. 89:301--318, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[G. H. Cottet. Multi-physics and particle methods. <i>COMPUTATIONAL FLUID AND SOLID MECHANICS 2003, VOLS 1 AND 2, PROCEEDINGS</i>, pages 1296--1298, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[G.-H. Cottet and P. Koumoutsakos. <i>Vortex Methods, Theory and Practice</i>. Cambridge University Press, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>349912</ref_obj_id>
				<ref_obj_pid>349900</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[G.-H. Cottet, P. Koumoutsakos, and M. L. O. Salihi. Vortex methods with spatially varying cores. 162(1):164--185, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. H. Cottet and E. Maitre. A level set method for fluid-structure interactions with immersed surfaces. <i>Mathematical Models and Methods In Applied Sciences</i>, 16(3):415--438, Mar 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>965247</ref_obj_id>
				<ref_obj_pid>965240</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[G.-H. Cottet and P. Poncet. Advances in direct numerical simulations of 3D wallbounded flows by Vortex-in-Cell methods. 193(1):136--158, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1189766</ref_obj_id>
				<ref_obj_pid>1189762</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[S. Elcott, Y. Y. Tong, E. Kanso, P. Schroder, and M. Desbrun. Stable, circulation-preserving, simplicial fluids. <i>ACM TRANSACTIONS ON GRAPHICS</i>, 26(1), 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1290449</ref_obj_id>
				<ref_obj_pid>1290206</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[M. Ellero, M. Serrano, and P. Espanol. Incompressible smoothed particle hydrodynamics. <i>JOURNAL OF COMPUTATIONAL PHYSICS</i>, 226(2):1731--1752, Oct 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[D. Enright, S. Marschner, and R. Fedkiw. Animation and rendering of complex water surfaces, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383260</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[R. Fedkiw, J. Stam, and H. W. Jensen. Visual simulation of smoke. In E. Fiume, editor, <i>SIGGRAPH 2001, Computer Graphics Proceedings</i>, pages 15--22. ACM Press / ACM SIGGRAPH, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>244315</ref_obj_id>
				<ref_obj_pid>244304</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[N. Foster and D. Metaxas. Realistic animation of liquids. <i>Graphical models and image processing: GMIP</i>, 58(5):471--483, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>899126</ref_obj_id>
				<ref_obj_pid>899116</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[F. Gibou, R. Fedkiw, R. Caflisch, and S. Osher. A level set approach for the numerical simulation of dendritic growth. <i>J. Sci. Comput.</i>, 19(1--3):183--199, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[R. A. Gingold and J. J. Monaghan. Smoothed particle hydrodynamics: theory and application to non-spherical stars. <i>Month Notices Roy. Astron. Soc.</i>, 181:375--389, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[M. Grayson. A short note on the evolution of surfaces via mean curvatures. 58:285--314, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>36901</ref_obj_id>
				<ref_obj_pid>36895</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[L. Greengard and V. Rokhlin. A fast algorithm for particle simulations. 73:325--348, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[F. H. Harlow. Particle-in-cell computing method for fluid dynamics. 3:319--343, 1964.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[L. Hernquist. Some cautionary remarks about smoothed particle hydrodynamics. <i>ASTROPHYSICAL JOURNAL</i>, 404(2):717--722, Feb 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[J. L. Hess. Higher order numerical solution of the integral equation for the two-dimensional Neumann problem. 2:1--15, 1973.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1103138</ref_obj_id>
				<ref_obj_pid>1103124</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[S. E. Hieber and P. Koumoutsakos. A lagrangian particle level set method. <i>J. Computational Physics</i>, 210:342--367, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1035694</ref_obj_id>
				<ref_obj_pid>1035692</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[S. E. Hieber, J. H. Walther, and P. Koumoutsakos. Remeshed smoothed particle hydrodynamics simulation of the mechanical behavior of human organs. 12(4):305--314, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[C. W. Hirt and B. D. Nichols. Volume of fluid (Vof) method for the dynamics of free boundaries. 39(1):201--225, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>62815</ref_obj_id>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[R. W. Hockney and J. W. Eastwood. <i>Computer Simulation Using Particles</i>. Institute of Physics Publishing, Bristol, PA, USA, 2 edition, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1297528</ref_obj_id>
				<ref_obj_pid>1297418</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[X. Y. Hu and N. A. Adams. An incompressible multi-phase sph method. <i>JOURNAL OF COMPUTATIONAL PHYSICS</i>, 227(1):264--278, Nov 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801284</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Y. Kawaguchi. A morphological study of the form of nature. <i>SIGGRAPH Comput. Graph.</i>, 16(3):223--232, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[S. Kern and P. Koumoutsakos. Simulations of optimized anguilliform swimming. <i>JOURNAL OF EXPERIMENTAL BIOLOGY</i>, 209(24):4841--4857, Dec 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[R. A. Kerr. Planetary origins: A quickie birth for jupiters and saturns. <i>Science</i>, 298(5599):1698b--1699, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[A. Kolb and N. Cuntz. Dynamic particle coupling for GPU-based fluid simulation. In <i>Proc. ASIM</i>, pages 722--727, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1058147</ref_obj_id>
				<ref_obj_pid>1058129</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[A. Kolb, L. Latta, and C. Rezk-Salama. Hardware-based simulation and collision detection for large particle systems. In <i>Proc. Graphics Hardware</i>, pages 123--131. ACM/Eurographics, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274152</ref_obj_id>
				<ref_obj_pid>274112</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[P. Koumoutsakos. Inviscid axisymmetrization of an elliptical vortex. <i>JOURNAL OF COMPUTATIONAL PHYSICS</i>, 138(2):821--857, Dec 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[P. Koumoutsakos. Multiscale flow simulations using particles. <i>ANNUAL REVIEW OF FLUID MECHANICS</i>, 37:457--487, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[P. KOUMOUTSAKOS and A. LEONARD. High-resolution simulations of the flow around an impulsively started cylinder using vortex methods. <i>JOURNAL OF FLUID MECHANICS</i>, 296:1--38, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>182019</ref_obj_id>
				<ref_obj_pid>182013</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[P. Koumoutsakos, A. Leonard, and F. P&#233;pin. Boundary conditions for viscous vortex methods. 113(1):52--61, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[R. Krasny. A study of singularity formation in a vortex sheet by the point vortex approximation. <i>JFM</i>, 167:65--93, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[A. Leonard. Review. vortex methods for flow simulation. 37:289--335, 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>226605</ref_obj_id>
				<ref_obj_pid>226583</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[R. J. LeVeque. High-resolution conservative algorithms for advection in incompressible flow. 33(2):627--665, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[W. Liu, S. Jun, and S. Zhang. Reproducing kernel particle methods. 20(8--9):1081--1106, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1373251</ref_obj_id>
				<ref_obj_pid>1373109</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[F. Losasso, J. O. Talton, N. Kwatra, and R. Fedkiw. Two-way coupled sph and particle level set fluid simulation. <i>IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS</i>, 14(4):797--804, Jul-Aug 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[L. B. Lucy. A numerical approach to the testing of the fission hypothesis. <i>Astron. J.</i>, 82:1013--1024, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[S. MALLAT and W. L. HWANG. Singularity detection and processing with wavelets. <i>IEEE TRANSACTIONS ON INFORMATION THEORY</i>, 38(2):617--643, Mar 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[F. Milde, M. Bergdorf, and P. Koumoutsakos. A hybrid model for turmor induced angioegenesis. <i>Biophys J</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[K. Miller and R. N. Miller. Moving finite elements. I. <i>SIAM J. Numer. Anal.</i>, 18(6):1019--1032, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274148</ref_obj_id>
				<ref_obj_pid>274112</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[M. L. Minion and D. L. Brown. Performance of under-resolved two-dimensional incompressible flow simulations, ii. <i>Journal of Computational Physics</i>, 138:734--765, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[R. Mittal and G. Iaccarino. Immersed boundary methods for viscous flow. 37:to appear, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[J. J. Monaghan. Extrapolating B splines for interpolation. 60(2):253--262, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[J. J. Monaghan. Smoothed particle hydrodynamics. <i>REPORTS ON PROGRESS IN PHYSICS</i>, 68(8):1703--1759, Aug 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[J. P. Morris. Simulating surface tension with smoothed particle hydrodynamics. 33(3):333--353, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[S. D. Muller, I. Mezic, J. H. Walther, and P. Koumoutsakos. Transverse momentum micromixer optimization with evolution strategies. <i>COMPUTERS and FLUIDS</i>, 33(4):521--531, May 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>512581</ref_obj_id>
				<ref_obj_pid>512576</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[S. Osher and R. P. Fedkiw. Level set methods: An overview and some recent results. 169(2):463--502, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>56815</ref_obj_id>
				<ref_obj_pid>56813</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[S. Osher and J. A. Sethian. Front propagating with curvature dependent speed: Algorithms based on Hamilton-Jacobi formulation. 79(1):12--49, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[S. J. Osher and R. P. Fedkiw. Level set methods and dynamic implicit surfaces. <i>Springer Verlag</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>587358</ref_obj_id>
				<ref_obj_pid>587162</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[M. L. Ould-Salihi, G.-H. Cottet, and M. El Hamraoui. Blending finite-difference and vortex methods for incompressible flow computations. 22(5):1655--1674, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[C. S. PESKIN. Numerical-analysis of blood-flow in heart. <i>JOURNAL OF COMPUTATIONAL PHYSICS</i>, 25(3):220--252, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1018053</ref_obj_id>
				<ref_obj_pid>1018014</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[H. Pfister and M. Gross. Point-based computer graphics. <i>IEEE COMPUTER GRAPHICS AND APPLICATIONS</i>, 24(4):22--23, Jul-Aug 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>585396</ref_obj_id>
				<ref_obj_pid>585388</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[P. Ploumhans, G. S. Winckelmans, J. K. Salmon, A. Leonard, and M. S. Warren. Vortex methods for direct numerical simulation of three-dimensional bluff body flows: Applications to the sphere at <i>Re</i> = 300, 500 and 1000. 178:427--463, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[S. Premoze, T. Tasdizen, J. Bigler, A. Lefohn, and R. T. Whitaker. Particle-based simulation of fluids. <i>COMPUTER GRAPHICS FORUM</i>, 22(3):401--410, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801167</ref_obj_id>
				<ref_obj_pid>964967</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[W. T. Reeves. Particle systems - a technique for modeling a class of fuzzy objects. <i>Computer Graphics</i>, 17:359--376, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>287247</ref_obj_id>
				<ref_obj_pid>287244</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[W. J. Rider and D. B. Kothe. Reconstructing volume tracking. 141:112--152, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[L. Rosenhead. The spread of vorticity in the wake behind a cylinder. 127(A):590--612, 1930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[L. Rosenhead. The formation of vortices from a surface of discontinuity. 134:170--192, 1931.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[I. F. Sbalzarini, A. Hayer, A. Helenius, and P. Koumoutsakos. Simulations of (an)isotropic diffusion on curved biological surfaces. <i>Biophys J</i>, 90(3):878--885, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[R. Scardovelli and S. Zaleski. Direct numerical simulation of free-surface and interfacial flow. 31:567--603, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[I. J. Schoenberg. Contribution to the problem of approximation of equidistant data by analytic functions. <i>Quart. Appl. Math.</i>, 4:45--99, 112--141, 1946.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073282</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[A. Selle, N. Rasmussen, and R. Fedkiw. A vortex particle method for smoke, water and explosions. <i>ACM Trans. Graph.</i>, 24(3):910--914, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[J. Sethian. Numerical algorithms for propagating interfaces: Hamilton-Jacobi equations and conservations laws. 31:131--161, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[J. A. Sethian. A fast marching level set method for monotonically advancing fronts. 93(4):1591--1595, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325510</ref_obj_id>
				<ref_obj_pid>325509</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[J. A. Sethian. Fast marching methods. <i>SIAM Rev.</i>, 41(2):199--235, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>512582</ref_obj_id>
				<ref_obj_pid>512576</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[J. A. Sethian. Evolution, implementation, and application of level set and fast marching methods for advancing fronts. 169(2):503--555, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[J. A. Sethian and P. Smereka. Level set methods for fluid interfaces. 35:341--372, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97923</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[K. Sims. Particle animation and rendering using data parallel computation. <i>Computer Graphics (Siggraph '90 proceedings)</i>, pages 405--413, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808571</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[A. R. Smith. Plants, fractals, and formal languages. <i>SIGGRAPH Comput. Graph.</i>, 18(3):1--10, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[H. A. STONE. A simple derivation of the time-dependent convective-diffusion equation for surfactant transport along a deforming interface. <i>PHYSICS OF FLUIDS A-FLUID DYNAMICS</i>, 2(1):111--112, Jan 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248793</ref_obj_id>
				<ref_obj_pid>248779</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[J. Strain. Fast adaptive 2D vortex methods. 132:108--122, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>349989</ref_obj_id>
				<ref_obj_pid>349920</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[J. Strain. A fast semi-lagrangian contouring method for moving interfaces. 161(2):512--536, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312454</ref_obj_id>
				<ref_obj_pid>312452</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[M. Sussman and E. Fatemi. An efficient, interface-preserving level set redistancing algorithm and its application to interfacial incompressible fluid flow. 20(4):1165--1191, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>182718</ref_obj_id>
				<ref_obj_pid>182683</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[M. Sussman, P. Smereka, and S. Osher. A level set approach for computing solutions to incompressible 2-phase flow. 114(1):146--159, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[C. Varea, J. L. Aragon, and R. A. Barrio. Turing patterns on a sphere. <i>PHYSICAL REVIEW E</i>, 60(4):4588--4592, Oct 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[O. V. Vasilyev. Solving multi-dimensional evolution problems with localized structures using second generation wavelets. <i>INTERNATIONAL JOURNAL OF COMPUTATIONAL FLUID DYNAMICS</i>, 17(2):151--168, Apr 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[L. Verlet. Computer experiments on classical fluids. I. Thermodynamical properties of Lennard-Jones molecules. 159(1):98--103, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>371653</ref_obj_id>
				<ref_obj_pid>371642</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[J. H. Walther and P. Koumoutsakos. Three-dimensional particle methods for particle laden flows with two-way coupling. 167:39--71, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[N. Zabusky, M. Hughes, and K. Roberts. Contour dynamics for the euler equations in two dimensions. 30:96--106, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[S. T. Zalesak. Fully multidimensional flux-corrected transport algorithms for fluids. 31(3):335--362, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401167</section_id>
		<sort_key>350</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Professional Development and Education: Get the Job You Want in Computer Graphics]]></section_title>
		<section_page_from>10</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098680</person_id>
				<author_profile_id><![CDATA[81332531549]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pamela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thompson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098681</person_id>
				<author_profile_id><![CDATA[81421595685]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fran]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Zandonella]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098682</person_id>
				<author_profile_id><![CDATA[81365596693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szymanski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401168</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<pages>102</pages>
		<display_no>26</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Get the job you want in computer graphics]]></title>
		<page_from>1</page_from>
		<page_to>102</page_to>
		<doi_number>10.1145/1401132.1401168</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401168</url>
		<abstract>
			<par><![CDATA[<p>What does it take to get a job in the computer graphics field? A top career coach and recruiter reveals the secrets of how to create an irresistible resume and showcase your talent in a demo reel to get the job you want. Sample resumes and demo reels are included. Guidelines for cover letters, portfolios and career tips and advice are included. Career Coach and Recruiter Pamela Thompson will be joined by Fran Zandonella who will present ideas and strategies for people seeking technical and programming jobs and Stan Szymanski, of Sony Imageworks, who will explain how to interview, negotiate and analyze the offer letter.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.7.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003580.10003583</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing profession->Computing occupations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098683</person_id>
				<author_profile_id><![CDATA[81332531549]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pamela]]></first_name>
				<middle_name><![CDATA[Kleibrink]]></middle_name>
				<last_name><![CDATA[Thompson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ideas to Go, Meridian ID]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098684</person_id>
				<author_profile_id><![CDATA[81421595685]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fran]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Zandonella]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fran Zandonella Consulting]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098685</person_id>
				<author_profile_id><![CDATA[81365596693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Szymanski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[I've provided some links to online technical resources, books, and sample interview questions and interviewing tips, as well as a glossary.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Get the Job You Want in Computer Graphics Organizer: Pamela Kleibrink Thompson Recruiter/Career Coach 
Ideas to Go 1752 W. Cedar Grove Meridian ID 83646-3637 USA PamRecruit@q.com Additional instructors: 
Fran R. Zandonella, CEO, Fran Zandonella Consulting, franz@siggraph.org Stan Szymanski, Senior Vice President, 
Digital Production Creative Resources, Sony Pictures Imageworks. Class Description: What does it take 
to get a job in the computer graphics field? A top career coach and recruiter reveals the secrets of 
how to create an irresistible resume and showcase your talent in a demo reel to get the job you want. 
Sample resumes and demo reels are included. Guidelines for cover letters, portfolios and career tips 
and advice are included. Career Coach and Recruiter Pamela Thompson will be joined by Fran Zandonella 
who will present ideas and strategies for people seeking technical and programming jobs and Stan Szymanski, 
of Sony Imageworks, who will explain how to interview, negotiate and analyze the offer letter. Prerequisites: 
No prior course is required. This course is designed for those seeking a career in computer graphics 
or for those who train others in computer graphics, such as teachers. The content is also appropriate 
for a veteran in the industry who is searching for his next position. Syllabus: Introduction: 5 minutes 
(Thompson) Resumes and Cover Letters: 20 minutes (Thompson) Portfolios: 10 minutes (Thompson) Technical 
Jobs: 50 minutes (Zandonella) Demo Reels and Shot Breakdown: 50 minutes (Thompson) Interviewing and the 
Offer 40 minutes (Symanski) Career Tips: 10 minutes (Thompson) Conclusion: 5 minutes (Thompson) Q&#38;A: 
35 minutes (Thompson, Zandonella, Szymanski) Get the Job You Want in Computer Graphics &#38;#169;Copyright 
2008 Pamela Kleibrink Thompson, All Rights Reserved Pamela Kleibrink Thompson, Recruiter/Hiring Strategist/Career 
Coach, Ideas to Go, PamRecruit@q.com Pamela Kleibrink Thompson is a recruiter/hiring strategist for such 
clients as Laika, Technicolor, Blue Sky Studios, Toybox, Framestore, Digital Domain, and Walt Disney 
Feature Animation. She is a career coach and consults with colleges and universities to design animation 
training programs. Her animation production background includes features such as Bebe's Kids, the Fox 
television series The Simpsons, and the original Amazing Stories episode of Family Dog. She writes a 
monthly column on Animation World Network (http://mag.awn.com) called The Career Coach. She is available 
for personal career coaching, speaking engagements, and recruiting. Contact her at PamRecruit@q.com 
1 Introduction What does it take to get a job at a visual effects, computer animation or interactive 
company? This course shows how to open the door to interviews, put your life on a one-page resume, and 
showcase your talent in a three-minute-or-less demo reel. Getting you an interview with someone who can 
hire you is the purpose of the resume, portfolio and demo reel. Prepare them with care. Ask others for 
feedback before you send them out. Everything you submit should be labeled clearly with your name, phone 
number and email address. Make it easy for the employer or recruiter to see your work and make it easy 
to contact you. Many studios have the same basic requirements for submission: a resume, a cover letter 
specifying your area of interest, a portfolio (samples of hand skills and traditional work), a demo reel 
and a shot breakdown. These course notes will discuss each of these requirements. 2 Resumes If your 
resume doesn t work, neither will you. As a recruiter and career coach I have seen thousands of resumes. 
Here is how to make yours effective: Make sure your contact information is on your resume and it is 
current. This includes your name, phone number (with current area code) and email address--if you have 
one. Make sure the contact information is easy to read and easy to find. Use a font where it is easy 
to distinguish a numeral 1" from the lower case letter l and in a large enough type face to be easily 
read (at least 10 point) Don t use a type font that is ornate. A resume is not the place to be creative. 
One resume posted at a SIGGRAPH career center laid out all the information in a spiral so you had to 
spin the resume to read it. Don t make someone hunt for your phone number or email address. The best 
place for contact information is near the top, right below your name. When emailing your resume to an 
employer or answering an online ad, don t send a web site url and expect the employer or recruiter to 
visit that web site to find your resume. If you want someone to get your resume, email it to them as 
a message, not an attachment. (Sometimes downloads don t work and many companies don t accept attachments 
due to viruses.) If you email a resume, make sure your name, phone number and email address are on it. 
Make sure your resume is readable without HTML code embedded in it. Don t email your resume as a jpeg 
attachment. They don t print well and most employers won t open them. Use bullets and break up large 
paragraphs of text. You don t want your resume set aside to be viewed later due to huge blocks of type 
that challenge the reader. List your skills. Be specific. Don t say "a variety of software packages." 
List the specific software you use. Don t bury your skills in a paragraph and make the reader hunt for 
the information. List your experience in reverse chronological order. Put the most recent information 
first. If your resume is more than one page, put your name, phone number, and email address on each page. 
Your resume is not your life story. Include only the information that is relevant to an employer. If 
you have 5 years experience in the computer graphics field, employers don t care that your first job 
was at a pizza parlor. Your resume should tell who you are--what you know (skills), what you ve done 
(accomplishments), and what you want to do (objective or goal). If you are changing careers, focus your 
resume on the job you want rather than the job you have. If your resume shows a variety of jobs, make 
sure you have an objective at the top that indicates what job you re seeking. Use paper that copies well--white 
or off-white. Test your resume. Copy it and make a copy of the copy. Surprised? Orange and dark blue 
paper turns black. Marbleized paper makes your resume look like someone poured coffee over it. After 
doing the copy test you'll find those beautiful graphics in the background are now some of the ugliest 
stuff you've seen on paper and what's more, you can no longer read your phone number or name which looked 
so crisp in front of the graphic on the original. Graphics or artwork on a gray scale behind the type 
doesn t copy or scan well. If you want someone to get a sample of your graphics include it on a separate 
page with your name and contact info. Proofread to get rid of typos and spelling mistakes. Ask a friend 
to review it too he or she may catch an error in your phone number you missed. Tips for a better resume: 
--Many companies scan resumes into computer databases so select a font that won t confuse the computer. 
--If you have a web site, include the address on your resume along with your email address and current 
phone number (with area code). --Review your resume every 6 months to update your skills and accomplishments. 
Emphasize accomplishments, not responsibilities. Your resume is a marketing tool. It is a chance to brag. 
Highlight any awards or special accomplishments. List your software skills and include the version of 
the software you use Maya 7.0" not just Maya. Keep the layout simple. Make your resume concise and easy 
to read. A resume needs to communicate clearly. A simple format such as this does the job. Name Contact 
Info Objective Skills (use bullet points) Experience Education 3 Cover Letters A cover letter should 
be sent with your resume. Keep it short. Use it to highlight your best skills, or mention qualifications 
not shown in the resume. Cover letters should always be addressed to a specific person. It s good to 
point out if you have a personal connection met you at a SIGGRAPH meeting, referred by someone, read 
about you in a magazine, etc. That s why it s important to go to all those events so you can hear about 
job openings and mention people s names in the cover letter! Cover letters should state what you want 
to do for the company, what job you are applying for, and why you are right for that job. Cover letters 
should demonstrate some knowledge of the company. Make it clear that you understand the needs and goals 
of the company and can be productive immediately. What special skills or expertise are you offering? 
What advantage will the company have if they hire you? Cover letters should be brief, but give some insight 
into your personality. You can use them to point out an asset you bring to the company that might not 
be obvious in the resume. 4 Portfolios If you are an artist, an outstanding portfolio and demo reel 
is essential. A portfolio of life drawing, illustration, photography (if you are interested in lighting), 
sculpture (if you are interested in modeling), character design or color design is a big plus. A foundation 
in fine art and training in aesthetics is advantageous for any aspiring CG artist. List these skills 
or education and training on your resume. You can include samples of traditional work on the end of your 
reel or include a disposable portfolio with your reel. Include only your best work and put your strongest 
piece first. Label each piece in your portfolio with your name and contact information as well as the 
medium the piece was done in. Label each piece indicating what project it was for and if it is your original 
design, state that. Pay attention to the layout of your portfolio so your work is presented, not just 
haphazardly thrown together. Label the spine of your portfolio with your name and contact info as well 
as the cover and include a resume inside. 5 Demo Reels What should you show? Before you create your 
reel, assess your strengths, skills and interests. There are many different jobs for artists. Make sure 
your demo reel and portfolio are relevant to the job you want. If you want a job as a character animator, 
your reel should emphasize acting and performance, not compositing. Your demo reel should reflect the 
very best you can do. Keep it short make them want to see more. Shot Breakdowns The shot breakdown (also 
called a demo reel breakdown) briefly describes your contribution to each shot and the tools used. Shot 
Breakdown Example: Shot 1: Witch Melting -- animated the witch melting using Maya 7.0; created the textures 
using Photoshop CS2. If you did everything on your reel, say so. Never claim anyone else s work. You 
can include a written shot breakdown or you can include slates on the tape before each segment describing 
what you did or superimpose the description over each scene. The breakdown identifies your responsibility 
on each sequence and shot and software uses (if applicable) and must be included.  What format is acceptable? 
Check the company s web site to see what formats are acceptable. For example, Hybride, a company in Canada, 
will accept reels in DVD format, VHS ½ NTSC or PAL. Hybride does not want to receive CD Roms or web site 
addresses. Most companies will not open attachments. Dreamworks will not look at jpg files or website 
links. Check company web sites to find out what the specific demo reel and portfolio requirements are 
for the areas that interest you. How are reels usually reviewed? At a group meeting, artists review many 
reels. This means that you must catch their attention from the start so put your best work first. The 
weakest part of your reel will also get their attention. If you have 5 years experience in the industry, 
work you did as a student should not be on your reel or in your portfolio. When should I submit? Companies 
accept submissions year round. If you submit your materials to companies at SIGGRAPH or any other conference, 
it may be a few months before your work is reviewed as companies get swamped with submissions during 
this time. It s better to submit your reel 3 to 4 months before a conference. Be patient and keep working 
at improving your skills. It s okay to submit new work every 6 months, as your skills improve. Quick 
tips For artists, a demo reel and portfolio are more important than a resume. Always include a resume 
and a shot breakdown with your reel. Your shot breakdown should always include your name and contact 
info. Your shot breakdown is a written outline that describes each shot and what you did for that segment. 
You could slate each shot on the reel with this information instead. Your breakdown list could include 
screen shots of the work on your reel. Pu the very best segment first. Remember your audience sees lots 
of demo reels and portfolios. Keep it moving. Demo reels will not be returned so never send your only 
copy to anyone. Your reel must be labeled with your name and contact info (phone and email address). 
 A screen shot of work from your reel helps it stand out from a sea of black video boxes or generic DVD 
cases. Remember to label the spine of your reel as well. A DVD box with screen shot and contact info. 
 Include slates on your reel or superimpose graphics with your name and contact information in case the 
label falls off. A recruiter at Animal Logic in Australia once bemoaned the fact that his office had 
piles of fabulous reels that he couldn t bear to part with even though there was no contact information 
on them. He had no idea how to find those talented artists. Label your reel and include an end slate 
on your reel with your name and contact info. Your demo reel should: *Contain only your best work and 
be of high caliber. * Be representative of your recent work and showcase your skills and talent. *Be 
no longer than 3 minutes. It can be shorter. People have been hired on 15 seconds or less. It s better 
to be impressive than excessive so take out anything that is not top quality. *Be a ½" VHS cassette 
in NTSC format unless the company accepts other formats. (This is the format almost all companies can 
deal with in the United States. If it's a PAL tape, be sure the company has a way to view it). Everyone 
has a VHS machine not everyone has a DVD, even today! *If you submit a DVD, keep the DVD menu simple 
to understand and easy to navigate. Test your DVD in several machines before you send it to ensure it 
will work. Don t include color bars or shrink wrap your reel. Never send masters or originals. Minimize 
erotica, satanic and violent material. Don't use loud, obnoxious music or elaborate sound. We are interested 
in the visuals, not the sound design, music or editing so don t work harder on the sound than the visuals. 
You may want to use music that is in the public domain such as a classical piece; but since your reel 
is not for commercial purposes other than for job hunting, copyright is not an issue. We often turn off 
the sound when reviewing reels. Don't do a chronological work history or include mediocre work. Don't 
save your best stuff for last. The viewer may never get to it. Don't include live action film without 
animation or computer graphics unless it is to show your compositing skills. Don't send the exact same 
reel in 6 months later. We have very good visual memories. Don't send unfinished work or early tests 
unless you're showing the progression of a finished shot. Don't expect prospective employers to visit 
a web site to view samples or a resume. Don t email images, unless you are requested to do so.  6 Jobs 
in Digital Visual Effects There is a specific sequence of steps in any visual effects shot using computer 
graphics--modeling, skeleton/bones/set-up, texturing, animation, lighting, and compositing/rendering. 
If you are skilled or talented in three or more of these areas you may want to consider a job with a 
small company, that uses generalists. You'll have an opportunity to do many different jobs. If you are 
a specialist, you'll probably be happier at a larger company where you will do one specific job and develop 
a specific skill set that you do well. This section will show you what is involved in each of these steps 
and the skills required You will also find out what a studio wants to see in a portfolio and demo reel 
from CG applicants for each specialty. Before you attend any large conference such as SIGGRAPH, check 
out the web site to find out who will be recruiting at the conference. It's a good idea to send your 
materials to the company as soon as possible before the show as companies are swamped with applicants 
during the conference. They will have more time to review your work before the show. If they review your 
work before the show and like what they see, you may be given an opportunity to interview with company 
representatives. Modeling In 2D animation, the first step in the process is prop, character and background 
design. In 3D computer animation this is called modeling. A model is a virtual object that is created, 
colored, textured and animated using computer graphics Modelers are responsible for creating complex, 
organic models needed for character animation, prop elements for effects, and virtual sets for layout. 
Modelers must build models that are high quality, efficient to render, and easy to animate. The model 
begins as a series of lines called a wireframe that outlines the shape of the object. These wireframes 
communicate just the basics about the 3D object to come. While incomplete, these transitional illustrations 
have the benefits of being easy and quick for the computer to calculate and manipulate. The elementary 
shapes of 3D objects can be quickly rotated in space and viewed from different angles.  Skill set: Know 
how to sculpt--how to model things in the real world. Know how to draw. A background in art, design, 
architecture, or film is a plus. Some general animation skills are needed for character modeling and 
testing. Demo Reel/Portfolio tips: Reality is key. If it is a horse it should look like a horse. Show 
a sense of proportion and detail unless you're going for a stylized cartoon look. Wireframes of models 
show how efficient you are as a modeler. Include low-poly work if you want to get into games. Include 
a hard copy or video output of digital models, photographs of traditional sculptures or models, a drawing 
portfolio of model designs and sketches, and life drawing. Your portfolio should show you understand 
modeling methods, know how to build true shapes and forms, and how to use shaders and textures to add 
detail. An ability to paint textures is a plus. Some modelers make a reel showing their model on a turntable, 
revolving slowly. Skeleton/Bones/Set-up After the character model is created as a wireframe, the model 
must be given a skeleton or bones. Once the skeleton is established, the limitations of movement must 
be specified by selecting the locations and types of joints. These selections are known as "controls." 
Controls define the way a character's limbs or an object's parts can move. This work is done by an animation 
or character set up technical director (TD), (which is also sometimes called a character engineer or 
motion technical director). Character set up TDs work with character animators and modelers to define 
and create the controls that will help the animator create a realistic, convincing performance. This 
job requires an understanding of animation principles and strong technical problem-solving skills. Character 
set up TDS also program tools to create the muscle movement that happens when joints flex. Skill set: 
Know how the body moves and works and know where and how the joints rotate. Know the many types of joints 
such as ball and socket, hinge, saddle, pivot, etc. Understand deformation issues such as how muscles 
properly animate. Some companies prefer you to have experience with motion capture or motion control 
and data conversion experience. Demo Reel Tips: Examples of work showing computer animation and organic 
modeling are important in a portfolio for this area. Show your solid understanding of anatomy and skeletal 
issues and your expertise in computer animation and character design. Texturing Texture painters help 
define the surface qualities of an object including colors and textures of the characters, props, and 
environments. Some studios may call this a render artist or a look development artist. Texture painters 
create the look of more complex CG elements such as principal characters. This could include painting 
texture maps (e.g., color maps, specular maps, displacement maps, etc.), working with TD's to enhance 
the look, and creating lighting setups. This artist typically has previous production experience or has 
demonstrated exceptional skills/abilities in CGI/traditional artwork. Skill set: Have a strong art background 
and expertise with digital paint programs such as Photoshop and experience with 3D paint applications. 
Understand CG lighting (ambient, specular, diffuse, bump, etc.). Some studios prefer familiarity with 
UNIX as well. Portfolio Tips: Use good textures that are appropriate for the model. Show that you can 
create your own textures and have good judgment on how to use them. Your textures should add detail that 
otherwise would have to be modeled. Animation Animation is the creation of the illusion of motion. By 
displaying sequential images one after another in rapid succession items drawn or recorded on the images 
appear to move. Character animation is animation in which objects or characters are animated to give 
the illusion of personality, life, and character. Character animators use the computer to bring digital 
characters to life. Computer character animators may have a background in traditional (hand-drawn) or 
stop-motion animation in addition to training in computer animation. The sensibilities and mind set of 
a good animator should transfer between mediums. Character animators are concerned with performance. 
Skill set: Demonstrate a strong background in traditional animation and traditional character development. 
Character animators need to have story telling ability and acting ability. An art background in life 
drawing, painting or sculpting is also helpful. Training should cover anatomy, perspective, and composition, 
including solving problems in weight, balance, movement, space, construction, and proportion. Artistic 
sensibility in the areas of modeling, lighting, and rendering is a plus. A knowledge of inverse kinematics 
systems, constraints and expressions is a plus, as is character animation experience with demonstrated 
abilities in facial animation and lip synching. Demo Reel Tips: A video reel of animated scenes which 
demonstrates digital acting and performance is required. The reel should convey nuances of emotion and 
personality, show weight, balance, and timing. It should include basic motions like lifting, sawing, 
pulling, pushing, and interacting with scene elements not just running, jumping, or walking. Work should 
demonstrate ability to define character personality, create fluid motion, organic movement and storytelling. 
Effects animators animate the non-character aspects of the shot including vehicles, natural effects like 
water, dust, tornadoes, and hurricanes, and any other effects like lasers, explosions or bullets. Effects 
animators add the forces of nature to an animated film, as well as animate props, furniture, or other 
objects. Effects aid in producing a believable world and setting the mood for the story. Skill set: Demonstrate 
expertise in the realistic creation of wind, rain, sunlight, mist, fog, shadows, and fire. In addition 
to having a passion for the elements of nature and particle effects animation, requirements for artists 
interested in this area of animation match those for a character animation artist. Demo Reel Tips: A 
video reel showing a variety of effects is required. Show proficiency in modeling, lighting, particle 
systems, rendering and texturing. Lighting Lighting artists work in the 3-D environment creating the 
look of individual elements and entire scenes. This can include the creation of textures or the subtle 
use of virtual lights to enhance the mood and tone of a scene. Skill set: Be able to light sets. Understand 
color, contrast, and lighting design, and be familiar with 3-D lighting. Know how lighting can be used 
to increase efficiency by reducing the number of elements that need to be built in a scene Demo Tips: 
The most overlooked aspect of a demo, lighting should create a mood or atmosphere. Don't have over-lit 
scenes just to show off models. Lighting should add excitement and depth to a scene. A portfolio showing 
video examples of 3-D lighting or traditional work, including painting, drawing, or photography, is appropriate. 
Compositing/Rendering The final step where the computer animated character is combined with the real 
actors and live action plate is called compositing. The compositor is responsible for integrating multiple 
independent elements which could include green screen elements, 3D elements, and background plates into 
the final image. It all comes together in rendering. As Terrence Masson states in his book CG 101, "Rendering 
is the cinematography of computer graphics." Rendering is the creation of images in the computer from 
the modeling, lighting, texturing and animation information. Skill set: Have a thorough understanding 
of color, light, film and traditional photographic techniques. Understand color difference matting. Have 
a strong visual sense and the ability to distinguish subtle differences that affect the matching of elements 
created in multiple mediums. Have an eye for color and scene match. Demo Reel Tips: Show skill in compositing 
moving footage, preferably film footage, animation and live action. Include before-and-after shots. Other 
examples of work in computer graphics or traditional art mediums are also encouraged. 7 Career Tips 
Once you get that job, do the very best job you can every single day. *Attitude is of paramount importance. 
Who would you rather work with 8 or more hours a day someone who is a positive, problem solver, eager 
to help or someone who complains about the work load, thinks the boss is crazy, and takes breaks every 
15 minutes. The hiring manager is looking for someone who not only is capable of doing the job (has the 
skill), but who will fit in (has the will). *There is no such thing as a small job. Do your best on every 
job you get and your circle of fans will grow. *Continue to network and keep your resume, portfolio and 
demo reel up to date. *Being a team player is essential to keeping a job and sustaining your career. 
Teamwork is essential in any job, but especially those in computer graphics. As an artist you may be 
used to working independently. But when working on a project, whether it s creating visual effects for 
a live action film, scenes in an animated project, or developing a video game, you will be working with 
many other people. Collaboration is key. Teamwork, communication and cooperation is essential. Every 
model you create, every shot you light, every frame of film you are involved with will be worked on by 
someone else as well. That is why so many job descriptions include the following: Must be a highly motivated 
self-starter, a true team player, extremely well organized and detail-oriented, with the ability to take 
direction and follow through. Must work well with others in a high volume, fast-paced environment. Flexibility 
and a sense of humor are a real plus. Qualities that are good for anyone in the field of computer graphics 
include being self-disciplined and being able to complete tasks independently. Higher positions require 
management skills. It s a big plus if you have a good understanding of egos and are aware of the politics 
involved, are diplomatic and honest, and have a high degree of integrity, and are able to handle stress. 
Here are a few skills you will need for a successful career: Work well with others: Nearly all projects 
require you to work as a member of a team. How you interact with others can have a significant and lasting 
impact on your career. Since most jobs are found through networking, make sure you are someone that people 
will want to work with again. Be supportive of other team members. During life drawing sessions after 
work one of the artists I supervised appointed himself critic and dispensed unsolicited advice and critiques 
to the other artists in the room. The arrogance of the critical artist was not appreciated and the other 
artists did not want to work with him. Don't be a prima donna. Don't expect to get special treatment 
and don't be someone who needs special treatment. If you become known as a person who is high maintenance, 
you will find it increasingly difficult to get hired. A person with a good reputation but little experience, 
talent or skills may be preferred over a person who is talented but difficult to deal with. Listen attentively: 
Be a good listener. Understand what is required and if you need clarification don't be afraid to ask 
questions. If you are unclear about your assignment, ask the person who assigned it to you. Follow directions 
and do the work that is required. If your job is to animate a model, do not redesign the model. Do not 
try to argue or belabor a point that has already been discussed and decided on. Communicate well: You 
must be able to express your ideas clearly and succinctly both to the artist at the desk next to yours 
as well as to supervisors. Contribute and Cooperate: At a video game company where I worked all the artists 
were expected to do all aspects of the animation process from designing characters all the way through 
animating and rendering those characters. One artist was adamant that he should be exempt from designing 
characters, that it didn't fall into his job description as animator. He was used to a studio that segmented 
all the jobs into well­defined roles. He stubbornly refused to design characters and was soon ousted 
from his team. No other teams wanted to bring him on (remember how important it is to work well with 
others) and he soon found himself out of a job. Demonstrate problem solving ability: What you demonstrate 
in your portfolio and demo reel is how you think. It illustrates not only your artistic ability but how 
you solve problems. Problem solving is a key skill that all employers want. Your demo reel gives the 
viewer insight into your problem solving ability and how your brain works. Complete work in Reasonable 
Time: Completing your job on time is essential so the other members of your team can do their jobs. Respect 
deadlines. Sustain Focus on Task: Stay focused on the job at hand. If you have trouble with your scene, 
don't wander in the halls and bother your co-workers. Seek advice from your supervisors if something 
is really stumping you. Contribute to Group Discussion: When it's appropriate, such as during brain storming 
sessions, voice your ideas and suggestions with a plan of action and possible solution. The more you 
know about the project, the more you'll be able to contribute. Be Eager to Learn: To build a successful 
career, you must be enthusiastic and ready to learn new skills. Techniques constantly evolve and you 
must be willing to try something new. That is the one constant in business---change! Be flexible and 
adaptable. Show Enthusiasm: Be passionate about what you do. Computer graphics requires patience both 
while at the workstation and also often between jobs. Love what you do and learn all you can about the 
history of computer graphics and animation, as well as techniques and trends. Use the time between jobs 
to expand your skill set and your network. Go Beyond What is Expected: A friend who is now a character 
designer started as a production assistant (P.A.) on a television show. She got that first break by being 
persistent and keeping in touch with the production staff. Once she got that job she not only did an 
outstanding job in her position and never complained about the low pay, but also made herself available 
to others who might need extra help and put in extra hours to learn all facets of the production. When 
it was time to hire a new person on the crew, everyone asked that the P.A. be hired. She has the same 
work ethic today, a decade later, as she showed on her first job. 8 Conclusion There is a world of opportunity 
in computer graphics. There are many jobs outside the huge companies that do visual effects. Some areas 
where computer graphics artists work include: music videos games ride films scientific applications: 
NASA visualizations product design: cars, toys, bicycles military simulations architectural firms motion 
picture industry: posters, advertising, movies: visual effects films, computer animated films previsualization 
television: prime time, Saturday morning, syndicated series, cable title sequences: motion pictures or 
television theme park design, themed restaurants, themed retail stores advertising, commercials direct 
to video features training films, corporate videos, industrial films internet: web sites slot machines 
multimedia and educational software legal recreations consumer products: packaging, advertising broadcast 
design and motion graphics: logos, station IDs exhibits: museums, trade shows medical illustration publishing 
and illustration: magazines,books, newspapers, comic books animation for mobile phones There is no single 
way to get in. Your most important marketing tool is your reputation. Make sure it is stellar! and don 
t burn any bridges. Design your marketing materials (the resume and demo reel) to reflect your strengths. 
Whether you submit a demo reel or portfolio, remember to always include a resume and breakdown with it. 
Have others take a look at them and give you feedback before you send them out. The resume, portfolio 
and demo reel are marketing materials prepare them with care. Remember, the purpose of your marketing 
materials are to get you an interview with someone who can hire you. An interviewer is trying to discover 
three things: 1) Can you do the job? Your demo reel, job experience and education show the talent and 
skill level you have and helps answer that question. 2) Will you do the job? The can-do attitude and 
enthusiasm you portray during the interview demonstrates a willingness to do the job. 3) Will you fit 
in? All the research you have done on the industry and the company and the people you interview with 
shows that you are one of the team ready to be part of their team. Update your skills. Sign up for classes, 
seminars, conferences and attend user group meetings. Find internet discussion groups where you can post 
your work for feedback. Anyone working in the visual effects, computer graphics and animation industry 
knows that learning is an ongoing process. You can t grow in your career unless you learn, so do your 
homework! Maintain a positive, problem solving attitude and professional approach on every job you have 
and build your network, and it s likely you will have a long, successful career. Recommended Reading: 
The Perfect Resume by Tom Jackson is a great guide to creating a resume that will work for you. The library 
will probably have a copy (650.14 J138p 1990) CG 101 by Terrence Masson, published by New Riders, 1999, 
gives an overview of CG. The Illusion of Life: Disney Animation by Frank Thomas and Ollie Johnston, Abbeville 
Press, 1981, is like a course on animation by two of Disney s Nine Old Men. Producing Animation by Catherine 
Winder and Zahra Dowlatabadi, Focal Press, gives a fabulous producer's overview of the animation process. 
But Producing Animation is not just for producers. Anyone who works in or aspires to work in animation 
would benefit from Producing Animation. The writers are both experienced production people and remind 
us how important attitude is and why it's essential to establish a sense of mutual respect for everyone 
involved on a project. Get the Job You Want in Computer Graphics Getting a Job as a Technical Director 
/ Software Engineer &#38;#169; Copyright 2008 Fran R. Zandonella, CEO, Fran Zandonella Consulting Fran 
Zandonella, CEO, Fran Zandonella Consulting, has programmed tools and supported artists working in 3D, 
2D, films, commercials, and shorts at LAIKA, Inc and Disney Feature Animation. She has worked in both 
Software / Research and Development, and Systems. In her time in the animation industry, she has written 
user interfaces, created tools for other software departments, evaluated new software, written documentation, 
performed light system administration and hardware administration duties, and managed render farms. 
Get the Job You Want in Computer Graphics Getting a Job as a Technical Director / Software Engineer &#38;#169; 
Copyright 2008 Fran R. Zandonella, CEO, Fran Zandonella Consulting Learn how to get a job as a technical 
director or software engineer in the computer graphics field, specifically in the animation and effects 
industries. This course reveals the education, skills, and self knowledge required to become a TD or 
Software Engineer, as well as the process to obtain and succeed in those careers. 1. What Does It Take? 
What does it take to get a technical job in Computer Graphics? How do you get your foot in the door? 
How can you advance in your career? Persistence aka passion, tenacity, stick-to-it-tiveness, focus 
No stopping for a lollypop ! The people in this industry are driven, intense, fun-loving folks. It takes 
these qualities to be an excellent technical person in this field. Note: Many people apply, but only 
1 2 percent get hired. Education and Skills Most technical positions in computer graphics require at 
least a Bachelor s degree in Computer Science, Engineering, Physics or Math. Some positions require PhD 
s. If you don t have a degree, it is not impossible, but you will need to demonstrate that you can do 
the work via a demo reel, selling your own software, writing software for an open source project, or 
creating software that is available via an online forum, for example at the highend3d website. For example: 
During the time I worked at Disney Animation, one third of the software department had PhD s, one third 
had Master s degrees, and one third had Bachelor s degrees. Other companies I have worked for only required 
Bachelor s degrees, but having a Masters in CG helps. Personality Most tech folk in CG are geeky. Geeks 
or Nerds like to solve problems, build things, and generally be technically creative. If you fit in the 
group, it is more likely that you will be able to get that job and that you will enjoy the job.  Process 
In many regards, the process for getting a job in CG is the same as for any field. Here are some particulars 
about this field. CG is a small and sexy industry. This means there is more competition to get in. Be 
prepared to fight for your job. Be prepared to work hard to get this job, and prepared to work hard to 
keep your job. For example, I had to work the day after Thanksgiving. The studio was mostly empty, but 
one of my co-workers wandered in around 1pm. He happily told me that he d been working on his software 
until 2am on Thanksgiving. Talk about dedication! 2. Work Arenas The Computer Graphics industry spans 
a wide range of disciplines. The main areas are Gaming, Animation / Visual EFX, and Scientific Visualization. 
Many other fields require graphics, so there are more work arenas than I will mention here. There are 
some other major fields like Photography, Virtual Reality, or Hardware. Be sure to investigate areas 
you may not have thought about, like law. Lawyers need animations and graphic examples to explain their 
complex case to the jurors.  Education &#38; Skills for Job Bachelors in Computer Science, Engineering, 
Physics, Math Preferably from a school that has well known professors in Computer Graphics Helpful to 
have work experience at Software Company, Bank (IT), etc. High pressure coding Reward is close customers 
 3. Education &#38; Skills for the Job Before I tell you HOW to get the job, I m going to talk about 
WHO will get the job. Some of the skills needed to get the job will be used at work in the job. Education 
It is best to have a degree, preferably from a well-known school for computer graphics like Stanford, 
MIT, Carnegie Melon, University of Utah, University of Washington, or University of North Carolina. Note, 
this is not an exhaustive list. Depending your area of interest, it may be helpful to have a PhD in CG, 
Physics, or Math. For example, understanding physics and math will help you model water, waves, dust 
or smoke. Art schools with an technical focus are also good places to get degrees. These schools include 
places like the Savannah College of Art and Design, Academy of Art in San Francisco, Gnomon or Cal Arts 
in LA, etc. Skills Just Out of School If you are just getting out of school, pursue an internship. If 
the company doesn t have an internship program, see if you can convince them to create one for you. Find 
someone there to become your mentor. Be persistent. Out of School If you didn t go the internship route, 
make sure that you are currently working in a company that provides an environment with high pressure 
coding, and close contact with customers. In the CG industry, your customers are often as close as the 
next pod or room down the hall. Programs are often needed quickly, as production demands a quick turn-around. 
Visual EFX requires more speed than animation due to its tight production schedules. Solving the problem 
and putting the images on the screen are more important than a perfect program. Skills for Job Experience 
writing Python / PERL scripts, C++ and C programs, possibly Java, openGL, mel  Areas of specialty: hair/fur, 
cloth, particles, physics (breaking glass, fire, smoke, water, etc, used in effects), lighting, shading, 
or rendering  Skills for the Job (continued) Get experience coding in Python, PERL, C++/C, openGL and 
mel. Java is also good to know, but not required as much. Learn more than one platform: Linux, Windows, 
and Mac OSX. Most studios use more than one platform, and most of the major studios have standardized 
on Linux. Specializing Consider specializing in hair/fur, cloth, particles, physics (breaking glass, 
fire, smoke, water, etc, used in effects), lighting, shading, or rendering. Shader writers often make 
six figures, sometimes around $200,000 / year. Skills for Job 3rd Party applications: Maya, XSI, 3dsmax, 
Mental Ray, RenderMan, Shake, Houdini, After Effects  Competitive  Fight for cool graphics projects 
even if you have a PhD! Skills for the Job (continued) 3rd Party Applications Learn 3rd party applications: 
Maya, XSI, 3dsmax, Mental Ray, RenderMan, Shake, Houdini, and After Effects. Maya has a fantastic student 
discount, and Houdini offers free classes in Los Angeles. Take a class at your local college to learn 
the software. Competitive Always keep your skills sharp, and be prepared to fight for the cool graphics 
tasks, even if you have a Ph.D. Even a Ph.D. dissertation in the subject is no guarantee that you will 
get that assignment. 4. Self-Assessment / Personality for Job Personality is key in getting the job 
and keeping it. Tenacity / passion / persistence / focus is required both in the job, and to get the 
job. Projects may go beyond one or two years, or your software may become popular and sticky (meaning 
that you will have a hard time moving away from that software and onto other projects). For example, 
it took me at least 3 years to break into the industry. The same is true for many of my friends. Some 
days, few companies are hiring due to something like actor or writer strikes. Other times, a big project 
comes into town and every studio is hiring for it, and there are lots of jobs. Keeping your skills sharp, 
and keeping in touch with your contacts will lead you to the job. Be prepared for opportunity s knock. 
 Love of Chaos Both the movie and gaming industries are vibrant and chaotic. Both the environment and 
the life­styles are chaotic. Are you ready to spend time unemployed? Ready to work long hours on moment 
s notice? Self-Assessment (continued) Work Under Pressure Both the gaming and movie industry are episodic 
and periodic. In games, crunch time comes during the summer and very early fall in order to get the games 
into the stores in time for Christmas. Movie crunch time occurs in the winter and early spring for June 
movie releases, or late summer and fall for November movie release. Good service In the movie industry, 
the tech folk serve the artists, who are the rock stars of the industry. As the gaming industry is more 
tech heavy, many of your customers are just like you your teammates and fellow gamers.  Self-Assessment 
(continued) Technical Chops You need to have the technical skills to do the job. Programming skills, 
understanding object oriented programming, writing design specs, writing good bug reports and resolutions, 
and communicating with the documentation specialists as well as your team are all critical to success. 
Humility In this industry, most of the public will never know the work you do. Our only public exposure 
is at SIGGRAPH and like conferences. We may do the coolest things, but the role we play is the Wizard 
Behind the Curtain. During the time I was at Disney, I fixed many problems for artists: rescued their 
artwork, got everyone back up and running from a down time when the artists were on double time, fixed 
messed up and missing frames of the films. I removed Vikings from shots where they weren t supposed to 
be in the movie Atlantis, and made Tarzan surf nicely through the trees. I created tools for artists 
to create amazing art. However, you won t see any of my work when you watch Atlantis or Tarzan. The artists 
appreciated my work, but the public will never see it.  Self-Assessment (continued) Pride As a group, 
software engineers do take pride in our work, and get to show off to others in the know at SIGGRAPH and 
other technical conferences like the Gamer Developer s Conference. In film, it is possible to win a Scientific 
Academy Award for ground-breaking innovations in the science and technology of film. Team Work This field 
is very team-work oriented. There is not too much opportunity to be a loner. A movie or game doesn t 
often get done by one person alone these days. Make sure you have good communication skills. Learn your 
communication style. (There are a lot of resources to learn about your style online.) Be professional 
and friendly with your colleagues.  Self-Assessment (continued) Competitive Make sure that your technical 
skills are the best that they can be. There are many smart and hardworking people who want to get into 
this field. You need to keep your tech skills competitive. Long Hours Be prepared to work a lot of long 
hours whatever it takes until the job is finished. During crunch time, many people put in 12 hour or 
longer days. Crunch time can be as short as a month, or as long as 6 months. The days are often fun and 
busy, and companies will often pay for meals during crunch. In the past some companies have even had 
hairstylists on-site, and provided laundry pick up.  Life-Work Balance Tips  Continuing Education 
 Job layoffs are periodic  Save your money  Keep another job in your back pocket   Need lawyer for 
contract review  5. Life-Work Balance Tips There is a lot of talk these days in our industry about Life-Work 
Balance. Some tips to help you maintain it are: Continuing Education Always set aside time and money 
for continuing education. Some companies help with this, but many do not, or your boss may not. Don t 
wait for someone to offer it, pursue it on your own. Bring new knowledge into the company. Job Layoffs 
Be prepared for periods of unemployment. Some people save up when they are working and travel when they 
are not. I recommend studying something else that you can flip over into until the market picks back 
up. Software engineers usually are not tied to a production, though in gaming they may be tied to a game. 
Both gaming and film and periodic, and the computer industry itself is periodic, so be prepared to be 
unemployed, even if you have superior skills. Contract Review Some companies in the industry require 
you to sign a contract. It is good to know a contract lawyer who can review such contracts to protect 
your rights. Life-Work Balance Tips (continued) Long Hours Keep Fit I ve mentioned it before: We work 
hard. Some companies have a 50 hour work week. During a production crunch, we work a lot of hours. Remember 
to exercise. This keeps your brain in top shape, and keeps you healthy. Sitting for long times at the 
computer can be hard on your body. Move it! Maintain Relationships Pay attention to your personal life. 
Make arrangements so your family remembers who you are. Fast-paced This industry is fast-paced. Technology 
changes rapidly. Remember to take time out to breathe, take breaks, and vacations! Take care of yourself 
so you don t get carpal tunnel, or back problems. If you have a physical problem or get sick, it might 
keep you away from work longer than a vacation. 6. How to Get an Internship Benefits The benefits of 
getting an internship are: Gaining first-hand knowledge of the industry and types of jobs in it. Acquiring 
a mentor and developing a network of contacts. Discovering whether you like the job/industry, and discovering 
your strengths and talents. Ask! Ask for the internship. Go out onto the Expo floor and ask each company 
you are interested in if they have an internship program. If they do, get the info for it, or get the 
contact info for a person to speak to about it. Check the websites of the companies that you are interested 
in. Call the company receptionist and ask about programs. If they don t have one, see if you can speak 
to a manager. At one of my old companies, we got an intern because he came to our office and gave the 
receptionist his resume. The manager was impressed by his motivation, his skills were OK, and we put 
him to work. Types of internships Some companies have internship programs, and they may be elaborate, 
or very simple. Start by looking on websites, or attending any orientations that your college may host. 
If a speaker from a particular company comes to your school, ask him or her about internship programs. 
 Design your own internship If the company does not have an internship program, ask to design your own. 
The young man I mentioned earlier offered to work for free. (Generally, I don t recommend that people 
work for free in this industry, but it did work for that young man.) Be polite, and persistent. Pros 
of an Internship  Find a Mentor Even if you are assigned a mentor, find someone who shares your interests 
and learn from them. If you are interested in color science, find the person at that company that does 
that work and take them out to lunch. Make Connections Make friends with the employees at your company, 
and then continue to keep in touch. People move around, and that person may remember you fondly when 
they are at their next company. Get Recommendations Before your internship ends, get recommendations 
from your boss and/or co­workers. If you do a good job, you might get hired at that company. Cons of 
an Internship What if you don t like it? Then you have learned something about yourself and the industry. 
Take time to reflect and refocus your interests and energy. Typecast in a Job If you find yourself typecast 
in a role you don t like, you will need to work harder to change your focus. Take it as a learning experience. 
Look for what transfers from the previous position and use that to push you towards the job you are interested 
in. Study more in areas unrelated to the job you didn t like. Have a Good Attitude People want to hang 
out with someone who is cool and fun. Even if life is getting you down, don t get others down. Be the 
person that everyone wants to have on their team. Become the go to person on a project. 7. Process for 
Interviews The process for interviews is similar whether it is for a regular, full-time job, or for an 
internship. Pre-Interview Do all the things Pam told you about in her part of the talk. Research the 
company and find out what the pay range is for that type of job. There are websites that do salary surveys. 
Ask people who work at that company who do the job you are interested in what the general pay range is. 
Don t ask them how much they make because most people find it rude. Know what you are worth. Recruiter 
/ HR At the beginning, you may contact or be contacted by a recruiter. The recruiters job is to find 
as many qualified people as possible to give to the hiring manager. Their job is both screen out people 
who don t fit the criteria as well as attract the people who do fit the criteria. HR may give you a basic 
phone screen to see if you generally are the type of person they will hire. These interviews are often 
not very technical. Technical phone screen After the recruiter or HR person passes your resume to the 
hiring manager, and you meet the criteria, you will receive a technical phone screen from the hiring 
manager or some members of the technical staff who have been tasked with finding out if you have the 
technical chops. The technical phone screen will last between half hour and an hour. You may also be 
emailed a timed test which would include writing some example code.  In-person Interview  Minimum 
of 2 hours, but may be upwards of 5 hours and 15 people  Group interviews are common, with as many as 
6 interviewers  May be technical or not  focus on personality, might meet potential teammates for the 
first time In-person interview If you pass the phone screen, and are a strong candidate, the company 
will bring you in for an in-person interview. Before interviewing, ask HR for a list of the people with 
whom you will be interviewing. Get this list at least one day ahead of the interview, and google the 
interviewers. That may lead to some good questions for you to ask them. For example, I ve worked with 
several Academy Award winners who won technical awards for their innovations. It is interesting (and 
flattering to the interviewer) to ask them about what they did to win the award. At one company I worked 
for, our entire team of 6 people interviewed each candidate in a small conference room. All that was 
missing was the bright lights and water dripping in the background! In-person Interview May require 
that you give a presentation or take a programming test, depending on company  Will be definitely asked 
about contents of your  resume May need a demo reel or to show sample code Stick to 2 minute limit for 
a description of what you have done in your previous work Keep the conversation flowing In-person interview 
(continued) I ve put a link to some good suggestions on how to perform on a programming test at the end 
of these slides. Be prepared to talk about your interview, but don t talk for too long (more than approximately 
2 minutes) or hog the conversation. Sample Questions in Interviews  How can you tell if 2 circles overlap? 
 What direction do the wheels of a train go?  Describe a graphics GPU pipeline (over the phone)  What 
is your favorite movie (or game)?  Sample Questions in Interviews These are questions that the interviewer 
from the company is asking you at the interview. These are actual questions I have been asked at interviews. 
 How can you tell if 2 circles overlap? What direction do the wheels of a train go? Describe a graphics 
GPU pipeline. (asked over the phone) What is your favorite movie (or game)? I honestly didn t have a 
good answer for this one because I wasn t expecting it. There are more sample questions at the end of 
this presentation, as well as a link to some of the types of questions you might be asked at a Google 
interview.  Got the Job Offer, Now What? Negotiating Know what you are worth  Take time to consider 
the job offer (at least 30 seconds in silence)  Be Prepared to Walk Away  8. Got the Job Offer, Now 
What? Negotiating When you have the verbal offer but haven t started yet, that is when you are at the 
strongest negotiating point. Stan will be covering this in more detail, so I ll just say a few things. 
Know what you are worth. This is where your research comes in handy from before your job interview. 
Take time to consider the job offer. At a minimum, take 30 seconds in silence. Ask them, Is this your 
best offer? Ask them if you can take 24 hours to respond to the offer. Discuss with family and close 
friends. If you feel the offer is too low, try to convince them to pay you more by reiterating your skills, 
the average salary (from the research that you did), or ask if you can have a salary review after 6 months 
on the job. If the offer is too high, make sure you understand what the job entails and that you can 
perform that job. Be Prepared to Walk Away This is probably not the only job offer you will get. If 
it is not the right job offer, don t feel guilty about walking away. Got the Job Offer, Now What? Before 
You Start the New Job Find out about the job  Review  Languages  Systems  3rd Party Applications 
  Find out about new city if relocating  Before You Start the New Job Find out as much about the job 
as you can before you start. Get familiar with the computer languages and systems they are using, the 
3rd party software they are using, and the general process of how the department works. You can ask the 
manager about this, or ask a future co-worker (who perhaps interviewed you). Be sure to also ask them 
about your new city if you are relocating. 9. The First 3 Months Take care of yourself! Relocation can 
be disturbing, so remember to exercise and eat right. Seek a Mentor And reconnect with former mentors 
who can help you with your transition into a new job. These are people you can go to for advice about 
the new company, or new responsibilities of your job. Keep Work Hours Sane While you may need to put 
in a few extra hours learning your new job, pace yourself and pay attention to the rest of your life, 
too. Maintain balance. The First 3 Months (continued) Set Expectations with Your Boss Don t rely on 
your boss to reach out to you. Take initiative to make the relationship work. Clarify expectations early 
and often. Learn your boss s style: what type of communication does he or she like? What constitutes 
success in your boss s mind? Don t get caught up in firefighting. Take time to understand the players: 
your boss, your team, and your customers (artists). The First 3 Months Set expectations with your boss 
 Make your boss look good  Learn the players (team mates, production staff, etc.)  Understand quotas 
/ work measurements  The First 3 Months (continued) / Set Expectations with Your Boss (continued) Make 
your boss look good, no matter what. Find out what is important to your boss and why, and then go code 
solutions, or negotiate solutions to those problems. Meet or beat your quotas or milestones. Get to know 
the teams (software engineers, production staff, other Technical Directors) who can help you out in a 
crunch. Find out what deadlines are looming, and what is important to the smooth release of the movie, 
game, or software. Find out who has influence with your department. Directors are God. Others to know 
are producers, department leads, and influential or senior artists. Understand how your work is measured 
(milestones or quotas). Companies desire a quick turn-around in your work. Speedy completion of the work 
is money in the bank to them. Don t sacrifice quality, but be efficient and avoid superfluous (and unappreciated) 
work. Find out what the project or shot turn-around time is for senior people, and for the average junior 
person. Aim for a faster turn-around time than the average junior person. Find out how the senior people 
quickly get their work done. (What corners are they cutting? What speeds up their process?) On one of 
my software projects, my co-worker came up with a brilliant and fun idea of automatically coloring some 
ink lines. He was so entranced with the idea that he immediately implemented it before we even spoke 
with the artists. When we spoke with the artists to get the requirements specifications, the artists 
mentioned that they didn t need that tool. They needed something completely different! Doh! The First 
3 Months Set expectations with your boss  Ask to be seated near people who share your work  Ask for 
a mentor  After 3 months: Find a new boss if needed  The First 3 Months (continued) Set Expectations 
with Your Boss (continued) Ask to be seated near people who share your work. These people can help you 
when you have questions, and you can learn by observing them work. Ask for a mentor. Ask to be set up 
with someone who can help you get started. Collect up your questions (to minimize interruptions) and 
ask questions of how to do things at that company and where to go for resources. After 3 months, find 
a new boss if needed. Not every boss is good, and not every boss is good for you. If your boss sucks, 
find a new one. Successful people serve on great teams with great bosses. Know yourself and focus on 
your strengths. 10. Technical Jobs in Animation and Visual Effects The most common technical jobs are: 
Technical Directors (TD) TD s perform technical or artistic work on shows or productions, sometimes within 
or across productions. Software Engineers / Research and Development TD Software or R&#38;D folks write 
code and create technical solutions across a show or across the company. IT / Systems Engineer / Support 
Engineer IT generally works across the company, so it will encompass all work within the facility, both 
business and artistic. Technical Directors (TDs) Provide technical solutions for a particular department. 
This may involve coding plugins in C/C++, writing scripts (PERL, Python), or set up for shots or expediting 
images through the pipeline. For some departments (example: Animation), the job focuses on writing tools 
for that department and non-technical artists. In other departments (example: EFX, Shader), the TD may 
create artwork for the shot in addition to writing tools. Character: Animation / Rigging Creates tools 
for the animators to make the characters perform. Shader Creates the look and feel of the character 
and other objects, for example: realistic-looking skin, or fur under different lighting conditions EFX 
Creates an effect, like explosion, fire, smoke, water. This may be done artistically or programmatically, 
or both (think art-directed water ). Lighting Creates lighting environment for entire show. Lighting 
interacts with everything in the shot. Solutions may be artistic or technical. Technical Jobs in Animation 
&#38; Visual Effects Engineers Software  User Interface (GUI / Artist's Tools)  Pipeline  Release 
or Software Configuration Management  Quality Assurance (QA)  Rendering  Engineers Software User Interface 
(GUI / Artist's Tools) Pipeline Release or Software Configuration Management Quality Assurance (QA) Rendering 
 Technical Director (TD)  No standard job description, dependent on company  Position is a blend of 
tech and art skills How much depends on company and position  Often assigned in a specific department: 
 Animation, EFX, Lighting, etc. May be show specific  Need to have an artistic demo reel  11. Technical 
Director (TD) There is no standard job description for this position. Each company has its own definition. 
In general, the position is a blend of technical and artistic skills. How much of each is defined by 
the company and the position. For example, at a company with artistic and technophobic animators, the 
Animation TD will write user friendly tools for the animators. In a company with technically adept animators, 
the Animation TD may also animate, or write programs that assist the animator, such as automatically 
animating tedious work. TD s are often assigned in a specific department: Animation, EFX, Lighting, etc. 
They be assigned to a specific show, or assigned to a department. In many companies, they have quotas 
(number of shots to complete in a week). They must turn the shots around quickly with the highest quality. 
To do this, they must understand Resource Management (memory, processor time, disk space trade-offs) 
so they can get their shot completed with the quickest turn-around time and highest quality desired. 
TD s (especially artistic TD s) need to have an artistic demo reel. Technical Director Character TD 
/ Rigger  Writes software to rig a character for an animator.  May set up for other departments besides 
animation.  Character TD / Rigger The Rigger writes software to rig a character (create the controls 
for the character) for the animation department. The TD may also set up tools or rigs for other departments. 
These tools control the skeleton, hair, cloth, and deformation (squash and stretch) for a character. 
Technical Director Shader TD  Writes software that encodes the "materials" properties of an object 
(examples: fur, skin, water)  May require the writing of original algorithms to produce a look never 
before seen in film or game  Works with various departments: Lighting, Surfacing / Texture Painting, 
Look Development, or Art departments  Shader TD A Shader TD writes software that encodes the "materials" 
properties of an object (examples: fur, skin, water). Shaders provide the surface color, texture, and 
illumination properties of all the objects in a film or game. These are created using programming techniques 
and digital paint, which occasionally requires original algorithmic work. Shader TD s work with various 
departments such as Lighting, Surfacing / Texture Painting, Look Dev or Art departments. Effects (EFX) 
TD / Artist The Effects TD creates effects programmatically. The EFX TD might program some procedural 
animation, write tools for non-technical artists to create effects, or develop an EFX system (dust, crowds, 
etc.). EFX often include: Smoke, Fire, Water, Particles (Breaking objects). Some areas of coding are: 
particle systems, fields, expressions, MEL scripts, soft bodies, rigid bodies, cloth dynamics &#38; particle 
instancing/flocking. EFX TD s develop EFX systems and use EFX Systems developed by other animators. These 
systems cover water (oceans, waves), dust (tornadoes, dust storms), crowds, feathers, breaking objects, 
fire, smoke, etc. The systems are used to simulate real objects (like an ocean) or to animate something 
too complex or time-consuming for artists to animate over many frames (crowds). EFX TD s need to know 
or be familiar with Houdini, Maya, and other 3rd party applications. Technical Director Lighting TD 
 Works with Shader TD in determining look of film through its lighting  May work with Director or Director 
of Photography  Designs tools for less technical lighters  Lighting TD The Lighting TD works with Shader 
TD in determining look of film through its lighting. The Lighting TD directs the lights onto objects 
within the frame of the movie. (This is opposed to the Shader TD s who specify what happens when light 
hits the objects.) If the shot is for a special effect in a live action movie, then the lighting must 
match lighting that has been filmed. In a purely animated feature, the lighter has more discretion. Lighting 
TD s have a basic knowledge of shader writing, and understand the basic physics of light, including the 
types of lighting (ambient, diffuse, specular), occlusion, and reflection and refraction. The Lighting 
TD may work with Director or Director of Photography. The Lighting TD designs tools for less technical 
lighters as well as lights shots. Technical Director Lighting TD  Knows scripting (general, shake scripting, 
RenderMan, mel, tcl, PERL, and Python) as well as C++/C  Lots of room for growth and exploration in 
this area  Lighting TD (continued) The Lighting TD needs to know scripting (general, shake scripting, 
RenderMan, mel, tcl, PERL, and Python) as well as C++/C. There is lots of room for growth and exploration 
in this area, particularly from a technical standpoint. Lighting often comes at the end of the pipeline 
and at the end of the movie production, so the Lighters do not have a lot of time for lighting. Any tools 
that improve this situation, provide more control and faster lighting, are highly desired. Software 
Engineer  Works in software department, not assigned to a production  Writes tools for a specific department, 
show, or entire company/pipeline (all shows)  Python, C++/C, PERL, Qt, tcl, OpenGL, mel, shell scripting, 
plugins (C++, python)  Linux, OSX, Windows platforms  12. Software Engineer Software engineers work 
in the software department, and are not assigned to a production. They are usually outside of a particular 
production and serve all productions and other departments. They write tools for a specific department, 
show, or for the entire company / or an all-show pipeline. Languages used are Python, C++/C, PERL, Qt, 
tcl, OpenGL, mel, shell scripting, shake scripting, and C++ or python for plugins. They may also program 
in HTML, JavaScript or Flash. Platforms used include Linux, OSX, and Windows. Some example tasks are: 
Writing a frame or movie viewer, to view all rendered frames in real time Writing a camera tracking program 
that allows the artist to easily place objects in a 3D space Writing a program that manages how software 
gets released to all users Software Engineer User Interface (GUI / Artist's Tools)  Writes the front 
end/interface for the tools, works with Software Engineer  Software Engineer + Knowledge of  how artists 
work with tools,  what they need,  what is good interface design   Graphical User Interface (GUI) 
Engineer The GUI Engineer writes the front end user interface for the tools, and works with other Software 
Engineers. For example, the GUI Engineer might write the code that only deals with the users, while another 
engineer deals with the back-end of the software, such as a database interface, or interface with other 
programs. A GUI Engineer has the same basic knowledge as a generalist Software Engineer, plus knowledge 
of: how artists work with tools, what they need, and what is good interface design. In addition to the 
usual software languages, GUI Engineers need to know Qt and tcl. Software Engineer Pipeline  Moves 
the data around through departments  Software Engineer + Knowledge of asset management, databases, parallel 
processes, data throughput  Pipeline Engineer A Pipeline Engineer moves the data (artwork and production 
information) around through departments. At this time, most pipelines are individual to the studio, but 
there are some commercial pipelines available. A Pipeline Engineer has the same basic knowledge as a 
generalist Software Engineer, plus knowledge of: asset management, databases, parallel processes, and 
data throughput. Pipeline Engineers write much of their code in a scripting language such as Python or 
PERL. Software Engineer Release / Software Configuration Management  Manages release of internal software, 
as well as 3rd party software  Maintains developer environment (check in and out of code -svn, clearcase, 
perforce)  Release Engineer / Software Configuration Management The Release Engineer manages release 
of internal software, as well as 3rd party software to all users and rendering systems at the facility. 
The Software Configuration Engineer maintains the developer environment (check in and out of code, using 
for example: svn, clearcase, or perforce). Software Engineer Quality Assurance  Performs quality assurance 
on software releases  Must be as familiar with the use of tools as the artists  Quality Assurance Engineer 
The QA Engineer performs quality assurance on software releases. This position requires imagination (to 
imagine what can go wrong) and determination (break that software!). The QA Engineer must be as familiar 
with the use of tools as the artists, and be able to think like an artist to help shake out bugs. Also, 
the QA Engineer may help with automating testing so that the focus remains on finding problems in newer 
tools, while maintaining the quality of the older tools via automatic testing. Software Engineer Rendering 
 Writes the code that takes the modeling, animation, shading, lighting data (or description) and turns 
it into the image ("renders" the image to the screen) in the fastest time possible  These positions 
span the range from queuing and database systems for rendering, to creating renderers like RenderMan 
and MentalRay, plugins to renderers  Rendering Engineer The Rendering Engineer writes the code that 
takes the modeling, animation, shading, lighting data (or descriptions) and turns it into the image ("renders" 
the image to the screen) in the fastest time possible. These positions span the range from queuing and 
database systems for rendering, to creating renderers like RenderMan and MentalRay, and writing plugins 
for renderers. Software Engineer Rendering  These positions may be specific, like creating a renderer 
for hair or breaking objects  The positions may be math or physics intensive (hair or broken objects 
renderer), or algorithmically intensive (better data structures to speed up a render)  Rendering Engineer 
(continued) These positions may be specific, like creating a renderer for hair or breaking objects. The 
positions may be math or physics intensive (hair or broken objects renderer), or algorithmically intensive 
(better data structures to speed up a render). The Rendering Engineer needs to understand distributed 
systems for distributed rendering. 13. IT / Systems / Support Tech Support Engineer (TSE) Data/Render 
Farm Wrangler System Administrator Network Engineer Hardware Engineer Support Tech Support Engineer 
(TSE) The position may be an Associate or Junior TD in a department, or centralized help desk position. 
The Jr. TD may receive questions that are simple and resolved in 2 minutes (like user typo s, etc.) to 
complex issues spanning many departments and covering major outtages. Data/Render Farm Wrangler This 
position may be filled with interns, or people with a Master s in Computer Science. It is a critical 
job, but might not have high status. Basically, you are babysitting the renders. That might involve notifying 
artists when renders fail, or completed, or may involve trouble-shooting failed renders or render farm 
problems. IT / Systems / Support System Administrator  Like a sys admin job anywhere  Requires extremely 
fast responses, especially during crunch  Large systems with lots of data  System Administrator System 
Administrators take care of the computers at a company. This involves installing and patching software, 
and setting up the basic operating system for the company. In the CG field, especially in film or games, 
the requirements of the job remain the same, with these differences: Requires extremely fast responses, 
especially during crunch.  Requires dealing with large systems with lots of data, similar to the banking 
industry.  Requires knowledge of a variety of platforms, especially Linux.  Maintains a variety of 
software, both artistic and business related.  Requires knowledge of hardware that supports large amounts 
of data.  IT / Systems / Support Network Engineer  Like a network engineer anywhere  Requires extremely 
fast responses, especially during crunch  Deal with both in-house network and world-wide networks (geographically 
dispersed projects)  Network Engineer / Network Administrator Network Engineers / Administrators take 
care of the network at a company. This involves installing and upgrading the network, as well as managing 
security software. In the CG field, especially in film or games, the requirements of the job remain the 
same, with these differences: Requires extremely fast responses, especially during crunch  Deals with 
both in-house network and world-wide networks (geographically dispersed projects)  IT / Systems / Support 
 Hardware Engineer  Delivers and sets up hardware  Job may range in desired tech abilities, and also 
might be included as part of another job (like TSE or SysAdmin)  Can be hard to move out of this, unless 
at chip company (designing hardware at a chip company i.e., NVIDIA)  Hardware Engineer The Hardware 
Engineer delivers and sets up hardware at an Animation or Visual Effects company. At a hardware or chip 
company (like NVIDIA or Intel), the Hardware Engineer will be designing the hardware graphics devices. 
This job will range in desired tech abilities, depending on the company. Tasks might be included as part 
of another job (like TSE or SysAdmin). It can be hard to move up from this job if you are just delivering 
and setting up hardware. A Day in a Tech Job in Animation or Visual Effects Hours: Long (50 hours or 
more), but flexible, often salaried (no overtime) Vacation: 2 weeks standard 14. A Day in a Tech Job 
in Animation or Visual Effects Most TD's start between 7am and 10am, but your hours are likely to be 
flexible. Many people stay late if they don't have a family or if they have family help (spouse, nanny, 
Mom, etc.). Several studios in the US have 50 hour work weeks as the default. Two weeks vacation in the 
US is standard to start. Some companies give PTO (paid time off) which combines your vacation and sick 
time into time off. A Day in a Tech Job in Animation or Visual Effects  Meetings:  TD Team meetings 
 Department meetings (Animation, Layout, etc)  Interdepartmental meetings  Meetings with the Director 
  Collaborate with artists (technical and techno-fearful)  A Day in a Tech Job in Animation or Visual 
Effects (continued) TD's may attend department meetings if assigned to a department (Animation, Layout, 
etc) in addition to their own team meetings (TD's), and meetings specific to their work (hair, cloth, 
inter-departmental meetings). TD s and Engineers collaborate with artists (technical and techno-fearful). 
They discover the requirements of the artist, write the tools for them, then meet again with the artists 
to train them and receive feedback on the tools. It is an iterative process. A Day in a Tech Job in Animation 
or Visual Effects  Collaborate with other engineers to create tools  Tools may be: paint tools, compositing 
programs, new buttons and tools for Maya or other third party package, or tools to create a particular 
effect, like waves  A Day in a Tech Job in Animation or Visual Effects (continued) TD s and Engineers 
collaborate with other engineers to create tools, creating the back-end portion of tools that the users 
don t see. (For example, users don t care where their artwork is stored as long as they get the correct 
artwork when they ask for it.) For example, tools created may be: paint tools,  compositing programs, 
 new buttons  and tools for Maya or other third party package, or tools to create a particular effect, 
like waves.   15. Other CG Jobs There are a lot of other jobs in the Computer Graphics field, such 
as Gaming, Scientific Visualization, Virtual Reality, Photography, Hardware (chip makers, display screens), 
Web, Law, etc. Unfortunately, there isn t time to go into all of the subfields possible. Gaming Gaming 
is actually a larger arena in the CG Field than Film. Games make more money than film, though they are 
not considered sexy by the general public. There are more technical jobs in this arena, and it is helpful 
to be a gamer to get a job at a game company. The main technical conference to attend is the Gamer Developer 
s Conference hosted usually in March in San Jose, CA. The IGDA is the International Game Developers Association: 
http://www.igda.org/ Scientific Visualization Scientific Visualization covers many disciplines of science, 
from neuroscience (mapping brains) to astronomy (mapping the universe). The work may be strictly artistic 
(simulating the travels of the Mars rover) to completely programmatic (modeling ocean waves to simulate 
tsunamis). Virtual Reality Virtual Reality spans many disciplines, such as Scientific Visualization, 
Gaming, and Training. The job may be artistic or computational, or both. Other CG Jobs (continued) Hardware 
(chip makers, display screens) Hardware companies make many devices used in CG, from graphics cards to 
3D Scanner and 3D printers. Web A lot of CG happens on the web, so much that it is easy to sub-specialize 
in the web. There are web graphics, web animation (flash), and a host of other avenues to explore. This 
area has a lower barrier to entry (cost of software and training). Law The justice system is big business, 
and technical folk are needed to help recreate crime scenes and explain testimony to juries. Etc. 16. 
Conclusion In conclusion, today I ve covered: What you Need to Get the Job: Education, Skills, Personality, 
&#38; Process to Get the Job  What it Takes to Succeed: Persistence / Tenacity / Passion / Focus / Dedication 
 Descriptions of some of the Technical Jobs Available There are Resources listed later in the Course 
Notes Remember Success always comes when preparation meets opportunity. -Henry Hartman (and Seneca, 
Roman dramatist, philosopher, &#38; politician ) Now you have the tools to take advantage of the opportunities 
that come your way.  Thank you! Questions? 17. Thank you! Questions? References  Sample Questions 
in Interviews  Tech Resources  Books  Links  Links for Connecting, Learning about Jobs, and Job Hunting 
 Glossary  References I ve provided some links to online technical resources, books, and sample interview 
questions and interviewing tips, as well as a glossary. Sample Questions in Interviews Questions that 
the interviewer from the company is asking you at the interview. How can you tell if 2 circles overlap? 
What direction do the wheels of a train go?  Sample Questions in Interviews Describe a graphics GPU 
pipeline (over the phone) What is the definition of gamma? Sample Questions in Interviews  Define dot 
product for a vector.  Define cross product for a vector.  What are kd-trees? Where and how would I 
use them?  Sample Questions in Interviews Define dot product for a vector. Define cross product for 
a vector. What are kd-trees? Where and how would I use them? Sample Questions in Interviews Compare 
and contrast diffuse versus specular reflection.  Describe a hash table. What are they used for? When 
would you use them?  Sample Questions in Interviews Compare and contrast diffuse versus specular reflection. 
Describe a hash table. What are they used for? When would you use them? Sample Questions in Interviews 
 If you have a linked list that wraps around (tail points back to the head), how can you tell that you 
have traversed the whole list?  Describe a typical programming use for a queue.  Sample Questions in 
Interviews If you have a linked list that wraps around (tail points back to the head), how can you tell 
that you have traversed the whole list? Describe a typical programming use for a queue. Sample Questions 
in Interviews  What is your favorite movie (or game)?  Given a vector A in the x-y plane of length 
|A| and angle theta to the x-axis, give the equations for the x and y components of  A. Sample Questions 
in Interviews What is your favorite movie (or game)? Given a vector A in the x-y plane of length |A| 
and angle theta to the x-axis, give the equations for the x and y components of A. Sample Questions in 
Interviews  Why would they design a man hole to be round?  You have a building with 100 floors and 
two eggs. Throwing the eggs out the window, find the last highest floor you can throw the egg out without 
breaking. If you were to throw an egg out the next floor higher, it would break.  Sample Questions in 
Interviews Why would they design a man hole to be round? You have a building with 100 floors and two 
eggs. Throwing the eggs out the window, find the last highest floor you can throw the egg out without 
breaking. If you were to throw an egg out the next floor higher, it would break. More Sample Interview 
Questions  Google Interview Questions by Jay Painter http://www.drizzle.com/~jpaint/google.html  Interview 
Questions That You Should Ask by Fran Zandonella http://www.franzand.com/S08/my_interview_que  stions.html 
More Sample Interview Questions Google Interview Questions by Jay Painter http://www.drizzle.com/~jpaint/google.html 
Interview Questions That You Should Ask by Fran Zandonella http://www.franzand.com/S08/my_interview_questions.html 
 Tech Resources Dive Into Python http://www.diveintopython.org/ Vector Math http://www.netcomuk.co.uk/~jenolive/homevec.html 
Linked Lists http://cslibrary.stanford.edu/105/ Books How to Get a Job in Computer Animation (Paperback) 
by Ed Harriss ISBN-13: 978-0974323008 Getting a Job in Computer Graphics: Real Advice from Reel People 
by Sean Wagstaff ISBN-13: 978­0782142570 Books How to Get a Job in Computer Animation (Paperback) by 
Ed Harriss ISBN-13: 978-0974323008 Getting a Job in Computer Graphics: Real Advice from Reel People 
by Sean Wagstaff ISBN-13: 978-0782142570 Books Computer Graphics Career Handbook by Ed Ferguson, Laura 
Carey Halas, Catherine Shadden Keith, Stephan R. Keith, Bob Powell http://education.siggraph.org/resources/cgsourc 
e/career/FrontPage/handbook.pdf (from 1991 but still good) The Art and Science of Digital Compositing 
by Ron Brinkman ISBN-13: 978-0121339609 Books (continued) Computer Graphics Career Handbook by Ed Ferguson, 
Laura Carey Halas, Catherine Shadden Keith, Stephan R. Keith, Bob Powell http://education.siggraph.org/resources/cgsource/career/FrontPage/handbook.pdf 
(from 1991 but still good) The Art and Science of Digital Compositing by Ron Brinkman ISBN-13: 978-0121339609 
 Links About Computer Graphics Jobs in Animation http://www.infotechemployment.com/computer-graphics-animation­ 
jobs.htm http://www.3drender.com/jobs/TD.htm http://www.skillset.org/animation/careers/3D_computer/article_4632_1 
.asp Links (continued) Google Interview Questions by Jay Painter http://www.drizzle.com/~jpaint/google.html 
Preparing For a Software Engineering Interview by Niniane Wang http://niniane.org/interview_howto.html 
 Links (continued) Interview Questions That You Should Ask by Fran Zandonella http://www.franzand.com/S08/my_interview_questions.html 
  Links for Connecting, Learning about Jobs, and Job Hunting  Inside CG http://www.insidecg.com/  
 VFX Pro http://VFXpro.com/  Highend 3D http://www.highend3d.com/  SIGGRAPH Jobs http://www.creativeheads.net/ 
  CG Society http://jobs.cgsociety.org/about.php/  Links for Connecting, Learning about Jobs, and Job 
Hunting Inside CG http://www.insidecg.com/ VFX Pro http://VFXpro.com/ Highend 3D http://www.highend3d.com/ 
SIGGRAPH Jobs http://www.creativeheads.net/ CG Society http://jobs.cgsociety.org/about.php/ Learning 
about Jobs from the Companies Digital Domain http://www.digitaldomain.com/ Careers > Schools  Disney 
http://corporate.disney.go.com/careers/students.html  DreamWorks/PDI http://www.dreamworksanimation.com/ 
 Look under Studio > A Day in the Life > Department Technical Directors  ILM https://jobs.lucasfilm.com/welcome.html 
  Laika http://careers.laika.com/school.php  Learning about Jobs from the Companies Digital Domain 
http://www.digitaldomain.com/ Careers -> Schools Disney http://corporate.disney.go.com/careers/students.html 
DreamWorks/PDI http://www.dreamworksanimation.com/ Look under Studio -> A Day in the Life -> Department 
Technical Directors ILM https://jobs.lucasfilm.com/welcome.html Laika http://careers.laika.com/school.php 
Learning about Jobs from the Companies Pixar http://www.pixar.com/companyinfo/jobs/uni newgrads.html 
http://www.pixar.com/companyinfo/jobs/uni internfaq.html Rhythm &#38; Hues http://www.rhythm.com/inside 
randh/opportunities_faq.shtml#faq 7 Sony Pictures Imageworks http://www.imageworks.com/jointheteam/academicresources.ph 
p Contains a list of recommended colleges, schools, and reading resources.  WETA http://www.wetadigital.com/digital/students/advice/ 
 Learning about Jobs from the Companies (continued) Pixar http://www.pixar.com/companyinfo/jobs/uni_newgrads.html 
http://www.pixar.com/companyinfo/jobs/uni_internfaq.html Rhythm &#38; Hues http://www.rhythm.com/inside_randh/opportunities_faq.shtml#faq7 
Sony Pictures Imageworks http://www.imageworks.com/jointheteam/academicresources.php Contains a list 
of recommended colleges, schools, and reading resources. WETA http://www.wetadigital.com/digital/students/advice/ 
 Glossary  EFX / FX Effects  GUI Graphical User Interface  IT Information Technology  QA Quality 
Assurance  R&#38;D Research and Development  TD Technical Director  TSE Technical Support Engineer 
 VR Virtual Reality  Glossary EFX/ FX Effects GUI Graphical User Interface QA Quality Assurance R&#38;D 
Research and Development TD Technical Director TSE Technical Support Engineer VR Virtual Reality Got 
Work! &#38;#169;Copyright 2008 Stan Szymanski Stan Szymanski, SVP Creative Resources Sony Pictures Imageworks 
Stan Szymanski is Senior Vice President, Digital Production Creative Resources, Sony Pictures Imageworks. 
Stan Szymanski's experience covers a wide range of visual effects experience on the creative, managerial 
and administrative sides of the business, including stints as a department manager, visual effects producer, 
production manager, postproduction supervisor and digital artist. Szymanski is responsible for Sony Pictures 
Imageworks strategy for sustainable growth, with particular emphasis on the best location and utilization 
of satellite facilities along with overseeing the cross-divisional integration of computer graphics supervisors, 
digital artists and other talented personnel. Szymanski s experience and knowledge are key to efficiently 
organizing teams with the required skills and artistry to complete these projects on time, within budget. 
Got Work! Stan Szymanski SVP Creative Resources Sony Pictures Imageworks Careers in VFX &#38; Animation 
How Do I Get In? What Type of Company? Generalist vs. Specialist? Ask Youself What Do I Want to Do? 
 What Am I Good At?  Who s Hiring for What? ! Do the Research What Projects are Green Lit?  Who s 
Doing the Work?  Rough Hiring Timeline  Network  Job Postings &#38; Websites  Making the Deal ! What 
Does the Position Pay? ! Who Do I Talk to? ! Do I Use Representation? ! Handling Multiple Offers ! How 
to Work with a Recruiter The Lingo ! Employment Status ! Contracts ! ROP ! Option Years? ! IP ! Exclusivity 
of Service ! Contract Language ! Negotiating Contract Terms ! Signing Bonuses ! Relocation Assistance 
Keeping the Job ! It s a Business ! Balance of Salaries and Revenue ! Match of Skills to Project Needs 
! Primary and Secondary Skills ! Self Promotion ! Don t Be Too Picky ! Talk to the Producers ! Other 
Offers And Another Thing ! Performance Reviews ! Negotiating Salary Increases ! Documents and Eligibility 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401169</section_id>
		<sort_key>370</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: High dynamic range imaging & image-based lighting]]></section_title>
		<section_page_from>11</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098686</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098687</person_id>
				<author_profile_id><![CDATA[81100331006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098688</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401170</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>137</pages>
		<display_no>27</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[High dynamic range imaging & image-based lighting]]></title>
		<page_from>1</page_from>
		<page_to>137</page_to>
		<doi_number>10.1145/1401132.1401170</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401170</url>
		<abstract>
			<par><![CDATA[<p>This class outlines recent advances in high dynamic range imaging (HDRI) - from capture to image-based lighting to display. In a hands-on approach, we show how HDR images and video can be captured, the file formats available to store them, and the algorithms required to prepare them for display on low dynamic range displays. The trade-offs at each step are assessed allowing attendees to make informed choices about data capture techniques, file formats and tone reproduction operators. In addition, the latest developments in image-based lighting will be presented.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098689</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dolby Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098690</person_id>
				<author_profile_id><![CDATA[81100331006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098691</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High Dynamic Range Imaging &#38;Image-based Lighting SIGGRAPH 2008 Half-day Class Los Angeles, CA Greg 
Ward Consultant Dolby Canada www.anyhere.com/gward Erik Reinhard. Lecturer .Department of Computer Science. 
University of Bristol. University of Central Florida www.cs.bris.ac.uk/~reinhard Paul Debevec. Research 
Associate Professor .. Institute for Creative TechnologiesUniversity of Southern California www.debevec.org 
 High Dynamic Range Imaging &#38; Image-based Lighting SIGGRAPH 2008 Half-day Class This class outlines 
recent advances in high dynamic range imaging (HDRI) ­from capture to image-based lighting to display. 
In a hands-on approach, we show how HDR images and video can be captured, the file formatsavailable to 
store them, and the algorithms required to prepare them fordisplay on low dynamic range displays. The 
trade-offs at each step areassessed allowing attendees to make informed choices about data capturetechniques, 
file formats and tone reproduction operators. In addition, the latest developments in image-based lighting 
will be presented. Prerequisites None. This course is intended for students, researchers, and industrialdevelopers 
in digital photography, computer graphics rendering, real-time photoreal graphics, game design and visual 
effects production (esp. rendering and compositing). Syllabus 1. Introduction &#38; Perspective (Ward, 
15 minutes) 2. HDR Image Capture &#38; Representation (Ward, 45 minutes) a. Encoding techniques &#38; 
color b. HDR image &#38; video formats c. Backwards-compatible HDRI compression d. HDR capture refinements 
 e. HDR cameras  3. HDR Tone-mapping &#38; Display (Reinhard, 60 minutes) a. Basic operations b. 
Global vs. Local TMOs c. TMO comparisons d. Forward + Reverse Sigmoid TMO  4. Image-based Lighting 
(Debevec, 60 minutes) a. Practical considerations for taking HDR images b. Shooting light probes: acquiring 
omnidirectional HDRI maps c. Shooting light probes with direct sunlight d. Classical image-based lighting 
 e. Converting light probes to constellations of light sources f. Importance sampling techniques g. 
Useful IBL approximations h. Image-based relighting  5. Questions (All, 30 minutes)  Instructor 
Information Greg Ward is a pioneer in the HDR space, having developed the first widely-used high dynamic 
range image file format in 1986 as part of the RADIANCE lighting simulationsystem. Since then, he has 
developed the LogLuv TIFF HDR image format, the JPEG-HDR format, and authored the application Photosphere, 
an HDR image builder and browsing program. More recently, he has been involved with Dolby Canada's HDR 
displaydevelopments, which employ dual modulators to show colors 30 times as bright and ten times as 
dark as conventional monitors. Working in the computer graphics research community for over 20 years, 
he has developed rendering algorithms, reflectance modelsand measurement systems, tone reproduction operators, 
HDR image processing techniques,and photo printer calibration methods. His past employers include the 
Lawrence Berkeley National Laboratory, EPFL Switzerland, SGI, Shutterfly, Exponent, and BrightSideTechnologies. 
Greg holds a bachelor's in Physics from UC Berkeley and a master's in Computer Science from SF State 
University. He is currently working as a consultant inAlbany, California. Erik Reinhard. is lecturer 
at the University of Bristol and holds a courtesy appointment atthe University of Central Florida. He 
has a B.S. and a TWAIO diploma in computer sciencefrom Delft University of Technology and a Ph.D. in 
computer science from the Universityof Bristol. He was a post-doctoral researcher at the University of 
Utah. He co-authored the first book on High Dynamic Range Imaging (Morgan Kaufmann Publishers, 2005). 
He isfounder and co-editor-in-chief of the journal ACM Transactions on Applied Perception, andguest editor 
of a special issue on Parallel Graphics and Visualisation for the journal ParallelComputing (March 2003), 
and a special issue on High Dynamic Range Imaging in theJournal of Visual Communication and Image Representation. 
He is also co-editor of Practical Parallel Rendering (A K Peters, 2002). His current interests are in 
visualperception and its application to computer graphics problems such as tone reproduction and color 
correction. Paul Debevec. is the associate director of graphics research at USC's Institute for Creative 
Technologies. Debevec's Ph.D. thesis (UC Berkeley, 1996) presented Façade, an image­based modeling and 
rendering system for creating photoreal architectural models fromphotographs. Using Facade he led the 
creation of virtual cinematography of the Berkeleycampus for his 1997 film The Campanile Movie whose 
techniques were used to createvirtual backgrounds in the 1999 film The Matrix. Subsequently, Debevec 
developed techniques for illuminating computer-generated scenes with real-world lighting capturedthrough 
high dynamic range photography, demonstrating new image-based lighting techniques in his films Rendering 
with Natural Light (1998), Fiat Lux (1999), and TheParthenon (2004); he also led the design of HDR Shop, 
the first widely-used high dynamicrange image editing program. At USC ICT, Debevec has led the development 
of a series ofLight Stage devices for capturing and simulating how objects and people reflect light,recently 
used to create realistic digital actors in films such as Spider Man 2 and Superman Returns. He is the 
recipient of ACM SIGGRAPH's first Significant New Researcher Award and a co-author of the 2005 book High 
Dynamic Range Imaging from Morgan Kaufmann. HDR Imaging &#38; Image-based Lighting Greg Ward Dolby Canada 
Erik Reinhard Bristol University Paul Debevec USC Institute for Creative Technologies HDR Imaging History 
 Negative photography always was MDR  Earliest exponent format in Utah Raster Toolkit, and supported 
by Rayshade  little-known -e option (Craig Kolb &#38; Rod Bogart) Radiance incorporated similar RGBE 
format includes image-processing &#38; conversion tools Important Milestones in HDRI Radiance first 
public release in 1987  Debevec &#38; Malik s HDR capture in 1997  Improvement on 1995 method of Mann 
 Debevec s image-based lighting paper 1998  HDR tone-mapping papers of 2002  Durand &#38; Dorsey, Fattal 
et al., Reinhard et al. Seetzen et al. HDR Display Systems 2004 Where HDRI Is Today Widespread use in 
special effects industry Image-based lighting is the standard technique HDR photography is still in its 
infancy Flikr and Photomatix -- someone help these guys! HDR video --what s that?  HDR displays in 
labs and at trade shows  Where HDRI Is Bound to Go Theater and Home Entertainment Like a picture window 
rather than a picture True HDR Photography: film squared  HDR Video --good-bye exposure problems  
Surround-light with your surround­sound?  Course Overview 1. Introduction &#38; Perspective Greg Ward 
2. HDR Image Capture &#38; Representation Greg Ward 3. HDR Tone-mapping &#38; Display Erik Reinhard 4. 
Image-based Lighting Paul Debevec 5. Questions  HDR Image Capture and Representation Greg Ward Dolby 
Canada Dynamic Range sRGB range CCIR-709 Color Space Human visible gamut is much larger than standard 
display s  Saturated blues, greens, and purples are lost in sRGB  Many HDR image formats also cover 
a larger color gamut  A Gamut Is a Volume!  HDR can represent brighter colors  This delays saturation 
near white  Result is larger color gamut  .Comparison of standard LCD display to BrightSide HDR display 
Value Encoding Methods Linear quantization  Gamma function (e.g., CRT curve)  Logarithmic encoding 
 Floating point  Perceptual   Perceptual Encoding 16-bit LogL Mantiuk et al., SIGGRAPH 2004 HDR 
Image &#38; Video Formats Available high dynamic-range formats: Radiance 32-bit RGBE and XYZE pictures 
 TIFF 48-bit integer and 96-bit float formats  SGI 24-bit and 32-bit LogLuv TIFF  ILM OpenEXR format 
 BrightSide JPEG-HDR format  Proposals and extensions: HDR extensions to MPEG from MPI [Mantiuk et 
al. 04, 06]  HDR extensions to JPEG 2000 from UFL [Xu et al. 2005]  HDR texture compression (Lunds 
Univ. &#38; Nokia papers)  Encoding Comparison Chart Encoding Bits / pixel Dynamic Range Quant. Step 
Covers Gamut sRGB 24 1:10^1.6 Variable No Radiance RGBE 32 1:10^76 1% No LogLuv 24 24 1:10^4.8 1.1% Yes 
LogLuv 32 32 1:10^38 0.3% Yes OpenEXR 48 1:10^10.7 0.1% Yes JPEG-HDR 1-7 1:10^9.5 Variable Can  IEEE 
96-bit TIFF Most accurate representation  Support (with compression) in Photoshop CS2  Uncompressed 
files are enormous  32-bit IEEE floats look like random bits 16-bit/sample TIFF (RGB48) Supported by 
Photoshop and TIFFlibrary  16 bits each of log red, green, and blue  5.4 orders of magnitude in < 1% 
steps  LZW lossless compression available  Does not cover visible gamut  Most applications think of 
max. value as white  Radiance RGBE and XYZE Simple format with free source code  8 bits each for 
3 mantissas and 1 exponent  76 orders of magnitude in 1% steps  Run-length encoding (20% avg. compr.) 
 RGBE format does not cover visible gamut  Dynamic range at expense of accuracy  Color quantization 
not perceptually uniform   SGI 24-bit LogLuv TIFF Codec Implemented in Leffler s TIFF library  10-bit 
LogL + 14-bit CIE (u ,v ) lookup  4.8 orders of magnitude in 1.1% steps  Just covers visible gamut 
and range  Amenable to tone-mapping as look-up  Dynamic range is less than we would like  No compression 
  SGI 32-bit LogLuv TIFF Codec  Implemented in Leffler s TIFF library  16-bit LogL + 8 bits each 
for CIE (u ,v )  38 orders of magnitude in 0.3% steps  Run-length encoding (30% avg. compr.)  Allows 
negative luminance value  Amenable to tone-mapping as look-up   ILM OpenEXR Format 16-bit/primary 
floating point (sign-e5-m10)  9.6 orders of magnitude in 0.1% steps  Additional order of magnitude 
near black  Wavelet compression of about 40%  Negative colors and full gamut RGB  Alpha and multichannel 
support  Open Source I/O library released Fall 2002  Slow to read and write  6 bytes per pixel, 2 
for each channel, compressed sign exponent mantissa Several lossless compression options, 2:1 typical 
 Compatible with the half datatype in NVidia's Cg Supported natively on GeForce FX and Quadro FX Available 
at www.openexr.com ILM s OpenEXR (.exr) OpenEXR Accuracy 1:108 dynamic range, covering visible gamut 
Visible error for 48-bit/pixel EXR half encoding Dolby Canada s JPEG-HDR Format Backwards-compatible 
JPEG extension forhigh dynamic range images  Very compact: 1/10th size of other formats  Naïve software 
displays tone-mapped sRGB  Different tone-mappings possible Desaturation can encompass visible gamut 
 Lossy encoding so repeated read/writedegrades  Expensive (three pass) write process   File Size 
&#38; HDR Adoption Compression can match size of JPEG images + 20%  Rationale for lossy HDR:  Lossy 
encodings are all about perception  Lossy HDR supports display to the limits of human vision  Required 
for digital photography &#38; web applications  Mantiuk et al. s MPEG-4 extension (SIGGRAPH 2004)  
Xu et al. s JPEG-2000 extension (CG+A 2005)  Microsoft s HD Photo  File Size &#38; HDR Adoption What 
if HDR format was backwards­compatible? Li et al. companding technique (SIGGRAPH 2005)  JPEG-HDR &#38; 
new MPI technique (SIGGRAPH 2006)  XDepth   HDRI Encoding Conclusions Sufficient still formats to 
meet most needs: Radiance RGBE for legacy systems  TIFF for greatest encoding variety  OpenEXR for 
good accuracy and support  JPEG-HDR for space efficiency  HDR texture formats are being proposed 
 HDR video formats are being demonstrated  HDR Capture Refinements Automatic exposure alignment  Ghost 
removal  Lens flare removal  Implementing HDR in still &#38; video cameras        Lens Flare 
Removal From left image, wemay directlymeasure the lens Point SpreadFunction (PSF)  PSF is a function 
of focal length and aperture, so comprehensivemeasurement is impractical  Estimate PSF from HDR Capture 
  For each hot pixel in input:  Find coldest relative pixel from at each radius  Consider overlapping 
hot pixel contributions   PSF is minimal, monotonically decreasing function measured relative to cold 
pixels  Computed by fitting 3rd degree polynomial over all identified cold pixels:  HDR Panorama Stitching 
  Separate into high &#38; low frequency bands  Blend low frequencies using entire overlap region 
 Splice high frequencies at detected edges  If no edges, then blend high frequencies as well   Assorted 
Pixels Nayar and Mitsunaga, IEEE CVPR 2000 Nayar and Narasimhan, ECCV 2002 Digital Still Camera  Conclusions 
 HDR capture is under active development  HDR video is challenging but has many potential benefits 
 Movie industry is an early adopter  Home entertainment market imminent  To n e R e p ro d u c tio 
n  E rik R e in h a rd University of Bristol reinhard@cs.bris.ac.uk   Tone R e pr oduc tion  M 
a tc h d y n a m ic ra n g e to d is p la y c a p a b ilitie s  P re s e rv e s o m e v is u a l q 
u a lity o f th e s c e n e  B r i g h t n e s s  C o n t r a s t  V i s i b i l i t y  A p p 
e a r a n c e  . . .     Loga r ithm ic s c a ling  S im p le v e rs io n :    H is togr a 
m a djus tm e nt  W a rd e t a l (1 9 9 7 ) u s e th e s h a p e o f th e im a g e s h is to g ra m 
 C o m p u t e h i s t o g r a m  C o m p u t e c u m u l a t e h i s t o g r a m  R e s u l t i 
s a m o n o t o n i c a l l y i n c r e a s i n g f u n c t i o n  R e s h a p e t h i s f u n c t 
i o n t o a v o i d s l o p e s g r e a t e r t h a n 1  R e m a p l u m i n a n c e s a c c o r d 
i n g t o t h i s f u n c t i o n   G loba l ope r a tor s  R e c a p :  E a s y t o i m p l e m 
e n t  F a s t / G P U i m p l e m e n t a t i o n s p o s s i b l e ?   O n e p a s s o v e r t 
h e i m a g e t o c o m p u t e l o g a v e r a g e  O n e p a s s t o c o m p r e s s p i x e l s 
  R e a s o n a b l e a m o u n t s o f c o m p r e s s i o n a n d p l a u s i b l e r e s u l t s 
a r e f r e q u e n t l y o b t a i n e d  U s e f u l f o r m e d i u m t o h i g h d y n a m i c 
r a n g e i m a g e s   Loc a l a da pta tion  T h e re is a c a tc h , th o u g h :  T h e s i 
z e o f t h e l o w - p a s s f i l t e r k e r n e l i s i m p o r t a n t .  T o o l a r g e a n 
d h a l o i n g a r t i f a c t s w i l l o c c u r  T o o s m a l l a n d c o m p r e s s i o n w 
i l l n o t b e b e t t e r t h a n g l o b a l o p e r a t o r s   Low -pa s s filte r k e r ne l 
s iz e  F o r e a c h p i x e l t h e s i z e o f t h e L P F k e r n e l s h o u l d b e s u c h t 
h a t i t d o e s n o t o v e r l a p w i t h s h a r p d i s c o n t i n u i t i e s  B u t w e s 
t i l l w a n t t o a v e r a g e o v e r t h e l a r g e s t s p a t i a l a r e a f o r w h i c h t 
h e a b o v e i s t r u e ( w h i c h m a y b e d i f f e r e n t f o r e a c h p i x e l )  I n p 
r a c t i c e o f t e n a s m a l l k e r n e l s i z e !  H ow to c om pute ?  M u lti-s c a le 
te c h n iq u e s u s in g d iffe re n c e o f G a u s s ia n s a n d s c a le s e le c tio n (R e in 
h a rd 2 0 0 2 , A s h ik h m in 2 0 0 2 )  E d g e -p re s e rv in g s m o o th in g o p e ra to r 
(b ila te ra l filte r, m e a n s h ift a lg o rith m , L C IS )  S u b -b a n d s y s te m s (L i 
2 0 0 5 )     Loc a l O pe ra tor s  S e c o n d a p p ro a c h :  R e p l a c e g l o b a l a 
d a p t a t i o n v a l u e w i t h a p e r - p i x e l l o c a l a d a p t a t i o n v a l u e Loc 
a l ope r a tor s  C h iu e t a l, R a h m a n e t a l, iC A M m o d e l:  i . e . d i v i d e t h 
e i m a g e b y a l o w - p a s s f i l t e r e d v e r s i o n o f i t s e l f D iv is ion-ba s e d 
 T o m in im iz e th e s e a rtifa c ts :  U s e la rg e filte r k e rn e l  D ur a nd a nd D or s 
e y 2 0 0 2  S p lit im a g e in to b a s e - a n d d e ta il la y e rs :  U s e e d g e - p r e s 
e r v i n g s m o o t h i n g o p e r a t o r t o f i l t e r i n l o g s p a c e a n d c a l l r e s 
u l t b a s e l a y e r   D i v i d e i m a g e b y b a s e l a y e r a n d c a l l r e s u l t d 
e t a i l l a y e r    Fa tta l e t a l 2 0 0 2  G ra d ie n t d o m a in c o m p re s s io n : 
 D i f f e r e n t i a t e i m a g e i n l o g s p a c e  A t t e n u a t e l a r g e g r a d i e 
n t s  I n t e g r a t e  E x p o n e n t i a t e      M ode l inv e r s ion  M o re p rin 
c ip le d a p p ro a c h  A p p l y T M O w i t h s c e n e - r e f e r r e d p a r a m e t e r s 
 A p p l y i n v e r s e T M O w i t h d i s p l a y - r e f e r r e d p a r a m e t e r s  C l a 
m p   E a rly T M O s to o k th is a p p ro a c h  C o lo r a p p e a ra n c e m o d e ls d o th 
is to o     M e a n dis pla y lum ina nc e  a s s u m in g lin e a r d a ta .  For w a r d + ba 
c k w a r d s igm oid  A m o u n ts to a p p ly in g a p o w e r fu n c tio n , i.e . g a m m a c o 
rre c tio n  E n d s u p b e in g s im ila r to T u m b lin -R u s h m e ie r's o p e ra to r  H 
e n c e s u ita b le fo r m e d iu m d y n a m ic ra n g e a p p lic a tio n s  Im p lic a tio n s 
fo r to n e re p ro d u c tio n a s w e ll a s c o lo r a p p e a ra n c e m o d e lin g !    S olutions 
?  M a y b e th e re e x is ts a th e o re tic a l re a s o n fo r w h y th e b a c k w a rd tra n 
s fo rm s h o u ld b e a lin e a r s c a lin g , in s te a d o f a n in v e rs e s ig m o id ?  M a 
y b e s ig m o id s a re n o t a g o o d s o lu tio n ?  M a y b e a p o w e r fu n c tio n is th e 
rig h t a n s w e r?  In a n y c a s e , n o n e o f th e s e s o lu tio n s is v e ry s a tis fy in 
g  C onc lus ions  T ra d e -o ffs e x is t b e tw e e n :  A m o u n t o f c o m p r e s s i o n 
  P r e s e n c e o f a r t i f a c t s  C o m p u t a t i o n t i m e  N e e d to re c o n s id 
e r a p p ly in g T M O s in b o th fo rw a rd a n d b a c k w a rd d ire c tio n s A c k now le dgm 
e nts  T h a n k s to :  O g u z A k y u z , E r u m K h a n , T i m o K u n k e l , G r e g W a r 
d , H e l g e S e e t z e n , R o l a n d F l e m i n g , H e i n r i c h B u e l t h o f f , T e d A 
d e l s o n , Y u a n z h e n L i , D a n i L i s c h i n s k i , R a n a a n F a t t a l , K a r o l 
M y s z k o w s k i , G r z e g o r z K r a w c z y k , R a f a l M a n t i u k , C h a r l e s H u g 
h e s , K a d i B o u a t o u c h , R e m i C o z o t , Y o a n n M a r i o n , J o n a t h a n B r o 
u i l l a t  A n d m a n y o t h e r s   Taking HDR Images Paul De be ve c Uni v ersi ty o f So 
ut her n Cal i f orni a Institute for C r eative Technologies Graphics La boratory SIGGRA P H 2008 Class 
Hi gh-Dyna m i c -Range I m agi n g &#38; I m age-Based Lighting Aug u st 200 8 www.d e b e vec.org 
/ gl.ict.usc.edu An Example Canon EOS 1Ds ISO 100, f/8, 1/2 sec RAW image: Greatest dynamic range Linear 
pixel values (inthe RAW file) F5AT8487.CRW High Dynamic Range Imaging ISO 100, f/8, 1/8000 sec High 
Dynamic Range Imaging ISO 100, f/8, 1/1000 sec High Dynamic Range Imaging ISO 100, f/8, 1/125 sec High 
Dynamic Range Imaging ISO 100, f/8, 1/15 sec High Dynamic Range Imaging ISO 100, f/8, ½ sec High Dynamic 
Range Imaging ISO 100, f/8, 4 sec SIGGRAPH 2008 Class: HDRI and Image-Based Lighting Image-Based Lighting 
(Paul Debevec) High Dynamic Range Imaging ISO 100, f/8, 30 sec High Dyna mic Range Imaging f/8, 1/8000th 
sec f/8, 1/1000th sec f/8, 1/125th sec f/8, 1/15th sec f/8, 1/2 sec f/8, 4 sec f/8, 30 sec 5000 cd/m2 
Sekonic Light Meter August 2008 Page 5        Recovering the Response Curve HDRShop www.hdrshop.c 
o m As su min g uni t ra dia n c e A f te r a d justi n g r a di an ce s to for ea ch pix e l obt a 
in a sm o o th cu rv e Pixel valu e 3 2 1 Pixel valu e log ExposureExposure log ExposureExposure 
Mitsunaga-Nay ar CVPR 99 Response Recovery Assumes pol y nomi al fo rm Esti mates unknown expos u re 
ti mes Vi gnetti ng co mpensati on http://www.c s .columbia . edu/CAV E / Grossb erg a n d Nay a r Resp 
on se reco ver y from i m age h i sto g ra ms  Robu st to s c ene moti on Wha t ca n b e Known ab out 
the Rad i ometric Re sponse fro m  Images? ECCV 2002  Lin , Gu, Yamazaki , Shum. Radiometric Calibration 
from a Sing le Image . CVPR 2004 As sumes that edge regions are blends of t w o colors, w i t h all 
in t e rmediat e v a lu es represe n ted equal l y (i n li near space ) Locate s edg e regions in image 
 Es timates r e sponse ba sed o n inter m ediate value distributions Emerging Sensor Technology Spatially-varying 
pixels Fuji Light Pr o b e Images: Hig h Dynamic Range Lighting Environments Funston Eucalyptus Beach 
Grove Grace Gallery Cathedral Uffizi Fr o m the L ight Probe I m age Galle ry : http://ww w.debevec.or 
g/Probes/ i  Mirrored ball, fisheyes, stitched photos, or scanning? Sources of Mirrored Balls  2-inch 
ch rome ball s ~ $20 ea .  McMa ster- C arr S u ppl y Company ww w.mcma ster .com  6-12 inch large 
gazing balls  Baker s Lawn Ornaments www.bakerslawnorn.com  Hollow Spheres, 2in 4 in  Dube J u 
ggling Eq uipment ww w.d u be. c om  FAQ on www.hdrshop.com  on Assembling a Ligh t P r obe from 
two images of a mirrored sphere See HDRShop Tutorial #5, www.hdrshop.com 0.34 59% Reflective Calibrating 
Mirrore d Sphere 0.58 Reflectivity Mirrored ball, fisheyes, stitched photos, or scanning? Fishey e 
Lens Radial Fallof f Sigma 8mm Canon / Nikon f/16 Sig m a 8mm f/8 fish eye le n s Mirrored ball, fisheyes, 
stitched photos, or scanning? Tiled Photographs Nodal Acquisition Rig See also www.kaidan.com  Mirrored 
ball, fisheyes, stitched photos, or scanning? Scanning Panoramic Cameras (Panoscan, Spheron) Pros: 
 very high res (10K x 7K+) Full sphere in one scan n o stitching Good dyna mic range, some are HDR Issue 
s : More ex pe nsiv e Scans take a while SIGGRAPH 2008 Class: HDRI and Image-Based Lighting Image-Based 
Lighting (Paul Debevec) High-Res Probes Gallery http://gl.ict.usc.e d u/ Da t a / H i g hR es P r o 
b e s / New Grace Doge s Pisa Cathedral Palace Piazza Banff Ennis-Brown Glacier House Types of Omnidirectional 
Images Latitude/Longitude Cube Map August 2008 Page 22 Type s of Om ni di rectional Images Mirrored 
Ball Angular Map Mirrored Ball Capturi n g Light Probe s i n the Sun Can we recover the sun? + a 
 + a   Adju st sp here inte n s ity Solve for Sun Scaling Factor + a Diffuse Ball a a = (1.166, 
0.973, 0.701)      Marvin Miller, NewTek IBL in Lightwave 3D 9 "A McI n to sh in the Kitche n" 
by Ma rc Jacq uier (2 004)    D i r e ct H D R Cap t ur e of t h e Sun and Sky Use Sigma 8mm fisheye 
lens and Cano n EO S 1D s to cove r entire sky Use 3. 0 ND f ilt er on lens  back t o cover f u ll 
ran g e of li ght  Stumpfel , Jo nes, Wenger, Tchou, Ha w k in s , and Debevec. Di r e c t i HDR Cap 
t ure of the Sun a n d Sky . T o ap pe ar in Af ri g r ap h 2004 . . Extreme HDR Image Series 1 se c 
1/4 sec 1/30 sec f/4 f/4 f/4 1/30 sec 1/250 sec 1/1000 sec 1/8000 sec f/16 f/16 f/16 f/16      
 SIGGRAPH 2008 Class: HDRI and Image-Based Lighting Image-Based Lighting (Paul Debevec)  August 2008 
Page 42  Assembled Panora ma Identi fied Light So urces  A Median Cut Algorithm fo r Light Sourc 
e Placement Inspired by Heckbert Color Image Quantization forFrame Buffer Display , SIGGRAPH 82 1. 
Add the e n tire light pro b e image to the region list as a single region  2. For ea ch re gion, subdi 
vide along the longest d i men s ion su ch th at it s l i g h t e n ergy is divided evenly  3. If th 
e nu mber of it erat ion s is less th an n , retu rn t o step 2 .   PaulDebevec,SIGGRAPH 2005 Poster 
Media n Cut Algorithm 1 region Media n Cut Algorithm 2 regions Media n Cut Algorithm 4 regions Media 
n Cut Algorithm 16 regions Media n Cut Algorithm 64 regions Media n Cut Algorithm 256 regions Final 
Light Sources 256 lights    Optional Improvement Divide each region to minimize the sum of the variances 
within each region, rather than dividing the energy equally. This can produce improved light clustering, 
but no longer keeps all lights at roughly equal energy. Implemen tation Detai l s at: http://gl.ict.usc.edu/Resear 
ch/M e d ianCut/ Links to im plementati ons: HDR Shop 1.0 p lugin i n Francesc o Banterle's Bant y's 
T oolkit beta 1.1 = > te xt file wi th light param e ters htt p ://www. bant e r l e .co m /francesco 
/down l oad.h tml J e r e my P r on k' s Image Based Lighting Tools for May a and Mental Ray http://www. 
happiestdays.co m / plugins_ibl T ools.php  Pharr and Humphr e ys 20 0 4 Improved infinite area light 
source sampling 64 Random Samples 64 Importance Samples Ambient Occlusion Landis, McGaugh, and Koch; 
Landis et alSIGGRAPH 2002 (a) diffuse env m apping (b) ambi ent oc cl usi o n map (a) * (b) F u ll 
IBL Ambient Occlusion Ambien t Occlu s ion Approx imat ion Ambient Occlusion F u ll IBL Optional 
Improvement Compute multiple ambien t occlusion maps for different regions of illumi nation. For example, 
compute ambient occlusi o n from just t h e sky (upper hemisphere) and fr om just the ground (lower hemisphere). 
 In comp ositing, modulate these occlusion maps by the average col o r of the indirect il lumi nation 
from each regi on. This can s i mulate di fferentl y colored in direct illu mination from the sky (e.g. 
blue) and the grou nd (e.g. green or tan). Divid i ng t h e sphere int o ligh t i n g regi on s yields 
a form of image-bas e d relighting, and can con v erge to the full image-based lighting solut i on. 
Ren e t al, Real-time Soft Shadows in Dynamic Scenes usi n g Spher i cal Harmonic Ex po n e nti a ti 
o n , SIG G R A PH 2006 Sphere Blockers Ren e t al, Real-time Soft Shadows in Dynamic Scenes usi n g 
Spher i cal Harmonic Ex po n e nti a ti o n , SIG G R A PH 2006 Actual Blockers Ren e t al, Real-time 
Soft Shadows in Dynamic Scenes usi n g Spher i cal Harmonic Ex po n e nti a ti o n , SIG G R A PH 2006 
 SIGGRAPH 2008 Class: HDRI and Image-Based Lighting Image-Based Lighting (Paul Debevec)  August 2008 
Page 59 David Burke, Abhijeet Ghosh and Wolfgang Heidrich. Bidirectio na l I m porta n c e Sampling 
fo r Dire ct Illu mi natio n . EGSR200 5. Uses Structu r ed Importance Resamp lin g ( S I R) S ee Also: 
 Tal b ot et al . Importance Resampl i n g f or Gl obal I llu min a t i o n , EG SR 2 005 Lawrence e 
t al, Adapt i v e Numerical Cumulative D i stribution Functions for Efficient Importance Sa mpl i ng 
. EG SR 2 005 . Ghosh a nd Hei d ri ch. Cor r elated Visibility Sam p lin g for Direct Il luminat i 
on . SI G G R AP H 200 5 Ske t ch.  SIGGRAPH 2008 Class: HDRI and Image-Based Lighting Image-Based Lighting 
(Paul Debevec)  August 2008 Page 61 Real-Time IBL with Spherical Ha rmo n ics  Frequency Space Enviro 
nment Map Rendering Ravi Ramamoorthi, Pat Han r ahan, SIGGRAPH 2002 Precompu ted Radiance Tr ansfer 
for Real-Time Rendering in Dynamic, Low-Frequency Lighting Envir o nm ents Peter-Pike Sloan, Jan Kautz, 
John Snyder, SIGGRAPH 2002  SIGGRAPH 2008 Class: HDRI and Image-Based Lighting Image-Based Lighting 
(Paul Debevec)  August 2008 Page 63   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401171</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>3</pages>
		<display_no>28</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[The hopeful future of high dynamic range imaging]]></title>
		<subtitle><![CDATA[<i>invited paper</i>]]></subtitle>
		<page_from>1</page_from>
		<page_to>3</page_to>
		<doi_number>10.1145/1401132.1401171</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401171</url>
		<abstract>
			<par><![CDATA[<p>This paper offers an overview of the challenges and opportunities presented by high dynamic range (HDR) imaging. We examine the length of the imaging pipeline, from creation and storage through image editing and viewing, and discuss how each stage is affected by a move to HDR.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098692</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BrightSide Technologies by Dolby]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering Synthetic Objects into Real Scenes: Bridging Traditional and Image-Based Graphics with Global Illumination and High Dynamic Range Photography. In <i>Proceedings of ACM SIGGRAPH</i> 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., and Malik, J. 1997. Recovering High Dynamic Range Radiance Maps from Photographs. In <i>Proceedings of ACM SIGGRAPH</i> 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566574</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Durand, F., and Dorsey, J. 2002. Fast Bilateral Filtering for the Display of High-Dynamic Range Images. <i>ACM Transactions on Graphics, 21</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566573</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Fattal, R., Lischinski, D., and Werman, M. 2002. Gradient Domain High Dynamic Range Compression. <i>ACM Transactions on Graphics, 21</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Kains, F., Bogart, R., Hess, D., Schneider, P., Anderson, B., 2002. <i>OpenEXR</i>. www.openexr.org/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Khan, E. A., Akyuz, A. O., Reinhard, E. 2006. Ghost Removal in High Dynamic Range Images, <i>IEEE International Conference on Image Processing</i> (accepted for publication)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882270</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kang, S. B., Uyttendaele, M., Winder, S., Szeliski, R. 2003. "High Dynamic Range Video," <i>Proceedings of ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1090154</ref_obj_id>
				<ref_obj_pid>1090122</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Krawczyk, G., Myszkowski, K., Seidel, H. P. 2005. Perceptual Effects in Real-time Tone Mapping, <i>Proc. of Spring Conf. on Computer Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>996342</ref_obj_id>
				<ref_obj_pid>993451</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Lowe D. G. 2004. Distinctive image features from scale-invariant keypoints. <i>International Journal of Computer Vision</i> 60, 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141946</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Mantiuk, R. Efremov, A., Myszkowski, K., Seidel, H-P. 2006. Backward Compatible High Dynamic Range MPEG Video Compression, <i>Proc. of SIGGRAPH '06 (Special issue of ACM Transactions on Graphics)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Meylan, L., Daly, S., S&#252;sstrunk, S. 2006. The Reproduction of Specular Highlights on High Dynamic Range Displays, <i>IS&T/SID 14&#60;&#60;sup&#62;th&#60;/sup&#62; Color Imaging Conf</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Petersen, J. 2004. float_to_exp(3) man page. www.cs.utah.edu/gdc/projects/urt/help/man3/float_to_exp.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1208706</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Ward, G., Pattanaik, S., Debevec, P. 2005. <i>High Dynamic Range Imaging: Acquisition, Display, and Image-based Lighting</i>, Morgan Kaufmann Publishers, San Francisco.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566575</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Stark, M., Shirley, P., Ferwerda, J. 2002. "Photographic Tone Reproduction for Digital Images," <i>ACM Transactions on Graphics</i>, 21, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276426</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Rempel, A., Trentacoste, M., Seetzen, H., Young, D., Heidrich, W., Whitehead, L., Ward, G. 2007. "Ldr2Hdr: On-the-fly Reverse Tone Mapping of Legacy Video and Photographs," <i>Proc. of SIGGRAPH '07 (Special issue of ACM Transactions on Graphics</i>).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015797</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Seetzen, H., Heidrich, W., Stuezlinger, W., Ward, G., Whitehead, L., Trentacoste, M., Ghosh, A., Vorozcovs, A. 2004. "High Dynamic Range Display Systems," <i>Proc. of SIGGRAPH '04 (Special issue of ACM Transactions on Graphics</i>).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Stokes, M., Anderson, M., Chandrasekar, S., and Motta, R. 1996. Standard Default Color Space for the Internet. www.w3.org/Graphics/Color/sRGB.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Tumblin, J. and Rushmeier, H. 1993. Tone Reproduction for Realistic Images, <i>IEEE Computer Graphics and Applications</i>, 13(6).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Ward, G. 2006. A General Approach to Backwards-Compatible Delivery of High Dynamic Range Images and Video, <i>Proceedings of the Fourteenth Color Imaging Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ward, G., and Simmons, M. 2004. JPEG-HDR: A Backwards-Compatible, High Dynamic Range Extension to JPEG, <i>Proceedings of the Thirteenth Color Imaging Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Ward, G. 2003. Fast, robust image registration for compositing high dynamic range photographcs from hand-held exposures, <i>Journal of Graphics Tools</i>, 8(2):17--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Ward Larson, G. 1998. Overcoming Gamut and Dynamic Range Limitations in Digital Images. <i>Proc. of IS&T 6th Color Imaging Conf</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Ward, G. 1994. The RADIANCE Lighting Simulation and Rendering System., In <i>Proc. of ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Ward, G. 1991. Real Pixels. In <i>Graphics Gems II</i>, edited by James Arvo, Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 21.1 / G. Ward 21.1: Invited Paper: The Hopeful Future of High Dynamic Range Imaging Greg Ward BrightSide 
Technologies by Dolby Presented at 2007 SID Conference in Long Beach, CA Abstract This paper offers an 
overview of the challenges and opportunities presented by high dynamic range (HDR) imaging. We examine 
the length of the imaging pipeline, from creation and storage through image editing and viewing, and 
discuss how each stage is affected by a move to HDR. Introduction Since the first release of Photoshop 
in 1990, imaging has been well-grounded in an 8-bit/sample (i.e., 24-bit/pixel RGB) world. This is not 
without reason. An 8-bit integer provides enough levels on a standard display that banding is almost 
negligible using a de facto .=2.2 encoding. Although considered hefty at its introduction, the 24-bit 
RGB standard is a reasonably efficient representation, which permits in-core editing of screen-sized 
displays, using less than 1 MByte for a VGA-resolution image. Years later, 24-bit color was also favored 
during the dissemination of color management systems, since it conveniently serves as an index in a 3-D 
lookup table. As we adopt a wider conduit for imaging, many of the decisions that came before need to 
be re-examined, and some need to be reconsidered. Naturally, there are practical limits to the changes 
we can make. Much of the imaging pipeline is ingrained to the point where major changes would cause a 
technological upheaval whose short-term costs would undermine or even outweigh its long-term benefits. 
The purpose of this paper is to examine some of these technological trade-offs, compare an ideal HDR 
imaging pipeline to a likely one, and consider the need for backward-compatibility. We hope this exposition 
will spur additional ideas and solutions to the problems presented. Image Creation Most modes of digital 
image creation, including paint software, still cameras, video cameras, and low-end rendering and animation 
systems, work in a 24-bit output-referred color space, such as sRGB [Stokes et al. 96]. This is a convenient 
choice for viewing and editing on common video display devices, such as CRTs and LCDs, which have a limited 
dynamic range and color gamut. So long as the color depth resolution of the created imagery meets or 
exceeds the output device, the latter is unlikely to show up deficiencies in the former. However, as 
we graduate to higher bit depths and perceptual range in our display systems, digital cinema and high-end 
home theater will expose inadequate source materials for what they are. The special effects industry 
was the first to recognize that greater bit depths were needed before computer graphics (CG) would blend 
seamlessly with live-action film footage. The greater color resolution, gamut, and dynamic range of film 
reveal the shortcomings of 24-bit output-referred encodings, which include the notion that a maximum 
value somehow corresponds to white. The real world presents highlights that are 1,000 to 10,000 times 
brighter than the 18% gray level commonly used as a reference in cinematography, and these same highlights 
must be represented in special effects work to incorporate lens flare and similar cues that something 
is brighter than white. In the absence of HDR, special effects lack the depth and realism of live action. 
An important driver for HDR in this context is the image-based lighting (IBL) technique introduced in 
[Debevec 98]. Using IBL, one may capture an HDR image of the environment reflected in a mirrored sphere, 
and use this to illuminate CG elements so they will blend convincingly with the captured film footage. 
Figure 1 outlines the basic application of this method. Image-based lighting is now a principal practice 
employed in special effects for film, and its reliance on HDR imagery in certain parts of the pipeline 
have led to a general migration towards floating point representation throughout. The gaming industry 
is also pushing HDR applications, especially since the majority of graphics cards now support 16-bit/channel 
floating point natively in their texturing and rendering engines. Figure 1. Image-based lighting [Debevec 
98]: The background plate (a) is multiplied against a shadow and scattering image (b). The CG elements 
(c) are illuminated by an HDR light probe then composited into the final frame (d). Capture Methods While 
rendering and special effects applications have already made the transition to high dynamic range, capture 
technology appears to be lagging behind. The principal methods today for HDR moving and still image capture 
are (respectively) film scanning and multiple, bracketed exposures. For movies, film offers a sufficient 
latitude that careful scanning can yield subtle shadows simultaneous with bright highlights, unlike today 
s digital video cameras. Unfortunately, scanning film reels with the necessary bit depth is an expensive 
and time­ 21.1 / G. Ward consuming operation, thus it is presently limited to big-budget productions. 
Furthermore, film scanning presents many challenges due to the carefully tuned and highly non-linear 
response of film to scene colors and intensities. Merging scanned film imagery with the linear world 
of computer graphics falls somewhere between science and art, and only a few houses have mastered it. 
If an HD­resolution digital video camera with true HDR output were to arrive on the market tomorrow, 
it might well change the entire industry. For still capture, the method of multiple exposures popularized 
in [Debevec &#38; Malik 97] ten years ago has yet to be replaced. Because a sequence of exposures necessarily 
span a substantial time frame, camera/tripod shake and scene movement are perennial problems. In the 
case of camera movement, fairly robust image alignment is available using either the Median Threshold 
Bitmap technique [Ward 03], Scale Invariant Image Feature Transforms [Lowe 04], or image-flow methods 
[Kang et al. 03]. Image-flow methods are also useful for rectifying scene movement. Other ghost removal 
techniques include variance­based segmentation [Reinhard et al. 05], and robust estimators [Khan et al. 
06]. An HDR still camera would eliminate the need for such workarounds, and open up a world of new possibilities 
beyond 24-bit RGB.  Image Transmission The first viable HDR image format was introduced as a little­known 
exponent extension to the Utah Raster Toolkit [Petersen 04]. Independently, the author developed a nearly 
identical 32-bit RGBE format as part of the Radiance rendering system [Ward 91] [Ward 94]. Several years 
later, 24-bit and 32-bit LogLuv extensions were added to the TIFF library, covering the full visible 
color gamut and dynamic range [Ward Larson 98]. More recently, the 48-bit/pixel EXR format was made public 
by Industrial Light and Magic in the form of the excellent OpenEXR C++ library [Kainz et al. 2002]. The 
current trend in HDR image transmission is towards better, customized compression algorithms. For example, 
ILM s EXR image data starts out with 50% more bits/pixel, but ends up taking the same or less space than 
Radiance RGBE thanks to its lossless wavelet compression. (Radiance uses a basic run-length encoding 
scheme.) In the realm of lossy compression algorithms, there is a trend towards backwards-compatible 
formats, such as JPEG-HDR [Ward &#38; Simmons 04] and MPEG-HDR [Mantiuk et al. 06]. These formats have 
the dual advantage of taking up a small fraction of the space of the lossless HDR streams, while being 
displayable using conventional hardware and software. They are thus ideally suited to the internet, video, 
and perhaps a new generation of digital cameras. Backwards-compatible HDR formats are a win-win for manufacturers 
and consumers alike. Easing the transition from 24­bit RGB to full dynamic-range and gamut capture, editing, 
and viewing has the potential to greatly accelerate market penetration. Early adopters would gain immediate 
access to an HDR world, while others would not be inconvenienced, and could even benefit from improved 
tone-mapping in their legacy content. A well­chosen strategy may even simplify the eventual retirement 
of low dynamic-range data years hence [Ward 06]. Image Editing As we noted above in our discussion of 
special effects, HDR offers numerous advantages for combining disparate image sources such as film and 
CG. In particular, the freedom from a hard ceiling on values and the inclusion of the entire visible 
gamut avoids a host of mapping problems. Similarly, HDR offers numerous opportunities for image editing, 
though it presents some challenges as well. The first challenge for image editors is to overcome the 
notion of a maximum value corresponding to white. This concept does not apply in the real world, until 
and unless you print something on paper. Being restricted to a maximum white during editing, even in 
cases where you do not intend to print, is unnecessarily constraining. One approach to this problem is 
to provide a slider to adjust the range of display values, or to edit locally exposed subregions. Until 
desktop systems are equipped with HDR displays, this may be a necessary compromise. A parallel challenge 
is the appropriate on-screen representation of out-of-gamut colors. To some degree, we face this already 
with 24-bit RGB in the disparities between input, display, and hardcopy devices. The usual approach is 
to map colors to the current display, optionally highlighting pixels that will be out-of-gamut upon printing. 
The situation becomes even more interesting when we consider colors outside the visible gamut, which 
most HDR formats can represent. Overall, HDR offers the simplicity of working in a scene-referred linear 
color space, which streamlines color management enormously. In Photoshop today, 32-bit/channel features 
are about where 16­bit/channel features were 10 years ago. Floating-point has been introduced, but most 
editing functions are currently disabled or restricted, and we expect the support to improve gradually 
with time. Cinematic image editing and compositing tools are pushing the envelope more forcefully, and 
there are other consumer-level image tools that are quite powerful in the HDR domain (e.g., Idruna s 
Photogenics HDR, Artizen HDR, and Cinepaint). Our fondest hope is that the current RAW format craze will 
give way to a more productive and long-term investment in HDR. In many ways, RAW is just a poor man s 
HDR, where every camera has its own representation and each software application has to extract the image 
as best it can. Building a scene-referred HDR pipeline is like building an autobahn suddenly there is 
a compelling reason to engineer better systems. In contrast, RAW is like 1000 gravel driveways, ultimately 
leading us nowhere. Image Viewing Ideally, everyone would have a high dynamic range display that would 
permit users to view and work effortlessly with HDR images [Seetzen et al. 2004]. While we re at it, 
let s give our displays four or five spectrally-spaced primaries so they cover the entire visible gamut. 
One and possibly both of these wishes will come true in the next 2-7 years, but in the meantime, we need 
some method to represent colors that are outside the range of our display. Regardless of display advances, 
printing will still require some gamut mapping, at least until someone invents self-luminous paper. Tone-mapping 
is the general technique employed to reduce an image s dynamic range and color gamut to fit on a monitor, 
projection system, or print. It has a long history, extending back to the invention of negative photography. 
Tone-mapping was first 21.1 / G. Ward introduced to computer graphics in 1993 [Tumblin &#38; Rushmeier 
93], and HDR tone-mapping has been explored extensively over the last five years [Reinhard et al. 05]. 
The best HDR tone­mapping algorithms also happen to be the most expensive, adjusting luminance values 
locally in a perceptually optimized way [Durand &#38; Dorsey 02] [Fattal et al. 02] [Reinhard et al. 
02]. By comparison, the older, global tone-mapping operators can be applied in real-time, providing convenient 
image editing and HDR video display [Krawczyk et al. 05]. Eventually, local TMOs may achieve real-time 
rates, but the path is not yet clear. Digital cinema will probably be the first arena where medium-to­high 
dynamic range imagery will be presented. After all, cinematic prints already encompass a wider gamut 
and dynamic range than other media, and moviegoers have come to expect a certain richness. No theater 
owner wants to make an expensive upgrade that amounts to a backwards step in quality, so HDR could even 
be considered a prerequisite for digital cinema. Once accustomed to a more exciting theater experience, 
consumers might start looking for equipment that offers similar dynamic range in their homes. In fact, 
the home theater market is currently moving towards HDR faster than either digital cinema or computer 
display equipment. This may be driven by vendors desires to win a competitive advantage before large, 
flat-screen televisions saturate the market. If such a transition happens too quickly, we may find ourselves 
in the awkward position of having to synthesize all our HDR imagery from LDR content essentially the 
inverse of the tone-mapping problem [Meylan et al. 06] [Rempel et al. 07].  Conclusion It is clear that 
high dynamic range imaging will someday dominate the market. The question is, when? Our best estimate 
is between 2 and 7 years, and many things can influence its advance. It is preferable that HDR be introduced 
well rather than quickly. As engineers and imaging scientists, we are not powerless to affect this process. 
Through careful planning and intelligent standards­making, we can grow our businesses while delivering 
well-timed improvements in professional and consumer equipment and software. The logical path is to introduce 
HDR to the high-end digital cinema market first, simultaneously with independent niche markets such as 
medical imaging, then allow a some years to pass before introducing HDR to the consumer. At that point, 
we will have settled the standards and worked out the kinks, the studios will have plenty of HDR content, 
and the job of consumer education will already be done. References DEBEVEC, P. 1998. Rendering Synthetic 
Objects into Real Scenes: Bridging Traditional and Image-Based Graphics with Global Illumination and 
High Dynamic Range Photography. In Proceedings of ACM SIGGRAPH 1998. DEBEVEC, P., and MALIK, J. 1997. 
Recovering High Dynamic Range Radiance Maps from Photographs. In Proceedings of ACM SIGGRAPH 1997. DURAND, 
F., and DORSEY, J. 2002. Fast Bilateral Filtering for the Display of High-Dynamic Range Images. ACM Transactions 
on Graphics, 21, 3. FATTAL, R., LISCHINSKI, D., and WERMAN, M. 2002. Gradient Domain High Dynamic Range 
Compression. ACM Transactions on Graphics, 21, 3. KAINS, F., BOGART, R., HESS, D., SCHNEIDER, P., ANDERSON, 
B., 2002. OpenEXR. www.openexr.org/. KHAN, E.A., AKYUZ, A.O., REINHARD, E. 2006. Ghost Removal in High 
Dynamic Range Images, IEEE International Conference on Image Processing (accepted for publication) KANG, 
S.B., UYTTENDAELE, M., WINDER, S., SZELISKI, R. 2003. High Dynamic Range Video, Proceedings of ACM SIGGRAPH. 
KRAWCZYK, G., MYSZKOWSKI, K., SEIDEL, H.P. 2005. Perceptual Effects in Real-time Tone Mapping, Proc. 
of Spring Conf. on Computer Graphics. LOWE D. G. 2004. Distinctive image features from scale-invariant 
keypoints. International Journal of Computer Vision 60, 2. MANTIUK, R. EFREMOV, A., MYSZKOWSKI, K., SEIDEL, 
H-P. 2006. Backward Compatible High Dynamic Range MPEG Video Compression, Proc. of SIGGRAPH '06 (Special 
issue of ACM Transactions on Graphics). MEYLAN, L., DALY, S., SÜSSTRUNK, S. 2006. The Reproduction of 
Specular Highlights on High Dynamic Range Displays, IS&#38;T/SID 14th Color Imaging Conf. PETERSEN, J. 
2004. float_to_exp(3) man page. www.cs.utah.edu/gdc/projects/urt/help/man3/float_to_exp.ht ml REINHARD, 
E., WARD, G., PATTANAIK, S., DEBEVEC, P. 2005. High Dynamic Range Imaging: Acquisition, Display, and 
Image­based Lighting, Morgan Kaufmann Publishers, San Francisco. REINHARD, E., STARK, M., SHIRLEY, P., 
FERWERDA, J. 2002. Photographic Tone Reproduction for Digital Images, ACM Transactions on Graphics, 21,3. 
REMPEL, A., TRENTACOSTE, M., SEETZEN, H., YOUNG, D., HEIDRICH, W., WHITEHEAD, L., WARD, G. 2007. Ldr2Hdr: 
On-the-fly Reverse Tone Mapping of Legacy Video and Photographs, Proc. of SIGGRAPH '07 (Special issue 
of ACM Transactions on Graphics). SEETZEN, H., HEIDRICH, W., STUEZLINGER, W., WARD, G., WHITEHEAD, L., 
TRENTACOSTE, M., GHOSH, A., VOROZCOVS, A. 2004. High Dynamic Range Display Systems, Proc. of SIGGRAPH 
04 (Special issue of ACM Transactions on Graphics). STOKES, M., ANDERSON, M., CHANDRASEKAR, S., and MOTTA, 
R. 1996. Standard Default Color Space for the Internet. www.w3.org/Graphics/Color/sRGB. TUMBLIN, J. AND 
RUSHMEIER, H. 1993. Tone Reproduction for Realistic Images, IEEE Computer Graphics and Applications, 
13(6). WARD, G. 2006. A General Approach to Backwards-Compatible Delivery of High Dynamic Range Images 
and Video, Proceedings of the Fourteenth Color Imaging Conference. WARD, G., AND SIMMONS, M. 2004. JPEG-HDR: 
A Backwards-Compatible, High Dynamic Range Extension to JPEG, Proceedings of the Thirteenth Color Imaging 
Conference. WARD, G. 2003. Fast, robust image registration for compositing high dynamic range photographcs 
from hand-held exposures, Journal of Graphics Tools, 8(2):17-30. WARD LARSON, G. 1998. Overcoming Gamut 
and Dynamic Range Limitations in Digital Images. Proc. of IS&#38;T 6th Color Imaging Conf. WARD, G. 1994. 
The RADIANCE Lighting Simulation and Rendering System. , In Proc. of ACM SIGGRAPH. WARD, G. 1991. Real 
Pixels. In Graphics Gems II, edited by James Arvo, Academic Press.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401172</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<pages>7</pages>
		<display_no>29</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A general approach to backwards-compatible delivery of high dynamic range images and video]]></title>
		<page_from>1</page_from>
		<page_to>7</page_to>
		<doi_number>10.1145/1401132.1401172</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401172</url>
		<abstract>
			<par><![CDATA[<p>We propose a general solution to the problem of decoding high dynamic range (HDR) information stored as a supplement to low dynamic range (LDR) images or video. Each LDR frame is paired with a lower resolution HDR version, and these are compressed separately using any of the existing methods appropriate to the task. On decode, the low-resolution HDR image is upsampled to match the resolution of the LDR version, and the high frequencies are transferred from the LDR to the HDR frame. The recovery process places no constraints on the color space or tone-mapping of the backwards-compatible LDR content, and is thus ideally suited to applications such as DVD movies that target legacy equipment while building in forward-compatibility with emerging HDR systems. A fast and simple recovery algorithm is demonstrated, followed by a more sophisticated and accurate technique. Examples are shown on computer-generated video frames as well as HDR captured video.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.1</cat_node>
				<descriptor>Information theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>E.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003712.10003713</concept_id>
				<concept_desc>CCS->Mathematics of computing->Information theory->Coding theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003712</concept_id>
				<concept_desc>CCS->Mathematics of computing->Information theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002978.10002979.10002985</concept_id>
				<concept_desc>CCS->Security and privacy->Cryptography->Mathematical foundations of cryptography</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003712</concept_id>
				<concept_desc>CCS->Mathematics of computing->Information theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098693</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BrightSide Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566574</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Durand &amp; Dorsey 2002} Fredo Durand &amp; Julie Dorsey, "Fast Bilateral Filtering for the Display of High-Dynamic Range Images," <i>ACM Transactions on Graphics</i>, 21, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566573</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Fattal et al. 2002} Raanan Fattal, D. Lischinski, M. Werman, "Gradient Domain High Dynamic Range Compression," <i>ACM Transactions on Graphics</i>, 21, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Larson et al. 1997} Greg Ward Larson, Holly Rushmeier, Christine Piatko, "A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes," <i>IEEE Trans. on Visualization and Computer Graphics</i>, 3, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073271</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Li et al. 2005} Yuanzhen Li, Lavanya Sharan, Edward Adelson, "Compressing and Companding High Dynamic Range Images with Subband Architectures," <i>Proc. of SIGGRAPH '05 (Special issue of ACM Transactions on Graphics)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015794</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Mantiuk et al. 2004} Rafal Mantiuk, Grzegorz Krawczyk, Karol Myszkowski, Hans-Peter Seidel, "Perception-motivated High Dynamic Range Video Encoding," <i>Proc. of SIGGRAPH '04 (Special issue of ACM Transactions on Graphics)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141946</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Mantiuk et al. 2006} Rafal Mantiuk, Alexander Efremov, Karol Myszkowski, Hans-Peter Seidel, "Backward Compatible High Dynamic Range MPEG Video Compression," <i>Proc. of SIGGRAPH '06 (Special issue of ACM Transactions on Graphics)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566575</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Reinhard et al. 2002} Erik Reinhard, M. Stark, P. Shirley, J. Ferwerda, "Photographic Tone Reproduction for Digital Images," <i>ACM Transactions on Graphics</i>, 21, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1208706</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Reinhard et al. 2005} Erik Reinhard, Greg Ward, Sumanta Pattanaik, Paul Debevec, <i>High Dynamic Range Imaging: Acquisition, Display, and Image-based Lighting</i>, Morgan Kaufmann Publishers, San Francisco.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Spaulding et al. 2003} Kevin Spaulding, G. J. Woolfe and R. L. Joshi, "Extending the Color Gamut and Dynamic Range of an sRGB Image using a Residual Image," <i>Color Res. Appl.</i> 28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1012566</ref_obj_id>
				<ref_obj_pid>1012551</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Ward &amp; Simmons 2004} Greg Ward &amp; Maryann Simmons, "Subband Encoding of High Dynamic Range Imagery," <i>First Symposium on Applied Perception in Graphics and Visualization</i> (APGV).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Ward &amp; Simmons 2005} Greg Ward &amp; Maryann Simmons, "JPEG-HDR: A Backwards-Compatible, High Dynamic Range Extension to JPEG," <i>Proceedings of the Thirteenth Color Imaging Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1100876</ref_obj_id>
				<ref_obj_pid>1100858</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Xu et al. 2005} Rulfeng Xu, Sumanta Pattanaik, Charles Hughes, "High-dynamic range still-image encoding in JPEG 2000," <i>IEEE Comp. Graph. and Appl.</i> 26, 6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A General Approach to Backwards-Compatible Delivery of High Dynamic Range Images and Video Presented 
at 2006 Color Imaging Conference in Scottsdale, AZ Greg Ward, BrightSide Technologies Abstract We propose 
a general solution to the problem of decoding high dynamic range (HDR) information stored as a supplement 
to low dynamic range (LDR) images or video. Each LDR frame is paired with a lower resolution HDR version, 
and these are compressed separately using any of the existing methods appropriate to the task. On decode, 
the low-resolution HDR image is upsampled to match the resolution of the LDR version, and the high frequencies 
are transferred from the LDR to the HDR frame. The recovery process places no constraints on the color 
space or tone-mapping of the backwards-compatible LDR content, and is thus ideally suited to applications 
such as DVD movies that target legacy equipment while building in forward­compatibility with emerging 
HDR systems. A fast and simple recovery algorithm is demonstrated, followed by a more sophisticated and 
accurate technique. Examples are shown on computer-generated video frames as well as HDR captured video. 
 Background Recent work in high dynamic range image encoding has focused on lossy compression, with a 
particular emphasis on backwards compatibility with existing formats [Spaulding et al. 2003] [Ward &#38; 
Simmons 2004, 2005] [Li et al. 2005] [Mantiuk et al. 2006]. This is an important trend for the practical 
adoption of scene­referred high dynamic range (HDR) digital imagery in a world where lossy, output-referred 
formats such as JPEG and MPEG dominate. The consumer market will not accept HDR formats that take 12 
times as much space as JPEG and 80 times as much space as MPEG, especially if they cannot be displayed 
with standard viewers and players. Such is the case for the most common HDR formats in use today: Radiance 
RGBE (.hdr), OpenEXR, and TIFF [Reinhard et al. 2005]. Though adoption of such lossless formats is taking 
place in high end tools such as Adobe Photoshop and systems such as Mac OS X and (hopefully) the next 
version of Windows, the space requirements of lossless HDR will preclude its use in digital photography, 
video, and web applications for the foreseeable future. Lossy HDR encodings that are not backwards-compatible 
[Mantiuk et al. 2004] [Xu et al. 2005] will eventually make it to the marketplace, but currently offer 
no practical transition path. Methods for backwards-compatible HDR image encoding can be divided into 
two categories: reversible tone-mapping [Li et al. 2005] and supplemental images [Spaulding et al. 2003] 
[Ward &#38; Simmons 2004, 2005] [Mantiuk et al. 2006]. Reversible tone­mapping presents a challenge to 
efficient encoding, because JPEG and MPEG tend to degrade information that is important for proper reconstruction. 
Thus, compression performance is much worse than with supplemental methods. Supplemental methods encode 
additional image data to recover the HDR original from the recorded low dynamic range (LDR) information, 
storing the extra data in an auxiliary stream that is ignored by naïve viewers and players. The key is 
to minimize the size of this auxiliary stream, and existing methods add between 5% and 30% to the LDR 
image size, depending on the method and settings. The problem with supplemental encoding schemes is their 
inherent complexity. Kodak s ERI format [Spaulding et al. 2003] uses a residual image with sophisticated 
color and bit manipulations to minimize the size of the auxiliary stream, achieving good compression 
but only modest gains in dynamic range. The backwards-compatible HDR version of MPEG introduced by Mantiuk 
et al. [2006] follows a similar approach, with a residual image storing the difference between a predictor 
function on the LDR data and an perceptual HDR color space. In both methods, the residual tends to be 
small and noise-like where the LDR image is within its output-referred gamut, but jumps abruptly wherever 
the LDR image saturates at the top end. This challenges standard lossy image compression techniques, 
which must be tailored to encode only the perceptually important information without introducing false 
contours at the gamut boundaries. The different bit sizes between the LDR and HDR data pose additional 
difficulties during encoding and decoding, and care must be taken not to introduce new quantization errors 
in the process. To avoid these complications, the JPEG-HDR encoding of Ward &#38; Simmons [2004, 2005] 
employs a ratio image in place of a residual, which can be multiplied against the decompressed LDR image 
to recover the HDR original. This simplifies the process by allowing a single 8-bit log channel to store 
the ratio between HDR and LDR pixels, but complexities creep back in when the ratio image is downsampled 
to reduce the size of the auxiliary stream. With ratio image downsampling, the LDR image must either 
be precorrected against lost resolution, or postcorrected using a resolution enhancement technique. Also, 
the LDR image must encode all the necessary color information, since the ratio image is only for the 
luminance channel. This places important restrictions on the tone-mapping operator and color space of 
the LDR data. Such restrictions are avoided in the method of Mantiuk et al. [2006], which encodes color 
information as well as HDR luminance in its residual image. The method we propose for backwards-compatible 
encoding of HDR imagery is inspired by previous work and motivated by the following observations: In 
today s applications, the LDRdata is more important than the HDRdata, and should not be compromised. 
 HDR imagery takes longer to decompress because hardware and software are tailored to 8-bit streams. 
 Both of these conditions will change in the next 5 years.  The main reason that no one has implemented 
the obvious solution of storing a full-blown HDR image in an auxiliary stream is that it would more than 
double the data size, incorporating largely redundant information. However, storing a low-resolution 
version of the HDR original has a number of advantages relative to our three observations. First, it 
would not affect the LDR data in any way. Second, it would decompress quickly because most algorithms 
are proportional to the number of pixels using the resolution in each dimension would speed decompression 
by a factor of 16. Third, as more applications come to rely on HDR imagery and the associated hardware 
and software get faster, the resolution of the auxiliary stream can be increased over time, eventually 
reversing the roles and importance of the LDR and HDR data streams. This is the essence of our proposal: 
along with the conventional LDR image stream (still image or video sequence), we store a corresponding 
HDR image stream at a reduced resolution. No special preparation is made on either stream, and no restrictions 
are placed on the color space or tone-mapping of the LDR data. Storing an HDR image with every LDR image 
may seem redundant, but only the low frequency information is repeated, and from this we can derive a 
correlation to better recover the high frequencies; so it is not redundant, but necessary. We retain 
the nicer features of the supplemental encoding methods, with none of the complexity at least none on 
the encoding side. Decoding is another matter, and the subject of this paper. We start by describing 
a basic recovery method that is fast but depends on a global tone-mapping operator, then describe a more 
advanced method that estimates the local tone-mapping response automatically. In our results section, 
we evaluate the performance of our two algorithms on a variety of source images and video, both synthetic 
and captured, and mapped to different color spaces using different tone-mapping operators. We conclude 
with some final observations and suggestions for future work. Figure 1. Tone curve inversion from 8-bit/channel 
original, showing quantization artifacts in bright region. Method Our decoder is given two versions 
of our image, one that is high resolution but low dynamic range, and one that is low resolution but high 
dynamic range. From these, we wish to derive an image that is high resolution and high dynamic range. 
If a global tone­mapping operator were used to generate the LDR image, we could try inverting this mapping 
to arrive at an HDR version, ignoring the low-resolution HDR information provided. However, we would 
run out of color resolution in places where the image values were clipped (out of gamut) or the tone 
curve underwent a large degree of compression. Such a result is shown in Figure 1. The sky region surrounding 
the sun shows quantization artifacts as a result of the expansion of LDR data. Clearly, we need the HDR 
data to supplement our results where the LDR image is inadequate. Basic Method Since we have a low resolution 
version of our HDR image, we can take a different approach. Rather than inverting the tone curve, we 
can take the high frequency data above the quantization threshold from the LDR image to augment the low 
frequency information in the HDR image. Specifically, we: 1. Convert our LDR image color space to approximately 
match the primaries of the HDR data. 2. Extract high frequencies from the LDR image between HDR and 
LDR image resolutions and apply quantization threshold. 3. Upsample the HDR image to LDR resolution 
and apply the high frequencies from Step 2.  The high frequency ratio image can be computed using a 
rational convolution filter, or with equal efficiency via a downsampling­upsampling-divide process: A. 
Downsample the image to lower resolution limit. B. Upsample again to original resolution. C. Divide the 
original image by resampled image from Step B. This results in a scaling image whose pixels have an average 
value of 1.0, which can be multiplied by an upsampled image to recover the missing high frequencies. 
It is critical that the upsampling method not introduce spurious high frequencies, therefore bilinear 
interpolation is preferred over the more usual bicubic basis in Step B. (We employed a separated Gaussian 
kernel with a radius of 0.6 pixels in a square footprint of 5x5 pixels for downsampling.) Figure 2 shows 
a high frequency image before and after a threshold of 1.5 quantization steps is applied. If we can invert 
our tone-mapping curve prior to Step 1 above, our results will be fairly accurate because we only take 
high frequency edge information from the LDR version, avoiding the quantization artifacts that were showing 
up in the smooth gradient regions. Figure 3 shows the comparison results, recovered from a 480x640 pixel 
LDR and a 120x160 HDR image. Unfortunately, we do not always know the tone-mapping that was applied to 
arrive at the LDR image, and even when we do, it may be too difficult or too expensive to invert, leaving 
us with inaccuracies in our high frequency data. Globally, our recovered images will look about right, 
but details may be softened or lost, as shown in Figure 4. In general, tone-mapping operators may be 
globally or locally determined, and may preserve visibility over the entire image or may allow some regions 
to saturate to black or to white. In saturated regions, we must fall back on the low frequency information 
in our HDR image. This is acceptable in most cases, as this information was selected out of the LDR version 
by the content creators. Figure 2. High frequency image before and after quantization threshold is applied. 
 Advanced Method The basic method just described is very similar to the postcorrection technique introduced 
by Ward &#38; Simmons [2004], which is known to be a crude approximation due to its ignorance of the 
tone-mapping function. When the LDR tone­mapping is complex or unknown, we still wish to make the best 
use of high frequencies possible. To accomplish this, we need to approximate the impulse response in 
the LDR data, which is the important part of the tone-mapping for our purposes. Because the tone-mapping 
operator can vary over the image, we need an approximation of the impulse function that is also allowed 
to vary. But how do we derive such an approximation? We need to relate our LDR and HDR data, but we cannot 
compare the frequencies we wish to recover, since they are missing from our HDR image. So, we settle 
for the closest information available the top frequencies in the lower resolution image, assuming the 
impulse response does not change dramatically from one band to the next.1 1 This assumption may be violated 
in tone-mapping operators that incorporate a sharpening stage. In such cases, we can either undo  Figure 
4. Basic method without knowledge of tone-mapping curve, showing loss of detail relative to Figure 3. 
The steps are listed below: 1. Convert our LDR image to match the color space of the HDR image, ISR. 
Call this ISOR. 2. Reduce to the resolution of ISOR to match that of ISR. Call this I SOR. 3. Extract 
high frequencies for grayscale versions of ISR and I SOR using a bandwidth equal to the resolution difference 
between ISR and ISOR. 4. Compute a spatially varying impulse response function between the I SOR upper 
band image and the ISR upper band image. (Estimating the impulse response is explained in the following 
section.) 5. Compute the highest frequencies of full-resolution image ISOR and apply the impulse response 
from Step 4 to recover the HDR high frequencies. 6. Upsample ISR to match the full LDR image resolution. 
 7. Multiply high frequency data from Step 5 to get our full­resolution HDR output.  the sharpening 
filter prior to recovery or accept that our final HDR output will exhibit a similarly sharpened result. 
Applying our more advanced method to the same tone-mapped image as before, we see improved sharpness 
in the unsaturated regions of Figure 5, based on the same input used to generate Figure 4. The additional 
high frequency and mapping calculations increase the processing time by a factor of 4, from 0.45 to 1.8 
seconds on a 1.5 GHz G4 processor, with both calculations proportional to the number of pixels. Figure 
5. Advanced method estimates the high frequency impulse response over the image, obtaining a sharp result 
without assumptions about the tone­mapping operator. Figure 6 shows a comparison between the simple method 
and the advanced method on a 2704x4064 LDR image mapped using Durand &#38; Dorsey s bilateral filter 
[2002], paired with a 676x1016 HDR version. The blue inset shows the red area recovered using the simple 
method without knowledge of the TMO. The green inset shows the same area mapped using the advanced method 
to estimate the TMO. Although neither reconstruction is perfect, the advanced method retains greater 
sharpness, at the expense of some over-shooting, visible as slight discolorations at high contrast edges. 
For comparison, the white inset shows the HDR data upsampled with a bicubic filter on the left, and the 
full­resolution original on the right. (Insets were tone-mapped using a histogram operator [Larson et 
al. 1997].) Estimating the Impulse Response Function In order to correctly map the high frequencies in 
the tone-mapped LDR image into the HDR domain, we need to estimate the local impulse response. We start 
by assuming that the impulse response function is monotonically increasing, at least locally. This is 
a reasonable assumption for any tone-mapping operator, since a decreasing impulse response would imply 
that larger gradients in the original yield smaller gradients in the tone­mapped image, which would show 
up as edges with reversed contrast. Even with this assumption, it is difficult to estimate a continuously 
changing function, so we further assume that the function is constant within small local regions of the 
image. In our implementation, we use overlapping blocks of roughly 64x64 pixels in the subsampled HDR 
input, regardless of the input image resolution. We found this to be a reasonable size to obtain a sampling 
of the impulse response. Figure 6. An image of the Stanford Memorial Church mapped with a bilateral 
filter, and recovered using the simple and advanced methods from a 4x downsampled HDR version. (Image 
courtesy Chris Cox of Adobe Systems.) Within each block, we put the luminance (gray) values from the 
LDR upper band image in one 4096-entry data array corresponding to 64x64 pixels, and the luminance values 
from the HDR upper band image in a second data array. We independently sort the two arrays, then extract 
the input-output pairs corresponding to every 164th entry. By construction, this creates a set of 25 
monotonically increasing coordinates, evenly spaced in the data population, which we can use in a linear 
or cubic interpolation of the impulse response function. The graph in Figure 7 show the original scatter 
of values from an example HDR/LDR mid-frequency block pair in Figure 6, and the sorted interpolation 
points. In regions such as the one chosen here, we may get diverging impulse responses that we combine 
into a single curve, but this is necessary in order to derive a smooth function of one variable.2 Besides 
enforcing monotonicity, independent sorting avoids outliers caused by minor misalignments between the 
LDR and HDR data. Computing a single impulse response function based on luminance further minimizes color 
shifts when we apply it independently to each channel during reconstruction. 2 Better correlations could 
be obtained by adding a second variable to our scheme, the original HDR luminance. This adds a level 
of complexity, but could improve the results for some tone­mapping operators. Figure 7. Mid-frequency 
data from a 64x64 pixel region and the derived impulse response function to be applied to the highest 
frequencies. The blocks we use to derive the local impulse response functions overlap, enabling us to 
smoothly interpolate the results over the image. This scheme is shown diagrammatically in Figure 8. Consider 
the four neighboring 64x64 blocks: A, B, C, &#38; D. Each has its center at the intersection of four 
32x32 pixel regions, and characterizes the response in the surrounding square using the method just described.3 
To map the response of the impulse for pixel P, we pass it through each of the four response functions, 
and linearly interpolate the results based on the position of P. To avoid excessive quantization noise 
in high gradient areas, we place a restriction on the maximum overall gain for the impulse response within 
each block. If the difference between the maximum and minimum output values is greater than 5 times the 
difference between the maximum and minimum input values, the response is scaled to reduce the average 
slope to fit the maximum 1:5 ratio. This limit is rarely reached in practice. Figure 8. Overlapping 
impulse response blocks permit smooth interpolation of output. 3 The 50% overlap we have chosen with 
each neighbor is adjustable. We recommend at least 25% neighbor overlap for smooth results. It is also 
important to consider values outside the range of the interpolated response pairs. We extrapolate the 
last value pair on top and bottom out to one half the distance between the last two points. After that, 
we cap the impulse response function, effectively cutting off high frequencies this far above the observed 
gradient values. This response extension is shown as the faded ends on the curve in Figure 7. Limiting 
extrapolation in this way reduces ringing artifacts caused by sharp edges in the LDR image.  Results 
We tested three variants of our algorithm each on a computer­generated HDR animation and two captured 
HDR video sequences. The first variant of our algorithm assumes a linear mapping between HDR and LDR 
(tone-mapped) versions. The second variant assumes a non-linear, global tone-mapping over the image. 
The third variant is our advanced method, which allows the tone-mapping to change locally and non-linearly. 
Figure 9 shows stills from our lighting simulation of an air traffic control tower, a beach at sunset, 
and a trip through a tunnel. Figure 9. Our three test scenes.: an HD-resolution animatin and two VGA­resolution 
video sequences. Not surprisingly, we found that the linear variant worked well enough on the linear 
tone-mapping operator, but there was considerable detail lost in the brighter regions where the linear 
operator had clipped (Figure 10). The linear assumption was not appropriate for any of the other tone-mapping 
operators, and tended to produce excessive sharpness in the highlights due to over-shooting (Figure 11). 
Also not a surprise, we found that the global, non-linear algorithm variant was acceptable for the global 
versions of the Reinhard [Reinhard et al. 2002] and histogram [Larson et al. 1997] operators, but degenerated 
with the bilateral filter [Durand &#38; Dorsey 2002] and the gradient domain operator [Fattal et al. 
2002], due to their local behavior. On the synthetic control tower animation, the advanced algorithm 
performed acceptably for every tone-mapping operator we tried, though clamping in the linear operator 
still lost high frequency information. In the HDR captured sequences, we noticed a few problems at the 
boundaries of bright objects, which appeared as colorful outlines. Even within some low-gradient fields, 
such as the orange sky of the sunset, patterns would occasionally emerge (Figure 12). These are due to 
the Bayer pattern of the color image sensor, and the best solution is to improve the demosaicing filter. 
Barring that, a comb filter could be applied during HDR recovery to reduce the appearance of Bayer mosaic 
remnants. Conclusion We have proposed a solution for backwards-compatible HDR imagery that stores a 
low-resolution HDR version of each frame as a supplement to the LDR data. This places the burden on the 
decompression engine to recover high-resolution HDR frames by combining the two streams. This approach 
is not necessarily better or faster than previous backwards-compatible solutions. The real benefit to 
our approach is the evolutionary path it offers, especially for video. Any backwards-compatible format 
is a stop-gap solution that requires compromises in encoding efficiency. This is illustrated by the lower 
performance of the backwards-compatible HDR extension to MPEG by Mantiuk et al. [2006] relative to their 
original proposal, which was not backwards-compatible [Mantiuk et al. 2004]. Unfortunately, backwards-compatibility 
is considered essential to market adoption, especially for video. Furthermore, once we settle on a new 
encoding standard, we are committed to it for about 10 years. This seems like a long time to be using 
an ornate, stop-gap format, and in the end we might prefer a simpler solution that provides a smooth 
transition to native HDR video. This is exactly what we offer. By logically separating the HDR and LDR 
data streams, tying the quality of each to its resolution alone, we provide a seamless upgrade path from 
the LDR world of today to the HDR world of tomorrow. We are free to standardize on the most efficient 
HDR encoding we can devise, with no compromises for backwards-compatibility. We can then incorporate 
this standard in new hardware and software, coupling it with a method to extract resolution from the 
legacy LDR stream. As time goes on, hardware and software will continue to improve, enabling real-time 
decoding of higher resolution HDR streams, simultaneously obviating the need for LDR data. Eventually, 
the LDR stream will become subservient to HDR, offering little more than a low-resolution tone-mapping 
suggestion for legacy devices. Color management will then move into display devices, and a high dynamic 
range profile connection space will become the preferred delivery medium. While the basic recovery method 
described can be implemented efficiently on the GPU and works well enough for global tone­mapping operators, 
the advanced method relies on accumulation and sorting operations that are more conveniently carried 
out on the CPU, and is currently too slow for real-time playback. It should be possible to implement 
the advanced method in a more GPU-friendly way, by replacing our derivation of the impulse response function 
or off-loading this efficiently. This is left as future work, along with the reverse problem of recovering 
high frequencies in an LDR image given a high resolution HDR frame. We expect the solution to look very 
similar to the current one, with better results thanks to the greater bit depth of the HDR stream.  
References [Durand &#38; Dorsey 2002] FREDO DURAND &#38; JULIE DORSEY, Fast Bilateral Filtering for the 
Display of High-Dynamic Range Images, ACM Transactions on Graphics, 21, 3. [Fattal et al. 2002] RAANAN 
FATTAL, D. LISCHINSKI, M. WERMAN, Gradient Domain High Dynamic Range Compression, ACM Transactions on 
Graphics, 21, 3. [Larson et al. 1997] GREG WARD LARSON, HOLLY RUSHMEIER, CHRISTINE PIATKO, A Visibility 
Matching Tone Reproduction Operator for High Dynamic Range Scenes, IEEE Trans. on Visualization and Computer 
Graphics, 3, 4. [Li et al. 2005] YUANZHEN LI, LAVANYA SHARAN, EDWARD ADELSON, Compressing and Companding 
High Dynamic Range Images with Subband Architectures, Proc. of SIGGRAPH '05 (Special issue of ACM Transactions 
on Graphics). [Mantiuk et al. 2004] RAFA MANTIUK, GRZEGORZ KRAWCZYK, KAROL MYSZKOWSKI, HANS-PETER SEIDEL, 
Perception-motivated High Dynamic Range Video Encoding, Proc. of SIGGRAPH '04 (Special issue of ACM Transactions 
on Graphics). [Mantiuk et al. 2006] RAFAL MANTIUK, ALEXANDER EFREMOV, KAROL MYSZKOWSKI, HANS-PETER SEIDEL, 
Backward Compatible High Dynamic Range MPEG Video Compression, Proc. of SIGGRAPH '06 (Special issue of 
ACM Transactions on Graphics). [Reinhard et al. 2002] ERIK REINHARD, M. STARK, P. SHIRLEY, J. FERWERDA, 
Photographic Tone Reproduction for Digital Images, ACM Transactions on Graphics, 21,3. [Reinhard et al. 
2005] ERIK REINHARD, GREG WARD, SUMANTA PATTANAIK, PAUL DEBEVEC, High Dynamic Range Imaging: Acquisition, 
Display, and Image-based Lighting, Morgan Kaufmann Publishers, San Francisco. [Spaulding et al. 2003] 
KEVIN SPAULDING, G. J. WOOLFE AND R. L. JOSHI, Extending the Color Gamut and Dynamic Range of an sRGB 
Image using a Residual Image, Color Res. Appl. 28. [Ward &#38; Simmons 2004] GREG WARD &#38; MARYANN 
SIMMONS, Subband Encoding of High Dynamic Range Imagery, First Symposium on Applied Perception in Graphics 
and Visualization (APGV). [Ward &#38; Simmons 2005] GREG WARD &#38; MARYANN SIMMONS, JPEG-HDR: A Backwards-Compatible, 
High Dynamic Range Extension to JPEG, Proceedings of the Thirteenth Color Imaging Conference. [Xu et 
al. 2005] RULFENG XU, SUMANTA PATTANAIK, CHARLES HUGHES, High­dynamic range still-image encoding in JPEG 
2000, IEEE Comp. Graph. and Appl. 26, 6.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401173</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>3</pages>
		<display_no>30</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Defining dynamic range]]></title>
		<page_from>1</page_from>
		<page_to>3</page_to>
		<doi_number>10.1145/1401132.1401173</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401173</url>
		<abstract>
			<par><![CDATA[<p>We introduce a new metric for dynamic range of displays that closely corresponds with human perception in practical settings. The <i>Number of Distinguishable</i> Grays is a count of the visible luminance steps from the deepest black to the brightest white a display produces under known ambient conditions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098694</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dolby Canada, Vancouver, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA["Flat Panel Display Measurements" 2.0 avail. from www.vesa.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA["Flat Panel Display Measurements Update" avail. from www.vesa.org]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tom Mainelli, "Tested contrast ratios rarely conform to vendors' specs," PC World e-zine www.pcworld.com/news/article/0,aid,110483,pg,1,00.asp, May 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA["Part 14: Grayscale Standard Display Function," <i>Digital Imaging and Communications in Medicine (DICOM)</i>, National Electrical Manufacturer's Assoc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237262</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[James Ferwerda, S. Pattanaik, P. Shirley and D. Greenberg. "A Model of Visual Adaptation for Realistic Image Synthesis," <i>Proceedings of ACM SIGGRAPH '96</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Jeffrey Lubin, A. Pica, "A Non-Uniform Quantizer Matched to Human Visual Performance," <i>Soc. for Information Display Digest</i>, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614380</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Greg Ward Larson, H. Rushmeier, C. Piatko, "A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes," <i>IEEE Transactions on Visualization and Computer Graphics</i>, Vol. 3, No. 4, December 1997. radsite.lbl.gov/radiance/papers/lbnl39882/tonemap.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 59.2 / G. Ward 59.2: Defining Dynamic Range Greg Ward Dolby Canada, Vancouver, BC, Canada Presented 
at 2008 SID Conference in Los Angeles, CA Abstract We introduce a new metric for dynamic range of displays 
that closely corresponds with human perception in practical settings. The Number of Distinguishable Grays 
is a count of the visible luminance steps from the deepest black to the brightest white a display produces 
under known ambient conditions. 1 Introduction Progress in high dynamic range displays relies on adequate 
methods for comparing and understanding differences between competing technologies and products. This 
is currently hampered by the lack of a clear definition of dynamic range, usually represented as the 
maximum contrast ratio or CR. The Video Electronics Standards Assoc. (VESA) 2.0 standard [1,2] specifies 
measurement conditions for full-screen white and full-screen black. The ratio of these two values is 
most often used by manufacturers to denote maximum contrast for their displays, despite complaints by 
both consumers and manufacturers that the specification permits excessive tampering with display settings 
prior to such measurements to obtain unrealistic CR numbers [3]. A more critical problem is the precise 
meaning of black in such measurements, which ignores the effect of ambient lighting and the importance 
of visible quantization in determining whether the measured minimum is useful in any practical context. 
Emerging high dynamic range displays may have 10, 12, and even 16-bit/channel inputs, and a metric that 
quantifies the effect of bit depth and gamma response on perception is essential. We address each of 
these issues in our recommended approach, and suggest a new dynamic range metric that incorporates ambient 
lighting and quantization, which we call the Number of Distinguishable Grays (NDG). The question NDG 
answers is, How many gray levels will a standard observer be able to distinguish between the darkest 
black and the brightest white in a given ambient environment? We submit that this is an easily understood 
concept from a buyer s perspective and a meaningful metric from a vendor s standpoint, which defines 
dynamic range more reliably than the contrast ratio. Furthermore, NDG values are perceptually well-spaced 
by design; i.e., a display with twice the NDG value has roughly twice the apparent dynamic range.  2 
NDG Method Determining the Number of Distinguishable Grays for a display requires the following three 
pieces of information, comprised of two measurements and a specified condition: 1. The gray level luminance 
response of the display for identical RGB inputs, Ld(i), including the range of i (e.g., 0-1023 for 10-bit/channel 
input). 2. The average hemispherical reflectance of the display surface. 3. The presumed ambient lighting 
for this NDG evaluation.  If the display is assumed to be in a completely darkened environment, the 
hemispherical reflectance is not needed. However, we strongly recommend that any new standard require 
at least one NDG evaluation under typical indoor lighting conditions, either home lighting (around 50 
lux vertical illuminance) for television units, or office lighting (around 200 lux) for video display 
terminals. This may be given in addition to the best case darkened viewing environment. Even for televisions, 
the worst case for front reflections may be dozens of times greater than the proposed 50 lux level. The 
NDG value may then be computed with the following formula: imax . Ld (i)- Ld (i -1) . NDG =.min. ,1. 
. D(Ld (i)+ L) . i=1 a where La is the reflected display luminance due to ambient lighting, computed 
as the vertical ambient illuminance times the hemispherical front reflectance, divided by p. The D(L) 
function is an agreed-upon threshold versus intensity or t.v.i. curve based on psychophysical data. A 
few such curves have been proposed, such as Barten s formula employed in the DICOM standard [4], Ferwerda 
et al. s perceptually-based tone-mapping [5], and Lubin and Pica s perceptual quantizer [6]. The choice 
of t.v.i. curve is not critical, so long as it is well-specified and adhered to by the standard. The 
min(f,1) function in the NDG formula serves an important purpose, which is to count fractional steps 
for sub-threshold jumps, while never giving more than one NDG per control quanta. In other words, we 
will not give an NDG of 4096 to a 12-bit display unless an observer can see every one of those steps. 
Nor will we give an NDG value of 256 to an 8-bit display if the quanta near white take supra-threshold 
steps while the bottom 20 quanta are lost in unwanted screen reflections. 59.2 / G. Ward Display Screen 
Reflectance Maximum Luminance Ambient Level (incident illuminance) Contrast Ratio Number of Distinguishable 
Grays 17 LCD 8-bit input A 2% 200 cd/m2 0 50 lux 200 400 244 113 240 237 234 17 LCD 8-bit input B 1% 
200 cd/m2 0 50 lux 200 400 303 176 240 238 236 17 LCD 8-bit input C 1% 400 cd/m2 0 50 lux 200 400 345 
244 242 242 241 17 LCD 10-bit input D 1% 400 cd/m2 0 50 lux 200 400 345 244 351 348 338 Table 1. The 
CRs and NDGs of four hypothetical displays under three ambient conditions. In a simple illustration of 
the NDG metric, we compared four hypothetical displays, measured at different ambient lighting levels. 
We assume an sRGB response from each, and a VESA CR of 400. LCD A has a screen reflectance of 2% and 
LCD B has a screen reflectance of 1%. Both A and B have the same maximum luminance of 200 cd/m2. In addition 
to having a lower screen reflectance, LCD C has a boosted maximum luminance of 400 cd/m2, and LCD D provides 
a 10-bit input. (These specifications are chosen for illustrative purposes rather than to match any actual 
display.) Our D(L) curve was adapted from [5], as described in the following subsection. The results 
of our virtual test are summarized in Table 1. As we can see from our table, the NDG values for displays 
A, B, and C are limited primarily by their 8-bit inputs, whose delta between levels is greater than the 
visible threshold over most of the range. Ambient lighting and display reflectance have only a minor 
influence by comparison, and it is only when we move to the 10-bit input of display D that the NDG becomes 
an actual count of visible thresholds over the display luminance range. It is important that the 10-bit 
response follows the sRGB curve. If we had switched to a linear response when we increased the number 
of bits to 10, we would have only have achieved a NDG of 298 at La=0. This is because the linear step 
sizes would exceed the D(La) value at the bottom end of the range. 2.1 Threshold vs. Intensity As noted 
earlier, calculating NDG from display output requires a human threshold vs. intensity curve relating 
threshold response to adaptation luminance. Psychophysical studies have established this non-linear relationship, 
shown for rods and cones in Figure 1 [5]. The piecewise fit to the combined curve is written in Table 
2 [7]. Figure 1. Plot of threshold vs. intensity (t.v.i.) for human vision. log10 of Just Noticeable 
Difference tvi(La) Applicable Adapted Luminance Range -2.86 log10(La) < -3.94 (0.405 log10(La) + 1.6)2.18 
- 2.86 -3.94 = log10(La) < -1.44 log10(La) - 0.395 -1.44 = log10(La) < -0.0184 (0.249 log10(La) + 0.65)2.7 
- 0.72 -0.0184 = log10(La) < 1.9 log10(La) - 1.255 log10(La) = 1.9 Table 2. Numerical fit to t.v.i. 
shown in Figure 1.  59.2 / G. Ward There is general agreement that Ferwerda s thresholds are on the 
high side, and banding may be visible in gradient images at thresholds as low as 1/10th of the reported 
t.v.i. curve (corresponding to 0.6% steps in the photopic region). The probable reason for this discrepancy 
is that the experiments from which this function is derived use a pulsing target on a constant background, 
and the thresholds for such transient stimuli are higher than those for static stimuli. We therefore 
recommend subtracting 0.95 from the formulae on the left side of Table 2, which is equivalent to dividing 
the JND threshold luminances by a factor of 9. This brings our t.v.i. function into better agreement 
with the Barten model recommended for radiology by the Digital Imaging and Communications in Medicine 
(DICOM) standards body, as shown in Figure 2. We did not follow the DICOM recommendation directly because 
their fit to Barten s data only covers part of the visible luminance range. This range, 0.05:4000 cd/m2, 
while greater than any conventional display, is on par with dual modulator displays just entering the 
market, and may be exceeded in the near future. It is therefore preferable to either extend the Barten 
fit to cover the full range of human vision, or adjust Ferwerda s model to match a more realistic threshold. 
We follow the latter strategy, although either approach will work. Figure 2. The DICOM fit to Barten 
s model, and our adjustment of Ferwerda s. We use this modified Ferwerda model for D(L): 10tvi(L )-0.95 
D(L) = Using a threshold model places an upper limit on the acceptable quantization error, and offers 
a staircase scale corresponding to visible luminance differences over a given display s dynamic range 
independent of quantization. This allows us to distinguish between a display that covers a wide dynamic 
range, but in a region where humans can barely see (e.g., below 10-2 cd/m2) versus a display that covers 
a range where we see well.  3 Conclusion Knowledgeable consumers and many manufacturers agree that the 
current practice for measuring maximum contrast ratio, if not the VESA 2.0 standard itself, is suspect 
to the point of being meaningless for comparisons. One solution is to shore up the VESA standard by requiring 
that manufacturers fix their monitor settings to their defaults prior to measurements. Product specifications 
should also include the effects of ambient illumination on delivered contrast, perhaps offering a few 
different levels so the consumer may judge appropriately for their intended application. However, fixing 
the contrast ratio measurement alone does not provide for intelligent evaluation of displays with greater 
bit depth and controllable dynamic range. For emerging high dynamic range displays, we need a sensible 
way to quantify the delivered range of visible luminances. In this paper, we have suggested the Number 
of Distinguishable Grays specification. By measuring the gray output levels of a display for each RGB=(i,i,i) 
input and applying the given formula, one arrives at a quantity with good correspondence to the actual 
visible differences, accounting simultaneously for the effects of brightness, contrast, ambient lighting, 
and quantization. With a modest education effort, we believe the NDG concept is intuitive and will be 
appreciated by consumers and professionals alike. 4 References [1] Flat Panel Display Measurements 2.0 
avail. from www.vesa.org [2] Flat Panel Display Measurements Update avail. from www.vesa.org [3] Tom 
Mainelli, Tested contrast ratios rarely conform to vendors' specs, PC World e-zine www.pcworld.com/news/article/0,aid,110483,pg,1,00.a 
sp, May 2003. [4] Part 14: Grayscale Standard Display Function, Digital Imaging and Communications in 
Medicine (DICOM), National Electrical Manufacturer s Assoc. [5] James Ferwerda, S. Pattanaik, P. Shirley 
and D. Greenberg. "A Model of Visual Adaptation for Realistic Image Synthesis," Proceedings of ACM SIGGRAPH 
'96 [6] Jeffrey Lubin, A. Pica, A Non-Uniform Quantizer Matched to Human Visual Performance, Soc. for 
Information Display Digest, 1991. [7] Greg Ward Larson, H. Rushmeier, C. Piatko, A Visibility Matching 
Tone Reproduction Operator for High Dynamic Range Scenes, IEEE Transactions on Visualization and Computer 
Graphics, Vol. 3, No. 4, December 1997. radsite.lbl.gov/radiance/papers/lbnl39882/tonemap.pdf   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401174</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>10</pages>
		<display_no>31</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Recovering high dynamic range radiance maps from photographs]]></title>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/1401132.1401174</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401174</url>
		<abstract>
			<par><![CDATA[<p>We present a method of recovering high dynamic range radiance maps from photographs taken with conventional imaging equipment. In our method, multiple photographs of the scene are taken with different amounts of exposure. Our algorithm uses these differently exposed photographs to recover the response function of the imaging process, up to factor of scale, using the assumption of reciprocity. With the known response function, the algorithm can fuse the multiple photographs into a single, high dynamic range radiance map whose pixel values are proportional to the true radiance values in the scene. We demonstrate our method on images acquired with both photochemical and digital imaging processes. We discuss how this work is applicable in many areas of computer graphics involving digitized photographs, including image-based modeling, image compositing, and image processing. Lastly, we demonstrate a few applications of having high dynamic range radiance maps, such as synthesizing realistic motion blur and simulating the response of the human visual system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Sensor fusion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010233</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Vision for robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098695</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098696</person_id>
				<author_profile_id><![CDATA[81100342430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jitendra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adams, A. <i>Basic Photo</i>, 1st ed. Morgan &amp; Morgan, Hastings-on-Hudson, New York, 1970.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chen, E. QuickTime VR - an image-based approach to virtual environment navigation. In <i>SIGGRAPH '95</i> (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., Taylor, C. J., and Malik, J. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. In <i>SIGGRAPH '96</i> (August 1996), pp. 11--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Faugeras, O. <i>Three-Dimensional Computer Vision</i>. MIT Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237262</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ferwerda, J. A., Pattanaik, S. N., Shirley, P., and Greenberg, D. P. A model of visual adaptation for realistic image synthesis. In <i>SIGGRAPH '96</i> (1996), pp. 249--258.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gortler, S. J., Grzeszczuk, R., Szeliski, R., and Cohen, M. F. The Lumigraph. In <i>SIGGRAPH '96</i> (1996), pp. 43--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>5603</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Horn, B. K. P. <i>Robot Vision</i>. MIT Press, Cambridge, Mass., 1986, ch. 10, pp. 206--208.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[James, T., Ed. <i>The Theory of the Photographic Process</i>. Macmillan, New York, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kaufman, J. E., Ed. <i>IES Lighting Handbook; the standard lighting guide</i>, 7th ed. Illuminating Engineering Society, New York, 1987, p. 24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218463</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Kolb, C., Mitchell, D., and Hanrahan, P. A realistic camera model for computer graphics. In <i>SIGGRAPH '95</i> (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Laveau, S., and Faugeras, O. 3-D scene representation as a collection of images. In <i>Proceedings of 12th International Conference on Pattern Recognition</i> (1994), vol. 1, pp. 689--691.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., and Hanrahan, P. Light field rendering. In <i>SIGGRAPH '96</i> (1996), pp. 31--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Madden, B. C. Extended intensity range imaging. Tech. rep., GRASP Laboratory, University of Pennsylvania, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Mann, S., and Picard, R. W. Being 'undigital' with digital cameras: Extending dynamic range by combining differently exposed pictures. In <i>Proceedings of IS&T 46th annual conference</i> (May 1995), pp. 422--428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[McMillan, L., and Bishop, G. Plenoptic Modeling: An image-based rendering system. In <i>SIGGRAPH '95</i> (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Schlick, C. Quantization techniques for visualization of high dynamic range pictures. In <i>Fifth Eurographics Workshop on Rendering (Darmstadt, Germany)</i> (June 1994), pp. 7--18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Szeliski, R. Image mosaicing for tele-reality applications. In <i>IEEE Computer Graphics and Applications</i> (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Tani, T. <i>Photographic sensitivity: theory and mechanisms</i>. Oxford University Press, New York, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Theuwissen, A. J. P. <i>Solid-state imaging with charge-coupled devices</i>. Kluwer Academic Publishers, Dordrecht; Boston, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Tumblin, J., and Rushmeier, H. Tone reproduction for realistic images. <i>IEEE Computer Graphics and Applications 13</i>, 6 (1993), 42--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J. Measuring and modeling anisotropic reflection. In <i>SIGGRAPH '92</i> (July 1992), pp. 265--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J. The radiance lighting simulation and rendering system. In <i>SIGGRAPH '94</i> (July 1994), pp. 459--472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J., Rushmeier, H., and Piatko, C. A visibility matching tone reproduction operator for high dynamic range scenes. Tech. Rep. LBNL-39882, Lawrence Berkeley National Laboratory, March 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Copyright &#38;#169;1997 by the Association for Computing Machinery, Inc. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for components of this work owned by others 
than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post 
on servers, or to distribute to lists, requires prior specific permission and/or a fee. Request permissions 
from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. Recovering High Dynamic 
Range Radiance Maps from Photographs Paul E. Debevec Jitendra Malik University of California at Berkeley1 
ABSTRACT We present a method of recovering high dynamic range radiance maps from photographs taken with 
conventional imaging equip­ment. In our method, multiple photographs of the scene are taken with different 
amounts of exposure. Our algorithm uses these dif­ferently exposed photographs to recover the response 
function of the imaging process, up to factor of scale, using the assumption of reci­procity. With the 
known response function, the algorithm can fuse the multiple photographs into a single, high dynamic 
range radiance map whose pixel values are proportional to the true radiance values in the scene. We demonstrate 
our method on images acquired with both photochemical and digital imaging processes. We discuss how this 
work is applicable in many areas of computer graphics involv­ing digitized photographs, including image-based 
modeling, image compositing, and image processing. Lastly, we demonstrate a few applications of having 
high dynamic range radiance maps, such as synthesizing realistic motion blur and simulating the response 
of the human visual system. CR Descriptors: I.2.10 [Arti.cial Intelligence]: Vision and Scene Understanding 
-Intensity, color, photometry and threshold­ing; I.3.7 [Computer Graphics]: Three-Dimensional Graphics 
and Realism -Color, shading, shadowing, and texture; I.4.1 [Image Processing]: Digitization -Scanning; 
I.4.8 [Image Processing]: Scene Analysis -Photometry, Sensor Fusion. 1 Introduction Digitized photographs 
are becoming increasingly important in com­puter graphics. More than ever, scanned images are used as 
texture maps for geometric models, and recent work in image-based mod­eling and rendering uses images 
as the fundamental modeling prim­itive. Furthermore, many of today s graphics applications require computer-generated 
images to mesh seamlessly with real photo­graphic imagery. Properly using photographically acquired imagery 
in these applications can greatly bene.t from an accurate model of the photographic process. When we 
photograph a scene, either with .lm or an elec­tronic imaging array, and digitize the photograph to obtain 
a two­dimensional array of brightness values, these values are rarely 1Computer Science Division, University 
of California at Berkeley, Berkeley, CA 94720-1776. Email: debevec@cs.berkeley.edu, ma­lik@cs.berkeley.edu. 
More information and additional results may be found at: http://www.cs.berkeley.edu/ debevec/Research 
true measurements of relative radiance in the scene. For example, if one pixel has twice the value of 
another, it is unlikely that it observed twice the radiance. Instead, there is usually an unknown, nonlinear 
mapping that determines how radiance in the scene becomes pixel values in the image. This nonlinear mapping 
is hard to know beforehand because it is actually the composition of several nonlinear mappings that 
occur in the photographic process. In a conventional camera (see Fig. 1), the .lm is .rst exposed to 
light to form a latent image. The .lm is then developed to change this latent image into variations in 
trans­parency, or density, on the .lm. The .lm can then be digitized using a .lm scanner, which projects 
light through the .lm onto an elec­tronic light-sensitive array, converting the image to electrical volt­ages. 
These voltages are digitized, and then manipulated before .­nally being written to the storage medium. 
If prints of the .lm are scanned rather than the .lm itself, then the printing process can also introduce 
nonlinear mappings. In the .rst stage of the process, the .lm response to variations in exposure X(which 
is E!t, the product of the irradiance Ethe .lm receives and the exposure time !t) is a non-linear function, 
called the characteristic curve of the .lm. Noteworthy in the typ­ical characteristic curve is the presence 
of a small response with no exposure and saturation at high exposures. The development, scan­ning and 
digitization processes usually introduce their own nonlin­earities which compose to give the aggregate 
nonlinear relationship between the image pixel exposures Xand their values Z. Digital cameras, which 
use charge coupled device (CCD) arrays to image the scene, are prone to the same dif.culties. Although 
the charge collected by a CCD element is proportional to its irradiance, most digital cameras apply a 
nonlinear mapping to the CCD outputs before they are written to the storage medium. This nonlinear map­ping 
is used in various ways to mimic the response characteristics of .lm, anticipate nonlinear responses 
in the display device, and often to convert 12-bit output from the CCD s analog-to-digital convert­ers 
to 8-bit values commonly used to store images. As with .lm, the most signi.cant nonlinearity in the response 
curve is at its sat­uration point, where any pixel with a radiance above a certain level is mapped to 
the same maximum image value. Why is this any problem at all? The most obvious dif.culty, as any amateur 
or professional photographer knows, is that of lim­ited dynamic range one has to choose the range of 
radiance values that are of interest and determine the exposure time suitably. Sunlit scenes, and scenes 
with shiny materials and arti.cial light sources, often have extreme differences in radiance values that 
are impossi­ble to capture without either under-exposing or saturating the .lm. To cover the full dynamic 
range in such a scene, one can take a series of photographs with different exposures. This then poses 
a prob­lem: how can we combine these separate images into a composite radiance map? Here the fact that 
the mapping from scene radiance to pixel values is unknown and nonlinear begins to haunt us. The purpose 
of this paper is to present a simple technique for recover­ing this response function, up to a scale 
factor, using nothing more than a set of photographs taken with varying, known exposure du­rations. With 
this mapping, we then use the pixel values from all available photographs to construct an accurate map 
of the radiance in the scene, up to a factor of scale. This radiance map will cover  Figure 1: Image 
Acquisition Pipeline shows how scene radiance becomes pixel values for both .lm and digital cameras. 
Unknown nonlin­ear mappings can occur during exposure, development, scanning, digitization, and remapping. 
The algorithm in this paper determines the aggregate mapping from scene radiance Lto pixel values Zfrom 
a set of differently exposed images. the entire dynamic range captured by the original photographs. 1.1 
Applications Our technique of deriving imaging response functions and recover­ing high dynamic range 
radiance maps has many possible applica­tions in computer graphics: Image-based modeling and rendering 
Image-based modeling and rendering systems to date (e.g. [11, 15, 2, 3, 12, 6, 17]) make the assumption 
that all the images are taken with the same exposure settings and .lm response functions. How­ever, almost 
any large-scale environment will have some areas that are much brighter than others, making it impossible 
to adequately photograph the scene using a single exposure setting. In indoor scenes with windows, this 
situation often arises within the .eld of view of a single photograph, since the areas visible through 
the win­dows can be far brighter than the areas inside the building. By determining the response functions 
of the imaging device, the method presented here allows one to correctly fuse pixel data from photographs 
taken at different exposure settings. As a result, one can properly photograph outdoor areas with short 
exposures, and in­door areas with longer exposures, without creating inconsistencies in the data set. 
Furthermore, knowing the response functions can be helpful in merging photographs taken with different 
imaging sys­tems, such as video cameras, digital cameras, and .lm cameras with various .lm stocks and 
digitization processes. The area of image-based modeling and rendering is working to­ward recovering 
more advanced re.ection models (up to complete BRDF s) of the surfaces in the scene (e.g. [21]). These 
meth­ods, which involve observing surface radiance in various directions under various lighting conditions, 
require absolute radiance values rather than the nonlinearly mapped pixel values found in conven­tional 
images. Just as important, the recovery of high dynamic range images will allow these methods to obtain 
accurate radiance val­ues from surface specularities and from incident light sources. Such higher radiance 
values usually become clamped in conventional im­ages. Image processing Most image processing operations, 
such as blurring, edge detection, color correction, and image correspondence, expect pixel values to 
be proportional to the scene radiance. Because of nonlinear image response, especially at the point of 
saturation, these operations can produce incorrect results for conventional images. In computer graphics, 
one common image processing operation is the application of synthetic motion blur to images. In our re­sults 
(Section 3), we will show that using true radiance maps pro­duces signi.cantly more realistic motion 
blur effects for high dy­namic range scenes. Image compositing Many applications in computer graphics 
involve compositing im­age data from images obtained by different processes. For exam­ple, a background 
matte might be shot with a still camera, live action might be shot with a different .lm stock or scanning 
pro­cess, and CG elements would be produced by rendering algorithms. When there are signi.cant differences 
in the response curves of these imaging processes, the composite image can be visually un­convincing. 
The technique presented in this paper provides a conve­nient and robust method of determining the overall 
response curve of any imaging process, allowing images from different processes to be used consistently 
as radiance maps. Furthermore, the recovered response curves can be inverted to render the composite 
radiance map as if it had been photographed with any of the original imaging processes, or a different 
imaging process entirely. A research tool One goal of computer graphics is to simulate the image formation 
process in a way that produces results that are consistent with what happens in the real world. Recovering 
radiance maps of real-world scenes should allow more quantitative evaluations of rendering al­gorithms 
to be made in addition to the qualitative scrutiny they tra­ditionally receive. In particular, the method 
should be useful for de­veloping re.ectance and illumination models, and comparing global illumination 
solutions against ground truth data. Rendering high dynamic range scenes on conventional display devices 
is the subject of considerable previous work, including [20, 16, 5, 23]. The work presented in this paper 
will allow such meth­ods to be tested on real radiance maps in addition to synthetically computed radiance 
solutions.  1.2 Background The photochemical processes involved in silver halide photography have been 
the subject of continued innovation and research ever since the invention of the daguerretype in 1839. 
[18] and [8] pro­vide a comprehensive treatment of the theory and mechanisms in­volved. For the newer 
technology of solid-state imaging with charge coupled devices, [19] is an excellent reference. The technical 
and artistic problem of representing the dynamic range of a natural scene on the limited range of .lm 
has concerned photographers from the early days [1] presents one of the best known systems to choose 
shutter speeds, lens apertures, and developing conditions to best co­erce the dynamic range of a scene 
to .t into what is possible on a print. In scienti.c applications of photography, such as in astron­omy, 
the nonlinear .lm response has been addressed by suitable cal­ibration procedures. It is our objective 
instead to develop a simple self-calibrating procedure not requiring calibration charts or photo­metric 
measuring devices. In previous work, [13] used multiple .ux integration times of a CCD array to acquire 
extended dynamic range images. Since direct CCD outputs were available, the work did not need to deal 
with the problem of nonlinear pixel value response. [14] addressed the prob­lem of nonlinear response 
but provide a rather limited method of re­covering the response curve. Speci.cally, a parametric form 
of the response curve is arbitrarily assumed, there is no satisfactory treat­ment of image noise, and 
the recovery process makes only partial use of the available data.  2 The Algorithm This section presents 
our algorithm for recovering the .lm response function, and then presents our method of reconstructing 
the high dynamic range radiance image from the multiple photographs. We describe the algorithm assuming 
a grayscale imaging device. We discuss how to deal with color in Section 2.6. 2.1 Film Response Recovery 
Our algorithm is based on exploiting a physical property of imaging systems, both photochemical and electronic, 
known as reciprocity. Let us consider photographic .lm .rst. The response of a .lm to variations in exposure 
is summarized by the characteristic curve (or Hurter-Drif.eld curve). This is a graph of the optical 
density Dof the processed .lm against the logarithm of the exposure X to which it has been subjected. 
The exposure Xis de.ned as the product of the irradiance Eat the .lm and exposure time, !t,so that its 
units are Jm.2. Key to the very concept of the characteris­tic curve is the assumption that only the 
product E!tis important, that halving Eand doubling !twill not change the resulting optical density D. 
Under extreme conditions (very large or very low !t), the reciprocity assumption can break down, a situation 
described as reciprocity failure. In typical print .lms, reciprocity holds to within 1 3stop1 for exposure 
times of 10 seconds to 1/10,000 of a second.2 In the case of charge coupled arrays, reciprocity holds 
under the as­sumption that each site measures the total number of photons it ab­sorbs during the integration 
time. After the development, scanning and digitization processes, we obtain a digital number Z, which 
is a nonlinear function of the orig­inal exposure Xat the pixel. Let us call this function f, which is 
the composition of the characteristic curve of the .lm as well as all the nonlinearities introduced by 
the later processing steps. Our .rst goal will be to recover this function f. Once we have that, we can 
com­pute the exposure Xat each pixel, as X=f.1(Z).We make the reasonable assumption that the function 
fis monotonically increas­ing, so its inverse f.1is well de.ned. Knowing the exposure Xand the exposure 
time !t, the irradiance Eis recovered as E=XI!t, which we will take to be proportional to the radiance 
Lin the scene.3 Before proceeding further, we should discuss the consequences of the spectral response 
of the sensor. The exposure Xshould be thought of as a function of wavelength X(,), and the abscissa 
on the R characteristic curve should be the integral X(,)R(,)d,where R(,)is the spectral response of 
the sensing element at the pixel lo­cation. Strictly speaking, our use of irradiance, a radiometric quan­tity, 
is not justi.ed. However, the spectral response of the sensor site may not be the photopic luminosity 
function VV, so the photomet­ric term illuminance is not justi.ed either. In what follows, we will use 
the term irradiance, while urging the reader to remember that the 3 11 stop is a photographic term for 
a factor of two; 1stop is thus 2 1 3 2An even larger dynamic range can be covered by using neutral density 
.lters to lessen to amount of light reaching the .lm for a given exposure time. A discussion of the modes 
of reciprocity failure may be found in [18], ch. 4. 3Lis proportional Efor any particular pixel, but 
it is possible for the proportionality factor to be different at different places on the sensor. One 
..2 d formula for this variance, given in [7], is E.L:cos 4a,where a 4f measures the pixel s angle from 
the lens optical axis. However, most mod­ern camera lenses are designed to compensate for this effect, 
and provide a nearly constant mapping between radiance and irradiance at f/8 and smaller apertures. See 
also [10]. quantities we will be dealing with are weighted by the spectral re­sponse at the sensor site. 
For color photography, the color channels may be treated separately. The input to our algorithm is a 
number of digitized photographs taken from the same vantage point with different known exposure durations 
!tj.4 We will assume that the scene is static and that this process is completed quickly enough that 
lighting changes can be safely ignored. It can then be assumed that the .lm irradiance values Eifor each 
pixel iare constant. We will denote pixel values by Zij where iis a spatial index over pixels and jindexes 
over exposure times !tj. We may now write down the .lm reciprocity equation as: Zij=f(Ei!tj) (1) Since 
we assume fis monotonic, it is invertible, and we can rewrite (1) as: f.1 (Zij)=Ei!tj Taking the natural 
logarithm of both sides, we have: lnf.1(Zij)=lnEi+ln!tj To simplify notation, let us de.ne function g=lnf.1.We 
then have the set of equations: g(Zij)=lnEi+ln!tj (2) where iranges over pixels and jranges over exposure 
durations. In this set of equations, the Zijare known, as are the !tj. The un­knowns are the irradiances 
Ei, as well as the function g, although we assume that gis smooth and monotonic. We wish to recover the 
function gand the irradiances Eithat best satisfy the set of equations arising from Equation 2 in a least-squared 
error sense. We note that recovering gonly requires recovering the .nite number of values that g(z)can 
take since the domain of Z, pixel brightness values, is .nite. Letting Zminand Zmaxbe the least and greatest 
pixel values (integers), Nbe the number of pixel locations and Pbe the number of photographs, we formulate 
the problem as one of .nding the (Zmax-Zmin+1)values of g(Z) and the Nvalues of lnEithat minimize the 
following quadratic ob­jective function: NP Zmax.1 XXX 00(z)2 O=[g(Zij)-lnEi-ln!tj]2 +,g i=1j=1 z=Zmin+1 
(3) The .rst term ensures that the solution satis.es the set of equa­tions arising from Equation 2 in 
a least squares sense. The second term is a smoothness term on the sum of squared values of the sec­ond 
derivative of gto ensure that the function gis smooth; in this 00(z discrete setting we use g)=g(z-1)-2g(z)+g(z+1).This 
smoothness term is essential to the formulation in that it provides coupling between the values g(z)in 
the minimization. The scalar ,weights the smoothness term relative to the data .tting term, and should 
be chosen appropriately for the amount of noise expected in the Zijmeasurements. Because it is quadratic 
in the Ei s and g(z) s, minimizing Ois a straightforward linear least squares problem. The overdetermined 
4Most modern SLR cameras have electronically controlled shutters which give extremely accurate and reproducible 
exposure times. We tested our Canon EOS Elan camera by using a Macintosh to make digital audio recordings 
of the shutter. By analyzing these recordings we were able to verify the accuracy of the exposure times 
to within a thousandth of a sec­ond. Conveniently, we determined that the actual exposure times varied 
by 11111 powers of two between stops ( 1 , , , , , ,1,2,4,8,16,32),rather 643216842111 than the rounded 
numbers displayed on the camera readout ( 1 , , ,, 60301581 , 1, 1, 2, 4, 8, 15, 30). Because of problems 
associated with vignetting, 42 varying the aperture is not recommended. system of linear equations is 
robustly solved using the singular value decomposition (SVD) method. An intuitive explanation of the 
pro­cedure may be found in Fig. 2. We need to make three additional points to complete our descrip­tion 
of the algorithm: First, the solution for the g(z)and Eivalues can only be up to a single scale factor 
C. If each log irradiance value lnEiwere re­placed by lnEi+C, and the function greplaced by g+C, the 
sys­tem of equations 2 and also the objective function Owould remain unchanged. To establish a scale 
factor, we introduce the additional 1 constraint g(Zmid)=0,where Zmid = 2(Zmin+Zmax),simply by adding 
this as an equation in the linear system. The meaning of this constraint is that a pixel with value midway 
between Zminand Zmaxwill be assumed to have unit exposure. Second, the solution can be made to have a 
much better .t by an­ticipating the basic shape of the response function. Since g(z)will typically have 
a steep slope near Zminand Zmax, we should ex­pect that g(z)will be less smooth and will .t the data 
more poorly near these extremes. To recognize this, we can introduce a weight­ing function w(z)to emphasize 
the smoothness and .tting terms to­ward the middle of the curve. A sensible choice of wis a simple hat 
function: . z-Z minfor z:1(Zmin+Zmax) w(z)= 12(4) Zmax -zfor z>(Zmin+Zmax) 2 Equation 3 now becomes: 
NP XX O=fw(Zij)[g(Zij)-lnEi -ln!tj]g 2 + i=1j=1 Zmax.1 X 00 ,[w(z)g(z)]2 z=Zmin+1 Finally, we need not 
use every available pixel site in this solu­tion procedure. Given measurements of Npixels in Pphotographs, 
we have to solve for Nvalues of lnEiand (Zmax -Zmin)sam­ples of g. To ensure a suf.ciently overdetermined 
system, we want N(P-1)>(Zmax -Zmin). For the pixel value range (Zmax ­Zmin)=255, P=11photographs, a choice 
of Non the or­der of 50 pixels is more than adequate. Since the size of the sys­tem of linear equations 
arising from Equation 3 is on the order of NxP+Zmax -Zmin, computational complexity considera­tions make 
it impractical to use every pixel location in this algo­rithm. Clearly, the pixel locations should be 
chosen so that they have a reasonably even distribution of pixel values from Zminto Zmax, and so that 
they are spatially well distributed in the image. Further­more, the pixels are best sampled from regions 
of the image with low intensity variance so that radiance can be assumed to be con­stant across the area 
of the pixel, and the effect of optical blur of the imaging system is minimized. So far we have performed 
this task by hand, though it could easily be automated. Note that we have not explicitly enforced the 
constraint that g must be a monotonic function. If desired, this can be done by trans­forming the problem 
to a non-negative least squares problem. We have not found it necessary because, in our experience, the 
smooth­ness penalty term is enough to make the estimated gmonotonic in addition to being smooth. To show 
its simplicity, the MATLAB routine we used to minimize Equation 5 is included in the Appendix. Running 
times are on the order of a few seconds. 2.2 Constructing the High Dynamic Range Radi­ance Map Once 
the response curve gis recovered, it can be used to quickly convert pixel values to relative radiance 
values, assuming the expo­sure !tjis known. Note that the curve can be used to determine ra­diance values 
in any image(s) acquired by the imaging process asso­ciated with g, not just the images used to recover 
the response func­tion. From Equation 2, we obtain: lnEi =g(Zij)-ln!tj (5) For robustness, and to recover 
high dynamic range radiance val­ues, we should use all the available exposures for a particular pixel 
to compute its radiance. For this, we reuse the weighting function in Equation 4 to give higher weight 
to exposures in which the pixel s value is closer to the middle of the response function: P P -ln!tj) 
j=1 w(Zij)(g(Zij) lnEi = P (6) P j=1 w(Zij) Combining the multiple exposures has the effect of reducing 
noise in the recovered radiance values. It also reduces the effects of imaging artifacts such as .lm 
grain. Since the weighting func­tion ignores saturated pixel values, blooming artifacts5 have little 
impact on the reconstructed radiance values. 2.2.1 Storage In our implementation the recovered radiance 
map is computed as an array of single-precision .oating point values. For ef.ciency, the map can be converted 
to the image format used in the RADIANCE [22] simulation and rendering system, which uses just eight 
bits for each of the mantissa and exponent. This format is particularly com­pact for color radiance maps, 
since it stores just one exponent value for all three color values at each pixel. Thus, in this format, 
a high dynamic range radiance map requires just one third more storage than a conventional RGB image. 
  2.3 How many images are necessary? To decide on the number of images needed for the technique, it 
is convenient to consider the two aspects of the process: 1. Recovering the .lm response curve: This 
requires a minimum of two photographs. Whether two photographs are enough can be understood in terms 
of the heuristic explanation of the process of .lm response curve recovery shown in Fig. 2. If the scene 
has suf.ciently many different radiance values, the entire curve can, in principle, be assembled by sliding 
to­gether the sampled curve segments, each with only two sam­ples. Note that the photos must be similar 
enough in their ex­posure amounts that some pixels fall into the working range6 of the .lm in both images; 
otherwise, there is no information to relate the exposures to each other. Obviously, using more than 
two images with differing exposure times improves per­formance with respect to noise sensitivity. 2. 
Recovering a radiance map given the .lm response curve: The number of photographs needed here is a function 
of the dy­namic range of radiance values in the scene. Suppose the range of maximum to minimum radiance 
values that we are  5Blooming occurs when charge or light at highly saturated sites on the imaging surface 
spills over and affects values at neighboring sites. 6The working range of the .lm corresponds to the 
middle section of the response curve. The ends of the curve, in which large changes in exposure cause 
only small changes in density (or pixel value), are called the toe and the shoulder. plot of g(Zij) 
from three pixels observed in five images, assuming unit radiance at each pixel normalized plot of g(Zij) 
after determining pixel exposures 6 6 4 4  log exposure (Ei * (delta t)j) 2 0 -2 -2 -4 -4 -6 -6 pixel 
value (Zij) pixel value (Zij) Figure 2: In the .gure on the left, the xsymbols represent samples of the 
gcurve derived from the digital values at one pixel for 5 different known exposures using Equation 2. 
The unknown log irradiance lnEihas been arbitrarily assumed to be 0. Note that the shape of the gcurve 
is correct, though its position on the vertical scale is arbitrary corresponding to the unknown lnEi.The 
+and .symbols show samples of gcurve segments derived by consideration of two other pixels; again the 
vertical position of each segment is arbitrary. Essentially, what we want to achieve in the optimization 
process is to slide the 3 sampled curve segments up and down (by adjusting their lnEi s) until they line 
up into a single smooth, monotonic curve, as shown in the right .gure. The vertical position of the composite 
curve will remain arbitrary. interested in recovering accurately is R, and the .lm is capa­ ble of representing 
in its working range a dynamic range of F. Then the minimum number of photographs needed is dR eto F 
ensure that every part of the scene is imaged in at least one photograph at an exposure duration that 
puts it in the work­ing range of the .lm response curve. As in recovering the re­sponse curve, using 
more photographs than strictly necessary will result in better noise sensitivity. If one wanted to use 
as few photographs as possible, one might .rst recover the response curve of the imaging process by pho­tographing 
a scene containing a diverse range of radiance values at three or four different exposures, differing 
by perhaps one or two stops. This response curve could be used to determine the working range of the 
imaging process, which for the processes we have seen would be as many as .ve or six stops. For the remainder 
of the shoot, the photographer could decide for any particular scene the number of shots necessary to 
cover its entire dynamic range. For diffuse in­door scenes, only one exposure might be necessary; for 
scenes with high dynamic range, several would be necessary. By recording the exposure amount for each 
shot, the images could then be converted to radiance maps using the pre-computed response curve.  2.4 
Recovering extended dynamic range from sin­gle exposures Most commericially available .lm scanners can 
detect reasonably close to the full range of useful densities present in .lm. However, many of these 
scanners (as well as the Kodak PhotoCD process) pro­duce 8-bit-per-channel images designed to be viewed 
on a screen or printed on paper. Print .lm, however, records a signi.cantly greater dynamic range than 
can be displayed with either of these media. As a result, such scanners deliver only a portion of the 
detected dynamic range of print .lm in a single scan, discarding information in either high or low density 
regions. The portion of the detected dynamic range that is delivered can usually be in.uenced by brightness 
or density adjustment controls. The method presented in this paper enables two methods for re­covering 
the full dynamic range of print .lm which we will brie.y outline7. In the .rst method, the print negative 
is scanned with the scanner set to scan slide .lm. Most scanners will then record the entire detectable 
dynamic range of the .lm in the resulting image. As before, a series of differently exposed images of 
the same scene can be used to recover the response function of the imaging system with each of these 
scanner settings. This response function can then be used to convert individual exposures to radiance 
maps. Unfortu­nately, since the resulting image is still 8-bits-per-channel, this re­sults in increased 
quantization. In the second method, the .lm can be scanned twice with the scanner set to different density 
adjustment settings. A series of dif­ferently exposed images of the same scene can then be used to re­cover 
the response function of the imaging system at each of these density adjustment settings. These two response 
functions can then be used to combine two scans of any single negative using a similar technique as in 
Section 2.2. 2.5 Obtaining Absolute Radiance For many applications, such as image processing and image 
com­positing, the relative radiance values computed by our method are all that are necessary. If needed, 
an approximation to the scaling term necessary to convert to absolute radiance can be derived using the 
ASA of the .lm8 and the shutter speeds and exposure amounts in the photographs. With these numbers, formulas 
that give an approx­imate prediction of .lm response can be found in [9]. Such an ap­proximation can 
be adequate for simulating visual artifacts such as glare, and predicting areas of scotopic retinal response. 
If desired, one could recover the scaling factor precisely by photographing a calibration luminaire of 
known radiance, and scaling the radiance values to agree with the known radiance of the luminaire. 2.6 
Color Color images, consisting of red, green, and blue channels, can be processed by reconstructing the 
imaging system response curve for 7This work was done in collaboration with Gregory Ward Larson 8Conveniently, 
most digital cameras also specify their sensitivity in terms of ASA. each channel independently. Unfortunately, 
there will be three un­known scaling factors relating relative radiance to absolute radi­ance, one for 
each channel. As a result, different choices of these scaling factors will change the color balance of 
the radiance map. By default, the algorithm chooses the scaling factor such that a pixel with value Zmidwill 
have unit exposure. Thus, any pixel with the RGB value (Zmid;Zmid;Zmid)will have equal radiance val­ues 
for R, G, and B, meaning that the pixel is achromatic. If the three channels of the imaging system actually 
do respond equally to achromatic light in the neighborhood of Zmid, then our procedure correctly reconstructs 
the relative radiances. However, .lms are usually calibrated to respond achromatically to a particular 
color of light C, such as sunlight or .uorescent light. In this case, the radiance values of the three 
channels should be scaled so that the pixel value (Zmid;Zmid;Zmid)maps to a radi­ance with the same color 
ratios as C. To properly model the color response of the entire imaging process rather than just the 
.lm re­sponse, the scaling terms can be adjusted by photographing a cali­bration luminaire of known color. 
 2.7 Taking virtual photographs The recovered response functions can also be used to map radiance values 
back to pixel values for a given exposure !tusing Equa­tion 1. This process can be thought of as taking 
a virtual photograph of the radiance map, in that the resulting image will exhibit the re­sponse qualities 
of the modeled imaging system. Note that the re­sponse functions used need not be the same response functions 
used to construct the original radiance map, which allows photographs acquired with one imaging process 
to be rendered as if they were acquired with another.9 progressing in 1-stop increments from 1of a second 
to 30 seconds. 30 250 200 pixel value Z 150  3 Results Figures 3-5 show the results of using our algorithm 
to determine the response curve of a DCS460 digital camera. Eleven grayscale pho­tographs .ltered down 
to 765x509resolution (Fig. 3) were taken at 1 f/8 with exposure times ranging from 30of a second to 30 
seconds, with each image receiving twice the exposure of the previous one. The .lm curve recovered by 
our algorithm from 45 pixel locations observed across the image sequence is shown in Fig. 4. Note that 
al­though CCD image arrays naturally produce linear output, from the curve it is evident that the camera 
nonlinearly remaps the data, pre­sumably to mimic the response curves found in .lm. The underlying registered 
(Ei!tj;Zij)data are shown as light circles underneath the curve; some outliers are due to sensor artifacts 
(light horizontal bands across some of the darker images.) Fig. 5 shows the reconstructed high dynamic 
range radiance map. To display this map, we have taken the logarithm of the radiance values and mapped 
the range of these values into the range of the display. In this representation, the pixels at the light 
regions do not saturate, and detail in the shadow regions can be made out, indicat­ing that all of the 
information from the original image sequence is present in the radiance map. The large range of values 
present in the radiance map (over four orders of magnitude of useful dynamic range) is shown by the values 
at the marked pixel locations. Figure 6 shows sixteen photographs taken inside a church with a Canon 
35mm SLR camera on Fuji 100 ASA color print .lm. A .sh­eye 15mm lens set at f/8 was used, with exposure 
times ranging from 30 seconds to 1of a second in 1-stop increments. The .lm was 1000 developed professionally 
and scanned in using a Kodak PhotoCD .lm scanner. The scanner was set so that it would not individually 
9Note that here we are assuming that the spectral response functions for each channel of the two imaging 
processes is the same. Also, this technique does not model many signi.cant qualities of an imaging system 
such as .lm grain, chromatic aberration, blooming, and the modulation transfer function. 100 50 0 log 
exposure X Figure 4: The response function of the DCS460 recovered by our al­gorithm, with the underlying 
(Ei!tj;Zij)data shown as light cir­cles. The logarithm is base e. Figure 5: The reconstructed high dynamic 
range radiance map, mapped into a grayscale image by taking the logarithm of the ra­diance values. The 
relative radiance values of the marked pixel lo­cations, clockwise from lower left: 1.0, 46.2, 1907.1, 
15116.0, and 18.0. Figure 6: Sixteen photographs of a church taken at 1-stop increments from 30 sec 
to 1sec. The sun is directly behind the rightmost stained 1000 glass window, making it especially bright. 
The blue borders seen in some of the image margins are induced by the image registration process. Red 
Green 250 250 200 200 pixel value Z pixel value Z 150 150 100 100 50 50 0 0 -10-5 0 5-10-5 0 5 log 
exposure X log exposure X (a) (b) Blue Red (dashed), Green (solid), and Blue (dash-dotted) curves 250 
250 200 200 pixel value Z pixel value Z 150 150 100 100 50 50 0 0 -10 -5 0 5 -5-4-3-2-1 0 1 2 log exposure 
X log exposure X (c) (d) Figure 7: Recovered response curves for the imaging system used in the church 
photographs in Fig. 8. (a-c) Response functions for the red, green, and blue channels, plotted with the 
underlying (Ei!tj;Zij)data shown as light circles. (d) The response functions for red, green, and blue 
plotted on the same axes. Note that while the red and green curves are very consistent, the blue curve 
rises signi.cantly above the others for low exposure values. This indicates that dark regions in the 
images exhibit a slight blue cast. Since this artifact is recovered by the response curves, it does not 
affect the relative radiance values. (a) (b) (c) (d) (e) (f) Figure 8: (a) An actual photograph, taken 
with conventional print .lm at two seconds and scanned to PhotoCD. (b) The high dynamic range radiance 
map, displayed by linearly mapping its entire dynamic range into the dynamic range of the display device. 
(c) The radiance map, displayed by linearly mapping the lower 0:1%of its dynamic range to the display 
device. (d) A false-color image showing relative radiance values for a grayscale version of the radiance 
map, indicating that the map contains over .ve orders of magnitude of useful dynamic range. (e) A rendering 
of the radiance map using adaptive histogram compression. (f) A rendering of the radiance map using histogram 
compression and also simulating various properties of the human visual system, such as glare, contrast 
sensitivity, and scotopic retinal response. Images (e) and (f) were generated by a method described 
in [23]. Images (d-f) courtesy of Gregory Ward Larson.  adjust the brightness and contrast of the images10 
to guarantee that each image would be digitized using the same response function. An unfortunate aspect 
of the PhotoCD process is that it does not scan precisely the same area of each negative relative to 
the extents of the image.11 To counteract this effect, we geometrically regis­tered the images to each 
other using a using normalized correlation (see [4]) to determine, with sub-pixel accuracy, corresponding 
pix­els between pairs of images. Fig. 7(a-c) shows the response functions for the red, green, and blue 
channels of the church sequence recovered from 28 pixel loca­tions. Fig. 7(d) shows the recovered red, 
green, and blue response curves plotted on the same set of axes. From this plot, we can see that while 
the red and green curves are very consistent, the blue curve rises signi.cantly above the others for 
low exposure values. This indicates that dark regions in the images exhibit a slight blue cast. Since 
this artifact is modeled by the response curves, it will not affect the relative radiance values. Fig. 
8 interprets the recovered high dynamic range radiance map in a variety of ways. Fig. 8(a) is one of 
the actual photographs, which lacks detail in its darker regions at the same time that many values within 
the two rightmost stained glass windows are saturated. Figs. 8(b,c) show the radiance map, linearly scaled 
to the display de­vice using two different scaling factors. Although one scaling fac­tor is one thousand 
times the other, there is useful detail in both im­ages. Fig. 8(d) is a false-color image showing radiance 
values for a grayscale version of the radiance map; the highest listed radiance value is nearly 250,000 
times that of the lowest. Figs. 8(e,f) show two renderings of the radiance map using a new tone reproduction 
algorithm [23]. Although the rightmost stained glass window has radiance values over a thousand times 
higher than the darker areas in the rafters, these renderings exhibit detail in both areas. Figure 9 
demonstrates two applications of the techniques pre­sented in this paper: accurate signal processing 
and virtual photog­raphy. The task is to simulate the effects of motion blur caused by moving the camera 
during the exposure. Fig. 9(a) shows the re­sults of convolving an actual, low-dynamic range photograph 
with a 37x1pixel box .lter to simulate horizontal motion blur. Fig. 9(b) shows the results of applying 
this same .lter to the high dy­namic range radiance map, and then sending this .ltered radiance map back 
through the recovered .lm response functions using the same exposure time !tas in the actual photograph. 
Because we are seeing this image through the actual image response curves, the two left images are tonally 
consistent with each other. However, there is a large difference between these two images near the bright 
spots. In the photograph, the bright radiance values have been clamped to the maximum pixel values by 
the response function. As a result, these clamped values blur with lower neighboring values and fail 
to satu­rate the image in the .nal result, giving a muddy appearance. In Fig. 9(b), the extremely high 
pixel values were represented properly in the radiance map and thus remained at values above the level 
of the response function s saturation point within most of the blurred region. As a result, the resulting 
virtual photograph exhibits several crisply-de.ned saturated regions. Fig. 9(c) is an actual photograph 
with real motion blur induced by spinning the camera on the tripod during the exposure, which is equal 
in duration to Fig. 9(a) and the exposure simulated in Fig. 9(b). Clearly, in the bright regions, the 
blurring effect is qualita­tively similar to the synthetic blur in 9(b) but not 9(a). The precise shape 
of the real motion blur is curved and was not modeled for this demonstration. 10This feature of the PhotoCD 
process is called Scene Balance Adjust­ment , or SBA. 11Thisisfarlessofaproblemforcinematicapplications, 
inwhichthe.lm sprocket holes are used to expose and scan precisely the same area of each frame.  (c) 
Actual blurred photograph Figure 9: (a) Synthetic motion blur applied to one of the origi­nal digitized 
photographs. The bright values in the windows are clamped before the processing, producing mostly unsaturated 
val­ues in the blurred regions. (b) Synthetic motion blur applied to a recovered high-dynamic range radiance 
map, then virtually re­photographed through the recovered .lm response curves. The ra­diance values are 
clamped to the display device after the processing, allowing pixels to remain saturated in the window 
regions. (c) Real motion blur created by rotating the camera on the tripod during the exposure, which 
is much more consistent with (b) than (a).  4 Conclusion We have presented a simple, practical, robust 
and accurate method of recovering high dynamic range radiance maps from ordinary pho­tographs. Our method 
uses the constraint of sensor reciprocity to derive the response function and relative radiance values 
directly from a set of images taken with different exposures. This work has a wide variety of applications 
in the areas of image-based modeling and rendering, image processing, and image compositing, a few of 
which we have demonstrated. It is our hope that this work will be able to help both researchers and practitioners 
of computer graphics make much more effective use of digitized photographs. Acknowledgments The authors 
wish to thank Tim Hawkins, Carlo S´equin, David Forsyth, Steve Chenney, Chris Healey, and our reviewers 
for their valuable help in revising this paper. This research was supported by a Multidisciplinary University 
Research Initiative on three dimen­sional direct visualization from ONR and BMDO, grant FDN00014­96-1-1200. 
  References [1] ADAMS,A. Basic Photo, 1st ed. Morgan &#38; Morgan, Hastings-on-Hudson, New York, 1970. 
[2] CHEN, E. QuickTime VR -an image-based approach to vir­tual environment navigation. In SIGGRAPH 95 
(1995). [3] DEBEVEC,P. E., TAYLOR,C. J., AND MALIK, J. Model­ing and rendering architecture from photographs: 
A hybrid geometry-and image-based approach. In SIGGRAPH 96 (August 1996), pp. 11 20. [4] FAUGERAS,O. 
Three-Dimensional Computer Vision.MIT Press, 1993. [5] FERWERDA,J. A., PATTANAIK,S.N., SHIRLEY,P., AND 
GREENBERG, D. P. A model of visual adaptation for realistic image synthesis. In SIGGRAPH 96 (1996), pp. 
249 258. [6] GORTLER,S.J.,GRZESZCZUK,R.,SZELISKI,R., ANDCO-HEN, M. F. The Lumigraph. In SIGGRAPH 96 (1996), 
pp. 43 54. [7] HORN,B.K. P. Robot Vision. MIT Press, Cambridge, Mass., 1986, ch. 10, pp. 206 208. [8] 
JAMES,T., Ed. The Theory of the Photographic Process. Macmillan, New York, 1977. [9] KAUFMAN,J.E., Ed. 
IES Lighting Handbook; the standard lighting guide, 7th ed. Illuminating Engineering Society, New York, 
1987, p. 24. [10] KOLB,C., MITCHELL,D., AND HANRAHAN, P. A realis­tic camera model for computer graphics. 
In SIGGRAPH 95 (1995). [11] LAVEAU, S., AND FAUGERAS, O. 3-D scene representation as a collection of 
images. In Proceedings of 12th International Conference on Pattern Recognition (1994), vol. 1, pp. 689 
691. [12] LEVOY,M., AND HANRAHAN, P. Light .eld rendering. In SIGGRAPH 96 (1996), pp. 31 42. [13] MADDEN, 
B. C. Extended intensity range imaging. Tech. rep., GRASP Laboratory, University of Pennsylvania, 1993. 
[14] MANN, S., AND PICARD, R. W. Being undigital with dig­ital cameras: Extending dynamic range by combining 
differ­ently exposed pictures. In Proceedings of IS&#38;T 46th annual conference (May 1995), pp. 422 
428. [15] MCMILLAN,L., AND BISHOP, G. Plenoptic Modeling: An image-based rendering system. In SIGGRAPH 
95 (1995). [16] SCHLICK, C. Quantization techniques for visualization of high dynamic range pictures. 
In Fifth Eurographics Workshop on Rendering (Darmstadt, Germany) (June 1994), pp. 7 18. [17] SZELISKI, 
R. Image mosaicing for tele-reality applications. In IEEE Computer Graphics and Applications (1996). 
[18] TANI,T. Photographic sensitivity : theory and mechanisms. Oxford University Press, New York, 1995. 
[19] THEUWISSEN,A.J.P. Solid-state imaging with charge­coupled devices. Kluwer Academic Publishers, Dordrecht; 
Boston, 1995. [20] TUMBLIN,J., AND RUSHMEIER, H. Tone reproduction for realistic images. IEEE Computer 
Graphics and Applications 13, 6 (1993), 42 48. [21] WARD, G. J. Measuring and modeling anisotropic re.ection. 
In SIGGRAPH 92 (July 1992), pp. 265 272. [22] WARD, G. J. The radiance lighting simulation and rendering 
system. In SIGGRAPH 94 (July 1994), pp. 459 472. [23] WARD,G.J., RUSHMEIER,H., AND PIATKO,C. A visi­bility 
matching tone reproduction operator for high dynamic range scenes. Tech. Rep. LBNL-39882, Lawrence Berkeley 
National Laboratory, March 1997. A Matlab Code Here is the MATLAB code used to solve the linear system 
that min­imizes the objective function Oin Equation 3. Given a set of ob­served pixel values in a set 
of images with known exposures, this routine reconstructs the imaging response curve and the radiance 
values for the given pixels. The weighting function w(z)is found in Equation 4. % % gsolve.m - Solve 
for imaging system response function % % Given a set of pixel values observed for several pixels in several 
% images with different exposure times, this function returns the % imaging system s response function 
g as well as the log film irradiance % values for the observed pixels. % % Assumes: % % Zmin = 0 % Zmax 
= 255 % % Arguments: % % Z(i,j) is the pixel values of pixel location number i in image j % B(j) is the 
log delta t, or log shutter speed, for image j % l is lamdba, the constant that determines the amount 
of smoothness % w(z) is the weighting function value for pixel value z % % Returns: % % g(z) is the log 
exposure corresponding to pixel value z % lE(i) is the log film irradiance at pixel location i % function 
[g,lE]=gsolve(Z,B,l,w) n = 256; A = zeros(size(Z,1)*size(Z,2)+n+1,n+size(Z,1)); b = zeros(size(A,1),1); 
 %% Include the data-fitting equations k = 1; for i=1:size(Z,1) for j=1:size(Z,2) wij = w(Z(i,j)+1); 
 A(k,Z(i,j)+1) = wij; A(k,n+i) = -wij; b(k,1) = wij * B(i,j); k=k+1; end end %% Fix the curve by 
setting its middle value to 0 A(k,129) = 1; k=k+1; %% Include the smoothness equations for i=1:n-2 
 A(k,i)=l*w(i+1); A(k,i+1)=-2*l*w(i+1); A(k,i+2)=l*w(i+1); k=k+1; end %% Solve the system using SVD 
 x = A\b; g = x(1:n); lE = x(n+1:size(x,1));   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>ONR and BMDO</funding_agency>
			<grant_numbers>
				<grant_number>FDN00014-96-1-1200</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401175</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<pages>10</pages>
		<display_no>32</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Rendering synthetic objects into real scenes]]></title>
		<subtitle><![CDATA[bridging traditional and image-based graphics with global illumination and high dynamic range photography]]></subtitle>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/1401132.1401175</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401175</url>
		<abstract>
			<par><![CDATA[<p>We present a method that uses measured scene radiance and global illumination in order to add new objects to light-based models with correct lighting. The method uses a high dynamic range image-based model of the scene, rather than synthetic light sources, to illuminate the new objects. To compute the illumination, the scene is considered as three components: the distant scene, the local scene, and the synthetic objects. The distant scene is assumed to be photometrically unaffected by the objects, obviating the need for reflectance model information. The local scene is endowed with estimated reflectance model information so that it can catch shadows and receive reflected light from the new objects. Renderings are created with a standard global illumination method by simulating the interaction of light amongst the three components. A differential rendering technique allows for good results to be obtained when only an estimate of the local scene reflectance properties is known.</p> <p>We apply the general method to the problem of rendering synthetic objects into real scenes. The light-based model is constructed from an approximate geometric model of the scene and by using a light probe to measure the incident illumination at the location of the synthetic objects. The global illumination solution is then composited into a photograph of the scene using the differential rendering technique. We conclude by discussing the relevance of the technique to recovering surface reflectance properties in uncontrolled lighting situations. Applications of the method include visual effects, interior design, and architectural visualization.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Sensor fusion</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010233</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Vision for robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098697</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adelson, E. H., and Bergen, J. R. <i>Computational Models of Visual Processing</i>. MIT Press, Cambridge, Mass., 1991, ch. 1. The Plenoptic Function and the Elements of Early Vision.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Azarmi, M. <i>Optical Effects Cinematography: Its Development, Methods, and Techniques</i>. University Microfilms International, Ann Arbor, Michigan, 1973.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. Texture and reflection in computer generated images. <i>Communications of the ACM 19</i>, 10 (October 1976), 542--547.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chen, E. Quick Time VR - an image-based approach to virtual environment navigation. In <i>SIGGRAPH '95</i> (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97894</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E. Incremental radiosity: An extension of progressive radiosity to an interactive synthesis system. In <i>SIGGRAPH '90</i> (1990), pp. 135--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378487</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cohen, M. F., Chen, S. E., Wallace, J. R., and Greenberg, D. P. A progressive refinement approach to fast radiosity image generation. In <i>SIGGRAPH '88</i> (1988), pp. 75--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Curless, B., and Levoy, M. A volumetric method for building complex models from range images. In <i>SIGGRAPH '96</i> (1996), pp. 303--312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794511</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Dana, K. J., Ginneken, B., Nayar, S. K., and Koenderink, J. J. Reflectance and texture of real-world surfaces. In <i>Proc. IEEE Conf. on Comp. Vision and Patt. Recog.</i> (1997), pp. 151--157.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., and Malik, J. Recovering high dynamic range radiance maps from photographs. In <i>SIGGRAPH '97</i> (August 1997), pp. 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., Taylor, C. J., and Malik, J. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. In <i>SIGGRAPH '96</i> (August 1996), pp. 11--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>893689</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., Yu, Y., and Borshukov, G. D. Efficient view-dependent image-based rendering with projective texture-mapping. Tech. Rep. UCB//CSD-98-1003, University of California at Berkeley, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732112</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Drettakis, G., Robert, L., and Bougnoux, S. Interactive common illumination for computer augmented reality. In <i>8th Eurographics workshop on Rendering, St. Etienne, France</i> (May 1997), J. Dorsey and P. Slusallek, Eds., pp. 45--57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Fielding, R. <i>The Technique of Special Effects Cinematography</i>. Hastings House, New York, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Fournier, A., Gunawan, A., and Romanzin, C. Common illumination between real and computer generated scenes. In <i>Graphics Interface</i> (May 1993), pp. 254--262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192171</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Gershbein, R., Schroder, P., and Hanrahan, P. Textures and radiosity: Controlling emission and reflection with texture maps. In <i>SIGGRAPH '94</i> (1994).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Goral, C. M., Torrance, K. E., Greenberg, D. P., and Battaile, B. Modeling the interaction of light between diffuse surfaces. In <i>SIGGRAPH '84</i> (1984), pp. 213--222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Gortler, S. J., Grzeszczuk, R., Szeliski, R., and Cohen, M. F. The Lumigraph. In <i>SIGGRAPH '96</i> (1996), pp. 43--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P. S. Survey of texture mapping. <i>IEEE Computer Graphics and Applications 6</i>, 11 (November 1986), 56--67.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. The rendering equation. In <i>SIGGRAPH '86</i> (1986), pp. 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Karner, K. F., Mayer, H., and Gervautz, M. An image based measurement system for anisotropic reflection. In <i>EUROGRAPHICS Annual Conference Proceedings</i> (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J., and van Doorn, A. J. Illuminance texture due to surface mesostructure. <i>J. Opt. Soc. Am. 13</i>, 3 (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Laveau, S., and Faugeras, O. 3-D scene representation as a collection of images. In <i>Proceedings of 12th International Conference on Pattern Recognition</i> (1994), vol. 1, pp. 689--691.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., and Hanrahan, P. Light field rendering. In <i>SIGGRAPH '96</i> (1996), pp. 31--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[McMillan, L., and Bishop, G. Plenoptic Modeling: An image-based rendering system. In <i>SIGGRAPH '95</i> (1995).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15909</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Nakamae, E., Harada, K., and Ishizaki, T. A montage method: The overlaying of the computer generated images onto a background photograph. In <i>SIGGRAPH '86</i> (1986), pp. 207--214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Porter, T., and Duff, T. Compositing digital images. In <i>SIGGRAPH 84</i> (July 1984), pp. 253--259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Sato, Y., Wheeler, M. D., and Ikeuchi, K. Object shape and reflectance modeling from observation. In <i>SIGGRAPH '97</i> (1997), pp. 379--387.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Smith, T. G. <i>Industrial Light and Magic: The Art of Special Effects</i>. Ballantine Books, New York, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Szeliski, R. Image mosaicing for tele-reality applications. In <i>IEEE Computer Graphics and Applications</i> (1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Turk, G., and Levoy, M. Zippered polygon meshes from range images. In <i>SIGGRAPH '94</i> (1994), pp. 311--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. J. Metropolis light transport. In <i>SIGGRAPH '97</i> (August 1997), pp. 65--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J. Measuring and modeling anisotropic reflection. In <i>SIGGRAPH '92</i> (July 1992), pp. 265--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J. The radiance lighting simulation and rendering system. In <i>SIGGRAPH '94</i> (July 1994), pp. 459--472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Watanabe, M., and Nayar, S. K. Telecentric optics for computational vision. In <i>Proceedings of Image Understanding Workshop (IUW 96)</i> (February 1996).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>138633</ref_obj_id>
				<ref_obj_pid>138628</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Y. Chen, and Medioni, G. Object modeling from multiple range images. <i>Image and Vision Computing 10</i>, 3 (April 1992), 145--155.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 To appear in the SIGGRAPH 98 conference proceedings Rendering Synthetic Objects into Real Scenes: Bridging 
Traditional and Image-based Graphics with Global Illumination and High Dynamic Range Photography Paul 
Debevec University of California at Berkeley1 ABSTRACT We present a method that uses measured scene 
radiance and global illumination in order to add new objects to light-based models with correct lighting. 
The method uses a high dynamic range image­based model of the scene, rather than synthetic light sources, 
to il­luminate the new objects. To compute the illumination, the scene is considered as three components: 
the distant scene, the local scene, and the synthetic objects. The distant scene is assumed to be pho­tometrically 
unaffected by the objects, obviating the need for re­.ectance model information. The local scene is endowed 
with es­timated re.ectance model information so that it can catch shadows and receive re.ected light 
from the new objects. Renderings are created with a standard global illumination method by simulating 
the interaction of light amongst the three components. A differen­tial rendering technique allows for 
good results to be obtained when only an estimate of the local scene re.ectance properties is known. 
We apply the general method to the problem of rendering syn­thetic objects into real scenes. The light-based 
model is constructed from an approximate geometric model of the scene and by using a light probe to measure 
the incident illumination at the location of the synthetic objects. The global illumination solution 
is then com­posited into a photograph of the scene using the differential render­ing technique. We conclude 
by discussing the relevance of the tech­nique to recovering surface re.ectance properties in uncontrolled 
lighting situations. Applications of the method include visual ef­fects, interior design, and architectural 
visualization. CR Descriptors: I.2.10 [Arti.cial Intelligence]: Vision and Scene Understanding -Intensity, 
color, photometry and threshold­ing; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
-Color, shading, shadowing, and texture; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism 
-Radiosity; I.4.1 [Image Processing]: Digitization -Scanning; I.4.8 [Image Processing]: Scene Analysis 
-Photometry, Sensor Fusion. 1Computer Science Division, University of California at Berke­ley, Berkeley, 
CA 94720-1776. Email: debevec@cs.berkeley.edu. More information and additional results may be found at: 
http://www.cs.berkeley.edu/ debevec/Research 1 Introduction Rendering synthetic objects into real-world 
scenes is an important application of computer graphics, particularly in architectural and visual effects 
domains. Oftentimes, a piece of furniture, a prop, or a digital creature or actor needs to be rendered 
seamlessly into a real scene. This dif.cult task requires that the objects be lit con­sistently with 
the surfaces in their vicinity, and that the interplay of light between the objects and their surroundings 
be properly simu­lated. Speci.cally, the objects should cast shadows, appear in re.ec­tions, and refract, 
focus, and emit light just as real objects would. Figure 1: The General Method In our method for adding 
synthetic objects into light-based scenes, the scene is partitioned into three components: the distant 
scene, the local scene, and the synthetic ob­jects. Global illumination is used to simulate the interplay 
of light amongst all three components, except that light re.ected back at the distant scene is ignored. 
As a result, BRDF information for the dis­tant scene is unnecessary. Estimates of the geometry and material 
properties of the local scene are used to simulate the interaction of light between it and the synthetic 
objects. Currently available techniques for realistically rendering syn­thetic objects into scenes are 
labor intensive and not always success­ful. A common technique is to manually survey the positions of 
the light sources, and to instantiate a virtual light of equal color and in­tensity for each real light 
to illuminate the synthetic objects. An­other technique is to photograph a reference object (such as 
a gray sphere) in the scene where the new object is to be rendered, and use its appearance as a qualitative 
guide in manually con.guring the lighting environment. Lastly, the technique of re.ection mapping is 
useful for mirror-like re.ections. These methods typically require considerable hand-re.nement and none 
of them easily simulates the effects of indirect illumination from the environment. Accurately simulating 
the effects of both direct and indirect light­ing has been the subject of research in global illumination. 
With a global illumination algorithm, if the entire scene were modeled with its full geometric and re.ectance 
(BRDF) characteristics, one could correctly render a synthetic object into the scene simply by adding 
it to the model and recomputing the global illumination solution. Un­fortunately, obtaining a full geometric 
and re.ectance model of a large environment is extremeley dif.cult. Furthermore, global il­lumination 
solutions for large complex environments are extremely computationally intensive. Moreover, it seems 
that having a full re.ectance model of the large-scale scene should be unnecessary: under most circumstances, 
a new object will have no signi.cant effect on the appearance of most of the of the distant scene. Thus, 
for such distant areas, know­ing just its radiance (under the desired lighting conditions) should suf.ce. 
Recently, [9] introduced a high dynamic range photographic technique that allows accurate measurements 
of scene radiance to be derived from a set of differently exposed photographs. This tech­nique allows 
both low levels of indirect radiance from surfaces and high levels of direct radiance from light sources 
to be accurately recorded. When combined with image-based modeling techniques (e.g. [22, 24, 4, 10, 23, 
17, 29]), and possibly active techniques for measuring geometry (e.g. [35, 30, 7, 27]) these derived 
radiance maps can be used to construct spatial representations of scene ra­diance. We will use the term 
light-based model to refer to a repre­sentation of a scene that consists of radiance information, possi­bly 
with speci.c reference to light leaving surfaces, but not neces­sarily containing material property (BRDF) 
information. A light­based model can be used to evaluate the 5D plenoptic function [1] P((;q;Vx;V;Vz)for 
a given virtual or real subset of space1.A y material-based model is converted to a light-based model 
by com­puting an illumination solution for it. A light-based model is differ­entiated from an image-based 
model in that its light values are ac­tual measures of radiance2, whereas image-based models may con­tain 
pixel values already transformed and truncated by the response function of an image acquisition or synthesis 
process. In this paper, we present a general method for using accurate measurements of scene radiance 
in conjunction with global illumi­nation to realistically add new objects to light-based models. The 
synthetic objects may have arbitrary material properties and can be rendered with appropriate illumination 
in arbitrary lighting environ­ments. Furthermore, the objects can correctly interact with the en­vironment 
around them: they cast the appropriate shadows, they are properly re.ected, they can re.ect and focus 
light, and they exhibit appropriate diffuse interre.ection. The method can be carried out with commonly 
available equipment and software. In this method (see Fig. 1), the scene is partitioned into three components. 
The .rst is the distant scene, which is the visible part of the environment too remote to be perceptibly 
affected by the syn­thetic object. The second is the local scene, which is the part of the environment 
which will be signi.cantly affected by the presence of the objects. The third component is the synthetic 
objects. Our ap­proach uses global illumination to correctly simulate the interaction of light amongst 
these three elements, with the exception that light radiated toward the distant environment will not 
be considered in the calculation. As a result, the BRDF of the distant environment need not be known 
 the technique uses BRDF information only for the local scene and the synthetic objects. We discuss the 
challenges in estimating the BRDF of the local scene, and methods for obtain­ing usable approximations. 
We also present a differential rendering 1Time and wavelength dependence can be included to represent 
the gen­eral 7D plenoptic function as appropriate. 2In practice, the measures of radiance are with respect 
to a discrete set of spectral distributions such as the standard tristimulus model. technique that produces 
perceptually accurate results even when the estimated BRDF is somewhat inaccurate. We demonstrate the 
general method for the speci.c case of ren­dering synthetic objects into particular views of a scene 
(such as background plates) rather than into a general image-based model. In this method, a light probe 
is used to acquire a high dynamic range panoramic radiance map near the location where the object will 
be rendered. A simple example of a light probe is a camera aimed at a mirrored sphere, a con.guration 
commonly used for acquiring envi­ronment maps. An approximate geometric model of the scene is cre­ated 
(via surveying, photogrammetry, or 3D scanning) and mapped with radiance values measured with the light 
probe. The distant scene, local scene, and synthetic objects are rendered with global illumination from 
the same point of view as the background plate, and the results are composited into the background plate 
with a dif­ferential rendering technique. 1.1 Overview The rest of this paper is organized as follows. 
In the next section we discuss work related to this paper. Section 3 introduces the ba­sic technique 
of using acquired maps of scene radiance to illuminate synthetic objects. Section 4 presents the general 
method we will use to render synthetic objects into real scenes. Section 5 describes a practical technique 
based on this method using a light probe to mea­sure incident illumination. Section 6 presents a differential 
render­ing technique for rendering the local environment with only an ap­proximate description of its 
re.ectance. Section 7 presents a sim­ple method to approximately recover the diffuse re.ectance char­acteristics 
of the local environment. Section 8 presents results ob­tained with the technique. Section 9 discusses 
future directions for this work, and we conclude in Section 10.  2 Background and Related Work The 
practice of adding new objects to photographs dates to the early days of photography in the simple form 
of pasting a cut-out from one picture onto another. While the technique conveys the idea of the new object 
being in the scene, it usually fails to produce an image that as a whole is a believable photograph. 
Attaining such realism requires a number of aspects of the two images to match. First, the camera projections 
should be consistent, otherwise the object may seem too foreshortened or skewed relative to the rest 
of the picture. Second, the patterns of .lm grain and .lm response should match. Third, the lighting 
on the object needs to be consistent with other objects in the environment. Lastly, the object needs 
to cast realistic shadows and re.ections on the scene. Skilled artists found that by giving these considerations 
due attention, synthetic objects could be painted into still photographs convincingly. In optical .lm 
compositing, the use of object mattes to prevent particular sections of .lm from being exposed made the 
same sort of cut-and-paste compositing possible for moving images. However, the increased demands of 
realism imposed by the dynamic nature of .lm made matching camera positions and lighting even more criti­cal. 
As a result, care was taken to light the objects appropriately for the scene into which they were to 
be composited. This would still not account for the objects casting shadows onto the scene, so often 
these were painted in by an artist frame by frame [13, 2, 28]. Digi­tal .lm scanning and compositing 
[26] helped make this process far more ef.cient. Work in global illumination [16, 19] has recently produced 
algo­rithms (e.g. [31]) and software (e.g. [33]) to realistically simulate lighting in synthetic scenes, 
including indirect lighting with both specular and diffuse re.ections. We leverage this work in order 
to create realistic renderings. Some work has been done on the speci.c problem of composit­ing objects 
into photography. [25] presented a procedure for ren­dering architecture into background photographs 
using knowledge of the sun position and measurements or approximations of the lo­cal ambient light. For 
diffuse buildings in diffuse scenes, the tech­nique is effective. The technique of re.ection mapping 
(also called environment mapping) [3, 18] produces realistic results for mirror­like objects. In re.ection 
mapping, a panoramic image is rendered or photographed from the location of the object. Then, the surface 
normals of the object are used to index into the panoramic image by re.ecting rays from the desired viewpoint. 
As a result, the shiny object appears to properly re.ect the desired environment3.How­ever, the technique 
is limited to mirror-like re.ection and does not account for objects casting light or shadows on the 
environment.  A common visual effects technique for having synthetic objects cast shadows on an existing 
environment is to create an approximate geometric model of the environment local to the object, and then 
compute the shadows from the various light sources. The shadows can then be subtracted from the background 
image. In the hands of professional artists this technique can produce excellent results, but it requires 
knowing the position, size, shape, color, and intensity of each of the scene s light sources. Furthermore, 
it does not account for diffuse re.ection from the scene, and light re.ected by the ob­jects onto the 
scene must be handled specially. To properly model the interaction of light between the objects and the 
local scene, we pose the compositing problem as a global illumi­nation computation as in [14] and [12]. 
As in this work, we apply the effect of the synthetic objects in the lighting solution as a dif­ferential 
update to the original appearance of the scene. In the pre­vious work an approximate model of the entire 
scene and its origi­nal light sources is constructed; the positions and sizes of the light sources are 
measured manually. Rough methods are used to esti­mate diffuse-only re.ectance characteristics of the 
scene, which are then used to estimate the intensities of the light sources. [12] addi­tionally presents 
a method for performing fast updates of the illu­mination solution in the case of moving objects. As 
in the previous work, we leverage the basic result from incremental radiosity [6, 5] that making a small 
change to a scene does not require recomputing the entire solution.  3 Illuminating synthetic objects 
with real light In this section we propose that computer-generated objects be lit by actual recordings 
of light from the scene, using global illumination. Performing the lighting in this manner provides a 
uni.ed and phys­ically accurate alternative to manually attempting to replicate inci­dent illumination 
conditions. Accurately recording light in a scene is dif.cult because of the high dynamic range that 
scenes typically exhibit; this wide range of brightness is the result of light sources being relatively 
concen­trated. As a result, the intensity of a source is often two to six orders of magnitude larger 
than the intensity of the non-emissive parts of an environment. However, it is necessary to accurately 
record both the large areas of indirect light from the environment and the con­centrated areas of direct 
light from the sources since both are signif­icant parts of the illumination solution. Using the technique 
introduced in [9], we can acquire correct measures of scene radiance using conventional imaging equipment. 
The images, called radiance maps, are derived from a series of im­ages with different sensor integration 
times and a technique for com­puting and accounting for the imaging system response function f. We can 
use these measures to illuminate synthetic objects exhibiting arbitrary material properties. Fig. 2 shows 
a high-dynamic range lighting environment with electric, natural, and indirect lighting. This environment 
was 3Using the surface normal indexing method, the object will not re.ect itself. Correct self-re.ection 
can be obtained through ray tracing. recorded by taking a full dynamic range photograph of a mirrored 
ball on a table (see Section 5). A digital camera was used to acquire a series images in one-stop exposure 
increments from 1to 1 4 10000 second. The images were fused using the technique in [9]. The environment 
is displayed at three exposure levels (-0, -3.5, and -7.0 stops) to show its full dynamic range. Recovered 
RGB ra­diance values for several points in the scene and on the two major light sources are indicated; 
the color difference between the tung­sten lamp and the sky is evident. A single low-dynamic range pho­tograph 
would be unable to record the correct colors and intensities over the entire scene. Fig. 3(a-e) shows 
the results of using this panoramic radiance map to synthetically light a variety of materials using 
the RADI-ANCE global illumination algorithm [33]. The materials are: (a) perfectly re.ective, (b) rough 
gold, (c) perfectly diffuse gray ma­terial, (d) shiny green plastic, and (e) dull orange plastic. Since 
we are computing a full illumination solution, the objects exhibit self-re.ection and shadows from the 
light sources as appropriate. Note that in (c) the protrusions produce two noticeable shadows of slightly 
different colors, one corresponding to the ceiling light and a softer shadow corresponding to the window. 
The shiny plastic object in (d) has a 4 percent specular component with a Gaussian roughness of 0.04 
[32]. Since the object s surface both blurs and attenuates the light with its rough specular compo­nent, 
the re.ections fall within the dynamic range of our display de­vice and the different colors of the light 
sources can be seen. In (e) the rough plastic diffuses the incident light over a much larger area. To 
illustrate the importance of using high dynamic range radi­ance maps, the same renderings were produced 
using just one of the original photographs as the lighting environment. In this single image, similar 
in appearance to Fig. 2(a), the brightest regions had been truncated to approximately 2 percent of their 
true values. The rendering of the mirrored surface (f) appears similar to (a) since it is displayed in 
low-dynamic range printed form. Signi.cant errors are noticeable in (g-j) since these materials blur 
the incident light. In (g), the blurring of the rough material darkens the light sources, whereas in 
(b) they remain saturated. Renderings (h-j) are very dark due to the missed light; thus we have brightened 
by a factor of eight on the right in order to make qualitative comparisons to (c-e) pos­sible. In each 
it can be seen that the low-dynamic range image of the lighting environment fails to capture the information 
necessary to simulate correct color balance, shadows, and highlights. Fig. 4 shows a collection of objects 
with different material prop­erties illuminated by two different environments. A wide variety of light 
interaction between the objects and the environment can be seen. The (synthetic) mirrored ball re.ects 
both the synthetic ob­jects as well as the environment. The .oating diffuse ball shows a subtle color 
shift along its right edge as it shadows itself from the windows and is lit primarily by the incandescent 
lamp in Fig. 4(a). The re.ection of the environment in the black ball (which has a specular intensity 
of 0.04) shows the colors of the light sources, which are too bright to be seen in the mirrored ball. 
A variety of shadows, re.ections, and focused light can be observed on the rest­ing surface. The next 
section describes how the technique of using radiance maps to illuminate synthetic objects can be extended 
to compute the proper photometric interaction of the objects with the scene. It also describes how high 
dynamic range photography and image-based modeling combine in a natural manner to allow the simulation 
of arbitrary (non-in.nite) lighting environments.  4 The General Method This section explains our method 
for adding new objects to light­based scene representations. As in Fig. 1, we partition our scene into 
three parts: the distant scene, the local scene, and the synthetic  Figure 2: An omnidirectional radiance 
map This full dynamic range lighting environment was acquired by photographing a mirrored ball balanced 
on the cap of a pen sitting on a table. The environment contains natural, electric, and indirect light. 
The three views of this image adjusted to (a) +0 stops, (b) -3.5 stops, and (c) -7.0 stops show that 
the full dynamic range of the scene has been captured without saturation. As a result, the image usefully 
records the direction, color, and intensity of all forms of incident light. Figure 3: Illuminating synthetic 
objects with real light (Top row: a,b,c,d,e) With full dynamic range measurements of scene radiance from 
Fig. 2. (Bottom row: f,g,h,i,j) With low dynamic range information from a single photograph of the ball. 
The right sides of images (h,i,j) have been brightened by a factor of six to allow qualitative comparison 
to (c,d,e). The high dynamic range measurements of scene radiance are necessary to produce proper lighting 
on the objects. Figure 4: Synthetic objects lit by two different environments (a) A collection of objects 
is illuminated by the radiance information in 2. The objects exhibit appropriate interre.ection. (b) 
The same objects are illuminated by different radiance information obtained in an outdoor urban environment 
on an overcast day. The radiance map used for the illumination is shown in the upper left of each image. 
Candle holder model courtesy of Gregory Ward Larson. objects. We describe the geometric and photometric 
requirements for each of these components. 1. A light-based model of the distant scene The distant scene 
is constructed as a light-based model. The synthetic objects will receive light from this model, so it 
is nec­essary that the model store true measures of radiance rather than low dynamic range pixel values 
from conventional im­ages. The light-based model can take on any form, using very little explicit geometry 
[23, 17], some geometry [24], moder­ate geometry [10], or be a full 3D scan of an environment with view-dependent 
texture-mapped [11] radiance. What is im­portant is for the model to provide accurate measures of inci­dent 
illumination in the vicinity of the objects, as well as from the desired viewpoint. In the next section 
we will present a convenient procedure for constructing a minimal model that meets these requirements. 
In the global illumination computation, the distant scene ra­diates light toward the local scene and 
the synthetic objects, but ignores light re.ected back to it. We assume that no area of the distant scene 
will be signi.cantly affected by light re­.ecting from the synthetic objects; if that were the case, 
the area should instead belong to the local scene, which contains the BRDF information necessary to interact 
with light. In the RADIANCE [33] system, this exclusively emissive behavior can be speci.ed with the 
glow material property. 2. An approximate material-based model of the local scene The local scene consists 
of the surfaces that will photomet­rically interact with the synthetic objects. It is this geome­try 
onto which the objects will cast shadows and re.ect light. Since the local scene needs to fully participate 
in the illumina­tion solution, both its geometry and re.ectance characteristics should be known, at least 
approximately. If the geometry of the local scene is not readily available with suf.cient accuracy from 
the light-based model of the distant scene, there are vari­ous techniques available for determining its 
geometry through active or passive methods. In the common case where the lo­cal scene is a .at surface 
that supports the synthetic objects, its geometry is determined easily from the camera pose. Meth­ods 
for estimating the BRDF of the local scene are discussed in Section 7. Usually, the local scene will 
be the part of the scene that is geo­metrically close to the synthetic objects. When the local scene 
is mostly diffuse, the rendering equation shows that the visible effect of the objects on the local scene 
decreases as the inverse square of the distance between the two. Nonetheless, there is a variety of circumstances 
in which synthetic objects can signif­icantly affect areas of the scene not in the immediate vicinity. 
Some common circumstances are: .If there are concentrated light sources illuminating the object, then 
the object can cast a signi.cant shadow on a distant surface collinear with it and the light source. 
 .If there are concentrated light sources and the object is .at and specular, it can focus a signi.cant 
amount of light onto a distant part of the scene. .If a part of the distant scene is .at and specular 
(e.g. a mirror on a wall), its appearance can be signi.cantly af­fected by a synthetic object. .If the 
synthetic object emits light (e.g. a synthetic laser), it can affect the appearance of the distant scene 
signi.­cantly.   These situations should be considered in choosing which parts of the scene should 
be considered local and which parts dis­tant. Any part of the scene that will be signi.cantly affected 
in 5 its appearance from the desired viewpoint should be included as part of the local scene. Since 
the local scene is a full BRDF model, it can be added to the global illumination problem as would any 
other object. The local scene may consist of any number of surfaces and ob­jects with different material 
properties. For example, the local scene could consist of a patch of .oor beneath the synthetic object 
to catch shadows as well as a mirror surface hanging on the opposite wall to catch a re.ection. The local 
scene re­places the corresponding part of the light-based model of the distant scene. Since it can be 
dif.cult to determine the precise BRDF char­acteristics of the local scene, it is often desirable to 
have only the change in the local scene s appearance be computed with the BRDF estimate; its appearance 
due to illumination from the distant scene is taken from the original light-based model. This differential 
rendering method is presented in Section 6. 3. Complete material-based models of the objects The synthetic 
objects themselves may consist of any variety of shapes and materials supported by the global illumination 
software, including plastics, metals, emitters, and dielectrics such as glass and water. They should 
be placed in their desired geometric correspondence to the local scene. Once the distant scene, local 
scene, and synthetic objects are properly modeled and positioned, the global illumination software can 
be used in the normal fashion to produce renderings from the desired viewpoints.  5 Compositing using 
a light probe This section presents a particular technique for constructing a light­based model of a 
real scene suitable for adding synthetic objects at a particular location. This technique is useful for 
compositing objects into actual photography of a scene. In Section 4, we mentioned that the light-based 
model of the dis­tant scene needs to appear correctly in the vicinity of the synthetic objects as well 
as from the desired viewpoints. This latter require­ment can be satis.ed if it is possible to directly 
acquire radiance maps of the scene from the desired viewpoints. The former require­ment, that the appear 
photometrically correct in all directions in the vicinity of the synthetic objects, arises because this 
information comprises the incident light which will illuminate the objects. To obtain this part of the 
light-based model, we acquire a full dy­namic range omnidirectional radiance map near the location of 
the synthetic object or objects. One technique for acquiring this radi­ance map is to photograph a spherical 
.rst-surface mirror, such as a polished steel ball, placed at or near the desired location of the syn­thetic 
object4. This procedure is illustrated in Fig. 7(a). An actual radiance map obtained using this method 
is shown in Fig. 2. The radiance measurements observed in the ball are mapped onto the geometry of the 
distant scene. In many circumstances this model can be very simple. In particular, if the objects are 
small and resting on a .at surface, one can model the scene as a horizontal plane for the resting surface 
and a large dome for the rest of the environment. Fig. 7(c) illustrates the ball image being mapped onto 
a table surface and the walls and ceiling of a .nite room; 5 shows the resulting light­based model. 
5.1 Mapping from the probe to the scene model To precisely determine the mapping between coordinates 
on the ball and rays in the world, one needs to record the position of the ball 4Parabolic mirrors combined 
with telecentric lenses [34] can be used to obtain hemispherical .elds of view with a consistent principal 
point, if so desired. relative to the camera, the size of the ball, and the camera param­eters such 
as its location in the scene and focal length. With this information, it is straightforward to trace 
rays from the camera cen­ter through the pixels of the image, and re.ect rays off the ball into the environment. 
Often a good approximation results from assum­ing the ball is small relative to the environment and that 
the camera s view is orthographic. The data acquired from a single ball image will exhibit a num­ber 
of artifacts. First, the camera (and possibly the photographer) will be visible. The ball, in observing 
the scene, interacts with it: the ball (and its support) can appear in re.ections, cast shadows, and 
can re.ect light back onto surfaces. Lastly, the ball will not re­.ect the scene directly behind it, 
and will poorly sample the area nearby. If care is taken in positioning the ball and camera, these ef­fects 
can be minimized and will have a negligible effect on the .nal renderings. If the artifacts are signi.cant, 
the images can be .xed manually in image editing program or by selectively combining im­ages of the ball 
taken from different directions; Fig. 6 shows a rela­tively artifact-free enviroment constructed using 
the latter method. We have found that combining two images of the ball taken ninety degrees apart from 
each other allows us to eliminate the camera s appearance and to avoid poor sampling. (a) (b) Figure 
6: Rendering with a Combined Probe Image The full dy­namic range environment map shown at the top was 
assembled from two light probe images taken ninety degrees apart from each other. As a result, the only 
visible artifact is small amount of the probe sup­port visible on the .oor. The map is shown at -4.5, 
0, and +4.5 stops. The bottom rendering was produced using this lighting information, and exhibits diffuse 
and specular re.ections, shadows from different sources of light, re.ections, and caustics. 6 5.2 Creating 
renderings To render the objects into the scene, a synthetic local scene model is created as described 
in Section 4. Images of the scene from the de­sired viewpoint(s) are taken (Fig. 7(a)), and their position 
relative to the scene is recorded through pose-instrumented cameras or (as in our work) photogrammetry. 
The location of the ball in the scene is also recorded at this time. The global illumination software 
is then run to render the objects, local scene, and distant scene from the de­sired viewpoint (Fig. 7(d)). 
The objects and local scene are then composited onto the back­ground image. To perform this compositing, 
a mask is created by rendering the objects and local scene in white and the distant scene in black. If 
objects in the distant scene (which may appear in front of the objects or local scene from certain viewpoints) 
are geomet­rically modeled, they will properly obscure the local scene and the objects as necessary. 
This compositing can be considered as a subset of the general method (Section 4) wherein the light-based 
model of the distant scene acts as follows: if (Vx;V;V)corresponds to an actual view of the scene, return 
the radiance value looking in direc­tion ((;q). Otherwise, return the radiance value obtained by casting 
the ray ((;q;Vx;V;Vz)onto the radiance-mapped distant scene yz y model. In the next section we describe 
a more robust method of com­positing the local scene into the background image.  6 Improving quality 
with differential ren­dering The method we have presented so far requires that the local scene be modeled 
accurately in both its geometry and its spatially varying material properties. If the model is inaccurate, 
the appearance of the local scene will not be consistent with the appearance of adjacent distant scene. 
Such a border is readily apparent in Fig. 8(c), since the local scene was modeled with a homogeneous 
BRDF when in reality it exhibits a patterned albedo (see [21]). In this section we describe a method 
for greatly reducing such effects. Suppose that we compute a global illumination solution for the local 
and distant scene models without including the synthetic ob­jects. If the BRDF and geometry of the local 
scene model were per­fectly accurate, then one would expect the appearance of the ren­dered local scene 
to be consistent with its appearance in the light­based model of the entire scene. Let us call the appearance 
of the lo­cal scene from the desired viewpoint in the light-based model LSb. In the context of the method 
described in Section 5, LSbis simply the background image. We will let LSnoobjdenote the appearance of 
the local scene, without the synthetic objects, as calculated by the global illumination solution. The 
error in the rendered local scene (without the objects) is thus: Errls=LSnoobj,LSb. This error results 
from the difference between the BRDF characteristics of the actual local scene as compared to the modeled 
local scene. Let LSobjdenote the appearance of the local environment as cal­culated by the global illumination 
solution with the synthetic objects in place. We can compensate for the error if we compute our .nal 
rendering LSfinalas: LSfinal=LSobj,Err ls Equivalently, we can write: LSfinal=LSb+(LSobj,LSnoobj) In 
this form, we see that whenever LSobjand LSnoobjare the same (i.e. the addition of the objects to the 
scene had no effect on the local scene) the .nal rendering of the local scene is equivalent to LSb(e.g. 
the background plate). When LSobjis darker than LSnoobj, light is subtracted from the background to form 
shadows,  Figure 5: A Light-Based Model A simple light-based model of a room is constructed by mapping 
the image from a light probe onto a box. The box corresponds to the upper half of the room, with the 
bottom face of the box being coincident with the top of the table. The model contains the full dynamic 
range of the original scene, which is not reproduced in its entirety in this .gure. and when LSobjis 
lighter than LSnoobjlight is added to the back­ground to produce re.ections and caustics. Stated more 
generally, the appearance of the local scene without the objects is computed with the correct re.ectance 
characteristics lit by the correct environment, and the change in appearance due to the presence of the 
synthetic objects is computed with the modeled re.ectance characteristics as lit by the modeled environment. 
While the realism of LSfinalstill bene.ts from having a good model of the re.ectance characteristics 
of the local scene, the perceptual ef­fect of small errors in albedo or specular properties is considerably 
reduced. Fig. 8(g) shows a .nal rendering in which the local en­vironment is computed using this differential 
rendering technique. The objects are composited into the image directly from the LSobj solution shown 
in Fig. 8(c). It is important to stress that this technique can still produce abi­trarily wrong results 
depending on the amount of error in the es­timated local scene BRDF and the inaccuracies in the light-based 
model of the distance scene. In fact, Errmay be larger than ls LSobj, causing LSfinalto be negative. 
An alternate approach is to compensate for the relative error in the appearance of the local scene: LSfinal 
=LSb(LSobjILSnoobj). Inaccuracies in the local scene BDRF will also be re.ected in the objects. In the 
next section we discuss techniques for estimating the BRDF of the local scene.  7 Estimating the local 
scene BRDF Simulating the interaction of light between the local scene and the synthetic objects requires 
a model of the re.ectance characteristics of the local scene. Considerable recent work [32, 20, 8, 27] 
has pre­sented methods for measuring the re.ectance properties of mate­rials through observation under 
controlled lighting con.gurations. Furthermore, re.ectance characteristics can also be measured with 
commercial radiometric devices. It would be more convenient if the local scene re.ectance could be estimated 
directly from observation. Since the light-based model contains information about the radiance of the 
local scene as well as its irradiance, it actually contains information about the local scene re.ectance. 
If we hypothesize re.ectance characteristics for the lo­cal scene, we can illuminate the local scene 
with its known irradi­ance from the light-based model. If our hypothesis is correct, then the appearance 
should be consistent with the measured appearance. This suggests the following iterative method for recovering 
the re­.ectance properties of the local scene: 1. Assume a re.ectance model for the local scene (e.g. 
diffuse only, diffuse + specular, metallic, or arbitrary BRDF, including spatial variation) 2. Choose 
approximate initial values for the parameters of the re­.ectance model 3. Compute a global illumination 
solution for the local scene with the current parameters using the observed lighting con­.guration or 
con.gurations. 4. Compare the appearance of the rendered local scene to its ac­tual appearance in one 
or more views. 5. If the renderings are not consistent, adjust the parameters of the re.ectance model 
and return to step 3.  Ef.cient methods of performing the adjustment in step 5 that ex­ploit the properties 
of particular re.ectance models are left as future work. However, assuming a diffuse-only model of the 
local scene in step 1 makes the adjustment in step 5 straightforward. We have: ZZ 2 2 Lr1((r;qr)= PdLi((i;qi)cos(isin(id(idqi= 
00 ZZ 2 2 Pd Li((i;qi)cos(isin(id(idqi 00 If we initialize the local scene to be perfectly diffuse (Pd 
=1) everywhere, we have: ZZ 2 2 Lr2((r;qr)= Li((i;qi)cos(isin(id(idqi 00 The updated diffuse re.ectance 
coef.cient for each part of the lo­cal scene can be computed as: 0 Lr1((r;qr)P= dLr2((r;qr) In this manner, 
we use the global illumination calculation to ren­der each patch as a perfectly diffuse re.ector, and 
compare the re­sulting radiance to the observed value. Dividing the two quantities yields the next estimate 
of the diffuse re.ection coef.cient P0.If d there is no interre.ection within the local scene, then the 
P0 esti­ d mates will make the renderings consistent. If there is interre.ection, then the algorithm 
should be iterated until there is convergence. For a trichromatic image, the red, green, and blue diffuse 
re­.ectance values are computed independently. The diffuse charac­teristics of the background material 
used to produce Fig. 8(c) were  (a) Acquiring the background photograph light probe  (b) Using the 
light probe (d) Computing the global illumination solution Figure 7: Using a light probe (a) The background 
plate of the scene (some objects on a table) is taken. (b) A light probe (in this case, the camera photographing 
a steel ball) records the incident radiance near the location of where the synthetic objects are to be 
placed. (c) A simpli.ed light-based model of the distant scene is created as a planar surface for the 
table and a .nite box to represent the rest of the room. The scene is texture-mapped in high dynamic 
range with the radiance map from the light probe. The objects on the ta­ble, which were not explicitly 
modeled, become projected onto the table. (d) Synthetic objects and a BRDF model of the local scene are 
added to the light-based model of the distant scene. A global illumi­nation solution of this con.guration 
is computed with light coming from the distant scene and interacting with the local scene and syn­thetic 
objects. Light re.ected back to the distant scene is ignored. The results of this rendering are composited 
(possibly with differ­ential rendering) into the background plate from (a) to achieve the .nal result. 
computed using this method, although it was assumed that the entire local scene had the same diffuse 
re.ectance. In the standard plastic illumination model, just two more co­ef.cients those for specular 
intensity and roughness need to be speci.ed. In Fig. 8, the specular coef.cients for the local scene 
were estimated manually based on the specular re.ection of the window in the table in Fig. 2.  8 Compositing 
Results Fig. 5 shows a simple light-based model of a room constructed us­ing the panoramic radiance map 
from Fig. 2. The room model be­gins at the height of the table and continues to the ceiling; its mea­surements 
and the position of the ball within it were measured man­ually. The table surface is visible on the bottom 
face. Since the room model is .nite in size, the light sources are effectively local rather than in.nite. 
The stretching on the south wall is due to the poor sam­pling toward the silhouette edge of the ball. 
Figs. 4 and 6 show complex arrangements of synthetic objects lit entirely by a variety of light-based 
models. The selection and com­position of the objects in the scene was chosen to exhibit a wide vari­ety 
of light interactions, including diffuse and specular re.ectance, multiple soft shadows, and re.ected 
and focused light. Each ren­dering was produced using the RADIANCE system with two dif­fuse light bounces 
and a relatively high density of ambient sample points. Fig. 8(a) is a background plate image into which 
the synthetic ob­jects will be rendered. In 8(b) a calibration grid was placed on the table in order 
to determine the camera pose relative to the scene and to the mirrored ball, which can also be seen. 
The poses were deter­mined using the photogrammetric method in [10]. In 8(c), a model of the local scene 
as well as the synthetic objects is geometrically matched and composited onto the background image. Note 
that the local scene, while the same average color as the table, is readily dis­tinguishable at its edges 
and because it lacks the correct variations in albedo. Fig. 8(d) shows the results of lighting the local 
scene model with the light-based model of the room, without the objects. This image will be compared 
to 8(c) in order to determine the effect the syn­thetic objects have on the local scene. Fig. 8(e) is 
a mask image in which the white areas indicate the location of the synthetic objects. If the distant 
or local scene were to occlude the objects, such regions would be dark in this image. Fig. 8(f) shows 
the difference between the appearance of the lo­cal scene rendered with (8(c)) and without (8(d)) the 
objects. For il­lustration purposes, the difference in radiance values have been off­set so that zero 
difference is shown in gray. The objects have been masked out using image 8(e). This difference image 
encodes both the shadowing (dark areas) and re.ected and focussed light (light areas) imposed on the 
local scene by the addition of the synthetic objects. Fig. 8(g) shows the .nal result using the differential 
rendering method described in Section 6. The synthetic objects are copied directly from the global illumination 
solution 8(c) using the object mask 8(e). The effects the objects have on the local scene are in­cluded 
by adding the difference image 8(f) (without offset) to the background image. The remainder of the scene 
is copied directly from the background image 8(a). Note that in the mirror ball s re­.ection, the modeled 
local scene can be observed without the effects of differential rendering a limitation of the compositing 
tech­nique. In this .nal rendering, the synthetic objects exhibit a consistent appearance with the real 
objects present in the background image 8(a) in both their diffuse and specular shading, as well as the 
di­rection and coloration of their shadows. The somewhat speckled nature of the object re.ections seen 
in the table surface is due to  (a) Background photograph (b) Camera calibration grid and light probe 
 (c) Objects and local scene matched to background (d) Local scene, without objects, lit by the model 
 (e) Object matte (f) Difference in local scene between c and d (g) Final result with differential rendering 
Figure 8: Compositing synthetic objects into a real scene using a light probe and differential rendering 
  the stochastic nature of the particular global illumination algorithm used. The differential rendering 
technique successfully eliminates the border between the local scene and the background image seen in 
8(c). Note that the albedo texture of the table in the local scene area is preserved, and that a specular 
re.ection of a background object on the table (appearing just to the left of the .oating sphere) is cor­rectly 
preserved in the .nal rendering. The local scene also exhibits re.ections from the synthetic objects. 
A caustic from the glass ball focusing the light of the ceiling lamp onto the table is evident.  9 Future 
work The method proposed here suggests a number of areas for future work. One area is to investigate 
methods of automatically recov­ering more general re.ectance models for the local scene geome­try, as 
proposed in Section 7. With such information available, the program might also also be able to suggest 
which areas of the scene should be considered as part of the local scene and which can safely be considered 
distant, given the position and re.ectance character­istics of the desired synthetic objects. Some additional 
work could be done to allow the global illumina­tion algorithm to compute the ilumination solution more 
ef.ciently. One technique would be to have an algorithm automatically locate and identify concentrated 
light sources in the light-based model of the scene. With such knowledge, the algorithm could compute 
most of the direct illumination in a forward manner, which could dra­matically increase the ef.ciency 
with which an accurate solution could be calculated. To the same end, use of the method presented in 
[15] to expedite the solution could be investigated. For the case of compositing moving objects into 
scenes, greatly increased ef.­ciency could be obtained by adapting incremental radiosity methods to the 
current framework. 10 Conclusion We have presented a general framework for adding new objects to light-based 
models with correct illumination. The method lever­ages a technique of using high dynamic range images 
of real scene radiance to synthetically illuminate new objects with arbitrary re­.ectance characteristics. 
We leverage this technique in a general method to simulate interplay of light between synthetic objects 
and the light-based environment, including shadows, re.ections, and caustics. The method can be implemented 
with standard global il­lumination techniques. For the particular case of rendering synthetic objects 
into real scenes (rather than general light-based models), we have presented a practical instance of 
the method that uses a light probe to record inci­dent illumination in the vicinity of the synthetic 
objects. In addition, we have described a differential rendering technique that can con­vincingly render 
the interplay of light between objects and the local scene when only approximate re.ectance information 
for the local scene is available. Lastly, we presented an iterative approach for determining re.ectance 
characteristics of the local scene based on measured geometry and observed radiance in uncontrolled lighting 
conditions. It is our hope that the techniques presented here will be useful in practice as well as comprise 
a useful framework for com­bining material-based and light-based graphics. Acknowledgments The author 
wishes to thank Chris Bregler, David Forsyth, Jianbo Shi, Charles Ying, Steve Chenney, and Andrean Kalemis 
for the various forms of help and advice they provided. Special gratitude is also due to Jitendra Malik 
for helping make this work possible. Discussions with Michael Naimark and Steve Saunders helped motivate 
this work. Tim Hawkins provided extensive assistance on improving and revising this pa­per and provided 
invaluable assistance with image acquisition. Gregory Ward Larson deserves great thanks for the RADIANCE 
lighting simulation system and his invalu­able assistance and advice in using RADIANCE in this research, 
for assisting with re­.ectance measurements, and for very helpful comments and suggestions on the paper. 
This research was supported by a Multidisciplinary University Research Initiative on three dimensional 
direct visualization from ONR and BMDO, grant FDN00014-96-1­1200. References [1] ADELSON, E. H., AND 
BERGEN, J. R. Computational Models of Visual Pro­cessing. MIT Press, Cambridge, Mass., 1991, ch. 1. The 
Plenoptic Function and the Elements of Early Vision. [2] AZARMI, M. Optical Effects Cinematography: Its 
Development, Methods, and Techniques. University Micro.lms International, Ann Arbor, Michigan, 1973. 
[3] BLINN, J. F. Texture and re.ection in computer generated images. Communica­tions of the ACM 19, 10 
(October 1976), 542 547. [4] CHEN, E. QuickTime VR -an image-based approach to virtual environment nav­igation. 
In SIGGRAPH 95 (1995). [5] CHEN, S. E. Incremental radiosity: An extension of progressive radiosity to 
an interactive synthesis system. In SIGGRAPH 90 (1990), pp. 135 144. [6] COHEN,M.F.,CHEN,S.E.,WALLACE,J.R., 
AND GREENBERG,D.P.Apro­gressive re.nement approach to fast radiosity image generation. In SIGGRAPH 88 
(1988), pp. 75 84. [7] CURLESS, B., AND LEVOY, M. A volumetric method for building complex mod­els from 
range images. In SIGGRAPH 96 (1996), pp. 303 312. [8] DANA, K. J., GINNEKEN, B., NAYAR, S. K., AND KOENDERINK, 
J. J. Re­.ectance and texture of real-world surfaces. In Proc. IEEE Conf. on Comp. Vision and Patt. Recog. 
(1997), pp. 151 157. [9] DEBEVEC, P. E., AND MALIK, J. Recoveringhighdynamicrangeradiancemaps from photographs. 
In SIGGRAPH 97 (August 1997), pp. 369 378. [10] DEBEVEC, P. E., TAYLOR, C. J., AND MALIK, J. Modeling 
and rendering ar­chitecture from photographs: A hybrid geometry-and image-based approach. In SIGGRAPH 
96 (August 1996), pp. 11 20. [11] DEBEVEC, P. E., YU, Y., AND BORSHUKOV, G. D. Ef.cient view-dependent 
image-based rendering with projective texture-mapping. Tech. Rep. UCB//CSD­98-1003, University of California 
at Berkeley, 1998. [12] DRETTAKIS, G., ROBERT, L., AND BOUGNOUX, S. Interactive common illu­mination 
for computer augmented reality. In 8th Eurographics workshop on Ren­dering, St. Etienne, France (May 
1997), J. Dorsey and P. Slusallek, Eds., pp. 45 57. [13] FIELDING, R. The Technique of Special Effects 
Cinematography. Hastings House, New York, 1968. [14] FOURNIER, A., GUNAWAN, A., AND ROMANZIN, C. Common 
illumination between real and computer generated scenes. In Graphics Interface (May 1993), pp. 254 262. 
[15] GERSHBEIN, R., SCHRODER, P., AND HANRAHAN, P. Textures and radiosity: Controlling emission and re.ection 
with texture maps. In SIGGRAPH 94 (1994). [16] GORAL, C. M., TORRANCE, K. E., GREENBERG, D. P., AND BATTAILE, 
B. Modeling the interaction of light between diffuse surfaces. In SIGGRAPH 84 (1984), pp. 213 222. [17] 
GORTLER, S. J., GRZESZCZUK, R., SZELISKI, R., AND COHEN, M. F. The Lumigraph. In SIGGRAPH 96 (1996), 
pp. 43 54. [18] HECKBERT, P. S. Survey of texture mapping. IEEE Computer Graphics and Applications 6, 
11 (November 1986), 56 67. [19] KAJIYA, J. The rendering equation. In SIGGRAPH 86 (1986), pp. 143 150. 
[20] KARNER, K. F., MAYER, H., AND GERVAUTZ, M. An image based measure­ment system for anisotropic re.ection. 
In EUROGRAPHICS Annual Conference Proceedings (1996). [21] KOENDERINK,J.J., ANDVAN DOORN,A.J.Illuminancetextureduetosurface 
mesostructure. J. Opt. Soc. Am. 13, 3 (1996). [22] LAVEAU, S., AND FAUGERAS, O. 3-D scene representation 
as a collection of images. In Proceedings of 12th International Conference on Pattern Recognition (1994), 
vol. 1, pp. 689 691. [23] LEVOY, M., AND HANRAHAN, P. Light .eld rendering. In SIGGRAPH 96 (1996), pp. 
31 42. [24] MCMILLAN, L., AND BISHOP, G. Plenoptic Modeling: An image-based ren­dering system. In SIGGRAPH 
95 (1995). [25] NAKAMAE, E., HARADA, K., AND ISHIZAKI, T. A montage method: The over­laying of the computer 
generated images onto a background photograph. In SIG-GRAPH 86 (1986), pp. 207 214. [26] PORTER,T., AND 
DUFF,T.Compositingdigitalimages.In SIGGRAPH 84 (July 1984), pp. 253 259. [27] SATO, Y., WHEELER, M. D., 
AND IKEUCHI, K. Object shape and re.ectance modeling from observation. In SIGGRAPH 97 (1997), pp. 379 
387. [28] SMITH, T. G. Industrial Light and Magic: The Art of Special Effects. Ballantine Books, New 
York, 1986. [29] SZELISKI, R. Image mosaicing for tele-reality applications. In IEEE Computer Graphics 
and Applications (1996). [30] TURK, G., AND LEVOY, M. Zippered polygon meshes from range images. In SIGGRAPH 
94 (1994), pp. 311 318. [31] VEACH, E., AND GUIBAS, L. J. Metropolis light transport. In SIGGRAPH 97 
(August 1997), pp. 65 76. [32] WARD, G. J. Measuring and modeling anisotropic re.ection. In SIGGRAPH 
92 (July 1992), pp. 265 272. [33] WARD, G. J. The radiance lighting simulation and rendering system. 
In SIG-GRAPH 94 (July 1994), pp. 459 472. [34] WATANABE, M., AND NAYAR, S. K. Telecentric optics for 
computational vi­sion. In Proceedings of Image Understanding Workshop (IUW 96) (February 1996). [35] 
Y.CHEN, AND MEDIONI, G. Object modeling from multiple range images. Im­age and Vision Computing 10, 3 
(April 1992), 145 155.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>ONR and BMDO</funding_agency>
			<grant_numbers>
				<grant_number>FDN00014-96-1-1200</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401176</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<pages>3</pages>
		<display_no>33</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[A median cut algorithm for light probe sampling]]></title>
		<page_from>1</page_from>
		<page_to>3</page_to>
		<doi_number>10.1145/1401132.1401176</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401176</url>
		<abstract>
			<par><![CDATA[<p>We present a technique for approximating a light probe image as a constellation of light sources based on a median cut algorithm. The algorithm is efficient, simple to implement, and can realistically represent a complex lighting environment with as few as 64 point light sources.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098698</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882314</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Agarwal, S., Ramamoorthi, R., Belongie, S., and Jensen, H. W. 2003. Structured importance sampling of environment maps. <i>ACM Transactions on Graphics 22</i>, 3 (July), 605--612.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cohen, J. M., and Debevec, P. 2001. The LightGen HDRShop plugin. http://www.hdrshop.com/main-pages/plugins.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808600</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Crow, F. C. 1984. Summed-area tables for texture mapping. In <i>Computer Graphics (Proceedings of SIGGRAPH 84)</i>, vol. 18, 207--212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801294</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P. 1982. Color image quantization for frame buffer display. In <i>SIGGRAPH '82: Proceedings of the 9th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 297--307.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882411</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Kollig, T., and Keller, A. 2003. Efficient illumination by high dynamic range images. In <i>Eurographics Symposium on Rendering: 14th Eurographics Workshop on Rendering</i>, 45--51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015750</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ostromoukhov, V., Donohue, C., and Jodoin, P.-M. 2004. Fast hierarchical importance sampling with blue noise properties. <i>ACM Transactions on Graphics 23</i>, 3 (Aug.), 488--495.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Median Cut Algorithm for Light Probe Sampling Paul Debevec* USC Institute for Creative Technologies 
Presented as Poster at SIGGRAPH 2005 ABSTRACT We present a technique for approximating a light probe 
image as a constellation of light sources based on a me­dian cut algorithm. The algorithm is ef.cient, 
simple to implement, and can realistically represent a complex lighting environment with as few as 64 
point light sources. Introduction The quality of approximating an image-based lighting (IBL) environment 
as a .nite number of point lights is increased if the light positions are chosen to follow the distribu­tion 
of the incident illumination; this has been a goal of previous strati.ed sampling approaches [Cohen and 
Debevec 2001; Kollig and Keller 2003; Agarwal et al. 2003; Ostromoukhov et al. 2004]. In this work, we 
show that subdividing the image into regions of equal energy achieves this property and yields a well-conditioned 
and easy to implement static sampling algorithm. Figure 1: The Grace Cathedral light probe subdivided 
into 64 re­gions of equal light energy using the median cut algorithm. The small circles are the 64 light 
sources chosen as the energy centroids of each region; the lights are all approximately equal in energy. 
Algorithm Taking inspiration from Paul Heckbert s median­cut color quantization algorithm [Heckbert 1982], 
we can partition a light probe image in the rectangular latitude-longitude format into 2n regions of 
similar light energy as follows: 1. Add the entire light probe image to the region list as a single region. 
 2. For each region in the list, subdivide along the longest dimen­sion such that its light energy is 
divided evenly. 3. If the number of iterations is less than n, return to step 2. 4. Place a light source 
at the center or centroid of each region, and set the light source color to the sum of pixel values within 
the region.  Implementation Calculating the total energy within regions of the image can be accelerated 
using a summed area table [Crow 1984]. Computing the total light energy is most naturally per­formed 
on a monochrome version of the lighting environment rather than the RGB pixel colors; such an image can 
be formed as a weighted average of the color channels of the light probe image, e.g. Y = 0.2125R + 0.7154G 
+ 0.0721B following ITU-R Recom­mendation BT.709. While the partitioning decisions are made on the monochrome 
image, the light source colors are computed using the corresponding regions in the original RGB image. 
The latitude-longitude mapping over-represents regions near the poles. To compensate, the pixels of the 
probe image should .rst be scaled by cosf where f is the pixel s angle of inclination. Deter­mining the 
longest dimension of a region should also take the over­representation into account; this can be accomplished 
by weighting a regions width by cos f for an inclination f at center of the region. Results Fig. 1 shows 
the Grace Cathedral lighting environ­ment partitioned into 64 light source regions, and Fig. 2 shows 
a small diffuse scene rendered with 16, 64, and 256 light sources chosen in this manner. Using 64 lights 
produces a close approxi­mation to a computationally expensive Monte Carlo solution, and the 256-light 
approximation is nearly indistinguishable.  Figure 2: (a-c) Noise-free renderings in the Grace Cathedral 
en­vironment approximated by 16, 64, and 256 light sources. (d) A not quite noise-free Monte Carlo rendering 
using 4096 randomly chosen rays per pixel. Conclusion The median cut technique is extremely fast com­pared 
to most other sampling techniques and produces noise-free renderings at the expense of bias inversely 
proportional to the num­ber of light sources used. In future work we will investigate the stability of 
the technique for animated lighting environments and explore adaptations for scenes with general BRDFs. 
References AGARWAL, S., RAMAMOORTHI, R., BELONGIE, S., AND JENSEN, H. W. 2003. Structured importance 
sampling of environment maps. ACM Transactions on Graphics 22, 3 (July), 605 612. COHEN, J. M., AND DEBEVEC, 
P. 2001. The LightGen HDRShop plugin. http://www.hdrshop.com/main-pages/plugins.html. CROW, F. C. 1984. 
Summed-area tables for texture mapping. In Computer Graphics (Proceedings of SIGGRAPH 84), vol. 18, 207 
212. HECKBERT, P. 1982. Color image quantization for frame buffer display. In SIG-GRAPH 82: Proceedings 
of the 9th annual conference on Computer graphics and interactive techniques, ACM Press, New York, NY, 
USA, 297 307. KOLLIG, T., AND KELLER, A. 2003. Ef.cient illumination by high dynamic range images. In 
Eurographics Symposium on Rendering: 14th Eurographics Workshop on Rendering, 45 51. OSTROMOUKHOV, V., 
DONOHUE, C., AND JODOIN, P.-M. 2004. Fast hierarchical importance sampling with blue noise properties. 
ACM Transactions on Graphics 23, 3 (Aug.), 488 495. *Email: debevec@ict.usc.edu Web: www.debevec.org/MedianCut/ 
 www.debevec.org/MedianCut/  (a) 4 regions (b) 16 regions Figure 1: The Grace Cathedral light probe 
subdivided into 4, 16, 64, and 256 regions of equal light energy using the median cut algorithm. (a) 
4 lights (b) 16 lights Figure 2: The Grace Cathedral light probe represented as 4, 16, 64, and 256 light 
sources chosen as the energy centroids of each region; each light is approximately equal energy. 1 www.debevec.org/MedianCut/ 
 (a) 4 lights (b) 16 lights Figure 3: Noise-free renderings in the Grace Cathedral environment approximated 
by 4, 16, 64, and 256 light sources. (b) a not quite noise-free Monte Carlo rendering using 4096 randomly 
chosen rays per pixel. 2 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401177</section_id>
		<sort_key>450</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: HDRi for artists]]></section_title>
		<section_page_from>12</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098699</person_id>
				<author_profile_id><![CDATA[81365595275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kirt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098700</person_id>
				<author_profile_id><![CDATA[81421595753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bloch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098701</person_id>
				<author_profile_id><![CDATA[81365594611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hilmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098702</person_id>
				<author_profile_id><![CDATA[81421597847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Zap]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andersson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098703</person_id>
				<author_profile_id><![CDATA[81365590839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[M]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401178</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<pages>6</pages>
		<display_no>34</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[High dynamic range imaging for artists]]></title>
		<page_from>1</page_from>
		<page_to>6</page_to>
		<doi_number>10.1145/1401132.1401178</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401178</url>
		<abstract>
			<par><![CDATA[<p>This course is to intended aid in the practical application of high dynamic range imaging techniques. This course will cover a brief overview of HDRI, pre-production techniques, production techniques, and post-production techniques. Finally, examples of how HDRI has been used in the motion picture and broadcast industries will also be shown.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098704</person_id>
				<author_profile_id><![CDATA[81365595275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kirt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Savannah College of Art and Design, Savannah, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098705</person_id>
				<author_profile_id><![CDATA[81421595753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bloch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[EdenFX, Hollywood, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098706</person_id>
				<author_profile_id><![CDATA[81365594611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hilmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098707</person_id>
				<author_profile_id><![CDATA[81421597847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Zap]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andersson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[mental images]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098708</person_id>
				<author_profile_id><![CDATA[81365590839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[M]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[visualZ, LLC (Owner & Artistic/Technical Director)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[The HDRI Galleries on http://www.hdrlabs.com/gallery/index.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Picturenaut on http://www.hdrlabs.com/picturenaut/index.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Smart IBL on http://www.hdrlabs.com/sibl/index.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bracketing Chart on http://employeepages.scad.edu/~kwitte/documents/Info_Sheets/Witte_Bracket_Chart.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Pano Calculator: http://www.hdrlabs.com/tools/panocalc.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[http://mentalraytips.blogspot.com/2007/03/understanding-photometric-and.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[http://mentalraytips.blogspot.com/search/label/production%20library]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kirt Witte's HDRI-related pages: http://employeepages.scad.edu/~kwitte/html/HDRI.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High Dynamic Range Imaging for Artists (SIGGRAPH 2008, Los Angeles, CA USA)  Kirt Witte (Course Organizer) 
Professor of Visual Effects School of Film and Digital Media Savannah College of Art and Design P.O. 
Box 3146 Savannah, GA 31402 USA Telephone: +1 (912) 525-8534 kwitte@scad.edu http://TheOtherSavannah.com 
http://employeepages.scad.edu/~kwitte/  Christian Bloch EdenFX 1438 N.Gower Street Box 19, Building 
50 Hollywood CA 90028 phone: (323) 993-7050 (Author of The HDRI Handbook) www.HDRLabs.com Blochi@HDRLabs.com 
 Hilmar Koch Industrial Light &#38; Magic http://www/ilm.com  Zap Andersson mental images http://www.mentalimages.com 
 Gary M Davis Autodesk (3D Animation &#38; Compositing Specialist) visualZ, LLC (Owner &#38; Artistic/Technical 
Director) http://www.visualZ.com   Course Abstract: This course is to intended aid in the practical 
application of high dynamic range imaging techniques. This course will cover a brief overview of HDRI, 
pre-production techniques, production techniques, and post-production techniques. Finally, examples of 
how HDRI has been used in the motion picture and broadcast industries will also be shown.  Prerequisites: 
 Participants should be familiar with basic techniques in digital photography and/or with basic computer 
graphics modeling and rendering. Basic compositing would also be helpful, but not required. Familiarity 
with specific image-editing and 3D modeling and rendering packages would be helpful. Prior knowledge 
of HDRI techniques and terms would be beneficial, but not required.  Level of Difficulty: Intermediate. 
 Intended Audience: This course is intended for 3D artists, photographers, compositors, students, educators, 
and anyone interested in visual effects, rendering, and video game design.  Course Syllabus: 8:30 8:40 
(Witte) Introduction: Welcome and Speaker Introductions  10 minutes 8:40 8:50 (Bloch) What is HDRI? 
Why do we need it? Demo of some immediate advantages in Photoshop CS3 20 minutes 9:00 9:10 (Bloch) File 
Formats, Bit Depths, Raw Versus JPEG, all Radiance files are not the same. 10 minutes 9:10 9:25 (Witte) 
Shooting, manipulating, and implementing chrome balls 15 minutes 9:25 9:35 (Bloch) Creating Radiance 
files <Picturenaut, Photoshop, PhotoSphere> gamma issues 10 minutes 9:35 9:55 (Bloch) HDRI usage at 
EdenFX Behind the scenes 20 minutes 9:55 10:00 (Bloch and Witte) Practical shooting advice Do s and 
Dont s Deadlines, Grey Cards, Calibration, Leveling, Fisheye vs Rectilinear (Tell Audience: READ the 
HDRI FAQ!) 5 minutes 10:00 to 10:15 BREAK 10:15 10:30 (Witte) Tonemapping examples, (local versus global) 
<Photoshop, Picturenaut, &#38; Photomatix Pro> 15 minutes 10:30 10:45 (Witte &#38; Bloch) Shooting &#38; 
stitching photographic segmental spherical panoramas - Nodal points, lens choice, stitching, masking, 
locking everything down, shooting fast <Realviz Stitcher &#38; PtGuiPro> 15 minutes 10:45 10:55 (Witte) 
Creating cgi HDRI spherical panoramas <Vue6 > 10 minutes 10:55 11:05 (Witte) Panoramic Photographic 
Art Panoramic Conversions Panoramic Warping < HDRshop, Stitcher, &#38; Flexify 2 > 10 minutes 11:05 
 11:25 (Andersson) HDRI in Mental Ray, What does that button actually do? Tips &#38; Tricks for tweaking 
HDRIs <Mental Ray, 3DS Max> 20 minutes 11:25 11:45 (Davis) 32-bit compositing: A new compositing paradigm 
< Toxxik, 3DS Max > 20 minutes 11:45 12:15 (Koch) Behind the Scenes: HDRI in the movie Transformers 
The reality of dynamic range, ad hoc vs. meticulous HDR capture, the hazards of film production, using 
HDRI images, pretending you are using HDRI images (faking dynamic range). 30 minutes 12:15 - 12:30 (All) 
Questions &#38; Answers  Course Presenter Information:  Kirt Witte is a professional photographer 
and a Professor of Visual Effects at the Savannah College of Art and Design, in Savannah, Georgia, USA. 
He received his B.S. in Photography in 1991 and his M.F.A. in Computer Art in 2005. He has been shooting 
panoramic photography since 1992 is a member of the International VR Photography Association. Witte has 
been involved with HDRI since 2002 and has been teaching a class High Dynamic Range Imaging at the Savannah 
College of Art and Design since 2005. In 2006, he won 1st Place (Abstract Category) in the International 
Color Photography Awards. He has worked in advertising, internet, and the video game industries and continues 
to do freelance work. Witte is currently finishing production of his first photography book called, The 
Other Savannah . To see his work visit his website at http://TheOtherSavannah..com or visit his faculty 
website at http://employeepages.scad.edu/~kwitte/  Christian Bloch is an acclaimed Visual Effects Artist 
who works for Eden FX in Hollywood, California. His work can be seen in StarTrek:Enterprise, Smallville, 
Invasion, Lost, 24, and a growing number of movies and commercials. He has been a pioneer in the practical 
application of HDRI in post-production, specifically under the budgetary and time restraints of TV production. 
A native of Germany, Bloch earned a degree in multimedia technology. Years of research and development 
went into his diploma thesis about HDRI, which was honored with the achievement award of the University 
of Applied Sciences Leipzig. Since his thesis was published online in July 2004, it has been downloaded 
more than 15,000 times, and it has been established as the primary source of information on HDRI in Germany. 
The HDRI HandBook is the successor to Bloch s diploma thesis, rewritten completely from the ground up 
in English and heavily expanded and updated. The German and French versions of the The HDRI Handbook 
are currently in production. To find out more, please visit: http://www.hdrlabs.com/book/index.html 
   Hilmar Koch, Computer Graphics Supervisor, joined Industrial Light &#38; Magic in 1998 as a Senior 
Technical Director. He holds an undergraduate degree in Arts from Columbia College Chicago and in Mathematics 
from the Technical University in Munich. Prior to joining ILM, he worked as a digital effects supervisor 
for Blue Sky Studios in New York. As a CG Supervisor, Koch specializes in rendering, computer lighting, 
digital effects, synthetic humans and digital environments. He collaborates closely with various departments, 
namely the CG artist group and the ILM stage and model shop. Koch is a liaison within ILM and in that 
role helps define the development plan for ILM's proprietary software. Koch is a native of Germany, born 
in Esslingen am Neckar and grew up in Munich. He has lived in the United States since 1994 and currently 
resides in Oakland, California with his wife and daughter. Feature Film Credits include: TRANSFORMERS 
( Computer Graphics Supervisor ) PIRATES OF THE CARIBBEAN: DEAD MAN S CHEST- Technical Director THE CHRONICLES 
OF NARNIA: THE LION, THE WITCH AND THE WARDROBE Technical Director CHICKEN LITTLE Computer Graphics 
Supervisor THE ADVENTURES OF SHARK BOY AND LAVA GIRL Computer Graphics Supervisor THE HULK - Sequence 
Supervisor HARRY POTTER AND THE SORCERER S STONE - Sequence Supervisor PEARL HARBOR - Technical Director 
THE PERFECT STORM- Technical Director GALAXY QUEST - Technical Director THE DEEP BLUE SEA - Technical 
Director WILD WILD WEST - Technical Director JACK FROST - Technical Director Hakan 'Zap' Andersson has 
been working as "Shader Wizard" at mental images since 2004 and is the author of numerous mental ray 
shaders, such as the the subsurface/skin shaders, the car paint shader, as well as the architectural- 
and production shader libraries. Prior to mental images, Zap worked at EMT, Genius CAD Software, and 
Autodesk where he wrote software and designed user friendly interfaces for advanced mechanical design 
software, as well as authored two US patents. Originally educated as an Engineer in Electronics, Zap's 
passion for computer graphics caused his graduation year "special project" to be an actual hand-built 
and hand-wired graphics card, for which he wrote his first ray tracer - a program that eventually evolved 
into his own rendering engine RayTracker. Today Zap spends his days (and nights) writing shaders, documentation 
and tutorials for mental ray, and sometimes makes presentations at user events and conventions, as well 
as maintains a mental ray tips blog (mentalraytips.blogspot.com). Occasionally he also makes little experimental 
visual effects movies, and he has a background in music production. Gary M. Davis began his career in 
after receiving a BFA in Computer Graphics from Bowling Green State University in 1992. Since that time, 
he has been heavily involved in visual effects and motion graphics for numerous clients in television, 
film, video games and architectural visualization projects. After spending nearly six-years developing 
ride films and digital photography systems for themed entertainment venues, at the turn of the Millennium 
he formed visualZ, LLC. From 2004-2007, Davis served as the only independent, worldwide certified training 
specialist for 3ds Max, Combustion and Toxxik. During this time he also authored The Focal Easy Guide 
to Combustion. At the Siggraph 2007 Conference, Davis was awarded the title of Autodesk 3ds Max Master 
2007. Shortly thereafter, he joined the Media and Entertainment Division of Autodesk as specialist for 
3D animation and compositing software solutions. He maintains visualZ as a consulting and training boutique 
in Orlando, Florida. http://www.visualZ.com Bibliography: Some of the extant materials the course will 
leverage are: ° The HDRI Galleries on http://www.hdrlabs.com/gallery/index.html ° Picturenaut on http://www.hdrlabs.com/picturenaut/index.html 
° Smart IBL on http://www.hdrlabs.com/sibl/index.html ° Bracketing Chart on http://employeepages.scad.edu/~kwitte/documents/Info_Sheets/Witte_Bracket_Chart.pdf 
° Pano Calculator: http://www.hdrlabs.com/tools/panocalc.html ° http://mentalraytips.blogspot.com/2007/03/understanding-photometric-and.html 
° http://mentalraytips.blogspot.com/search/label/production%20library Kirt Witte s HDRI-related pages: 
http://employeepages.scad.edu/~kwitte/html/HDRI.html 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401179</section_id>
		<sort_key>470</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Professional Development and Education: Introduction to computer graphics]]></section_title>
		<section_page_from>13</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098709</person_id>
				<author_profile_id><![CDATA[81100017052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glassner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401180</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<pages>356</pages>
		<display_no>35</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[An introduction to modeling]]></title>
		<page_from>1</page_from>
		<page_to>356</page_to>
		<doi_number>10.1145/1401132.1401180</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401180</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098710</person_id>
				<author_profile_id><![CDATA[81100017052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glassner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Coyote Wind Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401181</section_id>
		<sort_key>490</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Professional Development and Education: Introduction to SIGGRAPH and interactive computer graphics]]></section_title>
		<section_page_from>14</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098711</person_id>
				<author_profile_id><![CDATA[81100645484]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401182</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>175</pages>
		<display_no>36</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Introduction to shape grammars]]></title>
		<page_from>1</page_from>
		<page_to>175</page_to>
		<doi_number>10.1145/1401132.1401182</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401182</url>
		<abstract>
			<par><![CDATA[<p>The theory of shape grammars defines a formalism to address the ambiguity that quantitative and symbolic computations mostly help us rule out in creative processes. The theory was first launched by Stiny and Gips in 1972 and has evolved into a groundbreaking pragmatist philosophy of shape and design since.</p> <p>The course, composed of a 2 hour lecture and an optional one-day workshop for 10-12 participants, introduces the fundamentals of the theory and optionally a venue for attendees to put these to practice in a hands-on workshop. The lecture will focus on giving some basic knowledge of shapes, shape algebras, and shape rules in order to explain how shape grammars translate visual and spatial thinking into design computation. Multiple examples of generative designs produced using shape grammars will be presented. The workshop consists of one exercise where participants will explore spatial relations between a number of shapes, leading to the production of a series of designs to be built by hand, out of a prescribed material such as wooden blocks or paper.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098712</person_id>
				<author_profile_id><![CDATA[81365594108]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[&#214;zkar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Middle East Technical University, Ankara, Turkey]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098713</person_id>
				<author_profile_id><![CDATA[81421594224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sotirios]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kotsopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chomsky, N. 1957, <i>Syntactic Structures</i>, Mouton, The Hague]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Duarte, J P, 2005, Towards the customization of mass-housing: the grammar of Siza's houses at Malagueira, <i>Environment and Planning B: Planning and Design</i>, 32, pp. 347--380]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Flemming, U, 1987a, More than the sum of parts: the grammar of Queen Anne houses, <i>Environment and Planning B:Planning and Design 14</i> pp. 323--350]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>109152</ref_obj_id>
				<ref_obj_pid>109149</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Flemming, U, 1990, Syntactic Structures in Architecture, The Electronic Design Studio, MIT Press, Cambridge pp. 31--47]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Knight, T, 1980, The generation of Hepplewhite-style chair back designs, <i>Environment and Planning B: Planning and Design 7</i> pp. 227--238]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Knight, T, 1986, 'Transformation of the Meander Motif on Greek Geometric Pottery' <i>Design Computing 1</i> pp. 29--67]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Knight, T, 1989, 'Transformations of De Stijl art: the paintings of Georges Vantongerloo and Fritz Glarner' <i>Environment and Planning B: Planning and Design 16</i> pp. 51--98]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Koning H, and Eizenberg, J, 1981, The language of the prairie: Frank Lloyd Wright's prairie houses, <i>Environment and Planning B: Planning and Design 8</i> pp. 295--323]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kotsopoulos, S D, Constructing Design Concepts: A computational approach to the synthesis of architectural form, Doctorate Dissertation, Massachusetts Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Sass, L, 2007, A Palladian construction grammar-design reasoning with shape grammars and rapid prototyping <i>Environment and Planning B: Planning and Design 34</i> pp. 87--106]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Stiny, G, 1977, Ice-ray: a note on Chinese lattice designs, <i>Environment and Planning B4</i> pp. 89--98]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Stiny, G, 1980, Kindergarten grammars: designing with Froebel's building gifts, <i>Environment and Planning B 3</i>, pp. 409--462]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Stiny, G 2006, <i>Shape</i>, MIT Press]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Stiny, G and Gips, J, 1972, Shape Grammars and the Generative Specification, Petrocelli OR (ed) <i>Best computer papers of 1971</i>, pp. 125--135]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Stiny, G and Mitchell, W J, 1978, The Palladian grammar, <i>Environment and Planning B 5</i> pp. 5--18]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 INTRODUCTION TO SHAPE GRAMMARS SIGGRAPH 2008 Mine Özkar, Assistant Professor, Ph.D. Middle East Technical 
University Faculty of Architecture Department of Architecture Ankara, 06531 TURKEY Contact email: ozkar@metu.edu.tr 
 Sotirios Kotsopoulos, Ph.D. Digital Design Fabrication Group School of Architecture and Planning Massachusetts 
Institute of Technology Cambridge, MA 02139 USA INTRODUCTION TO SHAPE GRAMMARS Lecturers: Mine Ozkar 
and Sotirios Kotsopoulos Workshop instructors: Mine Ozkar and Sotirios Kotsopoulos The theory of shape 
grammars defines a formalism to address the ambiguity that quantitative and symbolic computations mostly 
help us rule out in creative processes. The theory was first launched by Stiny and Gips in 1972 and has 
evolved into a groundbreaking pragmatist philosophy of shape and design since. The course, composed of 
a 2 hour lecture and an optional one-day workshop for 10-12 participants, introduces the fundamentals 
of the theory and optionally a venue for attendees to put these to practice in a hands-on workshop. The 
lecture will focus on giving some basic knowledge of shapes, shape algebras, and shape rules in order 
to explain how shape grammars translate visual and spatial thinking into design computation. Multiple 
examples of generative designs produced using shape grammars will be presented. The workshop consists 
of one exercise where participants will explore spatial relations between a number of shapes, leading 
to the production of a series of designs to be built by hand, out of a prescribed material such as wooden 
blocks or paper. Prerequisites No prerequisites other than enthusiasm for shapes and keen interest in 
looking and seeing. SYLLABUS INTRODUCTION TO SHAPE GRAMMARS Lecturers: Mine Ozkar and Sotirios Kotsopoulos 
Class time: 1 hr 45 mins. Topics: Part I The theory 1. What are shape grammars? 2. Describing shape 
grammars in terms of seeing and counting 3. Describing shape grammars as a rule-based system 4. Decompositions 
 5. The mathematical set-up of shape grammars 6. Basic elements: shapes, labels, weights 7. Shape algebras 
 8. Shape boundaries 9. Part relations: embedding, overlapping, discrete elements 10. Euclidean transformations 
 11. Maximal shapes 12. Boolean operations on shapes  Part II Applications in design and design education 
13. Recapitulation of the main computational devices 13. Recapitulation of the main computational devices 
of shape grammars 14. Shape grammar applications in design analysis 15. Shape grammar applications 
in design synthesis  Mine Özkar, Ph.D., METU Sotirios Kotsopoulos, Ph.D., MIT Slide 1  Part I The 
theory 1. What are shape grammars? 2. Describing shape grammars in terms of seeing and counting 3. 
Describing shape grammars as a rule-based system 4. Decompositions 5. The mathematical set-up of shape 
grammars 6. Basic elements: shapes, labels, weights 7. Shape algebras 8. Shape boundaries 9. Part 
relations: embedding, overlapping, discrete elements 10. Euclidean transformations 11. Maximal shapes 
 12. Boolean operations on shapes  Part II Applications in design and design education 13. Recapitulation 
of the main computational devices of shape grammars 14. Shape grammar applications in design analysis 
 15. Shape grammar applications in design synthesis  Slide 2 Outline of the class showing the topics 
addressed.   a) A computation theory that defines a formalism to represent visual (and spatial) thinking; 
that handles ambiguities which symbols do away with. b) A philosophy of looking at the world that is 
not through learnt or imposed definitions but through those that have a practical meaning at a given 
point in time; that values the continuity of matter and flexibility in how to cut it up into its parts. 
 Slide 4 Shape grammars may be described at two levels. Firstly, it is a computation theory that defines 
a formalism to represent visual, or even spatial, thinking. At the same time, it handles ambiguities 
which conventional digital computing does away with. The phrase shape grammar more literally refers to 
visual design grammars. At the second level, the theory represents a philosophy of looking at the world 
that is not through learnt or imposed decompositions (definitions) but through those that have a practical 
meaning at that point in time. Slide 5 Shape grammars were first introduced in the beginning of the 
70s by George Stiny and James Gips. Published as one of the best computer papers of 1971, their Shape 
Grammars and Generative Specification paper introduced a set of generative rules for a few paintings 
done by Stiny himself. The three paintings in the article, are from a series called Urform. These are 
going to be the basis for illustrating various concepts of shape grammars in this class. Slide 6 Stiny 
(2006) claims that design is calculating while expanding the meaning of calculation to visual thinking 
via his theory of shape grammars. The motto design is calculating, was a starting point in 1971 as well. 
The reasoning behind a visual product was described using a grammar-like formalism with a vocabulary, 
a set of rules, and a series of computations that produced designs as if they were sentences . Computing 
---computare (to count) Calculating ---calculus (pebble) Slide 7 Stiny often equates the terms design, 
visual reasoning and calculation. This claim firstly enunciates an understanding that design has reasoning 
within. Secondly, in the theory of shape grammars, the terms calculation and computation, which are often 
interchangeably used, are seen under a new light. Counting is at the root of computing and calculating. 
Visual calculation on the other hand, gives room for seeing as well as for counting. How does one calculate 
with shapes? Urform II, George Stiny, 1970, acryclic on canvas 30 ins x 57 ins, blue, red, orange, yellow. 
 Slide 8 How does one calculate with shapes? Do visual kinds of thinking exclude calculation? Or does 
calculation reduced to counting exclude visual and spatial kinds of thinking? Stiny argues that one has 
to really see in order to count and that seeing is where creativity lies. Counting requires discrete 
parts.   But the painting is not simply the sum of discrete parts known beforehand. Slide 11 Alternatively, 
one can divide Urform II into some obvious parts, distinct therefore countable. However, the painting 
is possibly a much more dynamically formed formal arrangement and is not simply a sum of discrete parts 
that were known before hand.  One can see different parts to count.  Slide 12 There are always some 
other parts to see. Moreover, these may be the meaningful parts, or parts that are surprisingly merged 
with one another. In the visual world, there are wholes that coexist, and they share parts, or parts 
of parts. This image shows a part that is not readily there but can be seen.  Once seen, parts can be 
counted.  Slide 13 Calculation then, is to see first, then count. This way, we can calculate with different 
parts each time we look at Urform II. The shape shown exists in ten instances in Urform II: one large, 
nine small ones.  Varying parts and wholes coexist.  Slides 14  Parts and wholes coincide.  Slide 
15  The shape grammar way of seeing and counting is visual rules that tell: see the left side and then 
replace it with what is on the right. Slide 16 Stiny and Gip s explanation for the process behind the 
Urform series is a visual rule that tells one to see the left side to replace it with the right side. 
The illustration shows one such possible rule. These kinds of rules form the basis of shape grammars. 
 I can see two instances of the shape on the left side of the rule. Slide 17 This is how it basically 
works. Looking for the left side in an initial shape set, in this case Urform I, one can see two instances 
of it.  I apply my rule to one of them.  Slide 18 The second one is rotated 180o.   Or to both. 
 A . B Shape grammars is a rule-based formalism. Rules show the particular shapes to be replaced and 
the manner in which they are replaced. The marker shows how to align the two shapes. Rather than if A, 
then B, visual rules say see SHAPE1, do SHAPE2. Slide 21 A shape rule has two steps when applied: a 
recognition of a particular shape shown on the left side and its possible replacement shown on the right 
side. The defined rule is operational. The arrow indicates an action. The unique feature of a shape rule 
is that the left and right side are visually considered. As opposed to symbols, shapes can be looked 
at and seen differently.   Slide 22 Because shapes are visual, they can be decomposed in infinitely 
many ways. There should be no preconceived decompositions and primitives acquired through such operations. 
Visual rules, which are subjective, will call for various decompositions. For example, let us look at 
one of the most popular examples Stiny (2006) gives to explain why we need to be computing with visual 
rules. There is a shape, composed of three triangles that will be rotated around its center. The only 
catch is, it will be rotated by a rule that says rotate triangle.  Slide 23  The visual rule: rotate 
an equilateral triangle 180o around its center. Slide 24 Slide 25 In the nine step computation, Stiny 
shows that the initial definition of the shape, that is three triangles , changes in step 4 and then 
back again in step 6. Decompositions should not be timeless. The initial shape could have been drawn 
as three triangles, six lines, or 9 lines. Whatever the history, a new definition can always come up 
while working with shapes. Ambiguity should be maintained. Shapes, Labels and Weights Shape algebras 
Boundaries of shapes Part relations of shapes Euclidean transformations Maximal shapes Boolean operations 
with shapes  Slide 26 The mathematical set-up of the theory includes general definitions of shapes, 
shape, weight and label algebras, shape boundaries, part relations, Euclidean transformations, maximal 
shapes, and Boolean operations with shapes.  Basic elements of shapes are points, lines, planes, and 
solids, with labels, if necessary, to give abstract information about them, and weights, as indicators 
of magnitudes of some formal attributes. Slide 27 Shapes can be points, lines, planes, solids or combinations 
of these. Shapes also can have labels that indicate additional information about them and weights that 
indicate the magnitude of some formal properties. Labels are useful for adding more constraints necessary 
for tasks such as establishing the order in which rules are applied in computations. U0 0 U0 1 U0 2 
U0 3 U1 1 U1 2 U1 3 U2 2 U2 3 U3 3 Shapes are categorized under different shape algebras. The left index 
shows the dimension of the basic elements, and the right index shows thedimension in which these basic 
elements are combined in shapes. Slide 28 Basic elements in shapes are categorized under different shape 
algebras. The indices indicate the dimension of the basic element and the dimension in which these elements 
are combined and transformed. U0 0 U0 1 U0 2 U0 3 U1 1 U1 2 U1 3 U2 2 U2 3 U3 3 Atomic algebras (of 
points in space)  Slide 29 All shape algebras that have 0 for the first index are atomic. A basic element 
within these algebras can only be a point and has no parts other than itself. Symbols, for example, are 
elements of these algebras and have a dimension of zero. Also, units that add up to a sum of units belong 
in these algebras but in those that have the second index higher than 1. U0 0 U0 1 U0 2 U0 3 U1 1 U1 
2 U1 3 U2 2 U2 3 U3 3 Boolean algebra (of zeroes and ones)  Slide 30 The algebra where both indices 
are 0 is Boolean. There are only two values, null and one. Something either is or is not. U0 0 U0 1 
U0 2 U0 3 U1 1 U1 2 U1 3 U2 2 U2 3 U3 3 Algebras with part relations  Slide 31 All algebras with the 
indices equal to or larger than one, show different properties than atomic algebras. They do not have 
atoms but shapes with parts such as lines, planes, solids, etc. The number of members within a set in 
one of those algebras does not have to be finite. For example, in algebra U11, on a line space, there 
can be infinitely many lines of different lengths. Algebra Basic Number Boundary elements of parts shapes 
U0 j points finite none U1 j lines infinite U0 j U2 j planes infinite U1 j U3 j solids infinite U2 j 
 Slide 32 There is a clear relation between the categories of basic elements belonging to different 
algebras. The boundaries of solids are plane shapes, the boundaries of planes are line shapes, the boundaries 
of lines are points whereas points have no boundary. U12 and U22 algebras are combined when utilizing 
the relation between shapes and shapes on their boundaries. Slide 33 Shape boundaries constitute a practical 
relation between shapes, which, in turn, helps us in the way we visually think. The rule in the illustration 
is in U12+U22 algebra and is used to create the design which is in U22 algebra. Parts of plane boundaries 
appear as line shapes that are utilized in generating the final form with planes.  Three types of part 
relations are those of overlapping, embedded, or discrete shapes Slide 34 Part relations are what differentiates 
shapes from atoms. Three kinds of part relations are between overlapping, embedding and discrete shapes. 
 discrete    discrete   overlapping   overlapping  Slide 40 The two planes shown share a common 
part, and are overlapping. Both shapes have parts that are not common with the other.  embedding  Slide 
41 If two shapes have common parts and at least one of these shapes has no part that is not a part of 
the other, then this shape is said to be embedded within the other.  embedding  Rotation Translation 
Mirror reflection Scaling and combinations of these  Slide 43 Euclidean transformations that are used 
in shape grammars are translation, scaling, rotating and reflecting along with their combinations. In 
the example of the painting, I can relocate the left side of the rule in so many places using these transformations. 
I can scale it down and up, I can see its rotations, I can see its reflections, and I can see it in multiple 
places, which are illustrated in slides 44 through 48.  scaling   reflection   rotation   translation 
  Sum A + B Difference A B Product A · B = A (A B) Symmetric difference A . B = (A B) + (B A) A 
. B = (A + B) (A · B) Slide 49 Within the defined shape algebras, we can add and subtract shapes of 
the same kind of basic elements. We can also take their unions and products. We can combine algebras 
to do Boolean operations on different kinds of basic elements in parallel.  A maximal shape  Slide 
50 Operating outside the Weight algebra, the union of all planes is as shown and is called the maximal 
plane. A maximal shape can be defined as the union of all existing parts that are either overlapping 
with or embedded in one another. Planes that are in discrete relation through a shared boundary can also 
form a maximal shape in union. and embedding relations and with discrete relations where they share 
at least a boundary. The maximal shapes shown are also members of the set of discrete shapes shown at 
the beginning of the class as a possible decomposition. Symmetric difference of two planar shapes Sum 
of two line shapes Slide 52 Here are illustrations to possible Boolean operations on shapes based on 
the Urform series. The first operation shows the symmetric difference of two plane shapes of the same 
weight in U22 whereas the second operation shows the sum of the boundaries of these two planes in U12. 
  Slide 53 In another set of illustrations are a series of Boolean operations on shapes of equal weight 
value in U22. Let us assume that there are three initial shapes. Firstly, the difference of shapes one 
and two is calculated.   Slide 54 Then, the sum of shapes two and three is calculated and subtracted 
from the result of the first step.   Slide 55 Continuing with the operations, the product of shapes 
two and three is followed by the symmetric difference of the two. From this series of operations, three 
new shapes emerge. Finally, all of these three are assigned different weights and summed up. Up until 
this point, we have shown how shape algebras, Boolean operations and part relations all work together 
for computing with shapes. In the next part, more examples, from actual applications, will be utilized 
to illustrate these concepts further.  Slide 0  Slide 1 This section includes examples of applications 
of shape grammars in design education, and practice. The presentation can be divided in three parts: 
recapitulation, analysis, synthesis. After a brief recapitulation of the main computational devices of 
shape grammars, it follows an overview of their applications in the analysis of design languages. Some 
examples of applications of grammars in the synthesis of new design languages are also presented.  Slide 
2 An outline of important developments in the history of computation generative grammar a computational 
device able to generate all the grammatical sentences of a language and only those Chomsky 1957 Sotirios 
Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 3 Definition of grammar as presented in 
Chomsky s Syntactic Structures (1957)  Slide 4 Grammars are systems that contain computational rules. 
In the traditional grammars, the substitution rules are used to erase and rewrite symbols. Slide 5 For 
example the substitution rules of the above example specify the substitutions that are necessary in order 
to construct a specific set of sentences. in shape grammars substitution rules operate on points, lines, 
planes, solids Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 6 Shape grammars 
are rule based systems in which substitution rules operate on elements of all spatial dimensions, not 
just symbols. These elements include points, lines, planes or solids. example substitution rule operating 
on lines Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 7 Example of a shape 
rule applying on shapes made out of lines. derivation rule   example rule &#38; derivation Sotirios 
Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 8 Example of a possible derivation. other 
designs in the language example rule &#38; derivation Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 9 Examples of alterative designs in the same language. These are other possible 
arrangements belonging to the same set of productions. example substitution rule operating on solids 
Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 10 Example of a shape rule applying 
on shapes made out of solids.  derivation rule example rule &#38; derivation Sotirios Kotsopoulos shape 
grammar applications | SIGGRAPH 2008 Slide 11 Example of a derivation, after applying the specific rule. 
  shape grammar Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 13 In shape grammars, 
similar devices can be used at different stages of the design process to illustrate specific properties 
of a design. These may include general wall layouts, construction details, distribution of openings etc. 
  analysis the science of form (Göethe) corpus for compositions finite set of rules producing the corpus 
 Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 15 The group of examples that 
follow, concerns the use of shape grammars in the analysis of existing cohesive sets of designs that 
we call design languages . Some classic analysis applications and some less known analysis studies are 
presented in this section. the Ice-ray Grammar (Stiny 1977) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 16 The specific grammatical analysis concerns the generation of highly irregular 
patterns for ornamental window and grille designs. Parametric shape grammars are defined for the recursive 
generation of these patterns.  the Ice-ray Grammar (Stiny 1977) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 17 In the case of the ice ray pattern, [the artisan] divides the whole area into 
large and equal light spots, and then subdivides until it reaches the size desired; he seldom uses dividers 
in his work (Dye, 1949)  the Ice-ray Grammar (Stiny 1977) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 18 Four dividing rules are used to capture the actions of the artisan. They constitute 
a shape grammar. The exact divisions are specified by a set of parameters that take into account the 
ratio of the divided areas  the Ice-ray Grammar (Stiny 1977) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 19 Example of an ice-ray lattice design: Chengtu, Szechwan ice-ray design (1800 
AD).  the Ice-ray Grammar (Stiny 1977) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 
Slide 20 A generation of the previous ice-ray design by means of the shape grammar specified by the four 
rules.  the Ice-ray Grammar (Stiny 1977) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 
2008 Slide 21 Two more examples of ice-ray lattice designs: Kwanshien, Szzechwan, 1875 AD (left) and 
Jungking, Szechwan, 1725 AD (right)  the Queen Anne Grammar (Flemming 1986) Sotirios Kotsopoulos shape 
grammar applications | SIGGRAPH 2008 Slide 22 In the specific example, shape grammars are used to determine 
the generation of Queen Anne style houses, which dominated domestic architecture in the United States 
in the 1880s. Separate grammars are provided by the author for the generation of plans and for the articulation 
of plans in three dimensions. The grammars emphasize aspects of geometry and overall design and address 
how the individual parts and features are related to each other. the Queen Anne Grammar (Flemming 1986) 
 the Queen Anne Grammar: rules generating the basic layout Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 23 Initial shape and rules to allocate spaces around hall.  the Queen Anne Grammar: 
basic layouts generated by the rules 0-3 Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 
Slide 24 Layouts produced by the application of the rule schemata 0-3 constructive analysis for Loos 
house at the Lido (Flemming 1990) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 
25 An introduction to a series of architectural languages characterized by different vocabularies of 
elements and by grammars whose rules indicate how the elements can be placed in space to form compositions. 
Exercises with each language include the analysis of precedents; the generation of forms using a given 
rule set and follow up studies with expanded rule sets. Above an example of a constructive analysis for 
Loos house at the Lido. the Hepplewhite chair Grammar (Knight 1980) Sotirios Kotsopoulos shape grammar 
applications | SIGGRAPH 2008 Slide 26 The paper presents a parametric shape grammar for the generation 
of Hepplewhite-style chair-back designs. Three examples of the style are studied, and in particular the 
design of the back itself. The proposed parametric shape grammar defines its unique characteristics and 
constraints. The grammar specifies rule schemata that generate not only the three designs in the original 
corpus but also a wide range of new designs within the constraints of the paradigm. the ancient Greek 
Meander Grammar (Knight 1986) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 27 
A formal model is applied to describe change in the meanders of the Geometric ancient Greek ornamental 
style. Shape grammars are used to explicate the underlying design of the meander from its earliest known 
from to the more complex forms that evolved from it.  the De Stijl painting Grammar (Knight 1989) Sotirios 
Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 28 The paintings of two artists Georges 
Vantongerloo and Fritz Glarner are examined and parametric shape grammars are outlined that capture the 
transformations from one stylistic stage, or grammar, to the next. The rules of each grammar are divided 
into rules that define relationships between forms or lines, and rules that define relationships between 
colors. Form rules and color rules are subdivided into categories of form and color rules that have specific 
compositional functions. The categories allow the rules with similar functions to be compared in different 
stylistic stages. the Palladian Grammar (Stiny &#38; Mitchell 1978) Sotirios Kotsopoulos shape grammar 
applications | SIGGRAPH 2008 Slide 29 A parametric shape grammar is proposed for the generation of the 
Palladian villa plans in six steps, including the generation of the grid and of the parti, the wall layout, 
the organization of the rooms, the addition of the entrance and the arrangement of the openings.  the 
Palladian Grammar (Stiny &#38; Mitchell 1978) the Palladian Grammar: enfilade rules Sotirios Kotsopoulos 
shape grammar applications | SIGGRAPH 2008 Slide 30 Enfilade rules.  the Palladian Grammar (Stiny &#38; 
Mitchell 1978) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 31 The grammar allows 
the generation of the original Palladian plans, but also new compositions that follow the compositional 
restrictions of the original plan arrangements.  Little house Robie house Stiny house Stiny house F 
L Wright Prairie House Grammar (Konig &#38; Eizenberg 1981) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 32 A parametric shape grammar that generates the compositional forms and specifies 
the function zones of the F. L. Wright s prairie style houses. The establishment of the fire place is 
the key to the definition of the prairie-style house. Around this fireplace, functionally distinguished 
blocks are recursively added and interpenetrated to from basic compositions from which elaborated prairie 
style houses are derived. F L Wright Prairie House Grammar (Konig &#38; Eizenberg 1981) the Prairie 
House Grammar: basic composition rule schemata Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 
2008 Slide 33 Basic composition rule schemata F L Wright Prairie House Grammar (Konig &#38; Eizenberg 
1981) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 34 Admissible sequences of 
shape rule schemata applications that are used to generate basic compositions.  Winslow house Henderson 
house Thomas house Stiny house Mitchell house March house F L Wright Prairie House Grammar (Konig &#38; 
Eizenberg 1981) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 35 Examples of 
houses with four functional zones: living, service, porch, bedroom  a Palladian construction grammar 
(Sass 2007) the proposed corpus of Palladian villas: darkened images represent built villas Sotirios 
Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 36 A production system that generates information 
for physical model manufacture with a 3D-printing device, defined here as a construction grammar. The 
rules etsbasedon16th-century-masonryconstruction are used to generate a villa model as a 3D construct. 
The derivation of the grammar demonstrates a design process based on physical constraints as the primary 
means of grammar structuring. The paper claims that construction rules can be used to build villas in 
Palladio's corpus, starting with a floor-plan drawing as the initial shape, with little need for an elevation 
drawing. This paper introduces unexplored issues of physical reasoning in the field of computing and 
design as part of the rule-building process. As a detailed example, a Palladian villa (the Villa Cornaro) 
is fabricated as a 3D-printed model from an eleven-part set of rules based on field analysis of Palladio's 
constructed villas. a Palladian construction grammar (Sass 2007) wall rules Sotirios Kotsopoulos shape 
grammar applications | SIGGRAPH 2008 Slide 37 Wall rules.  a Palladian construction grammar (Sass 2007) 
 wall derivation for the villa Cornaro Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 
Slide 38 The derivation of wall rules to construct the base of the Villa Cornaro (lower panels). a Palladian 
construction grammar (Sass 2007) roof construction rules Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 39 Rules for the construction of the roof  a Palladian construction grammar (Sass 
2007) roof derivation for the villa Cornaro Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 
2008 Slide 40 Derivation of the roof a Palladian construction grammar (Sass 2007) mirroring the villa 
along the axis E-E and preparation for 3D printing Sotirios Kotsopoulos shape grammar applications | 
SIGGRAPH 2008 Slide 41 Final rule implementation that mirrors the villa about the axis E-E. Preparation 
for 3D printing by subdividing the model into eight discrete smaller sections a Palladian construction 
grammar (Sass 2007) final model of the villa Cornaro Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 42 Final 3D model of the Villa Cornaro Simmons Hall student dormitory (Kotsopoulos 
2005) an analysis of Holl s porosity Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 
Slide 43 This exercise uses shape rules and grammars in a retrospective analysis that captures the use 
of the concept of porosity . Porosity was used as a design concept by architect S. Holl and his team 
in designing the Simmons Hall student dormitory at MIT campus. p o r o s i t y porous, permeable honeycomb 
screen, net riddle, sponge pore opening, hole aperture, passageway cribiformity sieve- like, sieve pervious 
unrestricted Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 44 The list of words 
organized by Holl s design team, in an effort to provide a contextual definition of porosity . rule 
A Simmons Hall student dormitory (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar applications | 
SIGGRAPH 2008 Slide 46 The shape rule schema A appears at the top, in the first row. A possible  derivation 
of the general massing for the building appears in the second row. The third row contains the shape(s) 
that are subtracted in each step of the derivation. The third row contains the sum of all the subtracted 
shapes. Simmons Hall student dormitory (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 47 Rule variations of schema that deals with the generation of sieve-like windows 
on the panels of the elevations, and with their placement on the appropriate positions.  rule Sotirios 
Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 49 The rule schema is responsible for the 
generation of the sponge like cavities within the orthogonal grid of the building.   Simmons Hall student 
dormitory (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 52 
Example of a possible derivation of Simmons Hall according to the rule schemata , , .  Simmons Hall 
student dormitory (Kotsopoulos 2005) an analysis of Holl s concept of porosity A conceiving possible 
rules S = { S1, S2, S3, Sj } B testing rule sequences T = { T1, T2 Tk } S = S1 S2 S3  Sj Cj C organizing 
rules to productive processes S1 S2 : Sn R1 R2 G : Rn Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 54 An overview of the proposed process: starting from the articulation of possible 
rules, proceeding to the testing of these rules, and concluding with the organization of non-redundant 
processes that involve the most effective ones. synthesis spatial relationships corpus of designs with 
spatial vocabulary specific properties Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 
Slide 55 The examples that follow, concern the use of shape grammars in design synthesis, and the construction 
of new design languages . spatial relation Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 
2008 Slide 57 Froebel gifts were used for educational purpose in exercises with grammars. The exercises 
start with the introduction of a spatial relationship between two forms. Then, the articulation of the 
corresponding shape rules allows the production of alternative configurations, while taking into account 
the symmetry properties of the two participating shapes. rule design basic grammars rule design Sotirios 
Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 58 Examples of rules and possible designs 
are presented above. The symmetry of the participating forms is reduced with the use of labels.  Slide 
59 Example of formal exercise with Froebel blocks (cube and oblong). The exercise begins with the definition 
of a spatial relationship between the cube and the oblong, and the articulation of the corresponding 
shape rules. Designs are produced by applying the rules, while taking into account the symmetry properties 
of the two shapes. The exercise was given in the Introduction to Computational Design I: Theory and Applications, 
taught by Prof. Knight in the Fall of 1998, at the Massachusetts Institute of Technology. Student M. 
Panagopoulou. student project: formal exercise with Froebel blocks (S Kotsopoulos) rule rule derivation 
derivation Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 60 Another example of 
formal exercise with Froebel blocks (two half cubes). Student S. Kotsopoulos.  student project: courtyard 
houses in Malibu, CA, (Jin-Ho Park) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 
Slide 61 The design of courtyard house in Malibu. The composition begins with the definition of a spatial 
relationship between the participating shapes (left), and the articulation of the corresponding shape 
rules. Several compositions are produced by applying the rules. The above designs were developed in the 
Introduction to Computational Design I: Theory and Applications, taught by Prof. Knight. Student Jin-Ho 
Park.  student project: single-family houses in Netherlands (R Brown) Sotirios Kotsopoulos shape grammar 
applications | SIGGRAPH 2008 Slide 65 Another studio example in formal composition: designs for single-family 
houses in Netherlands. Again, the composition begins with the definition of spatial relationships among 
forms (left), and the articulation of corresponding rules. Several variations are produced by applying 
the rules. Student R. Brown  A. S iza s Housing Grammar for Malagueira (Duarte 2005) Sotirios Kotsopoulos 
shape grammar applications | SIGGRAPH 2008 Slide 67 The Malagueira grammar was constructed as an analysis 
tool, in an effort to capture the original language of A Siza s designs for low cost housing. But the 
research had also a strong synthesis component as it was intended to expand the initial design language 
without betraying the spirit of the initial designs.  A. S iza s Housing Grammar for Malagueira (Duarte 
2005) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 68 A parametric dissection 
rule that was used in the grammar.   A. S iza s Housing Grammar for Malagueira (Duarte 2005) Sotirios 
Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 70 A fragment from a derivation that shows 
the consecutive subdivisions   Habitat For Humanity Housing Grammar (Kotsopoulos 2005) an elementary 
exercise in formal composition Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 
73 The example presents the generation of low cost housing units for Habitat For Humanity, from scratch. 
The building programs, the sites provided by HFH, and the examples of existing housing units, in Roxbury, 
Massachusetts, became a basis for the development of what was envisioned to be used as: an elementary 
studio exercise in formal composition . Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios 
Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 74 The housing units were approached as 
configurations of building blocks, or rooms. These configurations are produced by applying rules that 
are able to generate preferred adjacencies.  Habitat For Humanity Housing Grammar (Kotsopoulos 2005) 
Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 75 The rules are organized in different 
levels. General rules are used first to organize parties. At a different level, rules apply on selected 
parties to generate wall layouts, or to introduce openings. Some examples of the produced arrangements 
are presented above.  Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape 
grammar applications | SIGGRAPH 2008 Slide 76 The process started naturally, from sketching possible 
houses (left). Digital models (right) were introduced at a later stage of this process.  modularity 
the "kit of parts" approach Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 77 
Key issue of the process was the gradual specification of a possible kit of parts . These combine to 
produce variation.  Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape 
grammar applications | SIGGRAPH 2008 Slide 78 Four spatial relations between any two rooms are presented 
(first row): (1) having a common boundary, (2) being discrete, (3) touching in a corner, (4) overlapping. 
Five parametric variations (second row) of the first spatial relation are used in the proposed grammar. 
  Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 79 The spatial relations introduce corresponding parametric rules.  Habitat For 
Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 
2008 Slide 80 The five parametric variations of spatial relations can be generated by the above two parametric 
rules.  shape grammar interpreter (Liew 2004) converted the shape rules to machine instructions to be 
executed by a digital machine Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 81 
A digital shape grammar interpreter was used to facilitate to express the rules in code and to automate 
the generation combinations rules described in LISP scripting language format a rule is composed of four 
parts: 1. Left-hand schema 2. Right-hand schema 3. Transformation mapping  4. Parameter mapping the 
geometry is described as a series of vectors. Each vector has three components: 1. Action 2. Vector 
 3. Label  a horizontal parti line that is 5 units long is described as: ((action line ) (vector 5 0) 
(label parti )) 5 Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 82 Lisp code 
was used to assist the fast generation of design alternatives. A line of length 5 is described as above. 
an additive rule of the form x x + y: (setq schema-left-rule '((geometry ((action "line") (vector w 
0) (label "parti")) ((action "line") (vector 0 h) (label "parti")) ((action "line") (vector (- w) 0) 
(label "parti")) ((action "line") (vector 0 (- h)) (label "parti")) ) (parameter-constraints (w (> w 
0)) -w (h (> h w)) ) -h h ) ) w Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 
83 The above sequence of code expressions describe the left hand side of the rule. (setq schema-right-rule 
((geometry ((action "line") (vector w 0) (label "parti")) ((action "line") (vector 0 h) (label "parti")) 
((action "line") (vector (- w) 0) (label "parti")) ((action "line") (vector 0 (- h)) (label "parti")) 
((action "move") (vector w (- h (* 0.375 w)))) ((action "line") (vector a 0) (label "parti")) ((action 
"line") (vector 0 b) (label "parti")) ((action "line") (vector (- a) 0) (label "parti )) ((action "line") 
(vector 0 (- b)) (label "parti")) ) (parameter-constraints (w (> w 0)) (h (> h w))-b  a -a (a (> a 
0)) b (b (> b 0)) )  ) ) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 84 The 
above sequence of code expressions describe the right hand side of the rule (setq tmap-rule '((delta-xo 
. 0) (delta-yo . 0) (delta-ro . 0) (delta-za . 0) ) ) (setq pmap-rule '((w . w) (h . h) (a . w) (b 
. (* 0.75 w)) ) ) (setq housing-rule '((left . schema-left-rule) (right . schema-right-rule) (tmap 
. tmap-rule)-b  a -a  (pmap . pmap-rule)-wb (success . nil) -h h (failure . nil) (applymode . "single") 
(rulename . "housing-rule") w ) ) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 
85 And, this final sequence of code expressions coordinate the left and right hand side of the rule, 
in rule application. Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape 
grammar applications | SIGGRAPH 2008 Slide 86 A possible derivation of a housing unit on the basis of 
the proposed rules   Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos 
shape grammar applications | SIGGRAPH 2008 Slide 90 Some 3D modeled and rendered examples of housing 
units.  Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar 
applications | SIGGRAPH 2008 Slide 91 Some 3D printed examples of possible housing units.  Habitat 
For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 
2008 Slide 92 The sublanguage A of designs contains housing units produced by a specific set of rules. 
 Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 93 The sublanguage B, contains housing units produced by an alternative set of 
rules.  Habitat For Humanity Housing Grammar (Kotsopoulos 2005) Sotirios Kotsopoulos shape grammar applications 
| SIGGRAPH 2008 Slide 94 And, the sublanguage C contains housing units produced by a third alternative 
rule set.     student project: fine arts museum in Taipei (Jin-Ho Park) Sotirios Kotsopoulos shape 
grammar applications | SIGGRAPH 2008 Slide 101 The composition begins with the definition of a spatial 
relationship between the participating shapes (left). Two schematic compositions are produced by applying 
the rule. Student Jin-Ho Park.   student project: elementary school complex in Los Angeles (R Brown) 
Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 104 The composition starts with 
the definition of a spatial relationship between the participating forms (left). Three schematic compositions 
are produced by applying the rule. Student R, Brown.  student project: cultural history museum, Los 
Angeles (Jin-Ho Park) Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 106 The generation 
of the composition is based on two spatial relationships between forms (left). Two schematic compositions 
are also presented above. Student R, Brown.   student project: apartment house complex in Manhattan, 
NY (M Sanal) rules Sotirios Kotsopoulos shape grammar applications | SIGGRAPH 2008 Slide 109 Another 
example of use of shape grammars in composition. The design for an apartment house complex in Manhattan, 
NY. The two rules that are used to generate the design, appear at the right (bottom). Student M Sanal. 
 student project: apartment house complex in Manhattan, NY (M Sanal) variations Sotirios Kotsopoulos 
shape grammar applications | SIGGRAPH 2008 Slide 110 Design variations produced by the same two rules. 
    BIBLIOGRAPHY Suggested reading Stiny, George, 2006, Shape, MIT Press  References Chomsky, N. 
1957, Syntactic Structures, Mouton, The Hague Duarte, J P, 2005, Towards the customization of mass-housing: 
the grammar of Siza s houses at Malagueira, Environment and Planning B: Planning and Design, 32, pp. 
347-380 Flemming, U, 1987a, More than the sum of parts: the grammar of Queen Anne houses, Environment 
and Planning B:Planning and Design 14 pp. 323-350 Flemming, U, 1990, Syntactic Structures in Architecture, 
The Electronic Design Studio, MIT Press, Cambridge pp. 31-47 Knight, T, 1980, The generation of Hepplewhite-style 
chair back designs, Environment and Planning B: Planning andDesign 7 pp. 227-238 Knight, T, 1986, Transformation 
of the Meander Motif on Greek Geometric Pottery Design Computing 1 pp. 29-67 Knight, T, 1989, Transformations 
of De Stijl art: the paintings of Georges Vantongerloo and Fritz Glarner Environment and Planning B: 
Planning and Design 16 pp. 51-98 Koning H, and Eizenberg, J, 1981, The language of the prairie: Frank 
Lloyd Wright's prairie houses, Environment andPlanning B: Planning and Design 8 pp. 295-323 Kotsopoulos, 
S D, Constructing Design Concepts: A computational approach to the synthesis of architectural form, Doctorate 
Dissertation, Massachusetts Institute of Technology. Sass, L, 2007, A Palladian construction grammar-design 
reasoning with shape grammars and rapid prototyping Environment and Planning B: Planning and Design 34 
pp. 87-106 Stiny, G, 1977, Ice-ray: a note on Chinese lattice designs, Environment and Planning B4 pp. 
89-98 Stiny, G, 1980, Kindergarten grammars: designing with Froebel s building gifts, Environment and 
Planning B 3, pp. 409-462 Stiny, G 2006, Shape, MIT Press Stiny, G and Gips, J, 1972, Shape Grammars 
and the Generative Specification, Petrocelli OR (ed) Best computer papers of 1971, pp. 125-135 Stiny, 
G and Mitchell, W J, 1978, The Palladian grammar, Environment and Planning B 5 pp. 5-18 Ilustration 
Credits for Slides Except for the two excerpts from Stiny and Gips (1972), all illustrations in Part 
I, including the remake of the nine step computation by Stiny (2006), are drawn by Mine Ozkar. All images 
indicated as student projects in Part II demonstrate student work from the course Introduction to Shape 
Grammars I&#38;II: Theory &#38; Applications, taught by Prof. T Knight since 1992 at UCLA and then MIT. 
The presentation of the analysis on Simmons Hall dormitory contains sketches, photographs and models 
by Steven Holl Architects NY, as well as computer generated drawings and models by Sotirios Kotsopoulos. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401183</section_id>
		<sort_key>510</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Professional Development and Education: Introduction to shape grammars]]></section_title>
		<section_page_from>15</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098714</person_id>
				<author_profile_id><![CDATA[81365594108]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[&#214;zkar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098715</person_id>
				<author_profile_id><![CDATA[81421594224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sotirios]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kotsopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401184</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<pages>85</pages>
		<display_no>37</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Introduction to SIGGRAPH and interactive computer graphics]]></title>
		<page_from>1</page_from>
		<page_to>85</page_to>
		<doi_number>10.1145/1401132.1401184</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401184</url>
		<categories>
			<primary_category>
				<cat_node>A.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>G.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950</concept_id>
				<concept_desc>CCS->Mathematics of computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011122</concept_id>
				<concept_desc>CCS->General and reference->Document types</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098716</person_id>
				<author_profile_id><![CDATA[81324487523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[SIGGRAPH Online Bibliography Database: http://www.siggraph.org/publications/bibliography]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1214845</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Francis Hill, <i>Computer Graphics Using OpenGL</i>, 3&#60;sup&#62;rd&#60;/sup&#62; Edition, Prentice Hall, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1051917</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Edward Angel, <i>Interactive Computer Graphics: A Top-down Approach with OpenGL</i>, 4&#60;sup&#62;th&#60;/sup&#62; Edition, Addison-Wesley, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>529420</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Alan Watt, <i>3D Computer Graphics</i>, 3&#60;sup&#62;rd&#60;/sup&#62; Edition, Addison-Wesley, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1088893</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Peter Shirley, <i>Fundamentals</i> of Computer Graphics, 2&#60;sup&#62;nd&#60;/sup&#62; Edition, AK Peters, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>861562</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Donald Hearn and Polly Baker, <i>Computer Graphics with OpenGL</i>, Pearson/Prentice-Hall, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>261113</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Olin Lathrop, <i>The Way Computer Graphics Works</i>, John Wiley &amp; Sons, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>533021</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner, <i>Graphics Gems</i>, Academic Press, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[James Arvo, <i>Graphics Gems 2</i>, Academic Press, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[David Kirk, <i>Graphics Gems 3</i>, Academic Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Paul Heckbert, <i>Graphics Gems 4</i>, Academic Press, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Alan Paeth, <i>Graphics Gems 5</i>, Academic Press, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286105</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Jim Blinn, <i>A Trip Down the Graphics Pipeline</i>, Morgan Kaufmann, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286104</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Jim Blinn, <i>Dirty Pixels</i>, Morgan Kaufmann, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2213</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[David Rogers, <i>Procedural Elements for Computer Graphics</i>, McGraw-Hill, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[SIGGRAPH Conference Final program.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1526133</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Michael Mortenseon, <i>Geometric Transformations for 3D Modeling</i>, 2&#60;sup&#62;nd&#60;/sup&#62; Edition, Industrial press, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1213606</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Michael Mortenson, <i>Geometric Modeling</i>, John Wiley &amp; Sons, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>507109</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Eric Lengyel, <i>Mathematics for 3D Game Programming and Computer Graphics</i>, Charles River Media, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>320367</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Jean Gallier, <i>Curves and Surfaces in Geometric Modeling</i>, Morgan Kaufmann, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>119990</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Walter Taylor, <i>The Geometry of Computer Graphics</i>, Wadsworth &amp; Brooks/Cole, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>151048</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Gerald Farin, <i>Curves and Surfaces for Computer Aided Geometric Design</i>, 3&#60;sup&#62;rd&#60;/sup&#62; Edition, Academic Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280475</ref_obj_id>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Gerald Farin and Dianne Hansford, <i>The Geometry Toolbox for Graphics and Modeling</i>, AK Peters, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>580358</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Joe Warren and Henrik Weimer, <i>Subdivision Methods for Geometric Design: A Constructive Approach</i>, Morgan Kaufmann, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Barrett O'Neil, <i>Elementary Differential Geometry</i>, Academic Press, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>184684</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Joseph O'Rourke, <i>Computational Geometry in C</i>, Cambridge University Press, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Christopher Hoffman, <i>Geometric &amp; Solid Modeling</i>, Morgan Kaufmann, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>578513</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[I. D. Faux and M. J. Pratt, <i>Computational Geometry for Design and Manufacture</i>, Ellis-Horwood, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286071</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Eric Stollnitz, Tony DeRose, and David Salesin, <i>Wavelets for Computer Graphics</i>, Morgan-Kaufmann, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>140548</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Ronen Barzel, <i>Physically-Based Modeling for Computer Graphics</i>, Academic Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>575646</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[David Rogers and J. Alan Adams, <i>Mathematical Elements for Computer Graphics</i>, McGraw-Hill, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130368</ref_obj_id>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[John Snyder, <i>Generative Modeling for Computer Graphics and Computer Aided Design</i>, Academic Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1177246</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Klaus Engel, Markus Hadwiger, Joe Kniss, Christof Rezk-Salama, and Daniel Weiskopf, <i>Real-Time Volume Graphics</i>, A.K. Peters, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>993936</ref_obj_id>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Christopher Johnson and Charles Hansen, <i>The Visualization Handbook</i>, Elsevier Academic Press, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[David Thompson, Jeff Braun, and Ray Ford, <i>OpenDX: Paths to Visualization</i>, Visualization and Imagery Solutions, Inc., 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Chandrajit Bajaj, <i>Data Visualization Techniques</i>, John Wiley &amp; Sons, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Min Chen, Arie Kaufman, and Roni Yagel, <i>Volume Graphics</i>, Springer-Verlag, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[William Schroeder, Ken Martin, and Bill Lorensen, <i>The Visualization Toolkit</i>, 3&#60;sup&#62;rd&#60;/sup&#62; Edition, Prentice-Hall, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Luis Ibanez and William Schroeder, <i>The ITK Software Guide: The Insight Segmentation and Registration Toolkit (version 1.4)</i>, Prentice-Hall, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Greg Nielson, Hans Hagen, and Heinrich M&#252;ller, <i>Scientific Visualization: Overviews, Methodologies, Techniques</i>, IEEE Computer Society Press, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Lenny Lipton, <i>The CrystalEyes Handbook</i>, StereoGraphics Corporation, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>204534</ref_obj_id>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Brand Fortner, <i>The Data Handbook: A Guide to Understanding the Organization and Visualization of Technical Data</i>, Spyglass, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>573078</ref_obj_id>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[William Kaufmann and Larry Smarr, <i>Supercomputing and the Transformation of Science</i>, Scientific American Library, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>160983</ref_obj_id>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Robert Wolff and Larry Yaeger, <i>Visualization of Natural Phenomena</i>, Springer-Verlag, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>169238</ref_obj_id>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[David McAllister, <i>Stereo Computer Graphics and Other True 3D Technologies</i>, Princeton University Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>562139</ref_obj_id>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Peter Keller and Mary Keller, <i>Visual Cues: Practical Data Visualization</i>, IEEE Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>863149</ref_obj_id>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Maureen Stone, <i>A Field Guide to Digital Color</i>, AK Peters, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>63450</ref_obj_id>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Roy Hall, <i>Illumination and Color in Computer Generated Imagery</i>, Springer-Verlag, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[David Travis, <i>Effective Color Displays</i>, Academic Press, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77787</ref_obj_id>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[L. G. Thorell and W. J. Smith, <i>Using Computer Color Effectively</i>, Prentice Hall, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>33404</ref_obj_id>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Edward Tufte, <i>The Visual Display of Quantitative Information</i>, Graphics Press, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>78223</ref_obj_id>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Edward Tufte, <i>Envisioning Information</i>, Graphics Press, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Edward Tufte, <i>Visual Explanations</i>, Graphics Press, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1198006</ref_obj_id>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Edward Tufte, <i>Beautiful Evidence</i>, Graphics Press, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>60571</ref_obj_id>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Howard Resnikoff, <i>The Illusion of Reality</i>, Springer-Verlag, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>527570</ref_obj_id>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner, <i>Principles of Digital Image Synthesis</i>, Morgan Kaufmann, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>154731</ref_obj_id>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Michael Cohen and John Wallace, <i>Radiosity and Realistic Image Synthesis</i>, Morgan-Kaufmann, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>94788</ref_obj_id>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner, <i>An Introduction to Ray Tracing</i>, Academic Press, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>519042</ref_obj_id>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Rosalee Wolfe, <i>3D Graphics: A Visual Approach</i>, Oxford Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Ken Joy et al, <i>Image Synthesis</i>, IEEE Computer Society Press, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>551861</ref_obj_id>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[David Ebert et al, <i>Texturing and Modeling</i>, 2&#60;sup&#62;nd&#60;/sup&#62; Edition, Academic Press, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>299787</ref_obj_id>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Alan Watt and Fabio Policarpo, <i>The Computer Image</i>, Addison-Wesley, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>320164</ref_obj_id>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Ron Brinkman, <i>The Art and Science of Digital Compositing</i>, Morgan Kaufmann, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>316738</ref_obj_id>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[John Miano, <i>Compressed Image File Formats</i>, Addison-Wesley, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>107217</ref_obj_id>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Alan Watt and Mark Watt, <i>Advanced Animation and Rendering Techniques</i>, Addison-Wesley, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>242521</ref_obj_id>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Nadia Magnenat Thalmann and Daniel Thalmann, <i>Interactive Computer Animation</i>, Prentice-Hall, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Philip Hayward and Tana Wollen, <i>Future Visions: New Technologies of the Screen</i>, Indiana University Press, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1197548</ref_obj_id>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[David Hodgson, Bryan Stratten, and Alice Rush, <i>Paid to Play: An Insider's Guide to Video Game Careers</i>, Prima, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1088897</ref_obj_id>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Alan Watt and Fabio Policarpo, <i>Advanced Game Development with Programmable Graphics Hardware</i>, AK Peters, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Jacob Habgood and Mark Overmars, <i>The Game Maker's Apprentice</i>, Apress, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[David Eberly, <i>3D Game Engine Design: A Practical Approach to Real-Time Computer Graphics</i>, Morgan Kaufmann, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Alan Watt and Fabio Policarpo, <i>3D Games: Real-time Rendering and Software Technology</i>, Addison-Wesley, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>507109</ref_obj_id>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Eric Lengyel, <i>Mathematics for 3D Game Programming and Computer Graphics</i>, Charles River Media, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1205650</ref_obj_id>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[David Bourg, <i>Physics for Game Developers</i>, O'Reilly and Associates, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>380465</ref_obj_id>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Munlo Coutinho, <i>Dynamic Simulations of Multibody Systems</i>, Springer Verlag, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>572747</ref_obj_id>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Mark DeLoura, <i>Game Programming Gems</i>, Charles River Media, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>516343</ref_obj_id>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Mark DeLoura, <i>Game Programming Gems 2</i>, Charles River Media, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>550581</ref_obj_id>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Mark DeLoura, <i>Game Programming Gems 3</i>, Charles River Media, 2002. http://www.gamedev.net http://www.gamasutra.net]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>208466</ref_obj_id>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[John Vince, <i>Virtual Reality Systems</i>, Addison-Wesley, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Gene Davis, <i>Learning Java Bindings For OpenGL (JOGL)</i>, AuthorHouse, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>261190</ref_obj_id>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Andrea Ames, David Nadeau, John Moreland, <i>The VRML 2.0 Sourcebook</i>, John Wiley &amp; Sons, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>579108</ref_obj_id>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Bruce Eckel, <i>Thinking in Java</i>, Prentice-Hall, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1237908</ref_obj_id>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[David Flanagan, <i>Java in a Nutshell</i>, O'Reilly &amp; Associates, 5&#60;sup&#62;th&#60;/sup&#62; edition, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>995050</ref_obj_id>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[David Flanagan, <i>Java Examples in a Nutshell</i>, O'Reilly &amp; Associates, 3&#60;sup&#62;rd&#60;/sup&#62; edition, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>550653</ref_obj_id>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Henry Sowizral, Kevin Rushforth, and Michael Deering, <i>The Java 3D API Specification</i>, Addison-Wesley, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>560434</ref_obj_id>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Rasmus Lerdorf and Kevin Tatroe, <i>Programming PHP</i>, O'Reilly, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1202387</ref_obj_id>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Yukihiro Matsumoto, <i>Ruby in a Nutshell</i>, O'Reilly, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1076523</ref_obj_id>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Randi Rost, <i>OpenGL Shading Language</i>, Addison-Wesley, 2006 (2&#60;sup&#62;nd&#60;/sup&#62; edition).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Steve Upstill, <i>The RenderMan Companion</i>, Addison-Wesley, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>555371</ref_obj_id>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Tony Apodaca and Larry Gritz, <i>Advanced RenderMan: Creating CGI for Motion Pictures</i>, Morgan Kaufmann, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975271</ref_obj_id>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Saty Raghavachary, <i>Rendering for Beginners: Image Synthesis using RenderMan</i>, Focal Press, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Randima Fernando, <i>GPU Gems</i>, NVIDIA, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Matt Pharr, Randima Fernando, <i>GPU Gems 2</i>, NVIDIA, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1407436</ref_obj_id>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Hubert Nguyen, <i>GPU Gems 3</i>, NVIDIA, 2007. http://www.clockworkcoders.com/oglsl]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[
<i>OpenGL 2.0 Reference Manual</i>, Addison-Wesley, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[
<i>OpenGL 2.0 Programming Guide</i>, Addison-Wesley, 2005 (5&#60;sup&#62;th&#60;/sup&#62; edition).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1202623</ref_obj_id>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Tom McReynolds and David Blythe, <i>Advanced Graphics Programming Using OpenGL</i>, Morgan Kaufmann, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>516514</ref_obj_id>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Edward Angel, <i>OpenGL: A Primer</i>, Addison-Wesley, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>318952</ref_obj_id>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner, <i>Recreational Computer Graphics</i>, Morgan Kaufmann, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>521041</ref_obj_id>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Anne Spalter, <i>The Computer in the Visual Arts</i>, Addison-Wesley, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[Jef Raskin, <i>The Humane Interface</i>, Addison-Wesley, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>61961</ref_obj_id>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Ben Shneiderman, <i>Designing the User Interface</i>, Addison-Wesley, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Clark Dodsworth, <i>Digital Illusion</i>, Addison-Wesley, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Isaac Victor Kerlow, <i>The Art of 3-D: Computer Animation and Imaging</i>, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Isaac Victor Kerlow and Judson Rosebush, <i>Computer Graphics for Designers and Artists</i>, Van Nostrand Reinhold, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581837</ref_obj_id>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[Mehmed Kantardzic, <i>Data Mining: Concepts, Models, Methods, and Algorithms</i>, Wiley, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[William Press, Saul Teukolsky, William Vetterling, and Brian Flannery, <i>Numerical Recipes in C</i>, Second Edition, Cambridge University Press, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[James Skakoon and W. J. King, <i>The Unwritten Laws of Engineering</i>, ASME Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Introduction to SIGGRAPH and Interactive Computer Graphics Mike Bailey Oregon State University mjb@cs.oregonstate.edu 
 Oregon State University Computer Graphics Mike Bailey Professor of Computer Science, Oregon State University 
PhD from Purdue University Has worked at Sandia Labs, Purdue University, Megatek, San Diego Supercomputer 
Center (UC San Diego), and OSU mjb@cs.oregonstate.edu r Or egon State Univ e rsity r Computer G raphi 
c s    Course Goals  Understand the different venues at SIGGRAPH, and how to strategically attend 
the ones that are best for you  Provide a background for papers, panels, and other courses  Help 
appreciate the Animation Festival  Get more from the vendor exhibits  Give our take on where the 
future is  Provide pointers for further study   r Or egon State Univ e rsity r Computer G raphi 
c s  Topics How to Attend SIGGRAPH 2008  The Graphics Process  Graphics Hardware  GPU programming 
  Graphics Input Dev i ces  Finding More Information   r Or egon State Univ e rsity r Computer 
G raphi c s  Introduction to SIGGRAPH and Interactive Computer Graphics Mike Bailey Oregon State University 
 mjb@cs.oregonstate.edu  r Or egon State Univ e rsity r Computer G raphi c s   Courses Monday 
Friday  Some are standard kno w ledge (e.g., this one )  Some are c u tting edge  Some are n 
e w topics  You got the full set of n o tes on the courses DVD   Or e gon S t a t e Univ e r s i 
ty Compute r Gr a phic s  Courses Strategy Choose courses that are useful  Choose courses that are 
mean ingful  Chose courses w h ere you really want to see the speaker(s )  Choose courses for which 
there w i ll be grea t vis u al presenta tions which cannot be rep l ic ated in the notes  Hop around 
bet w een courses to catch the best topics and speakers   Or e gon S t a t e Univ e r s i ty Compute 
r Gr a phic s  Papers Tuesday - Friday  Deep technical informa t ion  Printed in the proceedings 
and on the conference DVD Printed in the proceedings and on the conference DVD  Strategy: attend w 
h en an animatio n or inte rac t iv e technique is being discussed  Strategy: a ttend w h en you think 
you w ill not understand the topic from read i ng about it alone  Or e gon S t a t e Univ e r s i ty 
Compute r Gr a phic s  Panels Monday - Friday  Gene ra l dis c ussion, opinion, somet i m e s contro 
vers ial  Important point: sometimes transcribed, usually not  Strategy: a ttend w h en a topi c, 
speaker, or animation you really want to see is being discussed  Tuesday - Friday  Paper - like content, 
but shorter and less formal  Some of th e w o rk has been completed  Some of it is Work in Progress 
  Oft e nt ime , the source o f some rea lly cool ideas   Or e gon S t a t e Univ e r s i ty Compute 
r Gr a phic s  Talks Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s  Special Events Fast 
- Forward Papers review : Monday 6:00 - 8:00  ACM Stude nt Research Competition: Friday,  10:30 
12:15  Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s  Exhibition Tuesday Thursday  Many 
vendors time their hottest product releases for this w eek!  But, it s big, so go w i th a strategy! 
  Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s  Exhibition Strategy Buyer s Gu ide  
They are listed both by alphabetical name and by product category  Or e gon S t a t e Univ e r s i 
ty Compute r Gr a phic s     Art and Design Galleries Monday Friday  Al w ays fun and interesting 
!  Be sure to see it, ev en if you just stroll throug h w i thout stopping  Or e gon S t a t e Univ 
e r s i ty Compute r Gr a phic s  New Tech Demos Monday - Friday  Speculative research on graphics, 
visualiz ation, robotics, music, audio, displays, haptics , senso r s, gaming, w e b, AI, and e n tertainment 
  Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s                    Z-Buffer 
Steps 1. Compare the incoming Z value with the Z value already in that pixel 2. If the incoming Z value 
is closer to the viewer than what is already there, open the gates 3. Write the new pixel color and 
Z value  Oregon State University Computer Graphics         Pipeline Processor: Transformations 
 Translation  Rotation  Scaling   Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s  
 Pipeline Processor: Windowing a n d Clipping  Declare which portion of the 3D universe you are in ter 
e s t ed in viewing  This is ca lle d the vi ew vol u me  Clip away everything th at is outside the 
viewing volum e   Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s    Display List  A list 
of graphics instructions created ahead of tim e and then played back when needed  May or m a y not 
be edit able on ce it is created  Som e tim es the stru c t ur e of display lists is in the form of 
a scene graph  Display lists can liv e in CPU m e m o ry or on the graphics card ( disp lay o b je 
c t s )   Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s  The Limitations of using NTSC 
Video Cannot display saturated colors well  Expect an effective reso lution of (at best) ~640x480 
  Do not use single - pixel thick lines  Stay away from the edges of the screen  Som e colors have 
better video resolution than others   Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s  NTSC 
Cycles of Encoding per Scanline                       GPU Programming There are 
a lot of GPU shader programming resources at: http://cs.oregonstate.edu/~mjb/cs519 http://cs.oregonstate.edu/~mjb/glman 
Mike Bailey, mjb@cs.oregonstate.edu Or e gon S t a t e Univ e r s i ty Compute r Gr a phic s     
  Locator Display-to-Input Ratio DTI Ratio: the amount of cursor movement on the screen divided by 
the amount of hand movement Large: good for speed Small: Good for accuracy Also called Gain Or e gon 
S t a t e Univ e r s i ty Compute r Gr a phic s Ways to Read an Input Device Sampling: What is its 
input right now ?  Event-based: Wait until the user does something  Or e gon S t a t e Univ e r s 
i ty Compute r Gr a phic s      Where to Find More Information about Computer Graphics Mike Bailey 
Oregon State University 1. References 1.1 General Computer Graphics SIGGRAPH Online Bibliography Database: 
http://www.siggraph.org/publications/bibliography Francis Hill, Computer Graphics Using OpenGL, 3rd Edition, 
Prentice Hall, 2006. Edward Angel, Interactive Computer Graphics: A Top-down Approach with OpenGL, 4th 
Edition, Addison-Wesley, 2005. Alan Watt, 3D Computer Graphics, 3rd Edition, Addison-Wesley, 2000. Peter 
Shirley, Fundamentals of Computer Graphics, 2nd Edition, AK Peters, 2005. Donald Hearn and Polly Baker, 
Computer Graphics with OpenGL, Pearson/Prentice-Hall, 2004. Olin Lathrop, The Way Computer Graphics Works, 
John Wiley &#38; Sons, 1997. Andrew Glassner, Graphics Gems, Academic Press, 1990. James Arvo, Graphics 
Gems 2, Academic Press, 1991. David Kirk, Graphics Gems 3, Academic Press, 1992. Paul Heckbert, Graphics 
Gems 4, Academic Press, 1994. Alan Paeth, Graphics Gems 5, Academic Press, 1995. Jim Blinn, A Trip Down 
the Graphics Pipeline, Morgan Kaufmann, 1996. Jim Blinn, Dirty Pixels, Morgan Kaufmann, 1998. David Rogers, 
Procedural Elements for Computer Graphics, McGraw-Hill, 1997. SIGGRAPH Conference Final program. 1.2 
Math and Geometry Michael Mortenseon, Geometric Transformations for 3D Modeling, 2nd Edition, Industrial 
press, 2007. Michael Mortenson, Geometric Modeling, John Wiley &#38; Sons, 2006. May 6, 2008 1 Eric Lengyel, 
Mathematics for 3D Game Programming and Computer Graphics, Charles River Media, 2002. Jean Gallier, Curves 
and Surfaces in Geometric Modeling, Morgan Kaufmann, 2000. Walter Taylor, The Geometry of Computer Graphics, 
Wadsworth &#38; Brooks/Cole, 1992. Gerald Farin, Curves and Surfaces for Computer Aided Geometric Design, 
3rd Edition, Academic Press, 2001. Gerald Farin and Dianne Hansford, The Geometry Toolbox for Graphics 
and Modeling, AK Peters, 1998. Joe Warren and Henrik Weimer, Subdivision Methods for Geometric Design: 
A Constructive Approach, Morgan Kaufmann, 2001. Barrett O Neil, Elementary Differential Geometry, Academic 
Press, 1997. Joseph O Rourke, Computational Geometry in C, Cambridge University Press, 1996. Christopher 
Hoffman, Geometric &#38; Solid Modeling, Morgan Kaufmann, 1989. I.D. Faux and M.J. Pratt, Computational 
Geometry for Design and Manufacture, Ellis-Horwood, 1979. Eric Stollnitz, Tony DeRose, and David Salesin, 
Wavelets for Computer Graphics, Morgan-Kaufmann, 1996. Ronen Barzel, Physically-Based Modeling for Computer 
Graphics, Academic Press, 1992. David Rogers and J. Alan Adams, Mathematical Elements for Computer Graphics, 
McGraw-Hill, 1989. John Snyder, Generative Modeling for Computer Graphics and Computer Aided Design, 
Academic Press, 1992. 1.3 Scientific Visualization Klaus Engel, Markus Hadwiger, Joe Kniss, Christof 
Rezk-Salama, and Daniel Weiskopf, Real-Time Volume Graphics, A.K. Peters, 2006. Christopher Johnson and 
Charles Hansen, The Visualization Handbook, Elsevier Academic Press, 2005. David Thompson, Jeff Braun, 
and Ray Ford, OpenDX: Paths to Visualization, Visualization and Imagery Solutions, Inc., 2001. Chandrajit 
Bajaj, Data Visualization Techniques, John Wiley &#38; Sons, 1999. Min Chen, Arie Kaufman, and Roni Yagel, 
Volume Graphics, Springer-Verlag, 2000. William Schroeder, Ken Martin, and Bill Lorensen, The Visualization 
Toolkit, 3rd Edition, May 6, 2008 2 Prentice-Hall, 2004. Luis Ibanez and William Schroeder, The ITK Software 
Guide: The Insight Segmentation and Registration Toolkit (version 1.4), Prentice-Hall, 2003. Greg Nielson, 
Hans Hagen, and Heinrich Müller, Scientific Visualization: Overviews, Methodologies, Techniques, IEEE 
Computer Society Press, 1997. Lenny Lipton, The CrystalEyes Handbook, StereoGraphics Corporation, 1991. 
Brand Fortner, The Data Handbook: A Guide to Understanding the Organization and Visualization of Technical 
Data, Spyglass, 1992. William Kaufmann and Larry Smarr, Supercomputing and the Transformation of Science, 
Scientific American Library, 1993. Robert Wolff and Larry Yaeger, Visualization of Natural Phenomena, 
Springer-Verlag, 1993. David McAllister, Stereo Computer Graphics and Other True 3D Technologies, Princeton 
University Press, 1993. Peter Keller and Mary Keller, Visual Cues: Practical Data Visualization, IEEE 
Press, 1993. 1.4 Color and Perception Maureen Stone, A Field Guide to Digital Color, AK Peters, 2003. 
Roy Hall, Illumination and Color in Computer Generated Imagery, Springer-Verlag, 1989. David Travis, 
Effective Color Displays, Academic Press, 1991. L.G. Thorell and W.J. Smith, Using Computer Color Effectively, 
Prentice Hall, 1990. Edward Tufte, The Visual Display of Quantitative Information, Graphics Press, 1983. 
Edward Tufte, Envisioning Information, Graphics Press, 1990. Edward Tufte, Visual Explanations, Graphics 
Press, 1997. Edward Tufte, Beautiful Evidence, Graphics Press, 2006. Howard Resnikoff, The Illusion of 
Reality, Springer-Verlag, 1989. 1.5 Rendering Andrew Glassner, Principles of Digital Image Synthesis, 
Morgan Kaufmann, 1995. Michael Cohen and John Wallace, Radiosity and Realistic Image Synthesis, Morgan-Kaufmann, 
1993. Andrew Glassner, An Introduction to Ray Tracing, Academic Press, 1989. May 6, 2008 3 Rosalee Wolfe, 
3D Graphics: A Visual Approach, Oxford Press. Ken Joy et al, Image Synthesis, IEEE Computer Society Press, 
1988. 1.6 Images David Ebert et al, Texturing and Modeling, 2nd Edition, Academic Press, 1998. Alan 
Watt and Fabio Policarpo, The Computer Image, Addison-Wesley, 1998. Ron Brinkman, The Art and Science 
of Digital Compositing, Morgan Kaufmann, 1999. John Miano, Compressed Image File Formats, Addison-Wesley, 
1999. 1.7 Animation Alan Watt and Mark Watt, Advanced Animation and Rendering Techniques, Addison-Wesley, 
1998. Nadia Magnenat Thalmann and Daniel Thalmann, Interactive Computer Animation, Prentice-Hall, 1996. 
Philip Hayward and Tana Wollen, Future Visions: New Technologies of the Screen, Indiana University Press, 
1993. 1.8 Games David Hodgson, Bryan Stratten, and Alice Rush, Paid to Play: An Insider s Guide to Video 
Game Careers, Prima, 2006. Alan Watt and Fabio Policarpo, Advanced Game Development with Programmable 
Graphics Hardware, AK Peters, 2005. Jacob Habgood and Mark Overmars, The Game Maker s Apprentice, Apress, 
2006. David Eberly, 3D Game Engine Design: A Practical Approach to Real-Time Computer Graphics, Morgan 
Kaufmann, 2006. Alan Watt and Fabio Policarpo, 3D Games: Real-time Rendering and Software Technology, 
Addison-Wesley, 2001. Eric Lengyel, Mathematics for 3D Game Programming and Computer Graphics, Charles 
River Media, 2002. David Bourg, Physics for Game Developers, O Reilly and Associates, 2002. Munlo Coutinho, 
Dynamic Simulations of Multibody Systems, Springer Verlag, 2001. Mark DeLoura, Game Programming Gems, 
Charles River Media, 2000. Mark DeLoura, Game Programming Gems 2, Charles River Media, 2001. May 6, 2008 
4 Mark DeLoura, Game Programming Gems 3, Charles River Media, 2002. http://www.gamedev.net http://www.gamasutra.net 
 1.9 Virtual Reality John Vince, Virtual Reality Systems, Addison-Wesley, 1995. 1.10 The Web Gene Davis, 
Learning Java Bindings For OpenGL (JOGL), AuthorHouse, 2004. Andrea Ames, David Nadeau, John Moreland, 
The VRML 2.0 Sourcebook, John Wiley &#38; Sons, 1997. Bruce Eckel, Thinking in Java, Prentice-Hall, 
1998. David Flanagan, Java in a Nutshell, O Reilly &#38; Associates, 5th edition, 2005. David Flanagan, 
Java Examples in a Nutshell, O Reilly &#38; Associates, 3rd edition, 2004. Henry Sowizral, Kevin Rushforth, 
and Michael Deering, The Java 3D API Specification, Addison-Wesley, 1998. Rasmus Lerdorf and Kevin Tatroe, 
Programming PHP, O Reilly, 2002. Yukihiro Matsumoto, Ruby in a Nutshell, O Reilly, 2003. 1.11 Shaders 
Randi Rost, OpenGL Shading Language, Addison-Wesley, 2006 (2nd edition). Steve Upstill, The RenderMan 
Companion, Addison-Wesley, 1990. Tony Apodaca and Larry Gritz, Advanced RenderMan: Creating CGI for Motion 
Pictures, Morgan Kaufmann, 1999. Saty Raghavachary, Rendering for Beginners: Image Synthesis using RenderMan, 
Focal Press, 2005. Randima Fernando, GPU Gems, NVIDIA, 2004. Matt Pharr, Randima Fernando, GPU Gems 2, 
NVIDIA, 2005. Hubert Nguyen, GPU Gems 3, NVIDIA, 2007. http://www.clockworkcoders.com/oglsl May 6, 2008 
5 1.12 Miscellaneous OpenGL 2.0 Reference Manual, Addison-Wesley, 2006. OpenGL 2.0 Programming Guide, 
Addison-Wesley, 2005 (5th edition). Tom McReynolds and David Blythe, Advanced Graphics Programming Using 
OpenGL, Morgan Kaufmann, 2005. Edward Angel, OpenGL: A Primer, Addison-Wesley, 2002. Andrew Glassner, 
Recreational Computer Graphics, Morgan Kaufmann, 1999. Anne Spalter, The Computer in the Visual Arts, 
Addison-Wesley, 1999. Jef Raskin, The Humane Interface, Addison-Wesley, 2000. Ben Shneiderman, Designing 
the User Interface, Addison-Wesley, 1997. Clark Dodsworth, Digital Illusion, Addison-Wesley, 1997. Isaac 
Victor Kerlow, The Art of 3-D: Computer Animation and Imaging, 2000. Isaac Victor Kerlow and Judson Rosebush, 
Computer Graphics for Designers and Artists, Van Nostrand Reinhold, 1986. Mehmed Kantardzic, Data Mining: 
Concepts, Models, Methods, and Algorithms, Wiley, 2003. William Press, Saul Teukolsky, William Vetterling, 
and Brian Flannery, Numerical Recipes in C, Second Edition, Cambridge University Press, 1997. James Skakoon 
and W. J. King, The Unwritten Laws of Engineering, ASME Press, 2001. 2. Periodicals Computer Graphics 
and Applications: published by IEEE (http://www.computer.org, 714-821-8380) Computer Graphics World: 
published by Pennwell (http://www.cgw.com, 603-891-0123) Journal of Graphics Tools: published by A.K. 
Peters (http://www akpeters.com, 617-235-2210) Game Developer: published by CMP Media (http://www gdmag.com, 
415-905-2200) (Once a year publishes the Game Career Guide.) Computer Graphics Quarterly: published 
by ACM SIGGRAPH May 6, 2008 6 (http://www.siggraph.org, 212-869-7440) Transactions on Visualization and 
Computer Graphics: published by IEEE (http://www.computer.org, 714-821-8380) Transactions on Graphics: 
published by ACM (http://www.acm.org, 212-869-7440) Cinefex (http://www.cinefex.com, 951-781-1917) 3. 
Professional organizations ACM................Association for Computing Machinery http://www.acm.org, 
212-869-7440 SIGGRAPH.....ACM Special Interest Group on Computer Graphics http://www.siggraph.org, 212-869-7440 
SIGCHI............ ACM Special Interest Group on Computer-Human Interfaces http://www.acm.org/sigchi, 
212-869-7440 IEEE ................ Institute of Electrical and Electronic Engineers http://www.computer.org, 
202-371-0101 NAB ................ National Association of Broadcasters http://www.nab.org, 800-521-8624 
 ASME..............American Society of Mechanical Engineers http://www.asme.org, 800-THE-ASME IGDA...............International 
Game Developers Association http://www.igda.org, 856-423-2990 4. Conferences ACM SIGGRAPH: 2008: Los 
Angeles August 11-15 2009: New Orleans http://www.siggraph.org/s2008 IEEE Visualization: 2008: Columbus, 
OH October 19-24 http://vis.computer.org Game Developers Conference: 2009: San Francisco, CA March 
23-27 http://www.gdconf.com May 6, 2008 EforAll:Expo 2008: Los Angeles, CA --October 3-5 http://www.eforallexpo.com 
ASME International Design Engineering Technical Conferences (includes the Computers and Information in 
Engineering conference): 2008: New York, NY August 3-6 http://www.asmeconferences.org/idetc08 National 
Association of Broadcasters (NAB): 2009: Las Vegas, NV April 18-23 http://www.nab.org ACM SIGCHI: 2009: 
Boston, MA April 5-9 http://www.acm.org/sigchi ACM SIGARCH / IEEE Supercomputing: 2008: Austin, TX 
-- November 15-21 http://www.supercomputing.org 5. Graphics Performance Characterization The GPC web 
site tabulates graphics display speeds for a variety of vendors' workstation products. To get the information, 
visit: http://www.spec.org/benchmarks.html#gwpg May 6, 2008 8   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401185</section_id>
		<sort_key>530</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Professional Development and Education: Know your rights: a legal primer for software developers, artists and content creators]]></section_title>
		<section_page_from>16</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098717</person_id>
				<author_profile_id><![CDATA[81100042683]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Irizarry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098718</person_id>
				<author_profile_id><![CDATA[81319501414]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Silberman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098719</person_id>
				<author_profile_id><![CDATA[81365595777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Neer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gupta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401186</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>5</pages>
		<display_no>38</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Know your rights]]></title>
		<subtitle><![CDATA[a legal primer for software developers, artists and content creators]]></subtitle>
		<page_from>1</page_from>
		<page_to>5</page_to>
		<doi_number>10.1145/1401132.1401186</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401186</url>
		<categories>
			<primary_category>
				<cat_node>K.5.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>K.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003462</concept_id>
				<concept_desc>CCS->Social and professional topics->Computing / technology policy</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003462</concept_id>
				<concept_desc>CCS->Social and professional topics->Computing / technology policy</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Legal Aspects</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098720</person_id>
				<author_profile_id><![CDATA[81100042683]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Irizarry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Conoa, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098721</person_id>
				<author_profile_id><![CDATA[81319501414]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Silberman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intellectual Property & Outsourcing Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098722</person_id>
				<author_profile_id><![CDATA[81365595777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Neer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gupta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Walt Disney Company in Burbank, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Know Your Rights: A Legal Primer for Software Developers, Artists and Content Creators C:\Documents 
and Settings\Gil Irizarry\Desktop\main_logo.jpg C:\docs\conoa-grad-siggraph08.jpg SIGGRAPH2008  Gil 
Irizarry is the founder of Conoa, Inc., a graphics software development firm for film and video post-production. 
He is also a trained project manager with PMP and CSM certification and 20 years of experience. He is 
a graduate of Cornell University and has attended every SIGGRAPH conference since 1991. C:\Documents 
and Settings\Gil Irizarry\Desktop\main_logo.jpg C:\docs\conoa-grad-siggraph08.jpg SIGGRAPH2008 Speaker 
Bios  Gregory P. Silbermanis a patent attorney and partner in the Technology, Intellectual Property 
&#38; Outsourcing Group in Kaye Scholer'sNew York Office. His practice emphasizes representation of domestic 
and foreign clients in legal and business matters related to development and deployment of entertainment, 
software, technology, and services. He studied electrical engineering and computer science at the University 
of California, Berkeley and was formerly patent counsel at Lawrence Berkeley National Laboratory. He 
is also a Certified Information Systems Security Professional and a member of the Board of Editors for 
the Privacy &#38; Data Security Law Journal. C:\Documents and Settings\Gil Irizarry\Desktop\main_logo.jpg 
C:\docs\conoa-grad-siggraph08.jpg SIGGRAPH2008 Speaker Bios  NeerGupta is Executive Counsel for The 
Walt Disney Company in Burbank, California. His practice spans the breadth of the company's operations, 
and he is involved in patent protection, management, licensing, and dispute resolution for all of Disney's 
major businesses, including theme parks and resorts, studios, media networks, and consumer products. 
Prior to joining Disney, he was Senior Patent Counsel at ThinkFireServices, USA, an intellectual property 
consulting company based in New Jersey. He is a graduate of the New York University School of Law and 
Vanderbilt University School of Engineering, he has spoken at numerous conferences, and he is member 
of AIPLA and ACCA. C:\Documents and Settings\Gil Irizarry\Desktop\main_logo.jpg C:\docs\conoa-grad-siggraph08.jpg 
SIGGRAPH2008 Speaker Bios  For the latest version of this presentation, please go to http://www.conoa.com/siggraph08 
 C:\Documents and Settings\Gil Irizarry\Desktop\main_logo.jpg C:\docs\conoa-grad-siggraph08.jpg SIGGRAPH2008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401187</section_id>
		<sort_key>550</sort_key>
		<section_seq_no>17</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Line drawings from 3D models]]></section_title>
		<section_page_from>17</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098723</person_id>
				<author_profile_id><![CDATA[81100203803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Szymon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098724</person_id>
				<author_profile_id><![CDATA[81351604051]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Forrester]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cole]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098725</person_id>
				<author_profile_id><![CDATA[81100499844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeCarlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098726</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401188</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<pages>356</pages>
		<display_no>39</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Line drawings from 3D models]]></title>
		<page_from>1</page_from>
		<page_to>356</page_to>
		<doi_number>10.1145/1401132.1401188</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401188</url>
		<abstract>
			<par><![CDATA[<p>Nonphotorealistic rendering techniques, including line drawings, can be remarkably efficient at conveying shape and meaning with a minimum of visual distraction. This class will describe techniques for automated rendering of 3D models using a number of sparse line drawing styles, for both artistic and illustrative purposes. We will mathematically define lines such as silhouettes, contours, creases, suggestive contours and highlights, and apparent ridges and valleys. We then describe algorithms for finding lines efficiently, including object- and image-space methods, and discuss methods for stylization and level-of-detail control. Finally, we provide a brief introduction to concepts of visual perception, including the information content of line drawings and the effects of abstraction and detail on attention.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098727</person_id>
				<author_profile_id><![CDATA[81100203803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Szymon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098728</person_id>
				<author_profile_id><![CDATA[81351604051]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Forrester]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cole]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098729</person_id>
				<author_profile_id><![CDATA[81100499844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeCarlo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098730</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>330299</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cipolla, R., and Giblin, P. J. 2000. <i>Visual Motion of Curves and Surfaces</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Do Carmo, M. P. 1976. <i>Differential Geometry of Curves and Surfaces</i>. Prentice-Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gravesen, J., and Ungstrup, M. 2002. Constructing invariant fairness measures for surfaces. <i>Advances in Computational Mathematics 17</i>, 67--88.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77527</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J. 1990. <i>Solid Shape</i> MIT press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134035</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Moreton, H., and S&#233;quin, C. 1992. Functional optimization for fair surface design. In <i>Proceedings of ACM SIGGRAPH 1992</i>, vol. 26, 167--176.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383286</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Agrawala, M., and Stolte, C. 2001. Rendering Effective RouteMaps: Improving Usability Through Generalization. In <i>Proceedings of ACM SIGGRAPH 2001</i>, Computer Graphics Proceedings, Annual Conference Series, 241--250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>806007</ref_obj_id>
				<ref_obj_pid>800196</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Appel, A. 1967. The notion of quantitative invisibility and the machine rendering of solids. In <i>Proceedings of the 1967 22nd national conference</i>, ACM Press, New York, NY, USA, 387--393.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1006082</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ashikhmin, M. 2004. Image-space silhouettes for unprocessed models. In <i>GI '04: Proceedings of the 2004 conference on Graphics interface</i>, Canadian Human-Computer Communications Society, School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada, 195--202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>304999</ref_obj_id>
				<ref_obj_pid>304893</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Barequet, G., Duncan, C. A., Goodrich, M. T., Kumar, S., and Pop, M. 1999. Efficient perspective-accurate silhouette computation. In <i>SCG '99: Proceedings of the fifteenth annual symposium on Computational geometry</i>, ACM Press, New York, NY, USA, 417--418.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383681</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Barla, P., Thollot, J., and Sillion, F. X. 2005. Geometric Clustering for Line Drawing Simplification. In <i>Rendering Techniques 2005: 16th Eurographics Workshop on Rendering</i>, 183--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Barrow, H., and Tenenbaum, J. 1981. Interpreting Line Drawings as Three-Dimensional Surfaces. <i>Artificial Intelligence 17</i>, 75--116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>335604</ref_obj_id>
				<ref_obj_pid>335600</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Belhumeur, P. N., Kriegman, D. J., and Yuille, A. L. 1999. The Bas-Relief Ambiguity. <i>International Journal of Computer Vision 35</i>, 1, 33--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>792865</ref_obj_id>
				<ref_obj_pid>792757</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Belyaev, A., Pasko, A., and Kunii, T. 1998. Ridges and ravines on implicit surfaces. In <i>Computer Graphics International 98</i>, 530--535.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2145090</ref_obj_id>
				<ref_obj_pid>2145086</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Belyaev, A., and Anoshkina, E. 2005. Detection of Surface Creases in Range Data. In <i>Eleventh IMA Conference on The Mathematics of Surfaces</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826471</ref_obj_id>
				<ref_obj_pid>826028</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Benichou, F., and Elber, G. 1999. Output Sensitive Extraction of Silhouettes from Polygonal Geometry. In <i>PG '99: Proceedings of the 7th Pacific Conference on Computer Graphics and Applications</i>, IEEE Computer Society, Washington, DC, USA, 60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Bremer, D., and Hughes, J. 1998. Rapid approximate silhouette rendering of implicit surfaces. In <i>Implicit Surfaces 98</i>, 155--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276402</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Breslav, S., Szerszen, K., Markosian, L., Barla, P., and Thollot, J. 2007. Dynamic 2D patterns for shading 3D scenes. <i>ACM Trans. Graph. 26</i>, 3, 20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1037235</ref_obj_id>
				<ref_obj_pid>1037210</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Brosz, J., Samavati, F., and Sousa, M. C. 2004. Silhouette rendering based on stability measurement. In <i>SCCG '04: Proceedings of the 20th spring conference on Computer graphics</i>, ACM Press, New York, NY, USA, 157--167.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073222</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Burns, M., Klawe, J., Rusinkiewicz, S., Finkelstein, A., and DeCarlo, D. 2005. Line Drawings from Volume Data. In <i>SIGGRAPH '05: Proceedings of the 32nd annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA. To appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>11275</ref_obj_id>
				<ref_obj_pid>11274</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Canny, J. 1986. A Computational Approach to Edge Detection. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 8</i>, 6, 679--698.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124734</ref_obj_id>
				<ref_obj_pid>1124728</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Coconu, L., Deussen, O., and Hege, H.-C. 2006. Real-time penandink illustration of landscapes. In <i>NPAR '06: Proceedings of the 4th international symposium on Non-photorealistic animation and rendering</i>, 27--35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383942</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Cole, F., DeCarlo, D., Finkelstein, A., Kin, K., Morley, K., and Santella, A. 2006. Directing Gaze in 3D Models with Stylized Focus. <i>Eurographics Symposium on Rendering</i> (June), 377--387.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1377985</ref_obj_id>
				<ref_obj_pid>1377980</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Cole, F., and Finkelstein, A. 2008. Partial Visibility for Stylized Lines. In <i>NPAR 2008</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360687</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Cole, F., Golovinskiy, A., Limpaecher, A., Barros, H. S., Finkelstein, A., Funkhouser, T., and Rusinkiewicz, S. 2008. Where Do People Draw Lines? <i>ACM Transactions on Graphics (Proc. SIGGRAPH) 27</i>, 3 (Aug.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Cs&#233;bfalvi, B., Mroz, L., Hauser, H., K&#246;nig, A., and Gr&#246;ller, E. 2001. Fast Visualization of Object Contours by Non-Photorealistic Volume Rendering. In <i>Eurographics 01</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566650</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[DeCarlo, D., and Santella, A. 2002. Stylization and Abstraction of Photographs. <i>ACM Transactions on Graphics 21</i>, 3 (July), 769--776.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882354</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[DeCarlo, D., Finkelstein, A., Rusinkiewicz, S., and Santella, A. 2003. Suggestive contours for conveying shape. <i>ACM Trans. Graph. 22</i>, 3, 848--855.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>987661</ref_obj_id>
				<ref_obj_pid>987657</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[DeCarlo, D., Finkelstein, A., and Rusinkiewicz, S. 2004. Interactive rendering of suggestive contours with temporal coherence. In <i>NPAR '04: Proceedings of the 3rd international symposium on Nonphotorealistic animation and rendering</i>, ACM Press, New York, NY, USA, 15--145.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1274881</ref_obj_id>
				<ref_obj_pid>1274871</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[DeCarlo, D., and Rusinkiewicz, S. 2007. Highlight lines for conveying shape. In <i>NPAR '07: Proceedings of the 5th international symposium on Non-photorealistic animation and rendering</i>, 63--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344792</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Deussen, O., and Strothotte, T. 2000. Computer-generated pen-and-ink illustration of trees. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 13--18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>858657</ref_obj_id>
				<ref_obj_pid>858619</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Dong, F., Clapworthy, G. J., Lin, H., and Krokos, M. A. 2003. Nonphotorealistic Rendering of Medical Volume Data. <i>IEEE Comput. Graph. Appl. 23</i>, 4, 44--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91422</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Dooley, D., and Cohen, M. F. 1990. Automatic illustration of 3D geometric models: lines. In <i>SI3D '90: Proceedings of the 1990 symposium on Interactive 3D graphics</i>, ACM Press, New York, NY, USA, 77--82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732296</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Durand, F., Ostromoukhov, V., Miller, M., Duranleau, F., and Dorsey, J. 2001. Decoupling Strokes and High-Level Attributes for Interactive Traditional Drawing. <i>In Rendering Techniques 2001: 12th Eurographics Workshop on Rendering</i>, 71--82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375241</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Ebert, D., and Rheingans, P. 2000. Volume illustration: non-photorealistic rendering of volume models. In <i>VIS '00: Proceedings of the conference on Visualization '00</i>, IEEE Computer Society Press, Los Alamitos, CA, USA, 195--202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97890</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Elber, G., and Cohen, E. 1990. Hidden curve removal for free form surfaces. In <i>SIGGRAPH '90: Proceedings of the 17th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 95--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614310</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Elber, G. 1995. Line Art Rendering via a Coverage of Isoparametric Curves. <i>IEEE Transactions on Visualization and Computer Graphics 1</i>, 3, 231--239.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Elber, G. 1995. Line illustrations in computer graphics. <i>The Visual Computer 11</i>, 6, 290--296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614393</ref_obj_id>
				<ref_obj_pid>614269</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Elber, G. 1998. Line Art Illustrations of Parametric and Implicit Forms. <i>IEEE Transactions on Visualization and Computer Graphics 4</i>, 1, 71--81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Elber, G. 1999. Interactive Line Art Rendering of Freeform Surfaces. <i>Computer Graphics Forum 18</i>, 3, 1--1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340922</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Girshick, A., Interrante, V., Haker, S., and Lemoine, T. 2000. Line direction matters: an argument for the use of principal directions in 3D line drawings. In <i>NPAR '00: Proceedings of the 1st international symposium on Non-photorealistic animation and rendering</i>, ACM Press, New York, NY, USA, 43--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280950</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Gooch, A., Gooch, B., Shirley, P., and Cohen, E. 1998. A non-photorealistic lighting model for automatic technical illustration. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 447--452.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300526</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Gooch, B., Sloan, P.-P. J., Gooch, A., Shirley, P., and Riesenfeld, R. 1999. Interactive technical illustration. In <i>SI3D '99: Proceedings of the 1999 symposium on Interactive 3D graphics</i>, ACM Press, New York, NY, USA, 31--38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>558817</ref_obj_id>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Gooch, B., and Gooch, A. 2001. <i>Non-Photorealistic Rendering</i>. A K Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>966133</ref_obj_id>
				<ref_obj_pid>966131</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Gooch, B., Reinhard, E., and Gooch, A. 2004. Human facial illustrations: Creation and psychophysical evaluation. <i>ACM Trans. Graph. 23</i>, 1, 27--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1274880</ref_obj_id>
				<ref_obj_pid>1274871</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Goodwin, T., Vollick, I., and Hertzmann, A. 2007. Isophote distance: a shading approach to artistic stroke thickness. In <i>NPAR '07: Proceedings of the 5th international symposium on Non-photorealistic animation and rendering</i>, 53--62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383539</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Grabli, S., Turquin, E., Durand, F., and Sillion, F. 2004. Programmable Style for NPR Line Drawing. In <i>Eurographics Symposium on Rendering '04</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1026054</ref_obj_id>
				<ref_obj_pid>1025128</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Grabli, S., Durand, F., and Sillion, F. 2004. Density Measure for Line-Drawing Simplification. In <i>Proceedings of Pacific Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>856728</ref_obj_id>
				<ref_obj_pid>851039</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Hamel, J., Schlechtweg, S., and Strothotte, T. 1998. An Approach to Visualizing Transparency in Computer-Generated Line Drawings. In <i>IV '98: Proceedings of the International Conference on Information Visualisation</i>, IEEE Computer Society, Washington, DC, USA, 151.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345074</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A., and Zorin, D. 2000. Illustrating smooth surfaces. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 517--526.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>735238</ref_obj_id>
				<ref_obj_pid>647781</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A. 2001. Paint by relaxation. In <i>Computer Graphics International 2001</i>, 47--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192186</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Hsu, S. C., and Lee, I. H. H. 1994. Drawing and animation using skeletal strokes. In <i>SIGGRAPH '94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 109--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>833836</ref_obj_id>
				<ref_obj_pid>832271</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Interrante, V., Fuchs, H., and Pizer, S. 1995. Enhancing Transparent Skin Surfaces with Ridge and Valley Lines. In <i>VIS '95: Proceedings of the 6th conference on Visualization '95</i>, IEEE Computer Society, Washington, DC, USA, 52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245567</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Interrante, V., Fuchs, H., and Pizer, S. 1996. Illustrating transparent surfaces with curvature-directed strokes. In <i>VIS '96: Proceedings of the 7th conference on Visualization '96</i>, IEEE Computer Society Press, Los Alamitos, CA, USA, 211--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Isenberg, T., Halper, N., and Strothotte, T. 2002. Stylizing Silhouettes at Interactive Rates: From Silhouette Edges to Silhouette Strokes. <i>Computer Graphics Forum 21</i>, 3, 249--249.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>858656</ref_obj_id>
				<ref_obj_pid>858619</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Isenberg, T., Freudenberg, B., Halper, N., Schlechtweg, S., and Strothotte, T. 2003. A Developer's Guide to Silhouette Algorithms for Polygonal Models. <i>IEEE Comput. Graph. Appl. 23</i>, 4, 28--37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Isenberg, T., and Brennecke, A. 2006. G-strokes: A concept for simplifying line stylization. In <i>Computers &amp; Graphics</i>, vol. 30, 754--766.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Itti, L., and Koch, C. 2000. A Saliency-based search mechanism for overt and covert shifts of visual attention. <i>Vision Research 40</i>, 1489--1506.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Jeong, K., Ni, A., Lee, S., and Markosian, L. 2005. Detail control in line drawings of 3D meshes. In <i>The Visual Computer</i>, vol. 21, 698--706.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276401</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Judd, T., Durand, F., and Adelson, E. 2007. Apparent ridges for line drawing. <i>ACM Trans. Graph. 26</i>, 3, 19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566648</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Kalnins, R. D., Markosian, L., Meier, B. J., Kowalski, M. A., Lee, J. C., Davidson, P. L., Webb, M., Hughes, J. F., and Finkelstein, A. 2002. WYSIWYG NPR: drawing strokes directly on 3D models. In <i>SIGGRAPH '02: Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 755--762.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882355</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Kalnins, R. D., Davidson, P. L., Markosian, L., and Finkelstein, A. 2003. Coherent stylized silhouettes. <i>ACM Trans. Graph. 22</i>, 3, 856--861.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1023383</ref_obj_id>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Kalnins, R. D. 2004. <i>WYSIWYG NPR: Interactive Stylization for Stroke-Based Rendering of 3D Animation</i>. PhD thesis, Princeton University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1274879</ref_obj_id>
				<ref_obj_pid>1274871</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Kaplan, M. 2007. Hybrid quantitative invisibility. In <i>NPAR '07: Proceedings of the 5th international symposium on Non-photorealistic animation and rendering</i>, 51--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081511</ref_obj_id>
				<ref_obj_pid>1081432</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Kindlmann, G., Whitaker, R., Tasdizen, T., and Moller, T. 2003. Curvature-based transfer functions for direct volume rendering: methods and applications. In <i>IEEE Visualization 2003</i>, 513--520.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Knill, D. C. 1992. Perception of surface contours and surface shape: from computation to psychophysics. <i>Journal of the Optical Society of America A 9</i>, 9, 1449--1464.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Knill, D. C. 2001. Contour into texture: information content of surface contours and texture flow. <i>Journal of the Optical Society of America A 18</i>, 1, 12--35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J., and van Doorn, A. J. 1982. The Shape of Smooth Objects and the Way Contours End. <i>Perception 11</i>, 129--137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J. 1984. What does the occluding contour tell us about solid shape? <i>Perception 13</i>, 321--330.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77527</ref_obj_id>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J. 1990. <i>Solid Shape</i>. MIT press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J., van Doorn, A., Christou, C., and Lappin, J. 1996. Shape constancy in pictorial relief. <i>Perception 25</i>, 155--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J., and van Doorn, A. 1998. The Structure of Relief. <i>Advances in Imaging and Electron Physics 103</i>, 65--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. J., van Doorn, A. J., Kappers, A. M., and Todd, J. T. 2001. Ambiguity and the 'mental eye' in pictorial relief. <i>Perception 30</i>, 431--448.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340918</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Lake, A., Marshall, C., Harris, M., and Blackstein, M. 2000. Stylized rendering techniques for scalable real-time 3D animation. In <i>NPAR '00: Proceedings of the 1st international symposium on Non-photorealistic animation and rendering</i>, 13--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124735</ref_obj_id>
				<ref_obj_pid>1124728</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Lee, H., Kwon, S., and Lee, S. 2006. Real-time pencil rendering. In <i>NPAR '06: Proceedings of the 4th international symposium on Non-photorealistic animation and rendering</i>, 37--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276400</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Lee, Y., Markosian, L., Lee, S., and Hughes, J. F. 2007. Line drawings via abstracted shading. <i>ACM Trans. Graph. 26</i>, 3, 18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602131</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Lu, A., Morris, C. J., Ebert, D. S., Rheingans, P., and Hansen, C. 2002. Non-photorealistic volume rendering using stippling techniques. In <i>VIS '02: Proceedings of the conference on Visualization '02</i>, IEEE Computer Society, Washington, DC, USA, 211--218.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508542</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Lum, E. B., and Ma, K.-L. 2002. Hardware-accelerated parallel non-photorealistic volume rendering. In <i>NPAR '02: Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering</i>, ACM Press, New York, NY, USA, 67--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Lum, E. B., and Ma, K.-L. 2005. Expressive line selection by example. <i>The Visual Computer 21</i>, 8--10, 811--820.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267085</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Ma, K.-L., and Interrante, V. 1997. Extracting feature lines from 3D unstructured grids. In <i>VIS '97: Proceedings of the 8th conference on Visualization '97</i>, IEEE Computer Society Press, Los Alamitos, CA, USA, 285--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Malik, J. 1987. Interpreting Line Drawings of Curved Objects. <i>International Journal of Computer Vision 1</i>, 1, 73--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258894</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Markosian, L., Kowalski, M. A., Goldstein, D., Trychin, S. J., Hughes, J. F., and Bourdev, L. D. 1997. Real-time nonphotorealistic rendering. In <i>SIGGRAPH '97: Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 415--420.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Martin, D., Fekete, J., and Torres, J. C. 2002. Flattening 3D objects using silhouettes. <i>Computer Graphics Forum 21</i>, 3, 239--239.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Masuch, M., Schlechtweg, S., and Schnwlder, B. 1997. dali! -- Drawing Animated Lines! In <i>Simulation and Animation 97</i>, 87--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Masuch, M., Schuhmann, L., and Schlechtweg, S. 1998. Animating Frame-To-Frame-Coherent Line Drawings for Illustrated Purposes. In <i>Simulation and Animation 98</i>, 101--112.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>856773</ref_obj_id>
				<ref_obj_pid>851039</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Masuch, M., and Strothotte, T. 1998. Visualising Ancient Architecture using Animating Line Drawings. In <i>IV '98: Proceedings of the International Conference on Information Visualisation</i>, IEEE Computer Society, Washington, DC, USA, 261.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>987663</ref_obj_id>
				<ref_obj_pid>987657</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[McGuire, M., and Hughes, J. F. 2004. Hardware-determined feature edges. In <i>NPAR '04: Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering</i>, ACM Press, New York, NY, USA, 35--147.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Nagy, Z., and Klein, R. 2004. High-Quality Silhouette Illustration for Texture-Based Volume Rendering. In <i>WSCG '04</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Neumann, P., Isenberg, T., and Carpendale, M. S. T. 2007. NPR Lenses: Interactive Tools for Non-Photorealistic Line Drawings. In <i>Smart Graphics</i>, 10--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111435</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Ni, A., Jeong, K., Lee, S., Lee, S., and Markosian, L. 2006. Multiscale line drawings from 3D meshes. In <i>I3D '06: Proceedings of the 2006 symposium on Interactive 3D graphics and games</i>, 133--137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340920</ref_obj_id>
				<ref_obj_pid>340916</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Northrup, J. D., and Markosian, L. 2000. Artistic silhouettes: a hybrid approach. In <i>NPAR '00: Proceedings of the 1st international symposium on Non-photorealistic animation and rendering</i>, ACM Press, New York, NY, USA, 31--37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015768</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Ohtake, Y., Belyaev, A., and Seidel, H.-P. 2004. Ridge-valley lines on meshes via implicit surface fitting. <i>ACM Trans. Graph. 23</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Page, D. L., Koschan, A., Sun, Y., Paik, J., and Abidi, A. 2001. Robust Crease Detection and Curvature Estimation of Piecewise Smooth Surfaces from Triangle Mesh Approximations Using Normal Voting. In <i>Proc. Conference on Computer Vision and Pattern Recognition</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Pauly, M., Keiser, R., and Gross, M. 2003. Multi-Scale Feature Extraction on Point-Sampled Models. In <i>Proc. Eurographics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Pearson, D., and Robinson, J. 1985. Visual Communication at Very Low Data Rates. <i>Proc. IEEE 4</i> (Apr.), 795--812.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Phillips, F., Todd, J. T., Koenderink, J. J., and Kappers, A. M. 2003. Perceptual representation of visible surfaces. <i>Perception and Psychophysics 65</i>, 5, 747--762.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383328</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Praun, E., Hoppe, H., Webb, M., and Finkelstein, A. 2001. Realtime hatching. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 581.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300539</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Raskar, R., and Cohen, M. 1999. Image precision silhouette edges. In <i>SI3D '99: Proceedings of the 1999 symposium on Interactive 3D graphics</i>, ACM Press, New York, NY, USA, 135--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383525</ref_obj_id>
				<ref_obj_pid>383507</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Raskar, R. 2001. Hardware Support for Non-photorealistic Rendering. In <i>Proc. Graphics Hardware</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015779</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Raskar, R., Han Tan, K., Feris, R., Yu, J., and Turk, M. 2004. Non-photorealistic Camera: Depth Edge Detection and Stylized Rendering using Multi-Flash Imaging. <i>ACM Trans. Graph. 23</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258060</ref_obj_id>
				<ref_obj_pid>258049</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Rieger, J. H. 1997. Topographical Properties of Generic Images. <i>International Journal of Computer Vision 23</i>, 1, 79--92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[Rossignac, J., and van Emmerik, M. 1992. Hidden Contours on a Framebuffer. <i>Proc. of 7th Workshop on Computer Graphics Hardware</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826529</ref_obj_id>
				<ref_obj_pid>826029</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Rossl, C., and Kobbelt, L. 2000. Line-art rendering of 3D-models. In <i>Pacific Graphics 00</i>, 87--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Saito, T., and Takahashi, T. 1990. Comprehensible rendering of 3D shapes. In <i>SIGGRAPH '90: Proceedings of the 17th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 197--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>197974</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Saito, T. 1994. Real-time previewing for volume visualization. In <i>VVS '94: Proceedings of the 1994 symposium on Volume visualization</i>, ACM Press, New York, NY, USA, 99--106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192185</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M. P., Anderson, S. E., Barzel, R., and Salesin, D. H. 1994. Interactive pen-and-ink illustration. In <i>SIGGRAPH '94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 101--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237286</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[Salisbury, M., Anderson, C., Lischinski, D., and Salesin, D. H. 1996. Scale-dependent reproduction of pen-and-ink illustrations. In <i>SIGGRAPH '96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 461--468.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344935</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Sander, P. V., Gu, X., Gortler, S. J., Hoppe, H., and Snyder, J. 2000. Silhouette clipping. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 327--334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>987669</ref_obj_id>
				<ref_obj_pid>987657</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Santella, A., and DeCarlo, D. 2004. Visual interest and NPR: an evaluation and manifesto. In <i>NPAR 2004</i>, 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1008967</ref_obj_id>
				<ref_obj_pid>1008963</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[Schein, S., and Elber, G. 2004. Adaptive extraction and visualization of silhouette curves from volumetric datasets. <i>Vis. Comput. 20</i>, 4, 243--252.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Shesh, A., and Chen, B. 2008. Efficient and Dynamic Simplification of Line Drawings. <i>Computer Graphics Forum (Proc. Eurographics 2008) 27</i>, 2, 537--545.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Sousa, M. C., Foster, K., Wyvill, B., and Samavati, F. 2003. Precise Ink Drawing of 3D Models. <i>Computer Graphics Forum 22</i>, 3, 369--369.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[Sousa, M. C., and Prusinkiewicz, P. 2003. A Few Good Lines: Suggestive Drawing of 3D Models. <i>Computer Graphics Forum 22</i>, 3, 381--381.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Stevens, K. A. 1981. The Visual Interpretation of Surface Contours. <i>Artificial Intelligence 17</i>, 47--73.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>544522</ref_obj_id>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Strothotte, T., and Schlechtweg, S. 2002. <i>Non-Photorealistic Computer Graphics</i>. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>246268</ref_obj_id>
				<ref_obj_pid>246267</ref_obj_pid>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[Thirion, J.-P., and Gourdon, A. 1996. The 3D Marching Lines Algorithm. <i>Graphical Models and Image Processing 58</i>, 6 (Nov.), 503--509.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>375243</ref_obj_id>
				<ref_obj_pid>375213</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[Treavett, S. M. F., and Chen, M. 2000. Pen-and-Ink rendering in volume visualisation. In <i>VIS '00: Proceedings of the conference on Visualization '00</i>, IEEE Computer Society Press, Los Alamitos, CA, USA, 203--210.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[Waltz, D. L. 1975. Understanding Line Drawings of Scenes with Shadows. In <i>The Psychology of Computer Vision</i>, P. Winston, Ed. McGrawHill, 19--92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[Watanabe, K., and Belyaev, A. G. 2001. Detection of Salient Curvature Features on Polygonal Surfaces. <i>Computer Graphics Forum (Proc. Eurographics 2001) 20</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508540</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[Webb, M., Praun, E., Finkelstein, A., and Hoppe, H. 2002. Fine tone control in hardware hatching. In <i>NPAR '02: Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering</i>, 53--ff.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>833424</ref_obj_id>
				<ref_obj_pid>832264</ref_obj_pid>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[Whelan, J., and Visvalingam, M. 2003. Formulated silhouettes for sketching terrain. <i>Theory and Practice of Computer Graphics, 2003. Proceedings</i>, 90--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>987674</ref_obj_id>
				<ref_obj_pid>987657</ref_obj_pid>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[Wilson, B., and Ma, K.-L. 2004. Rendering complexity in computergenerated pen-and-ink illustrations. In <i>NPAR '04: Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering</i>, ACM Press, New York, NY, USA, 129--137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G., and Salesin, D. H. 1994. Computer-generated penandink illustration. In <i>SIGGRAPH '94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237287</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[Winkenbach, G., and Salesin, D. H. 1996. Rendering parametric surfaces in pen and ink. In <i>SIGGRAPH '96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 469--476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313164</ref_obj_id>
				<ref_obj_pid>1313046</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[Xie, X., He, Y., Tian, F., Seah, H.-S., Gu, X., and Qin, H. 2007. An Effective Illustrative Visualization Framework Based on Photic Extremum Lines (PELs). <i>IEEE Transactions on Visualization and Computer Graphics 13</i>, 6 (Nov./Dec.), 1328--1335.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386335</ref_obj_id>
				<ref_obj_pid>2386332</ref_obj_pid>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[Xu, H., Nguen, M., Yuan, X., and Chen, B. 2004. Interactive Silhouette Rendering for Point-Based Models. In <i>Proc. Eurographics Symposium on Point-Based Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338571</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[Yoshizawa, S., Belyaev, A., Yokota, H., and Seidel, H.-P. 2007. Fast and Faithful Geometric Algorithm for Detecting Crest Lines on Meshes. In <i>Pacific Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>988876</ref_obj_id>
				<ref_obj_pid>988834</ref_obj_pid>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[Zakaria, N., and Seidel, H.-P. 2004. Interactive stylized silhouette for point-sampled geometry. In <i>GRAPHITE '04: Proceedings of the 2nd international conference on Computer graphics and interactive techniques in Australasia and Southe East Asia</i>, ACM Press, New York, NY, USA, 242--249.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[Zander, J., Isenberg, T., Schlechtweg, S., and Strothotte, T. 2004. High Quality Hatching. <i>Computer Graphics Forum 23</i>, 3, 421--421.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Line Drawings from 3D Models SIGGRAPH 2008Class Tuesday,12August,8:30AM 12:15PM Organizer SzymonRusinkiewicz 
PrincetonUniversity  Lecturers ForresterCole PrincetonUniversity DougDeCarlo RutgersUniversity AdamFinkelstein 
PrincetonUniversity   Course Description Nonphotorealistic rendering techniques, including line drawings, 
can be remarkably ef.cient at conveying shape and meaning with a minimum of visual distraction. This 
class will describe techniques for auto­mated rendering of3D modelsusing anumberof sparselinedrawing 
styles,forboth artisticandillustrative purposes. We will mathematically de.ne lines such as silhouettes, 
contours, creases, suggestive contours and highlights, and apparent ridges and valleys. We then describe 
algorithms for .nding lines ef.ciently, including object-and image-space methods, and discuss methods 
for stylization and level-of-detail control. Finally, weprovide abriefintroductionto concepts of visualperception,including 
theinformation content oflinedrawingsand theeffects of abstraction anddetail onattention. Prerequisites 
Basicfamiliarity with thegraphicspipeline and someknowledge of calculus andlinear algebra. Instructor 
Bios Forrester Cole is afourth-yearPh.D. studentinthegraphicsgroupatPrincetonUniversity. His research 
interests aremainlyinnon-photorealistic rendering,particularly interactiveNPRalgorithms and algorithms 
for artistic stylization and abstraction. He is currently investigating improved algorithms for interactive, 
stylized line drawing. Prior to coming to Princeton, Forrester was a programmer at Pandemic Studios, 
wherehewroteengine codeforPlayStation2 andXboxgames. Doug DeCarlo received BSdegrees in computer science 
and computer engineering fromCarnegie Mel­lonin1991, andhisPh.D.in computer sciencefrom theUniversity 
ofPennsylvania in1998. Heis currently an associateprofessorintheDepartment ofComputerScience with ajoint 
appointmentintheCenterfor CognitiveScience atRutgersUniversity. His researchin computergraphics exploreshow 
accounts ofhuman perception and communication can inform the design of computer systems that engage in 
effective visual communication. Adam Finkelstein is an associate professor in the Computer Science Department 
at Princeton Univer­sity.Hiscurrent researchfocusesonnon-photorealistic rendering and animation.Adamreceived 
adoctorate from the University of Washington in 1996. From 1987 to 1990, he worked at Tibco developing 
software forpeople whotrade stock.Hereceived aBAin1987fromSwarthmoreCollege. Szymon Rusinkiewicz is an 
associateprofessor ofComputerScience atPrincetonUniversity. His work focuses on acquisition and analysis 
of the 3D shape and appearance of real-world objects, including the design of capture devices and data 
structures for ef.cient representation. He also investigates algorithms for processing complex datasets 
of shape and re.ectance, including registration, matching, completion, symmetry analysis, and sampling. 
In addition to data acquisition, his research interests include real-time rendering andperceptually-guideddepiction.HeobtainedhisPh.D.fromStanfordUniversity 
in2001. Acknowledgements Thanks to Rob Kalnins, Lee Markosian, Anthony Santella, and especially Mike 
Burns. Partially supported by the National Science Foundation through grants HLC 0308121, CCF 0347427, 
and CCF 05041185. Somephotographs in slides for PartVII courtesy http://philip.greenspun.com. Course 
Syllabus I. Introductiontothestudy oflines(:15 Finkelstein) Non-photorealistic rendering  Whatisconveyed?(shape, 
markings, material,shading)  Approaches to the study of line drawings  II. Artists linedrawings(:20 
Cole) Studying whatlines artistsdraw  Evaluation  III. Mathematicaldescriptionoflines(:45 Rusinkiewicz) 
 Silhouettes and occluding contours  Basics ofdifferential geometry for surfaces in3D  Suggestive contours 
and highlights; principal highlights  Surface ridges and valleys  Apparent ridges and valleys  Ridges 
and valleys of illumination  Otherlines(creases, surface markings, materialboundaries, topo(isoelevation) 
lines,principal cur­vature lines, etc.)  IV.Perceptionoflinedrawings(:25 DeCarlo) How muchinformation 
is there in drawings?  Ambiguities(bas-relief,linelabeling, etc.)  Psychophysical studies(gauge .gures,etc.) 
 Break (:15) V.Algorithmsforextracting lines(:30 Rusinkiewicz) Image-space algorithms  Bruteforce, 
"marching triangles"  Randomized methods  Hierarchical methods  Methods based on the Gauss map and 
other mappings  Determining line visibility  VI. Stylizationoflinedrawings(:25 Finkelstein) Parameterization 
 Temporal coherence  Userinteraction  VII. Abstractionand evaluation(:30 DeCarlo) Abstraction in 
NPR  Evaluation using eye tracking  VIII. Controlling detail and attention(:20 Cole) Control of line 
density  Stroke simpli.cation  Stylized focus   Notes onDifferentialGeometry De.ning and extracting 
suggestive contours, ridges, and valleys on a surface requires an understanding of the basics of differential 
geometry. Here we collect the necessary background, specializedtothe case of surfacesin3D and considering 
only orthonormalbases. For moredetails, consult[Cipolla andGiblin2000,doCarmo1976,Koenderink1990]. Webeginbyconsidering 
a smooth and closed surface S and apoint p .S sitting onthe surface. First-order structure: The .rst-order 
approximation of this surface around this point is the tangentplane there; the unit normal vector n at 
p isperpendicular to thisplane.(We use outward­pointing normal vectors.) Directions in the tangent plane 
at p can be described with respect to three-dimensional basis vectors {su, sv} that span the tangent 
plane. Generally, the three­dimensional vector x that sits in the tangent plane is written in local coordinates 
as [uv]T, where x = u su + v sv. Second-order structure: The unit normal n is a .rst-order quantity; 
it turns out that the interesting second-orderstructuresinvolvederivativesof normal vectors.A directional 
derivative of afunctionde.ned onthe surface(the unit normalbeing onepossiblefunction) speci.eshow that 
function changes as you move in a particular tangent direction. For instance, the directional derivativeDxn 
at p characterizeshow n tips asyou move alongthe surfacefrom p inthedirection x. (This same relationship 
can be conveyed by the differential dn(x), which describes how n changes as a function of a particular 
tangent vector x.) Since derivatives of unit vectors must be inperpendiculardirections, thederivatives 
of n liein the tangentplane. Thisdirectionalderivative canbe written as: Dxn = nu su + nv sv, (1) where 
 nu LMu =, (2) nvMNv wherethe entries L, M and N dependonthelocal surfacegeometry see[Cipolla andGiblin2000] 
fordetails. Note that Dxn dependslinearly on thelength of x. Now, suppose wehavetwo tangentvectors x1 
= u1su + v1sv and x2 = u2su + v2sv. The second fundamental form II at p is a symmetricbilinearform speci.edby: 
II(x1, x2) = (Dx1n) · x2 = (Dx2n) · x1 = [ u1 v1 ] L M M N u2 v2 . (3) 1 (Thisdiffersin signfrom[doCarmo1976] 
dueto our choice of outward pointing normals.) Since II is symmetric, we use II(x) as a shorthand to 
indicate only one vector product has been performed;so II(x)= Dxn, and II(x1, x2)= II(x1) · x2 = II(x2) 
· x1. Thenormal curvature of a surface S at apoint p measuresits curvaturein a speci.cdirection x inthetangentplane, 
andisde.nedinterms ofthe secondfundamentalform. The normalcurvature, written as .n(x),is: II(x, x) .n(x)= 
. x · x Notice how the length and sign of x do not affect the normal curvature. On a smooth surface, 
the normal curvature varies smoothly withdirection x, and rangesbetween theprincipal curvatures .1 and 
.2 at p. These are realizedintheir respectiveprincipal curvaturedirections e1 and e2, which areperpendicular 
and unitlength. The Gaussian curvature K is equal to the product of the principal curvatures: K = .1.2, 
and the mean curvature H is their average: H =(.1 + .2)/2. Wherever K is strictly negative(sothat only 
one of .1 or .2 is negative),there are twodirections alongwhich the curvatureis zero. These directions, 
called the asymptotic directions, play a central role in where suggestive contours are located. The vectorDxn 
canbebrokeninto two components;in thedirection of x, andperpendicularto it. Thelengthof the componentin 
thedirection of x is simplythe normalcurvature .n. Thelength of theperpendicular componentisknown as 
the geodesic torsion tg, anddescribeshowmuch the normal vector tilts to the side as you move in the direction 
of x. If we de.ne the perpendicular direction x. as: x. = n ×x then wehave: Dxn = II(x)= .n(x) x + tg(x) 
x. (4) It follows that tg(x)= II(x, x.)/x · x. Furthermore, when x is an asymptotic direction, Dxn is 
perpendicular tox andit canbe shown that t 2(x)= -K [Koenderink1990]. g Principal coordinates: Usingtheprincipaldirections{e1, 
e2}asthelocalbasisleadsto principal coordinates. In principal coordinates, the matrix in equations (2) 
and (3) is diagonal with the principalcurvatures as entries: .1 0 . 0 .2 Given [uv]T =[cosf sinf ]T, 
where f is the angle in the tangent plane measured between a particulardirection ande1, thisleads to 
the well-knownEulerformulafor normal curvature: 2 .n(f )= .1cos f + .2sin2f and thefollowingfor thegeodesic 
torsion: tg(f )=(.2-.1) sinf cosf (where the sign oftg depends on ourde.nition of x., above). 2 Third-order 
structure: In order to de.ne ridges and valleys, and to analyze how suggestive contours move across a 
surface, we will need additional notation that describes derivatives of curvature. The gradient ..r is 
a vector in the tangent plane that locally speci.es the magnitude anddirection of maximal changein .r 
on the surface. Inthefollowing, we useprincipalcoordinates,asthethird-orderderivativesare much simplerto 
state.Inthiscase, .ndingderivativesof normalcurvaturesinvolvestakingthedirectionalderivative of II in 
aparticulartangentdirectionx. The resultis writtenin terms of a symmetrictrilinearform C,builtfrom a2×2×2(rank-3)tensor 
whose entriesdependonthethirdderivatives ofthe surface [Gravesen andUngstrup2002]. Suchderivativeshavebeeningredientsin 
measures offairnessfor variational surface modeling[Gravesen andUngstrup2002,Moreton andS´equin1992]. 
We write C with either two or three arguments indicating how many times a vector is multiplied onto the 
underlying tensor. Thus, C(x, x) is a vector and C(x, x, x) is a scalar. The order of the arguments does 
not matter, as C is symmetric. In principal coordinates, the tensor describing C has4 unique entries[Gravesen 
andUngstrup2002]: P = De1.1, Q = De2.1, S = De1.2, and T = De2.2 Thisleadstothe .rst-orderapproximationof 
thematrixinequation(3) towards x = u e1 + v e2 as: LM .10 PQ QS + u + v (5) MN 0 .2 QS ST (written with 
the tensor expanded into two matrices on the right to avoidcumbersome notation, already multiplied once 
by [uv]T.) Finally, we note how to compute the gradient and directional derivative of the normal curvature 
.n using C: C(x, x) gu e1 + gv e2 ..n(x)= = , x · xx · x where gu Pu2 + 2Quv + Sv2 = gv Qu2 + 2Suv + 
Tv2 and Dx.n(x)C(x, x, x)Pu3 + 3Qu2 v + 3Suv2 + Tv3 == . x x 3 x 3 References CIPOLLA, R., AND GIBLIN, 
P. J. 2000. Visual Motion of Curves and Surfaces. Cambridge UniversityPress. DO CARMO, M. P. 1976. Differential 
Geometry of Curves and Surfaces. Prentice-Hall. GRAVESEN, J., AND UNGSTRUP, M. 2002. Constructing invariant 
fairness measures for surfaces. Advances in Computational Mathematics 17,67 88. KOENDERINK, J. J. 1990. 
Solid Shape. MITpress. MORETON, H., AND S´ 1992. Functional optimization for fair surface design. EQUIN, 
C. In Proceedings of ACM SIGGRAPH 1992, vol.26,167 176. 3 Published Research on Line Drawings from 3D 
Data The following list contains papers describing algorithms for pro­ducing line drawings from 3D data, 
as well as research on the per­ception oflinedrawings. Papers marked with an asterisk(*)are the most 
closely related to this course, and are recommended reading. Detailedbibliographical informationfollows 
this annotated list. Before 1990 [Appel 1967] Introduces an ef.cient algorithm for discover­ing hidden 
lines in drawings of 3D shapes by propagation of quantitative invisibility.  [Waltz 1975]Applies the 
technique of constraint satisfaction to search for consistent labelings of lines in line drawings of 
polyhedral scenes. It works from exhaustive catalogs of all possible con.gurations oflinejunctions. 
 [Barrow1981]Describes algorithmsfor the reconstruction of surfacesfromlinedrawingsby making a range 
of assumptions  (i.e. about surface smoothness). [Stevens 1981] A description of how repeated arrangements 
of lines in line drawings can give rise to shape percepts. Par­allellines(cuttinglines),geodesics andlinesof 
curvatureare investigated.  [Koenderink 1982] A discussion of what lines artists might be drawing. It 
includes a proof that ending contours must terminate in a concave way, yet points out that artists often 
draw convex endings.  * [Koenderink 1984] Seminal paper establishing the relation­shipbetweenthe curvature 
ofthecontourintheimage(the apparent curvature) and the Gaussian curvature of the corre­sponding point 
on the surface. [Pearson 1985] Examined the use of image valleys for pro­ducing linedrawingsfromphotographs. 
 [Canny 1986]Classic method for edge-detection inimages.  [Malik1987]Thegeneralization oflinelabeling[Waltz1975] 
to curved surfaces. It also includes an algorithm for pruning away unlikelyinterpretations.  [Dooley 
1990] Describes automatic generation of boundary, silhouette,discontinuity, andiso-parametriclinestodepict3D 
objects. Main focus is on stylization with an importance, line type, and hiddenness matrix.  [Elber 
1990]De.nes boundary, silhouette, discontinuity, and iso-parametric curves mathematically. Describes 
curve ex­traction, intersection, and visibility methods for B-spline sur­faces. Visibilitypropagatesquantitative 
invisibility.  [Koenderink 1990] Well-known and well-respected book on geometric andperceptual aspects 
of shape.  * [Saito 1990] Uses 2D image processing algorithms to draw discontinuities, edges, and iso-parametric 
lines over 2D ren­derings of3D models. 1992 [Knill 1992] Describes perceptual experiments that suggest 
ourperceptual systemsassumethatsurfacemarkings(i.e. re­.ectanceboundaries)line alonggeodesics on the 
surface.  [Rossignac1992]Introduces a methodofhidden-line removal for contours using thehardware z-buffer. 
 [Hsu 1994] Introduces a comprehensive framework for de­scribing and rendering heavily stylized complex 
2D strokes.  [Saito 1994]Proposes interactive visualization of volumetric databyhierarchically sampling 
areas ofinterestfrom thedata and rendering oriented lines.  1994 * [Salisbury1994]Presentsinteractive 
user-driven methods for creatingline drawings bypainting texture and tone over a 2D referenceimage, with 
optional extracted edges. * [Winkenbach 1994]Describes general principles for creating computer generated 
line drawings from 3D models. Intro­duces stroke textures to apply resolution-dependent texture and tone 
to 3D polygonal surfaces. Describes indication,a technique for elision of detail in drawn textures to 
enhance comprehensibility oflinedrawings.  1995 [Elber1995a]Describesgeneration ofline-artdrawings 
using iso-parametric curves to cover freeform surfaces. A lighting model affectsthedensity of curves 
and noiseis added to avoid a syntheticlook.  [Elber 1995b] Presents several ideas for the improvement 
of line drawings from 3D surfaces. Suggests performing depth cuing by modulating line width and intensity, 
drawing thin light strokes for background lines. Trimming of background lines near intersectionsis also 
suggested.  [Interrante 1995]Suggests using ridge and valley lines as an aide to transparent isosurface 
visualization. Lines are ex­tracted from an isosurface and opacity is modulated by prin­cipal curvature. 
 [Interrante 1996] Describes rendering of transparent isosur­faces using short strokes directed along 
principal curvature withlength modulated byprincipal curvature magnitude.  [Koenderink 1996] A psychophysical 
study that measures and compares shapeperceptsfrom(external) silhouettes,line drawings andshadedimages. 
It uses the samepsychophysical tasks as[Koenderink 2001].  1996 [Salisbury1996]Suggests representingpen-and-ink 
drawings with a grayscale image, a set of discontinuity segments, and associated stroke textures for 
the purposes of producing con­sistent drawings at any scale and resolution. A novel edge­reconstruction 
algorithm allows low-resolution grayscale im­ages to be up-sampled while maintaining sharp discontinu­ities. 
Line drawings are created by rendering the stroke tex­tures on thegrayscale image.  [Thirion 1996] Introduces 
the marching lines algorithm for extraction of line loops along intersections of two closed sur­faces 
in 3D space. Describes drawing crest lines as an aide to visualizingisosurfacesbyintersecting a maximal 
curvature surface with theisosurface.  [Winkenbach 1996] Describes rendering line-art drawings fromparametricfreeform 
surfaces. Details controlled-density hatching to modulate line width based on proximity to other lines. 
Also introduces a method to construct aplanar map for aparametric surface and a method of rendering shadows 
with strokes.  [Ma 1997] Argues for the bene.t of extracting feature lines (contours, ridges and valleys) 
from unstructured triangular meshes. Presents some results in the context of augmenting .uid .ow visualization. 
 1997 * [Markosian 1997]Introduces afastrandomized algorithmfor .nding and tracing contours on polygonal 
surfaces. Exploits frame-to-frame temporal coherence and uses random probes tolocate contour seedpoints 
and trace contours along the sur­face. Uses an acceleratedquantitativeinvisibilitypropagation method 
todifferentiatehidden lines. [Masuch1997]Describes a systemfor creatinganimatedline­drawingsfrom3Dpolygonal 
objects.  [Rieger1997]Presentscomprehensivederivationsand analy­sis of intensity ridges and valleys, 
edges, and other lines de­.ned on images.  [Belyaev 1998] Details and derives formulas for ridges and 
valleys onimplicit surfaces. Uses these togenerate ridges and valleys on an implicit surface as the intersection 
curves of a ridge and valley surface with theimplicit surface.  [Bremer 1998] Describes a method of 
extracting silhou­ettesfromimplicit surfaces using a seed-and-traverse method. Seedingisperformedby ray-surfaceintersection 
and walking along the surface to a silhouette. Traversal is done by Euler integration with apenalty to 
avoiditerativedrift.  [Elber 1998]Describes a method for creating line art render­ings from freeform 
parametric surfaces based on a uniform point sampling of the surface that is independent of the sur­faceparameterization. 
 [Gooch 1998]Proposes an NPR color shading model to aug­mentlinedrawingsin technical illustrations. 
 [Hamel 1998] Introduces transparency in line drawings by modifying line width, density, or style for 
occluded surfaces. Uses severalstatic2D renderings of a scene withtransparency enabled and disabled asinput 
to alinedrawing algorithm.  1998 [Koenderink 1998] A comprehensive overview of the struc­ture of relief 
(i.e. images, height .elds, etc...) that covers everythingfrom thehistoricaldevelopment of the concepts 
of ridges and valleys, to differential structure, to linear features such as the occluding contour and 
cliff curves(closely related to suggestive contours). Note: this was writtenforphysicists, so expect 
math.  [Masuch 1998a] Extends the daLi! system for creating an­imated line-drawings from 3D objects 
with frame-to-frame coherence.  [Masuch 1998b] Describes an application of the daLi! line­drawing systemin 
visualizing ancient architecture.  [Barequet1999]Introduces a method for ef.cientlydetecting silhouette 
edges in 3D meshes by solving a dual problem of intersecting aplane withline segments.  [Belhumeur 1999] 
A theoretical development of the ambi­guity present in shaded imagery. The generalized bas-relief transformation 
also preserves contours and shadow bound­aries.  [Benichou 1999] Details a method that allows for real-time 
selection and rendering of a complete set of silhouette edges froma3D mesh afterapreprocessing of themesh. 
 [Elber 1999] Introduces an interactive method for render­ing of silhouette strokes from freeform models. 
Generates silhouette-oriented strokesin apreprocessing step and uses an accelerated data structure to 
selectively render them in real­time.  [Gooch 1999]Outlines aspects of an interactive technical il­lustration 
system for 3D polygonal models. Describes hard­ware and software methods of drawing silhouettes as well 
as NPR shading models and shadows.  [Raskar 1999] Describes a hardware-accelerated method for rendering 
silhouette edges. It renders back-facing polygons offset towards the camera, such that they show through 
front­facingpolygons.  [Deussen 2000]Details an algorithm for the creation of pen­and-ink drawings of 
trees. Uses silhouettes and hatching to render trunks and branches. Combines individual leaves into largergroups 
representedbyabstract shapes to renderfoliage.  [Ebert 2000] Introduces NPR rendering techniques for 
vol­ume illustration. Topics covered include boundary and sil­houette enhancement, featurehalos, and 
tone shading.  [Girshick2000]Arguesforthe meritof usinglines alongprin­cipaldirections for conveying 
3D shape. Presentspsycholog­ical evidencetosupporttheimportance ofprincipaldirection in perception. Describes 
the creation of principal direction vector .elds on surfaces and the tracing of strokes through the vector 
.eld tocreatelinedrawings.  1999 2000 * [Hertzmann 2000] Describes line drawing rendering tech­niques 
for smooth mesh surfaces. Includes algorithms for fast deterministic detection of silhouettes using a 
dual sur­face intersection, cusp detection, visibility, and computation of smoothdirection .eldssuitableforhatching 
strokes. [Lake 2000]Describes drawing silhouette edges with special strokesbased on underlying curvature, 
and screen-space stoke texturingforhatching and shading.  [Northrup 2000]Details a set of rendering 
methods for lines extracted from 3D geometry. Describes how to smooth self­intersectingsilhouette edgesinimage 
space and createlonger, cleaner silhouette paths using an ID reference image. Sug­gests rendering linepaths 
as texture-mapped triangle strips to achieve various artisticbrush stroke effects.  [Rossl2000]Creates 
a speci.c style ofline-artdrawingsfrom 3D models using hatching along principal directions. The object 
is rendered with surface information (normal, curva­ture) to image space where user-guided segmentation 
is per­formed. Hatching is applied to segmented components along theirprincipaldirections.  [Sander2000]Contains 
a section on ef.cientsilhouettedetec­tion using ahierarchical cone structure.  [Treavett2000]Introduces3Dand 
multi-pass2D methodsfor pen-and-ink rendering of volumes using procedural textures in a standard volume 
rendering pipeline. Discusses integra­tion of the NPR steps with traditional rendering steps to gen­erateimages 
that are mixedphotorealistic andNPR.  [Cs´ebfalvi2001] Presents afast, automatic algorithmfor vi­sualization 
of contours in volumes by rendering areas of high gradientmagnitude modulatedby a view-dependent term. 
De­tails optimizations required to achieve interactive rendering.  [Gooch2001]Bookdescribinga variety 
of non-photorealistic rendering techniques and effects.  [Knill2001]Connects theinformationprovided 
bygeodesics onsurfaceswithhomogenoustexture .ow .  [Koenderink 2001]Afantasticdevelopment thatconnects 
the bas-relief ambiguity [Belhumeur 1999] in shaded imagery with a series of psychophysical experiments 
that demon­strated that percepts arising from photographs of simply shaded objects respect this ambiguity. 
The psychophysical tasks described here could be used to measure percepts of NPRimagery.  [Page 2001] 
Extracts ridge and valley lines from 3D objects using curvature estimated with a normal voting approach. 
 [Praun 2001] Describes a hardware-accelerated method of rendering3Dpolygonal models withlight-dependent 
hatched shading in real-time. Textures are oriented alongprincipaldi­rections such thathatching marks 
convey surface shape.  [Raskar 2001] Adapts previous two-pass hardware-assisted rendering techniques 
to single-pass rendering of silhouettes and creases using moderngraphicshardware.  [Watanabe2001]Detects 
ridge andvalleytriangles on meshes by looking for curvature extrema on the focal surface. The extreme 
regions are thresholded and thinned.  [Isenberg 2002] Describes an algorithm that simpli.es self­intersectingsilhouette 
edgesfrom3D meshes to createlonger, cleanerpaths suitablefor stylized rendering. Differsfrompre­vious 
approaches by using z-buffer techniques and avoiding the use of anID reference image.  2001 2002 [Kalnins 
2002] Details an interactive NPR rendering sys­tem for 3D meshes which allows for artist annotated strokes 
and brush styles to be consistently rendered across multiple frames and viewpoints. Artist input from 
one viewpoint is synthesized and propagated for other viewpoints and several models of scale-varying 
hatching areintroduced.  [Lu2002]Presents aninteractiveNPRvolume renderingtech­nique using stippling. 
Includes provisions for enhancement of silhouette andboundary regions and a method of sketching silhouette 
curves.  [Lum 2002] Describes a hardware-accelerated parallelized NPR volume rendering method for large 
volumes. Renders with tone shading, silhouettes and depth cuing at interactive frame-rates on computer 
clusters.  [Martin2002]Introducesamethodforcreating.atsilhouettes from3D meshes suitablefor stylized 
rendering. Resultsin an ordered stack of 2D .lled polygons which together compose alinedrawing of the3D 
model.  [Strothotte 2002] Provides an introduction and overview of NPRrenderingmethods and areas of 
application. Explains2D and 3D algorithms for pen-and-ink, pencil-sketch, painterly­effects, and otherNPR 
effects.  [Webb2002] Builds on[Praun2001] byintroducing re.ned control over hatching strokes, allowing 
for light areas to be textured with afew strong strokes.  2003 * [DeCarlo 2003] Describes a class of 
lines de.ned on a sur­face, suggestive contours, which complement the set of sur­face contoursindepictingshape. 
Suggestive contoursprovide interior detail to line drawings and are de.ned at points with radial curvature 
of zero. Both object-space and image-space extraction algorithms areprovided. [Dong 2003] Introduces 
new non-interactive silhouette and hatchingalgorithmsfor volumetricdata sets. Hatching strokes for a 
surface are generated from the surface as well as from data throughout the volume enclosed by the surface. 
* [Isenberg 2003] Surveys a wide variety of silhouette extrac­tion algorithmsforpolygonal models. * 
[Kalnins2003]Describes a methodfor maintainingframe-to­frame temporal coherency for stylized silhouettes 
by propa­gatingline strokeparameterizations between frames.  [Kindlmann 2003]Transfer function-based 
rendering of con­tour and ridge and valleylinesin volumedata. Contour thick­ness is controlled using 
estimated curvatures, and ridges and valleys are emphasized by thresholdingprincipal curvatures. * [Pauly2003]Extracts 
ridge andvalleylinesfrom unstructured 3D point clouds. Performs a scale-space analysis to keep the mostimportantlines 
at multiple scales. [Phillips 2003] Demonstrates that people can consistently mark ridges and valleys 
in shaded images. Also includes dis­cussion onthepresence of ridges and valleysinlinedrawings.  [Sousa 
2003a] Details a fast but non-interactive method for creatingpreciseinkdrawingsfromhighlytessellated3D 
mod­els. A fraction of mesh edges are selected to become strokes and aredrawn with width modulated by 
surface curvature.  [Sousa 2003b] Describes a complete system for generating line drawings from 3D 
meshes. Extracts silhouettes, bound­aries, ridges, and valleys; chains lines together; .ts curves to 
paths; and renderspaths sparsely with varying line width.  [Whelan 2003] Introduces a method for augmenting 
silhou­ettes for rendering terrain, by including silhouettes computed from viewpoints differentfrom the 
actual cameraposition.  [Ashikhmin 2004] Presents a simple image-based approach torendering silhouettesfrom3D 
meshes withoutanyprepro­cessing of the mesh surface.  [Brosz2004]Introduces animprovedmethod of silhouette 
se­lection by considering the stability of silhouettes over poten­tial silhouette edges. Resulting silhouettes 
canbe stylized and drawn withframe-to-frame coherence.  * [DeCarlo2004]Discusses the motion andstability 
of sugges­tive contours on surfaces anddescribes an algorithmforinter­active suggestive contour extraction 
and rendering. [Gooch2004]An evaluation ofmethodsforfacialillustrations and caricatures that compares 
photographs and drawings. It turns out thatillustrations arelearned faster thanphotographs in recognition 
tasks.  [Grabli 2004a] Describes a Python-based programmable ar­chitecture for the creation of NPR line 
drawings. Includes capabilities for line-type selection, line chaining, and styliza­tion as well asdescriptions 
of the requisitedata structures and support algorithms.  [Grabli 2004b]Introduces a method for measuring 
density in linedrawings and a simpli.cation methodbased on suchden­sity information. Differentiates between 
density measured in a completed drawing versus density measured during the drawingprocess. Discussesindication 
as well as strokeprior­itization.  [Kalnins 2004] Discusses the previously introduced WYSI-WYG NPR system 
in more depth, with sections about stroke representation, visibility, media simulation, temporally­coherent 
line stylization, and annotated hatching.  [McGuire 2004] Describes GPU hardware methods for ex­tracting 
and rendering stylized silhouettes from 3D meshes. Creates an edge mesh with edge information stored 
atits ver­tices, allowing for the vertex shader to operate on edge infor­mation.  [Nagy 2004] Describes 
a GPU hardware method for render­ing silhouettes on volumes. Uses texturing hardware and the fragment 
shader togenerate thinimage-spacelines, which are thenbroadened and rendered.  * [Ohtake2004]Estimates 
curvature andcurvaturederivative of 3D objectsusingimplicitfunction .ts,thenextractsridgeand valleylinesusing 
theseestimates.Curvature-based .lteringis used tokeep the most signi.cant lines. [Raskar 2004]Introduces 
a camera and multi-.ash setup for acquisition of silhouettes of real-world objects.  [Schein 2004] Presents 
a method for extracting silhouettes from volume data by modeling the data with trivariate tensor productB-splinefunctions 
andextracting silhouettesfromthe implicitfunction representation.  [Wilson 2004] Introduces a method 
for rendering line draw­ingsfromcomplex3Dgeometryinacomprehensible expres­sive manner. Areas of high 
complexity are rendered with ab­stractionby using ahybrid2D/3D rendering approach.  [Xu 2004] Presents 
an algorithm for real-time rendering of silhouettesfrom unstructured 3Dpoint clouds.  [Zakaria2004]Introduces 
an algorithmforinteractive extrac­tion of stylized silhouettesfrom unstructured3Dpoint clouds.  [Zander2004]Describes 
an algorithmforhigh-qualityrender­ingofhatching on3D meshesby selectively rendering stream­linesderived 
from surface curvature.  [Barla2005]Introduces a clusteringmethod to reduce screen­spaceline complexity. 
Similarlines are clusteredtogether and replacedby a representativeline.  [Belyaev 2005] Extracts ridge 
and valley lines from noisy rangedata,includingde-noising of normals via nonlineardif­fusion and Canny-like 
nonmaximum suppression.  [Burns2005]Introduces aninteractive methodforprobabilis­tic extraction and 
rendering of contours and suggestive con­toursfrom volumedata.  [Jeong 2005] Controls line density at 
different scales by computing a progressive-mesh representation of the model, then extracting lines on 
a mesh with an appropriate view­dependent level ofdetail.  [Lum2005]Uses an examplebasedapproach to 
allowthe user to select whichlines to renderfrom alarge candidate set. The user s examples are used to 
train a machinelearning classi.er.  [Coconu2006]Presents a methodforpen-and-ink illustration of scenes 
including large numbers of trees. Trees and other objects are represented by hierarchical set of spheres 
or other abstract shapes.  2005 2006 * [Cole 2006]Presents an interactive system for placing visual 
emphasis in stylized renderings. Includes a method for tem­porally coherent line density control. Analyzes 
the effect of the emphasis using an eye-tracking experiment. [Isenberg 2006] Proposes a framework for 
storing multiple attributes, such as visibility, with a stroke throughout the ren­deringpipeline, and 
applying rendering effectsbased on these attributesjustbeforethe .naldrawing step.  [Lee 2006] Describes 
a technique for rendering in a pen­cil drawing style. Contours are drawn with error and over­sketching, 
andhatching texturesimitatepencil strokes.  [Ni 2006] Controls line density at different scales by pre­computing 
multiple levels of smoothing for the target model. Thesedifferent smoothed models are theninterpolated 
at run­time, andlines extracted from theinterpolated result.  [Breslav 2007] De.nes a way to map screen-space 
patterns onto 3D models so that the patterns move closely with the model, but remain parameterized in 
screen-space. Related to [Kalnins2003],butwith2Dpatternsinstead ofline textures.  2007 * [DeCarlo 2007] 
Introduces new object-space line de.nitions called suggestive highlights and principal highlights, which 
abstract thebright regionsin aheadlitimage much as sugges­tive contours abstract thedark regions. [Goodwin 
2007] Proposes adjusting stroke width based on density of nearby isophotes. As a result, strokes are 
wider in areas where a shaded image wouldbedark. * [Judd 2007] Describes a new object-space line de.nition 
called apparent ridges. Apparent ridges are geometric ridges when the surface normal points towards the 
camera, but be­come occluding contours when the surface normal becomes perpendicular to the camera. [Kaplan 
2007] Presents an algorithm for determining the number of polygonal layers occluding a 3D line. This 
infor­mation canbe usedfor, among otherthings,drawing occluded linesin varying styles. * [Lee2007]Presents 
animage-space approachthat createsline drawingsby .nding edgesand ridgesinashadedimage.The lines respond 
to lighting and viewing changes. Toon shading is used to augment thelinesin areas ofbroad color. [Neumann 
2007]Presents a varietyof emphasis effectsbased on adjusting camera and styleparametersinside an area 
of ef­fect. Examplesincludelocally changing line color and width, and spatiallydistorting the model. 
 [Xie2007]Introduces a methodfor extractinglinesfrom a3D model by examining lighting in object-space. 
Allows for the userto add and manipulatelightsto control the resultinglines.  [Yoshizawa2007]Presentsamethodfor 
.ndingridgeand val­ley lines on meshes using differential identities on surfaces and focal surfaces. 
 [Cole 2008a] Introduces partial visibility for stylized lines, along with improvements of the itembuffer 
technique [Northrup 2000]to reduce visibility aliasing.  [Cole2008b]Presents a studyof artists drawingsdesigned 
to support analysis of thelocal surfacefeatures under the artists linesand acomparison with currentlinedrawing 
algorithms.  [Shesh 2008] Presents a fast and temporally coherent stroke simpli.cation scheme using 
an advanced datastructure called adeformable spanner.  References AGRAWALA,M., AND STOLTE,C.2001.RenderingEffectiveRouteMaps: 
Improving Usability Through Generalization. In Proceedings of ACM SIGGRAPH2001,Computer Graphics Proceedings, 
Annual Conference Series, 241 250. APPEL,A. 1967. Thenotionofquantitativeinvisibility andthemachine rendering 
of solids. In Proceedings of the 1967 22nd national confer­ence, ACMPress,NewYork,NY,USA,387 393. ASHIKHMIN,M. 
2004. Image-spacesilhouettesforunprocessedmodels. In GI 04: Proceedings of the 2004 conference on Graphics 
interface, Canadian Human-Computer Communications Society, School of Com­puterScience,University ofWaterloo,Waterloo,Ontario,Canada,195 
202. BAREQUET, G., DUNCAN, C. A., GOODRICH, M. T., KUMAR, S., AND POP,M.1999.Ef.cientperspective-accurate 
silhouette computation.In SCG 99: Proceedings of the .fteenth annual symposium on Computa­tionalgeometry, 
ACMPress,NewYork,NY,USA,417 418. BARLA,P.,THOLLOT,J., AND SILLION,F.X. 2005. GeometricClus­tering for 
Line Drawing Simpli.cation. In Rendering Techniques 2005: 16thEurographics Workshop onRendering, 183 
192. BARROW,H., AND TENENBAUM,J. 1981. InterpretingLineDrawings as Three-Dimensional Surfaces. Arti.cial 
Intelligence 17,75 116. BELHUMEUR,P.N.,KRIEGMAN,D. J., AND YUILLE,A.L. 1999. The Bas-Relief Ambiguity. 
International Journal of Computer Vision 35, 1, 33 44. BELYAEV, A., PASKO, A., AND KUNII, T. 1998. Ridges 
and ravines on implicit surfaces. In Computer GraphicsInternational 98,530 535. BELYAEV,A., AND ANOSHKINA,E. 
2005. Detection ofSurfaceCreases in Range Data. In Eleventh IMA Conference on The Mathematics of Surfaces. 
BENICHOU,F., AND ELBER,G. 1999. OutputSensitiveExtractionof Silhouettes from Polygonal Geometry. In PG 
99: Proceedings of the 7th Paci.c Conference on Computer Graphics and Applications, IEEE Computer Society, 
Washington, DC,USA,60. BREMER,D., AND HUGHES,J. 1998. Rapid approximate silhouette ren­dering ofimplicit 
surfaces. In Implicit Surfaces 98,155 164. BRESLAV, S., SZERSZEN, K., MARKOSIAN, L., BARLA, P., AND THOL-LOT,J. 
2007. Dynamic 2Dpatterns for shading 3D scenes. ACMTrans. Graph.26,3,20. BROSZ,J.,SAMAVATI,F., AND SOUSA,M.C.2004.Silhouette 
rendering based on stability measurement. In SCCG 04: Proceedings of the 20th spring conference on Computer 
graphics, ACM Press, New York, NY, USA,157 167. BURNS, M., KLAWE, J., RUSINKIEWICZ, S., FINKELSTEIN, 
A., AND DECARLO,D. 2005. LineDrawingsfromVolumeData. In SIGGRAPH 05: Proceedings of the 32nd annual conference 
on Computer graphics andinteractive techniques,ACMPress,NewYork,NY,USA. To appear. CANNY,J. 1986. AComputationalApproachtoEdgeDetection. 
IEEE Transactions on Pattern Analysis and Machine Intelligence 8, 6, 679 698. COCONU,L.,DEUSSEN,O., AND 
HEGE,H.-C. 2006. Real-timepen­and-ink illustration of landscapes. In NPAR 06: Proceedings of the 4th 
international symposium on Non-photorealistic animation and render­ing,27 35. COLE, F., DECARLO, D., 
FINKELSTEIN,A., KIN, K., MORLEY, K., AND SANTELLA,A.2006.DirectingGazein3DModels withStylizedFocus. EurographicsSymposium 
onRendering (June), 377 387. COLE,F., AND FINKELSTEIN,A. 2008. PartialVisibilityforStylized Lines. In 
NPAR2008. COLE, F., GOLOVINSKIY, A., LIMPAECHER, A., BARROS, H. S., FINKELSTEIN,A., FUNKHOUSER,T., AND 
RUSINKIEWICZ,S. 2008. Where Do People Draw Lines? ACMTransactions onGraphics(Proc. SIGGRAPH)27,3(Aug.). 
CS´ ¨¨ EBFALVI, B., MROZ, L., HAUSER, H., K ONIG, A., AND GROLLER, E. 2001. Fast Visualization of Object 
Contours by Non-Photorealistic Volume Rendering. In Eurographics 01. DECARLO,D., AND SANTELLA,A. 2002. 
Stylization andAbstraction of Photographs. ACMTransactions onGraphics21,3(July),769 776. DECARLO, D., 
FINKELSTEIN, A., RUSINKIEWICZ, S., AND SANTELLA, A. 2003. Suggestive contours for conveying shape. ACMTrans.Graph. 
22,3,848 855. DECARLO,D.,FINKELSTEIN,A., AND RUSINKIEWICZ,S. 2004. In­teractive rendering of suggestive 
contours with temporal coherence. In NPAR 04: Proceedings of the 3rd international symposium on Non­photorealistic 
animation and rendering, ACM Press, New York, NY, USA,15 145. DECARLO, D., AND RUSINKIEWICZ,S. 2007. 
Highlightlinesfor convey­ingshape. In NPAR 07:Proceedingsofthe5thinternational symposium onNon-photorealistic 
animation and rendering, 63 70. DEUSSEN,O., AND STROTHOTTE,T. 2000. Computer-generatedpen-and­inkillustration 
of trees. In SIGGRAPH 00:Proceedingsof the27th an­nual conference onComputergraphics andinteractive techniques, 
ACM Press/Addison-Wesley Publishing Co.,New York,NY,USA,13 18. DONG,F.,CLAPWORTHY,G.J.,LIN,H., AND KROKOS,M.A. 
2003. Nonphotorealistic Rendering of Medical Volume Data. IEEE Comput. Graph.Appl.23,4,44 52. DOOLEY,D., 
AND COHEN,M.F.1990.Automaticillustration of3Dgeo­metric models: lines. In SI3D 90: Proceedings of the 
1990 symposium onInteractive 3Dgraphics, ACMPress,New York,NY,USA,77 82. DURAND, F., OSTROMOUKHOV, V., 
MILLER, M., DURANLEAU, F., AND DORSEY,J. 2001. DecouplingStrokesandHigh-LevelAttributesfor Interactive 
Traditional Drawing. In Rendering Techniques 2001: 12th EurographicsWorkshop onRendering, 71 82. EBERT, 
D., AND RHEINGANS, P. 2000. Volume illustration: non­photorealistic rendering of volume models. In VIS 
00: Proceedings of the conference on Visualization 00, IEEE Computer Society Press, LosAlamitos, CA,USA,195 
202. ELBER, G., AND COHEN,E.1990.Hidden curve removalforfreeform sur­faces. In SIGGRAPH 90:Proceedingsof 
the17th annual conferenceon Computer graphics and interactive techniques, ACM Press, New York, NY,USA,95 
104. ELBER,G. 1995. LineArtRenderingviaaCoverageofIsoparametric Curves. IEEETransactions onVisualization 
andComputerGraphics1, 3,231 239. ELBER,G. 1995. Lineillustrationsincomputergraphics. The Visual Computer 
11,6,290 296. ELBER,G. 1998. LineArtIllustrations ofParametric andImplicitForms. IEEETransactions onVisualization 
andComputerGraphics4,1,71 81. ELBER,G. 1999. InteractiveLineArtRendering ofFreeformSurfaces. Computer 
Graphics Forum18,3,1 1. GIRSHICK, A., INTERRANTE, V., HAKER, S., AND LEMOINE, T. 2000. Line direction 
matters: an argument for the use of principal directions in 3D line drawings. In NPAR 00: Proceedings 
of the 1st international symposiumonNon-photorealistic animation and rendering,ACMPress, NewYork,NY,USA,43 
52. GOOCH,A.,GOOCH,B.,SHIRLEY,P., AND COHEN,E. 1998. Anon­photorealistic lighting modelfor automatic 
technical illustration. In SIG-GRAPH 98: Proceedings of the 25th annual conference on Computer graphics 
and interactive techniques, ACM Press, New York, NY, USA, 447 452. GOOCH, B., SLOAN, P.-P. J., GOOCH, 
A., SHIRLEY, P., AND RIESEN-FELD,R. 1999. Interactive technical illustration. In SI3D 99: Proceed­ings 
of the 1999 symposium on Interactive 3D graphics, ACM Press, NewYork,NY,USA,31 38. GOOCH,B., AND GOOCH,A. 
2001. Non-Photorealistic Rendering.AK Peters. GOOCH,B.,REINHARD,E., AND GOOCH,A. 2004. Humanfacialillus­trations: 
Creation and psychophysical evaluation. ACM Trans. Graph. 23,1,27 44. GOODWIN,T., VOLLICK,I., AND HERTZMANN,A. 
2007. Isophotedis­tance: a shading approach to artistic stroke thickness. In NPAR 07: Proceedings of 
the 5th international symposium on Non-photorealistic animation and rendering, 53 62. GRABLI, S., TURQUIN, 
E., DURAND, F., AND SILLION, F. 2004. Pro­grammable Style for NPR Line Drawing. In Eurographics Symposium 
onRendering 04. GRABLI, S., DURAND, F., AND SILLION, F. 2004. Density Measure for Line-Drawing Simpli.cation. 
In Proceedings ofPaci.cGraphics. HAMEL, J., SCHLECHTWEG, S., AND STROTHOTTE, T. 1998. An Ap­proach to 
Visualizing Transparency in Computer-Generated Line Draw­ings. In IV 98: Proceedings of the International 
Conference on Infor­mation Visualisation, IEEE Computer Society, Washington, DC, USA, 151. HERTZMANN,A., 
AND ZORIN,D. 2000. Illustrating smooth surfaces. In SIGGRAPH 00: Proceedings of the 27th annual conference 
on Com­puter graphics and interactive techniques, ACM Press/Addison-Wesley Publishing Co.,NewYork,NY,USA,517 
526. HERTZMANN,A. 2001. Paintby relaxation. In Computer Graphics Inter­national 2001,47 54. HSU,S.C., 
AND LEE,I.H.H.1994.Drawing and animation using skele­tal strokes. In SIGGRAPH 94: Proceedings of the 
21st annual con­ference on Computer graphics and interactive techniques, ACM Press, NewYork,NY,USA,109 
118. INTERRANTE,V.,FUCHS,H., AND PIZER,S. 1995. EnhancingTrans­parent Skin Surfaces with Ridge and Valley 
Lines. In VIS 95: Proceed­ingsof the6th conferenceonVisualization 95,IEEEComputerSociety, Washington, 
DC,USA,52. INTERRANTE, V., FUCHS, H., AND PIZER,S. 1996. Illustrating transpar­ent surfaces with curvature-directed 
strokes. In VIS 96: Proceedings of the 7th conference on Visualization 96, IEEE Computer Society Press, 
LosAlamitos, CA,USA,211 ff. ISENBERG, T., HALPER, N., AND STROTHOTTE, T. 2002. Stylizing Silhouettes 
at Interactive Rates: From Silhouette Edges to Silhouette Strokes. Computer Graphics Forum21,3,249 249. 
ISENBERG,T.,FREUDENBERG,B., HALPER,N., SCHLECHTWEG,S., AND STROTHOTTE,T. 2003. ADeveloper sGuidetoSilhouetteAlgo­rithmsforPolygonal 
Models. IEEEComput.Graph.Appl.23,4,28 37. ISENBERG,T., AND BRENNECKE,A. 2006. G-strokes:Aconceptfor simplifying 
line stylization. In Computers &#38; Graphics, vol. 30, 754 766. ITTI,L., AND KOCH,C. 2000. ASaliency-based 
searchmechanismfor overt and covert shifts of visual attention. Vision Research 40, 1489 1506. JEONG, 
K., NI, A., LEE, S., AND MARKOSIAN,L. 2005. Detail controlin linedrawings of3D meshes.In TheVisualComputer, 
vol.21,698 706. JUDD, T., DURAND, F., AND ADELSON,E. 2007. Apparent ridgesforline drawing. ACMTrans.Graph.26,3,19. 
KALNINS,R. D., MARKOSIAN, L., MEIER,B. J., KOWALSKI,M. A., LEE, J. C., DAVIDSON, P. L., WEBB, M., HUGHES, 
J. F., AND FINKELSTEIN,A. 2002. WYSIWYGNPR:drawing strokesdirectly on3D models. In SIGGRAPH 02: Proceedings 
of the 29th annual con­ference on Computer graphics and interactive techniques, ACM Press, NewYork,NY,USA,755 
762. KALNINS,R. D., DAVIDSON,P. L., MARKOSIAN, L., AND FINKEL-STEIN,A. 2003. Coherent stylized silhouettes. 
ACM Trans. Graph. 22,3,856 861. KALNINS,R. D. 2004. WYSIWYGNPR:Interactive Stylization forStroke-BasedRendering 
of3DAnimation. PhD thesis,Princeton University. KAPLAN,M.2007. Hybridquantitative invisibility. In NPAR 
07: Proceed­ingsofthe5thinternational symposiumonNon-photorealistic animation and rendering, 51 52. KINDLMANN, 
G., WHITAKER, R., TASDIZEN, T., AND MOLLER, T. 2003. Curvature-based transfer functions for direct volume 
rendering: methods and applications. In IEEEVisualization 2003,513 520. KNILL,D. C. 1992. Perception 
of surface contours and surface shape: from computation to psychophysics. Journal of the Optical Society 
of AmericaA9,9,1449 1464. KNILL,D.C. 2001. Contourintotexture: information content of surface contours 
and texture .ow. Journal of the Optical Society of America A 18,1,12 35. KOENDERINK, J. J., AND VAN DOORN,A. 
J. 1982. TheShape ofSmooth Objects and theWayContoursEnd. Perception 11,129 137. KOENDERINK,J.J. 1984. 
Whatdoesthe occluding contourtell us about solid shape? Perception 13,321 330. KOENDERINK,J. J. 1990. 
Solid Shape. MITpress. KOENDERINK, J. J., VAN DOORN, A., CHRISTOU, C., AND LAPPIN,J. 1996.Shape constancy 
inpictorial relief. Perception 25,155 164. KOENDERINK,J.J., ANDVAN DOORN,A.1998.TheStructure ofRelief. 
AdvancesinImaging andElectronPhysics103,65 150. KOENDERINK, J. J., VAN DOORN, A. J., KAPPERS, A. M., 
AND TODD, J.T.2001.Ambiguity andthe mental eye inpictorial relief. Perception 30,431 448. LAKE, A., MARSHALL,C., 
HARRIS,M., AND BLACKSTEIN,M. 2000. Stylized rendering techniques for scalable real-time 3D animation. 
In NPAR 00: Proceedings of the 1st international symposium on Non­photorealistic animation and rendering, 
13 20. LEE,H.,KWON,S., AND LEE,S. 2006. Real-timepencilrendering. In NPAR 06: Proceedings of the 4th 
international symposium on Non­photorealistic animation and rendering, 37 45. LEE,Y.,MARKOSIAN,L.,LEE,S., 
AND HUGHES,J. F. 2007. Line drawings via abstracted shading. ACMTrans.Graph.26,3,18. LU, A., MORRIS,C. 
J., EBERT, D. S., RHEINGANS,P., AND HANSEN,C. 2002. Non-photorealistic volume rendering using stippling 
techniques. In VIS 02: Proceedings of the conference on Visualization 02, IEEE Computer Society, Washington, 
DC,USA,211 218. LUM,E.B., AND MA,K.-L. 2002. Hardware-acceleratedparallel non­photorealistic volume rendering. 
In NPAR 02: Proceedings of the 2nd international symposium on Non-photorealistic animation and render­ing,ACMPress,NewYork,NY,USA,67 
ff. LUM,E.B., AND MA,K.-L.2005. Expressiveline selectionby example. TheVisual Computer 21,8-10,811 820. 
MA, K.-L., AND INTERRANTE, V. 1997. Extracting feature lines from 3D unstructured grids. In VIS 97: Proceedings 
of the 8th conference on Visualization 97, IEEEComputer Society Press, Los Alamitos, CA, USA,285 ff. 
MALIK,J. 1987. InterpretingLineDrawings ofCurvedObjects. Interna­tionalJournal ofComputerVision1,1,73 
103. MARKOSIAN,L.,KOWALSKI,M. A.,GOLDSTEIN,D., TRYCHIN,S.J., HUGHES, J. F., AND BOURDEV, L. D. 1997. 
Real-time nonphoto­realistic rendering. In SIGGRAPH 97: Proceedings of the 24th an­nual conference onComputergraphics 
andinteractive techniques, ACM Press/Addison-Wesley Publishing Co.,New York,NY,USA,415 420. MARTIN, D., 
FEKETE, J., AND TORRES,J.C. 2002.Flattening3D objects using silhouettes. Computer Graphics Forum21,3,239 
239. MASUCH,M.,SCHLECHTWEG,S., AND SCHNWLDER,B. 1997. dali! Drawing Animated Lines! In Simulation andAnimation97,87 
96. MASUCH, M., SCHUHMANN,L., AND SCHLECHTWEG,S. 1998. Animat­ing Frame-To-Frame-Coherent Line Drawings 
for Illustrated Purposes. In Simulation and Animation 98,101 112. MASUCH, M., AND STROTHOTTE, T. 1998. 
Visualising Ancient Archi­tecture using Animating Line Drawings. In IV 98: Proceedings of the International 
Conference on Information Visualisation, IEEEComputer Society, Washington, DC,USA,261. MCGUIRE,M., AND 
HUGHES,J.F. 2004. Hardware-determinedfeature edges. In NPAR 04: Proceedings of the 3rd international 
symposium onNon-photorealistic animation and rendering,ACMPress,New York, NY,USA,35 147. NAGY,Z., AND 
KLEIN,R. 2004. High-QualitySilhouetteIllustrationfor Texture-Based Volume Rendering. In WSCG 04. NEUMANN,P.,ISENBERG,T., 
AND CARPENDALE,M.S.T. 2007. NPR Lenses: Interactive Tools for Non-Photorealistic Line Drawings. In SmartGraphics, 
10 22. NI, A., JEONG, K., LEE, S., LEE, S., AND MARKOSIAN,L. 2006. Multi­scale line drawings from 3D 
meshes. In I3D 06: Proceedings of the 2006 symposium onInteractive3Dgraphics andgames,133 137. NORTHRUP,J. 
D., AND MARKOSIAN,L. 2000. Artisticsilhouettes: a hybrid approach. In NPAR 00: Proceedings of the 1st 
international symposiumonNon-photorealistic animationand rendering,ACMPress, NewYork,NY,USA,31 37. OHTAKE, 
Y., BELYAEV, A., AND SEIDEL,H.-P. 2004. Ridge-valley lines on meshes via implicit surface .tting. ACMTrans.Graph.23,3. 
PAGE,D.L.,KOSCHAN,A.,SUN,Y.,PAIK,J., AND ABIDI,A. 2001. RobustCreaseDetection andCurvatureEstimationofPiecewiseSmooth 
Surfaces from Triangle Mesh Approximations Using Normal Voting. In Proc.Conference onComputerVisionandPatternRecognition. 
PAULY, M., KEISER, R., AND GROSS, M. 2003. Multi-Scale Feature Extraction onPoint-SampledModels. In Proc.Eurographics. 
PEARSON,D., AND ROBINSON,J. 1985. VisualCommunication atVery LowData Rates. Proc.IEEE4 (Apr.),795 812. 
PHILLIPS,F., TODD, J. T., KOENDERINK, J. J., AND KAPPERS,A.M. 2003. Perceptual representation of visible 
surfaces. Perception andPsy­chophysics 65,5,747 762. PRAUN,E.,HOPPE,H.,WEBB,M., AND FINKELSTEIN,A. 2001. 
Real­time hatching. In SIGGRAPH 01: Proceedings of the 28th annual con­ference on Computer graphics and 
interactive techniques, ACM Press, NewYork,NY,USA,581. RASKAR,R., AND COHEN,M.1999.Imageprecision silhouette 
edges.In SI3D 99:Proceedingsof the1999 symposiumonInteractive3Dgraph-ics,ACMPress,New York,NY,USA,135 
140. RASKAR,R. 2001. HardwareSupportforNon-photorealisticRendering. In Proc.Graphics Hardware. RASKAR,R., 
HAN TAN,K.,FERIS,R.,YU,J., AND TURK,M. 2004. Non-photorealisticCamera:DepthEdgeDetection andStylizedRender­ing 
using Multi-Flash Imaging. ACMTrans.Graph.23,3. RIEGER,J.H. 1997. TopographicalProperties ofGenericImages. 
Inter­nationalJournal ofComputerVision23,1,79 92. ROSSIGNAC,J., ANDVAN EMMERIK,M. 1992. HiddenContours 
ona Framebuffer. Proc. of7th Workshop onComputer GraphicsHardware. ROSSL,C., AND KOBBELT,L.2000. Line-art 
rendering of3D-models. In Paci.cGraphics 00,87 96. SAITO, T., AND TAKAHASHI, T. 1990. Comprehensible 
rendering of 3-D shapes. In SIGGRAPH 90: Proceedings of the 17th annual confer­ence onComputergraphics 
andinteractive techniques,ACMPress,New York,NY,USA,197 206. SAITO, T. 1994. Real-time previewing for 
volume visualization. In VVS 94:Proceedings of the1994 symposium onVolumevisualization, ACM Press,NewYork,NY,USA,99 
106. SALISBURY, M. P., ANDERSON, S. E., BARZEL, R., AND SALESIN, D.H. 1994. Interactivepen-and-inkillustration. 
In SIGGRAPH 94: Proceedings of the 21st annual conference on Computer graphics and interactive techniques, 
ACMPress,New York,NY,USA,101 108. SALISBURY, M., ANDERSON, C., LISCHINSKI,D., AND SALESIN,D. H. 1996.Scale-dependent 
reproductionofpen-and-inkillustrations.InSIG-GRAPH 96: Proceedings of the 23rd annual conference on Computer 
graphics and interactive techniques, ACM Press, New York, NY, USA, 461 468. SANDER,P. V., GU, X., GORTLER, 
S. J., HOPPE,H., AND SNYDER,J. 2000. Silhouette clipping. In SIGGRAPH 00: Proceedings of the 27th annual 
conference on Computer graphics and interactive techniques, ACMPress/Addison-Wesley PublishingCo.,NewYork,NY,USA,327 
334. SANTELLA,A., AND DECARLO,D. 2004. Visualinterest andNPR:an evaluation and manifesto. In NPAR2004,71 
78. SCHEIN,S., AND ELBER,G. 2004. Adaptive extraction and visualization of silhouette curves from volumetric 
datasets. Vis. Comput. 20, 4, 243 252. SHESH,A., AND CHEN,B.2008.Ef.cient andDynamicSimpli.cation of 
Line Drawings. Computer Graphics Forum(Proc. Eurographics 2008) 27,2,537 545. SOUSA,M. C.,FOSTER,K.,WYVILL,B., 
AND SAMAVATI,F. 2003. Precise Ink Drawing of 3D Models. Computer Graphics Forum 22, 3, 369 369. SOUSA,M. 
C., AND PRUSINKIEWICZ,P. 2003. AFewGoodLines: Suggestive Drawing of 3D Models. Computer Graphics Forum 
22, 3, 381 381. STEVENS,K. A. 1981. TheVisualInterpretation ofSurfaceContours. Arti.cial Intelligence 
17,47 73. STROTHOTTE, T., AND SCHLECHTWEG, S. 2002. Non-Photorealistic Computer Graphics. MorganKaufmann. 
THIRION, J.-P., AND GOURDON,A. 1996. The3DMarching LinesAlgo­rithm. GraphicalModels andImageProcessing 
58,6(Nov.),503 509. TREAVETT,S. M. F., AND CHEN,M. 2000. Pen-and-Inkrenderingin volume visualisation. 
In VIS 00: Proceedings of the conference on Vi­sualization 00,IEEEComputerSocietyPress,LosAlamitos,CA,USA, 
203 210. WALTZ,D.L. 1975. UnderstandingLineDrawings ofScenes withShad­ows. In The Psychology of Computer 
Vision, P. Winston, Ed. McGraw-Hill, 19 92. WATANABE,K., AND BELYAEV,A.G. 2001. Detection ofSalientCurva­ture 
Features on Polygonal Surfaces. Computer Graphics Forum(Proc. Eurographics 2001) 20,3. WEBB, M., PRAUN, 
E.,FINKELSTEIN,A., AND HOPPE,H. 2002. Fine tone control inhardware hatching. In NPAR 02:Proceedingsof 
the2nd international symposium on Non-photorealistic animation and render­ing,53 ff. WHELAN,J., AND VISVALINGAM,M. 
2003. Formulated silhouettesfor sketching terrain. Theory and Practice of Computer Graphics, 2003. Proceedings, 
90 96. WILSON,B., AND MA,K.-L. 2004. Rendering complexityin computer­generated pen-and-ink illustrations. 
In NPAR 04: Proceedings of the 3rd international symposium on Non-photorealistic animation and ren­dering, 
ACMPress,New York,NY,USA,129 137. WINKENBACH, G., AND SALESIN,D. H. 1994. Computer-generated pen­and-ink 
illustration. In SIGGRAPH 94: Proceedings of the 21st an­nual conference onComputergraphics andinteractive 
techniques, ACM Press,New York,NY,USA,91 100. WINKENBACH,G., AND SALESIN,D. H. 1996. Renderingparametric 
surfacesinpen andink. In SIGGRAPH 96:Proceedingsof the23rd an­nual conference onComputergraphics andinteractive 
techniques, ACM Press,New York,NY,USA,469 476. XIE,X.,HE,Y.,TIAN,F.,SEAH,H.-S.,GU,X., AND QIN,H. 2007. 
An Effective Illustrative Visualization Framework Based on Photic Ex­tremum Lines (PELs). IEEE Transactions 
on Visualization and Com­puter Graphics 13,6(Nov./Dec.),1328 1335. XU,H.,NGUEN,M.,YUAN,X., AND CHEN,B. 
2004. InteractiveSil­houette Rendering forPoint-Based Models. In Proc.Eurographics Sym­posium onPoint-BasedGraphics. 
YOSHIZAWA, S., BELYAEV, A., YOKOTA, H., AND SEIDEL, H.-P. 2007. Fast and Faithful Geometric Algorithm 
for Detecting Crest Lines on Meshes. In Paci.c Graphics. ZAKARIA,N., AND SEIDEL,H.-P.2004.Interactive 
stylized silhouettefor point-sampled geometry. InGRAPHITE 04:Proceedingsof the2ndin­ternational conference 
onComputergraphicsandinteractivetechniques in Australasia and Southe East Asia, ACM Press, New York, 
NY, USA, 242 249. ZANDER, J., ISENBERG, T., SCHLECHTWEG, S., AND STROTHOTTE, T. 2004. High Quality Hatching. 
Computer Graphics Forum 23, 3, 421 421.  We taught a similar course at Siggraph 2005, so it s natural 
for those of you who attended it to wonder what s new this time? Indeed, there is substantial overlap 
in this offering. However there have also been a number of advances in the field since then and we ll 
try to highlight this new material. In particular, there have been several new algorithms proposed recently 
for drawing lines from smooth 3D shapes. In addition, there have been some studies on how people make 
line drawings. Finally, there is the usual version 2.0 overhaul. When it comes to making pictures with 
computer graphics there are a couple alternatives. Most traditional work in computer graphics has been 
striving towards realistic imagery. One reason is that it provides a way to accurately simulate our experiences 
in the real world. There is a terrible cost to this, unfortunately, which is that you have to get all 
the details just right, which imposes a great burden on the modeler. Fortunately there is an alternative. 
There is a branch of computer graphics that has been looking for more than a dozen years at how can we 
exploit principles of abstraction known to artists for many centuries to avoid having to worry about 
a myriad of perhaps unnecessary details. Here is a rendering of a realistic outdoor scene due to Deussen 
and others. It highlights the tradeoff that I was just mentioning. The model is extremely complex --the 
number of polygons would not fit in the memory of most computers today, let alone those of 1999 when 
it was created. Here is a different rendering of an outdoor scene by Deussen and others. Obviously, 
there is a lot less complexity there, and yet depending on the application it may be just as appropriate 
for conveying a particular sense of a place. In the 1990s, NPR emerged as an alternative to photorealism. 
Research efforts largely focused on ways to simulate various media such as technical illustration or 
pen-and-ink. The goal was to make an image that was believably created by hand in one of these traditional 
media. These systems were mostly-automatic: the program attempted to mimic what an artist/illustrator 
might do. As in the traditional media, line drawing played a key role. These methods were largely offline, 
meaning you wait and after a while you get an image. In the late 90 s Markosian showed that stylized 
line drawings could be generated in real time, responding to changes in camera. We can also use non-photorealism 
to guide the viewer s eye towards important features in a scene and away from unimportant fixtures. So, 
for example here, by using lines that have a crazy temporal quality, Curtis gave the standing figure 
a very energetic quality that draws your eye. Northrup and Markosian continued to work on temporal coherence 
and line stylization, producing imagery like this in real time by the beginning of this decade.  A single 
illustration can incorporate many different technical elements that combine to form a cohesive image. 
So there are many different tools that have emerged from the NPR community to be able to make such illustrations 
with computers. For example, we have cartoon shading, stylized strokes, effects for interaction of media 
with paper, the ability to draw specific marks onto surfaces, hatching, and automatic outlining tools. 
 In this course we focus on line drawings, in part because they tend to be the simplest form of rendering 
from 3D models and are often used as a component of more sophisticated rendering schemes. And within 
line drawings, most of our emphasis will be on outlines such as those shown here. There are three kinds 
of lines that contribute to the outlines. First are the silhouettes (equivalently called contours by 
some researchers) They separate front-facing from back-facing regions of the surface, as a function of 
view point Creases are path which are defined statically on the mesh surface generally representing 
sharp features, such as the those present on this mechanic part on the left. Other features behave similarly, 
such as marks drawn directly onto the surface denoting for example the boundary between two different 
surface textures. Third, in in 2003 we introduced suggestive contours which are dynamic features similar 
to silhouettes that are view dependent and help denote the shape. It turns out that suggestive contours 
may be thought of as places on the shape that would be silhouettes from nearby views. Since then there 
have been a number of related line definitions proposed, such as suggestive highlights or apparent ridges, 
and we will discuss these shortly. These three elements, though not exhaustive, can produce a wide range 
of line drawings, and often contribute to more complex illustrations. For the purposes of this course 
we will be ignoring more sophisticated line drawing methods used by artists (and that have been explored 
by the NPR community). For example, hatching lines can suggest material and tone and are often used in 
conjuntion with basic line drawings. I should also mention stippling, which can use a collection of 
small dots to convey tone. This is an important drawing technique and may be used in conjunction with 
lines. In contrast with hatching, these marks do not follow the curvatures of the shape and generally 
do not have the expressiveness to suggest surface texture. This course will dwell largely on these basic 
line drawing elements, because they give us plenty to talk about.  This is an example of a fairly state 
of the art computer generated line drawing. It uses suggestive contours, slightly stylized strokes, and 
some visual emphasis effects, and its a pretty nice, effective drawing. However, its pretty obvious 
that the work of human artists, at least the best of them, is still in a different class. Somehow this 
drawing manages to say a lot more than the CG version, even though the amount of shading and lines on 
the page is roughly equivalent. Of course, this comparison is unfair the cg model contains much less 
information than a view of a real lion. Additionally, there is no shame in not being able to draw as 
well as rembrandt. There are maybe a few people in history who could. Still, we'd like to learn what 
the best human artists are doing differently from our current algorithms, and hopefully simulate it. 
And to simulate it, we need to formalize it. This course is concerned with line drawings. "Line drawing" 
is often used as a shorthand for a particular kind of simple, sparse, monochrome drawing, but from the 
point of view of a computer algorithm, human line drawings include a huge set of variations that need 
to be examined. Here is a drawing that would likely be quite difficult to recreate using current computer 
algorithms. It's a sketch for a later painting, but Michelangelo signed it, so he must have thought that 
it was reasonably complete. Its hard even for a human to see what exactly the drawing is depicting something 
about men struggling, maybe in battle. The point of the drawing is in the way it shows the motion, the 
emotional content. By contrast, here is a drawing that is entirely explanatory. It exists to show the 
parts exactly, and part of the purpose of the drawing is that the viewer can see completely unambiguously 
which parts are depicted. The contrast between the two drawings is almost painful; you can feel your 
brain changing gears when you compare this drawing and the last. Beyond the basic purpose of a drawing, 
a major distinction is in how tightly controlled and precise the drawing is. This isn't just the difference 
between drawing quickly and taking your time, though the drawing on the left must have been much quicker 
than the one on the right. A precise, controlled style is best for unambiguously showing detail (you 
can count the number of people in the courtyard in the drawing on the right). However, a loose style 
also conveys information; it says that there is more there than you are showing right now, the artistic 
equivalent of waving hands and saying "et cetera." All line drawings ask the viewer to use their imagination, 
and a skilled artist uses this as a great point of strength. The artist can choose, however, how far 
to guide the viewer's imagination. The artist can leave almost everything unsaid, as in the left drawing 
(where only the basic outline of a classical interior can be made out), or detail almost every inch of 
the paper, as in the right (where you can look for a long time and still not catch all the tiny human 
figures). This is a different type of abstraction from using a loose or sparse style it is a careful 
depiction of an abstraction of the subject. An extreme example would be a cartoon character. Computer 
algorithms have so far dealt with this type of abstraction only in very specialized cases. There are 
a number of techniques used by human artists that are fairly well understood, such as simple abstraction, 
hatching, and shading. These techniques are described in detail by books on art instruction, and these 
descriptions can be translated more or less directly into algorithms. For example, this paper by Deussen 
et al. simulates quite closely the changes in style laid out by Guptill. The plants are made abstract 
by rounding the shapes, making the lines more sparse, and making the overall tone lighter. The plants 
are made more detailed by doing the opposite: adding detail leaf shapes and increasing the overall density 
of lines. Hatching is a technique for creating shading and suggesting shape using strokes of roughly 
similar width and length. Rules for effective shading are well laid out in the art literature. Four more 
or less constant levels of tone are commonly used: highlights, or essentially blank spaces, such as on 
the hand or the folds in cloth; single hatching, as on the body of the jacket; cross-hatching, as in 
the shadows of the folds; and undercuts, essentially blacked out areas in deep shadow. An important point 
is that hatching lines are usually rather straight, though tending to bend somewhat with the underlying 
surface. Hatching lines that follow the surface too closely for too long tend to appear as markings on 
the surface itself. Hertzmann's 2000 paper does a nice job of working these observations into an effective 
algorithm, with an example result on the right. Shading style varies widely, but it is common for artists 
to exaggerate shading and use false light to bring out the shape of their subject. In the case of terrain 
relief, rules for shading are codifed well enough that an artist's shading (left) can be reproduced quite 
closely (right). This algorithm uses a multi-resolution approach to locally manipulate the light direction 
much as an artist might, giving small details extra prominence and the curving sides of valleys roughly 
similar levels of shade. Unfortunately, not all the interesting effects are so well explained in the 
art literature. Even the simplest of line drawings by human artists contain effects that are difficult 
to effectively describe with our current knowledge. For example, consider making a simple, pure line 
drawing of this shape, with the object of convey the shape as well as possible. Ignore abstraction, hatching, 
or shading entirely. This is one answer to that prompt from an actual artist. Note that the drawing 
has been scanned and filtered so that each line is constant thickness and strength. This artist made 
some fairly obvious decisions, such as drawing all the silhouettes, but also some idiosyncratic decisions, 
such as extending the crease line at the front of the screwdriver far along the shaft. Other artists 
made similar decisions in many cases, but each drawing is unique. One shared property between all the 
drawings is the way the rear of the screwdriver is represented by three independent components, though 
some artists chose to complete each loop and some chose to leave the rest of the loop implied. Each drawing 
has its strengths and weaknesses, but they all get the important bits pretty much right. Lets look at 
how we might try to formalize these lines. There are several line drawing algorithms that can provide 
models for artists' lines. These algorithms will be described in detail later on in the course, but for 
now I'll just explain briefly. We know that edges in an image are perceptually important. Extracting 
edges from an image is also simple, in fact, it is probably the simplest line drawing algorithm we know. 
Variations on the theme include Canny edge detection and more sophisticated methods such as Kang 2007. 
 Besides image edges, we can look at ridges and valleys of the image. There is some reason to believe 
that artists sometime abstract shading with lines, which would correspond to making lines along the image 
intensity ridges and valleys. Although these lines are most obviously image space features, later on 
we will see how to describe some special cases in object space. Creases (lines of normal discontinuity) 
are obviously important lines to draw, as shown by the left image. Geometric ridges and valleys are essentially 
smoothed creases, i.e., lines where the normal is changing rapidly in one direction. These lines are 
intrinsic properties of the shape, and are therefore not dependent on lighting. Some examples of artists 
renderings using ridges are shown at right. The most prominent ridge-like features in both are along 
the nose. Occluding contours are known to be important lines to draw. Artists also seem to extend these 
contour lines, and draw lines where no true contour exists, but a contour would exist if the viewpoint 
were slightly changed. These lines are also not dependent on lighting note that the lines on the lower 
right of the golf ball (closer to the shade) are the same as the lines on the upper left (closer to the 
light). Later in the course you'll see how to formalize these lines to produce images such as the one 
on the right. We are left with the question of which model of lines to use. All the models appear to 
work in some cases, but it is not clear when and where to use each one. There are greater problems with 
the modeling approach, however. Even if we come up with a model that produces very similar lines to a 
given drawing, we have no way to evaluate how well the model truly matches. The shape that the artist 
considered when making the original drawing is unknown, and in fact, we often beg the question by inferring 
the shape from the drawing itself. Further, even when we do know the exact shape (such as when we have 
the 3D model), we cannot make a direct comparison back to the original drawing, since the resulting drawings 
are completely different. We need a way to directly compare an artist's drawing with our models. Recently 
a few studies have attempted to capture drawings of known shapes. The artists are asked to draw under 
controlled conditions, and their results captured. There are at least three options for drawing prompts: 
a physical 3D model, and animated rendering, and a set of still images. Phillips 2005 uses all three, 
while one of our current papers does only the last. Of course, for the drawings to be useful they must 
be registered somehow to the 3D model. Phillips allows the artists to draw freely, and then uses a matching 
algorithm to register the drawings to the model. While this method works for any prompt and provides 
the artist with maximum freedom, it does not offer good accuracy and the matching problem is hard and 
ambiguous. Another option is to ask the artists to register the drawings themselves. The artists first 
make a freehand drawing, as in the top of the page, and then copy their drawing on to a faint shaded 
version of the prompt. This method gives good accuracy and works around the matching problem by having 
the artists solve it for us. Here are some example results from our recent study. At the top you see 
the prompt image, and at bottom the average of many different artists drawings of that prompt. The artists 
seem generally guided by image space shading features, especially edges, which confirms the intuition 
that artists "draw what they see." Occluding contours are obviously very important, but features such 
as the circular ridge of the rockerarm are also highly agreed upon by different artists. Image space 
features are not the whole story, however. The average drawing for this shape seems to be best described 
by the geometric ridges and valleys, because the circular lines remain relatively constant through changes 
in the image intensity. However, while all the artists' lines can be classified as ridges or valleys, 
just selecting ridges and valleys by strength does not give us a matching to the artists' drawings. The 
artists made some complex editing decisions to leave out the lines along the top of the mounting holes, 
while leave in the lines around the whole circumference of the shape. As you can see from the two lower 
images, using a high enough threshold to exclude the short valley lines excludes the long ones as well, 
while using a low enough threshold to include the long lines includes the short lines as well. This 
brings us back to the screwdriver example. While not mentioned before, the screwdriver drawings were 
made by artists in our study. We therefore have the 3D model and view, and can create CG line drawings 
for a direct comparison. At top are three of the humans' drawings, and below are three CG drawings, roughly 
selected to match the humans' drawings. In the first pair, the shaft line is extended all the way to 
the hilt, and the loop features at the rear are completed. Note, however, that in the ridge and valley 
image, lines between loops are shared, unlike in the artist's drawing. In the second pair, the dimple 
in the top of the screwdriver is recorded, and the lower section of a loop in left implied as in the 
artist's drawing. However, the shaft line is missing, as is the back of the top loop. In the third pair, 
the crooked line in the middle of the side loop is present, along with the shaft line and some of the 
messy lines at the front of the screwdriver. However, the top loop is almost omitted in the image edges 
drawing, while it is completely present in the artist's drawing. In conclusion, human artists are still 
ahead of CG algorithms in many respects. However, CG is close behind in the case where the artistic technique 
is well explained and can be easily formalized. Since no well known formalizations exist for much of 
artistic technique, however, future study of human line drawings may require experiments with artists 
in order to make progress.  In this section of the class, we will look at mathematical definitions of 
linear features on surfaces. This is an important component in our study of line drawings, since it serves 
to formalize the intuitions we get from looking at art. Moreover, the mathematical definitions can be 
turned into algorithms for producing candidate line locations on surfaces or images; this serves as the 
backbone of NPR line­drawing systems. Finally, as we will see later, we can ask questions about what 
shaped is perceived, given each different line definition. Here s a hand-drawn illustration by John 
Flaxman that illustrated a 19th century translation of the Odyssey. Notice how there are a variety of 
lines illustrating various effects, including things like shading, but in particular there are many lines 
that convey shape. When a viewer looks at these lines, these lines are naturally interpreted as indicating 
shape: they are not perceived as lines drawn on the surface! In studying these shape-conveying lines, 
people have proposed three categories of mathematical definitions. First, there are image­space lines 
corresponding to features extracted from an image. In the case of lines from 3D models, the images are 
usually renderings with Lambertian reflection, in most cases using a headlight as the only light source. 
Next, there are object-space features computed directly on the 3D surface. Finally, there are object-space 
features whose definition depends on the view position. Image-space lines have a very intuitive motivation 
and a strong connection to art: artists are frequently taught to draw what they see . Another big advantage 
is that they translate into efficient CG algorithms, because they can often be implemented in graphics 
hardware. On the other hand, such GPU-computed lines can be difficult to stylize, since they are often 
extracted as collections of pixels rather than as complete curves. As an example of image-space lines, 
isophotes are curves of constant illumination, which also correspond to toon shading boundaries. Another 
definition for image-space lines is edges. Using the definition of the popular Canny edge detector, these 
are locations where the image gradient magnitude has a local maximum, when looking in the gradient s 
direction. (In practice, the gradient direction is sometimes quantized to 45-degree increments, to simplify 
finding the local maxima.) Canny s algorithm also includes a sophisticated system of hysteresis thresholding, 
to keep the most important lines. A final very important class of image-space lines are the image intensity 
ridges and valleys . These are curves on the surface of locally maximal/minimal intensity, and are very 
naturally drawn in white/black on a mid-tone color, as in this example from Lee et al. There are several 
different definitions of image ridges and valleys, many of which were originally developed in the course 
of analyzing watercourses on terrain (see, for example, Rieger s paper). The definition used by Lee et 
al. looks for local maxima/minima of intensity, in a direction determined by fitting a paraboloid locally 
and looking for the direction of highest curvature. In other words, the direction is the eigenvector 
corresponding to the largest-magnitude eigenvalue of the image Hessian (matrix of second partial derivatives). 
 For the next class of lines, we consider object-space definitions that look directly at the 3D shape 
of a surface. View-independent lines are those that are defined only based on the shape, without considering 
the viewer s position. This is somehow intuitively satisfying, since these lines depend only on the surface 
shape, and a view-independent definition means that these lines can be precomputed and then re-used for 
multiple viewer positions. However, it has been observed that, under changing view, these lines are more 
naturally interpreted as markings on the surface, rather than as lines depicting the shape of the surface. 
We believe that further perceptual studies will certainly be necessary to investigate this. One example 
of view-independent lines are the constant-altitude lines found on topographic maps. They certainly are 
effective at conveying shape, both through the shape of the lines and their density. Another example, 
appropriate for objects with sharp folds (i.e., discontinuities in the normal), is lines at those creases 
of the surface. This works well for polyhedral objects, and is a frequent ingredient in technical drawings. 
The definition is very simple: just look for a dihedral angle (that is, the angle between two faces connected 
along an edge) smaller than a threshold. Unfortunately, the natural generalization for smooth surfaces, 
ridge and valley lines, leads to mixed results. For example, the ridge lines on the rounded cube at left 
successfully convey its shape. If you look at the picture on the right, you can see that some of the 
ridge and valley lines, such as those around the eyes, do a good job of marking features. However, other 
lines look like surface markings and no good artist would include them in a hand-made drawing. The definition 
of ridge and valley lines is slightly complex: they are local maxima of principal curvature, in the corresponding 
principal direction. (There will be a description of differential-geometry principles later, which should 
clarify what this means!) This brings us to the third class of mathematical line definitions, and the 
one on which we will spend the most time. These are lines defined on the 3D surface, but taking the view 
position into account. (To be precise, sometimes we think of an orthographic viewer, and so talk about 
a view direction rather than a viewer position.) These are the lines that appear to be most effective 
at conveying shape, and lend themselves naturally to many stylization techniques. On the other hand, 
they do have to be recomputed whenever view changes, so they aren t necessarily the most efficient (though 
there has been some progress in GPU-based implementations). First, we start with the silhouette: the 
boundary between the object and the background. Here, we draw the silhouette on the right, superimposed 
over a contrast-reduced version of the photograph on the left (just so we can see what s going on). Silhouettes 
are obviously very important, and are an essential ingredient in any line drawing. However, as you can 
see here, they re clearly not enough. Another common definition is a generalization of silhouettes called 
occluding contours (or sometimes interior silhouettes ). These mark any depth discontinuities, not just 
those against the background. As seen on the right, this adds a lot of important detail to the drawing, 
but still does not convey shallow features, particularly those viewed head-on. Still, these are a common 
component in NPR systems. So, to complement silhouettes and contours we need another line definition. 
If we go back and look at a real line drawing, we see that there are, in fact, more lines that pretty 
clearly are not contours (and not ridges or valleys). For example, there s this line on the chest that 
s clearly on the right side of the torso, not in the middle of the valley . Another example is the line 
on the back of the horse. See how it misses the ridges and valleys on the back of the horse. Also, it 
s certainly not a contour. We hypothesize that these lines are almost contours : if you moved your head 
a bit to the left, this line would in fact become a contour. We call these lines suggestive contours 
, and we ll later see how to formalize what they are and how to find them on a surface. So, here s what 
suggestive contours look like on the David. You can see that they complement the contours nicely (in 
fact, you can prove that they line up with the contours in the image). Also, they include a lot of the 
detail that s missing in the contours-only drawing. It turns out that in order to formalize different 
families of 3D lines on a surface, we ll need some math from a field called differential geometry. This 
is the field that concerns itself with what it means to take derivatives of curves and surfaces. You 
are already familiar with a first-order differential quantity of surfaces: the normal. In fact, as has 
already been mentioned, occluding contours critically depend on the normal: they are zeros of the dot 
product between the normal and the view direction. In a very similar way, different kinds of lines, like 
suggestive contours, will have definitions that depend on higher-order derivatives. So, let s move on 
to exploring second-order derivatives, or curvatures. To start with, let s recall the familiar definition 
of the curvature of a curve: at each point, it is the reciprocal of the radius of a circle that best 
approximates the curve locally. The sharper the bend in the curve, the higher the curvature. Curvature 
has units of one-over-length: if you scale an entire curve up by a factor of two, all the curvatures 
are halved. For a surface, we can talk about the curves formed by intersecting the surface with any 
plane containing the normal. These are called normal curves , and their curvature is normal curvature 
. So, for each point on the surface, there are many different curvatures, corresponding to all the different 
normal planes passing through that point. There is something interesting that happens, though. For a 
smooth surface, the variation of normal curvature with direction can t be arbitrary it has a very specific 
form. Imagine setting up a local orthonormal coordinate system in the tangent plane at a point on a surface. 
For any direction (s,t), expressed in terms of that coordinate system, we can find the normal curvature 
in that direction in terms of a simple formula involving a symmetric matrix II. This matrix is known 
as the second fundamental tensor , and as we ll see is related to how much the surface is bent. Note 
that if you were to expand this formula you d get terms quadratic in s and t: this whole expression is 
therefore just a fancy way of writing a quadratic form. What exactly is II? What are its elements? It 
turns out that there are many formulas for them, all involving some notion of the second derivative of 
the surface. For example, they are precisely the second­order terms in a Taylor series expansion of the 
surface (assuming that z is oriented along the surface normal). Equivalently, they are the derivatives 
of the normal as you move along the surface. Incidentally, if you look for formulas like this in various 
textbooks, there s a good chance you may see them written with the opposite sign. This is because when 
writing the formulas you need to establish the conventions of whether normals are considered to point 
into or out of the surface, and in addition whether convex surfaces are taken to have positive or negative 
curvature. We assume that convex surfaces have positive curvature, and we use the usual graphics convention 
of outward-pointing normals, leading to the signs used here. In many situations it is convenient to 
rotate the local coordinate system to make the matrix II diagonal. It turns out you can always do this: 
the new coordinate axes are the eigenvectors of II. (You might recall a neat theorem from linear algebra 
that the eigenvalues of symmetric matrices are guaranteed to be real: here s a real-life application 
that relies on this fact.) Once you ve done this change of coordinates, the new axes are known as the 
principal directions, and the corresponding curvatures are the principal curvatures. If we plug in the 
new form of II into the formula for normal curvature, we see that all normal curvatures have to lie between 
the principal curvatures. So, the principal curvatures are the minimum and maximum curvatures for any 
direction (at that point on the surface). This leads to something called Euler s formula for normal 
curvature, which expresses the curvature in any direction as a function of the principal curvatures. 
 When talking about curvatures, there are a couple more terms that often crop up: Gaussian curvature 
and mean curvature. These are equal to the product and average of the principal curvatures, and can also 
be computed directly from II (expressed in terms of any coordinate system), as the determinant and trace. 
Notice one interesting feature about Gaussian curvature: it has units of curvature squared , which is 
different from all the other flavors of curvature we ve talked about. Gaussian and mean curvature are 
very useful for qualitatively talking about the shape of a surface. The most basic classification looks 
at the sign of Gaussian curvature. If it is positive, then the principal curvatures are either both positive 
or both negative (and you can tell which one by looking at the sign of the mean curvature). Points of 
positive Gaussian curvature are known as elliptic points, and are either convex or concave regions. 
If the Gaussian curvature is negative, we have what are known as hyperbolic points, at which the surface 
is saddle-shaped. So, if you look in one direction the surface is convex, while in the perpendicular 
direction the surface is concave. Finally, we have parabolic points, at which one of the principal curvatures 
is zero. The most basic shape with zero Gaussian curvature is a cylinder, but there are many more complex 
surfaces at which K=0 as well. In general, except for degenerate cases like cylinders, the parabolic 
points will form curves on the surface (known as parabolic lines), separating regions of positive and 
negative Gaussian curvature. As long as we re talking about parabolic lines in a course about line drawings, 
it would be remiss not to relate an anecdote about the mathematician Felix Klein, who thought that parabolic 
lines might, in fact, be interesting lines to draw on a surface. He had them drawn (probably by some 
poor grad student) on the Apollo of Belvedere. Unfortunately, the experiment didn t turn out that great. 
A little later, we ll see that Klein wasn t entirely wrong: it is possible to select a subset of the 
parabolic lines that look OK Now that we have some math under our belts, let us look in more detail 
at the different types of lines. We begin with occluding contours, sometimes also called interior and 
exterior silhouettes. There are a few different ways of defining these, of which a very straightforward 
definition is simply those locations at which, from the current viewpoint, there is a depth discontinuity. 
Note that these are view-dependent lines, which implies both benefits and drawbacks. On the plus side, 
the view dependence makes it much more likely that these lines are interpreted as conveying shape, rather 
than as surface markings. On the other hand, this means that the lines will have to be recomputed for 
each frame, and potentially makes it harder to do things like line drawings in stereo. On smooth surfaces, 
there is another definition of contours that is useful: contours are those surface locations where the 
surface normal n is perpendicular to the viewing direction v. That is, places where n dot v is equal 
to zero. We can apply the same principles to polyhedra (i.e., polygonal meshes), but there s a problem. 
With only a tiny bit of noise, we can run into a situation where polygons along the boundary are just 
barely front-or back-facing, and the boundary between them is not a simple curve: it can entirely surround 
certain faces. This isn t necessarily a problem if the only thing you re doing is drawing the curve, 
since it will be viewed edge-on. However, if you are doing any further processing on the curve, such 
as trying to draw them with stylization, this can lead to big problems. So, there s a frequently-used 
alternative for polygonal meshes that tends to produce much nicer curves. It is a bit similar to Phong 
shading, in that it involves starting with per-vertex normals and interpolating them across a face. Once 
you know n at each point, you can find n dot v, and locate the curve on the face that corresponds to 
n · v = 0. A slightly simpler variant of this is to just compute n dot v at the vertices, interpolate 
across the face, and figure out where the zero crossing is. We can apply the same principles to polyhedra 
(i.e., polygonal meshes), but there s a problem. With only a tiny bit of noise, we can run into a situation 
where polygons along the boundary are just barely front-or back-facing, and the boundary between them 
is not a simple curve: it can entirely surround certain faces. This isn t necessarily a problem if the 
only thing you re doing is drawing the curve, since it will be viewed edge-on. However, if you are doing 
any further processing on the curve, such as trying to draw them with stylization, this can lead to big 
problems. Here is your brain on contours. Here is your brain on suggestive contours. Any questions? 
 Now that we ve seen contours, let s move on to defining suggestive contours. Here s the first definition: 
contours in nearby views. What happens if we start with the viewpoint at top (which produces a contour), 
then move the viewpoint down a little bit? First, the contour slides along a surface to a new location 
where its surface normal is perpendicular to the new view direction. But something else happens here 
too. A new contour appears that does not correspond to any in a closer viewpoint. This is a suggestive 
contour from the original viewpoint. The other contour corresponds to that in the original viewpoint, 
and is not a suggestive contour. Adding this qualification to our definition completes it. While intuitive, 
the first definition doesn t really lead to efficient algorithms for computing suggestive contours. So, 
let s look at a second definition (which can be proven equivalent to the first one). The idea is that 
suggestive contours are places where n dot v doesn t quite make it to zero (at which point we d have 
a contour), but is a local minimum on the surface. That is, the location of a suggestive contour from 
this viewpoint (assumed to be distant) is where the normal is locally closest to perpendicular to the 
view direction, as you consider points along this normal slice of the surface. This involves moving in 
the direction w , which we define to be the projection of v, the view direction, into the local tangent 
plane of the surface. While definition 2 is better from the point of view of computation, we can transform 
it into yet another form that is still more convenient. The basic idea is that we re looking for local 
minima, so we use the usual definition that minima are places where the derivative is zero, and the next 
higher-order derivative is positive (which distinguishes them from maxima). Now, it turns out that derivatives 
of n dot v are related to curvature. In particular, the derivative of n dot v has the same zeros as 
a quantity called radial curvature , which is just the curvature in the direction w (which, you ll recall, 
is the projection of the view direction). So, our third definition of suggestive contours is that they 
are zeros of radial curvature, subject to a derivative test. This test needs to enforce that the directional 
derivative of radial curvature, in the direction w, is positive. To figure out what that is, we ll need 
to go back and look at the next higher order of surface differentials. So, to recap, the most computationally 
convenient definition of suggestive contours involves finding the zeros of radial curvature, which you 
compute by multiplying II by w twice, then checking the sign of the directional derivative of radial 
curvature, which you get by multiplying the C tensor by w three times (there s also an extra term due 
to the chain rule, which accounts for the change of w itself as you move in the w direction). Once we 
know about curvatures, derivatives of curvature are really nothing special. The only really interesting 
thing is that, as opposed to the normal (which was a vector) and the second fundamental matrix II, the 
derivative of curvature is now a three­dimensional tensor, which can be thought of as a vector of matrices 
or as a cube of numbers . In order to get the derivative of curvature in a particular direction, you 
multiply this tensor by that direction three times. So, to recap, the most computationally convenient 
definition of suggestive contours involves finding the zeros of radial curvature, which you compute by 
multiplying II by w twice, then checking the sign of the directional derivative of radial curvature, 
which you get by multiplying the C tensor by w three times (there s also an extra term due to the chain 
rule, which accounts for the change of w itself as you move in the w direction). Here is your elephant 
on contours (rest of not-a-joke omitted in the interests of good taste) Here s another example  It 
turns out that suggestive contours have some nice properties that let them complement contours very nicely. 
First, they can either anticipate contours by showing up in nearby viewpoints (i.e., definition 1), or 
extend contours in one view. Here we use the color convention that contours are green while suggestive 
contours are drawn in blue. Moreover, in the case of extension, the suggestive contours line up (with 
G1 continuity) with contours in the image. The above property is useful, because in real-world images 
it can in fact be difficult to tell exactly where a contour ends. The fact that suggestive contours extend 
contours smoothly means that there is a single line that corresponds well to the behavior visible in 
such cases. Another property that becomes apparent from definition 3 is that contours show up at inflections 
(of normal curves) on the surface, but only when viewed from the convex side. In this case, the derivative 
test eliminates the suggestive contour at the rightmost viewpoint. So, considering an inflection on 
a surface, there is some region of viewpoints from which suggestive contours get drawn, a region where 
they don t, and a threshold direction at which you start getting contours. Moving the viewpoint out 
of the plane, we see that suggestive contours can only happen when a surface is viewed from a very particular 
direction such that the curvature is zero. If you rotated the viewpoint one way, you d get positive curvatures, 
and negative curvatures if you rotated the other way. Note also that having a direction for which the 
curvature is zero implies that the principal curvatures (which are the minimum and maximum limits for 
normal curvature) can t be both positive or both negative. This, in turn, implies that in order to get 
a suggestive contour, the Gaussian curvature must be negative (or at worst zero). This property can 
be illustrated empirically as well. Here we ve taken a model and plotted a histogram of how many views 
have suggestive contours at each point on the surface. Comparing this to regions of negative Gaussian 
curvature, we see complete agreement. We also see the surprising fact that suggestive contours tend to 
hug the lines of zero Gaussian curvature (i.e., our friends the parabolic lines). We ll see later how 
to show this mathematically, but meanwhile let s think back to Klein s experiment. Even if the suggestive 
contours were always close to the parabolic lines, there s still a big difference between drawing them 
and our definition of suggestive contours: the derivative test. In fact, if we didn t apply the derivative 
test, the lines of zero radial curvature would look pretty bad: just as bad as drawing all the parabolic 
lines. (Here we also show zeros of mean curvature for the sake of completeness.) If we add a derivative 
test, you can see that parabolic lines suddenly don t look so bad, though in general the suggestive contours 
still look better (and have the other properties of lining up with contours, etc.) We also need to consider 
what happens with suggestive contours on backfacing parts of the object; such lines are an effective 
ingredient in transparent renderings. In fact, this was worked out in a paper on volumetric line drawings 
at Siggraph 2005. The first definition of suggestive contours still applies: contours in nearby viewpoints. 
We need to change definition 2 a little bit. When looking at how values of n dot v change across the 
surface, we are still looking for places where n dot v is almost but doesn t quite reach zero. The difference 
is that we re now looking for maxima of n dotv: negative maxima. For the third definition, we are now 
looking for places where the radial curvature is zero where the radial curvature is increasing AWAY from 
the camera. So the sign of our derivative test gets flipped. Perhaps a good way of thinking about this 
is that for backfaces, we re just considering what the inside-out version of the surface looks like (where 
the normals and curvatures are negated). Of course, these suggestive contours still smoothly extend transparently 
rendered contours. So, here s an example of contours and suggestive contours (together with cutting-plane 
intersections) produced from volume data. Although (it is our belief that) suggestive contours are useful, 
there are shapes for which even this is not enough. For example, convex surfaces such as this rounded 
cube have no suggestive contours, yet probably need some more lines to be conveyed clearly. Another 
situation in which it is not clear that suggestive contours provide the right answer are two-tone comics, 
such as this example by Frank Miller. Zooming in, we see that the black line in the white region, which 
might be a suggestive contour, does not line up with the white line in the black region. Therefore, we 
hypothesize that there is something else going on with those white lines. To see how to add more lines, 
it is useful to step back a moment and compare suggestive contours with some image-space lines. For example, 
starting with the head-lit diffuse-shaded view at left, we see that there is a remarkable match between 
the valleys of illumination and the drawing containing contours and suggestive contours. (Incidentally, 
there isn t a perfect match, especially where the surface is twisting in weird ways, and there is ongoing 
research to characterize the exact conditions under which these two families of lines match.) So, in 
looking for more families of lines to complement contours and suggestive contours, it makes sense to 
look at the other flavor of image intensity extrema: ridges of illumination. Unfortunately, even if 
we set up the problem in a restricted way (headlight, diffuse shading), the answer turns out to be very 
messy mathematically. Instead, we will look for simpler line definitions, which nevertheless qualitatively 
match intensity ridges well (just as suggestive contours qualitatively match intensity valleys). To do 
this, we will look at local maxima of n dot v, in the projected view direction w and its perpendicular. 
This corresponds to the definition of suggestive contours as local minima of n dot v in the direction 
w. (Incidentally, we have also examined local minima of n· v in the direction perpendicular to w they 
don t appear to be especially interesting ) To review, w is just the projection of the view direction 
into the tangent plane of the surface. w is also in the tangent plane, and perpendicular to w. In the 
above drawing, w would be pointing towards you.  Now we can define the two families of highlight lines 
corresponding to the above definitions. Suggestive highlights are local maxima of n dot v (corresponding 
to image intensity under a headlight) in the w direction, while principal highlights are the local maxima 
in the w direction. The styles that we will examine draw principal highlights in white (as opposed to 
the contours and suggestive contours, which are drawn in black). We find that this makes it easier for 
the visual system to interpret them. Let s look at suggestive highlights. These are just the inflections 
we threw away (with the derivative test) when finding suggestive contours. Principal highlights are 
another matter entirely. If we go through the math, we find that just as the derivatives of n · v in 
the w direction were related to the radial curvature, the derivatives in the w direction are related 
to another quantity called radial torsion . Intuitively, torsion represents the twisting of the normal 
direction as we move along the surface. Surfaces of zero radial torsion (corresponding to the maxima 
of n · v) are the ones that don t exhibit this twist, in the view direction. This turns out to happen 
precisely when the view direction is aligned with one of the principal directions. We now move to a 
test in the principal highlight definition designed to keep only the strong intensity maxima in the w 
direction. The intuition is that when the view direction is aligned with the higher-curvature principal 
direction e1, the surface is not curving very quickly in the perpendicular direction. The opposite case, 
when we are looking along the ridge (and w is aligned with e2, the weaker principal direction), leads 
to strong intensity maxima in the perpendicular direction, because that s the direction in which the 
normal is changing quickly. So, we keep the locations where w is along e2, which means that it is perpendicular 
to e1. So, instead of basing our definition for principal highlights on torsion, we adopt w · e1 = 0 
as the primary definition of principal highlights. As with suggestive highlights, there is a derivative 
test necessary to keep only local maxima, not minima. Here s what the different definitions look like 
for a simple model. At left, we see all zeros of radial torsion. At center, we keep only those locations 
where w lines up with e2, and at right we apply a derivative test (with a small but non-zero threshold). 
 It is possible to show that principal highlights correspond exactly to converting the depth map to a 
range image, then looking for extrema (illumination ridge and valley lines) according to a particular 
definition due to Saint-Venant. In other words, they are one possible view-dependent analog to the view-independent 
crest lines. (Another possible view-dependent analog is apparent ridges, which we will see later.) By 
looking at the sign of the first principal curvature, it is possible to classify these lines as ridge-like 
or valley-like, and use this additional information to stylize them differently or omit one or the other 
family. Here are some examples with both suggestive contours and suggestive and principal highlights. 
(Drawn together with a gray background and subtle toon shading.) Here is a slightly different style, 
where we still draw the lines in black and white, but draw the shape with a black/white toon shader (so 
that only suggestive contours are visible in the white regions, and only suggestive highlights are visible 
in the dark regions). This corresponds to the style of the Frank Miller comic we saw earlier, as well 
as this painting of a golf ball by Roy Lichtenstein. Looking at this painting, we note that the direction 
of the half­round strokes in the dark region corresponds well with our suggestive-highlight rendering. 
In contrast, simply inverting the suggestive contours in the black toon region (as shown at right) gives 
lines that face the wrong way, and don t match the Lichtenstein painting any more. There is one final 
recently-introduced family of lines that we will look at, namely apparent ridges. These were motivated 
by some drawings, such as this one by Matisse, that seem to combine ridge­like features with contours. 
The approach of apparent ridges is to apply the standard ridge and valley line definition (local maxima 
of principal curvature, in the corresponding principal direction), but replace the use of standard surface 
curvature with a view-dependent quantity that takes foreshortening into account. Specifically, the curvatures 
in the projected view direction are divided by n · v, to account for the fact that normals vary more 
rapidly with respect to screen-space location where the surface is tilted away from the viewer. Here 
is a visualization of view-dependent curvature, as compared to standard curvature. It is obvious that 
view-dependent curvature grows quickly near occluding contours, leading to a strong tendency for the 
technique to place lines near those locations. Here is a comparison of apparent ridges (right) to renderings 
with suggestive contours and view-independent ridge/valley lines. Looking at the qualitative behavior 
of apparent ridges, we see that they match standard ridges and valleys when viewed head-on. As the view 
becomes more oblique, they smoothly slide along the surface until they reach the contours. In fact, they 
connect up to the contours smoothly (just like suggestive contours). In summary, we have seen three 
classes of mathematical line definitions: image-space, view-independent object-space, and view­dependent 
object-space. Although the jury s still out, it appears that the latter category is the most interesting 
because it yields shape­conveying lines, while being amenable to sophisticated NPR stylization algorithms. 
This table classifies the lines we ve seen according to the order of derivatives used in their definition. 
(Incidentally, we don t think there are especially interesting line definitions in the empty boxes, but 
we certainly could be wrong ) You ve heard about the different types of lines that can appear in line 
drawings. Now we re ready to talk about how people perceive line drawings. Line drawings bring together 
an abundance of lines to yield a depiction of a scene. Take a look at this print by Dürer. It uses different 
types of lines that convey geometry and shading in a way that s compatible with our visual perception. 
We seem to interpret this scene easily and accurately. *** Some of the lines in this drawing only convey 
geometry. But the fullness of this drawing comes from Dürer s use of hatching and cross-hatching. These 
patterns of lines convey shading through their local density and convey geometry through their direction. 
 Other drawings rely on little or no shading. In this drawing by Flaxman, shading is limited to the 
cast shadows on the floor. The detail in the cloth here is conveyed with lines such as contours, creases, 
and maybe other lines such as suggestive contours, or ridges and valleys. While artists can make drawings 
like this, they can t really explain what they re doing. They rely on their training, and use their own 
perception to judge the effects of their decisions.  It s actually a little surprising that line drawings 
are effective at all. At first, line drawings just seem to be too ambiguous. An infinite number of 3D 
curves can project to the same line in the image. All images have this ambiguity, but in photographs, 
there are many other visual cues, such as shading and texture, that help to indicate shape. Here, we 
re just looking at individual lines. But it turns out that individual lines contain a wealth of information 
about shape. This information is typically local in nature. But our perception is somehow able to bring 
all of this together into a coherent whole. Well, sort of. Line drawings of impossible 3D objects show 
us that this coherence is NOT global. The Penrose triangle, which was inspired by the work of Escher, 
is perhaps the simplest of the impossible figures. When you first look at it, it seems to be an ordinary 
object. Closer inspection is a little unsettling, and its inconsistencies are easily revealed. Vasarely 
pushed this idea even further, and made pictures such as this one that encourage us to explore several 
different inconsistent interpretations at the same time.  Although you might think the Penrose triangle 
shows that there are no global effects for visual inference, it s not that easy. Take a look at these 
two drawings. The figure on the left appears to be raised in the center, while the figure on the right 
appears to have a flat top, and bends along its length. If we compare these two drawings line by line, 
 the only difference is the line along the bottom. Nobody knows whether we see this difference because 
we consistently integrate local information, or perform certain types of non-local inference.  Use of 
non-local inference is plausible. Algorithms exist for searching among the space of possibilities. Waltz 
s method for line-labeling starts with catalogs of all possible line junctions, which are places where 
two or more lines meet. Here s the catalog of 18 junctions that lets you classify any trihedral vertex 
in a polyhedral scene. CONVEX lines are labeled with a PLUS, CONCAVE lines with a MINUS. Arrows mark 
visual occlusions, where the closer surface is to the RIGHT of the arrow. Algorithms for constraint 
satisfaction compute all possible configurations of junctions for a particular picture. For an impossible 
figure, this set is empty.  This idea can be extended for line drawings that contain smooth surfaces. 
 First, you need a more comprehensive junction catalog. Then, you need methods that can prune away large 
numbers of unreasonable interpretations, to prevent a combinatorial explosion. These algorithms only 
label lines with a type. They don t infer geometry. Furthermore, existing algorithms are restricted 
to lines from contours and creases, and sometimes lines from shadows.  While these algorithms suggest 
that exhaustive search might be a viable method for scene interpretation, they don t say anything directly 
about how PEOPLE interpret line drawings. In fact, not very much is known about that. Even so, we can 
still be very specific about what INFORMATION is available in a line drawing. This is the information 
that our perceptual systems are probably using. Essentially, each line in a drawing places a constraint 
on the depicted shape. In the end, the geometry that results is never unique. But our perceptual systems 
excel at uncovering the most reasonable and most likely interpretations. So now let s go through the 
kinds of information that different types of lines provide. First, we ll consider lines that mark FIXED 
locations on a shape, such as creases, ridges and valleys, and surface markings. Then, we ll consider 
VIEW DEPENDENT lines. The most important is the CONTOUR, which lets us infer surprisingly rich information 
about the shape. There are also lines whose locations are lighting-dependent, such as edges of shadows, 
but I m not going to be discussing those. Of all these lines, creases and contours are well understood. 
Research on the information other types of lines provide is ongoing. Creases mark discontinuities in 
surface orientation, and are typically visible in a REAL image as a discontinuity in tone. The crease 
can be concave or convex. But local information doesn t let us determine which. The algorithms for line 
labeling I mentioned earlier proceeded by considering every possibility, and then enforced consistency 
across the whole drawing.  Ridges and valleys mark locally maximal changes in surface orientation. 
In real images, they can appear as smooth but sudden changes in tone. The ridges on this rounded cube 
are particularly effective at conveying its shape, when drawn with the contours.  Research on the use 
of ridges and valleys in line drawings is ongoing. When used alongside contours, ridges and valleys 
can produce an effective rendering of a shape. The valleys on the side of the horse are quite convincing. 
In other cases, they look like surface markings, such as the ridges on its head. Ridges and valleys 
are reasonable candidates for line drawings, as there is psychological evidence that viewers can reliably 
locate them in realistic images.  Markings on a surface can appear as arbitrary lines inside the shape. 
 However, for a certain type of line known as a geodesic, they can also convey shape. Geodesics are 
simply lines on the surface that are locally shortest paths. Stevens points out that for many fabricated 
objects, surface markings are commonly along geodesics. Take for instance the label on the cylinder on 
the left. For a more general class of surfaces, Knill draws connections between texture patterns and 
sets of parallel geodesics.  When used in repeating patterns, other curves can be effective as well. 
Sets of parallel lines, which are often used to construct plots of 3D functions, are one notable example. 
 The images that result are analogous to using a periodic solid texture. Stevens points out that all 
one needs to do to infer the shape is to build correspondences between adjacent lines, matching up points 
with equal tangent vectors.  The use of repeating patterns of lines forms the basis of hatching. These 
lines convey shape in two different ways; they convey shape directly when they are drawn along geodesics. 
 And they convey shape indirectly through careful control of their density, which can be used to produce 
a gradation of tone across the surface. Particularly effective renderings are obtained when lines of 
curvatures are used, which align with the principal directions of the surface. These also happen to be 
geodesics. So that s it for lines whose locations are FIXED on the shape. Next are lines whose location 
depends on the viewpoint. The contour is the most notable example. There are two situations when contours 
are formed. On a smooth surface, contours are produced when the surface is viewed edge-on. On an arbitrary 
surface, contours can also appear along a crease.  In either case, sitting on the surface is a 3D curve 
known as the CONTOUR GENERATOR. This curve marks all local changes in visibility across the shape. For 
a typical viewpoint, the contour generator consists of a set of isolated loops. It projects into the 
image to become the contour. So not all parts of the contour are visible. Let s consider the different 
cases of visibility for contours. On a smooth surface, the first case is when one part of the shape occludes 
another more distant part. This appears in the image as a T-junction, where the contour goes behind 
another part of the shape. **** At the location where the visibility changes, the visual ray is tangent 
to the surface in two places. The contour then continues behind the shape, **** and is occluded. This 
can be seen in this transparent line-drawing of a torus.  The second case occurs where the contour 
comes to an end in the image. An ending contour. **** When the occluded part of the contour continues, 
it does so at a cusp in the contour.  This cusp occurs because the contour generator lines up with the 
viewing direction, so that its tangent projects to a point.  At an ending contour, the radial curvature 
is zero, which means that we re looking along an inflection; an asymptotic direction of the surface. 
 The last case is a local occlusion; places where the surface has no choice but to occlude itself. **** 
These are locations where the radial curvature is negative.  In transparent renderings of contours, 
one typically does not draw the local occlusions, as the results can be confusing. The image on the right 
here draws these contours. One is marked with an arrow. These curves actually correspond to regular contours 
for an inside-out version of the surface. Here are the three cases, all together.  Now, let s consider 
what the contours look like in the image. The apparent curvature is simply the curvature of the contour 
in the drawing. The convex parts of the contour have positive apparent curvature, the concave parts 
have negative apparent curvature, and it s zero at the inflections. At the ending contours, the apparent 
curvature is infinite due to the cusp.  Koenderink proved a surprising and important relationship between 
the apparent curvature and the Gaussian curvature. Specifically, for visible parts of the contour on 
a smooth surface, they have the same sign. This means we can infer the sign of the Gaussian curvature 
simply by looking at the contour. **** CONVEX parts of the contour correspond to locations where the 
Gaussian curvature is POSITIVE: elliptic regions. **** INFLECTIONS on the contour correspond to locations 
 where the Gaussian curvature is ZERO. **** CONCAVE parts of the contour correspond to locations where 
the Gaussian curvature is NEGATIVE: saddle-shaped regions. **** Koenderink gives a formula that connects 
these two quantities, that involves the distance to the camera and the radial curvature.  A related 
result is that since ending contours only occur where the Gaussian curvature is negative, the contours 
must end in a concave way, approaching their end with negative apparent curvature. **** But Koenderink 
and van Doorn also noticed that artists tend to draw lines that are missing these concave endings. It 
turns out this concave ending can be difficult to discern, as is the case for this Gaussian bump. DEMO 
 Contours are typically easy to detect in real images, at least when the lighting is right. And there 
are many studies that demonstrate how people use them for visual inference. However, in many cases, it 
s not easy to determine where a contour ends. Here s an example photograph of napkin. Even if we zoom 
in, it s still not clear whether the surface occludes itself or whether it s simply heavily foreshortened. 
Observations like this make sense of line types that extend ending contours: suggestive contours and 
apparent ridges. Suggestive contours are another type of line to draw, and whether they are in fact 
detected and represented by our perceptual processes is still an open question. They do seem to produce 
 convincing renderings of shape in many cases. The fact that suggestive contours smoothly line up with 
contours in the image is encouraging. **** In fact, if the lines aren t color coded, it s difficult 
to tell where one starts and the other ends.  We can say something about what information they provide. 
Recall from earlier how suggestive contours can only appear where the Gaussian curvature is negative. 
 In many cases, the suggestive contours approach the parabolic lines away from the contour. On this 
pear, we see how the suggestive contour skims along the parabolic line. DEMO We hope to be able to say 
more about this in the future.  We can make similar statements about apparent ridges. Near the contour, 
apparent ridges behave like suggestive contours. They extend ending contours. As the surface faces more 
towards the viewer, the location of apparent ridges approaches ordinary ridges and valleys. And of course, 
in both of these cases, apparent ridges are surface locations where the normal vector is changing maximally. 
 We can compare renderings with ridges and valleys to renderings with suggestive contours. On the horse 
from this viewpoint, the rendering with just valleys is actually quite convincing. As noted earlier, 
many of the ridges appear as surface markings here. For the valley rendering, some features are missing, 
but the more salient features on the side of the horse are depicted. Note the slight differences between 
the lines from suggestive contours, and from valleys. The shapes they convey appear to be a little different. 
 Clearly there is a lot of interesting work to do here. This concludes our discussion of what information 
particular lines provide.  Of course, this information can only be used if we know the TYPES of the 
lines when we re given a drawing. Earlier we discussed algorithms for line drawing interpretation; approaches 
like this are reasonable to consider for this purpose. But even if we do use these algorithms, there 
are often several different labelings that are consistent. Given the line drawing on the left which 
depicts an elliptical shape with a bump, we can successfully label the green points as contours. The 
red point, however, can be either a contour or suggestive contour. Two possible shapes that match these 
labelings are shown on the right. Presumably this problem cannot be solved in general. There will always 
be ambiguity. It s possible that when artists make line drawings, they re careful to shape the remaining 
ambiguity so it won t be a distraction. And even with a line labeling, there is the ambiguity of projection. 
These three interpretations have the same line labeling, but different geometries. At first, this seems 
hopeless. Yet sketching interfaces like Igarashi s Teddy seem to be quite successful by using inflation. 
 How can this be? Well, there are reasonable constraints on smoothness that we can expect of the underlying 
shape. We also presume that the artist has drawn all of the important lines, so that no extra wiggles 
remain. These issues are the source of one crucial challenge for sketch-based shape modeling.  We can 
be more specific with regard to this ambiguity. For real images, there are well defined ambiguities 
for particular types of imagery. One notable example is the ambiguity that remains when viewing a shape 
under Lambertian illumination. There is a group of shape distortions that can be applied to a shape, 
that with an corresponding transformation of the lighting positions, approximately produce the same image. 
 This is the three-dimensional projective mapping known as the generalized bas-relief transformation. 
As shown here, it moves points along visual rays and preserves planes. It also preserves contours, boundaries 
of shadows, and the relative signs of curvature on the shape. Perhaps most interestingly is that when 
you ask people to describe the shapes they see in shaded imagery, they answer consistently modulo this 
ambiguity transformation.  So how can we be sure that a line drawing we make is perceived accurately? 
As you saw earlier, one possible path is to compare that line drawing to those made by skilled artists. 
 Another way, based in psychophysics, is to simply ask the viewer questions about the shape they see. 
 If this is done right, you can reconstruct their percept and compare it to the original shape, given 
the appropriate ambiguity transformation. Koenderink and colleagues already performed a study like this 
on a single line drawing. Their results suggest that the bas-relief ambiguity might be the appropriate 
one to consider here. However, this ambiguity may only be resolved locally, where different parts of 
the shape are locally consistent, but not necessarily in a global sense.  So what kinds of questions 
can you ask viewers? In psychophysics, the answer is: very simple ones, and lots of them. Koenderink 
describes a set of psychophysical methods for obtaining information about what shape a viewer perceives. 
 The first they describe is called RELATIVE DEPTH PROBING. The viewer is shown a display like this one, 
and is simply asked which point appears to be closer. They are asked this question for many pairs of 
points.  Another method is known as DEPTH PROFILE ADJUSTMENT. Here, the viewer adjusts points to match 
the profile of a particular marked cross-section on the display.  Their third method is known as GAUGE 
FIGURE ADJUSTMENT. Here, the viewer uses a trackball to adjust a small figure that resembles a thumbtack, 
so that it looks like its sitting on the surface. All of these methods are successful. But gauge figure 
adjustment seems to give the best information given a fixed number of questions.  So in summary, Each 
type of line in a line drawing conveys specific information about shape. There is a fair amount of evidence 
that people use this information. But how exactly people use this information is still unknown. We re 
very encouraged by how a combination of computer graphics and psychophysics can lead to answers to these 
questions.  There are two major classes of algorithms for extracting most kinds of lines from 3D meshes. 
First, there are image-space algorithms that render something (such as a depth map or cosine-shaded model), 
then extract lines by doing some sort of image processing on the framebuffer (for simple operations such 
as thresholding, there are often ways of achieving the same effect using texture mapping, or vertex or 
pixel shaders). The advantage of this kind of algorithm is that it can be fast, easy to implement, and 
provides some notion of view-dependent level of detail. A major disadvantage is that it makes it difficult 
to control the appearance and stylization of the resulting lines. A second class of algorithm operates 
in object space on the model directly. These algorithms tend to be a little more complex, and it is 
more difficult to adapt them to take advantage of graphics hardware. On the other hand, they provide 
good control over stylization. Finally, there are hybrid (usually multi-pass) algorithms, which perform 
a bit of processing in object space, but the lines ultimately show up only in the frame buffer. These 
are much less general than the other kinds of algorithms, and are specialized for e.g. contours. Let 
s start with occluding contours(or interior and exterior silhouettes), and look at image-space algorithms. 
A very simple technique is to render a lit version of the model (without color), then perform a thresholding 
step: any region darker than a threshold is set to black (or the line color), and anything above the 
threshold is set to the background color. There are many ways to do this thresholding step as part of 
the rendering, using pixel shaders, texture mapping, environment mapping, etc. One major problem with 
this algorithm is that the thickness of the lines can vary, sometimes quite a bit. There are a few tricks 
to get around this problem. First, you can use a texture map indexed by n dot v, but use mipmapping. 
The trick is to make the width of the black region in the texture map the same width in all mipmap levels. 
 Another solution is to take advantage of the fact that, for a constant-curvature region, you can determine 
how thick the lines will be as a function of radial curvature. Then, the approximation is to change the 
threshold depending on the square root of radial curvature. There s a second, completely different, 
image-space algorithm that s possible. Now, instead of renderingn dot v, we render a color that depends 
on depth (or just look at the depth buffer instead of the color buffer). The image processing operation 
we have to do here is more complicated: edge detection instead of just thresholding. This is an interesting 
tradeoff: we have made the rendering simpler, but the image processing more complex. Let s now move 
to object-space algorithms for contour extraction. Recall that we talked about two possible definitions 
of contours on polygonal meshes: contours along the mesh edges (separating front­facing and back-facing 
faces), or contours within faces (zeros of interpolated n dot v). For the first definition, a simple 
brute-force algorithm is just to loop over all edges, and check whether each has one adjacent frontface 
and one adjacent backface. For the first definition, a simple brute-force algorithm is just to loop 
over all edges, and check whether each has one adjacent frontface and one adjacent backface. This has 
the disadvantage that the contour, when viewed as a path along mesh edges, can form loops. The other 
definition involves computing n dot v at each vertex, then looping through all faces of the mesh. For 
each face, you first ask whether n dot v has a different sign at some vertex. If so, you interpolate 
along edges connecting positive-(n dot v) vertices and negative-(n dot v) vertices to find zeros, then 
connect the two points with a segment. Both of these object-space algorithms are brute-force: they require 
looping over all the edges, vertices, and/or faces of the model. There is a large body of work on acceleration 
techniques that try to reduce running time. For the contours-within-faces case, one popular technique 
is to construct a hierarchical data structure, where each node stores a bounding cone of the normals 
below it. At run time, the tree is traversed, and any nodes for which the cone is entirely frontfacing 
or entirely backfacing can be pruned. Another interesting acceleration technique involves the Gauss 
map. As a preprocess, a data structure is built that represents the space of possible directions (the 
space of directions conceptually corresponds to a sphere, but usually a cubemap is easiest to work with). 
For each edge, we compute an arc (shown in yellow) between the directions corresponding to the normals 
of the two faces touching that edge. Each direction intersected by this arc gets a pointer back to the 
edge. At run time, we check all directions corresponding to the normals perpendicular to the view: any 
arc that intersects that circle of directions (shown in green) represents an edge that is part of the 
silhouette (in practice, a superset of edges is generated because of the discretization of the Gauss 
map, so candidate edges must be verified). A very different sort of acceleration technique, most suited 
to interactive systems, relies on randomization. We pick random faces on the mesh, and check whether 
they contain a contour. If so, we follow the contour by walking to adjacent faces, eventually extracting 
an entire contour loop. In order to improve the efficiency of the random testing, we can test those faces 
that contained a contour in the previous frame before resorting to the random testing. This algorithm, 
of course, is not guaranteed to find all the contours unless we test all faces. However, it is very likely 
that all significant contours will be found, and the reliance on temporal continuity means that it is 
very likely that after a few frames it will find everything. Regardless of the details, all object-space 
contour finding algorithms must deal with the problemof visibility. Althoughwe ll look at some strategies 
for this later on, for now let us emphasize the fact that there are two ways in which a contour can be 
invisible: it can be occluded by a distant portion of the mesh, or it can be occluded locally. The latter 
pieces of the contour can be identified simply by checking the sign of the radial curvature, so at least 
part of the visibility problem can be solved locally. Full visibility is usually resolved using an algorithm 
such as ray tracing or z­buffering. Let s look atone algorithmin the hybrid category. Imagine doing 
a standard rendering pass, then keep the z-buffer on and render just the backfaces slightly enlarged 
(which can be done by actually changing the geometry, or by rendering the backfaces using thick outlines). 
Around the contours, the second rendering pass will peek out from behindthe geometry rendered on the 
first pass. This is a nice algorithm because it can be very fast (modern graphics hardware can do it 
in one pass), and requires neither additional data structures nor image processing. However, just as 
with image-space algorithms, there is no control over stylization.  Here is your brain on contours. 
Here is your brain on suggestive contours. Any questions? Let s move on to algorithms for suggestive 
contours. There are three different definition, and each gives rise to a different algorithm. The first 
definition, contours in nearby views , is difficult to work with and requires a search over viewpoints. 
 Let s move on to algorithms for suggestive contours. There are three different definition, and each 
gives rise to a different algorithm. The first definition, contours in nearby views , is difficult to 
work with and requires a search over viewpoints. The seconddefinition, local minima of n dot v , gives 
rise to an image-space algorithm in which an (n dot v)-shaded image is rendered, anda valley detection 
filter is used to detect valleys of intensity. Finally, the thirddefinition, zeros of radial curvature 
(subject to a derivative test) naturallyleads to an object-space algorithm. This algorithm extracts 
loops where radial curvature is zero, using either a brute-force approach or one of the acceleration 
techniques we talkedabout Then, the derivative (in the projection of the view direction, which we ve 
been callingw) of the radial curvature is tested at each point along the curve, and we reject regions 
where it s negative. Finally, we can stylize the lines however we want, such as this style that fades 
out strokes as the derivative of curvature approaches zero. This algorithm can be augmented to throw 
out some of the unstable lines. The idea is that if, at an inflection corresponding to a zero of radial 
curvature, the curvature is varying rapidly, that location is stable. On the other hand, these shallow 
inflectionsare rather unstable  The addition of the slightest bit of noise causes perturbations in the 
suggestive contours, and might introduce new ones (or delete existing ones). So, one way to prune strokes 
is to apply some threshold to the magnitude of the curvature derivative, which eliminates these shallow 
inflections. We can derive this speed from the implicit function theorem, which says that we have to 
look at both how quickly radial curvature is changing with respect to camera motion (numerator), and 
how quickly radial curvature is varying over the surface (denominator). We can derive this speed from 
the implicit function theorem, which says that we have to look at both how quickly radial curvature is 
changing with respect to camera motion (numerator), and how quickly radial curvature is varying over 
the surface (denominator). Working out the math, we get this formula. Looking at the individual terms, 
we can see that velocity will be largest, hence the curves most unstable, when the terms in the denominator 
are zero. These correspond to sin(theta)=0 (looking at the surface) and gradient(kr)=0 (shallow inflections). 
Conversely, when the terms in the numerator are zero we have the maximal stability. This happens when 
cos(theta) is near zero (i.e., approaching a true contour), or when the Gaussian curvature is small (approaching 
the parabolic lines). This is a mathematical explanation of why suggestive contours (when considered 
over all views) tend to hug the parabolic lines. Here are a couple of examples of pruning according 
to the formula for the speed (right), or according to a simpler formula that just tries to avoid shallow 
inflections and lines seen head-on (center). Very similar acceleration techniques to those used for 
contours can be used for suggestive contours. The performance of the randomized algorithm across a flythrough 
involving several views is presented here. Using the lines from the previous frame as seeds had a fairly 
large impact, while another technique (walking downhill from the random seeds in search of a zero of 
radial curvature) shows limited improvement. Overall, very decent results can be obtained by testing 
10% of the faces or less. Finally, there s a way to use the graphics hardware to extract suggestive 
contours, similar to the use of texture maps indexed by (n dot v) to draw contours. The idea is to use 
a texture map with a dark line in part of it, with the horizontal texture coordinate indexed by radial 
curvature and the vertical coordinate indexed by the derivative (possibly with some sin(theta) terms 
as well). The dark part of the texture map will only be accessed if the radial curvature is near zero 
and the derivative is greater than zero (or some threshold). Note that in most cases the curvature and 
derivative willhave to be computed by hand at each vertex, and the correct texture coordinates passed 
in. Here s a comparison of a few differentalgorithms. The first two images come from the object-space 
algorithm, with and without fading of strokes. The rightmost image was done using the texturemap-based 
algorithm. Finally, let s lookbrieflyat algorithms for computingridge and valley lines. Because these 
are defined in terms of high-order derivatives, which are often noisy, a big challenge is in getting 
good, robust estimates of these differential quantities. A paper from last year achieved good results 
by computing the derivatives using implicit function fits, then doing some filtering on the resulting 
strokes. Another interesting approach is to look for lines that are stable over different scales of 
filtering. This algorithm actually operates on unorganizedpoint clouds, and doesn t need a full mesh. 
  I.d like to start this section with a quote from David Byrne in an article for Utne. In the article 
he was mostly talking about 2D design and illustration, but I believe his point translates to renderings 
from 3D as well. Various aspects of stylization include: Abstracted shading such as toon shading and 
tonal marks such as hatching; Stylized brushes that taper at their ends, have varying width and transparency, 
wobble along their paths, etc; Paper or media effects that effect the lines that are drawn as well as 
the shaded or blank areas of the drawing; Also tightly coupled with stylization is abstraction, which 
Doug will address in greater detail in Part 6 of the course. Here is an overview of this section.  
We.ll start by talking about stylized lines.  Strokes are the fundamental primitive of a line drawing. 
Each individual stroke has many qualities in addition to its path --it can have varying thickness, wiggliness, 
opacity, and texture, not to mention its time-dependent nature. These qualities give line drawings much 
of their character or charm, and can convey feeling as well. Here are a few more examples of the range 
of effects possible, showing up close what the brush might look like. Lines can be made to wiggled with 
offsets, textured with watercolor strokes, broken into dashes for mechanical drawings, and even geometric 
effects can even be suggested, such as the thorny stylization used on silhouettes of this cactus. In 
1994, Hsu presented a system called Skeletal Strokes that described how such qualities could be applied 
to CG lines. Indeed many of these features are now standard fare in commercial programs such as Adobe 
Illustrator. Thinking about the problem of drawing such stylized marks efficiently as part of an interactive 
rendering system for 3D models, Northrup et al. described how to map such stylization onto textured triangle 
strips drawn with a standard graphics interface such as OpenGL. This technique was adapted by Kalnins 
et al to handle 2D textures along the length of the stroke as well as a little more emphasis on media 
interaction with paper. The lines can thus be drawn with very stylized brushes that simulate the look 
of real media, at interactive frame rates. The next question to address is how do we know which lines 
extracted from a 3D model are visible? There are a number of challenges for computing visibility. Of 
course it is possible to fire a ray from many points each line to the viewer and see if it intersects 
anything along the way. However, that strategy is expensive, meaning it won.t support interactive applications 
for even moderately complex scenes. For interactivity, why not just use the z-buffer? Well it turns out 
that if you draw the model and also thick, wobbly lines in 3D, then the z-buffer algorithm will chop 
the lines in ugly ways where they happen to penetrate the model. Finally, a particular challenge for 
silhouette lines is that by definition they exist right at the boundary between visible and invisible 
parts of the model, making many visibility algorithms naturally unstable at such points. Three general 
strategies have been used for visibility of lines. First, there are hardware-based methods that cause 
visible --or invisible ­-silhouette lines to appear without explicitly searching for them on the model. 
Such methods are fast, but admit only minimal control over the stylization of the lines they reveal. 
Second, while ray casting can be slow, analysis of the shapes involved can reduce the number of rays 
cast. Third, a hybrid method based on an item buffer can compute visibility with image space precision. 
I.ll go into a little more detail on these last two strategies. Here.s how the item buffer works. You 
draw the polygons of the model, using polygon offset, to set the z-buffer. Then you draw the lines of 
the model, each colored by a unique color that identifies that line. Then to determine which parts of 
which lines should be drawn as strokes, you walk each line checking for the appropriate ID/color. Then 
you draw those visible portions as strokes. Way back in 1967, Appel introduced the notion of quantitative 
invisibility wherein the set of lines is examined to find explicit places where visibility can change. 
After such analysis, only a few rays need to be fired to determine the visibility for the whole drawing. 
In the Real-time NPR system, Markosian et al offered some improvements to Appel.s algorithm, requiring 
even fewer rays to be fired. Based on these algorithms, they were able to render drawings such as the 
one on the right for very complex models faster than could be done by rendering the full mesh in graphics 
hardware of the time. However, these algorithms that analyze quantitative invisibility are difficult 
to implement because they are sensitive to certain kinds of numerical instabilities. Therefore, Northrup 
and others developed a hybrid visibility system using an item buffer rendered in graphics hardware. The 
idea is that you render the entire mesh as well as the lines of your line drawing, each primitive marked 
with a specific ID. Then you can walk over the lines searching for in the reference image to see if the 
appropriate ID is in the neighborhood. If so, you consider that part of the line to be visible. Kalnins 
et al used this method as well. Furthermore they were able to leverage the item buffer for propagating 
parameterization of the lines from one frame to the next for temporal coherence, as I will describe shortly. 
 But first, let me discuss the question, how can an artist specify what kind of stylization he would 
like for a model? In 2002 my student Rob Kalnins and others presented a system called WYSIWYG NPR, which 
stands for What you see is what you get non-photorealistic rendering. The basic idea is that an artist 
should be able to draw right onto the 3D scene how he would like it to looks. A handful of other researchers 
have considered tools to creating stylized 3D scenes from scratch via drawing. In these approaches, the 
artist.s input produces a form of stylized NPR geometry. These systems are wonderful because you start 
with nothing, and produce both a 3D model and its stylization at the same time. But they make assumptions 
about what you might mean when you draw a stroke and are therefore limited in the kinds of models that 
can be created. In this sense the WYSIWYG NPR system more like that of Hanrahan and Haeberli in which 
you start with a model and draw directly on it. However, in Hanrahan.s system they were making textures 
for the model, rather than thinking about annotating a model with an NPR rendering style. Here.s how 
the WYSIWYG NPR system works. The designer begins by importing some existing 3D geometry like this tea 
cup mesh. He then describes its stylized appearance annotating the objects with stylized strokes drawn 
directly into the scene. When the user moves to new camera positions, the system automatically retains 
these stylizations. Furthermore, under animation, the system also ensures that the stylizations *evolve* 
in a coherent fashion. We have found that the payoff in aesthetic flexibility is immediate. Even with 
a relatively small toolbox of effects that describe shortly by simply permitting the designer to express 
the stylization via hands-on means a wide range of diverse aesthetics can be achieved. All of these 
results were produced by annotating the same 3D tea cup mesh from the previous slide. The following 
video clip shows the kind of interface presented to the artist. The WYSIWYG NPR system provides a bunch 
of different tools to produce a combined effect. From an organizational perspective, we can group these 
components into two broad categories: parametric and direct controls. Let me begin with parametric category 
 Parametric controls are rendering options tuned by sliders, or checkboxes, for instance to adjust parameters 
like colors for shaders, widths for brush styles, and textures for paper and media simulations. Controls 
like these are common to much of the previous work in NPR, and indeed our system also employed these 
sorts of components. Though necessary, I won.t really focus on these tools today. Instead, I.ll focus 
on the second category --direct controls which take their input via a more natural means, such as a 
tablet. These controls allow the artist to influence the look of the scene by sketching strokes directly 
into it. The first of the direct controls are concerned with creating detail marks on object surfaces, 
such as the strokes depicting a label on this apple sauce can. To create detail marks, the user simply 
selects a brush style, and sketches features onto the mesh. This has the effect of defining stroke paths 
embedded in the 3D surface. As described earlier, each path is rendered using a stroke primitive based 
on triangle strips. Note that under magnified or oblique views, this approach differs from texture maps 
into two ways. First, there are no blurring or aliasing artifacts, because we render these paths using 
a stroke primitive, and second, we have explicit control over stroke width. For instance, we may desire 
to keep stroke widths close to their original extent in all views, suggestive of a real brush that cannot 
change size. The designer can control this behavior independently under foreshortening and magnification. 
Here strokes maintain their width in foreshortening, even extending beyond the surface in the limit, 
while under minification, widths are allowed to shrink to avoid a piling up of detail. In this case, 
the artist employs a subtle effect: the purple flower strokes shrink more quickly than the green ones, 
so the large central leaf dominates in distant views. The designer can achieve a wide range of effect 
with this straight forward tool. A few strokes can depict labels and decals as in the upper images, while 
many wide stroke can create rich painterly effects and many narrow strokes can lead to detailed line 
drawings. I.ll show several of these scenes in animation in a little while. The next direct effect is 
hatching. Much like decals, hatching is drawn directly on the surface in the location and style desired 
by the artist. Because these elements are not necessary for sparse line drawings and because of time 
limitations I.m going to push on to talk about outlines.  In earlier sessions we talked about silhouettes, 
creases and suggestive contours. These three elements, though not exhaustive, can produce a wide range 
of line drawings (and are often components of more complex drawings). Let me show you a video of outline 
stylization in action.  We alleviate the tedium of over-sketching all creases of a model, by providing 
two schemes for generating stylization from example input. The first method rubber-stamps a single stylization 
prototype, while the second method synthesizes novel stylization from a small set of examples. The latter 
method is achieved by feeding the examples through a Markov process model using a method similar to that 
of Schodl.s Video Textures paper. Stylizing the outlines of a model presents a particular challenge 
for temporal coherence. On the left, we show the mechanical part with no stylization yet applied to its 
lines this rendering scheme can be easily implemented in graphics hardware, but it also has the added 
advantage of being easy to animate because there are no stylized features to keep track of over time. 
On the right, we show the same mesh rendered with stylized lines. Such stylization can lead to a wide 
range of rich aesthetics, but this comes at the cost of additional challenges during animation, since 
we.ll have to keep track of the salient features of the stylization over time. So the last part of this 
section addresses the challenge of providing temporal coherence for such lines. Just as a reminder, 
we.re dealing with a variety of styles of lines, and we.d like their fine-scale detail to be coherent 
from one frame to the next. Imagine that the dashed line is some crease we want to stylize. We call 
its screen-space projection a brush path. If the user then over­sketches the crease to stylize it. The 
system records it as a sequence of screen space offsets perpendicular to the brush path. To apply the 
stylization to this crease in another view, the 3D crease is once again projected to yield the new brush 
path and the offsets are are used to generate the stylized path. For creases, the relationship between 
the original base path and the new base path is obvious because the crease remains at a fixed location 
on the mesh in all views. However, for silhouettes this problem is more complicated. To see why, we 
must consider the silhouette rendering pipeline. Beginning with the triangle mesh The first step is 
to extract the silhouette contours from the underlying geometry. Szymon discussed this problem in an 
earlier session. The second stage is to compute visibility, as I discussed moments ago. And the final 
stage is to render these visible paths using the stylization provided by the artist. Maintaining temporal 
coherence of these effect is challenging. To illustrate this I.ll show you an animation of this octopus 
--first without and then with --explicit attention to the coherence of the stylization from frame to 
frame -Initially, we assign the stylization using the natural arc-length parameterization of the silhouettes. 
This intrinsic parameterization leads to coherence artifacts, such as popping. and swimming.. -To solve 
this problem we propagate parameterization information from frame to frame. This allows us to explicitly 
assign stylization with the goal of coherence. Notice how the artifacts have been vastly reduced. So, 
which part of the silhouette stylization pipeline, described earlier, is responsible for producing coherence? 
 Well, it turns out we skipped a step. Before the visible paths from stage 2 can be rendered with strokes 
in stage 3, it is necessary to parameterize those paths. This is the missing step. As demonstrated with 
the octopus example, the coherence of the underlying parameterization is revealed by the stylization. 
If we do not explicitly assign parameterization with coherence, the the lack of coherence arising from 
an implicit parameterizations will be revealed. There had in fact been a small amount of previous work 
in this area of the pipeline -Masuch in 98 demonstrated how to maintain coherence for the specific case 
of a single stroke under partial occlusion. -Bourdev in the same year presented a more generate framework. 
His sample propagation approach inspired the approach I.m about to describe. Let me concisely state 
the problem we.ll address here. Given some stylization of the silhouettes of an animated scene at time 
i, how can we achieve a coherent stylization in the next frame, i+1? Note that our motivation for considering 
consecutive frames -­rather than the entire animation as a whole --is that we want to achieve coherence 
in interactive in addition to offline settings. Recall that the stylized strokes are generated on top 
of a set of continuously parameterized paths. To sample this parameterization state for some given frame, 
we record a set of discrete samples along each parameterized path. Within each sample we store the unique 
id. of the path it came from (as denoted by the color coding in the figures), as well as the value of 
the parameterization at it.s position along on the path and its location on a triangle of the mesh (so 
the sample can be located in the next frame, even if the mesh deforms). Propagating the samples from 
frame i into frame i+1 is a two step process. First we transform the samples to their new 3d location, 
accounting for animation and camera changes. In this example, the torus has been rotated about a vertical 
axis. The samples from frame i will still lie on the surface of the mesh, but in general they will not 
coincide with the silhouettes of frame i+1. So, the second step registers these samples with the new 
silhouettes by walking in 2D using the item buffer I mentioned earlier, which allows us to efficiently 
search for the silhouettes in real-time, as follows. We begin by projecting the transformed sample into 
the item buffer. We then search along the projected normal direction, since near silhouettes, this will 
point toward the boundary. If a silhouette pixel is found, we register the sample at the intersection 
point. The final step is to parameterize the brush paths with a continuous function, based on the set 
of samples that were propagated from the previous frame. To see how this works, consider the example 
of the red brush path. Each parameter sample, tj, that arrives on this brush path can be recorded in 
terms of its position, s, measured in arc-length along the brush path. This sample contributes one parameterization 
data­point to the graph on the right. Indeed we can plot all of the parameter samples vs. their arrival 
position along the brush path. Next, to assign a distinct parameterization to all point along the brush 
path, we need only fit some continuous function to this data. Fitting requires that we identify an objective 
function for coherence. However, one of the results of this work was the observation that there is no 
single coherence goal suitable for all situations. First, consider the dotted lines used to stylize this 
mechanic part. Dotted lines are image-space features. We want the dot spacing to remain constant in image-space. 
Notice how the dots maintain their spacing even when the object is foreshortened. On the other end hand, 
consider thorny features on this cactus. These are object-spaced features. We expect them to appear stuck 
to the object like geometry. However, if we employ the image-space coherence goal. Notice that in order 
to maintain image-space density the thorns slide over the surface as the cactus foreshortens. So image 
space coherence is not suitable for object-space features. Instead, object-space coherence would have 
features appear stuck on the silhouette. Notice how the thorns remain in place under foreshortening, 
allowing density to change. Finally, if we apply the object-space coherence goal to image-space features 
the density of the dots distorts in an inappropriate way. In short, our goal for how to fit those samples 
depends on the nature of the stylization, and typically, one would choose to strike some balance between 
the two forms of coherence. Let me show some video that reveals these effects in action.  Summary 
 Now that you have an idea of how an artist can make a drawing, we can talk about how artists control 
what content you get out of their drawings. I ll now talk about methods for achieving meaningful abstraction, 
and ways to assess how effective a drawing actually is.  Artists can design effective imagery by changing 
or leaving out specific visual content. The result of this process encourages particular interpretations 
for the viewer and enhances the viewer s understanding of the scene or situation. This is the process 
of abstraction. It s a tool for effective visual communication. Here, Guptill adapts the shading in 
this drawing to guide your attention to different parts of the scene. Artists often omit content, such 
as the detail on the flowers on the top left. Contrast this with the detail Guptill included on the 
right in order to stress the flowers. Same for the cherries here, where the focus can be on the leaves, 
on the cherries, or split between both. There are a variety of means to do this. The particular visual 
style and medium determine the kinds of omissions and distortions that are possible.  In this example 
by Crane, for instance, the form of the elements of a scene are very different between these two different 
renderings. Apart from selectively including various scene elements such as the clouds, we see how Crane 
adapts the details in the shape of the distant trees and buildings. However, their rough shape remains 
so that we can easily identify these objects.  How does this work in NPR? There are a range of possibilities. 
The most important difference comes from how the important content is selected. But techniques also 
differ in how they go about retaining or removing content, given a particular visual style and medium. 
 Let s start with NPR approaches that work from photographs. For painterly rendering, a fully automatic 
approach might attempt to preserve important content as determined by a computational model of image 
salience, which predicts how noticeable particular parts of the image are. When such predictions select 
the important content, this is quite effective.  Another approach to producing effective artistic renderings 
is to have a user explicitly mark important regions of a picture. The understanding is that these regions 
of the picture will be rendered with finer detail. In 3D scenes, the important content is located based 
on where it is in the world. However, in restricted domains where the content is known, such as the rendering 
of 3D models of trees, heuristics can be applied that create effective omissions. Here, detail in the 
center is left out as the tree is drawn smaller.  In more general domains, a user must specify the important 
content, just as they do in images. This was the case in Winkenbach s system for pen and ink illustration, 
where the process of texture indication was guided by a set of marks drawn by the user.  Working with 
3D scenes presents a new problem since the same objects can be viewed at various distances by simply 
moving the camera. Sometimes important information just can t be included without causing clutter. Thus, 
one approach is to explicitly measure the density of lines in a potential rendering. Some approaches 
work by using models of clutter effectively, alongside methods for prioritizing content. However, it 
s difficult to get this working in animation without introducing temporal artifacts. Another possible 
approach echoes work on level-of-detail. Build a multi-resolution representation of a scene, and select 
the scene elements based on viewing distance. This simple approach avoids clutter, although without any 
real guarantee. It s possible to combine both of these ideas, which controls emphasis and manages clutter. 
Forrester will be going into the details of such a system. Here is an example from my work with Anthony 
Santella. A photograph is transformed into a stylized version which consists of black lines and uniformly 
colored regions. The interaction with the user is minimal: they simply look at the photograph for a 
short period of time. A recording of the user s eye movements provides the information required to perform 
meaningful abstraction. For the rest of this talk, I ll be explaining why this is a reasonable approach, 
and how we evaluated the effectiveness of this system. It s all about how our attention shifts and how 
our eyes move. Our eyes are constantly moving. **** Here is an example recording. Several times each 
second our eyes undergo rapid motions known as saccades. These are punctuated with stabilizing motions 
known as fixations, where our eyes are held fixed over a particular location. It has been demonstrated 
through a range of psychological studies that longer fixations indicate INTEREST on the part of the viewer. 
 We can record eye movements using commercial eye trackers, such as this one. So we can indirectly 
record the content of interest for a particular viewer.  Our system starts with photograph, **** decomposes 
it into a hierarchy of visual elements, **** and renders a subset of these elements into an output image. 
 **** The features to render are selected by a perceptual model that draws upon a recording of a viewer 
s eye movements.  Our image analysis starts with edge detection, **** and a set of image segmentations 
performed at a range of resolutions. **** Finer scale segmentations contain more detail. **** We build 
a hierarchical segmentation by inferring containment relationships between regions across resolutions. 
 This is our image representation.  We then can prune the segmentation tree **** based on predictions 
made by the perceptual model. The perceptual model decides which of these visual elements will be included 
in the result. It does this based on the size and local contrast of each visual element, and for how 
long it was fixated. In short, noticeable regions are more likely to be preserved, particularly when 
the viewer examined it closely. So anything that the viewer probably didn t notice will be removed. 
We render the leaves of the pruned tree into the output image. We also smooth these regions so that 
the detail in their boundaries reflect the appropriate scale. Larger regions are smoothed more. The 
lines are also selected using a perceptual model. They are smoothed by a fixed amount. This means that 
they won t always line up with the regions. In areas where the regions are heavily simplified, this misalignment 
will be larger. The result of this is a sketchy look where detail was removed.  The final picture is 
made by overlaying the lines on top of the regions. Here, the small photograph on top is the original, 
below are the users fixations, each white circle is a fixation, where its size indicates fixation length. 
The scale at the lower left is one second. The foreground figure is clear, as the viewer examined that 
location. Figures in the background have had most of their detail removed.  You can compare this to 
results where we don t use eye movements, but instead a global control for detail. With high detail, 
the background looks distractingly fussy. With less detail, important features such as the face are lost. 
 We can assess whether these images achieve meaningful abstraction. One approach to evaluating imagery 
is to measure performance in a particular task. Another is to measure the activity or effort required 
of the user. A commonly used approach in evaluating an interface measures how much the user must move 
the mouse while performing some task. It s not a performance measure, but rather an indirect measure 
of effort. We take this approach, and a natural activity to measure is eye movements. As briefly mentioned 
before, we know eye movements reflect viewers interests and goals. Because of this link to cognition, 
 they ve been used in the past to evaluate complicated visual displays that must provide efficient access 
to information. We hope to find that, for our images, viewers concentrate more on the areas that were 
highlighted with increased detail. To test this we can compare fixations over these images to those 
on the original photograph. But there are some other interesting possibilities.  One possibility is 
uniform detail control, which uses a global threshold in place of eye movement recordings.  Another 
approach is to choose locations for increased detail automatically. Methods for predicting salience 
combine a number of filters to create **** a map of feature contrast for an image. Bright areas are 
potentially interesting, and algorithms can use them to pick a set of locations completely automatically. 
**** So like eye tracking, the output of the salience method is a series of points to be rendered with 
increased detail. In the upcoming discussion, I ll refer to both fixations and salience points used 
to control detail as DETAIL POINTS. Here again is the design of our system. Eye movements are input 
to the perceptual model.  Uniform detail uses a global weight to control detail across the image. We 
do this for two levels of detail: low and high. Finally, a model of salience developed by Itti and Koch 
determines the visually distinctive content. This model selects a set of low-level image features that 
might catch your attention. Here are the five conditions. We show each subject in our experiment one 
of these pictures. We do this for 50 different photographs.  These five images will let us distinguish 
between different hypothesis regarding how people examine images with modulated style and detail. Specifically, 
we can analyze the effects of style, and of controlling detail both globally and locally. The result 
is data like this. This combines 10 viewers of each image together for 4 of the conditions. We also 
analyzed the data individually for each viewer, but here I ll only talk about results of analysis collapsed 
over viewers. We can see differences in distribution of data across conditions. But how can we quantify 
them?  Our approach is to cluster the data. To divide it into limited regions of the image that were 
viewed coherently. Once we ve cut the data up into clusters, we can compare the number of clusters, 
and the distance from cluster centers to the detail points. One set of detail points is marked here with 
black circles.  When examining the number of clusters, we found that modulating detail holds viewer 
interest moreso than uniform detail. We found that viewers examine 10 to 20 percent fewer locations 
when the detail is modulated. This effect is larger when the detail is meaningful.  When measuring 
the distance from cluster centers to detail points, we can verify that people were in fact looking in 
the right places. This might seem just like what you d expect given what we know artists do. But it wasn 
t clear that this is what we d find. It might have been that these techniques simply cannot capture how 
artists can guide our attention, as there are certainly many other tools that artists use to do this 
same thing.  In summary, we find that in achieving meaningful abstraction, altering the style is not 
enough, globally changing the detail is not enough, and using heuristic measures of importance is not 
enough. We need to locally adapt the detail in a meaningful way. We re also encouraged by the use of 
eye-tracking to evaluate the effectiveness of NPR displays.   In three dimensions, we would like the 
same control over abstraction as in two dimensions, but there are several additional requirements. We'll 
focus on animated and interactive applications, since if we only care about still images, we can just 
use the techniques Doug described. For animation, we need temporal, or frame-to-frame coherence between 
consecutive frames. For interactive applications, we need both temporal coherence and good performance. 
A desirable, though not strictly necessary, quality is flexibility in the types of lines that the method 
can handle. Line density is one of the most important cues for abstraction in line drawings. In the 
image shown, the central building is obviously emphasized while the surrounding buildings are not, and 
the only variation between the two is the line density. However, the basic pipeline described previously 
hasno natural control over screen space line density -lines are drawn wherever the extraction and visibility 
algorithm determine they should be. The simplest way to deal with inordinate line density is to nip 
it in the bud by creating an abstraction of the model itself, such as by smoothing. The lines are then 
extracted from this smooth model, and rendered as normal. So, how do you make an abstraction of a model? 
One way is to create a specialized method tailored to a specific type of model, such as bushy trees. 
In this example, we represent each leaf with a disk, and vary the radius of these disks to control the 
level of abstraction. This method has the nice effect of not only controlling density, but also varying 
the shape of the lines themselves. More generally, we can create a set of simplified meshes using some 
low-pass filtering method. We then interpolate between these meshes at runtime, giving precidence to 
the smooth mesh in areas where low density are desired, and the detail mesh for areas of high density. 
The major issue is that this method is restricted to smooth models that can be effectively filtered without 
losing their character. If you have shapes that cannot be filtered easily, such as these buildings, 
or worse, have a set of lines built in to the model itself (such as the lines on the building in the 
foreground), then its difficult to use a model based strategy. A stroke based strategy can be more general 
because it doesn't depend on changing the model. The density control happens in image space, after the 
lines are extracted and turned into 2D strokes. The source of the lines is not important, be it extracted 
silhouettes and suggestive contours or manually added decorative line, because they all get turned into 
2D strokes. The simplest way to control line density in image space is to keep track of the density 
as each new stroke is drawn. The density function is usually defined as a smoothed version of the drawing 
in progress. When the density reaches a user-specificied threshold, the remaining strokes in that area 
are dropped. The strokes must be ordered somehow by importance, as a random ordering will cause undesired 
results such as in the middle image. Ordering the strokes by an importance metric can improve the results 
by making sure the more important strokes (here, the strokes that lie along large depth discontinuties) 
get drawn first. There are a number of tricks that can be performed in the image domain. One is to simulate 
indication by drawing strokes more densly around the edges of the object. This effect can be achieved 
by first drawing the entire scene and blurring it to get a smooth version. The smooth version is then 
run through an edge detector, which produces an image with smooth dark areas around the edges of the 
shape. This image can then be used as a target density image to locally control the stroke density across 
the image. Unfortunately, dropping strokes at an arbitrary threshold generally leads to terrible temporal 
coherence. Superior results can be achieved by smoothly merging strokes instead of dropping them. Techniques 
to merge dense strokes are generally called stroke simplification techniques. The general idea is to 
identify a set of similar, nearby strokes, merge them into a cluster, and then replace the cluster with 
a single representative stroke. The subtly arises from the problem of choosing a representative stroke. 
Sometimes long, curly strokes (such as the stroke near the lion's shoulder) need to be broken into a 
series of strokes. A naive algorithm for clustering strokes is at least O(n^2), since every stroke needs 
to be compared to every other stroke to find the closest matches. Shesh 2008 proposes to use an advanced 
datastructure called a 1+epsilon deformable spanner, which is essentially a graph structure that allows 
finding all pairs of nearest points in roughly linear time for reasonable graphs. Shesh demonstrates 
the system at interactive framerates with good temporal coherence. However, the implementation of the 
system is somewhat complex due to the use of the spanner structure. The final line density control scheme 
that I will mention is the priority buffer, which is inspired by the item buffer Adam mentioned before. 
The priority buffer test has several important benefits. It has temporal coherence suitable for animation. 
It is stroke based so it handles all types of lines, with the caveat that the priority ordering must 
be well defined. It is also fast and relatively easy to implement, especially if you are already using 
an item buffer to compute line visibility. The priority buffer is a second offscreen buffer, where lines 
are ordered by a priority value rather than depth. We locally vary the width of the lines so that the 
lines are thin where line density should be high, and wide where line density should be low. The effect 
is that in areas of low density, wide, high priority lines cover low priority lines and obscure them. 
We vary the final weight of each line according to its visibility in the priority buffer. So for example, 
the lines on the left here are thin, corresponding to high density in the final rendering, and the lines 
on the right are wide, corresponding to low density. The item buffer and priority buffers are similar, 
but they are necessarily separate. Of course, the ordering of the lines is by depth in the item buffer 
and by priority in the priority buffer. Somewhat more subtly, however, lines in the item buffer should 
be as thin as possible while avoiding rasterization errors. If lines are drawn wide in the item buffer, 
the visibility test will be inaccurate and we will get haloing and other artifacts. The priority buffer 
buffer needs wide lines, but it also needs accurate visibility to function, so we have to perform the 
visibility test first followed by the priority test. This method also has some drawbacks. To achieve 
the temporal coherence, we vary the line weight continuously, but this can look odd for styles such as 
pen and ink. Second, we need to have a good priority ordering. Currently, we use 3D line length for priority: 
that is, a line s priority is proportional to its length in world space, prior to any visibility clipping. 
this function tends to bring out long lines like the rooflines in the right picture. our function can 
have problems where a large number of nearby lines have exactly the same length. you can see the problem 
in dense bars in the middle picture. in this case the priority is effectively random. Of course, our 
priority function can be arbitrary, and we could choose other functions to, for example, prioritize silhouettes 
or lines with high depth discontinuity. For really high quality results the user could actually go in 
and manually set the priority in these kinds of situations. Of course, density is not the only way to 
control abstraction in line drawings. Line style and texture, such as in the Rembrandt sketch, are important. 
Color saturation and intensity, while not strictly line drawing techniques, can also be very effective. 
In the Homer watercolor the region to the right is faded and desaturated, and therefore deemphasized, 
while the boat right next to it is emphasized withbold colors. These are the main rendering effects 
that we use to control abstraction. They can be broken down broadly into color effects and line effects. 
For color, we can fade out the color away from the point of emphasis, desaturate the color, and blur 
the color. We can similarly fade out the lines, change the line texture, and locally adjust the line 
density using the priority buffer. To control all these effects it is helpful to define a single scalar 
value, which we call stylized focus. We can define the focus value pretty much arbitrarily, so long as 
it is relatively smooth. In the 2006 paper, we define four methods: distance from a point in image space 
(2D), distance from a point in world space (3D), distance from a focal plane (meant to simulate camera 
defocus) and segmentation, where each model section gets a constant focus value. This is an example 
of the 2D mode, where the focus value falls off linearly from 1 inside the yellow ring to 0 outside the 
red ring. The same visualization for the 3D, world space method The camera focal place method, where 
the focal plane is placed in the distance. Finally, a simple example of segmentation. Once we have 
created the stylized focus effect, a natural question to ask is how well it actually works. We ran a 
study using the same eye tracking hardware that Doug described earlier. The subjects were shown a range 
of scenes and effects. The scenes were created to isolate individual rendering effects, so for instance 
only colors or only lines, or both color and lines, but where only the lines are emphasized. We then 
compared the eye tracking data for the emphasized versions with unemphasized control images. The results 
show that our emphasis effect does draw the viewers eye in all cases, though the effect can be small. 
For example, the left image shows a strong case, while the right image shows a weak one. The circles 
indicate eye fixations. The effect exists when either lines or color are emphasized alone, though it 
is strongest when both are used together. Composition of the scene is still important the stylized focus 
effect does not prevent the viewer from looking at deemphasized areas, as shown by the right image. 
This image shows an eye fixation on a heavily deemphasized object. We are never going to eliminate fixations 
in deemphasized areas. However, some areas (such as this skyscraper, with lots of line detail) need more 
deemphasis than others. We currently have no way to tell when this is the case. There are various algorithms 
for automatically calculating some kind of saliency map for the image. You could imagine running our 
algorithm, calculating a saliency map from the result, and using that saliency map to determine where 
further deemphasis was required, and then iterating until the emphasized areas had the desired saliency. 
 The color fading effect can resemble fog, and look bad in some cases. A better effect can be gained 
by simplifying the color without fading it out. Basically, we quantize the colors into a couple of desaturated 
buckets. This effect was also proposed by Barla in relation to toon shading. A big limitation of this, 
and most other abstraction approaches we examined, is that If there is a big area of the image without 
much detail cannot be effectively emphasized. Possible solutions to this include using some kind of hatching 
scheme to add line detail even where there is none originally, though hatching of course affects the 
tone of the image. This situation may also not arise very often, because it is rare that we want to draw 
attention to a big, blank area of the scene.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation</funding_agency>
			<grant_numbers>
				<grant_number>HLC 0308121CCF 0347427CCF 05041185</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
	<section>
		<section_id>1401189</section_id>
		<sort_key>570</sort_key>
		<section_seq_no>18</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Complexity and Accessibility: Massive model visualization techniques]]></section_title>
		<section_page_from>18</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098731</person_id>
				<author_profile_id><![CDATA[81100541084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kasik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098732</person_id>
				<author_profile_id><![CDATA[81100128205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dietrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098733</person_id>
				<author_profile_id><![CDATA[81100044134]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Enrico]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gobbetti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098734</person_id>
				<author_profile_id><![CDATA[81100207265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098735</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098736</person_id>
				<author_profile_id><![CDATA[81100159926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slusallek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098737</person_id>
				<author_profile_id><![CDATA[81365596570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Abe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stephens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098738</person_id>
				<author_profile_id><![CDATA[81100019061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Sung-Eui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401190</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<pages>188</pages>
		<display_no>40</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Massive model visualization techniques]]></title>
		<subtitle><![CDATA[course notes]]></subtitle>
		<page_from>1</page_from>
		<page_to>188</page_to>
		<doi_number>10.1145/1401132.1401190</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401190</url>
		<abstract>
			<par><![CDATA[<p>Interactive display and visualization of large geometric and textured models is becoming a fundamental capability. There are numerous application areas, including games, movies, CAD, virtual prototyping, and scientific visualization. One of observations about geometric models used in interactive applications is that their model complexity continues to increase because of fundamental advances in 3D modeling, simulation, and data capture technologies. As computing power increases, users take advantage of the algorithmic advances and generate even more complex models and datasets. Therefore, there are many cases where we are required to visualize massive models that consist of hundreds of millions of triangles and, even, billions of triangles. However, interactive visualization and handling of such massive models still remains a challenge in computer graphics and visualization. In this monograph we discuss various techniques that enable interactive visualization of massive models. These techniques include visibility computation, simplification, levels-of-detail, and cache-coherent data management. We believe that the combinations of these techniques can make it possible to interactively visualize massive models in commodity hardware.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098739</person_id>
				<author_profile_id><![CDATA[81100541084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kasik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Boeing Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098740</person_id>
				<author_profile_id><![CDATA[81100128205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dietrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098741</person_id>
				<author_profile_id><![CDATA[81100044134]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Enrico]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gobbetti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CRS4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098742</person_id>
				<author_profile_id><![CDATA[81100207265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CRS4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098743</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098744</person_id>
				<author_profile_id><![CDATA[81100159926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slusallek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saarland University in Trace GmbH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098745</person_id>
				<author_profile_id><![CDATA[81365596570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Abe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stephens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098746</person_id>
				<author_profile_id><![CDATA[81100019061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Sung-Eui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{ABCO&#60;sup&#62;+&#60;/sup&#62;01} Marc Alexa, Johannes Behr, Daniel Cohen-Or, Shachar Fleishman, David Levin, and Cl&#225;udio T. Silva. Point Set Surfaces. In <i>Proceedings of IEEE Visualization 2001</i>, pages 21--28, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{ABD&#60;sup&#62;+&#60;/sup&#62;03} S. Alstrup, M. A. Bende, E. D. Demaine, M. Farach-Colton, T. Rauhe, and M. Thorup. Efficient tree layout in a multilevel memory hierarchy. <i>Computing Research Repository (CoRR)</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{ABF04} L. Arge, G. Brodal, and R. Fagerberg. Cache oblivious data structures. <i>Handbook on Data Structures and Applications</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300554</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{ACW&#60;sup&#62;+&#60;/sup&#62;99} D. Aliaga, J. Cohen, A. Wilson, H. Zhang, C. Erikson, K. Hoff, T. Hudson, W. Stuerzlinger, E. Baker, R. Bastos, M. Whitton, F. Brooks, and D. Manocha. MMR: An integrated massive model rendering system using geometric and image-based acceleration. In <i>Proc. of ACM Symposium on Interactive 3D Graphics</i>, pages 199--206, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267104</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{AL97} Daniel G. Aliaga and Anselmo A. Lastra. Architectural Walkthroughs Using Portal Textures. In <i>Proceedings of IEEE Visualization 1997</i>, pages 355--362, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253800</ref_obj_id>
				<ref_obj_pid>253607</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{AM96} R. Abarbanel and W. McNeely. Flythru the boeing 777. <i>ACM SIGGRAPH Visual Proceeding</i>, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360537</ref_obj_id>
				<ref_obj_pid>360529</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{AM00} U. Assarsson and T. M&#246;ller. Optimized view frustum culling algorithms for bounding boxes. <i>Journal of Graphic Tools</i>, 5, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Ame07} The american heritage dictionary of the english language. <i>Houghton Mifflin Company</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{AMH02} Tomas Akenine-M&#246;ller and Eric Haines. <i>Real-Time Rendering, Second Edition</i>. A K Peters, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{App68} Arthur Appel. Some techniques for shading machine renderings of solids. In <i>AFIPS 1968 Spring Joint Computer Conf.</i>, volume 32, pages 37--45, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91416</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{ARB90} John M. Airey, John H. Rohlf, and Frederick P. Brooks, Jr. Towards image realism with interactive update rates in complex virtual building environments. <i>Computer Graphics (1990 Symposium on Interactive 3D Graphics)</i>, 24(2):41--50, March 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>314475</ref_obj_id>
				<ref_obj_pid>314464</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{AS94} Pankaj K. Agarwal and Subhash Suri. Surface approximation and geometric partitions. In <i>Proc. 5th ACM-SIAM Sympos. Discrete Algorithms</i>, page 24?33, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{ASNB00} C. Andujar, C. Saona, I. Navazo, and P. Brunet. Integrating occlusion culling and levels of detail through hardly-visible sets. <i>Computer Graphics Forum</i>, 19(3):499--506, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{ASVN00} C. And&#250;jar, C. Saona-V&#225;zquez, and I. Navazo. Lod visibility culling and occluder synthesis. <i>Computer-Aided Design</i>, 32(13):773--783, October 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>48535</ref_obj_id>
				<ref_obj_pid>48529</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{AV88} A. Aggarwal and J. S. Vitter. The input/output complexity of sorting and related problems. <i>Communications of the ACM</i>, 31:1116--1127, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1143112</ref_obj_id>
				<ref_obj_pid>1143079</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{BD06} Lionel Baboud and Xavier D&#233;coret. Rendering geometry with relief textures. In Carl Gutwin and Stephen Mann, editors, <i>Graphics Interface</i>, pages 195--201. Canadian Human-Computer Communications Society, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{BG02} A. Bogomjakov and C. Gotsman. Universal rendering sequences for transparent vertex caching of progressive meshes. In <i>Computer Graphics Forum</i>, pages 137--148, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{BGM&#60;sup&#62;+&#60;/sup&#62;07} L. Borgeat, G. Godin, P. Massicotte, G. Poirier, F. Blais, and J. Beraldin. Analyzing large multi-scale datasets: The case of the virtual mona lisa. <i>IEEE Computer Graphics and Applications</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1229396</ref_obj_id>
				<ref_obj_pid>1229390</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{BGMP07} Fabio Bettio, Enrico Gobbetti, Fabio Marton, and Giovanni Pintore. High-quality networked terrain rendering from compressed bitstreams. In <i>Proc. ACM Web3D International Symposium</i>, pages 37--44. ACM Press, New York, NY, USA, April 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>792949</ref_obj_id>
				<ref_obj_pid>792757</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{BHS98} J. Bittner, V. Havran, and P. Slavik. Hierarchical visibility culling with occlusion trees. In <i>Proceedings of computer Graphics International '98</i>, pages 207--219, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Bit02} Jiri Bittner. <i>Hierarchical Techniques for Visibility Computations</i>. Ph.d. thesis, Department of Computer Science and Engineering, Faculty of Electrical Engineering, Czech Technical University in Prague, October 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897846</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Bro92} F. Brooks. Walkthrough project: Final technical report to national science foundation computer and information science and engineering. Technical report, University of North Carolina-Chapel Hill, Computer Science, TR92-026, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1318757</ref_obj_id>
				<ref_obj_pid>1318709</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Bru07} B. Bruderin. Interviews3d - a platform for interactive handling of massive data sets. <i>IEEE Computer Graphics and Applications</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581922</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{BSGM02} B. Baxter, A. Sud, N. Govindaraju, and D. Manocha. GigaWalk: Interactive walkthrough of complex 3D environments. <i>Proc. of Eurographics Workshop on Rendering</i>, pages 203--214, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>504959</ref_obj_id>
				<ref_obj_pid>504949</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Bux02} B. Buxton. Less is more (more or less), in the invisible future: The seamless integration of technology into everyday life. <i>McGraw-Hill</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{BW03} Jiri Bittner and Peter Wonka. Visibility in computer graphics. <i>Journal of Environment and Planning B: Planning and Design</i>, 30(5):729--756, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{BWPP04} Jiri Bittner, Michael Wimmer, Harald Piringer, and Werner Purgathofer. Coherent hierarchical culling: Hardware occlusion queries made useful. <i>Computer Graphics Forum</i>, 23(3):615--624, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>883446</ref_obj_id>
				<ref_obj_pid>882473</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{BWW01} Jiri Bittner, Peter Wonka, and Michael Wimmer. Visibility preprocessing for urban scenes using line space subdivision. In <i>PG '01: Proceedings of the 9th Pacific Conference on Computer Graphics and Applications</i>, page 276, Washington, DC, USA, 2001. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Cat74} Edwin E. Catmull. <i>A Subdivision Algorithm for Computer Display of Curved Surfaces</i>. PhD thesis, Dept. of CS, U. of Utah, December 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1779191</ref_obj_id>
				<ref_obj_pid>1779178</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{CBWR07} Jean Pierre Charalambos, Jiri Bittner, Michael Wimmer, and Eduardo Romero. Optimized hlod refinement driven by hardware occlusion queries. In <i>Advances in Visual Computing (Third International Symposium on Computer Vision - ISVC 2007)</i>, Lecture Notes in Computer Science, volume 4841, pages 106--117. Springer, November 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{CC78&gt; E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. <i>Computer-Aided Design</i>, 10:350--355, September 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{CCMS97} A. Ciampalini, P. Cignoni, C. Montani, and R. Scopigno. Multiresolution decimation based on global error. <i>The Visual Computer</i>, 13(5):228--246, June 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{CDBG&#60;sup&#62;+&#60;/sup&#62;07} Paolo Cignoni, Marco Di Benedetto, Fabio Ganovelli, Enrico Gobbetti, Fabio Marton, and Roberto Scopigno. Ray-Casted BlockMaps for Large Urban Models Visualization. In <i>Computer Graphics Forum (Proceedings of Eurographics)</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{CGG&#60;sup&#62;+&#60;/sup&#62;03a} Paolo Cignoni, Fabio Ganovelli, Enrico Gobbetti, Fabio Marton, Federico Ponchio, and Roberto Scopigno. BDAM - batched dynamic adaptive meshes for high performance terrain visualization. <i>Computer Graphics Forum</i>, 22(3):505--514, September 2003. Proc. Eurographics 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{CGG&#60;sup&#62;+&#60;/sup&#62;03b} Paolo Cignoni, Fabio Ganovelli, Enrico Gobbetti, Fabio Marton, Federico Ponchio, and Roberto Scopigno. Planet-sized batched dynamic adaptive meshes (pbdam). In <i>Proceedings IEEE Visualization</i>, pages 147--155, Conference held in Seattle, WA, USA, October 2003. IEEE Computer Society Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015802</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{CGG&#60;sup&#62;+&#60;/sup&#62;04} Paolo Cignoni, Fabio Ganovelli, Enrico Gobbetti, Fabio Marton, Federico Ponchio, and Roberto Scopigno. Adaptive tetrapuzzles: efficient out-of-core construction and visualization of gigantic multiresolution polygonal models. <i>ACM Transactions on Graphics</i>, 23(3):796--803, August 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{CGG&#60;sup&#62;+&#60;/sup&#62;05} Paolo Cignoni, Fabio Ganovelli, Enrico Gobbetti, Fabio Marton, Federico Ponchio, and Roberto Scopigno. Batched multi triangulation. In <i>Proceedings IEEE Visualization</i>, pages 207--214, Conference held in Minneapolis, MI, USA, October 2005. IEEE Computer Society Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218395</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{Che95} Shenchang Eric Chen. Quicktime VR - an image-based approach to virtual environment navigation. In Robert Cook, editor, <i>SIGGRAPH 95 Conference Proceedings</i>, Annual Conference Series, pages 29--38. ACM SIGGRAPH, Addison Wesley, August 1995. held in Los Angeles, California, 06-11 August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081409</ref_obj_id>
				<ref_obj_pid>1081407</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{CKS03} Wagner T. Corr&#234;a, James T. Klosowski, and Cl&#225;udio T. Silva. Visibility-based prefetching for interactive out-of-core rendering. In <i>Proceedings of PVG 2003 (6th IEEE Symposium on Parallel and Large-Data Visualization and Graphics)</i>, pages 1--8, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{Cla76} J. H. Clark. Hierarchical geometric models for visible surface algorithms. <i>Communications of the ACM</i>, 19(10):547--554, October 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>207162</ref_obj_id>
				<ref_obj_pid>207110</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{CM95} S. Coleman and K. McKinley. Tile size selection using cache organization and data layout. <i>SIGPLAN Conference on Programming Language Design and Implementation</i>, pages 279--290, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>940008</ref_obj_id>
				<ref_obj_pid>939836</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{CMRS03} P. Cignoni, C. Montani, C. Rocchini, and R. Scopigno. External memory management and simplification of huge meshes. In <i>IEEE Transaction on Visualization and Computer Graphics</i>, pages 525--537, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231896</ref_obj_id>
				<ref_obj_pid>2231876</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{COCSD03} Daniel Cohen-Or, Yiorgos L. Chrysanthou, Cl&#225;udio T. Silva, and Fr&#233;do Durand. A survey of visibility for walkthrough applications. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 9(3):412--431, July/September 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{COFHZ98} Daniel Cohen-Or, Gadi Fibich, Dan Halperin, and Eyal Zadicario. Conservative visibility and strong occlusion for viewspace partitioning of densely occluded scenes. <i>Computer Graphics Forum</i>, 17(3):243--254, 1998. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{Coo84} R. L. Cook. Shade trees. <i>Computer Graphics (SIGGRAPH 84 Proceedings)</i>, 18(3):223--231, July 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{COZ98} Daniel Cohen-Or and Eyal Zadicario. Visibility streaming for network-based walkthroughs. In <i>Graphics Interface '98</i>, pages 1--7, June 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015817</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{CSAD04} David Cohen-Steiner, Pierre Alliez, and Mathieu Desbrun. Variational shape approximation. <i>ACM Transactions on Graphics</i>, 23(3):905--914, August 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237242</ref_obj_id>
				<ref_obj_pid>237218</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{CT96} S. Coorg and S. Teller. Temporally coherent conservative visibility. In <i>Proc. 12th Annu. ACM Symp. Comp. Geom.</i>, pages 78--87, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253312</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{CT97} S. Coorg and S. Teller. Real-time occlusion culling for models with large occluders. In <i>1997 Symposium on Interactive 3D Graphics</i>, pages 83--90, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237220</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{CVM&#60;sup&#62;+&#60;/sup&#62;96} Jonathan Cohen, Amitabh Varshney, Dinesh Manocha, Greg Turk, Hans Weber, Pankaj Agarwal, Frederick P. Brooks, Jr., and William Wright. Simplification envelopes. In Holly Rushmeier, editor, <i>SIGGRAPH 96 Conference Proceedings</i>, Annual Conference Series, pages 119--128. ACM SIGGRAPH, Addison Wesley, August 1996. held in New Orleans, Louisiana, 04-09 August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566618</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{DD02} Florent Duguet and George Drettakis. Robust epsilon visibility. In John Hughes, editor, <i>Proceedings of ACM SIGGRAPH 2002</i>. ACM Press / ACM SIGGRAPH, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344891</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{DDTP00} F. Durand, G. Drettakis, J. Thollot, and C. Puech. Conservative visibility preprocessing using extended projections. In <i>SIGGRAPH 00 Conference Proceedings</i>, pages 239--248, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{Dee95} Michael F. Deering. Geometry compression. In <i>ACM SIGGRAPH</i>, pages 13--20, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1252491</ref_obj_id>
				<ref_obj_pid>1251973</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{DGBGP05} Pablo Diaz-Gutierrez, Anusheel Bhushan, M. Gopi, and Renato Pajarola. Constrained strip generation and management for efficient interactive 3d rendering. In <i>Computer Graphics International</i>, pages 115--121, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{dL04} Rodrigo de Toledo and Bruno Levi. Extending the graphic pipeline with new GPU-accelerated primitives. In <i>Proc. 24th gOcad Meeting</i>, Nancy, France, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1779246</ref_obj_id>
				<ref_obj_pid>1779178</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{dLP07} Rodrigo de Toledo, Bruno Levy, and Jean-Claude Paul. Iterative methods for visualization of implicit surfaces on gpu. In <i>ISVC, International Symposium on Visual Computing</i>, Lecture Notes in Computer Science, Lake Tahoe, Nevada/California, November 2007. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>568523</ref_obj_id>
				<ref_obj_pid>568522</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{DPS02} J. Diaz, J. Petit, and M. Serna. A survey of graph layout problems. <i>ACM Computing Surveys</i>, 34(3):313--356, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{DS78} D. Doo and M. Sabin. Behaviour of recursive division surfaces near extraordinary points. <i>Computer-Aided Design</i>, 10:356--360, September 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{DSSD99} Xavier Decoret, Fran&#231;ois Sillion, Gernot Schaufler, and Julie Dorsey. Multilayered impostors for accelerated rendering. <i>Computer Graphics Forum</i>, 18(3):61--73, September 1999. ISSN 1067-7055.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1318755</ref_obj_id>
				<ref_obj_pid>1318709</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{DSW07} A. Dietrich, A. Stephens, and I. Wald. Exploring a boeing 777: Ray tracing largescale cad data. <i>IEEE Computer Graphics and Applications</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{Dur99} F. Durand. <i>3D Visibility: Analytical study and Applications</i>. PhD thesis, Universite Joseph Fourier, Grenoble, France, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>267028</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{DWS&#60;sup&#62;+&#60;/sup&#62;97} M. Duchaineau, M. Wolinsky, D. E. Sigeti, M. C. Miller, C. Aldrich, and M. B. Mineev-Weinstein. ROAMing Terrain: Real-time Optimally Adapting Meshes. In <i>Proc. IEEE Visualization</i>, pages 81--88, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300532</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{EM99} C. Erikson and D. Manocha. GAPS: General and automatic polygon simplification. In <i>Proc. of ACM Symposium on Interactive 3D Graphics</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364376</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{EMB01} C. Erikson, D. Manocha, and B. Baxter. Hlods for fast display of large static and dynamic environments. <i>Proc. of ACM Symposium on Interactive 3D Graphics</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{FKST96} T. A. Funkhouser, D. Khorramabadi, C. H. Sequin, and S. Teller. The ucb system for interactive visualization of large architectural models. <i>Presence</i>, 5(1):13--44, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>796479</ref_obj_id>
				<ref_obj_pid>795665</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{FLPR99} M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In <i>Foundations of Computer Science</i>, pages 285--297, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288222</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{FMP98} Leila De Floriani, Paola Magillo, and Enrico Puppo. Efficient Implementation of Multi-Triangulations. In <i>Proceedings of IEEE Visualization 1998</i>, pages 43--50, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{FNB03} M. Franquesa-Niubo and P. Brunet. Collision prediction using mktrees. <i>Proc. CEIG</i>, pages 217--232, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>338066</ref_obj_id>
				<ref_obj_pid>338029</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{FTW00} P. Fishburn, P. Tetali, and P. Winkler. Optimal linear arrangement of a rectangular grid. <i>Discrete Mathematics</i>, 213(1):123--139, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{GD98} J. P. Grossman and William J. Dally. Point Sample Rendering. In <i>Rendering Techniques 1998 (Proceedings of the Eurographics Workshop on Rendering)</i>, pages 181--192, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{GGH02} Xianfeng Gu, Steven J. Gortler, and Hugues Hoppe. Geometry Images. In <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 335--361, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{GGSC96} Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The lumigraph. In <i>Proceedings of SIGGRAPH 96</i>, Computer Graphics Proceedings, Annual Conference Series, pages 43--54, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258849</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{GH97} Michael Garland and Paul S. Heckbert. Surface simplification using quadric error metrics. In <i>Proceedings of SIGGRAPH 97</i>, Computer Graphics Proceedings, Annual Conference Series, pages 209--216, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288280</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{GH98} Michael Garland and Paul S. Heckbert. Simplifying surfaces with color and texture using quadric error metrics. In <i>IEEE Visualization '98</i>, pages 263--270, October 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>330876</ref_obj_id>
				<ref_obj_pid>330873</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{GI99} Joseph Gil and Alon Itai. How to pack trees. <i>Journal of Algorithms</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{Gib50} J. Gibson. The perception of the visual world. 1950.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{GJS76} M. Garey, D. Johnson, and L. Stockmeyer. Some simplified np-complete graph problems. <i>Theoretical Computer Science 1</i>, pages 237--267, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166147</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{GKM93} Ned Greene, Michael Kass, and Gavin Miller. Hierarchical Z-Buffer Visibility. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 231--238, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319213</ref_obj_id>
				<ref_obj_pid>2318936</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{GL96} C. Gotsman and M. Lindenbaum. On the metric properties of discrete space-filling curves. <i>IEEE Transactions on Image Processing</i>, 5(5):794--797, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1652364</ref_obj_id>
				<ref_obj_pid>1652312</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{GM04} Enrico Gobbetti and Fabio Marton. Layered point clouds: a simple and efficient multiresolution structure for distributing and rendering gigantic point-sampled models. <i>Computers &amp; Graphics</i>, 28(6):815--826, December 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073277</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{GM05} Enrico Gobbetti and Fabio Marton. Far Voxels - a multiresolution framework for interactive rendering of huge complex 3d models on commodity graphics platforms. <i>ACM Transactions on Graphics</i>, 24(3):878--885, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{GMC&#60;sup&#62;+&#60;/sup&#62;06} Enrico Gobbetti, Fabio Marton, Paolo Cignoni, Marco Di Benedetto, and Fabio Ganovelli. C-BDAM - compressed batched dynamic adaptive meshes for terrain rendering. <i>Computer Graphics Forum</i>, 25(3):333--342, September 2006. Proc. Eurographics 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>802343</ref_obj_id>
				<ref_obj_pid>800073</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{Gol81} R Goldstein. Defining the bounding edges of a synthavision solid model. In <i>18th Conference on Design Automation</i>, pages 457--461, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{GP07} Markus Gross and Hans-Peter Pfister, editors. <i>Point-based Graphics</i>. Elsevier Sciences Ltd., 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{GSF99} Craig Gotsman, Oded Sudarsky, and Jeffrey A. Fayman. Optimized occlusion culling using five-dimensional subdivision. <i>Computers &amp; Graphics</i>, 23(5):645--654, October 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641501</ref_obj_id>
				<ref_obj_pid>641480</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{GSYM03} Naga K. Govindaraju, Avneesh Sud, Sung-Eui Yoon, and Dinesh Manocha. Interactive visibility culling in complex environments using occlusion-switches. In <i>2003 ACM Symposium on Interactive 3D Graphics</i>, pages 103--112, April 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1061350</ref_obj_id>
				<ref_obj_pid>1061347</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{GZ05} Michael Garland and Yuan Zhou. Quadric-based simplification in any dimension. <i>ACM Transactions on Graphics</i>, 24(2):209--239, April 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{Hav00} Vlastimil Havran. <i>Heuristic Ray Shooting Algorithms</i>. Ph.d. thesis, Department of Computer Science and Engineering, Faculty of Electrical Engineering, Czech Technical University in Prague, November 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{HDD&#60;sup&#62;+&#60;/sup&#62;93} Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. Mesh optimization. In James T. Kajiya, editor, <i>Computer Graphics (SIGGRAPH '93 Proceedings)</i>, volume 27, pages 19--26, August 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{HHS06} Vlastimil Havran, Robert Herzog, and Hans-Peter Seidel. On the fast construction of spatial data structures for ray tracing. In <i>Proceedings of IEEE Symposium on Interactive Ray Tracing 2006</i>, pages 71--80, September 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>262847</ref_obj_id>
				<ref_obj_pid>262839</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{HMC&#60;sup&#62;+&#60;/sup&#62;97} T. Hudson, D. Manocha, J. Cohem, M. Lin, K. Hoff, and H. Zhang. Accelerated occlusion culling using shadow frusta. In <i>Proc. 13th Annu. ACM Sympos. Comput. Geom.</i>, pages 1--10, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383685</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{HMN05} Denis Haumont, Otso Makinen, and Shaun Nirenstein. A low dimensional framework for exact polygon-to-polygon occlusion queries. In Oliver Deussen, Alexander Keller, Kavita Bala, Philip Dutr?, Dieter W. Fellner, and Stephen N. Spencer, editors, <i>Rendering Techniques</i>, pages 211--222. Eurographics Association, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{HMS06} Warren Hunt, William R. Mark, and Gordon Stoll. Fast kd-tree construction with an adaptive error-bounded heuristic. In <i>2006 IEEE Symposium on Interactive Ray Tracing</i>. IEEE, Sept. 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237216</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{Hop96} Hugues Hoppe. Progressive meshes. In Holly Rushmeier, editor, <i>SIGGRAPH 96 Conference Proceedings</i>, Annual Conference Series, pages 99--108. ACM SIGGRAPH, Addison Wesley, August 1996. held in New Orleans, Louisiana, 04-09 August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>288221</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{Hop98} H. Hoppe. Smooth view-dependent level-of-detail control and its aplications to terrain rendering. In <i>IEEE Visualization '98 Conf.</i>, pages 35--42, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311565</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{Hop99a} H. Hoppe. Optimization of mesh locality for transparent vertex caching. <i>ACM SIGGRAPH</i>, pages 269--276, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319357</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[{Hop99b} Hugues H. Hoppe. New quadric metric for simplifying meshes with appearance attributes. In <i>IEEE Visualization '99</i>, pages 59--66, October 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[{HPB05} Mathias Heyer, Sebastian Pf&#252;tzer, and Beat Br&#252;derlin. Visualization Server for Very Large Virual Reality Scenes. In <i>4. Paderborner Workshop Augmented &amp; Virtual Reality in der Produktentstehung</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882366</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[{IG03} M. Isenburg and S. Gumhold. Out-of-core compression for gigantic polygon meshes. In <i>ACM Trans. on Graphics (Proc. of ACM SIGGRAPH)</i>, volume 22, pages 935--942, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{IL05} Martin Isenburg and Peter Lindstrom. Streaming meshes. <i>IEEE Visualization</i>, pages 231--238, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[{ILGS03a} M. Isenburg, P. Lindstrom, S. Gumhold, and J. Snoeyink. Large mesh simplification using processing sequences. <i>IEEE Visualization</i>, pages 465--472, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[{ILGS03b} Martin Isenburg, Peter Lindstrom, Stefan Gumhold, and Jack Snoeyink. Large Mesh Simplification using Processing Sequences. In <i>Proceedings of IEEE Visualization 2003</i>, pages 465--472, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581922</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[{ISGM02} William V. Baxter III, Avneesh Sud, Naga K. Govindaraju, and Dinesh Manocha. Gigawalk: Interactive walkthrough of complex environments. In <i>Rendering Techniques 2002: 13th Eurographics Workshop on Rendering</i>, pages 203--214, June 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>133261</ref_obj_id>
				<ref_obj_pid>133257</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[{JM92} M. Juvan and B. Mohar. Optimal linear labelings and eigenvalues of graphs. <i>Discrete Applied Mathematics</i>, 36(2):153--168, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581920</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[{JW02} Stefan Jeschke and Michael Wimmer. Textured depth meshes for realtime rendering of arbitrary scenes. In Simon Gibson and Paul Debevec, editors, <i>Proceedings of the 13th Eurographics Workshop on Rendering (RENDERING TECHNIQUES-02)</i>, pages 181--190, Aire-la-Ville, Switzerland, June 26--28 2002. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[{JWS02} Stefan Jeschke, Michael Wimmer, and Heidrun Schumann. Layered environmentmap impostors for arbitrary scenes. In <i>Graphics Interface</i>, pages 1--8, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1436763</ref_obj_id>
				<ref_obj_pid>1435660</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[{Kas04} D. Kasik. Strategies for consistent image partitioning. <i>IEEE Multimedia</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602153</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[{KBG02} Z. Karni, A. Bogomjakov, and C. Gotsman. Efficient compression and rendering of multi-resolution meshes. In <i>IEEE Visualization</i>, pages 347--54, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732124</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[{KCCO00} Vladlen Koltun, Yiorgos Chrysanthou, and Daniel Cohen-Or. Virtual occluders: An efficient intermediate pvs representation. In <i>Rendering Techniques 2000: 11th Eurographics Workshop on Rendering</i>, pages 59--70, June 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>760611</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[{KCCO01} Vladlen Koltun, Yiorgos Chrysanthou, and Daniel Cohen-Or. Hardware-accelerated from-region visibility using a dual ray space. In <i>Proceedings of the 12th Eurographics Workshop on Rendering Techniques</i>, pages 205--216, London, UK, 2001. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>287111</ref_obj_id>
				<ref_obj_pid>287098</ref_obj_pid>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[{KK98} G. Karypis and V. Kumar. Multilevel k-way partitioning scheme for irregular graphs. <i>Journal of Parallel and Distributed Computing</i>, pages 96--129, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>302408</ref_obj_id>
				<ref_obj_pid>302405</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[{KKF99} D. Kasik, C. Kimball, and J. Felt K. Frazier. A flexible approach to alliances of complex applications. <i>International Conference on Software Engineering</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1236293</ref_obj_id>
				<ref_obj_pid>1236246</ref_obj_pid>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[{KKM07} Adarsh Krishnamurthy, Rahul Khardekar, and Sara McMains. Direct evaluation of nurbs curves and surfaces on the gpu. In <i>SPM '07: Proceedings of the 2007 ACM symposium on Solid and physical modeling</i>, pages 329--334, New York, NY, USA, 2007. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245624</ref_obj_id>
				<ref_obj_pid>244979</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[{KLS96} Reinhard Klein, Gunther Liebich, and Wolfgang Straer. Mesh reduction with error control. In <i>IEEE Visualization '96</i>. IEEE, October 1996. ISBN 0-89791-864-9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[{KMGL99} S. Kumar, D. Manocha, W. Garret, and M. Lin. Hierarchical back-face computation. <i>Computer and Graphics</i>, 25(5):681--692, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614454</ref_obj_id>
				<ref_obj_pid>614278</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[{KS00} James T. Klosowski and Cl&#225;udio T. Silva. The Prioritized-Layered Projection Algorithm for Visible Set Estimation. In <i>IEEE Transaction on Visualization and Computer Graphics</i>, pages 108--123, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614506</ref_obj_id>
				<ref_obj_pid>614284</ref_obj_pid>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[{KS01} James T. Klosowski and Cl&#225;udio T. Silva. Efficient conservative visibility culling using the prioritized-layered projection algorithm. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 7(4):365--379, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141939</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[{LB06} Charles Loop and Jim Blinn. Real-time gpu rendering of piecewise algebraic surfaces. <i>ACM Transactions on Graphics</i>, 25(3):664--670, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1117411</ref_obj_id>
				<ref_obj_pid>1117410</ref_obj_pid>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[{LCCO06} Alon Lerner, Yiorgos Chrysanthou, and Daniel Cohen-Or. Efficient cells-and-portals partitioning: Research articles. <i>Comput. Animat. Virtual Worlds</i>, 17(1):21--40, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>199422</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[{LG95} D. P. Luebke and C. Georges. Portals and mirrors: Simple, fast evaluation of potentially visible sets. In <i>Proc. Symp. Interactive 3-D Graphics</i>, pages 105--106, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[{LH96} Marc Levoy and Pat Hanrahan. Light field rendering. In <i>SIGGRAPH 96 Conference Proceedings</i>, pages 31--42, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344912</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[{Lin00} Peter Lindstrom. Out-of-core simplification of large polygonal models. In <i>Proceedings of ACM SIGGRAPH 2000</i>, Computer Graphics Proceedings, Annual Conference Series, pages 259--262, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641500</ref_obj_id>
				<ref_obj_pid>641480</ref_obj_pid>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[{Lin03} Peter Lindstrom. Out-of-core construction and visualization of multiresolution surfaces. In <i>ACM 2003 Symposium on Interactive 3D Graphics</i>, pages 93--102, 239, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807465</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[{Lip80} A. Lippman. Movie-maps: An application of the optical videodisc to computer graphics. <i>Computer Graphics (SIGGRAPH ?80 Proceedings)</i>, 14(3):32?42, July 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601729</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[{LP01} Peter Lindstrom and V. Pascucci. Visualization of large terrains made easy. <i>IEEE Visualization</i>, pages 363--370, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>863276</ref_obj_id>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[{LRC&#60;sup&#62;+&#60;/sup&#62;02} Davide Luebke, Martin Reddy, Jonathan Cohen, Abitabh Varshney, Benjamin Watson, and Robert Huebner. <i>Level of detail for 3D graphics: applications and theory</i>. Morgan Kaufmann, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882313</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[{LSCO03} Tommer Leyvand, Olga Sorkine, and Daniel Cohen-Or. Ray space factorization for from-region visibility. <i>ACM Transactions on Graphics</i>, 22(3):595--604, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618821</ref_obj_id>
				<ref_obj_pid>616070</ref_obj_pid>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[{Lue01} David P. Luebke. A Developer's Survey of Polygonal Simplification Algorithms. <i>IEEE Computer Graphics and Applications</i>, 21(3):24--35, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>129</ref_seq_no>
				<ref_text><![CDATA[{LV03} P. Lyman and H. Varian. How much information? http://www2.sims.berkeley.edu/research/projects/how-much-info/., 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>130</ref_seq_no>
				<ref_text><![CDATA[{LW85} Marc Levoy and Turner Whitted. The Use of Points as a Display Primitive. Technical Report TR 85-022, University of North Carolina at Chapel Hill, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>131</ref_seq_no>
				<ref_text><![CDATA[{LYTM06} Christian Lauterbach, Sung-Eui Yoon, David Tuft, and Dinesh Manocha. Rtdeform: Interactive ray tracing of dynamic scenes using bvhs. In <i>Proceedings of IEEE Symposium on Interactive Ray Tracing 2006</i>, September 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>132</ref_seq_no>
				<ref_text><![CDATA[{MAM05} Fr&#233;d&#233;ric Mora, Lilian Aveneau, and Michel M&#233;riaux. Coherent and exact polygon-to-polygon visibility. In <i>Proc. WSCG</i>, pages 87--94, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218398</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>133</ref_seq_no>
				<ref_text><![CDATA[{MB95} Leonard McMillan and Gary Bishop. Plenoptic Modeling: An Image-Based Rendering System. In <i>ACM Computer Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 39--46, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383919</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>134</ref_seq_no>
				<ref_text><![CDATA[{MBW06} Oliver Mattausch, Jiri Bittner, and Michael Wimmer. Adaptive visibility-driven view cell construction. In <i>Rendering Techniques 2006: 17th Eurographics Workshop on Rendering</i>, pages 195--206, June 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1268571</ref_obj_id>
				<ref_obj_pid>1268517</ref_obj_pid>
				<ref_seq_no>135</ref_seq_no>
				<ref_text><![CDATA[{MBWW07} Oliver Mattausch, Jiri Bittner, Peter Wonka, and Michael Wimmer. Optimized subdivisions for preprocessed visibility. In <i>Graphics Interface 2007</i>, pages 335--342, May 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>136</ref_seq_no>
				<ref_text><![CDATA[{Moo65} G. Moore. Cramming more components onto int:spegrated circuits. <i>Electronics maganize</i>, 1965.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>137</ref_seq_no>
				<ref_text><![CDATA[{Moo91} A. Moore. <i>A Tutorial on kd-trees</i>. PhD thesis, University of Cambridge, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>138</ref_seq_no>
				<ref_text><![CDATA[{MP80} D. Muradyan and T. Piliposyan. Minimal numberings of vertices of a rectangular lattice. In <i>Akad. Nauk. Arimjan</i>, pages 21--27, 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383562</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>139</ref_seq_no>
				<ref_text><![CDATA[{NB04} Shaun Nirenstein and Edwin Blake. Hardware accelerated visibility preprocessing using adaptive sampling. In <i>Rendering Techniques 2004: 15th Eurographics Workshop on Rendering</i>, pages 207--216, June 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581921</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>140</ref_seq_no>
				<ref_text><![CDATA[{NBG02} S. Nirenstein, E. Blake, and J. Gain. Exact from-region visibility culling. In <i>EGRW '02: Proceedings of the 13th Eurographics workshop on Rendering</i>, pages 191--202, Aire-la-Ville, Switzerland, Switzerland, 2002. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>141</ref_seq_no>
				<ref_text><![CDATA[{ND05} M. A. Sabin N. Dodgson, M. S. Floater, editor. <i>Advances in Multiresolution for Geometric Modelling</i>. Springer, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>142</ref_seq_no>
				<ref_text><![CDATA[{NFLYCO99} Boaz Nadler, Gadi Fibich, Shuly Lev-Yehudi, and Daniel Cohen-Or. A qualitative and quantitative visibility analysis in urban scenes. <i>Computers &amp; Graphics</i>, 23(5):655--666, October 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>740945</ref_obj_id>
				<ref_obj_pid>647898</ref_obj_pid>
				<ref_seq_no>143</ref_seq_no>
				<ref_text><![CDATA[{NRS97} R. Niedermeier, K. Reinhardt, and P. Sanders. Towards optimal locality in mesh-indexings. In <i>Fundamentals of Computation Theory</i>, pages 364--375, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>144</ref_seq_no>
				<ref_text><![CDATA[{NRS02} R. Niedermeier, K. Reinhardt, and P. Sanders. Towards optimal locality in mesh-indexings. <i>Discrete Applied Mathematics</i>, 117(1):211--237, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344947</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>145</ref_seq_no>
				<ref_text><![CDATA[{OBM00} Manuel M. Oliveira, Gary Bishop, and David McAllister. Relief Texture Mapping. In <i>ACM Computer Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 359--368, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>146</ref_seq_no>
				<ref_text><![CDATA[{oM95} National Library of Medicine. The visible human project. http://www.nlm.nih.gov/research/visible/visible_human.html, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>624083</ref_obj_id>
				<ref_obj_pid>623274</ref_obj_pid>
				<ref_seq_no>147</ref_seq_no>
				<ref_text><![CDATA[{PAC&#60;sup&#62;+&#60;/sup&#62;97} David Patterson, Thomas Anderson, Neal Cardwell, Richard Fromm, Kimberly, Keaton, ChristoforosKazyrakis, Randi Thomas, and Katherine Yellick. A case for intelligent ram. <i>IEEE Micro.</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>148</ref_seq_no>
				<ref_text><![CDATA[{PCM07} Flicker fusion rate. <i>PCMag.Com Encyclopedia Terms</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>582036</ref_obj_id>
				<ref_obj_pid>582034</ref_obj_pid>
				<ref_seq_no>149</ref_seq_no>
				<ref_text><![CDATA[{PF01} V. Pascucci and R. J. Frank. Global static indexing for real-time exploration of very large regular grids. In <i>Supercomputing</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1275121</ref_obj_id>
				<ref_obj_pid>1275117</ref_obj_pid>
				<ref_seq_no>150</ref_seq_no>
				<ref_text><![CDATA[{PG07} Renato Pajarola and Enrico Gobbetti. Survey on semi-regular multiresolution models for interactive terrain rendering. <i>The Visual Computer</i>, 23(8):583--605, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>151</ref_seq_no>
				<ref_text><![CDATA[{PGSS06} Stefan Popov, Johannes G&#252;nther, Hans-Peter Seidel, and Philipp Slusallek. Experiences with streaming construction of SAH KD-trees. In <i>Proceedings of the 2006 IEEE Symposium on Interactive Ray Tracing</i>, pages 89--94, September 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073292</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>152</ref_seq_no>
				<ref_text><![CDATA[{POC05} Fabio Policarpo, Manuel M. Oliveira, and Jo&#227;o Luiz Dihl Comba. Real-time relief mapping on arbitrary polygonal surfaces. <i>ACM Trans. Graph</i>, 24(3):935, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>153</ref_seq_no>
				<ref_text><![CDATA[{Pri00} Chris Prince. Progressive meshes for large models of arbitrary topology. Master's thesis, Department of Computer Science and Engineering, University of Washington, Seattle, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>154</ref_seq_no>
				<ref_text><![CDATA[{RB93} J. Rossignac and P. Borrel. Multi-resolution 3D approximation for rendering complex scenes. In <i>Second Conference on Geometric Modelling in Computer Graphics</i>, pages 453--465, June 1993. Genova, Italy.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>155</ref_seq_no>
				<ref_text><![CDATA[{RL00a} S. Rusinkiewicz and M. Levoy. Qsplat: A multiresolution point rendering system for large meshes. <i>Proc. of ACM SIGGRAPH</i>, pages 343--352, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>156</ref_seq_no>
				<ref_text><![CDATA[{RL00b} Szymon Rusinkiewicz and Marc Levoy. Qsplat: A multiresolution point rendering system for large meshes. In <i>Proceedings of ACM SIGGRAPH 2000</i>, Computer Graphics Proceedings, Annual Conference Series, pages 343--352, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>157</ref_seq_no>
				<ref_text><![CDATA[{RL00c} Szymon Rusinkiewicz and Marc Levoy. QSplat: A Multiresolution Point Rendering System for Large Meshes. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 343--352, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>158</ref_seq_no>
				<ref_text><![CDATA[{RR96} R. Ronfard and J. Rossignac. Full-range approximation of triangulated polyhedra. <i>Computer Graphics Forum (Eurographics'96 Proc.)</i>, 15(3):67--76, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073329</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>159</ref_seq_no>
				<ref_text><![CDATA[{RSH05} Alexander Reshetov, Alexei Soupikov, and Jim Hurley. Multi-Level Ray Tracing Algorithm. In <i>ACM Transaction of Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 1176--1185, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>176761</ref_obj_id>
				<ref_obj_pid>176756</ref_obj_pid>
				<ref_seq_no>160</ref_seq_no>
				<ref_text><![CDATA[{RW94} C. Ruemmler and J. Wilkes. An introduction to disk drive modeling. <i>IEEE Computer</i>, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>161</ref_seq_no>
				<ref_text><![CDATA[{Sag94} Hans Sagan. <i>Space-Filling Curves</i>. Springer-Verlag, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>162</ref_seq_no>
				<ref_text><![CDATA[{Sam06} Hanan Samet, editor. <i>Foundations of Multidimensional and Metric Data Structures</i>. Morgan Kaufmann, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>261150</ref_obj_id>
				<ref_obj_pid>261135</ref_obj_pid>
				<ref_seq_no>163</ref_seq_no>
				<ref_text><![CDATA[{SC97} M. Slater and Y. Chrysanthou. View volume culling using a probabilistic caching scheme. In <i>Proceedings of the ACM Symposium on Virtual Reality Software and Technology</i>, pages 71--78, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>164</ref_seq_no>
				<ref_text><![CDATA[{SCC&#60;sup&#62;+&#60;/sup&#62;02} C. Silva, Y.-J. Chiang, W. Correa, J. El-Sana, and P. Lindstrom. Out-of-core algorithms for scientific visualization and computer graphics. In <i>IEEE Visualization Course Notes</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602225</ref_obj_id>
				<ref_obj_pid>602220</ref_obj_pid>
				<ref_seq_no>165</ref_seq_no>
				<ref_text><![CDATA[{SCD02} S. Sen, S. Chatterjee, and N. Dumir. Towards a theory of cache-efficient algorithms. <i>Journal of the ACM</i>, 49:828--858, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732280</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>166</ref_seq_no>
				<ref_text><![CDATA[{SD01} Marc Stamminger and George Drettakis. Interactive Sampling and Rendering for Complex and Procedural Geometry. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques</i>, pages 151--162, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>167</ref_seq_no>
				<ref_text><![CDATA[{SDB97} Fran&#231;ois Sillion, George Drettakis, and Benoit Bodelet. Efficient Impostor Manipulation for Real-Time Visualization of Urban Scenery. In <i>Computer Graphics Forum (Proceedings of Eurographics)</i>, pages 207--218, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601690</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>168</ref_seq_no>
				<ref_text><![CDATA[{SG01} E. Shaffer and M. Garland. Efficient adaptive simplification of massive meshes. In <i>IEEE Visualization 2001</i>, pages 127--134, October 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>169</ref_seq_no>
				<ref_text><![CDATA[{SGHS98} Jonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. Layered Depth Images. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 231--242, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280882</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>170</ref_seq_no>
				<ref_text><![CDATA[{SGwHS98} Jonathan Shade, Steven J. Gortler, Li wei He, and Richard Szeliski. Layered depth images. In <i>Proceedings of SIGGRAPH 98</i>, Computer Graphics Proceedings, Annual Conference Series, pages 231--242, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344886</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>171</ref_seq_no>
				<ref_text><![CDATA[{SJDS00} G. Schaufler, J. Dorsey, X. Decoret, and F. X. Sillion. Conservative volumetric visibility with occluder fusion. In <i>SIGGRAPH 00 Conference Proceedings</i>, pages 229--238, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>172</ref_seq_no>
				<ref_text><![CDATA[{SMS&#60;sup&#62;+&#60;/sup&#62;07} Shevtsov, Maxim, Soupikov, Alexei, Kapustin, and Alexander. Highly parallel fast kd-tree construction for interactive ray tracing of dynamic scenes. <i>Computer Graphics Forum</i>, 26(3):395--404, September 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>731964</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>173</ref_seq_no>
				<ref_text><![CDATA[{Ste97} A. James Stewart. Hierarchical visibility in terrains. In <i>Eurographics Rendering Workshop 1997</i>, pages 217--228, June 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1318761</ref_obj_id>
				<ref_obj_pid>1318709</ref_obj_pid>
				<ref_seq_no>174</ref_seq_no>
				<ref_text><![CDATA[{Ste07} J. Stevens. Concepts and concerns related to the visualization of complex automotive data. <i>IEEE Computer Graphics and Applications</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>175</ref_seq_no>
				<ref_text><![CDATA[{Str74} Wolfgang Strasser. <i>Schnelle Kurven- und Flaechendarstellung auf graphischen Sichtgeraeten</i>. PhD thesis, TU Berlin, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>351672</ref_obj_id>
				<ref_obj_pid>351631</ref_obj_pid>
				<ref_seq_no>176</ref_seq_no>
				<ref_text><![CDATA[{Stu99} Wolfgang Stuerzlinger. Imaging all visible surfaces. In <i>Graphics Interface '99</i>, pages 115--122, June 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>177</ref_seq_no>
				<ref_text><![CDATA[{Sut63} Ivan E. Sutherland. Sketchpad: a man-machine graphical communication system. <i>SJCC</i>, 1963.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386396</ref_obj_id>
				<ref_obj_pid>2386388</ref_obj_pid>
				<ref_seq_no>178</ref_seq_no>
				<ref_text><![CDATA[{SWBG06} Christian Sigg, Tim Weyrich, Mario Botsch, and Markus Gross. Gpu-based raycasting of quadratic surfaces. In <i>Symposium on Point - Based Graphics 2006</i>, pages 59--66, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>179</ref_seq_no>
				<ref_text><![CDATA[{SWS07} K. Sun, G. Watson, and C. Seeling. Shader algorithm for the interactive, stereoscopic visualization of crash worthiness simulations. <i>IEEE Computer Graphics and Applications</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>180</ref_seq_no>
				<ref_text><![CDATA[{SZL92} William J. Schroeder, Jonathan A. Zarge, and William E. Lorensen. Decimation of Triangle Meshes. In <i>ACM Computer Graphics (Proceedings of ACM SIGGRAPH</i>), pages 65--70, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>181</ref_seq_no>
				<ref_text><![CDATA[{TC05} J. Thomas and K. Cook. Illuminating the path: The research and development agenda for visual analytics. <i>IEEE Press</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187858</ref_obj_id>
				<ref_obj_pid>1187627</ref_obj_pid>
				<ref_seq_no>182</ref_seq_no>
				<ref_text><![CDATA[{TCM06} M. Tarini, P. Cignoni, and C. Montani. Ambient occlusion and edge cueing for enhancing real time molecular visualization. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 12(5):1237--1244, September/October 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134029</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>183</ref_seq_no>
				<ref_text><![CDATA[{Tel92} Seth J. Teller. Computing the antipenumbra of an area light source. In <i>Computer Graphics (Proceedings of SIGGRAPH 92)</i>, pages 139--148, July 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122725</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>184</ref_seq_no>
				<ref_text><![CDATA[{TS91} S. J. Teller and C. H. Sequin. Visibility preprocessing for interative walkthroughs. <i>Computer Graphics (SIGGRAPH 91 Proceedings)</i>, 25(4):61--69, July 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122727</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>185</ref_seq_no>
				<ref_text><![CDATA[{VdMG91} Luiz Velho and Jonas de Miranda Gomes. Digital halftoning with space filling curves. In <i>ACM SIGGRAPH</i>, pages 81--90, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383843</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>186</ref_seq_no>
				<ref_text><![CDATA[{vdPS99} Michiel van de Panne and James Stewart. Efficient compression techniques for precomputed visibility. In <i>Eurographics Rendering Workshop 1999</i>, June 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>187</ref_seq_no>
				<ref_text><![CDATA[{vEB77} P. van Emde Boas. Preserving order in a forest in less than logarithmic time and linear space. <i>Inf. Process. Lett.</i>, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>384193</ref_obj_id>
				<ref_obj_pid>384192</ref_obj_pid>
				<ref_seq_no>188</ref_seq_no>
				<ref_text><![CDATA[{Vit01} J. Vitter. External memory algorithms and data structures: Dealing with massive data. <i>ACM Computing Surveys</i>, pages 209--271, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>269238</ref_obj_id>
				<ref_obj_pid>262839</ref_obj_pid>
				<ref_seq_no>189</ref_seq_no>
				<ref_text><![CDATA[{vKvOB&#60;sup&#62;+&#60;/sup&#62;97} M. van Kreveld, R. van Oostrum, C. Bajaj, V. Pascucci, and D. R. Schikore. Contour trees and small seed sets for isosurface traversal. In <i>Symp. on Computational Geometry</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>190</ref_seq_no>
				<ref_text><![CDATA[{Wal04} Ingo Wald. <i>Realtime Ray Tracing and Interactive Global Illumination</i>. PhD thesis, Computer Graphics Group, Saarland University, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1524953</ref_obj_id>
				<ref_obj_pid>1524874</ref_obj_pid>
				<ref_seq_no>191</ref_seq_no>
				<ref_text><![CDATA[{Wal07} Ingo Wald. On fast construction of sah based bounding volume hierarchies. In <i>Proceedings of the 2007 Eurographics/IEEE Symposium on Interactive Ray Tracing</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>192</ref_seq_no>
				<ref_text><![CDATA[{WBP98} Yigang Wang, Hujun Bao, and Qunsheng Peng. Accelerated walkthroughs of virtual environments based on visibility preprocessing and simplification. <i>Computer Graphics Forum</i>, 17(3), 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1206075</ref_obj_id>
				<ref_obj_pid>1189762</ref_obj_pid>
				<ref_seq_no>193</ref_seq_no>
				<ref_text><![CDATA[{WBS07} Ingo Wald, Solomon Boulos, and Peter Shirley. Ray tracing deformable scenes using dynamic bounding volume hierarchies. <i>ACM Transactions on Graphics</i>, 26(1):6.1--6.10, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383545</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>194</ref_seq_no>
				<ref_text><![CDATA[{WDS04} Ingo Wald, Andreas Dietrich, and Philipp Slusallek. An Interactive Out-of-Core Rendering Framework for Visualizing Massively Complex Models. In <i>Proceedings of the Eurographics Symposium on Rendering</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276490</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>195</ref_seq_no>
				<ref_text><![CDATA[{WFH&#60;sup&#62;+&#60;/sup&#62;07} Tim Weyrich, Cyril Flaig, Simon Heinzle, Simon Mall, Timo Aila, Kaspar Rohrer, Daniel Fasnacht, Norbert Felber, Stephan Oetiker, Hubert Kaeslin, Mario Botsch, and Markus Gross. A Hardware Architecture for Surface Splatting. In <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</i>, page 90, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383299</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>196</ref_seq_no>
				<ref_text><![CDATA[{WFP&#60;sup&#62;+&#60;/sup&#62;01} Michael Wand, Matthias Fischer, Ingmar Peter, Friedhelm Meyer auf der Heide, and Wolfgang Stra&#223;er. The Randomized z-Buffer Algorithm: Interactive Rendering of Highly Complex Scenes. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 361--370, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>197</ref_seq_no>
				<ref_text><![CDATA[{WH06} Ingo Wald and Vlastimil Havran. On building fast kd-trees for ray tracing, and on doing that in o(n log n). In <i>Proceedings of IEEE Symposium on Interactive Ray Tracing 2006</i>, pages 61--69, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>198</ref_seq_no>
				<ref_text><![CDATA[{Wie02} Jens-Michael Wierum. Logarithmic path-length in space-filling curves. In <i>14th Canadian Conference on Computational Geometry</i>, pages 22--26, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>199</ref_seq_no>
				<ref_text><![CDATA[{Wik07} Flicker fusion threshold article. <i>Wikipedia</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>200</ref_seq_no>
				<ref_text><![CDATA[{WK03} J. Wu and L. Kobbelt. A stream algorithm for the decimation of massive meshes. In <i>Proc. Graphics Interface</i>, pages 185--192, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383912</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>201</ref_seq_no>
				<ref_text><![CDATA[{WK06} C. W&#228;chter and A. Keller. Instant ray tracing: The bounding interval hierarchy. In <i>Proceedings of the Eurographics Symposium on Rendering</i>, pages 139--149, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>202</ref_seq_no>
				<ref_text><![CDATA[{WLML99} A. Wilson, E. Larsen, D. Manocha, and M. C. Lin. Partitioning and handling massive models for interactive collision detection. <i>Computer Graphics Forum (Proc. of Eurographics)</i>, 18(3):319--329, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882325</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>203</ref_seq_no>
				<ref_text><![CDATA[{WM03} Andrew Wilson and Dinesh Manocha. Simplifying Complex Environments Using Incremental Textured Depth Meshes. In <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRPAH)</i>, pages 678--688, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283912</ref_obj_id>
				<ref_obj_pid>1283900</ref_obj_pid>
				<ref_seq_no>204</ref_seq_no>
				<ref_text><![CDATA[{WMS06} Sven Woop, Gerd Marmitt, and Philipp Slusallek. B-KD Trees for Hardware Accelerated Ray Tracing of Dynamic Scenes. In <i>Proceedings of Graphics Hardware</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>205</ref_seq_no>
				<ref_text><![CDATA[{WPS&#60;sup&#62;+&#60;/sup&#62;03} Ingo Wald, Timothy J. Purcell, Joerg Schmittler, Carsten Benthin, and Philipp Slusallek. Realtime Ray Tracing and its use for Interactive Global Illumination. In <i>Eurographics 2003 State of the Art Reports</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>206</ref_seq_no>
				<ref_text><![CDATA[{WSBW01} Ingo Wald, Philipp Slusallek, Carsten Benthin, and Markus Wagner. Interactive rendering with coherent ray tracing. <i>Computer Graphics Forum</i>, 20(3):153--164, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073211</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>207</ref_seq_no>
				<ref_text><![CDATA[{WSS05} Sven Woop, J&#246;rg Schmittler, and Philipp Slusallek. RPU: A Programmable Ray Processing Unit for Realtime Ray Tracing. In <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 434--444, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383565</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>208</ref_seq_no>
				<ref_text><![CDATA[{WTL&#60;sup&#62;+&#60;/sup&#62;04} Xi Wang, Xin Tong, Stephen Lin, Shimin Hu, Baining Guo, and Heung-Yeung Shum. Generalized displacement maps. In Dieter Fellner and Stephen Spencer, editors, <i>Proceedings of the 2004 Eurographics Symposium on Rendering</i>, pages 227--234. Eurographics Association, June 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>760610</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>209</ref_seq_no>
				<ref_text><![CDATA[{WWS00} P. Wonka, M. Wimmer, and D. Schmalstieg. Visibility preprocessing with occluder fusion for urban walkthroughs. In <i>11th Eurographics Workshop on Rendering</i>, pages 71--82, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>210</ref_seq_no>
				<ref_text><![CDATA[{WWS01a} Michael Wimmer, Peter Wonka, and Francois Sillion. Point-based impostors for real-time visualization, May 29 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>211</ref_seq_no>
				<ref_text><![CDATA[{WWS01b} Peter Wonka, Michael Wimmer, and Fran&#231;ois X. Sillion. Instant visibility. <i>Computer Graphics Forum</i>, 20(3):411--421, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882272</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>212</ref_seq_no>
				<ref_text><![CDATA[{WWT&#60;sup&#62;+&#60;/sup&#62;03} Lifeng Wang, Xi Wang, Xin Tong, Stephen Lin, Shi-Min Hu, Baining Guo, and Heung-Yeung Shum. View-Dependent Displacement Mapping. In <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 334--339, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141914</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>213</ref_seq_no>
				<ref_text><![CDATA[{WWZ&#60;sup&#62;+&#60;/sup&#62;06} Peter Wonka, Michael Wimmer, Kaichi Zhou, Stefan Maierhofer, Gerd Hesina, and Alexander Reshetov. Guided visibility sampling. <i>ACM Transactions on Graphics</i>, 25(3):494--502, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383860</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>214</ref_seq_no>
				<ref_text><![CDATA[{YCM07} Sungeui Yoon, Sean Curtis, and Dinesh Manocha. Ray tracing dynamic scenes using selective restructuring. <i>Proc. of Eurographics Symposium on Rendering</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187849</ref_obj_id>
				<ref_obj_pid>1187627</ref_obj_pid>
				<ref_seq_no>215</ref_seq_no>
				<ref_text><![CDATA[{YL06} Sung-Eui Yoon and Peter Lindstrom. Mesh layouts for block-based caches. <i>IEEE Transactions on Visualization and Computer Graphics (Proceedings Visualization)</i>, 12(5), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1165424</ref_obj_id>
				<ref_obj_pid>1165407</ref_obj_pid>
				<ref_seq_no>216</ref_seq_no>
				<ref_text><![CDATA[{YLM06} Sung-Eui Yoon, Christian Lauterbach, and Dinesh Manocha. R-LODs: Fast LOD-Based Ray Tracing of Massive Models. <i>The Visual Computer</i>, 22(9--11):772--784, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073278</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>217</ref_seq_no>
				<ref_text><![CDATA[{YLPM05} Sung-Eui Yoon, Peter Lindstrom, Valerio Pascucci, and Dinesh Manocha. Cache-Oblivious Mesh Layouts. <i>Proc. of ACM SIGGRAPH</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>218</ref_seq_no>
				<ref_text><![CDATA[{YM06} Sung-Eui Yoon and Dinesh Manocha. Cache-efficient layouts of bounding volume hierarchies. <i>Computer Graphics Forum (Eurographics)</i>, 25:507--516, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>219</ref_seq_no>
				<ref_text><![CDATA[{Yoo05} Sung-eui Yoon. <i>Interactive Visualization and Collision Detection using Dynamic Simplification and Cache-Coherent Layouts</i>. PhD thesis, University of North Carolina at Chapel Hill, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1034440</ref_obj_id>
				<ref_obj_pid>1032664</ref_obj_pid>
				<ref_seq_no>220</ref_seq_no>
				<ref_text><![CDATA[{YSGM04} Sung-Eui Yoon, Brian Salomon, Russell Gayle, and Dinesh Manocha. Quick-VDR: Interactive View-dependent Rendering of Massive Models. <i>IEEE Visualization</i>, pages 131--138, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1070740</ref_obj_id>
				<ref_obj_pid>1070610</ref_obj_pid>
				<ref_seq_no>221</ref_seq_no>
				<ref_text><![CDATA[{YSGM05} Sung-Eui Yoon, Brian Salomon, Russell Gayle, and Dinesh Manocha. Quick-VDR: Out-of-Core View-Dependent Rendering of Gigantic Models. <i>IEEE Transactions on Visualization and Computer Graphics</i>, pages 369--382, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1057450</ref_obj_id>
				<ref_obj_pid>1057432</ref_obj_pid>
				<ref_seq_no>222</ref_seq_no>
				<ref_text><![CDATA[{YSLM04} S. Yoon, B. Salomon, M. C. Lin, and D. Manocha. Fast collision detection between massive models using dynamic simplification. <i>Eurographics Symposium on Geometry Processing</i>, pages 136--146, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258781</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>223</ref_seq_no>
				<ref_text><![CDATA[{ZMHH97} H. Zhang, D. Manocha, T. Hudson, and K. Hoff. Visibility culling using hierarchical occlusion maps. <i>Proc. of ACM SIGGRAPH</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602140</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>224</ref_seq_no>
				<ref_text><![CDATA[{ZT02} Eugene Zhang and Greg Turk. Visibility-guided simplification. In <i>Proc. IEEE Visualization</i>, pages 267--274, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Course Notes: Massive ModelVisualizationTechniques David Kasik1 Andreas Dietrich Enrico Gobbetti The 
Boeing Company NVIDIA Corporation CRS4 Fabio Marton Dinesh Manocha Philipp Slusallek CRS4 University 
of North Carolina Saarland University inTrace GmbH Abe Stephens2 Sung-EuiYoon SCI Institute KAIST University 
of Utah 1Course organizer. 2Course notes editor.  Contents 1 Course Description 3 1.1 Prerequisites...................................... 
3 1.2 Speaker Information ................................. 3 2 Synthesis Lecture on Real-Time Massive 
Model Rendering 6 3 Introduction to Massive Model Visualization 137 4 Rendering Approaches 146 5 Level 
of Detail Approaches 161 6 Parallel Programming Paradigms 182 7 Ray Tracing 188 8 Cache Coherent Layouts 
215 1 Course Description Massivemodel visualization seeks to provide users the ability to interact with 
3D models of almost unlimited size and complexity mainly with respect to geometry but increasingly in 
terms of appearance, illumination, visibility, and other features that create the illusion of photo realism. 
Numerous domains create highly complex digital models. Examples include industrial CAD modelsof airplanes,ships, 
production plants,andbuildings; geographic information systems;oil andgasexploration; medical imaging; 
scanned3D models; un-organized information spaces; and high-end scienti.c simulations. The digital modelsmay 
contain millions,even billionsof3D primitives. The primitives include points,surfaces,voxels,andhigher 
dimensionaldataforms.Eachprimitiveisoften associatedwith a complex set of parameters. We can store the 
data, post inquiries to search engines to analyze it, produce reports (including still pictures and .lms), 
and derive other information about it.Wehave just not been able to see it interactively in real time. 
Interactive performanceinvolves satisfyinga person that new frames are createdfast enough to convince 
the visual system that movement is continuous. Developing a solution that makes a person think that interaction 
is happening in real-time requires a system-level solution. In short, addressing only one or two parts 
of the system may solve the problem in a way that will continue to break when users want to visualize 
different types of datasets. This course offers details about the system issues involved in developing 
solutions for mas­sive model visualization. Included are in-depth discussions of hidden surface algorithms 
and their strengths and limitations, level-of-detail approaches, parallel programming paradigms, and 
opti­mizing disk access and memory utilization. 1.1 Prerequisites General knowledge of the difference 
between ray tracing and rasterization.  Familiarity with computing system architecture, display list 
concepts, graphics hardware accelerators, and parallel processing.  This course is intended for users 
of complex models and practitioners whobuild real-time 3D applications.The techniquesare applicabletoanycommunitythatcommonly 
reducesmodeldetail (e.g.,games) orworks only with model chunks (e.g., CAD).We contrast the strategies 
needed for real-time in a limited computing environment. 1.2 Speaker Information Dave Kasik is the Boeing 
EnterpriseVisualization Architect. His research interests include inno­vativecombinationsof basic3D graphicsand 
user interface technologiesand increasingawareness ofthe impactof visualization technology insideand 
outside Boeing.DavehasaBAin Quantitative Studies from the Johns Hopkins University and an MS in Computer 
Science from the University of Colorado.Heisa memberofIEEE,ACM,ACM SIGGRAPH(hehas attendedall SIGGRAPH 
conferences),andACMSIGCHI.Heisamemberofthe editorialboardforIEEEComputerGraph­ics and Applications. Andreas 
Dietrich received a masters degree (Dipl.-Technoinform.) in computer science from Kaiserslautern University 
in 2001. He has been a research assistant and PhD student at the Com­puter Graphics Group at Saarland 
University until 2007. He is now working at NVIDIA research. His work focuses on GPU ray tracing, real-time 
ray tracing for VR applications, natural phemodel visualization. Enrico Gobbetti is the director of the 
Advanced Computing and Communications Pro-gram and of theVisual Computing group at the CRS4 resarch center 
in Italy. He holds an Engineering degree (1989) and a Ph.D. degree (1993) in Computer Science from the 
Swiss Federal Institute of Technology in Lausanne (EPFL). Prior to joining CRS4, he held re-search and 
teaching positions at the Swiss Federal Institute ofTechnology in Lausanne, the University of Maryland 
Baltimore County, and theNASA Center of Excellence in Space Data and Information Sciences. At CRS4, Enricodevelopedandmanageda 
researchpro-gramsupportedthroughindustrialandgovernment grants. His research spans many areas of computer 
graphics and is widely published in major journals and conferences. He reg-ularly serves as program committee 
member or reviewer for international conferences and journals and is currently Associate Editor of Computer 
Graphics Forum. Technologies developed by his group have found practical use in as diverse real-world 
applications as internet geoviewing, scientic data analysis, and surgical training. Fabio Marton Fabio 
Martonisa researcherintheVisual Computing(ViC) groupatthe Center for Advanced Studies, Research, and 
Development in Sardinia (CRS4). He holds a Laurea (M. Sc.) degree (1999) in Computer Engineering from 
the UniversityofPadova. His current research interests include out-of-core data processing, multiresolution 
modeling and time-critical rendering. For more information, see www.crs4.it/vic Dinesh Manocha is currently 
a distinguished professor of Computer Science at the University of North Carolina at Chapel Hill. He 
was selected as an Alfred P. Sloan Research Fellow and has received signi.cant awards from NSF, ONR, 
Honda, and UNC. He has received numerous best paper awards and has served on program committees for computer 
graphics and geometric modeling conferences. Manocha has been working on large model visualization for 
more than 10 years. His research group at UNC Chapel Hill has published numerous papers at SIGGRAPH. 
He has also organized SIGGRAPH courses on interactive walkthroughs, large model visualization, and GPGPU. 
Sung-Eui Yoon is currently an assistant professor atKorea Advanced Institute of Science and Technology 
(KAIST). He received the B.S. and M.S. degrees in computer science from Seoul Na­tional University in 
1999 and 2001 respectively. He received his Ph.D. degree in computer science from the University of North 
Carolina at Chapel Hill in 2005. He was a postdoctoral scholar at Lawrence Livermore National Laboratory. 
His research interests include scalable graphics and ge­ometric algorithms, interactive rendering, geometric 
problems, and cache-coherent algorithms and layouts. Abe Stephens is a PhD student at the Scienti.c Computing 
and Imaging Institute at the Uni­versity of Utah. His work focuses on spatial/temporal adaptive methods, 
parallel rendering, and large data visualization.Heisa principle contributortothe Manta InteractiveRayTracerandhas 
worked with SGI, Intel, and NVIDIA to improve interactive ray tracing on parallel systems. Abe received 
a BS in Computer Science from Rensselaer Polytechnic Institute in 2003. Philipp Slusallek is professor 
for computer graphics and digital media at Saarland University, Germany. Before joining SaarlandUniversityhewas 
visiting assistant professoratStanford Uni­versity. He received a Diploma/MSc in physics from the University 
of Tbingen and a Doctor/PhD in computer science from the University of Erlangen. Philipp has published 
and taught exten­sively, including courses on real-time ray tracing at SIGGRAPH05 and SIGGRAPH06. He 
is the principal investigator for the OpenRT project to establish real-time ray-tracing as an alternative 
technology for interactive and photorealistic 3D graphics. Recently he co-founded inTrace , a spin-offcompanythat 
commercializes real-time ray tracing technology.  Synthesis Lecture on Real-Time Massive Model Rendering 
                                                   
               Introduction to Massive Model Visualization     Rendering Approaches 
        Level of Detail Approaches           Parallel Programming Paradigms   7 
Ray Tracing 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401191</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<pages>33</pages>
		<display_no>41</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Massive model visualization using realtime ray tracing]]></title>
		<page_from>1</page_from>
		<page_to>33</page_to>
		<doi_number>10.1145/1401132.1401191</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401191</url>
		<abstract>
			<par><![CDATA[<p>In the last years real-time ray tracing has become an attractive alternative to rasterization based rendering, particularly for highly complex datasets including both surface and volume data. Ray tracing [7, 15] is a much more flexible rendering algorithm than triangle rasterization found in most of todays graphics cards. Employing it in a real-time context might at first sound a bit surprising as ray tracing is mostly known for its application in high-quality off-line image generation, as e.g. in the motion picture industry. Infamous for its long rendering times, ray tracing was not used for interactive purposes until recently [13, 14, 19]. What makes it attractive for massive model rendering is not only its simplicity and robustness, but especially its versatility.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098747</person_id>
				<author_profile_id><![CDATA[81100128205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dietrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saarland University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098748</person_id>
				<author_profile_id><![CDATA[81100159926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slusallek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saarland University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882314</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sameer Agarwal, Ravi Ramamoorthi, Serge Belongie, and Henrik Wann Jensen. Structured Importance Sampling of Environment Maps. In <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 605--612, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[John Amanatides and Andrew Woo. A Fast Voxel Traversal Algorithm for Ray Tracing. In <i>Computer Graphics Forum</i>, pages 3--10, 1987. (Proceedings of Eurographics).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Carsten Benthin, Ingo Wald, and Philipp Slusallek. A Scalable Approach to Interactive Global Illumination. In <i>Computer Graphics Forum</i>, pages 621--630, 2003. (Proceedings of Eurographics).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>361007</ref_obj_id>
				<ref_obj_pid>361002</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Jon Louis Bentley. Multidimensional Binary Search Trees Used for Associative Searching. <i>Communications of the ACM</i>, 18(9):509--517, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2381368</ref_obj_id>
				<ref_obj_pid>2381356</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Andreas Dietrich, Carsten Colditz, Oliver Deussen, and Philipp Slusallek. Realistic and Interactive Visualization of High-Density Plant Ecosystems. In <i>Natural Phenomena 2005, Proceedings of the Eurographics Workshop on Natural Phenomena</i>, pages 73--81, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Andreas Dietrich, Gerd Marmitt, and Philipp Slusallek. Terrain guided multi-level instancing of highly complex plant populations. In <i>Proceedings of the 2006 IEEE Symposium on Interactive Ray Tracing</i>, pages 169--176, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>94788</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner. <i>An Introduction to Ray Tracing</i>. Morgan Kaufmann, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>983550</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Mel Gorman. <i>Understanding the Linux Virtual Memmory Manager (Bruce Perens' Open Source Series)</i>. Prentice Hall, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732285</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Alexander Keller and Wolfgang Heidrich. Interleaved Sampling. In <i>Rendering Techniques 2001 (Proceedings of the 12th Eurographics Workshop on Rendering)</i>, pages 269--276, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882411</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Thomas Kollig and Alexander Keller. Efficient Illumination by High Dynamic Range Images. In <i>Rendering Techniques 2003 (Proceedings of the 14th Eurographics Workshop on Rendering)</i>, pages 45--50, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618821</ref_obj_id>
				<ref_obj_pid>616070</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[David P. Luebke. A Developer's Survey of Polygonal Simplification Algorithms. <i>IEEE Computer Graphics and Applications</i>, 21(3):24--35, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. David MacDonald and Kellogg S. Booth. Heuristics for Ray Tracing using Space Subdivision. In <i>Proceedings of Graphics Interface '89</i>, pages 152--163, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Michael J. Muuss. Towards Real-Time Ray-Tracing of Combinatorial Solid Geometric Models. In <i>Proceedings of BRL-CAD Symposium '95</i>, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300537</ref_obj_id>
				<ref_obj_pid>300523</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Steven Parker, Peter Shirley, Yarden Livnat, Charles Hansen, and Peter-Pike Sloan. Interactive Ray Tracing. In <i>Proceedings of Interactive 3D Graphics</i>, pages 119--126, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>940410</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Peter Shirley and R. Keith Morley. <i>Realistic Ray Tracing, Second Edition</i>. A K Peters, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Peter-Pike Sloan, Jan Kautz, and John Snyder. Precomputed Radiance Transfer for Real-Time Rendering in Dynamic, Low-Frequency Lighting Environments. In <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</i>, pages 527--536, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2386128</ref_obj_id>
				<ref_obj_pid>2386124</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Abraham Stephens, Solomon Boulos, James Bigler, Ingo Wald, and Steven Parker. An Application of Scalable Massive Model Interaction using Shared-Memory Systems. In <i>Proceedings of the Eurographics Symposium on Parallel Graphics and Visualization</i>, pages 19--26, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Kelvin Sung and Peter Shirley. <i>Ray Tracing with the BSP Tree</i>. Academic Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald. <i>Realtime Ray Tracing and Interactive Global Illumination</i>. PhD thesis, Computer Graphics Group, Saarland University, 2004. Available at http://www.mpi-sb.mpg.de/~wald/PhD/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081418</ref_obj_id>
				<ref_obj_pid>1081407</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald, Carsten Benthin, and Philipp Slusallek. Distributed Interactive Ray Tracing of Dynamic Scenes. In <i>Proceedings of the IEEE Symposium on Parallel and Large-Data Visualization and Graphics (PVG)</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732298</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Ingo Wald, Philipp Slusallek, and Carsten Benthin. Interactive Distributed Ray Tracing of Highly Complex Models. In <i>Rendering Techniques 2001 (Proceedings of the 12th Eurographics Workshop on Rendering)</i>, pages 274--285, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Sergey Zhukov, Andrei Iones, and Grigorij Kronin. An Ambient Light Illumination Model. In <i>Rendering Techniques 1998 (Proceedings of the 9th Eurographics Workshop on Rendering)</i>, pages 45--56, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401192</section_id>
		<sort_key>600</sort_key>
		<section_seq_no>19</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Motion planning and autonomy for virtual humans]]></section_title>
		<section_page_from>19</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098749</person_id>
				<author_profile_id><![CDATA[81100306359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pettre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098750</person_id>
				<author_profile_id><![CDATA[81100215070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kallmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098751</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401193</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>31</pages>
		<display_no>42</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Motion planning and autonomy for virtual humans]]></title>
		<page_from>1</page_from>
		<page_to>31</page_to>
		<doi_number>10.1145/1401132.1401193</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401193</url>
		<abstract>
			<par><![CDATA[<p>An enormous amount of Motion Planning techniques has been developed in the past decade specifically targeting applications in Computer Animation. Going beyond the traditional path planning for navigation, recent techniques address challenging problems in cluttered environments ranging from crowd navigation among obstacles to multi-agent cooperative manipulation and to whole-body manipulation and locomotion planning. Given these recent advances, Motion Planning has already become a main tool for controlling autonomous virtual characters and will become crucial for empowering the next generation of Virtual Humans with the Motion Autonomy that will be needed in increasingly complex, interactive and realistic Computer Games and Virtual Reality Applications.</p> <p>These notes present for the first time a systematic and comprehensive exposition of the main Motion Planning techniques that have been developed for applications in Computer Animation, in particular for the animation of Virtual Humans (VHs). These notes comprehensively document the class "Motion Planning and Autonomy for Virtual Humans" delivered at SIGGRAPH 2008.</p> <p>We start with the basic concepts of Motion Planning and then present techniques for increasingly complex problems: ranging from the navigation of single and multiple VHs to object manipulation and synchronization of manipulation and locomotion. We also explain how Motion Planning techniques can handle challenging problems involving underactuated and redundant skeletal structures of Virtual Humans and show examples of complex motions planned in high-dimensional configuration spaces subjected to geometric and kinematic constraints. The advantages of configuration-space Motion Planning are in particular emphasized, for instance in contrast with common approaches based on executing end-effector trajectories with Inverse Kinematics. The described techniques expose the pluridisciplinary aspects of Computer Graphics and Robotics, from the Motion Planning origins in Robotics to its continuous development relying on Graphics tools, to the current increasing need of motion autonomy in Computer Animation. After reading these notes, the reader will obtain a clear understanding of the potential of Motion Planning and the new dimension of motion autonomy that is being achieved by its variety of techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Plan execution, formation, and generation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098752</person_id>
				<author_profile_id><![CDATA[81100306359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pettr&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, Centre de Recherche INRIA de Rennes - Bretagne-Atlantique, Rennes cedex, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098753</person_id>
				<author_profile_id><![CDATA[81100215070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kallmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Merced, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098754</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2212094</ref_obj_id>
				<ref_obj_pid>2211657</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[G. Arechavaleta, J.-P. Laumond, H. Hicheur, and A. Berthoz. An optimality principle governing human locomotion. <i>IEEE Trans. on Robotics</i>, 24(1), 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Francis Avnaim and J.-D. Boissonnat. Practical exact motion planning of a class of robots with three degrees of freedom. In <i>Proceedings of the first Canadian Conference on Computational Geometry</i>, page 19, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. E. Bobrow, S. Dubowsky, and J. S. Gibson. Time-optimal control of robotic manipulators along specified paths. <i>International Journal of Robotics Research</i>, 4(3):3--17, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74357</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Bruderlin and T. W. Calvert. Goal-directed, dynamic animation of human walking. In <i>SIGGRAPH '89: Proceedings of the 16th annual conference on Computer Graphics and Interactive Techniques</i>, pages 233--242, New York, NY, USA, 1989. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>49142</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. F. Canny. <i>The Complexity of Robot Motion Planning</i>. ACM Doctoral Dissertation Award. MIT Press, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>636889</ref_obj_id>
				<ref_obj_pid>636886</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Min Gyu Choi, Jehee Lee, and Sung Yong Shin. Planning biped locomotion using motion capture data and probabilistic roadmaps. <i>ACM Transactions on Graphics</i>, 22(2):182--203, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[H. Choset and J. Burdick. Sensor based motion planning: The hierarchical generalized Voronoi graph. In <i>Algorithms for Robot Motion and Manipulation</i>, pages 47--61. A K Peters, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Howie Choset, Kevin M. Lynch, Seth Hutchinson, George Kantor, Wolfram Burgard, Lydia Kavraki, and Sebastian Thrun. <i>Principles of Robot Motion: Theory, Algorithms and Implementations</i>. The MIT Press, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1138457</ref_obj_id>
				<ref_obj_pid>1138450</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Claudia Esteves, Gustavo Arechavaleta, Julien Pettre, and Jean-Paul Laumond. Animation planning for virtual characters cooperation. <i>ACM Transactions on Graphics</i>, 25(2):319--339, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Foskey, M. Garber, M. C. Lin, and D. Manocha. A voronoi-based hybrid motion planner. In <i>Intelligent Robots and Systems, 2001. Proceedings. 2001 IEEE/RSJ International Conference on</i>, volume 1, pages 55--60, 29 Oct.-3 Nov. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L. J. Guibas, C. Holleman, and L. E. Kavraki. A probabilistic roadmap planner for flexible objects with a workspace medial-axis-based sampling approach. In <i>Intelligent Robots and Systems, 1999. IROS '99. Proceedings. 1999 IEEE/RSJ International Conference on</i>, volume 1, pages 254--259, 17--21 Oct. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. Halperin. Robust geometric computing in motion. <i>International Journal of Robotics Research</i>, 21(3):219--232, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Halim Hicheur, Quang-Cuong Pham, Gustavo Arechavaleta, Jean-Paul Laumond, and Alain Berthoz. The formation of trajectories during goal-oriented locomotion in humans. i. a stereotyped behaviour. <i>Eur J Neurosci</i>, 26(8):2376--2390, Oct 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[K. Hoff, T. Culver, J. Keyser, M. Lin, and D. Manocha. Interactive motion planning using hardware accelerated computation of generalized voronoi diagrams. <i>IEEE Conference on Robotics and Automation</i>, pages pp. 2931--2937, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D. Hsu, J.-C. Latombe, and R. Motwani. Path planning in expansive configuration spaces. <i>International Journal of Computational Geometry and Applications</i>, 9(4/5):495--512, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[X. Jiang and M. Kallmann. Learning humanoid reaching tasks in dynamic environments. In <i>Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)</i>, San Diego CA, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Marcelo Kallmann, Amaury Aubel, Tolga Abaci, and Daniel Thalmann. Planning collision-free reaching motions for interactive object manipulation and grasping. <i>Computer graphics Forum (Proceedings of Eurographics'03)</i>, 22(3):313--322, September 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Marcelo Kallmann, Robert Bargmann, and Maja J. Matari&#263;. Planning the sequencing of movement primitives. In <i>Proceedings of the International Conference on Simulation of Adaptive Behavior (SAAB)</i>, pages 193--200, Santa Monica, CA, July 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Marcelo Kallmann, Ari Shapiro, and Petros Faloutsos. Planning motions in motion. In <i>ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA'06), Posters and Demos Proceedings</i>, pages 23--24, Vienna, Austria, September 2-4 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[L. E. Kavraki, P. Svestka, J.-C. Latombe, and M. H. Overmars. Probabilistic roadmaps for path planning in high-dimensional configuration spaces. <i>Robotics and Automation, IEEE Transactions on</i>, 12(4):566--580, Aug. 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>73427</ref_obj_id>
				<ref_obj_pid>73393</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Klara Kedem and Micha Sharir. An automatic motion planning system for a convex polygonal mobile robot in 2-dimensional polygonal space. In <i>Symposium on Computational Geometry</i>, pages 329--340, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Y. Koga, K. Kondo, J. Kuffner, and J. C. Latombe. Planning motions with intentions. <i>Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, pages 395--408, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Y. Koga, K. Kondo, James J. Kuffner, and J. C. Latombe. Planning motions with intentions. In <i>SIGGRAPH '94: Proceedings of the annual conference on Computer Graphics and Interactive Techniques</i>, pages 395--408, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>931642</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[James J. Kuffner. <i>Autonomous Agents for Real-time Animation</i>. PhD thesis, Stanford University, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Jr. Kuffner, J. J. and S. M. LaValle. Rrt-connect: An efficient approach to single-query path planning. In <i>Robotics and Automation, 2000. Proceedings. ICRA '00. IEEE International Conference on</i>, volume 2, pages 995--1001, 24-28 April 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>662216</ref_obj_id>
				<ref_obj_pid>645625</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[F. Lamiraux and J.-P. Laumond. From paths to trajectories for multi-body mobile robots. In <i>Proceedings of the Fifth International Symposium on Experimental Robotics</i>, pages 301--309. Springer-Verlag, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>532147</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J.-C. Latombe. <i>Robot Motion Planning</i>. Boston: Kluwer Academic Publishers, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073408</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Manfred Lau and James J. Kuffner. Behavior planning for character animation. In <i>SCA '05: Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, pages 271--280, New York, NY, USA, 2005. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[S. LaValle. Rapidly-exploring random trees: A new tool for path planning. Technical Report TR98-11, Computer Science Department, Iowa State University, October 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1213331</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. M. LaValle. <i>Planning Algorithms</i>. Cambridge University Press, Cambridge, U.K., 2006. Available at http://planning.cs.uiuc.edu/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1309766</ref_obj_id>
				<ref_obj_pid>1309288</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[T. Lozano-Perez. Spatial planning: A configuration space approach. <i>Computers, IEEE Transactions on</i>, C-32(2):108--120, Feb 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>359164</ref_obj_id>
				<ref_obj_pid>359156</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Tom&#225;s Lozano-P&#233;rez and Michael A. Wesley. An algorithm for planning collision-free paths among polyhedral obstacles. <i>Commun. ACM</i>, 22(10):560--570, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>135734</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Atsuyuki Okabe, Barry Boots, Kokichi Sugihara, and Sung Nok Chiu. <i>Spatial tessellations: Concepts and applications of Voronoi diagrams</i>. Probability and Statistics. Wiley &amp; Sons, NYC, 2nd edition, 2000. 671 pages.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[S. Paris, J. Pettr&#233;, and S. Donikian. Pedestrian reactive navigation for crowd simulation: a predictive approach. <i>Computer Graphics Forum: Eurographics'07</i>, 26 (3), 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[J. Pettre, H. Grillon, and D. Thalmann. Crowds of moving objects: Navigation planning and simulation. In <i>Robotics and Automation, 2007 IEEE International Conference on</i>, pages 3062--3067, 10-14 April 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1144487</ref_obj_id>
				<ref_obj_pid>1144457</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Julien Pettr&#233;, Pablo de Heras Ciechomski, Jonathan Ma&#239;m, Barbara Yersin, Jean-Paul Laumond, and Daniel Thalmann. Real-time navigating crowds: scalable simulation and rendering: Research articles. <i>Comput. Animat. Virtual Worlds</i>, 17(3-4):445--455, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1133119</ref_obj_id>
				<ref_obj_pid>1133115</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Julien Pettre and Jean-Paul Laumond. A motion capture-based control-space approach for walking mannequins. <i>Computer Animation and Virtual Worlds</i>, 17:109--126, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846313</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Julien Pettre, Jean-Paul Laumond, and T. Simeon. A 2-stage locomotion planner for digital actors. In <i>SCA '03: Proceedings of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[J. T. Schwartz and M. Sharir. On the piano movers probelem ii, general techniques for computing topological properties of real algebraic manifolds. <i>Advances of Applied Maths</i>, 4:298--351, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230124</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Ari Shapiro, Marcelo Kallmann, and Petros Faloutsos. Interactive motion correction and object manipulation. In <i>ACM SIGGRAPH Symposium on Interactive 3D graphics and Games (I3D'07)</i>, Seattle, April 30 - May 2 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Kang Shin and N. McKay. Minimum-time control of robotic manipulators with geometric path constraints. <i>Automatic Control, IEEE Transactions on</i>, 30(6):531--541, Jun 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[J.-J.E. Slotine and H. S. Yang. Improving the efficiency of time-optimal path-following algorithms. <i>Robotics and Automation, IEEE Transactions on</i>, 5(1):118--124, Feb. 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[A. Sud, E. Andersen, S. Curtis, M. Lin, and D. Manocha. Real-time path planning for virtual agents in dynamic environments. <i>Proc. of IEEE VR</i>, pages 91--98, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1315201</ref_obj_id>
				<ref_obj_pid>1315184</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[A. Sud, R. Gayle, E. Andersen, S. Guy, M. Lin, and D. Manocha. Real-time navigation of independent agents using adaptive roadmaps. <i>Proc. of ACM VRST</i>, 2007. to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1342272</ref_obj_id>
				<ref_obj_pid>1342250</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Jur van den Berg, Sachin Patil, Jason Sewall, Dinesh Manocha, and Ming Lin. Interactive navigation of individual agents in crowded environments. <i>Symposium on Interactive 3D Graphics and Games (I3D)</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Gokul Varadhan and Dinesh Manocha. Star-shaped roadmaps - a deterministic sampling approach for complete motion planning. In <i>Proceedings of Robotics: Science and Systems</i>, Cambridge, USA, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[S. A. Wilmarth, N. M. Amato, and P. F. Stiller. Maprm: a probabilistic roadmap planner with sampling on the medial axis of the free space. In <i>Robotics and Automation, 1999. Proceedings. 1999 IEEE International Conference on</i>, volume 2, pages 1024--1031, 10-15 May 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2212180</ref_obj_id>
				<ref_obj_pid>2211661</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[E. Yoshida, C. Esteves, I. Belousov, J. P. Laumond, T. Sakaguchi, and K. Yokoi. Planning 3d collision-free dynamic robotic motion through iterative reshaping. <i>IEEE Trans. on Robotics</i>, Conditionnaly accepted.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[E. Yoshida, A. Mallet, F. Lamiraux, O. Kanoun, O. Stasse, M. Poirier, P. F. Dominey, J. P. Laumond, and K. Yokoi. "give me the purple ball" he said to hrp-2 n. 14. In <i>Proceedings of the IEEE-RAS Conf. on Humanoids</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401194</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<pages>38</pages>
		<display_no>43</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Autonomous navigation for a virtual human - part I]]></title>
		<page_from>1</page_from>
		<page_to>37</page_to>
		<doi_number>10.1145/1401132.1401194</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401194</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.4</cat_node>
				<descriptor>Navigation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10003254</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Hypertext / hypermedia</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10010515</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Multi / mixed media creation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098755</person_id>
				<author_profile_id><![CDATA[81100342764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gleicher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin - Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401195</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>9</pages>
		<display_no>44</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Autonomous navigation for a virtual human - part II]]></title>
		<page_from>1</page_from>
		<page_to>8</page_to>
		<doi_number>10.1145/1401132.1401195</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401195</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098756</person_id>
				<author_profile_id><![CDATA[81314489092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Claudia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Esteves]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401196</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<pages>22</pages>
		<display_no>45</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Autonomous navigation for crowds of virtual humans]]></title>
		<subtitle><![CDATA[part I: interactive design of virtual population using navigation graphs]]></subtitle>
		<page_from>1</page_from>
		<page_to>21</page_to>
		<doi_number>10.1145/1401132.1401196</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401196</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098757</person_id>
				<author_profile_id><![CDATA[81100306359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pettr&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401197</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<pages>42</pages>
		<display_no>46</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Autonomous navigation for crowds of virtual humans]]></title>
		<subtitle><![CDATA[part II: motion planning techniques for large-scale crowd simulation]]></subtitle>
		<page_from>1</page_from>
		<page_to>41</page_to>
		<doi_number>10.1145/1401132.1401197</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401197</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098758</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401198</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<pages>98</pages>
		<display_no>47</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Autonomous object manipulation for virtual humans]]></title>
		<page_from>1</page_from>
		<page_to>97</page_to>
		<doi_number>10.1145/1401132.1401198</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401198</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098759</person_id>
				<author_profile_id><![CDATA[81100215070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kallmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Baerlocher 2001} P. Baerlocher, "Inverse Kinematics Techniques for the Interactive Posture Control of Articulated Figures", PhD Thesis 2383, Swiss Federal Institute of Technology (EPFL), 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124585</ref_obj_id>
				<ref_obj_pid>1124573</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Bretl 2006} T. Bretl, "Motion Planning of Multi-Limbed Robots Subject to Equilibrium Constraints: The Free-Climbing Robot Problem", Int'l Jounal of Robotics Research 25(4), 2006, 317--342.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Burns and Brock 2005} B. Burns and O. Brock, "Sampling-Based Motion Planning Using Predictive Models", In Proc. of ICRA, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237244</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Gottschalk et al 1996} S. Gottschalk, M. Lin, and D. Manocha, "Obbtree: A Hierarchical Structure for Rapid Interference Detection", In Proc. Of SIGGRAPH 1996, 171--180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Hauser et al 2007} K. Hauser, V. Ng-Thowhing, and H. Gonzalez-Ba&#241;os, "Multi-Modal Motion Planning for a Humanoid Robot Manipulation Task", In Proc. of the Int'l Symposium on Robotics Research (ISRR) 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230123</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Heck and Gleicher 2007} R. Heck and M. Gleicher, "Parametric Motion Graphs", Proc. of Interactive 3D Graphics and Games (I3D 2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Jiang and Kallmann 2007} X. Jiang and M. Kallmann, "Learning Humanoid Reaching Tasks in Dynamic Environments", In Proc. of IROS, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Kalisiak and van de Panne 2001} M. Kalisiak and M. van de Panne, "A Grasp-Based Motion Planning Algorithm for Character Animation", The Journal of Visualization and Computer Animation 12(3), 2001, 117--129.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Kallmann 2004} M. Kallmann, "Interaction with 3-D Objects", In Handbook of Virtual Humans, John Wiley &amp; Sons, UK, 2004, 303--322.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Kallmann 2005} M. Kallmann, "Scalable Solutions for Interactive Virtual Humans that can Manipulate Objects", AIIDE 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1361706</ref_obj_id>
				<ref_obj_pid>1361705</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Kallmann 2008} M. Kallmann, "Analytical Inverse Kinematics with Body Posture Control", Computer Animation and Virtual Worlds, 19(2), May 2008, 79--91.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Kallmann and Mataric' 2004} M. Kallmann and M. Mataric', "Motion Planning Using Dynamic Roadmaps", In Proc. of ICRA 2004, 4399--4404.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Kallmann et al 2003} M. Kallmann, A. Aubel, T. Abaci, and D. Thalmann, "Planning Collision-Free Reaching Motions for Interactive Object Manipulation and Grasping", Proceedings of Eurographics 2003, 313--322.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Kallmann et al 2004} M. Kallmann, R. Bargmann and M. Mataric', "Planning the Sequencing of Movement Primitives", in Proc. of the Int'l Conference on Simulation of Adaptive Behavior (SAB), 2004, 193--200.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Kavraki et al 1996} L. Kavraki, P. Svestka, J.-C. Latombe, and M. Overmars, "Probabilistic Roadmaps for Fast Path Planning in High-Dimensional Configuration Spaces", IEEE Transactions on Robotics and Automation 12, 1996, 566--580.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Koga et al 1994} Y. Koga, K. Kondo, J. Kuffner, and J.-C. Latombe, "Planning motions with intentions", In Proc. of SIGGRAPH 1994, 395--408.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015760</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{Kovar and Gleicher 2004} L. Kovar and M. Gleicher, "Automated Extraction and Parameterization of Motions in Large Data Sets", ACM Trans. on Graphics (Proc. of SIGGRAPH 2004).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826533</ref_obj_id>
				<ref_obj_pid>826029</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{Kuffner and Latombe 2000} J. Kuffner and J.-C. Latombe, "Interactive Manipulation Planning for Animated Characters", In Pacific Graphics 2000 (short paper).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{Kuffner and LaValle 2000} J. Kuffner and S. LaValle, "RRT-Connect: An Efficient Approach to Single-Query Path Planning", In Proceedings of IEEE Int'l Conference on Robotics and Automation (ICRA), 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{Kuffner et al 2001} J. Kuffner, K. Nishiwaki, S. Kagami, M. Inaba, and H. Inoue, "Footstep planning among obstacles for biped robots", In Proc. of IEEE/RSJ Int'l Conf. on Intelligent Robots and Systems (IROS 2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Leven and Hutchinson 2000} P. Leven and S. Hutchinson, "Toward Real-Time Path Planning in Changing Environments", Proc. of the fourth International Workshop on the Algorithmic Foundations of Robotics (WAFR), March 2000, pp. 363--376.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Maciejewski and Klein 1985} A. Maciejewski and C. Klein, "Obstacle Avoidance for Kinematically Redundant Manipulators in Dynamically Varying Environments," Intl. Journal of Robotics Research, 4(3), 1985, 109--117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1010375</ref_obj_id>
				<ref_obj_pid>1009389</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Redon et al 2004} S. Redon, Y. Kim, M. Lin, D. Manochal and J. Templeman, "Interactive and Continuous Collision Detection for Avatars in Virtual Environments". In Proc. of the IEEE Virtual Reality 2004]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Rose et al 2001} C. Rose III, P-P. Sloan, and M. Cohen, "Artist-Directed Inverse-Kinematics Using Radial Basis Function Interpolation", Computer graphics Forum, 20(3), 2001, 239--250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276510</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Safonova and Hodgins 2007} A. Safonova and J. Hodgins, "Construction and Optimal Search of Interpolated Motion Graphs", ACM Trans. on Graphics (Proc. of SIGGRAPH 2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{Schwarzer et al 2002} F. Schwarzer, M. Saha, and J. Latombe, "Exact Collision Checking of Robot Paths", In Proc. of the Workshop on Algorithmic Foundations of Robotics, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230124</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{Shapiro et al 2007} A. Shapiro, M. Kallmann, and P. Faloutsos, "Interactive Motion Correction and Object Manipulation", ACM SIGGRAPH Symposium on Interactive 3D graphics and Games (I3D), 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358832</ref_obj_id>
				<ref_obj_pid>358829</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{Tolani et al 2000} D. Tolani, A. Goswami, and N. Badler, "Real-Time Inverse Kinematics Techniques for Anthropomorphic Limbs", Graphical Models and Image Processing, 62(5), 2000, 353--388.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015756</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{Yamane et al 2004} K. Yamane, J. Kuffner, and J. Hodgins, "Synthesizing Animations of Human Manipulation Tasks." ACM Trans. on Graphics (Proc. of SIGGRAPH 2004), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401199</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<pages>11</pages>
		<display_no>48</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Digression]]></title>
		<subtitle><![CDATA[back to real?]]></subtitle>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/1401132.1401199</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401199</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10003809.10011254.10011258</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Algorithm design techniques->Dynamic programming</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010205</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098760</person_id>
				<author_profile_id><![CDATA[81100468739]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jean-Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laumond]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Yoshida, I. Belousov, C. Esteves and J-P. Laumond. <i>Humanoid Motion Planning for Dynamic Tasks</i>. IEEE Int. Conf. on Humanoid Robots, Tsukuba (Japan), 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H. Hicheur, Q. C. Pham, G. Arechavaleta, J. P. Laumond, A. Berthoz. <i>The formation of trajectories during goal-oriented locomotion in humans. I. A stereotyped behaviour</i>. European J. of Neuroscience, 26 (8), 2007]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2212094</ref_obj_id>
				<ref_obj_pid>2211657</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Arechavaleta, J. P. Laumond, H. Hicheur A. Berthoz. <i>An optimal principle governing human walking</i>. IEEE Transactions on Robotics, 24 (1), 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401200</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>11</pages>
		<display_no>49</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Graph-based motion synthesis]]></title>
		<subtitle><![CDATA[an annotated bibliography]]></subtitle>
		<page_from>1</page_from>
		<page_to>11</page_to>
		<doi_number>10.1145/1401132.1401200</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401200</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Graph algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Plan execution, formation, and generation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Control theory</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010214</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Computational control theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10010917</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Graph algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098761</person_id>
				<author_profile_id><![CDATA[81100342764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[Lee]]></middle_name>
				<last_name><![CDATA[Gleicher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin - Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401201</article_id>
		<sort_key>690</sort_key>
		<display_label>Article No.</display_label>
		<pages>6</pages>
		<display_no>50</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Manipulation planning for virtual humans]]></title>
		<subtitle><![CDATA[summary of representative papers]]></subtitle>
		<page_from>1</page_from>
		<page_to>6</page_to>
		<doi_number>10.1145/1401132.1401201</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401201</url>
		<abstract>
			<par><![CDATA[<p>Several representative papers are discussed in each part of the course notes and I summarize here a selection of these papers, in order to provide a short document offering a quick roadmap of the relevant literature.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Computations on discrete structures</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098762</person_id>
				<author_profile_id><![CDATA[81100215070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kallmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Rose et al 2001} C. Rose III, P-P. Sloan, and M. Cohen, "Artist-Directed Inverse-Kinematics Using Radial Basis Function Interpolation", Computer graphics Forum, 20(3), 2001, 239--250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015760</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Kovar and Gleicher 2004} L. Kovar and M. Gleicher, "Automated Extraction and Parameterization of Motions in Large Data Sets", ACM Trans. on Graphics (Proc. of SIGGRAPH 2004).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276510</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Safonova and Hodgins 2007} A. Safonova and J. Hodgins, "Construction and Optimal Search of Interpolated Motion Graphs", ACM Trans. on Graphics (Proc. of SIGGRAPH 2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230123</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Heck and Gleicher 2007} R. Heck and M. Gleicher, "Parametric Motion Graphs", Proc. of Interactive 3D Graphics and Games (I3D 2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Maciejewski and Klein 1985} A. Maciejewski and C. Klein, "Obstacle Avoidance for Kinematically Redundant Manipulators in Dynamically Varying Environments," Intl. Journal of Robotics Research, 4(3), 1985, 109--117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Baerlocher 2001} P. Baerlocher, "Inverse Kinematics Techniques for the Interactive Posture Control of Articulated Figures", PhD Thesis 2383, Swiss Federal Institute of Technology (EPFL), 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Burns and Brock 2005} B. Burns and O. Brock, "Sampling-Based Motion Planning Using Predictive Models", In Proc. of ICRA, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Jiang and Kallmann 2007} X. Jiang and M. Kallmann, "Learning Humanoid Reaching Tasks in Dynamic Environments", In Proc. of IROS, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401202</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<pages>10</pages>
		<display_no>51</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Motion graphs]]></title>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/1401132.1401202</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401202</url>
		<abstract>
			<par><![CDATA[<p>In this paper we present a novel method for creating realistic, controllable motion. Given a corpus of motion capture data, we automatically construct a directed graph called a <i>motion graph</i> that encapsulates connections among the database. The motion graph consists both of pieces of original motion and automatically generated transitions. Motion can be generated simply by building walks on the graph. We present a general framework for extracting particular graph walks that meet a user's specifications. We then show how this framework can be applied to the specific problem of generating different styles of locomotion along arbitrary paths.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation with constraints]]></kw>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motion synthesis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098763</person_id>
				<author_profile_id><![CDATA[81100314305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lucas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kovar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin-Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098764</person_id>
				<author_profile_id><![CDATA[81100342764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gleicher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin-Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098765</person_id>
				<author_profile_id><![CDATA[81100026067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fr&#233;d&#233;ric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pighin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566606</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arikan, O., and Forsythe, D. 2002. Interactive motion generation from examples. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bowden, R. 2000. Learning statistical models of human motion. In <i>IEEE Workshop on Human Modelling, Analysis, and Synthesis, CVPR 2000</i>, IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Brand, M., and Hertzmann, A. 2000. Style machines. In <i>Proceedings of ACM SIGGRAPH 2000</i>, Annual Conference Series, ACM SIGGRAPH, 183--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>241078</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bruderlin, A., and Calvert, T. 1996. Knowledge-driven, interactive animation of human running. In <i>Graphics Interface</i>, Canadian Human-Computer Communications Society, 213--221.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bruderlin, A., and Williams, L. 1995. Motion signal processing. In <i>Proceedings of ACM SIGGRAPH 95</i>, Annual Conference Series, ACM SIGGRAPH, 97--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383287</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Faloutsos, P., van de Panne, M., and Terzopoulos, D. 2001. Composable controllers for physics-based character animation. In <i>Proceedings of ACM SIGGRAPH 2001</i>, Annual Conference Series, ACM SIGGRAPH, 251--260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>377412</ref_obj_id>
				<ref_obj_pid>376890</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Galata, A., Jognson, N., and Hogg, D. 2001. Learning variable-length markov models of behavior. <i>Computer Vision and Image Understanding Journal 81</i>, 3, 398--413.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gleicher, M. 1998. Retargeting motion to new characters. In <i>Proceedings Of ACM SIGGRAPH 98</i>, Annual Conference Series, ACM SIGGRAPH, 33--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364400</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Gleicher, M. 2001. Motion path editing. In <i>Proceedings 2001 ACM Symposium on Interactive 3D Graphics</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hodgins, J. K., Wooten, W. L., Brogan, D. C., and O'Brien, J. F. 1995. Animating human athletics. In <i>Proceedings of ACM SIGGRAPH 95</i>, Annual Conference Series, ACM SIGGRAPH, 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kovar, L., Gleicher, M., and Schreiner, J. 2002. Footskate cleanup for motion capture editing. Tech. rep., University of Wisconsin, Madison.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274990</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Lamouret, A., and Panne, M. 1996. Motion synthesis by example. <i>Computer animation and Simulation</i>, 199--212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Lee, J., and Shin, S. Y. 1999. A hierarchical approach to interactive motion editing for human-like figures. In <i>Proceedings of ACM SIGGRAPH 99</i>, Annual Conference Series, ACM SIGGRAPH, 39--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566607</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Lee, J., Chai, J., Reitsma, P. S. A., Hodgins, J. K., and Pollard, N. S. 2002. Interactive control of avatars animated with human motion data. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lee, J. 2000. <i>A hierarchical approach to motion analysis and synthesis for articulated figures</i>. PhD thesis, Department of Computer Science, Korea Advanced Institute of Science and Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566604</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Li, Y., Wang, T., and Shum, H.-Y. 2002. Motion texture: A two-level statistical model for character motion synthesis. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Mizuguchi, M., Buchanan, J., and Calvert, T. 2001. Data driven motion transitions for interactive games. In <i>Eurographics 2001 Short Presentations</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>823423</ref_obj_id>
				<ref_obj_pid>822088</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Molina-Tanco, L., and Hilton, A. 2000. Realistic synthesis of novel human movements from a database of motion capture examples. In <i>Proceedings of the Workshop on Human Motion</i>, IEEE Computer Society, 137--142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Multon, F., France, L., Cani, M.-P., and Debunne, G. 1999. Computer animation of human walking: a survey. <i>The Journal of Visualization and Computer Animation 10</i>, 39--54. Published under the name Marie-Paule Cani-Gascuel.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237258</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Perlin, K., and Goldberg, A. 1996. Improv: A system for scripting interactive actors in virtual worlds. In <i>Proceedings of ACM SIGGRAPH 96</i>, ACM SIGGRAPH, 205--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. 1995. Real time responsive animation with personality. <i>IEEE Transactions on Visualization and Computer Graphics 1</i>, 1 (Mar.), 5--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872897</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Pullen, K., and Bregler, C. 2000. Animating by multi-level sampling. In <i>IEEE Computer Animation Conference</i>, CGS and IEEE, 36--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566608</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Pullen, K., and Bregler, C. 2002. Motion capture assisted animation: Texturing and synthesis. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237229</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Rose, C., Guenter, B., Bodenheimer, B., and Cohen, M. F. 1996. Efficient generation of motion transitions using spacetime constraints. In <i>Proceedings of ACM SIGGRAPH 1996</i>, Annual Conference Series, ACM SIGGRAPH, 147--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Rose, C., Cohen, M., and Bodenheimer, B. 1998. Verbs and adverbs: Multidimensional motion interpolation. <i>IEEE Computer Graphics and Application 18</i>, 5, 32--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Sch&#246;dl, A., Szeliski, R., Salesin, D., and Essa, I. 2000. Video textures. In <i>Proceedings of ACM SIGGRAPH 2000</i>, Annual Conference Series, ACM SIGGRAPH, 489--498.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383288</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Sun, H. C., and Metaxas, D. N. 2001. Automating gait animation. In <i>Proceedings of ACM SIGGRAPH 2001</i>, Annual Conference Series, ACM SIGGRAPH, 261--270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Washburn, D. 2001. The quest for pure motion capture. <i>Game Developer</i> (December).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618477</ref_obj_id>
				<ref_obj_pid>616049</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Wiley, D., and Hahn, J. 1997. Interpolation synthesis of articulated figure motion. <i>IEEE Computer Graphics and Application 17</i>, 6, 39--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Witkin, A., and Popovi&#263;, Z. 1995. Motion warping. In <i>Proceedings of ACM SIGGRAPH 95</i>, Annual Conference Series, ACM SIGGRAPH, 105--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NSF</funding_agency>
			<grant_numbers>
				<grant_number>CCR-9984506IIS-0097456</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401203</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<pages>9</pages>
		<display_no>52</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Snap-together motion]]></title>
		<subtitle><![CDATA[assembling run-time animations]]></subtitle>
		<page_from>1</page_from>
		<page_to>9</page_to>
		<doi_number>10.1145/1401132.1401203</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401203</url>
		<abstract>
			<par><![CDATA[<p>Many virtual environments and games must be populated with synthetic characters to create the desired experience. These characters must move with sufficient realism, so as not to destroy the visual quality of the experience, yet be responsive, controllable, and efficient to simulate. In this paper we present an approach to character motion called <i>Snap-Together Motion</i> that addresses the unique demands of virtual environments. Snap-Together Motion (STM) pre-processes a corpus of motion capture examples into a set of short clips that can be concatenated to make continuous streams of motion. The result process is a simple graph structure that facilitates efficient planning of character motions. A user-guided process selects "common" character poses and the system automatically synthesizes multi-way transitions that connect through these poses. In this manner well-connected graphs can be constructed to suit a particular application, allowing for practical interactive control without the effort of manually specifying all transitions.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motion synthesis]]></kw>
			<kw><![CDATA[virtual environments]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098766</person_id>
				<author_profile_id><![CDATA[81100342764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gleicher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin, Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098767</person_id>
				<author_profile_id><![CDATA[81421593450]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hyun]]></first_name>
				<middle_name><![CDATA[Joon]]></middle_name>
				<last_name><![CDATA[Shin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin, Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098768</person_id>
				<author_profile_id><![CDATA[81100314305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lucas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kovar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin, Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098769</person_id>
				<author_profile_id><![CDATA[81100263010]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jepsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin, Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566606</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Okan Arikan and D. A. Forsythe. Interactive motion generation from examples. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Matthew Brand and Aaron Hertzmann. Style machines. In <i>Proceedings of ACM SIGGRAPH 2000</i>, Annual Conference Series, pages 183--192, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218421</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Armin Bruderlin and Lance Williams. Motion signal processing. In <i>Proceedings of ACM SIGGRAPH 95</i>, Annual Conference Series, pages 97--104, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher. Retargeting motion to new characters. In <i>Proceedings Of ACM SIGGRAPH 98</i>, Annual Conference Series, pages 33--42, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364400</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher. Motion path editing. In <i>Proceedings 2001 ACM Symposium on Interactive 3D Graphics</i>, March 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Jessica K. Hodgins, Wayne L. Wooten, David C. Brogan, and James F. O'Brien. Animating human athletics. In <i>Proceedings of ACM SIGGRAPH 95</i>, Annual Conference Series, pages 71--78, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218486</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Myoung-Jun Kim, Myoung-Soo Kim, and Sung Yong Shin. A general construction scheme for unit quaternion curves with simple high order derivatives. In <i>Proceedings of ACM SIGGRAPH 1996</i>, Annual Conference Series, pages 369--376, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Lucas Kovar, Michael Gleicher, and Fred Pighin. Motion graphs. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545277</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Lucas Kovar, John Schreiner, and Michael Gleicher. Footskate cleanup for motion capture editing. In <i>ACM Symposium on Computer Animation 2002</i>, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566607</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jehee Lee, Jinxiang Chai, Paul S. A. Reitsma, Jessica K. Hodgins, and Nancy S. Pollard. Interactive control of avatars animated with human motion data. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311539</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Jehee Lee and Sung Yong Shin. A hierarchical approach to interactive motion editing for human-like figures. In <i>Proceedings of ACM SIGGRAPH 99</i>, Annual Conference Series, pages 39--48, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566604</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Yan Li, Tianshu Wang, and Heung-Yeung Shum. Motion texture: A two-level statistical model for character motion synthesis. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566596</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[C. Karen Liu and Zoran Popovi&#263;. Synthesis of complex dynamic character motion from simple animations. In <i>Proceedings of ACM SIGGRAPH 2002</i>, Annual Conference Series, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>554939</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Alberto Menache. <i>Understanding Motion Capture for Computer Animation and Video Games</i>. Academic Press, San Diego, CA, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Mark Mizuguchi, John Buchanan, and Tom Calvert. Data driven motion transitions for interactive games. In <i>Eurographics 2001 Short Presentations</i>, September 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545279</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Sang Il Park, Hyun Joon Shin, and Sung Yong Shin. On-line locomotion generation based on motion blending. In <i>ACM Symposium on Computer Animation 2002</i>, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin. Real time responsive animation with personality. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 1(1):5--15, March 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237258</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin and Athomas Goldberg. Improv: A system for scripting interactive actors in virtual worlds. In <i>Proceedings of ACM SIGGRAPH 96</i>, pages 205--216, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311536</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Zoran Popovi&#263; and Andrew Witkin. Physically based motion transformation. In <i>Proceedings of ACM SIGGRAPH 99</i>, Annual Conference Series, pages 11--20, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[C. Rose, M. Cohen, and B. Bodenheimer. Verbs and adverbs: Multidimensional motion interpolation. <i>IEEE Computer Graphics and Application</i>, 18(5):32--40, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345012</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Arno Schodl, Richard Szeliski, David H. Salesin, and Irfan Essa. Video textures. In <i>Proceedings of ACM SIGGRAPH 2000</i>, Annual Conference Series, pages 489--498, August 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618477</ref_obj_id>
				<ref_obj_pid>616049</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[D. Wiley and J. Hahn. Interpolation synthesis of articulated figure motion. <i>IEEE Computer Graphics and Application</i>, 17(6):39--45, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and Zoran Popovi&#263;. Motion warping. In <i>Proceedings of ACM SIGGRAPH 95</i>, Annual Conference Series, pages 105--108, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NSF</funding_agency>
			<grant_numbers>
				<grant_number>CCR-9984506CCR-0204372</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401204</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<pages>22</pages>
		<display_no>53</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Animation planning for virtual characters cooperation]]></title>
		<page_from>1</page_from>
		<page_to>22</page_to>
		<doi_number>10.1145/1401132.1401204</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401204</url>
		<abstract>
			<par><![CDATA[<p>This paper presents an approach to automatically compute animations for virtual (human-like and robot) characters cooperating to move bulky objects in cluttered environments. The main challenge is to deal with 3D collision avoidance while preserving the believability of the agent's behaviors. To accomplish the coordinated task, a geometric and kinematic decoupling of the system is proposed. This decomposition enables us to plan a collision-free path for a reduced system, then to animate locomotion and grasping behaviors independently, and finally to automatically tune the animation to avoid residual collisions. These three steps are applied consecutively to synthesize an animation. The different techniques used, such as probabilistic path planning, locomotion controllers, inverse kinematics and path planning for closed kinematic chains are explained, and the way to integrate them into a single scheme is described.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[autonomous characters]]></kw>
			<kw><![CDATA[behavior modeling]]></kw>
			<kw><![CDATA[motion control]]></kw>
			<kw><![CDATA[motion planning]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098770</person_id>
				<author_profile_id><![CDATA[81314489092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Claudia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Esteves]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratoire d'Analyse et d'Architecture des Syst&#232;mes, Toulouse, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098771</person_id>
				<author_profile_id><![CDATA[81350598218]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gustavo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arechavaleta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratoire d'Analyse et d'Architecture des Syst&#232;mes, Toulouse, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098772</person_id>
				<author_profile_id><![CDATA[81100306359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pettr&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratoire d'Analyse et d'Architecture des Syst&#232;mes, Toulouse, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098773</person_id>
				<author_profile_id><![CDATA[81100468739]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jean-Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laumond]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratoire d'Analyse et d'Architecture des Syst&#232;mes, Toulouse, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>514264</ref_obj_id>
				<ref_obj_pid>514236</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Badler, N., Erignac, C., and Liu, Y. 2002. Virtual humans for validating maintenance procedures. <i>Commun. ACM 45</i>, 7, 56--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>162261</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Badler, N. I., Phillips, C., and Webber, B. 1993. <i>Simulating Humans: Computer Graphics Animation and Control</i>. Oxford University Press, Inc., New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1016493</ref_obj_id>
				<ref_obj_pid>1016488</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baerlocher, P. and Boulic, R. 2004. An inverse kinematics architecture enforcing and arbitrary number of strict priority levels. <i>Visual Comput. 20</i>, 402--417.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218405</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blumberg, B. and Galyean, T. 1995. Multi-level direction of autonomous creatures for real-time virtual environments. <i>Comput. Graph. 29</i> (Annual Conference Series), 47--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344865</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Brand, M. and Hertzmann, A. 2000. Style machines. In <i>Proceedings of SIGGRAPH</i>. ACM Press/Addison-Wesley Publishing Co., 183--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>636889</ref_obj_id>
				<ref_obj_pid>636886</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Choi, M., Lee, J., and Shin, S. 2003. Planning biped locomotion using motion capture data and probabilistic roadmaps. <i>ACM Trans. Graph. 22</i>, 2, 182--203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cort&#233;s, J. and Sim&#233;on, T. 2003. Probabilistic motion planning for parallel mechanisms. In <i>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618558</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Earnshaw, R., Magnenat-Thalmann, N., Terzopoulos, D., and Thalmann, D. 1998. Guest editors' introduction: Computer animation for virtual humans. <i>IEEE Comput. Graph. App. 18</i>, 5, 20--23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383287</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Faloutsos, P., van de Panne, M., and Terzopoulos, D. 2001. Composable controllers for physics-based character animation. In <i>Proceedings of SIGGRAPH</i>. ACM Press, 251--260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Han, L. and Amato, N. 2000. A kinematics-based probabilistic roadmap method for closed chain systems. In <i>Proceedings of the International Workshop on Algorithmic Foundations of Robotics (WAFR)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401209</ref_obj_id>
				<ref_obj_pid>1401132</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kallman, M., Aubel, A., Abaci, T., and Thalmann, D. 2003. Planning collision-free reaching motions for interactive object manipulation and grasping. In <i>Proceedings of ACM SIGGRAPH/Eurographics</i>, P. Brunet and D. Fellner, Eds. Vol. 22. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kallmann, M., Bargmann, R., and Mataric, M. 2004. Planning the sequencing of movement primitives. In <i>Proceedings of the International Conference on Simulation of Adaptive Behavior (SAB)</i>. Los Angeles, CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kavraki, L. E., Svestka, P., Latombe, J.-C., and Overmars, M. 1996. Probabilistic roadmaps for path planning in high-dimensional configuration spaces. <i>IEEE Trans. Robotics and Automat. 12</i>, 4, 566--580.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Koga, Y., Kondo, K., Kuffner, J., and Latombe, J.-C. 1994. Planning motions with intentions. In <i>Proceedings of SIGGRAPH</i>. ACM Press, New York, NY, 395--408.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Kondo, K. 1991. Inverse kinematics of a human arm. <i>J. Robotics Syst. 8</i>, 2, 115--175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Kovar, L., Gleicher, M., and Pighin, F. 2002. Motion graphs. In <i>Proceedings of SIGGRAPH</i>. ACM Press, New York, NY, 473--482.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Kuffner, J. 1998. Goal-directed navigation for animated characters using real-time path planning and control. Lecture Notes in Computer Science vol. 1537, 171--186.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>662216</ref_obj_id>
				<ref_obj_pid>645625</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Lamiraux, F. and Laumond, J.-P. 1997. From paths to trajectories for multi-body mobile robots. In <i>Proceedings of the 5th International Symposium on Experimental Robotics (ISER)</i>. Barcelona, Spain, 237--245.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>532147</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Latombe, J.-C. 1991. <i>Robot Motion Planning</i>. Kluwer Academic Press, Boston, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>521883</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Laumond, J.-P., Ed. 1998. <i>Robot Motion Planning and Control</i>. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[LaValle, S. M. 1998. Rapidly-exploring random trees: A new tool for path planning. Tech. rep., Computer Science Department. Iowa State University. (Oct).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[LaValle, S. M., Yakey, J., and Kavraki, L. E. 1999. A probabilistic roadmap approach for systems with closed kinematic chains. In <i>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791872</ref_obj_id>
				<ref_obj_pid>791221</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Liu, Y. and Badler, N. I. 2003. Real-time reach planning for animated characters using hardware acceleration. In <i>Proceedings of the International Conference on Computer Animation and Social Agents (CASA)</i>. Washington, DC, IEEE Computer Society, 86.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500923</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Parent, R. 2001. <i>Computer Animation: Algorithms and Techniques</i>. Morgan-Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. 1995. Real time responsive animation with personality. <i>IEEE Trans. Visualiz. Comput. Graph. 1</i>, 1, 5--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1133119</ref_obj_id>
				<ref_obj_pid>1133115</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Pettr&#233;, J. and Laumond, J.-P. 2005. A motion capture-based control-space approach for walking mannequins. <i>Comput. Animat. Virtual Worlds 16</i>, 1--18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846313</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Pettr&#233;, J., Laumond, J.-P., and Sime&#243;n, T. 2003. A 2-stages locomotion planner for digital actors. In <i>Proceedings of ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)</i>. San Diego, CA, 258--264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Reeds, J. and Shepp, R. 1990. Optimal paths for a car that goes both forward and backwards. <i>Pacific J. Mathemat. 145</i>, 2, 367--393.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Renaud, M. 2000. A simplified inverse kinematic model calculation method for all 6r type manipulators. In <i>Proceedings of the International Conference in Mechanical Design and Production</i>. 15--25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618560</ref_obj_id>
				<ref_obj_pid>616054</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Rose, C., Cohen, M., and Bodenheimer, B. 1998. Verbs and adverbs: Multidimensional motion interpolation. <i>IEEE Comput. Graph. Appli. 18</i>, 5, 32--41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Shiller, Z., Yamane, K., and Nakamura, Y. 2001. Planning motion patterns of human figures using a multi-layered grid and the dynamics filter. In <i>Proceedins of the IEEE International Conference on Robotics and Automation (ICRA)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Sim&#233;on, T., Laumond, J.-P., and Lamiraux, F. 2001. Move3d: A generic platform for motion planning. In <i>Proceedings of the 4th International Symposium on Assembly and Task Planning (ISATP)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Sim&#233;on, T., Laumond, J.-P., and Nissoux, C. 2000. Visibility based probabilistic roadmaps for motion planning. <i>Advanced Robotics 14</i>, 6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358832</ref_obj_id>
				<ref_obj_pid>358829</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Tolani, D., Goswami, A., and Badler, N. I. 2000. Real-time inverse kinematics techniques for anthropomorphic limbs. <i>Graphical Models 62</i>, 353--388.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218419</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Unuma, M., Anjyo, K., and Takeuchi, R. 1995. Fourier principles for emotion-based human figure animation. In <i>Proceedings of SIGGRAPH</i>. ACM Press, 91--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Witkin, A. and Popovic, Z. 1995. Motion warping. In <i>Proceedings of SIGGRAPH'95</i>. ACM Press, 105--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015756</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Yamane, K., Kuffner, J., and Hodgins, J. K. 2004. Synthesizing animations of human manipulation tasks. In <i>Proceedings of SIGGRAPH</i>. ACM Press, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231892</ref_obj_id>
				<ref_obj_pid>2231876</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Yamane, K. and Nakamura, Y. 2003. Natural motion animation through constraining and deconstraining at will. <i>IEEE Trans. Visualiz. Comput. Graph. 9</i>, 3, 352--360.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>195827</ref_obj_id>
				<ref_obj_pid>195826</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Zhao, J. and Badler, N. I. 1994. Inverse kinematics positioning using nonlinear programming for highly articulated figures. <i>ACM Trans. Graph. 14</i>, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>European Community</funding_agency>
			<grant_numbers>
				<grant_number>FP5 IST 2001-39250 MovieFP6 IST 002020 Cogniron</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401205</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<pages>7</pages>
		<display_no>54</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Crowds of moving objects]]></title>
		<subtitle><![CDATA[navigation planning and simulation]]></subtitle>
		<page_from>1</page_from>
		<page_to>7</page_to>
		<doi_number>10.1145/1401132.1401205</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401205</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a solution to interactive navigation planning and real-time simulation of a very large number of entities moving in a virtual environment. From the environment geometry analysis, we deduce a structure called <i>navigation graph</i>, which is the base to our method. After the description of this structure, we introduce a set of algorithms dedicated to answer navigation queries with a set of various solution paths and to execute the planned navigation in an efficient manner. We equally demonstrate method performance and robustness over several examples.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.4</cat_node>
				<descriptor>Navigation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10003254</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Hypertext / hypermedia</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510.10010515</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation->Multi / mixed media creation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098774</person_id>
				<author_profile_id><![CDATA[81100306359]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pettr&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA-INRIA, Campus de Beaulieu, Rennes, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098775</person_id>
				<author_profile_id><![CDATA[81365594004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Helena]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grillon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[EPFL-VRlab, Lausanne, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098776</person_id>
				<author_profile_id><![CDATA[81100534488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thalmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[EPFL-VRlab, Lausanne, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>532147</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J.-C. Latombe. <i>Robot Motion Planning</i>. Boston: Kluwer Academic Publishers, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>521883</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jean-Paul P. Laumond. <i>Robot Motion Planning and Control</i>. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1213331</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[S. M. LaValle. <i>Planning Algorithms</i>. Cambridge University Press, Cambridge, U.K., 2006. Available at http://planning.cs.uiuc.edu/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Xuejun Sheng. Motion planning for computer animation and virtual reality applications. <i>CA</i>, 00:56, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>6812</ref_obj_id>
				<ref_obj_pid>6806</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[O. Khatib. Real-time obstacle avoidance for manipulators and mobile robots. <i>International Journal of Robotics Research</i>, 5(1):90--98, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618355</ref_obj_id>
				<ref_obj_pid>616041</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Parris K. Egbert and Scott H. Winkler. Collision-free object movement using vector fields. <i>IEEE Computer Graphics and Applications</i>, 16(4):18--24, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1309766</ref_obj_id>
				<ref_obj_pid>1309288</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Tomas Lozano-Perez. Spatial planning: A configuration space approach. <i>IEEE Transactions on Computers</i>, 32(2):108--120, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[L. Kavraki, P. Svestka, J.-C. Latombe, and M. Overmars. Probabilistic roadmaps for path planning in high-dimensional configuration spaces. <i>Proceedings of IEEE Transactions on Robotics and Automation</i>, pages 566--580, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[James Kuffner and Steven LaValle. Rrt-connect: An efficient approach to single-query path planning. In <i>IEEE International Conference on Robotics and Automation</i>, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>636889</ref_obj_id>
				<ref_obj_pid>636886</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. G. Choi, J. Lee, and S. Y. Shin. Planning biped locomotion using motion capture data and probabilistic roadmaps. <i>SIGGRAPH'03: ACM Transactions on Graphics</i>, 22(2):182--203, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846313</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Julien Pettr&#233;, Jean Paul Laumond, and Thierry Sim&#233;on. A 2-stages locomotion planner for digital actors. <i>SCA'03: Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, pages 258--264, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028526</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[A. Kamphuis and M. H. Overmars. Finding paths for coherent groups using clearance. <i>SCA'04: Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, pages 19--28, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073410</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[M. Sung, L. Kovar, and M. Gleicher. Fast and accurate goal-directed motion synthesis for crowds. <i>SCA'05: Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, pages 291--300, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>515547</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Steve Rabin. <i>Al Game Programming Wisdom</i>. Charles River Media, Inc., Rockland, MA, USA, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[James Kuffner. Goal-directed navigation for animated characters using real-time path planning and control. <i>CAPTECH</i>, pages 171--186, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. Bohlin. Path planning in practice; lazy evaluation on a multi-resolution grid. In <i>Proceedings IEEE/RSJ International Conference on Intelligent Robots and Systems</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073371</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[W. Shao and D. Terzopoulos. Autonomous pedestrians. <i>SCA'05: Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, pages 19--28, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[F. Lamarche and S. Donikian. Crowds of virtual humans: a new approach for real time navigation in complex and structured environments. <i>Eurographics'04: Computer Graphics Forum</i>, 23(3):509--518, September 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>285891</ref_obj_id>
				<ref_obj_pid>285869</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[S. J. Fortune. Voronoi diagrams and delaunay triangulations. <i>CRC Handbook of Discrete and Computational Geometry</i>, pages 377--388, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1144487</ref_obj_id>
				<ref_obj_pid>1144457</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Julien Pettr&#233;, Pablo de Heras Ciechomski, Jonathan Ma&#239;m, Barbara Yersin, Jean-Paul Laumond, and Daniel Thalmann. Real-time navigating crowds: scalable simulation and rendering: Research articles. <i>Comput. Animat. Virtual Worlds</i>, 17(3--4):445--455, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[C. W. Reynolds. Steering behaviors for autonomous characters. <i>Proc. of Game Developers Conference</i>, pages 763--782, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401206</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<pages>9</pages>
		<display_no>55</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Real-time path planning for virtual agents in dynamic environments]]></title>
		<page_from>1</page_from>
		<page_to>9</page_to>
		<doi_number>10.1145/1401132.1401206</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401206</url>
		<abstract>
			<par><![CDATA[<p>We present a novel approach for real-time path planning of multiple virtual agents in complex dynamic scenes. We introduce a new data structure, <i>Multi-agent Navigation Graph</i> (MaNG), which is constructed from the first- and second-order Voronoi diagrams. The MaNG is used to perform route planning and proximity computations for each agent in real time. We compute the MaNG using graphics hardware and present culling techniques to accelerate the computation. We also address undersampling issues for accurate computation. Our algorithm is used for real-time multi-agent planning in pursuit-evasion and crowd simulation scenarios consisting of hundreds of moving agents, each with a distinct goal.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[crowd simulation]]></kw>
			<kw><![CDATA[motion planning]]></kw>
			<kw><![CDATA[voronoi diagram]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098777</person_id>
				<author_profile_id><![CDATA[81100512563]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Avneesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sud]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098778</person_id>
				<author_profile_id><![CDATA[81332488226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andersen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098779</person_id>
				<author_profile_id><![CDATA[81335489690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curtis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098780</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098781</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>116880</ref_obj_id>
				<ref_obj_pid>116873</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[F. Aurenhammer. Voronoi diagrams: A survey of a fundamental geometric data structure. <i>ACM Comput. Surv.</i>, 23(3):345--405, Sept. 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826615</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[O. B. Bayazit, J.-M. Lien, and N. M. Amato. Better group behaviors in complex environments with global roadmaps. <i>Int. Conf. on the Sim. and Syn. of Living Sys. (Alife)</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Bennewitz and W. Burgard. Finding solvable priority schemes for decoupled path planning techniquesfor teams of mobile robots. <i>Proceedings of the 9th Int. Symposium on Intelligent Robotic Systems (SIRS)</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Champagne and W. Tang. Real-time simulation of crowds using voronoi diagrams. <i>EG UK Theory and Practice of Computer Graphics</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H. Choset and J. Burdick. Sensor based motion planning: The hierarchical generalized Voronoi graph. In <i>Algorithms for Robot Motion and Manipulation</i>, pages 47--61. A K Peters, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[H. Choset, K. Lynch, S. Hutchinson, G. Kantor, W. Burgard, L. Kavraki, and S. Thrun. <i>Principles of Robot Motion: Theory, Algorithms, and Implementations</i>. MIT Press, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[O. C. Cordeiro, A. Braun, C. B. Silveria, S. R. Musse, and G. G. Cavalheiro. Concurrency on social forces simulation model. <i>First International Workshop on Crowd Simulation</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Denny. Solving geometric optimization problems using graphics hardware. In <i>Proc. of Eurographics</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[I. Fischer and C. Gotsman. Fast approximation of high order Voronoi diagrams and distance transforms on the GPU. Technical report CS TR-07-05, Harvard University, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Foskey, M. Garber, M. Lin, and D. Manocha. A voronoi-based hybrid planner. <i>Proc. of IEEE/RSJ Int. Conf. on Intelligent Robots and Systems</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311538</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Funge, X. TU, and D. Terzopoulos. Cognitive modeling: Knowledge, reasoning and planning for intelligent characters. <i>Proc. of ACM SIGGRAPH</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1252471</ref_obj_id>
				<ref_obj_pid>1251973</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[P. Glardon, R. Boulic, and D. Thalmann. Dynamic obstacle clearing for real-time character animation. <i>Computer Graphics International</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[L. Guibas, C. Holleman, and L. Kavraki. A probabilistic roadmap planner for flexible objects with a workspace medial-axis-based sampling approach. In <i>Proc. of IROS</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D. Helbing, L. Buzna, and T. Werner. Self-organized pedestrian crowd dynamics and design solutions. <i>Traffic Forum 12</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311567</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[K. Hoff, T. Culver, J. Keyser, M. Lin, and D. Manocha. Fast computation of generalized voronoi diagrams using graphics hardware. <i>Proceedings of ACM SIGGRAPH 1999</i>, pages 277--286, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[K. Hoff, T. Culver, J. Keyser, M. Lin, and D. Manocha. Interactive motion planning using hardware accelerated computation of generalized voronoi diagrams. <i>IEEE Conference on Robotics and Automation</i>, pages pp. 2931--2937, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364383</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[K. Hoff, A. Zaferakis, M. Lin, and D. Manocha. Fast and simple 2d geometric proximity queries using graphics hardware. <i>Proc. of ACM Symposium on Interactive 3D Graphics</i>, pages 145--148, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028526</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[A. Kamphuis and M. Overmars. Finding paths for coherent groups using clearance. <i>Proc. of ACM SIGGRAPH / Eurographics Symposium on Computer Animation</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[F. Lamarche and S. Donikian. Crowd of virtual humans: a new approach for real-time navigation in complex and structured environments. <i>Computer Graphics Forum</i>, 23(3 (Sept)), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>532147</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[J.-C. Latombe. <i>Robot Motion Planning. Kluwer Academic Publishers</i>, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[T.-T. Li and H.-C. Chou. Motion planning for a crowd of robots. <i>Proc. of IEEE Int. Conf. on Robotics and Automation</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>833408</ref_obj_id>
				<ref_obj_pid>832264</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[C. Loscos, D. Marchal, and A. Meyer. Intuitive crowd behaviour in dense urban environments using local laws. <i>Theory and Practice of Computer Graphics (TPCG'03)</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[S. R. MUSSE and D. Thalmann. A model of human crowd behavior: Group inter-relationship and collision detection analysis. <i>Computer Animation and Simulation</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>135734</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[A. Okabe, B. Boots, and K. Sugihara. <i>Spatial tessellations: concepts and applications of Voronoi diagrams</i>. Wiley &amp; Sons, 1992. ISBN 0 471 93430 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[L. E. PARKER. Designing control laws for cooperative agent teams. <i>Proc. of IEEE Int. Conf. on Robotics and Automation</i>, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[N. Pelechano, K. O'Brien, B. Silverman, and N. Badler. Crowd simulation incorporating agent psychological models, roles and communication. <i>First International Workshop on Crowd Simulation</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. Pettre, J.-P. Laumond, and D. Thalmann. A navigation graph for real-time crowd animation on multilayered and uneven terrain. <i>First International Workshop on Crowd Simulation</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[C. W. Reynolds. Flocks, herds, and schools: A distributed behavioral model. In M. C. Stone, editor, <i>Computer Graphics (SIGGRAPH '87 Proceedings)</i>, volume 21, pages 25--34, July 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[M. SOFTWARE. http://www.massivesoftware.com, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[G. Still. <i>Crowd Dynamics</i>. PhD thesis, University of Warwik, UK, 2000. Ph.D. Thesis.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142006</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[A. Sud, N. Govindaraju, R. Gayle, I. Kabul, and D. Manocha. Fast proximity computation among deformable models using discrete voronoi diagrams. <i>ACM Trans. Graph. (Proc ACM SIGGRAPH)</i>, 25(3):1144--1153, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111432</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[A. Sud, N. Govindaraju, R. Gayle, and D. Manocha. Interactive 3d distance field computation using linear factorization. In <i>Proc. ACM Symposium on Interactive 3D Graphics and Games</i>, pages 117--124, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[A. Sud, M. A. Otaduy, and D. Manocha. DiFi: Fast 3D distance field computation using graphics hardware. <i>Computer Graphics Forum (Proc. Eurographics)</i>, 23(3):557--566, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[M. Sung, M. Gleicher, and S. Chenney. Scalable behaviors for crowd simulation. <i>Computer Graphics Forum</i>, 23(3 (Sept)), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073410</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[M. Sung, L. KOVAR, and M. Gleicher. Fast and accurate goal-directed motion synthesis for crowds. <i>Proc. of SCA 2005</i>, pages 291--300, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142008</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[A. Treuille, S. Cooper, and Z. Popovic. Continuum crowds. <i>Proc. of ACM SIGGRAPH</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192170</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[X. Tu and D. Terzopoulos. Artificial fishes: Physics, locomotion, perception, behavior. In A. Glassner, editor, <i>Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24--29, 1994)</i>, Computer Graphics Proceedings, Annual Conference Series, pages 43--50. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0-89791-667-0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[J. Vleugels and M. H. Overmars. Approximating Voronoi diagrams of convex sites in any dimension. <i>International Journal of Computational Geometry and Applications</i>, 8:201--222, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[S. A. Wilmarth, N. M. Amato, and P. F. Stiller. Maprm: A probabilistic roadmap planner with sampling on the medial axis of the free space. <i>IEEE Conference on Robotics and Automation</i>, pages 1024--1031, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401207</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<pages>10</pages>
		<display_no>56</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Real-time navigation of independent agents using adaptive roadmaps]]></title>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/1401132.1401207</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401207</url>
		<abstract>
			<par><![CDATA[<p>We present a novel algorithm for navigating a large number of independent agents in complex and dynamic environments. We compute adaptive roadmaps to perform global path planning for each agent simultaneously. We take into account dynamic obstacles and inter-agents interaction forces to continuously update the roadmap by using a physically-based agent dynamics simulator. We also introduce the notion of 'link bands' for resolving collisions among multiple agents. We present efficient techniques to compute the guiding path forces and perform lazy updates to the roadmap. In practice, our algorithm can perform real-time navigation of hundreds and thousands of human agents in indoor and outdoor scenes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.11</cat_node>
				<descriptor>Intelligent agents</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010219.10010221</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Distributed artificial intelligence->Intelligent agents</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098782</person_id>
				<author_profile_id><![CDATA[81100512563]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Avneesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sud]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098783</person_id>
				<author_profile_id><![CDATA[81319492027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Russell]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gayle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098784</person_id>
				<author_profile_id><![CDATA[81332488226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andersen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098785</person_id>
				<author_profile_id><![CDATA[81365594733]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098786</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098787</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ashida, K., Lee, S. J., Allbeck, J., Sun, H., Badler, N., and Metaxas, D. 2001. Pedestrians: Creating agent behaviors through statistical analysis of observation data. <i>Proc. Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826615</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bayazit, O. B., Lien, J.-M., and Amato, N. M. 2002. Better group behaviors in complex environments with global roadmaps. <i>Int. Conf. on the Sim. and Syn. of Living Sys. (Alife)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bon, G. L. 1895. <i>The Crowd: A Study of the Popular Mind</i>. Reprint available from Dover Publications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cordeiro, O. C., Braun, A., Silveria, C. B., Musse, S. R., and Cavalheiro, G. G. 2005. Concurrency on social forces simulation model. <i>First International Workshop on Crowd Simulation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311538</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Funge, J., Tu, X., and Terzopoulos, D. 1999. Cognitive modeling: Knowledge, reasoning and planning for intelligent characters. <i>Proc. of ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Garaerts, R., and Overmars, M. H. 2007. The corridor map method: Real-time high-quality path planning. In <i>ICRA</i>, 1023--1028.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gayle, R., Lin, M., and Manocha, D. 2005. Constraint based motion planning of deformable robots. <i>IEEE Conf. on Robotics and Automation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gayle, R., Sud, A., Lin, M., and Manocha, D. 2007. Reactive deformation roadmaps: Motion planning of multiple robots in dynamic environments. In <i>Proc IEEE International Conference on Intelligent Robots and Systems</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Helbing, D., Buzna, L., and Werner, T. 2003. Self-organized pedestrian crowd dynamics and design solutions. <i>Traffic Forum 12</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1247227</ref_obj_id>
				<ref_obj_pid>1247226</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Helbing, D., Buzna, L., Johansson, A., and Werner, T. 2005. Self-organized pedestrian crowd dynamics: experiments, simulations and design solutions. <i>Transportation science</i>, 1--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hoogendoorn, S. P., Luding, S., Bovy, P., Schrecklenberg, M., and Wolf, D. 2000. <i>Traffic and Granular Flow</i>. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Jakobsen, T. 2001. Advanced character physics. In <i>Game Developer's Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028526</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kamphuis, A., and Overmars, M. 2004. Finding paths for coherent groups using clearance. <i>Proc. of ACM SIGGRAPH / Eurographics Symposium on Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>6812</ref_obj_id>
				<ref_obj_pid>6806</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Khatib, O. 1986. Real-time obstable avoidance for manipulators and mobile robots. <i>IJRR 5</i>, 1, 90--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lamarche, F., and Donikian, S. 2004. Crowd of virtual humans: a new approach for real-time navigation in complex and structured environments. <i>Computer Graphics Forum 23</i>, 3 (Sept).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1213331</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lavalle, S. M. 2006. <i>Planning Algorithms</i>. Cambridge University Press (also available at http://msl.cs.uiuc.edu/planning/).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Lerner, A., Chrysanthou, Y., and Lischinski, D. 2007. Crowds by example. <i>Computer Graphics Forum (Proceedings of Eurographics)</i> 26, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Li, Y., and Gupta, K. 2007. Motion planning of multiple agents in virtual environments on parallel architectures. In <i>ICRA</i>, 1009--1014.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>833408</ref_obj_id>
				<ref_obj_pid>832264</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Loscos, C., Marchal, D., and Meyer, A. 2003. Intuitive crowd behaviour in dense urban environments using local laws. <i>Theory and Practice of Computer Graphics (TPCG'03)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Musse, S. R., and Thalmann, D. 1997. A model of human crowd behavior: Group inter-relationship and collision detection analysis. <i>Computer Animation and Simulation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Pelechano, N., O'Brien, K., Silverman, B., and Badler, N. 2005. Crowd simulation incorporating agent psychological models, roles and communication. <i>First International Workshop on Crowd Simulation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Pettre, J., Laumond, J.-P., and Thalmann, D. 2005. A navigation graph for real-time crowd animation on multilayered and uneven terrain. <i>First International Workshop on Crowd Simulation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Quinlan, S., and Khatib, O. 1993. Elastic bands: Connecting path planning and control. <i>Proc. of IEEE Conf. on Robotics and Automation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. W. 1987. Flocks, herds, and schools: A distributed behavioral model. <i>Comput. Graph. 21</i>, 4, 25--34. Proc. SIGGRAPH '87.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1183333</ref_obj_id>
				<ref_obj_pid>1183316</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. 2006. Big fast crowds on ps3. In <i>sandbox '06: Proceedings of the 2006 ACM SIGGRAPH symposium on Videogames</i>, ACM Press, New York, NY, USA, 113--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Rodriguez, S., Lien, J.-M., and Amato, N. M. 2006. Planning motion in completely deformable environments. <i>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</i> (May).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Schreckkenberg, M., and Sharma, S. D. 2001. <i>Pedestrian and Evacuation Dynamics</i>. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073371</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Shao, W., and Terzopoulos, D. 2005. Autonomous pedestrians. In <i>SCA '05: Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, ACM Press, New York, NY, USA, 19--28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111432</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Sud, A., Govindaraju, N., Gayle, R., and Manocha, D. 2006. Interactive 3d distance field computation using linear factorization. In <i>Proc. ACM Symposium on Interactive 3D Graphics and Games</i>, 117--124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Sud, A., Andersen, E., Curtis, S., Lin, M., and Manocha, D. 2007. Realtime path planning for virtual agents in dynamic environments. <i>Proc. of IEEE VR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Sugiyama, Y., Nakayama, A., and Hasebe, K. 2001. 2-dimensional optimal velocity models for granular flows. In <i>Pedestrian and Evacuation Dynamics</i>, 155--160.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Sung, M., Gleicher, M., and Chenney, S. 2004. Scalable behaviors for crowd simulation. <i>Computer Graphics Forum 23</i>, 3 (Sept).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073410</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Sung, M., Kovar, L., and Gleicher, M. 2005. Fast and accurate goal-directed motion synthesis for crowds. <i>Proc. of SCA 2005</i>, 291--300.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Thalmann, D., O'Sullivan, C., Ciechomski, P., and Dobbyn, S. 2006. <i>Populating Virtual Environments with Crowds</i>. Eurographics 2006 Tutorial Notes.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142008</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Treuille, A., Cooper, S., and Popovic, Z. 2006. Continuum crowds. <i>Proc. of ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192170</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Tu, X., and Terzopoulos, D. Artificial fishes: Physics, locomotion, perception, behavior. In <i>Proceedings of SIGGRAPH '94</i>, A. Glassner, Ed., 43--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Verlet, L 1967. Computer experiments on classical fluids. I. Thermodynamical properties of Lennard-Jones molecules. <i>Phys. Rev.</i>, 159, 98--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Yang, Y., and Brock, O. 2006. Elastic roadmaps: Globally task-consistent motion for autonomous mobile manipulation. <i>Proceedings of Robotics: Science and Systems</i> (August).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Zucker, M., Kuffner, J., and Branicky, M. 2007. Multipartite rrts for rapid replanning in dynamic environments. <i>Proc. IEEE Int. Conf. on Robotics and Automation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401208</article_id>
		<sort_key>760</sort_key>
		<display_label>Article No.</display_label>
		<pages>9</pages>
		<display_no>57</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Interactive motion correction and object manipulation]]></title>
		<page_from>1</page_from>
		<page_to>8</page_to>
		<doi_number>10.1145/1401132.1401208</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401208</url>
		<abstract>
			<par><![CDATA[<p>Editing recorded motions to make them suitable for different sets of environmental constraints is a general and difficult open problem. In this paper we solve a significant part of this problem by modifying full-body motions with an interactive randomized motion planner. Our method is able to synthesize collision-free motions for specified linkages of multiple animated characters in synchrony with the characters' full-body motions. The proposed method runs at interactive speed for dynamic environments of realistic complexity. We demonstrate the effectiveness of our interactive motion editing approach with two important applications: (a) motion correction (to remove collisions) and (b) synthesis of realistic object manipulation sequences on top of locomotion.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[character animation]]></kw>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motion editing]]></kw>
			<kw><![CDATA[object manipulation]]></kw>
			<kw><![CDATA[virtual humans]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098788</person_id>
				<author_profile_id><![CDATA[81100431086]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shapiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098789</person_id>
				<author_profile_id><![CDATA[81100215070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kallmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Merced]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098790</person_id>
				<author_profile_id><![CDATA[81100370834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Petros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566606</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arikan, O., and Forsyth, D. A. 2002. Interactive motion generation from examples. In <i>SIGGRAPH '02: Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 483--490.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>338429</ref_obj_id>
				<ref_obj_pid>338401</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bandi, S., and Thalmann, D. 2000. Path finding for human motion in virtual environments. <i>Computational Geometry: Theory and Applications 15</i>, 1--3, 103--127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>636889</ref_obj_id>
				<ref_obj_pid>636886</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Choi, M. G., Lee, J., and Shin, S. Y. 2002. Planning biped locomotion using motion capture data and probabilistic roadmaps. <i>ACM Transactions on Graphics 22</i>, 2, 182--203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237244</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Gottschalk, S., Lin, M. C., and Manocha, D. 1996. Obbtree: A hierarchical structure for rapid interference detection. <i>Computer Graphics SIGGRAPH'96 30</i>, Annual Conference Series, 171--180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781241</ref_obj_id>
				<ref_obj_pid>781238</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Grassia, S. 1998. Practical parameterization of rotations using the exponential map. <i>Journal of Graphics Tools 3</i>, 3, 29--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Grochow, K., Martin, S., Hertzmann, A., and Popovi, Z., 2004. Style-based inverse kinematics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hsu, D., Kindel, R., Latombe, J., and Rock, S. 2002. Randomized kinodynamic motion planning with moving obstacles. <i>International Journal of Robotics Research 21</i>, 3, 233--255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kallmann, M., Aubel, A., Abaci, T., and Thalmann, D. 2003. Planning collision-free reaching motions for interactive object manipulation and grasping. <i>Computer graphics Forum (Proceedings of Eurographics'03) 22</i>, 3 (September), 313--322.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Koga, Y., Kondo, K., Kuffner, J. J., and Latombe, J.-C. 1994. Planning motions with intentions. In <i>Proceedings of SIGGRAPH'94</i>, ACM Press, 395--408.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>18944</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Korein, J. U. 1985. <i>A Geometric Investigation of Reach</i>. The MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kovar, L., Gleicher, M., and Pighin, F. 2002. Motion graphs. In <i>SIGGRAPH '02: Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 473--482.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826533</ref_obj_id>
				<ref_obj_pid>826029</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kuffner, J. J., and Latombe, J.-C. 2000. Interactive manipulation planning for animated characters. In <i>Proceedings of Pacific Graphics'00</i>. poster paper.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kuffner, J. J., and LaValle, S. M. 2000. Rrt-connect: An efficient approach to single-query path planning. In <i>Proceedings of IEEE Int'l Conference on Robotics and Automation (ICRA'00)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073373</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Kwon, T., and Shin, S. Y. 2005. Motion modeling for on-line locomotion synthesis. In <i>SCA '05: Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, ACM Press, New York, NY, USA, 29--38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073409</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lai, Y.-C., Chenney, S., and Fan, S. 2005. Group motion graphs. In <i>Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073408</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lau, M., and Kuffner, J. 2005. Behavior planning for character animation. In <i>Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LaValle, S., and Kuffner, J., 2000. Rapidly-exploring random trees: Progress and prospects. In Workshop on the Algorithmic Foundations of Robotics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LaValle, S. 1998. Rapidly-exploring random trees: A new tool for path planning. Tech. Rep. 98--11, Iowa State University, Computer Science Department, October.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791872</ref_obj_id>
				<ref_obj_pid>791221</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Liu, Y., and Badler, N. I. 2003. Real-time reach planning for animated characters using hardware acceleration. In <i>Proceedings of Computer Animation and Social Agents (CASA'03)</i>, 86--93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846313</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Pettr&#233;, J., Laumond, J.-P., and Sime&#243;n, T. 2003. A 2-stages locomotion planner for digital actors. In <i>Proceedings of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 258--264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Schwarzer, F., Saha, M., and Latombe, J.-C. 2002. Exact collision checking of robot paths. In <i>Proceedings of the Workshop on Algorithmic Foundations of Robotics (WAFR'02)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089519</ref_obj_id>
				<ref_obj_pid>1089508</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Shapiro, A., Faloutsos, P., and Ng-Thow-Hing, V. 2005. Dynamic animation and control environment. In <i>GI '05: Proceedings of the 2005 conference on Graphics interface</i>, Canadian Human-Computer Communications Society, School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada, 61--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Tolani, D., and Badler, N. 1996. Real-time inverse kinematics of the human arm. <i>Presence 5</i>, 4, 393--401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015756</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Yamane, K., Kuffner, J. J., and Hodgins, J. K. 2004. Synthesizing animations of human manipulation tasks. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH'04) 23</i>, 3, 532--539.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>NSF</funding_agency>
			<grant_numbers>
				<grant_number>CCF-0429983</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401209</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<pages>11</pages>
		<display_no>58</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Planning collision-free reaching motions for interactive object manipulation and grasping]]></title>
		<page_from>1</page_from>
		<page_to>11</page_to>
		<doi_number>10.1145/1401132.1401209</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401209</url>
		<abstract>
			<par><![CDATA[<p>We present new techniques that use motion planning algorithms based on probabilistic roadmaps to control 22 degrees of freedom (DOFs) of human-like characters in interactive applications. Our main purpose is the automatic synthesis of collision-free reaching motions for both arms, with automatic column control and leg flexion. Generated motions are collision-free, in equilibrium, and respect articulation range limits. In order to deal with the high (22) dimension of our configuration space, we bias the random distribution of configurations to favor postures most useful for reaching and grasping. In addition, extensions are presented in order to interactively generate object manipulation sequences: a probabilistic inverse kinematics solver for proposing goal postures matching pre-designed grasps; dynamic update of roadmaps when obstacles change position; online planning of object location transfer; and an automatic stepping control to enlarge the character's reachable space. This is, to our knowledge, the first time probabilistic planning techniques are used to automatically generate collision-free reaching motions involving the entire body of a human-like character at interactive frame rates.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098791</person_id>
				<author_profile_id><![CDATA[81100215070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kallmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California, Los Angeles, United States]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098792</person_id>
				<author_profile_id><![CDATA[81100269613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Amaury]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aubel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Animation, Glendale, United States]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098793</person_id>
				<author_profile_id><![CDATA[81100316197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tolga]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abaci]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swiss Federal Institute of Technology, Lausanne, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098794</person_id>
				<author_profile_id><![CDATA[81100534488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thalmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swiss Federal Institute of Technology, Lausanne, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>162261</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[N. I. Badler, C. B. Phillips, and B. L. Webber. Simulating Humans: Computer Graphics, Animation and Control. ISBN 0-19-507359-2, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>532147</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J.-C. Latombe. Robot Motion Planning. ISBN 0-7923-9206-X, Kluwer Academic Publishers, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Y. Koga, K. Kondo, J. Kuffner, and J. Latombe. Planning Motions with Intentions. <i>Proc. of SIGGRAPH'94</i>, 395--408, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826533</ref_obj_id>
				<ref_obj_pid>826029</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. J. Kuffner and J. C. Latombe. Interactive manipulation planning for animated characters. <i>Proc. of Pacific Graphics'00</i>, poster paper, Hong Kong, October 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218422</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and Z. Popovic. Motion Warping. <i>Proc. of SIGGRAPH'95</i>, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280820</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Gleicher. Retargeting Motion to New Characters. <i>Proc. of SIGGRAPH'98</i>, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[L. Kovar, M. Gleicher, and F. Pighin. Motion Graphs. <i>Proc. of SIGGRAPH'02</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566604</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Y. Li, T. Wang, and H.-Y. Shum. Motion Texture: A Two-Level Statistical Model for Character Motion Synthesis. <i>Proc. of SIGGRAPH'02</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[C. F. Rose, P.-P. J. Sloan, and M. F. Cohen. Artist-Directed Inverse-Kinematics Using Radial Basis Function Interpolation. <i>Proc. of Eurographics</i>, &#60;b&#62;20&#60;/b&#62;(3), 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[D. Tolani, and N. Badler. Real-Time Inverse Kinematics of the Human Arm. <i>Presence</i>, &#60;b&#62;5&#60;/b&#62;(4):393--401, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[P. Baerlocher, and R. Boulic. Task-priority Formulations for the Kinematic Control of Highly Redundant Articulated Structures. <i>Proc. of IROS'98</i>, Victoria, Canada, Oct. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[X. Wang, and J.-P. Verriest. A Geometric Algorithm to Predict the Arm Reach Posture for Computer-aided Ergonomic Evaluation. <i>Journal of Vis. and Comp. Animation</i>, &#60;b&#62;9&#60;/b&#62;(1):33--47, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[L. Kavraki, P. Svestka, J. Latombe, and M. Overmars. Probabilistic Roadmaps for Fast Path Planning in High-Dimensional Configuration Spaces. <i>IEEE Transactions on Robotics and Automation</i>, &#60;b&#62;12&#60;/b&#62;:566--580, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[S. La Valle. Rapidly-Exploring Random Trees: A New Tool for Path Planning. <i>Technical Report</i> 98--11, Computer Science Dept., Iowa State University, Oct. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[R. Bohlin and L. Kavraki. Path Planning using Lazy PRM. In Proc. of IEEE Int. <i>Conference on Robotics and Automation</i>, ICRA, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[T. Simeon, J. P. Laumond, and C. Nissoux. Visibility Based Probabilistic Roadmaps for Motion Planning. <i>Advanced Robotics Journal</i>, &#60;b&#62;14&#60;/b&#62;(2), 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. J. Kuffner and S. M. La Valle. RRT-Connect: An efficient approach to single-query path planning. <i>In Proc. IEEE Int'l Conf. on Robotics and Automation (ICRA'2000)</i>, San Francisco, CA, April 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>338429</ref_obj_id>
				<ref_obj_pid>338401</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[S. Bandi and D. Thalmann. Path Finding for Human Motion In Virtual Environments. <i>Computational Geometry</i> &#60;b&#62;15&#60;/b&#62;(1--3):103--127, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[R. Bindiganavale, J. Granieri, S. Wei, X. Zhao, and N. Badler. Posture interpolation with collision avoidance. <i>Computer Animation '94</i>, Geneva, Switzerland, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Y. Aydin, and M. Nakajima. Database guided computer animation of human grasping using forward and inverse kinematics. <i>Computer &amp; Graphics</i>, &#60;b&#62;23&#60;/b&#62;:145--154, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122754</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[H. Rijpkema and M. Girard. Computer Animation of Knowledge-Based Human Grasping. <i>Proc. of SIGGRAPH'91</i>, 339--348, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. Kallmann. Object Interaction in Real-Time Virtual Environments. DSc Thesis 2347, EPFL, January 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[A. A. Khwaja, M. O. Rahman, and M. G. Wagner. Inverse Kinematics of Arbitrary Robotic Manipulators using Genetic Algorithms. J. Lenarcic and M. L. Justy, editors, <i>Advances in Robot Kinematics: Analysis and Control</i>, 375--382. Kluwer Academic Publishers, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[M.-H. Lavoie, and R. Boudreau. Obstacle Avoidance for Redundant Manipulators Using a Genetic Algorithm. <i>CCToMM Symp. on Mechan., Machines, and Mechatronics</i>, Canada, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[T.-Y. Li, and Y.-C. Shie. An Incremental Learning Approach to Motion Planning with Roadmap Management. <i>Proc. of International Conference on Robotics and Automation (ICRA)</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781241</ref_obj_id>
				<ref_obj_pid>781238</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[S. Grassia. Practical Parametrization of Rotations Using the Exponential Map. <i>Journal of Graphics Tools</i>, &#60;b&#62;3&#60;/b&#62;(3):29--48, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>18944</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J. U. Korein. A Geometric Investigation of Reach. The MIT Press, Cambridge, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617638</ref_obj_id>
				<ref_obj_pid>616016</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[G. Monheit and N. Badler. A Kinematic Model of the Human Spine and Torso. <i>IEEE Computer Graphics and Applications</i>, &#60;b&#62;11&#60;/b&#62;(2):29--38, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237244</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[S. Gottschalk, M. C. Lin, and D. Manocha. OBBTree: A Hierarchical Structure for Rapid Interference Detection. <i>Proc. of ACM SIGGRAPH</i>, 171--180, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[P. Fitts. The Information Capacity of the Human Motor System in Controlling the Amplitude of Movement. <i>Journal of Experimental Psychology</i>, &#60;b&#62;47&#60;/b&#62;:381--391, 1954.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Federal Office for Education and Science in the framework of the European project STAR (Service and Training through Augmented Reality)</funding_agency>
			<grant_numbers>
				<grant_number>IST-2000-28764</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
	<section>
		<section_id>1401210</section_id>
		<sort_key>780</sort_key>
		<section_seq_no>20</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: OpenGL: what's coming down the graphics pipeline]]></section_title>
		<section_page_from>20</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098795</person_id>
				<author_profile_id><![CDATA[81322506343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shreiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401211</article_id>
		<sort_key>790</sort_key>
		<display_label>Article No.</display_label>
		<pages>69</pages>
		<display_no>59</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[OpenGL]]></title>
		<subtitle><![CDATA[what's coming down the graphics pipeline]]></subtitle>
		<page_from>1</page_from>
		<page_to>69</page_to>
		<doi_number>10.1145/1401132.1401211</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401211</url>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098796</person_id>
				<author_profile_id><![CDATA[81322506343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shreiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ARM]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401212</section_id>
		<sort_key>800</sort_key>
		<section_seq_no>21</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Practical global illumination with irradiance caching]]></section_title>
		<section_page_from>21</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098797</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kriv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098798</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098799</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098800</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098801</person_id>
				<author_profile_id><![CDATA[81100285050]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Per]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Christensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098802</person_id>
				<author_profile_id><![CDATA[81365590895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tabellion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401213</article_id>
		<sort_key>810</sort_key>
		<display_label>Article No.</display_label>
		<pages>20</pages>
		<display_no>60</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Practical global illumination with irradiance caching]]></title>
		<page_from>1</page_from>
		<page_to>20</page_to>
		<doi_number>10.1145/1401132.1401213</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401213</url>
		<abstract>
			<par><![CDATA[<p>Since its invention 20 years ago, irradiance caching has been successfully used to accelerate global illumination computation in the Radiance lighting simulation system. Its widespread use had to wait until computers became fast enough to consider global illumination in production rendering. Since then, its use is ubiquitous. Virtually all commercial and open-source rendering software base the global illumination computation upon irradiance caching. Although elegant and powerful, the algorithm often fails to produce artifact-free images. Unfortunately, practical information on implementing the algorithm is scarce.</p> <p>The objective of the class is twofold. The first and main objective is to expose the irradiance caching algorithm along with all the details and tricks upon which the success of its practical implementation is dependent. Various image artifacts that the basic algorithm can produce will be shown along with a recipe to suppress them. We will also put strong emphasis on practical aspects of irradiance caching integration in production environments and discuss the particularities used in two big production houses, namely PDI/DreamWorks and Pixar.</p> <p>The second objective is to acquaint the audience with the recent research results that increase the speed and extend the functionality of basic irradiance caching. Those include: exploiting temporal coherence to suppress temporal flickering; extending the caching mechanism to rendering glossy surfaces; accelerating the algorithm by porting it to the GPU. Advantages and disadvantages of those methods will be discussed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098803</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University in Prague]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098804</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[France Telecom R&D]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098805</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Anyhere Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098806</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098807</person_id>
				<author_profile_id><![CDATA[81100285050]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Per]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Christensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098808</person_id>
				<author_profile_id><![CDATA[81365590895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tabellion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401214</article_id>
		<sort_key>820</sort_key>
		<display_label>Article No.</display_label>
		<pages>25</pages>
		<display_no>61</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Global illumination with Monte Carlo ray tracing]]></title>
		<page_from>1</page_from>
		<page_to>25</page_to>
		<doi_number>10.1145/1401132.1401214</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401214</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.3</cat_node>
				<descriptor>Probabilistic algorithms (including Monte Carlo)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003671</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098809</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[&#268;VUT v Praze --- CTU Prague]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401215</article_id>
		<sort_key>830</sort_key>
		<display_label>Article No.</display_label>
		<pages>28</pages>
		<display_no>62</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Irradiance caching algorithm]]></title>
		<page_from>1</page_from>
		<page_to>28</page_to>
		<doi_number>10.1145/1401132.1401215</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401215</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Radiometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098810</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dolby Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401216</article_id>
		<sort_key>840</sort_key>
		<display_label>Article No.</display_label>
		<pages>11</pages>
		<display_no>63</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Implementation of irradiance caching in <i>radiance</i>]]></title>
		<page_from>1</page_from>
		<page_to>11</page_to>
		<doi_number>10.1145/1401132.1401216</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401216</url>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098811</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dolby Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401217</article_id>
		<sort_key>850</sort_key>
		<display_label>Article No.</display_label>
		<pages>52</pages>
		<display_no>64</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Problems & solutions]]></title>
		<subtitle><![CDATA[implementation details]]></subtitle>
		<page_from>1</page_from>
		<page_to>52</page_to>
		<doi_number>10.1145/1401132.1401217</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401217</url>
		<abstract>
			<par><![CDATA[<p>This part of the course summarizes the common problems encountered when implementing irradiance caching. For each of the problems we describe one or more possible solutions to alleviate it. It should be stressed that this part describes the solutions used in my own implementation but slightly different solutions may be used in other implementations. Various tricks used at PDI/Dreamworks are described by Eric Tabellion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098812</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University, Prague]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401218</article_id>
		<sort_key>860</sort_key>
		<display_label>Article No.</display_label>
		<pages>37</pages>
		<display_no>65</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Irradiance caching and photon mapping]]></title>
		<page_from>1</page_from>
		<page_to>37</page_to>
		<doi_number>10.1145/1401132.1401218</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401218</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098813</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401219</article_id>
		<sort_key>870</sort_key>
		<display_label>Article No.</display_label>
		<pages>49</pages>
		<display_no>66</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Extension to glossy surfaces]]></title>
		<subtitle><![CDATA[radiance caching]]></subtitle>
		<page_from>1</page_from>
		<page_to>49</page_to>
		<doi_number>10.1145/1401132.1401219</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401219</url>
		<abstract>
			<par><![CDATA[<p>The interpolation scheme used in irradiance caching for diffuse surfaces can be also used on glossy surfaces. However, some modifications are necessary due to the view-dependence of reflection on glossy surfaces. These modifications will be discussed in this part of the course.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098814</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University, Prague]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383577</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[P. Gautron, J. K&#345;iv&#225;nek, S. Pattanaik, and K. Bouatouch, A Novel Hemispherical Basis for Accurate and Efficient Rendering, Eurographics Symposium on Rendering, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1080029</ref_obj_id>
				<ref_obj_pid>1079840</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. K&#345;iv&#225;nek, P. Gautron, S. Pattanaik, and K. Bouatouch, Radiance Caching for Efficient Global Illumination Computation, IEEE TVCG, Vol. 11, No. 5, Sep./Oct. 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383911</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. K&#345;iv&#225;nek, K. Bouatouch, S. Pattanaik, and J. &#381;&#225;ra, Making Radiance and Irradiance Caching Practical: Adaptive Caching and Neighbor Clamping, Eurographics Symposium on Rendering, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401220</article_id>
		<sort_key>880</sort_key>
		<display_label>Article No.</display_label>
		<pages>61</pages>
		<display_no>67</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Hardware implementation]]></title>
		<page_from>1</page_from>
		<page_to>61</page_to>
		<doi_number>10.1145/1401132.1401220</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401220</url>
		<abstract>
			<par><![CDATA[<p>In the classical Irradiance Caching algorithm, rays are first traced from the viewpoint towards the scene. For each corresponding intersection point, the irradiance cache is queried to determine whether a new irradiance record has to be created. When a new record is required, the irradiance value is computed using distribution ray tracing. The newly created record is then stored in an octree, called the irradiance cache. For nearby intersection points the record is retrieved by querying the octree.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098815</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thomson Corporate Research, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{BP04} Michael Bunnell and Fabio Pellacini. <i>GPU Gems: Shadow map antialiasing</i>, pages 185--192. Addison Wesley, 1 edition, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{CG85} Michael Cohen and Donald P. Greenberg. The Hemi-Cube: A Radiosity Solution for Complex Environments. In <i>Proceedings of SIGGRAPH</i>, volume 19, pages 31--40, August 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569052</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{CHH02} Nathan A. Carr, Jesse D. Hall, and John C. Hart. The ray engine. In <i>Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</i>, pages 37--46. Eurographics Association, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Hei91} Tim Heidmann. Real shadows, real time. <i>Iris Universe</i>, 18:23--31, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Int02} Intel Corporation. <i>AGP v3.0 Specification</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{KGBP05} Jaroslav K&#345;iv&#225;anek, Pascal Gautron, Kadi Bouatouch, and Sumanta Pattanaik. Improved radiance gradient computation. In <i>Proceedings of SCCG</i>, pages 149--153, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383550</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{LC04} Bent Dalgaard Larsen and Niels Jorgen Christensen. Simulating photon mapping for real-time applications. In A. Keller and H. W. Jensen, editors, <i>Proceedings of Eurographics Symposium on Rendering</i>, pages 123--131, June 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{PCI03} PCI-SIG. <i>PCI-Express v1.0e Specification</i>. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{RSC87} W. T. Reeves, D. H. Salesin, and R. L. Cook. Rendering antialiased shadows with depth maps. In <i>Proceedings of SIGGRAPH</i>, volume 21, pages 283--291, August 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566616</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{SD02} Marc Stamminger and George Drettakis. Perspective shadow maps. In <i>Proceedings of SIGGRAPH</i>, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74368</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{SP89} Francois Sillion and Claude Puech. A General Two-Pass Method Integrating Specular and Diffuse Reflection. In <i>Proceedings of SIGGRAPH</i>, volume 23, pages 335--344, July 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015748</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{TL04} Eric Tabellion and Arnauld Lamorlette. An approximate global illumination system for computer-generated films. In <i>Proceedings of SIGGRAPH</i>, August 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{War94} Gregory J. Ward. The RADIANCE Lighting Simulation and Rendering System. In <i>Computer Graphics Proceedings, Annual Conference Series, 1994 (Proceedings of SIGGRAPH</i>, pages 459--472, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{WH92} Gregory J. Ward and Paul Heckbert. Irradiance Gradients. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 85--98, Bristol, UK, May 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Wil78} Lance Williams. Casting curved shadows on curved surfaces. In <i>Proceedings of SIGGRAPH</i>, pages 270--274, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{WRC88} Gregory J. Ward, Francis M. Rubinstein, and Robert D. Clear. A Ray Tracing Solution for Diffuse Interreflection. In <i>Proceedings of SIGGRAPH</i>, volume 22, pages 85--92, August 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401221</article_id>
		<sort_key>890</sort_key>
		<display_label>Article No.</display_label>
		<pages>49</pages>
		<display_no>68</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Temporal radiance caching]]></title>
		<page_from>1</page_from>
		<page_to>49</page_to>
		<doi_number>10.1145/1401132.1401221</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401221</url>
		<abstract>
			<par><![CDATA[<p>Very often global illumination methods aim at simulating the light/matter interactions within static scenes. Accounting for the displacement of objects and light sources either require a complete recomputation of the global illumination solution for each frame of the animation, or involve complex data structures and algorithms for temporal optimization. Furthermore, the global illumination solutions commonly exhibit low temporal quality when used in dynamic scenes: flickering, popping, ... In the context of computer-assisted effects for movies, high quality global illumination is obtained through temporal filtering: a 30 fps animation is first rendered at 60 fps by recomputing the global illumination for each frame. Then, each frame of the 30 fps animation is generated by averaging two frames of the 60 fps animation, hence reducing the temporal artifacts at the cost of high computational cost. For interactive applications such as video games, the illumination must be computed interactively. In this case, approximate models are generally preferred, such as the precomputation of a static global illumination solution, and the update of direct lighting only at runtime.</p> <p>This chapter describes a simple and accurate method based on temporal caching for the computation of global illumination effects in animated environments [GBP07], where viewer, objects and light sources move. This approach focuses on a temporal optimization for lighting computation based on the irradiance caching [WRC88] technique. As this algorithm leverages the spatial coherence of indirect lighting to reduce the cost of global illumination, we consider here an extension of these methods for sparse temporal sampling and interpolation. In [WRC88], Ward et al. propose a reuse an irradiance value in the neighborhood of the actual computation point. While the weighting function and gradients of [WRC88] account for the spatial change of irradiance, Temporal Radiance Caching considers the temporal change of the indirect lighting (Figure 8.2).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Radiometry</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098816</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Thomson Corporate Research, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1071869</ref_obj_id>
				<ref_obj_pid>1071866</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Foley05} Foley T., Sugerman J. "KD-tree acceleration structures for a GPU raytracer", 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Krivanek05a} Krivanek J., Gautron P., Pattanaik S., Bouatouch K. "Radiance Caching for Efficient Global Illumination Computation", 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Krivanek05b} Krivanek J., Gautron P., Bouatouch K., Pattanaik S. "Improved Radiance Gradient Computation", 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{LC04} Larsen B. D., Christensen N. "Simulating photon mapping for real-time applications", 2004]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Purcell02} Purcell T. J., Buck I., Mark W. R., Hanrahan P. "Ray tracing on programmable graphics hardware", 2002]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Purcell03} Purcell T. J., Donner C., Cammarano M., Jensen H. W., Hanrahan P. "Photon mapping on programmable graphics hardware", 2003]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Walter99} Walter B., Drettakis G., Parker S. "Interactive rendering using the render cache", 1999]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Ward88} Ward G. J., Rubinstein F. M., Clear R. D. "A ray tracing solution for diffuse interreflection", 1988]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Ward92} Ward G. J., Heckbert P. S. "Irradiance gradients", 1992]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Wil78} Williams L. "Casting curved shadows on curved surfaces", 1978]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231916</ref_obj_id>
				<ref_obj_pid>2231878</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{GBP07} Pascal Gautron, Kadi Bouatouch, and Sumanta Pattanaik. Temporal radiance caching. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{TMS02} Takehiro Tawara, Karol Myszkowski, and Hans-Peter Seidel. Localizing the final gathering for dynamic scenes using the photon map. In <i>VMV</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581901</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{WDG02} Bruce Walter, George Drettakis, and Donald P. Greenberg. Enhancing and optimizing the render cache. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 37--42, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383819</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{WDP99} Bruce Walter, George Drettakis, and Steven Parker. Interactive rendering using the render cache. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 235--246, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{WH92} Gregory J. Ward and Paul Heckbert. Irradiance Gradients. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 85--98, Bristol, UK, May 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{WRC88} Gregory J. Ward, Francis M. Rubinstein, and Robert D. Clear. A Ray Tracing Solution for Diffuse Interreflection. In <i>Proceedings of SIGGRAPH</i>, volume 22, pages 85--92, August 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401222</article_id>
		<sort_key>900</sort_key>
		<display_label>Article No.</display_label>
		<pages>47</pages>
		<display_no>69</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Irradiance caching at DreamWorks]]></title>
		<page_from>1</page_from>
		<page_to>47</page_to>
		<doi_number>10.1145/1401132.1401222</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401222</url>
		<abstract>
			<par><![CDATA[<p>Global Illumination at DreamWorks refers to rendering one bounce of diffuse interreflection, as well as ambient occlusion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098817</person_id>
				<author_profile_id><![CDATA[81365590895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tabellion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401223</article_id>
		<sort_key>910</sort_key>
		<display_label>Article No.</display_label>
		<pages>26</pages>
		<display_no>70</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Irradiance caching in Pixar's RenderMan]]></title>
		<page_from>1</page_from>
		<page_to>26</page_to>
		<doi_number>10.1145/1401132.1401223</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401223</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098818</person_id>
				<author_profile_id><![CDATA[81100285050]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Per]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Christensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401224</article_id>
		<sort_key>920</sort_key>
		<display_label>Article No.</display_label>
		<pages>8</pages>
		<display_no>71</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[A ray tracing solution for diffuse interreflection]]></title>
		<page_from>1</page_from>
		<page_to>8</page_to>
		<doi_number>10.1145/1401132.1401224</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401224</url>
		<abstract>
			<par><![CDATA[<p>An efficient ray tracing method is presented for calculating interreflections between surfaces with both diffuse and specular components. A Monte Carlo technique computes the indirect contributions to illuminance at locations chosen by the rendering process. The indirect illuminance values are averaged over surfaces and used in place of a constant "ambient" term. Illuminance calculations are made only for those areas participating in the selected view, and the results are stored so that subsequent views can reuse common values. The density of the calculation is adjusted to maintain a constant accuracy, permitting less populated portions of the scene to be computed quickly. Successive reflections use proportionally fewer samples, which speeds the process and provides a natural limit to recursion. The technique can also model diffuse transmission and illumination from large area sources, such as the sky.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Monte Carlo technique]]></kw>
			<kw><![CDATA[caching]]></kw>
			<kw><![CDATA[complexity]]></kw>
			<kw><![CDATA[diffuse]]></kw>
			<kw><![CDATA[illuminance]]></kw>
			<kw><![CDATA[interreflection]]></kw>
			<kw><![CDATA[luminance]]></kw>
			<kw><![CDATA[radiosity]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[specular]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098819</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Berkeley Laboratory, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098820</person_id>
				<author_profile_id><![CDATA[81100573338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Clear]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Berkeley Laboratory, Berkeley, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>356797</ref_obj_id>
				<ref_obj_pid>356789</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Butley, Jon Louis and Jerome Friedman, "Data Structures for Range Searching," <i>ACM Computing Surveys</i>, Vol. 11, No. 4, 1979, pp. 397--409.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cohen, Michael and Donald Greenberg "A Radiosity Solution for Complex Environments," <i>Computer Graphics</i>, Vol. 19, No. 3, July 1985, pp. 31--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1436158</ref_obj_id>
				<ref_obj_pid>1435615</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cohen, Michael, Donald Greenberg, David Immel, Phillip Brock, "An Efficient Radiosity Approach for Realistic Image Synthesis," <i>IEEE Computer Graphics and Applications</i>, Vol. 6, No. 2, March 1986, pp. 26--35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357293</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cook, Robert L. and Kenneth E. Torrance, "A Reflection Model for Computer Graphics," <i>ACM Transactions on Graphics</i>, Vol. 1, No. 1, January 1982, pp. 7--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cook, Robert, Thomas Porter, Loren Carpenter, "Distributed Ray Tracing," <i>Computer Gracphics</i>, Vol. 18, No. 3, July 1984, pp. 137--147.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cook, Robert L., "Stochastic Sampling in Computer Graphics," <i>ACM Transactions on Gracphics</i>, Vol. 5, No. 1, January 1986, pp. 51--72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15901</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Immel, David S., Donald P. Greenburg, Michael F. Cohen, "A Radiosity Method for Non-Diffuse Environments," <i>Computer Graphics</i>, Vol. 20, No. 4, August 1986, pp. 133--142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kajiya, James T., "The Rendering Equation," <i>Computer Gracphis</i>, Vol. 20, No. 4, August 1986, pp. 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kaufman, John, <i>IES Lighting Handbook</i>, Reference Volume, IESNA, New York, NY, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325169</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Nishita, Tomoyuki, and Eihachiro Nakamae, "Continuous Tone Representation of Three-Dimensional Objects Taking Account of Shadows and Interreflection," <i>Computer Graphics</i>, Vol. 19, No. 3, July 1985, pp. 23--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>539488</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Rubsenstein, R. Y., <i>Simulation and the Monte Carlo Method</i>, J. Wiley, New York, 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Siegel, R. and J. R. Howell, <i>Thermal Radiation Heat Transfer</i>, Hemisphere Publishing Corp., Washington DC., 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37438</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Wallace, John R., Michael F. Cohen, Donald P. Greenburg, "A Two-Pass Solution to the Rendering Equation: A Synthesis of Ray Tracing and Radisoity Methods," <i>Computer Graphics</i>, Vol. 21, No. 4, July 1987, pp. 311--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357335</ref_obj_id>
				<ref_obj_pid>357332</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Weghorst, Hank, Gary Hooper, Donald P. Greenberg. "Improved computational methods for ray tracing" <i>ACM Transactions on Graphics</i>, Vol. 3, No. 1, January 1984, pp. 52--69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner, "An Improved Illumination Model for Shaded Display," <i>Communications of the ACM</i>, Vol. 23, No. 6, June 1980, pp. 343--349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401225</article_id>
		<sort_key>930</sort_key>
		<display_label>Article No.</display_label>
		<pages>17</pages>
		<display_no>72</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Irradiance gradients]]></title>
		<page_from>1</page_from>
		<page_to>17</page_to>
		<doi_number>10.1145/1401132.1401225</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401225</url>
		<abstract>
			<par><![CDATA[<p>A new method for improving the accuracy of a diffuse interreflection calculation is introduced in a ray tracing context. The information from a hemispherical sampling of the luminous environment is interpreted in a new way to predict the change in irradiance as a function of position and surface orientation. The additional computation involved is modest and the benefit is substantial. An improved interpolation of irradiance resulting from the gradient calculation produces smoother, more accurate renderings. This result is achieved through better utilization of ray samples rather than additional samples or alternate sampling strategies. Thus, the technique is applicable to a variety of global illumination algorithms that use hemicubes or Monte Carlo sampling techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Interpolation formulas</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Smoothing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098821</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gregory]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Polytechnique Federale de Lausanne, Lausanne]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098822</person_id>
				<author_profile_id><![CDATA[81100383628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Heckbert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Delft University of Technology, Delft, Netherlands]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>144727</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Heckbert91a} Paul Heckbert, <i>Simulating Global Illumination Using Adaptive Meshing</i>, PhD Thesis, Tech. Report UCB/CSD 91/636, Computer Science Division, University of California at Berkeley, June 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>894053</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Heckbert91b} Paul Heckbert and Jim Winget, "Finite Element Methods for Global Illumination," Tech. Report UCB/CSD 91/643, Computer Science Division, University of California at Berkeley, July 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Kajiya86} James T. Kajiya, "The Rendering Equation," <i>Computer Graphics</i>, Vol. 20, No. 4, August 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>866560</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Lischinski91} Dani Lischinski, Filippo Tampieri, and Donald P. Greenberg, <i>Improving Sampling and Reconstruction Techniques for Radiosity</i>, Computer Science Dept., Cornell University, Tech. Report 91--1202, Aug. 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130799</ref_obj_id>
				<ref_obj_pid>130745</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Max92} Nelson Max and Michael Allison, "Linear Radiosity Approximation using Vertex-to-Vertex Form Factors," <i>Graphics Gems III</i>, edited by David Kirk, Academic Press, 1992 (to appear).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Ward88a} Gregory Ward and Francis Rubinstein, "A New Technique for Computer Simulation of Illuminated Spaces," <i>Journal of the Illuminating Engineering Society</i>, Vol. 17, No. 1, Winter 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Ward88b} Gregory Ward, Francis Rubinstein, and Robert Clear, "A Ray Tracing Solution for Diffuse Inter-reflection," <i>Computer Graphics</i>, Vol. 22, No. 4, August 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Ward90} Gregory Ward, "Visualization," <i>Lighting Design and Application</i>, Vol. 20, No. 6, June 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Ward91} Gregory Ward, "Adaptive Shadow Testing for Ray Tracing," <i>Second Eurographics Workshop on Rendering</i>, Barcelona, Spain, April 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Whitted80} Turner Whitted, "An Improved Illumination Model for Shaded Display," <i>Communications of the ACM</i>, Vol. 23, No. 6, June 1980, pp. 343--349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>U.S. Department of Energy</funding_agency>
			<grant_numbers>
				<grant_number>DE-AC03-76SF00098</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401226</article_id>
		<sort_key>940</sort_key>
		<display_label>Article No.</display_label>
		<pages>40</pages>
		<display_no>73</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Indirect calculation]]></title>
		<page_from>1</page_from>
		<page_to>40</page_to>
		<doi_number>10.1145/1401132.1401226</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401226</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401227</article_id>
		<sort_key>950</sort_key>
		<display_label>Article No.</display_label>
		<pages>8</pages>
		<display_no>74</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[An approximate global illumination system for computer generated films]]></title>
		<page_from>1</page_from>
		<page_to>8</page_to>
		<doi_number>10.1145/1401132.1401227</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401227</url>
		<abstract>
			<par><![CDATA[<p>Lighting models used in the production of computer generated feature animation have to be flexible, easy to control, and efficient to compute. Global illumination techniques do not lend themselves easily to flexibility, ease of use, or speed, and have remained out of reach thus far for the vast majority of images generated in this context. This paper describes the implementation and integration of indirect illumination within a feature animation production renderer. For efficiency reasons, we choose to partially solve the rendering equation. We explain how this compromise allows us to speed-up final gathering calculations and reduce noise. We describe an efficient ray tracing strategy and its integration with a micro-polygon based scan line renderer supporting displacement mapping and programmable shaders. We combine a modified irradiance gradient caching technique with an approximate lighting model that enhances caching coherence and provides good scalability to render complex scenes into highresolution images suitable for film. We describe the tools that are made available to the artists to control indirect lighting in final renders. We show that our approach provides an efficient solution, easy to art direct, that allows animators to enhance considerably the quality of images generated for a large category of production work.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[distributed ray tracing]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[irradiance caching]]></kw>
			<kw><![CDATA[micro-polygon]]></kw>
			<kw><![CDATA[rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098823</person_id>
				<author_profile_id><![CDATA[81365590895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tabellion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098824</person_id>
				<author_profile_id><![CDATA[81100259193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Arnauld]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lamorlette]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Apodaca, T., Quaroni, G., Bredow, R., Goldman, D., Landis, H., Gritz, L., and Pharr, M. RenderMan in production. In <i>SIGGRAPH'2002, Course #16</i>, San Antonio, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Arvo, J. 1986. Backward ray tracing. In Course Notes of the 1986 Conference on Computer Graphics and Interactive Techniques, no. 12, (Dallas, Texas, Aug. 18--22). ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340504</ref_obj_id>
				<ref_obj_pid>340501</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. H. 2000. Faster Photon Map Global Illumination. Journal of Graphics Tools, volume 4, number 3, pages 1--10. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. H., Laur, D. M., Fong, J., Wooten, W. L., and Batali, D. 2003. Ray Differentials and Multiresolution Geometry Caching for Distribution Ray Tracing in Complex Scenes. <i>Computer Graphics Forum (Eurographics 2003 Conference Proceedings)</i>, pages 543--552. Blackwell Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L., Carpenter, L., and Catmull, E. 1987. The Reyes image rendering architecture. In Maureen C. Stone, editor, <i>Computer Graphics (SIGGRAPH '87 Proceedings)</i>, pages 95--102, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Dutr&#233;, P., and Willems, Y. D. 1994. Importance-driven Monte Carlo Light Tracing. In <i>Proceedings of 5. Eurographics Workshop on Rendering</i>, pages 185--194, Darmstadt.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>863712</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Driemeyer, T., Herken, R. 2003. <i>Programming mental ray</i>. Second, revised edition. Springer Verlag Wien New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97895</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P. S. 1990. Adaptive Radiosity Textures for Bidirectional Ray Tracing. <i>Computer Graphics</i> &#60;b&#62;24&#60;/b&#62; (4), pages 145--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311555</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Igehy, H. 1999. Tracing ray differentials. <i>Computer Graphics</i>, 33(Annual Conference Series):179--186.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15901</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Immel, D. S., Cohen, M. F., Greenberg, D. P. 1986. A radiosity method for non-diffuse environments. <i>Computer Graphics</i> &#60;b&#62;20&#60;/b&#62; (4), pages 133--142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 1995. Importance driven path tracing using the photon map. <i>Rendering Techniques '95 (Proceedings of the Sixth Eurographics Workshop on Rendering)</i>, pages 326--335. Springer Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 1996. Global illumination using photon maps. <i>Rendering Techniques '96 (Proceedings of the Seventh EurographicsWorkshop on Rendering)</i>, pages 21--30. Springer Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1103920</ref_obj_id>
				<ref_obj_pid>1103900</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Christensen, P. H., Kato, T., and Suykens, F. 2002. A Practical Guide to Global Illumination using Photon Mapping. In <i>SIGGRAPH'2002, Course #43</i>, San Antonio, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T. 1986. The Rendering Equation. <i>Computer Graphics</i> &#60;b&#62;20&#60;/b&#62; (4), pages 143--149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P. and Willems, Y. D. 1993. Bidirectional Path Tracing. In <i>Proceedings of CompuGraphics</i>, pages 95--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P. 1996. <i>Mathematical Models and Monte Carlo Algorithms for Physcially Based Rendering</i>. Ph.d. thesis, Katholieke University, Leuven, Belgium 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275462</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Pharr, M., and Hanrahan, P. 1996. Geometry caching for ray tracing displacement maps. In Xavier Pueyo and Peter Schr&#246;der, editors, <i>Eurographics Workshop on Rendering</i>, pages 31--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258791</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Pharr, M., Kolb, C., Gershbein, R., and Hanrahan, P. 1997. Rendering complex scenes with memory-coherent ray tracing. In <i>SIGGRAPH 97 Conference Proceedings</i>, pages 101--108, August.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>155112</ref_obj_id>
				<ref_obj_pid>155090</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Rushmeier, H., Patterson, C., and Veerasamy, A. 1993. Geometric simplification for indirect illumination calculations, in Proc. Graphics Interface '93, (Toronto, Ontario), pp. 227--236.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614296</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Slusallek, P., Seidel, H. P. 1995. Vision: An Architecture for Global Illumination Calculations, in <i>IEEE Transactions on Visualization and Computer Graphics</i>, 1(1), pp. 77--96, March.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Upstill, S. 1990. <i>The Renderman Companion</i>. AddisonWesley, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. 1994. Bidirectional Estimators for Light Transport. In <i>Proceedings of the 5th Eurographics Workshop on Rendering</i>, pages 147--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. 1995. Optimally Combinig Sampling Techniques for Monte Carlo Rendering. <i>Computer Graphics</i> &#60;b&#62;29&#60;/b&#62; (4), pages 419--428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. 1997. Metropolis Light Transport. <i>Computer Graphics</i> &#60;b&#62;31&#60;/b&#62; (3), pages 65--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Ward, G., and Heckbert, P. 1992. Irradiance gradients. <i>Third Eurographics Workshop on Rendering</i>, pages 85--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Ward, G. 1994. The RADIANCE Lighting simulation and Rendering System. In <i>Computer Graphics</i>, pages 459--472, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401228</article_id>
		<sort_key>960</sort_key>
		<display_label>Article No.</display_label>
		<pages>19</pages>
		<display_no>75</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Radiance caching for efficient global illumination computation]]></title>
		<page_from>1</page_from>
		<page_to>19</page_to>
		<doi_number>10.1145/1401132.1401228</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401228</url>
		<abstract>
			<par><![CDATA[<p>In this paper we present a ray tracing based method for accelerated global illumination computation in scenes with low-frequency glossy BRDFs. The method is based on sparse sampling, caching, and interpolating radiance on glossy surfaces. In particular we extend the irradiance caching scheme of [WRC88] to cache and interpolate directional incoming radiance instead of irradiance. The incoming radiance at a point is represented by a vector of coefficients with respect to a spherical or hemispherical basis. The surfaces suitable for interpolation are selected automatically according to the glossiness of their BRDF. We also propose a novel method for computing translational radiance gradient at a point.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[directional distribution]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
			<kw><![CDATA[spherical harmonics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098825</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Projet Siames]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098826</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Projet Siames]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098827</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Projet Siames]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098828</person_id>
				<author_profile_id><![CDATA[81100332780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kadi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouatouch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Projet Siames]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166137</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{AH93} Larry Aupperle and Pat Hanrahan. A hierarchical illumination algorithm for surfaces with glossy reflection. <i>Computer Graphics</i>, 27(Annual Conference Series):155--162, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192250</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Arv94} James Arvo. The irradiance jacobian for partially occluded polyhedral sources. In <i>Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, pages 343--350. ACM Press, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>336417</ref_obj_id>
				<ref_obj_pid>336414</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{BDT99} Kavita Bala, Julie Dorsey, and Seth Teller. Radiance interpolants for accelerated bounded-error ray tracing. <i>ACM Trans. Graph.</i>, 18(3):213--256, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882318</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{BWG03} Kavita Bala, Bruce Walter, and Donald Greenberg. Combining edges and points for interactive high-quality rendering. <i>ACM Trans. Graph. (Proceedings of SIGGRAPH 2003)</i>, 22(3), July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237749</ref_obj_id>
				<ref_obj_pid>237748</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{CLSS97} Per H. Christensen, Dani Lischinski, Eric J. Stollnitz, and David H. Salesin. Clustering for glossy global illumination. <i>ACM Trans. Graph.</i>, 16(1):3--33, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37434</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{CMS87} Brian Cabral, Nelson Max, and Rebecca Springmeyer. Bidirectional reflection functions from surface bump maps. In <i>Proceedings of the 14th annual conference on Computer graphics and interactive techniques</i>, pages 273--281. ACM Press, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>990006</ref_obj_id>
				<ref_obj_pid>990002</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{GD04} Xavier Granier and George Drettakis. A final reconstruction approach for a unified global illumination algorithm. <i>ACM Trans. Graph.</i>, 23(2):163--189, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383577</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{GKPB04} Pascal Gautron, Jaroslav K&#345;iv&#225;nek, Sumanta N. Pattanaik, and Kadi Bouatouch. A novel hemispherical basis for accurate and efficient rendering. In <i>Eurographics Symposium on Rendering (to appear)</i>, June 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Gro92} Eduard Groeller. <i>Coherence in Computer Graphics</i>. PhD thesis, Technische universit&#228;t Wien, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280888</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Guo98} B. Guo. Progressive radiance evaluation using directional coherence maps. In <i>Computer Graphics (SIGGRAPH '98 Proceedings)</i>, pages 255--266, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Hec91} Paul S. Heckbert. <i>Simulating Global Illumination Using Adaptive Meshing</i>. PhD thesis, University of California, Berkeley, June 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{HS95} Nicolas Holzschuch and Fran&#231;ois Sillion. Accurate computation of the radiosity gradient with constant and linear emitters. In <i>Sixth Eurographics Workshop on Rendering</i>, June 1995. Dublin, Ireland.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311554</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{HS99} Wolfgang Heidrich and Hans-Peter Seidel. Realistic, hardware-accelerated shading and lighting. In <i>Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, pages 171--178. ACM Press/Addison-Wesley Publishing Co., 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{HSA91} Pat Hanrahan, David Salzman, and Larry Aupperle. A rapid hierarchical radiosity algorithm. <i>Computer Graphics (Proceedings of SIGGRAPH 1991)</i>, 25(4):197--206, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{Jen01} Henrik Wann Jensen. <i>Realistic Image Synthesis Using Photon Mapping</i>. AK Peters, July 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Kat02} T. Kato. Photon mapping in Kilauea. In <i>Siggraph 2002, Course Notes No. 43, A Practical Guide to Global Illumination using Photon Mapping</i>, pages 122--191, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581934</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{KSS02} Jan Kautz, Peter-Pike Sloan, and John Snyder. Fast, arbitrary BRDF shading for low-frequency lighting using spherical harmonics. In <i>Proceedings of the 13th Eurographics workshop on Rendering</i>, pages 291--296. Eurographics Association, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649002</ref_obj_id>
				<ref_obj_pid>645310</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{KvDS96} J. J. Koenderink, A. J. van Doorn, and M. Stavridi. Bidirectional reflection distribution function expressed in terms of surface scattering modes. <i>ECCV</i>, B:28--39, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275460</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{LF96} Robert R. Lewis and Alain Fournier. Light-driven global illumination with a wavelet representation. In <i>Rendering Techniques '96 (Proceedings of the Seventh Eurographics Workshop on Rendering)</i>, pages 11--20, New York, NY, 1996. Springer-Verlag/Wien.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{LTG92} Daniel Lischinski, Filippo Tampieri, and Donald P. Greenberg. Discontinuity meshing for accurate radiosity. <i>IEEE Computer Graphics and Applications</i>, 12(6):25--39, November 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{Mak96} Oleg A. Makhotkin. Analysis of radiative transfer between surfaces by hemispherical harmonics. <i>Journal of Quantitative Spectroscopy and Radiative Transfer</i>, 56(6):869--879, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{PB95} Sumanta N. Pattanaik and Kadi Bouatouch. Haar wavelet: A solution to global illumination with general surface properties. In <i>Photorealistic Rendering Techniques (Proceedings of Fifth Eurographics Workshop on Rendering)</i>. Springer-Verlag, Berlin, June 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Ram02} Ravi Ramamoorthi. <i>A Signal-Processing Framework for Forward and Inverse Rendering</i>. PhD thesis, Stanford University, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882442</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{RFS03} Jaume Rigau, Miquel Feixas, and Mateu Sbert. Refinement criteria based on f-divergences. In <i>Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 260--269. Eurographics Association, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566611</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{RH02} Ravi Ramamoorthi and Pat Hanrahan. Frequency space environment map rendering. In <i>Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, pages 517--526. ACM Press, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122739</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{SAWG91} Fran&#231;ois X. Sillion, James R. Arvo, Stephen H. Westin, and Donald P. Greenberg. A global illumination solution for general reflectance distributions. In <i>Proceedings of the 18th annual conference on Computer graphics and interactive techniques</i>, pages 187--196. ACM Press, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{SC94} Peter Shirley and Kenneth Chiu. Notes on adaptive quadrature on the hemisphere. Technical Report TR-411, Dept. of Computer Science, Indiana University, July 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{SDS95} Fran&#231;ois Sillion, George Drettakis, and Cyril Soler. A clustering algorithm for radiance calculation in general environments. In <i>Rendering Techniques</i>, June 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{SH95} P. Schr&#246;der and Patrick Hanrahan. Wavelet methods for radiance computations. In G. Sakas and P. Shirley, editors, <i>Photorealistic Rendering Techniques (Proceedings Fifth Eurographics Workshop on Rendering)</i>, pages 310--326. Springer-Verlag, Berlin, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882281</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{SHHS03} Peter-Pike Sloan, Jesse Hall, John Hart, and John Snyder. Clustered principal components for precomputed radiance transfer. <i>ACM Trans. Graph. (Proceedings of SIGGRAPH 2003)</i>, 22(3):382--391, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>282231</ref_obj_id>
				<ref_obj_pid>280953</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{SHS98} P. Slusallek, W. Heidrich, and H.-P. Seidel. Radiance maps: An image-based approach to global illumination. SIGGRAPH '98, Technical Sketch, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{SKS02} Peter-Pike Sloan, Jan Kautz, and John Snyder. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. <i>ACM Trans. Graph. (Proceedings of SIGGRAPH 2002)</i>, 21(3):527--536, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{SP01} Xavier Serpaggi and Bernard P&#233;roche. An adaptive method for indirect illumination using light vectors. In A. Chalmers and T.-M. Rhyne, editors, <i>Computer Graphics Forum (EUROGRAPHICS 2001 Proceedings)</i>, volume 20(3), pages 278--287. Blackwell Publishing, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{SS95} Peter Schr&#246;der and Wim Sweldens. Spherical wavelets: efficiently representing functions on the sphere. In <i>Proceedings of the 22nd annual conference on Computer graphics and interactive techniques</i>, pages 161--172. ACM Press, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732132</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{SS00} Maryann Simmons and Carlo H. S&#233;quin. Tapestry: A dynamic mesh-based display representation for interactive rendering. In <i>Proceedings of the 11th Eurographics Workshop on Rendering</i>, pages 329--340, June 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{SSG&#60;sup&#62;+&#60;/sup&#62;00} Marc Stamminger, Annette Scheel, Xavier Granier, Frederic Perez-Cazorla, George Drettakis, and Fran&#231;ois Sillion. Efficient glossy global illumination with interactive viewing. <i>Computer Graphics Forum</i>, 19(1), 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{SW94} Peter Shirley and Changyaw Wang. Direct lighting calculation by Monte Carlo integration. In P. Brunet and F. W. Jansen, editors, <i>Photorealistic Rendering in Computer Graphics (Proceedings of the Second Eurographics Workshop on Rendering)</i>, pages 54--59, New York, NY, 1994. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{Sze75} Gabor Szeg&#246;. <i>Orthogonal polynomials</i>. American Mathematical Society, Providence, Rhode Island, 4 edition, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1009583</ref_obj_id>
				<ref_obj_pid>1009379</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{TMS04} Takehiro Tawara, Karol Myszkowski, and Hans-Peter Seidel. Exploiting temporal coherence in final gathering for dynamic scenes. In <i>Proceedings of Computer Graphics International (to appear)</i>, June 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566613</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{TPWG02} Parag Tole, Fabio Pellacini, Bruce Walter, and Donald P. Greenberg. Interactive global illumination in dynamic scenes. <i>ACM Trans. Graph. (Proceedings of SIGGRAPH 2002)</i>, 21(3):537--546, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{War92} Gregory J. Ward. Measuring and modeling anisotropic reflection. In <i>Proceedings of the 19th annual conference on Computer graphics and interactive techniques</i>, pages 265--272. ACM Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{War94} Gregory J. Ward. The radiance lighting simulation and rendering system. In <i>Proceedings of the 21st annual conference on Computer graphics and interactive techniques</i>, pages 459--472. ACM Press, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{WAT92} Stephen H. Westin, James R. Arvo, and Kenneth E. Torrance. Predicting reflectance functions from complex surfaces. In <i>Proceedings of the 19th annual conference on Computer graphics and interactive techniques</i>, pages 255--264. ACM Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{WC92} James C. Wyant and Katherine Creath. Basic wavefront aberration theory for optical metrology. In <i>Applied optics and Optical Engineering, Vol XI</i>, pages 27--39. Academic Press, Inc., 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581901</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{WDGD02} Bruce Walter, George Drettakis, Donald P. Greenberg, and Oliver Deussen. Enhancing and optimizing the render cache. In <i>Proceedings of the 10th Eurographics Workshop on Rendering</i>, June 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383819</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{WDP99} Bruce Walter, George Drettakis, and Steven Parker. Interactive rendering using render cache. In <i>Proceedings of the 13th Eurographics Workshop on Rendering</i>, pages 19--30, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{Wei04a} Eric W. Weisstein. Legendre polynomial. From <i>MathWorld</i> - A Wolfram Web Resource. http://mathworld.wolfram.com/LegendrePolynomial.html, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{Wei04b} Eric W. Weisstein. Spherical coordinates. From <i>MathWorld</i> - A Wolfram Web Resource. http://mathworld.wolfram.com/SphericalCoordinates.html, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{WH92} Gregory J. Ward and Paul S. Heckbert. Irradiance gradients. In <i>Proceedings of the 2nd Eurographics Workshop on Rendering</i>, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{WRC88} Gregory J. Ward, Francis M. Rubinstein, and Robert D. Clear. A ray tracing solution for diffuse interreflection. In <i>Proceedings of the 15th annual conference on Computer graphics and interactive techniques</i>, pages 85--92. ACM Press, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{ZSP98} Jacques Zaninetti, Xavier Serpaggi, and Bernard P&#233;roche. A vector approach for global illumination in ray tracing. <i>Computer Graphics Forum (EUROGRAPHICS '98 Proceedings)</i>, 17(3), 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401229</article_id>
		<sort_key>970</sort_key>
		<display_label>Article No.</display_label>
		<pages>5</pages>
		<display_no>76</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Improved radiance gradient computation]]></title>
		<page_from>1</page_from>
		<page_to>5</page_to>
		<doi_number>10.1145/1401132.1401229</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401229</url>
		<abstract>
			<par><![CDATA[<p>We describe a new and accurate algorithm for computing translational gradients of incoming radiance in the context of a ray tracingbased global illumination solution. The gradient characterizes how the incoming directional radiance function changes with displacement on a surface. We use the gradient for a smoother radiance interpolation over glossy surfaces in the framework of the radiance caching algorithm. The proposed algorithm generalizes the irradiance gradient computation by [Ward and Heckbert 1992] to allow its use for non-diffuse, glossy, surfaces. Compared to previous method for radiance gradient computation, the new algorithm yields better gradient estimates in the presence of significant occlusion changes in the sampled environment, allowing a smoother indirect illumination interpolation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[irradiance gradients]]></kw>
			<kw><![CDATA[radiance caching]]></kw>
			<kw><![CDATA[radiance gradients]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098829</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University, Univ. of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098830</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098831</person_id>
				<author_profile_id><![CDATA[81100332780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kadi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouatouch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA/INRIA Rennes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098832</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383578</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Annen, T., Kautz, J., Durand, F., and Seidel, H.-P. 2004. Spherical harmonic gradients for mid-range illumination. In <i>Proceedings of the Eurographics Symposium on Rendering 2004</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192250</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Arvo, J. 1994. The irradiance jacobian for partially occluded polyhedral sources. In <i>Proceedings of SIGGRAPH '94</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383577</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gautron, P., K&#345;iv&#225;nek, J., Pattanaik, S. N., and Bouatouch, K. 2004. A novel hemispherical basis for accurate and efficient rendering. In <i>Eurographics Symposium on Rendering</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Holzschuch, N., and Sillion, F. 1995. Accurate computation of the radiosity gradient with constant and linear emitters. In <i>Sixth Eurographics Workshop on Rendering</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1080029</ref_obj_id>
				<ref_obj_pid>1079840</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[K&#345;iv&#225;nek, J., Gautron, P., Pattanaik, S., and Bouatouch, K. 2005. Radiance caching for efficient global illumination computation. <i>Transactions on Visualization and Computer Graphics (accepted for publication)</i>. Also available as Technical Report #1623, IRISA, http://graphics.cs.ucf.edu/RCache/index.php.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P. F., Foo, S.-C., Torrance, K. E., and Greenberg, D. P. 1997. Non-linear approximation of reflectance functions. In <i>Proceedings of SIGGRAPH '97</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Schr&#246;der, P., and Sweldens, W. 1995. Spherical wavelets: efficiently representing functions on the sphere. In <i>Proceedings of SIGGRAPH</i>, ACM Press, 161--172.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J., and Heckbert, P. S. 1992. Irradiance gradients. In <i>Eurographics Workshop on Rendering</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J., Rubinstein, F. M., and Clear, R. D. 1988. A ray tracing solution for diffuse interreflection. In <i>Proceedings of SIGGRAPH '88</i>, 85--92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J. 1992. Measuring and modeling anisotropic reflection. In <i>Proceedings of SIGGRAPH '92</i>, ACM Press, 265--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Westin, S. H., 2000. Lafortune BRDF for RenderMan. http://www.graphics.cornell.edu/westin/lafortune/lafortune.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401230</article_id>
		<sort_key>980</sort_key>
		<display_label>Article No.</display_label>
		<pages>12</pages>
		<display_no>77</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Making radiance and irradiance caching practical]]></title>
		<subtitle><![CDATA[adaptive caching and neighbor clamping]]></subtitle>
		<page_from>1</page_from>
		<page_to>12</page_to>
		<doi_number>10.1145/1401132.1401230</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401230</url>
		<abstract>
			<par><![CDATA[<p>Radiance and irradiance caching are efficient global illumination algorithms based on interpolating indirect illumination from a sparse set of cached values. In this paper we propose an adaptive algorithm for guiding spatial density of the cached values in radiance and irradiance caching. The density is adapted to the rate of change of indirect illumination in order to avoid visible interpolation artifacts and produce smooth interpolated illumination. In addition, we discuss some practical problems arising in the implementation of radiance and irradiance caching, and propose techniques for solving those problems. Namely, the neighbor clamping heuristic is proposed as a robust means for detecting small sources of indirect illumination and for dealing with problems caused by ray leaking through small gaps between adjacent polygons.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098833</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University in Prague, Czech Republic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098834</person_id>
				<author_profile_id><![CDATA[81100332780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kadi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouatouch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA -- INRIA Rennes, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098835</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098836</person_id>
				<author_profile_id><![CDATA[81100152267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ji&#345;&#237;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[&#381;&#225;ra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University in Prague, Czech Republic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>154731</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{CW93} Cohen M. F., Wallace J. R.: <i>Radiosity and Realistic Image Synthesis</i>. Morgan Kaufmann, San Francisco, CA, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073320</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{DHS*05} Durand F., Holzschuch N., Soler C., Chan E., Sillion F. X.: A frequency analysis of light transport. <i>ACM Trans. Graph. (Proceedings of SIGGRAPH 2005) 24</i>, 3 (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{DS04} Dmitriev K., Seidel H.-P.: Progressive path tracing with lightweight local error estimation. In <i>Vision, modeling, and visualization 2004 (VMV-04)</i> (2004).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{Due05} Duer A.: On the ward model for global illumination. Unpublished material, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{GH97} Gibson S., Hubbold R.: Perceptually-driven radiosity. <i>Computer Graphics Forum 16</i>, 2 (June 1997).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383661</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{GKBP05} Gautron P., K&#345;iv&#225;nek J., Bouatouch K., Pattanaik S. N.: Radiance cache splatting: A GPU-friendly global illumination algorithm. In <i>Rendering Techniques 2005</i> (June 2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383577</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{GKPB04} Gautron P., K&#345;iv&#225;nek J., Pattanaik S. N., Bouatouch K.: A novel hemispherical basis for accurate and efficient rendering. In <i>Rendering Techniques 2004</i> (June 2004), pp. 321--330.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{HHS05} Havran V., Herzog R., Seidel H.-P.: Fast final gathering via reverse photon mapping. <i>Computer Graphics Forum 24</i> (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Jen01} Jensen H. W.: <i>Realistic Image Synthesis Using Photon Mapping</i>. A. K. Peters Ltd., Natick, MA, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Ministry of Education of the Czech Republic</funding_agency>
			<grant_numbers>
				<grant_number>LC-06008MSM 6840770014</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401231</article_id>
		<sort_key>990</sort_key>
		<display_label>Article No.</display_label>
		<pages>10</pages>
		<display_no>78</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Radiance cache splatting]]></title>
		<subtitle><![CDATA[a GPU-friendly global illumination algorithm]]></subtitle>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/1401132.1401231</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401231</url>
		<abstract>
			<par><![CDATA[<p>Fast global illumination computation is a challenge in several fields such as lighting simulation and computergenerated visual effects for movies. To this end, the irradiance caching algorithm is commonly used since it provides high-quality rendering in a reasonable time. However this algorithm relies on a spatial data structure in which nearest-neighbors queries and data insertions are performed alternately within a single rendering step. Due to this central and permanently modified data structure, the irradiance caching algorithm cannot be easily implemented on graphics hardware. This paper proposes a novel approach to global illumination using irradiance and radiance cache: the <i>radiance cache splatting</i>. This method directly meets the processing constraints of graphics hardware since it avoids the need of complex data structure and algorithms. Moreover, the rendering quality remains identical to classical irradiance and radiance caching. Our renderer shows an implementation of our algorithm which provides a significant speedup compared to classical irradiance caching.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098837</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida, Orlando, FL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098838</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida, Orlando, FL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098839</person_id>
				<author_profile_id><![CDATA[81100332780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kadi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouatouch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA (Universit&#233; de Rennes 1), Rennes, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098840</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida, Orlando, FL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{BP04} Bunnell M., Pellacini F.: <i>GPU Gems: Shadow map antialiasing</i>, 1 ed. Addison Wesley, 2004, pp. 185--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{BWPP04} Bittner J., Wimmer M., Piringer H., Purgathofer W.: Coherent hierarchical culling: Hardware occlusion queries made useful. In <i>Proceedings of Eurographics</i> (2004), pp. 615--624.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325171</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{CG85} Cohen M., Greenberg D. P.: The hemi-cube: A radiosity solution for complex environments. In <i>Proceedings of SIGGRAPH</i> (1985), vol. 19, pp. 31--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569052</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{CHH02} Carr N. A., Hall J. D., Hart J. C.: The ray engine. In <i>Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware</i> (2002), pp. 37--46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844182</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{CHH03} Carr N. A., Hall J. D., Hart J. C.: GPU algorithms for radiosity and subsurface scattering. In <i>Proceedings of SIGGRAPH/Eurographics Workshop on Graphics hardware</i> (2003), pp. 51--59.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1006078</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{CHL04} Coombe G., Harris M. J., Lastra A.: Radiosity on graphics hardware. In <i>Proceedings of Graphics Interface</i> (2004), pp. 161--168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053460</ref_obj_id>
				<ref_obj_pid>1053427</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{DS05} Dachsbacher C., Stamminger M.: Reflective shadow maps. In <i>Proceedings of the Symposium on Interactive 3D graphics and games</i> (2005), pp. 203--231.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383577</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{GKPB04} Gautron P., K&#345;iv&#225;nek J., Pattanaik S., Bouatouch K.: A novel hemispherical basis for accurate and efficient rendering. In <i>Proceedings of Eurographics Symposium on Rendering</i> (2004), pp. 321--330.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{GTGB84} Goral C. M., Torrance K. E., Greenberg D. P., Battaile B.: Modelling the interaction of light between diffuse surfaces. In <i>Proceedings of SIGGRAPH</i> (1984), vol. 18, pp. 212--222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383549</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{GWS04} G&#252;nther J., Wald I., Slusallek P.: Realtime caustics using distributed photon mapping. In <i>Proceedings of Eurographics Symposium on Rendering</i> (2004), pp. 111--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Jen01} Jensen H. W.: <i>Realistic Image Synthesis Using Photon Mapping</i>. AK Peters, July 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{KGBP05} K&#345;iv&#225;nek J., Gautron P., Bouatouch K., Pattanaik S.: Improved radiance gradient computation. In <i>Proceedings of SCCG</i> (2005), pp. 149--153.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1080029</ref_obj_id>
				<ref_obj_pid>1079840</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{KGPB05} K&#345;iv&#225;nek J., Gautron P., Pattanaik S., Bouatouch K.: Radiance caching for efficient global illumination computation. <i>To appear in IEEE Transactions on Visualization and Computer Graphics</i> (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581934</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{KSS02} Kautz J., Sloan P.-P., Snyder J.: Fast, arbitrary brdf shading for low-frequency lighting using spherical harmonics. In <i>Proceedings of Eurographics workshop on Rendering</i> (2002), Eurographics Association, pp. 291--296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383550</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{LC04} Larsen B. D., Christensen N.: Simulating photon mapping for real-time applications. In <i>Proceedings of Eurographics Symposium on Rendering</i> (2004), pp. 123--131.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>604511</ref_obj_id>
				<ref_obj_pid>604471</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{LP03} Lavignotte F., Paulin M.: Scalable photon splatting for global illumination. In <i>Proceedings of GRAPHITE</i> (2003), pp. 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383579</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{LSSS04} Liu X., Sloan P.-P., Shum H.-Y., Snyder J.: All-frequency precomputed radiance transfer for glossy objects. In <i>Proceedings of Eurographics Symposium on Rendering</i> (2004), pp. 337--344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569059</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{MM02} Ma V. C. H., McCool M. D.: Low latency photon mapping using block hashing. In <i>Proceedings of SIGGRAPH/Eurographics Workshop on Graphics Hardware</i> (2002), pp. 89--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946934</ref_obj_id>
				<ref_obj_pid>946250</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{NPG03} Nijasure M., Pattanaik S., Goel V.: Interactive global illumination in dynamic environments using commodity graphics hardware. In <i>Proceedings of Pacific Graphics</i> (2003), pp. 450--454.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{NPG04} Nijasure M., Pattanaik S., Goel V.: Real-time global illumination on the GPU. <i>To appear in Journal of Graphics Tools</i> (2004).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566640</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{PBMH02} Purcell T. J., Buck I., Mark W. R., Hanrahan P.: Ray tracing on programmable graphics hardware. In <i>Proceedings of SIGGRAPH</i> (2002), pp. 703--712.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844181</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{PDC*03} Purcell T. J., Donner C., Cammarano M., Jensen H. W., Hanrahan P.: Photon mapping on programmable graphics hardware. In <i>Proceedings of Graphics Hardware</i> (2003), pp. 41--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732111</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{SB97} Sturzlinger W., Bastos R.: Interactive rendering of globally illuminated glossy scenes. In <i>Proceedings of Eurographics Workshop on Rendering</i> (1997), pp. 93--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882281</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{SHHS03} Sloan P.-P., Hall J., Hart J., Snyder J.: Clustered principal components for precomputed radiance transfer. In <i>Proceedings of SIGGRAPH</i> (2003), pp. 382--391.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{SKS02} Sloan P.-P., Kautz J., Snyder J.: Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. <i>Proceedings of SIGGRAPH</i> (2002), 527--536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74368</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{SP89} Sillion F., Puech C.: A general two-pass method integrating specular and diffuse reflection. In <i>Proceedings of SIGGRAPH</i> (1989), vol. 23, pp. 335--344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015748</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{TL04} Tabellion E., Lamorlette A.: An approximate global illumination system for computer generated films. In <i>Proceedings of SIGGRAPH</i> (2004), pp. 469--476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566613</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{TPWG02} Tole P., Pellacini F., Walter B., Greenberg D. P.: Interactive global illumination in dynamic scenes. In <i>Proceedings of SIGGRAPH</i> (2002), pp. 537--546.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{War94} Ward G. J.: The Radiance lighting simulation and rendering system. In <i>Proceedings of SIGGRAPH</i> (1994), pp. 459--472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{War04} Ward G. J.: <i>Radiance Synthetic Imaging System</i>. http://radsite.lbl.gov/radiance, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882415</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{WBS03} Wald I., Benthin C., Slusallek P.: Interactive global illumination in complex and highly occluded environments. In <i>Proceedings of Eurographics Symposium on Rendering</i> (2003), pp. 74--81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{WH92} Ward G. J., Heckbert P. S.: Irradiance gradients. In <i>Proceedings of Eurographics Workshop on Rendering</i> (1992), pp. 85--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Wil78} Williams L.: Casting curved shadows on curved surfaces. In <i>Proceedings of SIGGRAPH</i> (1978), pp. 270--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{WRC88} Ward G. J., Rubinstein F. M., Clear R. D.: A ray tracing solution for diffuse interreflection. In <i>Proceedings of SIGGRAPH</i> (1988), pp. 85--92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>337722</ref_obj_id>
				<ref_obj_pid>337680</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{WS99} Ward G., Simmons M.: The holodeck ray cache: an interactive rendering system for global illumination in nondiffuse environments. <i>ACM Trans. Graph. 18</i>, 4 (1999), 361--368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{WS03} Wand M., Strasser W.: Real-time caustics. In <i>Proceedings of Eurographics</i> (2003), pp. 611--620.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383580</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{WTL04} Wang R., Tran J., Luebke D.: All-frequency relighting of non-diffuse objects using separable BRDF approximation. In <i>Proceedings of Eurographics Symposium on Rendering</i> (2004), pp. 345--354.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401232</article_id>
		<sort_key>1000</sort_key>
		<display_label>Article No.</display_label>
		<pages>29</pages>
		<display_no>79</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Temporal radiance caching]]></title>
		<page_from>1</page_from>
		<page_to>29</page_to>
		<doi_number>10.1145/1401132.1401232</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401232</url>
		<abstract>
			<par><![CDATA[<p>We present a novel method for fast, high quality computation of glossy global illumination in complex animated environments. Building on the irradiance caching and radiance caching algorithms, our method leverages temporal coherence by introducing temporal gradients. Using our approach, part of the global illumination solution computed in previous frames is adaptively reused in the current frame. Our simple adaptive reusing scheme allows to obtain fast rendering times while avoiding the presence of flickering artifacts and global illumination ghosts. By reusing data in several frames, our method yields a significant speedup compared to classical computation in which a new cache is computed for every frame. Moreover, temporal gradients do not rely on any new, complicated data structure. This method can be straightforwardly included in any existing renderer based on irradiance and radiance caching. Furthermore, our method can be easily implemented using GPUs for improved performance.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[dynamic environments]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[irradiance and radiance caching]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098841</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Syst&#232;mes cognitifs, Projets SIAMES]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098842</person_id>
				<author_profile_id><![CDATA[81100332780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kadi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouatouch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Syst&#232;mes cognitifs, Projets SIAMES]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098843</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Syst&#232;mes cognitifs, Projets SIAMES]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192195</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{BFMZ94} Gary Bishop, Henry Fuchs, Leonard McMillan, and Ellen J. Scher Zagier. Frameless rendering: double buffering considered harmful. In <i>Proceedings of SIGGRAPH</i>, pages 175--176, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{BP01} Gonzalo Besuievsky and Xavier Pueyo. Animating radiosity environments through the multi-frame lighting method. <i>Journal of Visualization and Computer Graphics</i>, 12:93--106, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275478</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{BS96} Gonzalo Besuievsky and Mateu Sbert. The multi-frame lighting method: A monte carlo based solution for radiosity in dynamic environments. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 185--194, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{BWCG86} Daniel R. Baum, John R. Wallace, Michael F. Cohen, and Donald P. Greenberg. The back-buffer algorithm: An extension of the radiosity method to dynamic environments. <i>The Visual Computer</i>, 2(5):298--306, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Dam01} Cyrille Damez. <i>Simulation Globale de l'Eclairage Pour Des Sequences Animees Prenant En Compte la Coherence Temporelle</i>. PhD thesis, Universite Joseph Fourier, Grenoble, France, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581900</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{DBMS02} Kirill Dmitriev, Stefan Brabec, Karol Myszkowski, and Hans-Peter Seidel. Interactive global illumination using selective photon tracing. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 25--36, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{DDM02} Cyrille Damez, Kirill Dmitriev, and Karol Myszkowski. Global illumination for interactive applications and high-quality animations. In <i>Proceedings of Eurographics</i>, pages 55--77, September 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258772</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{DS97} George Drettakis and Francois X. Sillion. Interactive update of global illumination using a line-space hierarchy. In <i>Proceedings of SIGGRAPH</i>, volume 31, pages 57--64, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{DSH01} Cyrille Damez, Francois X. Sillion, and Nicolas Holzschuch. Space-time hierarchical radiosity with clustering and higher-order wavelets. In <i>Proceedings of Eurographics</i>, pages 129--141, September 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>990006</ref_obj_id>
				<ref_obj_pid>990002</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{GD04} Xavier Granier and George Drettakis. A final reconstruction approach for a unified global illumination algorithm. <i>ACM Transactions on Graphics</i>, 23(2):163--189, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383661</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{GKBP05} Pascal Gautron, Jaroslav K&#345;iv&#225;nek, Kadi Bouatouch, and Sumanta Pattanaik. Radiance cache splatting: A GPU-friendly global illumination algorithm. In <i>Proceedings of Eurographics Symposium on Rendering</i>, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383577</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{GKPB04} Pascal Gautron, Jaroslav K&#345;iv&#225;nek, Sumanta Pattanaik, and Kadi Bouatouch. A novel hemispherical basis for accurate and efficient rendering. In <i>Proceedings of Eurographics Symposium on Rendering</i>, pages 321--330, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{GTGB84} Cindy M. Goral, Kenneth E. Torrance, Donald P. Greenberg, and Bennett Battaile. Modelling the interaction of light between diffuse surfaces. In <i>Proceedings of SIGGRAPH</i>, pages 212--222, July 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{Jen01} Henrik Wann Jensen. <i>Realistic Image Synthesis Using Photon Mapping</i>. AK Peters, July 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{KGBP05} Jaroslav K&#345;iv&#225;nek, Pascal Gautron, Kadi Bouatouch, and Sumanta Pattanaik. Improved radiance gradient computation. In <i>Proceedings of SCCG</i>, pages 149--153, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1080029</ref_obj_id>
				<ref_obj_pid>1079840</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{KGPB05} Jaroslav K&#345;iv&#225;nek, Pascal Gautron, Sumanta Pattanaik, and Kadi Bouatouch. Radiance caching for efficient global illumination computation. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 11(5):550--561, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614546</ref_obj_id>
				<ref_obj_pid>614289</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{MPT03} Ignacio Mart&#237;n, Xavier Pueyo, and Dani Tost. Frame-to-frame coherent animation with two-pass radiosity. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 9(1):70--84, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{PTMG97} Xavier Pueyo, Dani Tost, Ignacio Mart&#237;n, and Blanca Garcia. Radiosity for dynamic environments. <i>The Journal of Visualization and Computer Animation</i>, 8(4):221--231, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{SKDM05} Miloslaw Smyk, Shin-ichi Kinuwaki, Roman Durikovic, and Karol Myszkowski. Temporally coherent irradiance caching for high quality animation rendering. In <i>Proceedings of Eurographics</i>, volume 24, pages 401--412, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{TMS02} Takehiro Tawara, Karol Myszkowski, and Hans-Peter Seidel. Localizing the final gathering for dynamic scenes using the photon map. In <i>VMV</i>, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1009583</ref_obj_id>
				<ref_obj_pid>1009379</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{TMS04} T. Tawara, K. Myszkowski, and H.-P. Seidel. Exploiting temporal coherence in final gathering for dynamic scenes. In <i>Proceedings of Computer Graphics International</i>, pages 110--119, June 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566613</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{TPWG02} Parag Tole, Fabio Pellacini, Bruce Walter, and Donald P. Greenberg. Interactive global illumination in dynamic scenes. In <i>Proceedings of SIGGRAPH</i>, pages 537--546, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581901</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{WDG02} Bruce Walter, George Drettakis, and Donald P. Greenberg. Enhancing and optimizing the render cache. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 37--42, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383819</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{WDP99} Bruce Walter, George Drettakis, and Steven Parker. Interactive rendering using the render cache. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 235--246, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{WH92} Gregory J. Ward and Paul S. Heckbert. Irradiance gradients. In <i>Proceedings of Eurographics Workshop on Rendering</i>, pages 85--98, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{WRC88} Gregory J. Ward, Francis M. Rubinstein, and Robert D. Clear. A ray tracing solution for diffuse interreflection. In <i>Proceedings of SIGGRAPH</i>, pages 85--92, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401233</section_id>
		<sort_key>1010</sort_key>
		<section_seq_no>22</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Principles of appearance acquisition and representation]]></section_title>
		<section_page_from>22</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098844</person_id>
				<author_profile_id><![CDATA[81365593328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weyrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098845</person_id>
				<author_profile_id><![CDATA[81100574409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawrence]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098846</person_id>
				<author_profile_id><![CDATA[81100504611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lensch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098847</person_id>
				<author_profile_id><![CDATA[81100203803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Szymon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098848</person_id>
				<author_profile_id><![CDATA[81100015971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zickler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401234</article_id>
		<sort_key>1020</sort_key>
		<display_label>Article No.</display_label>
		<pages>119</pages>
		<display_no>80</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Principles of appearance acquisition and representation]]></title>
		<page_from>1</page_from>
		<page_to>119</page_to>
		<doi_number>10.1145/1401132.1401234</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401234</url>
		<abstract>
			<par><![CDATA[<p>Algorithms for scene understanding and realistic image synthesis require accurate models of the way real-world materials scatter light. This class describes recent work in the graphics community to measure the spatially- and directionally-varying reflectance and subsurface scattering of complex materials, and to develop efficient representations and analysis tools for these datasets. We describe the design of acquisition devices and capture strategies for BRDFs and BSSRDFs, efficient factored representations, and a case study of capturing the appearance of human faces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098849</person_id>
				<author_profile_id><![CDATA[81100258924]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weyrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098850</person_id>
				<author_profile_id><![CDATA[81100574409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawrence]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098851</person_id>
				<author_profile_id><![CDATA[81100504611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lensch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098852</person_id>
				<author_profile_id><![CDATA[81100203803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Szymon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098853</person_id>
				<author_profile_id><![CDATA[81100015971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zickler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Ami02} Isaac Amidror. Scattered data interpolation methods for electronic imaging systems: a survey. <i>Journal of Electronic Imaging</i>, 11(2):157--176, April 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383270</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{BG01} S. Boivin and A. Gagalowicz. Image-based rendering of diffuse, specular and glossy surfaces from a single image. <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, pages 107--116, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>965470</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{BL03} George Borshukov and J. P. Lewis. Realistic human face rendering for "the matrix reloaded". In <i>ACM SIGGRAPH 2003 Conference Abstracts and Applications (Sketch)</i>, page 1, New York, NY, USA, 2003. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>944937</ref_obj_id>
				<ref_obj_pid>944919</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{BNJ03} D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. <i>Journal of Machine Learning Research</i>, 3:993--1022, January 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{BP01} Svetlana Barsky and Maria Petrou. Colour photometric stereo: Simultaneous reconstruction of local gradient and colour of rough textured surfaces. In <i>ICCV</i>, pages 600--605, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{BV99} Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In <i>SIGGRAPH '99: Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, pages 187--194, New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{CCT} Camera calibration toolbox for matlab. http://www.vision.caltech.edu/bouguetj/calib_doc/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{CD02} Oana G. Cula and Kristin J. Dana. Image-based skin analysis. In <i>Texture 2002</i>, The Second International Workshop on Texture Analysis and Synthesis, pages 35--42, Copenhagen, Denmark, June 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073328</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{CJAMJ05} Petrik Clarberg, Wojciech Jarosz, Tomas Akenine-M&#246;ller, and HenrikWann Jensen. Wavelet importance sampling: Efficiently evaluating products of complex functions. <i>ACM Transactions on Graphics (Proc. SIGGRAPH 2005)</i>, 24(3), 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{CLFS07} T. Chen, H. P. A. Lensch, C. Fuchs, and H. P. Seidel. Polarization and Phase-Shifting for 3D Scanning of Translucent Objects. <i>Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on</i>, pages 1--8, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{Coo84} R. L. Cook. Shade trees. In <i>Computer Graphics</i>, volume 18 of <i>SIGGRAPH 84 Proceedings</i>, pages 223--231, July 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357293</ref_obj_id>
				<ref_obj_pid>357290</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{CT82} Robert L. Cook and Kenneth E. Torrance. A reflection model for computer graphics. <i>ACM Transactions On Graphics</i>, 1(1):7--24, January 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Dan92} James L. Dannemiller. Spectral reflectance of natural objects: how many basis functions are necessary? <i>Journal of the Optical Society of America A</i>, 9(4):507--515, April 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{DHT&#60;sup&#62;+&#60;/sup&#62;00a} P. Debevec, T. Hawkins, C. Tchou, H.-P. Duiker, W. Sarokin, and M. Sagar. Acquiring the Reflectance Field of a Human Face. In <i>Proc. ACM SIGGRAPH</i>, pages 145--156, July 2000. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{DHT&#60;sup&#62;+&#60;/sup&#62;00b} Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring the reflectance field of a human face. In <i>Computer Graphics</i>, SIGGRAPH 2000 Proceedings, pages 145--156, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073308</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{DJ05a} Craig Donner and Henrik Wann Jensen. Light diffusion in multi-layered translucent materials. <i>ACM Transactions on Graphics</i>, 24(3):1032--1039, August 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073308</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{DJ05b} Craig Donner and Henrik Wann Jensen. Light diffusion in multi-layered translucent materials. <i>ACM Transactions on Graphics (Proc. SIGGRAPH 2005)</i>, 24(3):1032--1039, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383946</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{DJ06} Craig Donner and Henrik W. Jensen. A spectral bssrdf for shading human skin. In <i>Proceedings of the 16th Eurographics Symposium on Rendering</i>, pages 409--417. Eurographics Association, June 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{DM97} Paul Debevec and Jitendra Malik. Recovering high dynamic range radiance maps from photographs. In <i>Computer Graphics</i>, SIGGRAPH 97 Proceedings, pages 369--378, Los Angeles, CA, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>794511</ref_obj_id>
				<ref_obj_pid>794189</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{DvGNK97} Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, and Jan J. Koenderink. Reflectance and texture of real-world surfaces. In <i>IEEE Conference on Computer Vision and Pattern Recognition</i>, pages 151--157, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{DvGNK99} Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, and Jan J. Koenderink. Reflectance and texture of real world surfaces. <i>ACM Transactions on Graphics</i>, 1(18):1--34, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{DW04} K. J. Dana and J. Wang. Device for convenient measurement of spatially varying bidirectional reflectance. <i>Journal of the Optical Society of America A</i>, 21(1):1--12, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383917</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{ECJ&#60;sup&#62;+&#60;/sup&#62;06} Per Einarsson, Charles-Felix Chabert, Andrew Jones, Wan-Chun Ma, Bruce Lamond, Tim Hawkins, Mark Bolas, Sebastian Sylwan, and Paul Debevec. Relighting human locomotion with flowed reflectance fields. In <i>Rendering Techniques 2006: 17th Eurographics Workshop on Rendering</i>, pages 183--194, June 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1243984</ref_obj_id>
				<ref_obj_pid>1243980</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{FBLS07} Martin Fuchs, Volker Blanz, Hendrik P. A. Lensch, and Hans-Peter Seidel. Adaptive sampling of reflectance fields. <i>ACM Transactions on Graphics</i>, 26(2), June 2007. Article 10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Fee04} Catherine Feeny. Servers, spydercams and 'Spider-Man 2'. In <i>VFX Pro</i>, http://www.uemedia.net/CPC/vfxpro/printer_9050.shtml, July 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{FFP05} L. Fei-Fei and P. Perona. A bayesian hierarchical model for learning natural scene categories. In <i>IEEE Computer Vision and Pattern Recognition</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{Fit88} T. B. Fitzpatrick. The validity and practicality of sun-reactive skin types I through VI. <i>Arch. Dermatology</i>, 124:869--871, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{FL00} Brian V. Funt and Benjamin C. Lewis. Diagonal versus affine transformations for color correction. <i>Journal of the Optical Society of America A</i>, 17(11):2108--2112, November 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053686</ref_obj_id>
				<ref_obj_pid>1053554</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{FLS05} Martin Fuchs, Hendrik P. A. Lensch, and Hans-Peter Seidel. Reflectance from images: A model-based approach for human faces. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 11(3):296--305, 2005. Member-Volker Blanz.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{GaHSSR04} Andrew Gelman, John B. Carlin ad Hal S. Stern, and Donald B. Rubin. <i>Bayesian Data Analysis</i>. Chapman and Hall, 2nd edition, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{GCHS03} Dan B. Goldman, Brian Curless, Aaron Hertzmann, and Steven M. Seitz. Shape and spatially-varying brdfs from photometric stereo. Technical Report 04-05-03, University of Washington, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1097667</ref_obj_id>
				<ref_obj_pid>1097114</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{GCHS05a} Dan B Goldman, Brian Curless, Aaron Hertzmann, and Steven M. Seitz. Shape and spatially-varying BRDFs from photometric stereo. In <i>IEEE International Conference on Computer Vision</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1097667</ref_obj_id>
				<ref_obj_pid>1097114</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{GCHS05b} D. B. Goldman, B. Curless, A. Hertzmann, and S. M. Seitz. Shape and spatially-varying brdfs from photometric stereo. <i>IEEE International Conference on Computer Vision</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882438</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Geo03} Athinodoros S. Georghiades. Recovering 3-D shape and reflectance from a small number of photographs. In <i>Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 230--240, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{GGSC96} Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The lumigraph. In <i>Computer Graphics</i>, SIGGRAPH 96 Proceedings, pages 43--54, New Orleans, LS, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015807</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{GLL&#60;sup&#62;+&#60;/sup&#62;04a} Michael Goesele, Hendrik P. A. Lensch, Jochen Lang, Christian Fuchs, and Hans-Peter Seidel. DISCO - Acquisition of Translucent Objects. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2004)</i>, 23(3), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015807</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{GLL&#60;sup&#62;+&#60;/sup&#62;04b} Michael Goesele, Hendrik P. A. Lensch, Jochen Lang, Christian Fuchs, and Hans-Peter Seidel. DISCO---Acquisition of translucent objects. <i>ACM Transactions on Graphics (SIGGRAPH 2004)</i>, 24(3):835--844, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276</ref_obj_id>
				<ref_obj_pid>1271</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{GMSW84} P. Gill, W. Murray, M. Saunders, and M. Wright. Procedures for optimization problems with a mixture of bounds and general linear constraints. In <i>ACM Transactions on Mathematical Software</i>, 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383926</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{GTLL06} Gaurav Garg, Eino-Ville Talvala, Marc Levoy, and Hendrik P. A. Lensch. Symmetric photography: Exploiting data-sparseness in reflectance fields. In <i>Rendering Techniques 2006: 17th Eurographics Workshop on Rendering</i>, pages 251--262, June 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141952</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{GTR+06} Jinwei Gu, Chien-I Tu, Ravi Ramamoorthi, Peter Belhumeur, Wojciech Matusik, and Shree Nayar. Time-varying surface appearance: Acquisition, modeling and rendering. <i>ACM Transactions on Graphics (SIGGGRAPH 2006)</i>, 25(3), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>303816</ref_obj_id>
				<ref_obj_pid>303815</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{Hac99} W. Hackbusch. A Sparse Matrix Arithmetic based on <i>H</i>-Matrices. Part I: Introduction to <i>H</i>-matrices. <i>Computing</i>, 62(2):89--108, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{HB95} David J. Heeger and James R. Bergen. Pyramid-based texture analysis/synthesis. In <i>Proceedings of SIGGRAPH 95</i>, Computer Graphics Proceedings, Annual Conference Series, pages 229--238, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Hea89} G. Healey. Using color for geometry-insensitive segmentation. <i>J. Optical Society of America A</i>, 6(6):920--937, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383666</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{HED05} Tim Hawkins, Per Einarsson, and Paul Debevec. A dual light stage. In <i>Rendering Techniques 2005: 16th Eurographics Workshop on Rendering</i>, pages 91--98, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>965569</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{H&#233;r03} Christophe H&#233;ry. Face cloning at ILM. In <i>SIGGRAPH 2003 Course "Digital Face Cloning"</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{HK93} Pat Hanrahan and Wolfgang Krueger. Reflection from layered surfaces due to subsurface scattering. In <i>Computer Graphics</i>, SIGGRAPH 93 Proceedings, pages 165--174, Anaheim, CA, August 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{Hof99} Thomas Hofmann. Probabilistic latent semantic analysis. In <i>Proceedings of Uncertainty in Artificial Intelligence</i>, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882341</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{HP03} Jefferson Y. Han and Ken Perlin. Measuring bidirectional texture reflectance with a kaleidoscope. <i>ACM Transactions on Graphics</i>, 22(3):741--748, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{HW79} J. A. Hartigan and M. A. Wong. A k-means clustering algorithm. <i>Applied Statistics</i>, 28:100--108, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383575</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{HWT&#60;sup&#62;+&#60;/sup&#62;04} Tim Hawkins, Andreas Wenger, Chris Tchou, Andrew Gardner, Fredrik G&#246;ransson, and Paul Debevec. Animatable facial reflectance fields. In <i>Rendering Techniques '04 (Proceedings of the Second Eurographics Symposium on Rendering)</i>, pages 309--320, Norrk&#246;ping, Sweden, June 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{INN05} Takanori Igarashi, Ko Nishino, and Shree K. Nayar. The appearance of human skin. Technical report, Department of Computer Science, Columbia University CUCS-024-05, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{JMLH01a} Henrik Wann Jensen, Stephen R. Marschner, Marc Levoy, and Pat Hanrahan. A practical model for subsurface light transport. In <i>Proceedings of ACM SIGGRAPH 2001</i>, Computer Graphics Proceedings, Annual Conference Series, pages 511-518, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{JMLH01b} Henrik Wann Jensen, Steven R. Marschner, Marc Levoy, and Pat Hanrahan. A practical model for subsurface light transport. In <i>Computer Graphics</i>, SIGGRAPH 2001 Proceedings, pages 511--518, Los Angeles, CA, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{Jor99} Michael Jordan, editor. <i>Learning in Graphical Models</i>. MIT Press, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>959207</ref_obj_id>
				<ref_obj_pid>959196</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{KP03} Jan Koenderink and Sylvia Pont. The secret of velvety skin. <i>Machine Vision and Application</i>, 14:260--268, 2003. Special Issue on Human Modeling, Analysis, and Synthesis.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{Law06} Jason Lawrence. <i>Acquisition and Representation of Material Appearance for Editing and Rendering</i>. PhD thesis, Department of Computer Science, Princeton University, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141949</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{LBAD&#60;sup&#62;+&#60;/sup&#62;06a} Jason Lawrence, Aner Ben-Artzi, Christopher DeCoro, Wojciech Matusik, Hanspeter Pfister, Ravi Ramamoorthi, and Szymon Rusinkiewicz. Inverse shade trees for non-parametric material representation and editing. <i>ACM Transactions on Graphics (SIGGRAPH 2006)</i>, 25(3), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141949</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{LBAD&#60;sup&#62;+&#60;/sup&#62;06b} Jason Lawrence, Aner Ben-Artzi, Christopher DeCoro, Wojciech Matusik, Hanspeter Pfister, Ravi Ramamoorthi, and Szymon Rusinkiewicz. Inverse shade trees for non-parametric material representation and editing. <i>ACM Transactions on Graphics (SIGGRAPH 2006)</i>, 25(3), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>81040</ref_obj_id>
				<ref_obj_pid>81024</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{LBS90} H. C. Lee, E. J. Breneman, and C. P. Schulte. Modeling light relfection for computer color vision. <i>IEEE Trans. Pattern Analysis and Machine Intelligence</i>, 12(4):402--409, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{LFTG97} Eric P. F. Lafortune, Sing-Choong Foo, Kenneth E. Torrance, and Donald P. Greenberg. Non-linear approximation of reflectance functions. In <i>SIGGRAPH '97: Proceedings of the 24th annual conference on Computer graphics and interactive techniques</i>, pages 117--126, New York, NY, USA, 1997. ACM Press/Addison-Wesley Publishing Co.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{LFTW06} H. Li, S. C. Foo, K. E. Torrance, and S. H. Westin. Automated three-axis gonioreflectometer for computer graphics applications. <i>Optical Engineering</i>, 45:043605, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732303</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{LKG&#60;sup&#62;+&#60;/sup&#62;01} Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter Seidel. Image-based reconstruction of spatially varying materials. In <i>Proceedings of the 12th Eurographics Workshop on Rendering</i>, pages 104--115, June 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>636891</ref_obj_id>
				<ref_obj_pid>636886</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{LKG&#60;sup&#62;+&#60;/sup&#62;03} Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter Seidel. Image-based reconstruction of spatial appearance and geometric detail. <i>ACM Transactions on Graphics</i>, 22(2):234--257, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{LKK98} R. Lu, J. Koenderink, and A. Kappers. Optical properties (Bidirectional Reflectance Distribution Functions) of velvet. <i>Applied Optics</i>, 37(25):5974--5984, September 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015751</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{LRR04} Jason Lawrence, Szymon Rusinkiewicz, and Ravi Ramamoorthi. Efficient BRDF importance sampling using a factored representation. <i>ACM Transactions on Graphics (SIGGRAPH 2004)</i>, 23(3):496--505, August 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{LS99} D. Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. <i>Nature</i>, 401:788--791, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2384346</ref_obj_id>
				<ref_obj_pid>2384344</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{MBK05} Gero M&#252;ller, Gerhard H. Bendels, and Reinhard Klein. Rapid synchronous acquisition of geometry and appearance of cultural heritage artefacts. In <i>VAST 2005: 6th International Symposium on Virtual Reality, Archaeology and Intelligent Cultural Heritage</i>, pages 13--20, November 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{McA02a} David McAllister. <i>A Generalized Surface Appearance Representation for Computer Graphics</i>. Ph.d. thesis, University of North Carolina (UNC), Chapel Hill, NC, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{McA02b} David K. McAllister. <i>A Generalized Surface Appearance Representation for Computer Graphics</i>. PhD thesis, University of North Carolina at Chapel Hill, NC, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{Mer84} S. Mersch. Polarized lighting for machine vision applications. In <i>Proc. RI/SME Third Annual Applied Machine Vision Conf.</i>, pages 40--54. Schaumburg, IL, Feb. 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{Mit} Mitsubishi Electric Research Laboratories (MERL), ETH Zurich. The MERL/ETH skin reflectance database. http://www.merl.com/facescanning/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569057</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{MLH02} D. K. McAllister, A. Lastra, and W. Heidrich. Efficient rendering of spatial bi-directional reflectance distribution functions. <i>Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</i>, pages 79--88, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{MMS&#60;sup&#62;+&#60;/sup&#62;05} G. M&#252;ller, J. Meseth, M. Sattler, R. Sarlette, and R. Klein. Acquisition, synthesis and rendering of bidirectional texture functions. <i>Computer Graphics Forum</i>, 24(1), 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{MN99} T. Mitsunaga and S. K. Nayar. Radiometric Self Calibration. In <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, volume 1, pages 374--380, Jun 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882439</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{MPBM03a} W. Matusik, H. Pfister, M. Brand, and L. McMillan. Efficient isotropic BRDF measurement. <i>Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 241--247, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882343</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{MPBM03b} Wojciech Matusik, Hanspeter Pfister, Matthew Brand, and Leonard McMillan. A data-driven reflectance model. <i>ACM Transactions on Graphics (SIGGRAPH 2003)</i>, 22(3):759--770, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882315</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{MPDW03} Vincent Masselus, Pieter Peers, Philip Dutr&#233;, and Yves D. Willems. Relighting with 4d incident light fields. <i>ACM Transactions on Graphics</i>, 22(3):613--620, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{MSK06} Gero M&#252;ller, Ralf Sarlette, and Reinhard Klein. Data-driven local coordinate systems for image-based rendering. <i>Computer Graphics Forum</i>, 25(3):369--378, September 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383829</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{MWL&#60;sup&#62;+&#60;/sup&#62;99} Steven R. Marschner, Steven H. Westin, Eric P. F. Lafortune, Kenneth E. Torrance, and Donald P. Greenberg. Image-based BRDF measurement including human skin. In <i>Proceedings of the 10th Eurographics Workshop on Rendering</i>, pages 139--152, Granada, Spain, June 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{MWLT00} Steven R. Marschner, Steven H. Westin, Eric P. F. Lafortune, and Kenneth E. Torrance. Image-based measurement of the Bidirectional Reflectance Distribution Function. <i>Applied Optics</i>, 39(16):2592--2600, June 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073262</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{MZD05} Wojciech Matusik, Matthias Zwicker, and Fr&#233;do Durand. Texture design using a simplicial complex of morphable textures. <i>ACM Transactions on Graphics</i>, 24(3):787--794, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1069142</ref_obj_id>
				<ref_obj_pid>1068508</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{MZKB05} Satya P. Mallick, Todd E. Zickler, David J. Kriegman, and Peter N. Belhumeur. Beyond lambert: Reconstructing specular surfaces using color. In <i>CVPR '05: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) --- Volume 2</i>, pages 619--626, Washington, DC, USA, 2005. IEEE Computer Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383671</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{NDM05} Addy Ngan, Fr&#233;do Durand, and Wojciech Matusik. Experimental analysis of brdf models. In <i>Proceedings of the 15th Eurographics Symposium on Rendering</i>, pages 117--226. Eurographics Association, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073226</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{NRDR05} Diego Nehab, Szymon Rusinkiewicz, James Davis, and Ravi Ramamoorthi. Efficiently combining positions and normals for precise 3d geometry. <i>ACM Transactions on Graphics (Proc. SIGGRAPH 2005)</i>, 24(3):536--543, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{NRH&#60;sup&#62;+&#60;/sup&#62;77} F. E. Nicodemus, J. C. Richmond, J. J. Hsia, I. W. Ginsberg, and T. Limperis. Geometric considerations and nomenclature for reflectance. Monograph 161, National Bureau of Standards (US), October 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{OCV} Open source computer vision library. http://opencvlibrary.sourceforge.net/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280825</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{PHL&#60;sup&#62;+&#60;/sup&#62;98} Fr&#233;d&#233;ric Pighin, Jamie Hecker, Dani Lischinski, Richard Szeliski, and David H. Salesin. Synthesizing realistic facial expressions from photographs. In <i>Computer Graphics</i>, volume 32 of <i>SIGGRAPH 98 Proceedings</i>, pages 75--84, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141950</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{PvBM&#60;sup&#62;+&#60;/sup&#62;06a} Pieter Peers, Karl vom Berge, Wojciech Matusik, Ravi Ramamoorthi, Jason Lawrence, Szymon Rusinkiewicz, and Philip Dutr&#233;. A compact factored representation of heterogeneous subsurface scattering. <i>ACM Transactions on Graphics (SIGGRAPH 2006)</i>, 25(3), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141950</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{PvBM&#60;sup&#62;+&#60;/sup&#62;06b} Pieter Peers, Karl vom Berge, Wojciech Matusik, Ravi Ramamoorthi, Jason Lawrence, Szymon Rusinkiewicz, and Philip Dutr&#233;. A compact factored representation of heterogeneous subsurface scattering. <i>ACM Transactions on Graphics (SIGGRAPH 2006)</i>, 25(3), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141950</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{PvBM&#60;sup&#62;+&#60;/sup&#62;06c} Pieter Peers, Karl vom Berge, Wojciech Matusik, Ravi Ramamoorthi, Jason Lawrence, Szymon Rusinkiewicz, and Philip Dutr&#233;. A compact factored representation of heterogeneous subsurface scattering. <i>ACM Transactions on Graphics</i>, 25(3):746--753, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{Rus98} Szymon Rusinkiewicz. A new change of variables for efficient BRDF representation. In G. Drettakis and N. Max, editors, <i>Rendering Techniques '98 (Proceedings of Eurographics Rendering Workshop '98)</i>, pages 11--22, New York, NY, 1998. Springer Wien.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186371</ref_obj_id>
				<ref_obj_pid>1186223</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{Sag04} Mark Sagar. Reflectance field rendering of human faces for "Spider-Man 2". In <i>ACM SIGGRAPH 2004 Sketches</i>, August 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073257</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{SCG&#60;sup&#62;+&#60;/sup&#62;05} Pradeep Sen, Billy Chen, Gaurav Garg, Stephen R. Marschner, Mark Horowitz, Marc Levoy, and Hendrik P. A. Lensch. Dual photography. <i>ACM Transactions on Graphics</i>, 24(3):745--755, August 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{Sha85} S. Shafer. Using color to separate reflection components. <i>COLOR research and applications</i>, 10(4):210--218, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946723</ref_obj_id>
				<ref_obj_pid>946247</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{SNB03} Y. Y. Schechner, S. K. Nayar, and P. N. Belhumeur. A Theory of Multiplexed Illumination. In <i>IEEE International Conference on Computer Vision (ICCV)</i>, volume 2, pages 808--815, Oct 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{Sta} A. Standard. E1392-90, Standard practice for angle resolved optical scatter measurements on specular or diffuse surfaces. <i>American Society for Testing and Materials</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732287</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[{Sta01} Jos Stam. An illumination model for a skin layer bounded by rough surfaces. In <i>Proceedings of the 12th Eurographics Workshop on Rendering Techniques</i>, pages 39--52, Vienna, Austria, London, UK, June 2001. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[{SWI97} Yoichi Sato, Mark D. Wheeler, and Katsushi Ikeuchi. Object shape and reflectance modeling from observation. In <i>Computer Graphics</i>, SIGGRAPH 97 Proceedings, pages 379--387, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882344</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[{TOS&#60;sup&#62;+&#60;/sup&#62;03} Norimichi Tsumura, Nobutoshi Ojima, Kayoko Sato, Mitsuhiro Shiraishi, Hideto Shimizu, Hirohide Nabeshima, Syuuichi Akazaki, Kimihiko Hori, and Yoichi Miyake. Image-based skin color and texture analysis/synthesis by extracting hemoglobin and melanin information in the skin. <i>ACM Transactions on Graphics (Proc. SIGGRAPH 2003)</i>, 22(3):770--779, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{TS67} Kenneth E. Torrance and Ephraim M. Sparrow. Theory for off-specular reflection from roughened surfaces. <i>JOSA</i>, 57(9):1105--1114, September 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[{TW89} S. Tominga and B. A. Wandell. Standard surface-reflectance model and illuminant estimation. <i>J. Optical Society of America A</i>, 6(4):576--584, April 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073311</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[{TWL&#60;sup&#62;+&#60;/sup&#62;05} Xin Tong, Jiaping Wang, Stephen Lin, Baining Guo, and Heung-Yeung Shum. Modeling and rendering of quasi-homogeneous materials. <i>ACM Transactions on Graphics (SIGGRAPH 2005)</i>, 24(3):1054--1061, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566634</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[{TZL&#60;sup&#62;+&#60;/sup&#62;02} Xin Tong, Jingdan Zhang, Ligang Liu, Xi Wang, Baining Guo, and Heung-Yeung Shum. Synthesis of bidirectional texture functions on arbitrary surfaces. In <i>Proceedings of SIGGRAPH</i>, pages 665--672. ACM Press, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015725</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[{VT04} M. Alex Vasilescu and Demetri Terzopoulos. TensorTextures: Multilinear image-based rendering. <i>ACM Transactions on Graphics (SIGGRAPH 2004)</i>, 23(3), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[{War92} G. Ward. Measuring and modeling anisotropic reflection. <i>Computer Graphics</i>, 26(Annual Conference Series):265--273, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[{Wey06} Tim Weyrich. <i>Acquisition of Human Faces using a Measurement-Based Skin Reflectance Model</i>. PhD thesis, Department of Computer Science, ETH Zurich, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073258</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[{WGT&#60;sup&#62;+&#60;/sup&#62;05} Andreas Wenger, Andrew Gardner, Chris Tchou, Jonas Unger, Tim Hawkins, and Paul Debevec. Performance relighting and reflectance transformation with time-multiplexed illumination. <i>ACM Transactions on Graphics</i>, 24(3):756--764, August 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1198591</ref_obj_id>
				<ref_obj_pid>1198555</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[{Wil05} Lance Williams. Case study: The gemini man. In <i>SIGGRAPH 2005 Course 'Digital Face Cloning'</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141987</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[{WMP&#60;sup&#62;+&#60;/sup&#62;06a} Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd Bickel, Craig Donner, Chien Tu, Janet McAndless, Jinho Lee, Addy Ngan, Henrik Wann Jensen, and Markus Gross. Analysis of human faces using a measurement-based skin reflectance model. <i>ACM Transactions on Graphics</i>, 25(3):1013--1024, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141987</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[{WMP&#60;sup&#62;+&#60;/sup&#62;06b} Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd Bickel, Craig Donner, Chien Tu, Janet McAndless, Jinho Lee, Addy Ngan, Henrik Wann Jensen, and Markus Gross. Analysis of human faces using a measurement-based skin reflectance model. <i>ACM Transactions on Graphics (Proc. SIGGRAPH 2006)</i>, pages 1013--1024, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[{WSB&#60;sup&#62;+&#60;/sup&#62;98} D. R. White, P. Saunders, S. J. Bonsey, J. van de Ven, and H. Edgar. Reflectometer for Measuring the Bidirectional Reflectance of Rough Surfaces. <i>Appl. Opt</i>, 37:3450--3454, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141951</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[{WTL&#60;sup&#62;+&#60;/sup&#62;06} Jiaping Wang, Xin Tong, Stephen Lin, Hujun Bao, Baining Guo, and Heung-Yeung Shum. Appearance manifolds for modeling time-variant appearance of materials. <i>ACM Transactions on Graphics (SIGGRAPH 2006)</i>, 25(3), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311559</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[{YDMH99} Yizhou Yu, Paul Debevec, Jitendra Malik, and Tim Hawkins. Inverse global illumination: Recovering reflectance models of real scenes from photographs. In <i>Computer Graphics</i>, SIGGRAPH 99 Proceedings, pages 215--224, Los Angeles, CA, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[{YNBK07} Shuntaro Yamazaki, Srinivasa G. Narasimhan, Simon Baker, and Takeo Kanade. Coplanar shadowgrams for acquiring visual hulls of intricate objects. In <i>Proc. IEEE International Conference of Computer Vision (ICCV)</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649328</ref_obj_id>
				<ref_obj_pid>645317</ref_obj_pid>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[{ZBK02} Todd Zickler, Peter N. Belhumeur, and David J. Kriegman. Helmholtz stereopsis: Exploiting reciprocity for surface reconstruction. In <i>ECCV '02: Proceedings of the 7th European Conference on Computer Vision-Part III</i>, pa--es 869--884, London, UK, 2002. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383690</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[{ZERB05} Todd Zickler, Sebastian Enrique, Ravi Ramamoorthi, and Peter Belhumeur. Reflectance sharing: Image-based rendering from a sparse set of images. In <i>Proceedings of the Eurographics Symposium on Rendering</i>, pages 253--264. Eurographics Association, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1155432</ref_obj_id>
				<ref_obj_pid>1155317</ref_obj_pid>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[{ZREB06} T. Zickler, R. Ramamoorthi, S. Enrique, and P. N. Belhumeur. Reflectance Sharing: Predicting Appearance from a Sparse Set of Images of a Known Shape. <i>Pattern Analysis and Machine Intelligence, IEEE Transactions on</i>, 28(8):1287--1302, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Principles of Appearance Acquisition and Representation SIGGRAPH 2008 Class Notes TimWeyrich Princeton 
University USA Jason Lawrence University ofVirginia USA Hendrik Lensch Max-Planck-Institut für Informatik 
Germany Szymon Rusinkiewicz Princeton University USA Todd Zickler Harvard University USA Los Angeles, 
August 2007 Class Description Algorithms for scene understanding and realistic image synthesis require 
accurate mod­els of the way real-world materials scatter light. This class describes recent work in the 
graphics community to measure the spatially-and directionally-varying re.ectance and subsurface scattering 
of complex materials, and to develop ef.cient representations and analysis toolsfor these datasets.Wedescribethedesignof 
acquisitiondevicesand capture strategies for BRDFs and BSSRDFs, ef.cient factored representations, and 
a case study of capturing the appearance of human faces. Prerequisites Basic familiarity with the computer 
graphics pipeline, together with some knowledge of linear algebra and calculus. Slides The slides presented 
in class are available as separate supplemental material. They are closely aligned with the class notes 
and may be used as an additional reference. Short Biographies Jason Lawrence is an Assistant Professor 
in the Computer Science Department at the UniversityofVirginia.He holdsa Ph.D.in computer sciencefrom 
Princeton University. Jason s principalresearchinterests focusonthe acquisitionandef.cientrepresentationof 
real-world material appearance.Herecentlyintroducedan Inverse ShadeTrees frame­work forrepresenting measured 
surfacere.ectance throughahierarchical decomposition designed for ef.cient rendering and editing. Hendrik 
P. A. Lensch is the head of an independent research group General Ap­pearance Acquisition and Computational 
Photography at the MPI Informatik in Saar­bruecken, Germany. The group is part of the Max Planck Center 
forVisual Computing and Communication (Saarbruecken/Stanford). Hereceived his diplomain computers science 
from the University of Erlangen in 1999. He worked as a research associate at Hans-Peter Seidel s computer 
graphics group at the MPI Informatik in Saarbruecken and received his PhD from Saarland University in 
2003. He spent two years (2005 2006) as a visiting assistant professor at StanfordUniversity, USA. His 
research interests include 3D appearance acquisition, image-based and computational photography rendering. 
He has given several lectures and tutorials about this topic at various conferences including SIGGRAPH 
courses on realistic materials in 2002 and 2005. Szymon Rusinkiewicz is an associate professor of Computer 
Science at Princeton Uni­versity. His work focuses on acquisition and analysis of the 3D shape and appearance 
of real-world objects, including the design of capture devices and data structures for ef­.cient representation. 
He also investigates algorithms for processing complex datasets of shape and re.ectance, including registration, 
matching, completion, symmetry analy­sis, and sampling. In addition to data acquisition, his research 
interests include real-time rendering and perceptually-guided depiction. He obtained his Ph.D. from StanfordUni­versity 
in 2001. TimWeyrich isa Post-doctoralTeaching Fellowat Princeton University, workinginthe Computer Graphics 
Group Princeton. His research interests are appearance modeling, 3D reconstruction, cultural heritage 
acquisition, and point-based graphics. Prior to com­ing to Princeton in Fall 2006, he received his PhD 
from ETH Zurich, Switzerland, where he developeda novel face scanner to analyze human skinre.ectance, 
allowing for photo­realistic reconstructions of human faces. He received his Diploma degree in computer 
science from the University of Karlsruhe (TU), Germany, in 2001. Todd Zickler received his Ph.D. in electrical 
engineering fromYale University in 2004 and is currently an assistant professor in the School of Engineering 
and Applied Sciences at HarvardUniversity. Hisresearch spans computervision, computer graphics and image 
processing, and he is currently focused on developing representations of appearance and exploiting them 
for visual inference. In 2006, he was the recipient of a career awardfrom the US NSF titled, Foundations 
for Ubiquitous Image-based Appearance Capture. Contents 1 Radiometryand Appearance Models 1 1.1 Radiometry 
..................................... 2 1.2 Surface Re.ectance ................................. 4 
1.3 Subsurface Scattering ................................ 7 1.4 Generalizing Re.ectanceand Scattering 
..................... 9  2 Principles of Acquisition 11 2.1 Homogeneous Re.ectance:BRDF......................... 
11 2.1.1 The Goniore.ectometer .......................... 12 2.1.2 Image-based measurement of planar 
samples . . . . . . . . . . . . . 12 2.1.3 Image-based measurement of curved samples . . . . . . . . 
. . . . . 13 2.1.4 Image-based measurement of arbitrary shapes . . . . . . . . . . . . . 13 2.2 Spatially-varying 
Re.ectance: SVBRDF ..................... 14 2.2.1 Planar Surfaces: The Spatial Goniore.ectometer . . 
. . . . . . . . . . 14 2.2.2 Curved Surfaces .............................. 15 2.2.3 Separability:TheDichromaticModel 
.................. 17 2.2.4 CaseStudy: Re.ectanceSharing ..................... 17 2.3 Subsurface scattering: 
BSSRDF .......................... 18 2.4 Calibration...................................... 19 2.4.1 
Geometric calibration ........................... 19 2.4.2 Radiometric calibration .......................... 
20 2.4.3 Colorimetric calibration .......................... 20 2.4.4 Shape..................................... 
21  3 Spatially-Varying Re.ectance Models 23 3.1 Acquisition...................................... 24 
 3.2 Representation .................................... 24 3.2.1 Basis Decomposition ............................ 
24 3.2.2 ParametricModels ............................. 25 3.3 The InverseShadeTree Framework ........................ 
26 3.3.1 Alternating Constrained Least Squares . . . . . . . . . . . . . . . . . 28 3.4 Conclusion and 
Directionsof Future Research . . . . . . . . . . . . . . . . . 29 3.4.1 Parametricvs. Non-Parametric 
...................... 30 3.4.2 OpenProblems ............................... 30  4 From BSSRDF to 
8D Re.ectance Fields 33 4.1 BTFsand DistantLight Re.ectance Fields .................... 33 4.2 BSSRDFs....................................... 
34 4.3 Diffuse Subsurface Scattering ........................... 34 4.4 ArbitraryLightTransport ............................. 
35 4.4.1 SingleView SingleProjector ....................... 36 4.4.2 8D Re.ectance Fields ............................ 
37 4.5 ConclusionsandFutureWork ........................... 39 4.5.1 OpenProblems ............................... 
39  5 The HumanFace ScannerProject 41 5.1 PreviousWork.................................... 41 5.2 Skin 
Appearance Acquisition ........................... 42 5.2.1 Subsurface Scattering Acquisition .................... 
43 5.2.2 Re.ectanceField Acquisition ....................... 43 5.3 FaceDataProcessing ................................ 
44 5.3.1 System Calibration ............................. 44 5.3.2 Geometry Re.nement ........................... 
47 5.4 Re.ectanceModelFit................................ 47 5.5 Re.ectance Analysis ................................ 
49 5.6 AppearanceTransfer ................................ 51 5.7 Conclusion...................................... 
52 List of Figures 53 Bibliography 55  1 Radiometryand Appearance Models Szymon Rusinkiewicz, Princeton 
University Comprehending the visual world around us requires us to understand the role of materi­als. 
In essence, we think of the appearance of a material as being a function of how that material interactswithlight.The 
materialmaysimplyre.ectlight,oritmay exhibitmore complex phenomena such as subsurface scattering. Re.ectance 
itself is a complex phenomenon. In general, a surface may re.ect a different amount of light at each 
position, and for each possible direction of incident and exitant light (Figure 1.1, left). So, to completely 
characterize a surfaces re.ection we need a six­dimensional function giving the amount of light re.ected 
for each combination of these variables (position and incident and exitant directions are two dimensions 
each). Note that this does not even consider such effects as time or wavelength dependence. We will consider 
those later, but for now let us simply ignore all time dependence, and assume that any wavelength dependence 
is aggregated into three color channels: red, green, and blue. These re.ectance functions embody a signi.cant 
amount of information. They can tell us whether a surface is shiny or matte, metallic or dielectric, 
smooth or rough. Knowing the re.ectance function for a surface allows us to make complete predictions 
of how that surface appears under any possible lighting. For translucent surface, the interaction with 
light can no longerbe described as simple re.ection. Thisisbecauselight leavesthe surfaceatadifferentpointthanwhereit 
entered (Figure1.1,right).So,inorderto characterizesuch surfacesweneeda functionthatgives the amount 
of light that is scattered from each possible position (2D) to each other position (another 2D). If we 
wanted to be even more correct, of course, we would need to account for the directional dependency as 
well. So, now that we have some idea of how we can understand appearance, thereremains the question of 
why we may wish to do so. In addition to the obvious application domain of image synthesis, having a 
complete knowledge of a materials appearance can help us interpret images. It will aid in 3D reconstruction, 
view interpolation, and object recogni­tion. Furthermore, knowing how to characterize materials can help 
us understand how humans perceive surfaces. This SIGGRAPH class covers the basic principles of how materials 
aredescribed, how the appearanceofreal-world objects maybe measured, and howa knowledgeof appearance 
aids in a variety of applications. This .rst section of the class covers foundational topics. We will 
learn aboutradiometry and see the de.nition of the BRDF: a function describing surfacere.ectanceatapoint.Wewill 
cover generalizationsoftheBRDF,includingspatial variation and subsurface scattering. Finally, we will 
consider the many different types of data that can be captured that characterize appearance, and how 
they relate to each other. 1.1 Radiometry Let us start with the basics. Light is a form of electromagnetic 
energy, and so can be measured in Joules. Because it is most useful to think of continuous light .ow, 
instead of individual pulses, we will most often be interested in the amount of energy .owing per unit 
time. This is known as radiant .ux (F)or just power, and hence may be measured using theSI unitsofWatts. 
Although having a way of characterizing the total .ow of light power is useful, we will need to consider 
more complex quantities in order to talk about concepts such as light sources and surface re.ectance. 
Point Light in a Direction: Consider an ideal light source (idealized as a point in space). If the light 
were being emitted uniformly in all directions, describing its power (in Watts) would tell us all we 
wanted to know. However, it is possible that light is not being emitted equally in all directions. In 
this case, characterizing the power being emitted in a particular direction requires a different unit. 
In such cases, we can talk about the amount of power being emitted per unit solid angle. So what exactly 
is a solid angle, and how is it measured? Auseful analogy is to the way an angle is de.ned in the plane. 
One radian is de.ned as the angle subtended by an arc of a circle, with the arc length being equal to 
the circles radius. Equivalently, an angle in radians maybe calculatedby dividing the lengthofa circular 
arcby the radius. Moving to the concept of solid angles, we will be working in three dimensions (vs. 
two for angles), and will be looking at a sphere (vs. a circle). The basic unit of solid angle is known 
as the steradian, and is de.ned as the area of some region on a sphere divided by thesquareofthespheres 
radius.Acompletespherethushas4p steradians, and smaller solid angles de.ne smaller regions of the space 
of directions. So, measuring the directional power of a point light source can be done using the units 
ofWattsper steradian.The same amountofpower emittedintoa smallersolidanglewill result in a larger measurement 
(think of the case of a laser, which has a relatively small power but is concentrated into a very small 
solid angle). LightFalling ona Surface: Another radiometric quantity we often wish to measure is called 
irradiance.Itrepresentsthe amountoflight falling ontoa surface. Becausethe same radiant .ux will be more 
concentrated when falling onto a smaller area of surface than a larger surface, we de.ne irradiance as 
power per unit area. Given this de.nition of irradiance, there are two immediate and easily-observed 
laws that emerge. The .rstis the inverse-square law: movinga point light source awayfroma surface reduces 
irradiance in proportion to the inverse square of the distance. Secondly, tiltinga surfaceawayfromapointlightresultsinalower 
irradiance,inproportiontothe cosine of the angle between the surface normal and the direction towards 
the light. This cosine law is often written as the dot product between the (unit-length) surface normal 
and light vectors. Light Emitted from a Surface in a Direction: We now come to the .nal, and most complex, 
radiometric quantity we are going to consider, which describes the emission of light from a surface. 
This can be thought of as combining the two concepts we just saw: the emitted light can vary with direction 
(hence we must control for its directional distribution, as we did with the point-light case), and we 
are interested in the amount of light emitted per unit surface area. Hence, we arrive at the de.nition 
of radiance: power emitted per unit area (perpendicular to the viewing direction) per unit solid angle. 
Radiance is perhaps the most fundamental unit in computer vision and graphics. It is easy to show that 
the irradiance on a camera sensor is proportional to the radiance of the surfacesitisimaging,so cameras 
see the radianceof surfaces.Thepixelvalueswedeal with in digital images are (ignoring nasty things like 
gamma) proportional to radiance!  1.2 Surface Re.ectance We are nowready to use what we know about radiometry 
to de.ne the BRDF [NRH+77]. Thisis the Bidirectional Re.ectance Distribution Function, andit describes 
surfacere.ec­tionatapoint. Formally,itisthe ratio betweenthere.ected radianceofa surfaceandthe irradiance 
that caused thatre.ection. The radiance and irradiance are each measured ata particular angle of exitant 
and incident light, respectively, so the BRDF is usually written asafunctionoffour variables:thepolaranglesoflightcomingintoandoutofthe 
surface. dLo(.o) fr(.i . .o)= fr(.i, .i,.o, .o)= (1.1) dEi(.i) The BRDF is often written as a differential 
quantity. This is to emphasize that there is no such thing as light arriving from exactly one direction, 
and beingre.ected into exactly one outgoing direction. Rather, we must look at non-zero ranges of incident 
and exitant solid angles, and consider the limit as those approach zero. Because BRDFs are 4D functions, 
they are a bit trickyto visualize directly. Instead, we of­ten visualize two-dimensionalslicesofthis 
function.Figure1.4showsonesliceofaBRDF, corresponding to one direction of incidence (the arrow) and all 
possible directions of re­.ection.Theblue surfaceisa hemispherestretchedsuchthatits radiusinanydirectionis 
the re.ected radiance in that direction, and is known as a goniometric plot. Youwillnotethat,forthis 
particularBRDF,muchofthe incidentlightisre.ectedequally in all directions. This is the constant-radius 
(spherical) portion of the surface you see. However, there is also a bump in the surface, indicating 
that there is a concentrated re­.ection in one particular direction. If we change the direction of incidence, 
we see that the constant portion of the function was unchanged, but the position of the bump moved. In 
fact, the bump always appears around the direction of ideal mirror re.ection of the incident direction. 
This is known asa specular highlight, andit givesa surfacea shiny appearance. Properties of the BRDF: 
Before we look at speci.c BRDF models, let us discuss a few propertiessharedbyallBRDF functions.The.rstis 
energy conservation:itis impossible fora surface tore.ect more light than was incident on it! Expressing 
this mathematically, we see that the integral of the BRDF over all outgoing directions, scaled by a cosine 
term to account for foreshortening, must be less than one: fr cos .o d.o = 1. (1.2) O A second, more 
subtle, property of BRDFs is that they must be unchanged when the angles of incidence and exitance are 
swapped: fr(.i . .o)= fr(.o . .i). (1.3) This is a condition known as Helmholtz reciprocity, and is due 
to the symmetry of light transport. Some systems, such as Todd Zickler s work on Helmholtz stereopsis, 
have relied on this property. Some, but not all, BRDFs have a property called isotropy: they are unchanged 
if the in­coming and outgoing vectors are rotated by the same amount around the surface nor­mal. In this 
case, there is a useful simpli.cation that may be made: the BRDF is really a 3-dimensional function in 
this case, and depends only on the difference between the azimuthal angles of incidence and exitance. 
The inverse of isotropy is anisotropy. An anisotropic BRDF does not remain constant when the incoming 
and outgoing angles are rotated. In this case, a full four-dimensional function is necessary to characterize 
the behavior of the surface. Anisotropic materials are frequently encounteredwhenthe surfacehasastronglydirectionalstructureatthe 
small scale: brushed metals are one example (Figure 1.5). Lambertian BRDF: We now turn to looking at 
speci.c examples of BRDFs. We will look at simple examples, such that the re.ectance may be written as 
a mathematical for­mula. Real surfaces, of course, are more complex than this, and mathematical models 
frequently do not predict the re.ectance with great accuracy. The simplest possible BRDFisjusta constant. 
fr = const. = ./p (1.4) This results in a matte or diffuse appearance, and is known as ideal Lambertian 
re­.ectance. This BRDF is frequently written as a constant . divided by p. In this case, . is interpretedasthediffuse 
albedo:itisthe fractionoflightthatisre.ected(vs. absorbed) by the surface. Plugging this BRDF into the 
energy conservation integral veri.es that the surface conserves energy precisely when the albedo is less 
than or equal to one. Blinn-Phong BRDF: Another simple analytic BRDF is the Blinn-Phong model, de­signed 
to represent glossy materials: fr = ./p + ks(n · h)a . (1.5) In contrast to the Lambertian BRDF, the 
distribution of re.ected light is not constant. In factthereisalobe centeredaroundthedirectionofidealmirrorre.ectionforeach 
incident angle, containing signi.cantly more energy than the rest of the domain. This is known as the 
specular lobe, and its size and fall-off are controlled by the parameters ks and a. This lobe is what 
produces the specular highlights on this vase that help give it a shiny appearance. Torrance-Sparrow 
BRDF: A more complex, yet more realistic, BRDF was originally developedin the physics communitybyTorrance 
and Sparrow [TS67], and wasre.ned for computer graphicsby Cook andTorrance [CT82]. DGF fr = (1.6) p cos 
.i cos .o For the purposes of calculating re.ectance, this model assumes that at a very small scale the 
surface consists of tiny, mirror-re.ective microfacets oriented in random directions. There are three 
major terms in the model that describe the angular distribution of micro­facets, how many are visible 
from each angle, and how light re.ects from each facet. ..2 tan ß - e m D= (1.7) 4m2 cos4 ß The .rst 
term D in theTorrance-Sparrow model describes what is the density of facets facingin any possible direction. 
Notice that partof this termresemblesa Gaussian. 2(n · h)(n · v) 2(n · h)(n · l) G= min1, , (1.8) (v 
· h)(v · l) The next term G in theTorrance-Sparrow model accounts for the fact that not all facets are 
visiblefromalldirections, becausetheyare hiddenbythe facetsinfrontof them. Finally, the re.ection from 
each facet is described by the Fresnel term F, which predicts that re.ection increases towards grazing 
angles. Other BRDF Features: Another commonly-observed characteristics of BRDFs is an increaseinlightre.ectedintoallgrazingangles,asistypicalfor 
dusty surfaces. Finally, someBRDFs includearetro-re.ective component.Thatis,they scatterlightmoststrongly 
back into the direction from which it arrived. The paint found on roads and street signs is a common 
example of this phenomenon. Such paint contains crystals that produce a cornerre.ector con.guration Beyond 
BRDFs: Although we could continue to develop mathematical BRDF formu­las of increasing sophistication 
that explain a greater and greater variety of optical phe­nomena, over the past decade it has become 
increasingly practical to simply measure the BRDFs of real material samples [MPBM03b]. In fact, it is 
one of our main arguments in this class that measured data can capture a greater variety of real-world 
optical phenom­ena with greater accuracy than is possible with analytic models. Of course, the BRDF is 
merely the beginning of our study of the appearance of materials. Real-world objects will exhibit more 
complex behaviors, such as a BRDF that changes from point to point on the surface. Adding two spatial 
dimensions to the four directional dimensions of the BRDF leads us to the six-dimensional Spatially-Varying 
BRDF. Later in this class you will hear about the challenges of capturing, representing, editing, and 
analyzing these complex functions [LBAD+06a]. 1.3 Subsurface Scattering Even the SVBRDF is not enough 
to characterize all materials. Many surfaces exhibit translucency: a phenomenon in which light enters 
the object, is re.ected inside the ma­terial, and eventually re-emerges from a different point on the 
surface. Such sub-surface scattering can have a dramatic effect on appearance, as can be seen from these 
computer graphics simulations that differ in only one respect: the left image simulates surface re­.ection 
only, while the right image includes sub-surface scattering [PvBM+06a]. In order to cope with subsurface 
scattering, we will need to examine more complex ap­pearance functions: those that can include the phenomenon 
of light leaving the surface at a different point than the one at which it entered. The BSSRDF: The relevant 
function is known as the Bidirectional Scattering-Surface Re.ection Distribution Function, or BSSRDF: 
S(xi,yi,.i, .i,xo,yo,.o, .o). (1.9) You will notice that we have taken the SVBRDF and added two morevariables,represent­ingthe 
surface locationatwhichthelight leavesthe surface.Wearenowuptoa function of8variables! As we will see 
later in the class, the high dimensionality of this function leads to great dif­.culty in capturing and 
working with the BSSRDF directly, especially if a high sampling rate in each dimension is desired [GLL+04b]. 
Because of the enormous size of the BSSRDF, approximations to it have become quite popular. One of the 
most powerful approximations relies on the fact that, in many cases, the appearance is dominated by light 
that has re.ected many times within the material. In this case, the details of each scattering event 
become unimportant, and the appearance is well approximated by thinking of light diffusing away from 
the location at which it enters the surface, much as heat might spread [JMLH01b]. It turns out that the 
pattern of diffusion is well approximated by a dipole: a combination of a point light some distance below 
the point at which light entered the surface, and a negative light source some (slightly larger) distance 
above the surface. Combining the contributions of these two light sources with Fresnel terms for light 
entering and leaving the surface yields a simple, yet powerful, model: S= F(.i) R( xi - xo ) F (.o). 
(1.10) Because of the symmetry of diffusion, the model is effectively a function of only one variable: 
the distance between the points of incidence and exitance. This dipole model, originally introduced in 
2001, has become very popular for simulating subsurface scattering in many materials, and we will see 
applications of it later in this class. Homogeneous and Heterogeneous Scattering: Of course, the dipole 
approximation assumes a uniform material: the same amount of scattering everywhere on the surface. Formorerealistic 
surfaces,youmightneedtoadd someofthe complexityofthe BSSRDF back in, by considering spatial variation. 
For example, in Figure 1.6 you can clearly see how internal structure affects the scattering. 1.4 Generalizing 
Re.ectance and Scattering So, does the BSSRDF cover all possible aspects of surface appearance? No! First, 
we could consider all of the functions we have talked about as being dependent on the wavelength of light. 
Moreover, some surfaces are .uorescent: they emit light at different wavelengths than those present in 
the incident light. Some other surfaces may have appearance that changes over timebecause of chemical 
changes, physical processes such as drying, or weathering. Other surfaces might capture light and re-emit 
it later, leading to phosphorescence and other such phenomena. Thus, a complete description of light 
scattering at a surface needs to add at least two wavelength and two time dimensions to the BSSRDF. Scattering 
from a region of space would add two additional spatial dimensions. So, we can think of all of the functions 
weve seen as specializations of a 12-dimensional scattering function. While nobody has really tried to 
capture the full function, many efforts exist to capture one or more of its low-dimensional subsets. 
In fact, it can be ar­guedthatoverthepastdecade,researchershaveexploredmostofthe subsetsthat make sense, 
up to the limits of acquisition devices.  2 Principles of Acquisition Todd Zickler, Harvard University 
Inthischapterwewill considerthe measurementof surfacere.ectionpropertiesinorder of increasing complexity,from 
homogeneous BRDF (functions of at most .ve dimensions) to general subsurface scattering(a functionofat 
most nine dimensions). In designing an acquisition system, the four (competing) factors that need to 
be consid­ered are acquisition time, accuracy and precision, cost, and generality. Here, generality refers 
the the breadth of materials that are to be considered. It is possible to build an ef­.cient system for 
measuring the BRDF of spherical surfaces, for example, but not every material can be painted on to a 
sphere. 2.1 Homogeneous Re.ectance: BRDF TheBRDFisthesimplestre.ectionmodelwewill examine.Weconsiderittobeafunction 
of at most .ve dimensions, one of which is the spectral dimension. An isotropic BRDF has an angular domain 
whose dimensionisreducedby one. One generally measuresaBRDFbyilluminatinga(locally) .at surface withacollumnated 
beam of light with direction(.i)and measuresthe spectral radiancere.ectedinan output direction(.o). The 
input and output directions are assumed to be known relative to the local coordinate system of the planar 
surface patch. Since the BRDF is a derivative quantity, we can only measure it s average over .nite spa­tial 
and angular intervals. Indeed, truly in.nitesimal elements of solid angle due not contain measureable 
amounts of radiant .ux. [NRH+77] This is not usually a problem in the angular sense. The solid angle 
subtended by a typical sensor is small enough that the BRDF can be considered constant within it. One 
must be more careful in the spatial sense. It is essential that the spatial scale of the measurements 
be chosen such that the assumptions underlying the BRDF (i.e., radiant .ux emitted from a point is due 
only to .ux incident at that point) are satis.ed. In image-based BRDF measurement systems, where high 
resolution cameras are used to measure re.ected .ux, this generally means that images must be downsampled 
to obtain valid BRDF measurements. As an example, consider the measurement geometry shown in the simple 
schematic on the slides.Aportionofaplanar sampleisobservedbyasensorthroughanoptical system (e.g.,bya 
single elementofaCCD array.) The .niteareaofthe sensor back-projectstoa .nite area on the surface Ao. 
In order for this system to provide an accurate measurement of the BRDF, we require that both the illumination 
and the surface scattering effects be uniform across the surface over a larger area Ai . Ao. Also, Ai 
must be large enough to guarantee that .ux incident outside of Ai would not contribute signi.cantly to 
the radiance re.ected within Ao. Detailed guidelines for BRDF measurement can be found in [NRH+77, Sta]. 
2.1.1 The Goniore.ectometer Aclassic device for measuring a general, anisotropic BRDF is the four-axis 
goniore.ec­tometer. In this device, a combination of servo motors are used to position a source and detector 
at various locations on the hemisphere above a planar material sample. The de­tector is typically linked 
to a spectroradiometer or another optical assembly that permits recording of dense spectral measurements 
for each con.guratoin of the source and detec­tor [WSB+98]. This measurement process is a lengthy one, 
and it can require days to measure a single material. The advantage of this approach, however, is that 
the system can be carefully calibrated and measurements can be quite repeatable. Also, the ability to 
capture dense spectral information provides a tremendous advantage over the camera-based systems that 
are ubiquitous in the vision and graphics communities. Acquisition time and equipment cost can be reduced 
if one is willing to restrict one s attention to isotropic BRDFs. In this case, the function being measured 
has only three angular dimensions,soonerequiresonlythreedegreesoffreedominthe acquisitionsys­tem.Inthegoniore.ectometerrecentlybuiltat 
Cornell [LFTW06],thisis accomplishedby having two degrees of freedom in the orientation of the planar 
sample and one additional degree of freedom in the angular position of the source. Using this system, 
one can ac­quire31 spectral samplesper camera/source position(roughly10nmincrements overthe visible spectrum), 
and capturing 1000 angular samples (which is a very sparse covering of the 3D angular domain) takes approximately 
10 hours. 2.1.2 Image-based measurement of planar samples BRDF acquisitioncanbemadelesscostlyand time-consumingbyreplacingasimplepho­todetectorbya 
camera. Acamera s sensor contains millionsof photo-sensitive elements, and by using lenses and mirrors, 
these elements can be used to collect a large number of re.ectance samples simultaneously. An early exampleof 
thisisWard s measurement system[War92],in whichthe radiance emittedbyaplanar sampleisre.ectedfromahalf-silvered 
hemisphereand capturedbya camera with a .sh-eye lens. In this way, almost the entire output hemisphere 
is captured by a single image. The two degrees of freedom in the incident direction are controlled by 
arotation of the source arm(about point C in the .gure) and the rotation of the planar sample. A very 
nice property of this system is that it allows the measurement of retro­re.ection directions, meaning 
those for which the incident and re.ected directions are equal. This is not possible with either of the 
two goniore.ectometers described earlier. Using this system,Wardclaimed thata4D anisotropic BRDF couldbe 
measuredin ten minutes. What are we trading for this gain in ef.ciency? Spectral resolution, for one. 
If we use a white light source (uniform spectral distribution) and anRGB camera, we obtain only three 
spectral measurements for each angular con.guration, and these measurements are weighted averages over 
large, overlapping intervals of the visible spectrum. This is a serious limitation if we want to be able 
to predict the appearance of the material un­der a source with a different spectral distribution. Without 
dense spectral information, physically-accurate color reproduction is generally unattainable. Another 
limitation is the complexityoftherequired calibrationprocedure.InWard s system,weneedtoknow the map from 
pixels in the camera to output directions in the coordinate system of the sample. If we want to be precise, 
we also need to know the exitant solid angle that is effectively subtendedby each pixel. In addition 
to this geometric calibration information, we need radiometricinformation including the optical fall-offin 
the lens system and the radiometric camera response. The complexity of this process reduces the accuracy 
and repeatability of the measurements. This is an example of a design decision in which one trades precision 
for a decrease in acquisition time. 2.1.3 Image-based measurement of curved samples An alternative approach 
is to eliminated the hemispherical re.ector and to use curved material samples instead of a planar one 
[MWLT00, LKK98]. For isotropic materials, one can use a sphere. The surface normal varies from point 
and point and so does the local input and output directions. This means that each image provides a very 
dense (near continuous) slice of samples embedded in the 3D isotropic BRDF domain. Matusik et al. [MPBM03b] 
captured an extensive database of isotropic BRDFs in this way. Using cylinders instead of spheres, one 
can do this for anisotropic materials as well. For example, one can cut strips of an anisotropic material 
at different orientations relative to the material s tangent direction and paste these strips onto a 
cylinder [NDM05]. The cylinder provides one degree of freedom in its surface normal, and two more degrees 
of freedomare obtainedbyrotatingthe cylinderandthe source.The fourthand .naldegree of freedom comes from 
the number of strips , which is typically coarsely sampled. 2.1.4 Image-based measurement of arbitraryshapes 
All methods discussed so far are limited to materials that exist as planar samples or that that can be 
painted on onto a known shape such as a sphere or a cylinder. What about materials for which this is 
not possible? Well, we can measure the material properties on any surface as long as the surface shape 
is known. So if capture the shape using a reconstruction system (laser scanner, structured-lighting, 
photometric stereo, etc.) and then carefully align this shape with the acquired images [MWL+99], we can 
use the very same procedure outlines above. This obviously introduces additionals sources of error and 
bias, so here we are trading precision and accuracy for increased generality. With all of these source 
of error, BRDF measurements from captured, arbitray shapes is often prohibitively noisy. One direction 
for future work is the design of systems that recover both shape and re.ectance from the same image data. 
More on this later.  2.2 Spatially-varying Re.ectance: SVBRDF Next we allow spatial variation in the 
re.ectance function, which increases the dimen­sion by two. Note that despite allowing spatial variation, 
we maintain our assumption regarding spatial scale and sub-surface scattering. Namely, we assume that 
the surface area observed by each photo detector is large enough that sub-surface scattering effects 
are negligible and that the surface is locally homogeneous. This guarantees that the ap­pearanceof each 
small surface element canberepresentedbya BRDF. Since cameras are used for the measurement of SVBRDF, 
essentially all acquisition sys­temstodatehave consideredonlysparsespectralsampling(RGB).Forthisreason,wecan 
effectively ignore the spectral dimension in this section. Even with this simpli.cation, we are left 
with the formidable task of measuring a function of .ve or six dimensions. 2.2.1 Planar Surfaces: The 
Spatial Goniore.ectometer Acquisition of a spatially-varying BRDF can be thought of as the measurement 
of mul­tiple, distinct BRDF one at each point on a surface. SVBRDF acquisition systems are therefore 
closely related to the BRDF acquisitions systems just discussed. A spatial goniore.ectometer is a good 
example and is exactly what the name suggests. It functions like a standard goniore.ectometer, except 
that the single photodetector is replaced by a camera. The example shown in the slides was built by David 
McAllis­ter [McA02b]andis similarinspirittoan earlier versionby KristenDana [DvGNK99].In the example 
shown, the planar sample has spatially-varying re.ectance, and it s orienta­tionis controlledbya two-axis 
pan/tilt head. The source canberotated aboutthe sample as well, which produces a three-axis spatial goniore.ectometer. 
Assuming a columnated source (and an orthographic camera for simplicity), each image yields a dense 2D 
spa­tial slice of the SVBRDF corresponding to .xed input and output directions. Of course, a three-axis 
device such as this one is useful for materials having an isotropic BRDF in each local region. BRDF samples 
collected by this device are very non-uniformly distributed in the 5D do­main. There is near continuous 
sampling of the spatial dimensions but only as many samples of the angular dimensions as there are positions 
of the sample and illuminant. This can be changed by modifying the acquisition system. As was the case 
for single BRDF, lenses and mirrors can be used to redirect input and output rays. Here the motive is 
not to decrease the acquisition time, however, but simply to alter the sampling pattern (and perhaps 
to improve precision). One such system uses a beam splitter and a parabolic mirror to increase angular 
sam­pling rates at the expense of spatial sampling rates [DW04]. The parabolic mirror re.ects aportionoftheoutput 
hemispherefromasinglesmallregiontowardthecamera,thereby providing near-continuous sampling of this angular 
interval. The same mirror is used to direct an incident collumnated beam of light toward the surface 
patch being observed. The direction of the incident ray is controleed by translating an aperture in front 
of the light source, and the surface sampling point (x,y) is changedby translating the parabolic mirror. 
While the altered sampling pattern isinteresting, the great strength of this sys­tem is that all required 
movements are pure translations. One expects this to be highly repeatable. Before we continue,Iwould 
like to pause to clarify some terminology. In the literature, one often sees the term Bi-directionalTexture 
Function (BTF), which, like the SVBRDF, is a function of two spatial and four angular dimensions. For 
the purpose of this session, BTF and SVBRDF are not different. UnlikeaSVBRDF,a BTF I(x,.i,.o) incorporate 
non-local effects such as cast shadows, occlusions, mutual illumination and sub-surface scattering that 
are highly dependent on the non-local shape of the surface. This means, for example, that the appearance 
at a single surface point cannot be well-represented by a parametric BRDF model.Weignore these non-localeffectshereandrestrictout 
attentionto SVBRDF. Arbitrary, non-local scattering will be discussed in a later section. 2.2.2 Curved 
Surfaces Most interesting spatially-varying surfaces are not planar nor can they be painted on to a planar 
substrate. Thus, there is often the need to measure the SVBRDF on a curved surface directly, and as for 
BRDF, this can be done when the 3D geometry of the surface is known. An example of a suitable acquisition 
system is the StanfordSpherical Gantry. This system can sampleallsix dimensionsofthe SVBRDF de.nedona 
curved surface.In this case, one needs to sample the entire sphere of directions and not just a hemisphere. 
To get a sense of how much data is required to densely sample the SVBRDF of a regular surface, we can 
perform a simple counting exercise. When the shape of the object is known and the source and view directions 
are given, each pixel in an image provides one sample of the SVBRDF (or the BRDF at a particular surface 
point). Sampling the BRDFat every surface pointin5. or1. angular increments therefore requires millions 
or hundreds-of millions of images, respectively. Clearly, capturing millions of images per object is 
impractical, so we look to reduce this burden using one or more of: improved acquisition systems  parametric 
BRDF models; and  knowledgeof generalre.ectance phenomena  Each is discussed separately below. Aquisition 
Systems Anumber of acquisition systems have beendeveloped over the past ten years. Thanks to rapid advancements 
in LEDs and digital cameras, it is becoming less and less expensive to build devices capable of capturing 
large numbers of images very quickly. In designing these sytems there are a number of trade-offs. For 
example, it is much easier to calibrate a system like the Stanford spherical gantry or Light Probe One 
at USC that uses only one camera and one light source. But acquisition time for these systems willgenerally 
be much larger than systems equipped with multiple sources and cameras. Parametric Approaches The use 
of parametric models generally reduces the number ofrequired input images and hence therequired acquisition 
time. In the general case, one must densely sample the 4D (or 3D isotropic) BRDF at each surface point. 
When we use a parametric model like the Phong model, however, we need only estimate a handful of parameters 
at each point. Of course, this approach assumes that the appearance of the object we wish to acquire 
can be accurately represented by our chosen BRDF model, which may very well not be the case. For example, 
a Cook-Torrance model does well at representing plastics and metals, but it cannot represent retro-re.ection 
effects. Nonetheless, if one is interesting in acquiring a particular class of surfaces that can be well-represented 
by a particular model, a parametric approach might be appropriate. Indeed, as we will see with the human 
face project in the last section of this document, when used in conjunction with other re.ectance representations, 
parametric model-based representations can produce stunningresults. There have been a number of parametric 
approaches presented over the past decade (e.g., [SWI97, YDMH99, BG01, LKG+01, Geo03, GCHS03, MLH02]). 
These methods vary widely in terms of the model (or models) used, the type of input data, and the procedure 
used for .tting. Some of these methods are described in more detail in Sect. 4. General Re.ectance Properties 
Instead of relying on parametric models, or in ad­dition to using them, we can reduce the number of required 
input images by exploiting what we know about re.ectance in general. We have already discussed isotropy 
and reciprocity as being common, and useful, prop­erties of re.ectance. These are important from an acquisition 
standpoint because they substantially reduce the angular domain and thus the number of required images. 
We have also already discussedcompressibility. Thisrefers to the fact that even though the BRDF can change 
rapidly in some regions of its angular domain, it often changes slowly overmuchofit.IfonerepresentsaBRDFina 
waveletbasis,for example,itislikelytobe very sparse [MPBM03a]. By using appropriate sampling schemes, 
compressibility can be exploited for acquisition as well. Another well-known and well-used property is 
separability. Separability refers to the fact that spatially-varying re.ectance can often be written 
as a linear component of diffuse and specular components. This is useful because when isolated, each 
of these compo­nents exhibits different (and exploitable) behavior. For example, the diffuse component 
is typically well-represented by a Lambertian model and can often be reliably estimated over the entire 
surface from just a handful images. Spatial smoothness refers to the fact that for many surfaces, re.ectance 
is slowly vary­ing from point to point. This is especially true for the specular re.ectance component 
(e.g.[SWI97, ZREB06]).Thus,knowledgeofthere.ectanceatonepointonasurfacecan sayquiteabit aboutthere.ectanceat 
another. Finally, spatial regularity is another way of describing the correlation between the re­.ectance 
at distinct surface points on the same surface. Here, it is assumed that the re.ectance at all surface 
points can be written as linear combinations of a single set of basis BRDF (e.g., [LKG+01, GCHS05b]). 
Before we see an example of an approach that exploits these properties for acquisition, we will take 
a moment to describe separability in more detail. 2.2.3 Separability: The Dichromatic Model The dichromatic 
model of re.ectance is a common special case of the BRDF model, and it was introduced to the computer 
vision community by Shafer [Sha85] as a model for the re.ectance of dielectrics. It assumes that the 
BRDF of the surface can be decomposed into two additive components: the interface (specular) re.ectance 
and the body (diffuse) re.ectance. Furthermore, it assumes that each of these two components can be factored 
into a univariate function of wavelength and a multivariate function that depends on the imaging geometry. 
That is, f(.,.i,.o)= gd(.) fd(.i,.o)+ gs(.) f s(.i,.o). Ifwe further assumethatthe indexofrefractiononthe 
surfaceis constant overthe visible spectrum a valid assumption for many materials it follows that gs(.) 
is a constant function [LBS90]. This leads to the common expression for the BRDF of a separable (or dichromatic) 
surface, f(.,.i,.o)= gd(.) fd(.i,.o)+ fs(.i,.o), (2.1) where fs(.i,.o)= gs f s(.i,.o). The function gd(.) 
is often referred to as the spectral re.ectance of the material. Even though it was originally used to 
describe the re.ectance of dielectrics [Sha85], the dichromatic model has been used successfully as an 
approximation of the re.ectance of manydifferent materials. Empiricallyithasshowntobe suitableforcertaintypesofplant 
leaves,cloth,wood,andtheskinoffruits[LBS90,TW89]in additiontoalarge numberof dielectrics [Hea89]. Separability 
is useful from an aquisition standpoint because the diffuse and specular com­ponents tend to exhibit 
different spatio-angular characteristics that can each be exploited. If we now consider spatial variation, 
the diffuse component is nearly Lamberian but typ­ically varies rapidly over the surface: gd(.,x) fd(x,.i) 
 gd(.,x) fd(x) (2.2) We often refer to such surfaces as having signi.canttexture. The specular component, 
on the other hand, is typically non-Lambertian, but changes slowly from point to point. 2.2.4 Case Study: 
Re.ectance Sharing The properties described in the previous section have been exploited in various ways 
by both parametric and non-parametric techniques for aquisition. Rather than list them all here,wewill 
describeoneexamplethatexploitsmanyofthem.Werefertothemethodas re.ectance sharing [ZREB06], and it is 
a non-parametric approach that seeks to acquire the SVBRDF of a surface from a very sparse set of images. 
The input is a set of images of a known three-dimensional shape, with each image being captured under 
a columnated illumination in a known direction. As described earlier, each pixel in one of these images 
provides one sample of the BRDF at the correpsonding surface point. The input images are assumed to be 
decomposed into their specular and diffuse components. In practice this is often done using polarizing 
.lters on the camera and light source as follows. Two exposures are captured for each view/lighting con­.guration, 
one with the polarizers aligned (to observe the sum of specular and diffuse components), and one with 
the source polarizer rotated by 90. (to observe the diffuse component only.) The specular component is 
then given by the difference between these two exposures. (See, e.g., [Mer84].) If one assumes the diffuse 
component to be Lambertian it can be measured from as little as one image. We are simply required to 
estimate an RGB diffuse texture map aRGB(x,y). Estimating the specular component of the SVBRDF is more 
dif.cult, and this where isotropy, reciprocity, compressibility and spatial smoothness play an important 
role. Isotropy, reciprocity and compressibility are exploited by representing the angu­lar dimensions 
of the SVBRDF in terms of Syzmon Rusinkiewicz s halfway/difference parameterization [Rus98]. This is 
a natural way to shrink the angular domain and to separate angular intervals that typically exhibit rapid 
variation from those that do not. To exploit spatial smoothness, we view each pixel as sample lying in 
the (5D isotropic) SVBRDF domain,andwenotethateachimageprovidesa near-continuous samplingofa 2D slice 
in this domain. SVBRDF estimation is formulated as a scatter-data interpolation problem, in which we 
simultaneously interpolate the samples in both the angular and spatial dimensions. Usingthisapproach,the 
samplingofthe SVBRDF domainishighly non-uniform.Weob­tainonlyasparsesetof2D slices(oneper image), whilethe 
samplingalongeach2D sheet is very dense. Fortunately, one can show that the densly-sampledregionsof the 
SVBRDF correspond nicely with the dimensions in which we typically observe rapid angular vari­ation, 
such as the half-angle dimensions near small half-angle values. For thisreason, one can often recover 
accurate SVBRDF from a very small number of images. The example in the slide shows the specular lobes 
at two points on the surface in the case where only fourinputimagesaregiven. Plausibleresultsare obtained 
eventhoughatmostfourre­.ectance samples are available at each surface point. Once the SVBRDF is recovered, 
one can use it to predict the apperance of the surface in novel view and lighting conditions.  2.3 Subsurface 
scattering: BSSRDF Ageneral BSSRDF is a function of nine dimensions if we include the spectral dimension. 
Even if we ignorethe dependency on wavelength, densely sampling an eight dimensional space is an extremely 
burdensome process. We will see some acquisition systems that begin to address this in the next section. 
Here we focus on a common simpli.cation that relies on a factored form of the BSSRDF. As mentionsed earler,the 
most common simpli.cation of the BSSRDF comes from assum­ing a surface to be homogeneous and highly scattering. 
S(.,xi,.i,xo,.o)= Ft(.,.i)R(.,||xi - xo||)Ft(.,.o). (2.3) In this case, provided that that index of refraction 
is known, the measurement process becomes one of estimating R(·), whichisa functionof one dimension [JMLH01b]. 
Jensen and colleagues used the setup shown in the slides to measure this function. Light is focussed 
at a single point on a planar, homogeneous surface, and this point is viewed by a camera. Since the surface 
is isotropically scattering, the radiance it emits is radially symmetric about the point of incidence. 
Thus, it is suf.cient to examine a single surface line that contains this point, andexamplesof the intensities 
observed along one such line are shownin the graph. Followinga calibrationprocedure, these observed intensities 
can be used to estimate the function R. More complex acquisition systems can be built to measure more 
general BSSRDF repre­sentations that partially account for inhomogenieties. For example,Tong et al. [TWL+05], 
consider quasi-inhomogeneous materialsthatarehomogeneousatalarge scalebuthet­erogeneous locally. Theirrepresentationis 
givenby S(.,xi,.i,xo,.o)= fi(.i)Rd(xi,xo) fo(xo,.o), (2.4) which includes a spatially-varying exiting 
function . The acquisition system used to acquire data and .t this model incorporates a laser and a number 
of cameras and light sources. 2.4 Calibration Cameras and light sources used for acquisition must be 
calibrated both geometrically and radiometrically. In addition, if objects of general shape are being 
used, their shape must be precisely known. 2.4.1 Geometric calibration Geometric calibration involves 
the recovery of a camera s extrinsic parameters (position and orientation relative to a world coordinate 
system) and intrinsic parameters (focal length, radial distortion parameters, etc.). Free and reliable 
tools for geometric camera calibration are readily available [OCV, CCT]. Likewise, geometric calibration 
for light sources requires the determination of their positions and orientations. The positions of point 
sources are typically measured by placing a number of shiny spheres at the posi­tion of the material 
sample. When the camera and spherepositions are known, highlights observed in the images of the illuminated 
spheres provide constraints on the source po­sitions (e.g. [YNBK07]). Source orientation is typically 
not measured directly. Instead, one simply locks down the sources in their known positions and measures 
the non­uniformity of their angular output patterns during a radiometric calibration procedure. 2.4.2 
Radiometric calibration Radiometric camera calibration involves two stages. First, one must determine 
the radio­metric response function of the camera. This is the non-linear mapping that often exists between 
the irradiance incident on the image plane and the recorded intensity. Standard methods for doing this 
exist, and they usually involve imagingacalibration targetorcap­turing multiple exposures of a static 
scene [DM97, MN99]. Asecond step is required to recover the optical fall-offof the lens system. An imaging 
system that uses an ideal thin lens exhibits a relationship between scene radiance and image irradiance 
that falls-off as cos4a, where a is the anglebetween the principle incoming ray and the optical axis. 
Ina real camera, the optical fall-offmust be measured for each zoom and aperture setting. It can be measured, 
for example, by capturing an image of a uniform, diffuse surface on a cloudy day. Asevere limitation 
that comes with the use of digital cameras for re.ectometry is their limited dynamic range. In order 
to measure high radiance values at specular peaks while maintaining suf.cient signal to noise ratios 
in darker regions, one must acquire several images with different exposures and merge them intoasingle 
high dynamic range (HDR) image [DM97, MN99].If onehasalarge numberof sourcesinthe acquisition system, 
one can additionally multiplex the lighting to reduce the required dynamic range [SNB03]. Radiometric 
source calibration involves the measurement of output non-uniformity in a coordinate system that is rigidly 
associated with the source. On typically does this by imaging a planar diffuse surface (e.g., Spectralon) 
whose position is know relative to a camera and source. Common sources of noise include thermal noise 
in the camera and temporal source varia­tions. The former canbereducedby capturinga black image for each 
camera (e.g., with the lens cap on) and subtracting this from all images used for measurement. Temporal 
source variations are more dif.cult to deal with, but fortunately, with advancs in LED lighting, this 
is becoming less of an issue. 2.4.3 Colorimetric calibration Typical camera-based acquisition systems 
limit spectral measurements to three wide­band observations (RGB). If this is done using a color camera 
equipped with Bayer .l­ter, one must be sure to do all processing on raw data as opposed to that which 
is dis­torted by a demosaicking algorithm. Ideally, color information should be acquired using a grayscale 
camera equipped with a set of .lters so that trichromatic or multi-spectral measurements are obtained 
at every pixel. Advances in electrically tunable liquid crys­tal spectral .lters may eventually enable 
this approach to provide high spectral samping rates in a reasonable amount of time. Whenever RGB re.ectance 
measurements are made, one must be aware that these mea­surements are valid only for the particular spectral 
power distribution of the light source. Using these triples to render synthetic images under different 
lighting will generally not produce physically accurate results. In addition, if mutliple cameras are 
used, one must compensate for variations between their .lter sets. An example of system that relies on 
these calibration procedures will be described in the .nal section of this document. 2.4.4 Shape If 
the materials being measured exist on arbitrarily-shaped surfaces, the shape of these surfaces must be 
known in the same coordinate system de.ned by the geometric calibra­tion of the cameras and sources. 
From the perspective of measuring re.ectance, three­dimensional surfacereconstructionand alignmentcanthusbe 
viewedas another calibra­tion step. In this context, one of the requiremetns of a 3D reconstruction system 
is that it recov­ers shape in a manner that is not biased by the material properties of the surface being 
measured. Indeed, we do not want the calibration of the measurement system to depend on the signal (re.ectance) 
being measured. Common approaches to recovering shape for general materials is to use structured lighting 
from lasers or projectors. This can work well even for shiny and translucent surfaces [CLFS07]. One disadvantage 
of this approach is that the recovered shape must then be aligned with the images used for measuring 
re­.ectance. Any alignment errors are manifested in noisy re.ectance samples. Another disadvantage is 
that it does not directly estimated surface normals, which are ulitimately required for measuring re.ectance. 
Estimating surface normals from a range-scan requires differentiation of the discrete shape, and this 
is an additional source of noise for re.ectance measurements. By di­rectly estimating surface normals, 
this source of noise can be eliminated. Surface normals can be estimated using photometric stereo, but 
in it s classic formulation, this violates the requirement of being independent of re.ectance. Recently, 
we have seen the devel­opment of color-based techniques can be used to create photometric stereo systems 
that are independent of dichromatic re.ectance [MZKB05], and Helmholtz stereopsis provides a means of 
estimating surface normals in a manner that is independent of re.ectance for any surface whosere.ectance 
canberepresentedby an SVBRDF [ZBK02]. Current best practice is to recover coarse gemoetry using structured 
lighting and then to combine this with surface normal estimates obtained photometrically [NRDR05]. An 
example of this will be discussed in the .nal section of this document. An alternative approachisto develop 
methodsthat simulataneously estimateshapeandrelfectance.To date, this approach has been restricted to 
methods that rely on parametric re.ectance models [GCHS03, Geo03]. Doing this for more general representations 
of re.ectance re­mains an open problem.  3 Spatially-Varying Re.ectance Models Jason Lawrence, University 
of Virginia The appearanceof opaque surfacesis characterizedbythe Spatially-Varying Bidirectional Re.ectance 
Distribution Function (SVBRDF). The SVBRDF is equal to the amount of light re.ectedfroman object s surfaceasa 
functionofthe positionalongthe surface (u,v) and the direction of incidence .i and re.ectance .o, parameterized 
with respect to the local surface normal and tangent directions: S(u,v,.i,.o) (3.1) Technically, the 
SVBRDF also depends on wavelength., although we will generally ig­nore this by assuming it is expressed 
within a tristimulus color space (e.g. RGB, HSV, etc.). Auseful way to think about this function is that 
it encodes the BRDF at every sur­face location, which is readily veri.ed by .xing the spatial coordinates, 
resluting in a 4D function de.ned over incoming andre.ected directions. Thefore, the SVBRDF has6de­grees 
of freedom and it is precisely this high dimensionality which makes it dif.cult to measure and represent. 
This partof theclass brie.yreviews modern methods for measuring the SVBRDFofreal­world materials before 
focusing on the challenges and emerging strategies for represent­ing these datasets.  3.1 Acquisition 
Measurements of the SVBRDF of a real-world object are captured in images taken from different viewing 
directions and under varying point illumination. Figure 3.1 showa few measurementsofa slightly embossedgreeting 
cardthat showsa metallic dove framedby a depiction of the sun. Note the change in both viewing direction 
and light position in these images. This same measurement strategy also works for curved objects, although 
knowledge of the 3D shape is required to correctly interpret the position of each measurement within 
the local coordinate frame. 3.2 Representation The datasets that result from these acquisition procedures 
are typically massive. For ex­ample,thedove datasetinFigure3.1 consistsof5 × 400 cameraxlight positions 
foratotal of2,000 images coveringaspatialareaof470 × 510 (totaling 5.5 GB of storage). However, many 
applications in computer graphics and vision require a compact representation. The clearest example from 
graphics is perhaps interactive rendering tasks which have strictbandwidthrequirements. Inthe contextof 
physically-basedor global illumination rendering, it is also important that a representation support 
ef.cient sampling so that it may be used within Monte Carlo simulations of light transport. Finally, 
the ability to edit these datasets is beginning to receive more attention as measured materials move 
into production settings where designers require the same level of control that conventional parametric 
models have provided. Thispartofthe class focusesononlytwoof thesegoals which turnouttobeveryrelated: 
providingarepresentation thatis both compact and editable. 3.2.1 Basis Decomposition The general strategy 
is to perform some type of basis decomposition of the SVBRDF. Speci.cally, the input measurements are 
projected into a K-dimensional linear subspace spanned by the functions .k(.i,.o) (which are de.ned over 
the same domain as the BRDF) and Tk(u,v), the coordinates within this basis that best .t the input: K 
S(u,v,.i,.o) . Tk(u,v).k(.i,.o) (3.2) k=1 Whenever Kislessthanthe average numberofre.ectance measurementsateach 
surface location, this process compresses the data at the cost of some numerical error. In practice, 
Kis typically several orders of magnitude smallerso the compression ratios aresigni.cant (e.g., K= 3is 
suf.cient to represent the dove dataset). Although there are potentially many different K-dimensional 
bases with comparable nu­merical error, the goal of producing a .nal representation that supports editing 
requires identifying those which reveal the intuitive latent structure in these datasets. In particu­lar, 
we will often be interested in computing a decomposition that is consistent with how the input was physically 
manufactured. In the case of the dove greeting card we would prefera 3-dimensional basis that clearly 
separates the two typesof colored paper (yellow and blue) from the metallic material distributed in the 
shape of a dove. The main differ­ence between the strategies we will review relate to the properties 
and constraints that are placed on the bases and coordinates and the speci.c algorithms used to execute 
the decomposition. 3.2.2 Parametric Models Existing representations can be broadly classi.ed as being 
either parametric or non­parametric which refers to the representation of the basis BRDFs. One of the 
early para­metric methods was introduced by Lensch et al. [LKG+01, LKG+03] (earlier work by McAllister 
[McA02a] is similar, but without the critical clustering step that computes a low-dimensional basis onto 
which the data is then projected). Lensch et al. [LKG+01] ac­quire high-dynamic range images of an object 
under varying positions of a light-source and from different viewing angles. As is often the case for 
this type of acquisition, the positionof the lightis estimatedfrom its image observedina setofprecisely 
aligned and highly-re.ective spheres (e.g., ball bearings) placed in the camera s .eld of view. A3D model 
of the object s shape is obtained from either a structured light scanner (after coatingtheobjectinaremovablediffusepowder)oraCT 
scanner.Eachre.ectanceimage is then aligned to the 3D model using an iterative optimization procedure 
that maximizes overlapping silhouette boundaries. Once aligned, the set of image values at a particular 
surface location can be interpreted as measurementsofthe BRDFat thatposition. The parametersofa single-lobe 
isotropic Lafortune analytic BRDF model [LFTG97] to a subset of the entire collection of BRDF measurements 
collected across the surface. Because this analytic model is non-linear in its parameters, this step 
requires non-linear optimization for which they use the Levenberg-Marquardt (LM) algorithm. The covariance 
matrix resulting from the LM algorithm provides the direction in this parameter space along which there 
was maximum variance in the observations. This in­formationisusedtodriveadivisive clustering algorithmthatrepeatedlysplitsthe 
cluster with the greatest inter-cluster error, generating two new cluster centers positioned on op­posing 
sides of this principal direction of variance. This process continues until either a user speci.ed number 
of clusters or an error threshold is met. Lastly, the cluster centers are interpreted as de.ning a linear 
basis onto which the mea­surements at each location are projected, a process that requires solving a 
single linear system. This method achieves intuitive separations for several real-world datasets, al­thoughtherearesituationinwhichaclusteringapproachwouldfailto 
isolateaSVBRDF s component BRDFs (some examples are shown in Section 3.3). Goldman et al. [GCHS05a] also 
describes a paramtric approach to representing spatially­varying re.ectance. However, they chose to .t 
measured data to the Ward BRDF model [War92] and added the surface orientation at each point as free 
parameters in the optimization (in addition to the basis BRDF parameters and spatial blending weights). 
Finally, they speci.cally discuss how the .nal representation supports editing.  3.3 The Inverse ShadeTree 
Framework Conceptually, non-parametric models are computed in an identical fashion in that some type 
of Expectation-Maximization algorithm is used that alternates between estimating thebasisBRDFsandtheir 
blending weights.ThekeydifferenceisthatthebasisBRDFsare represented in tabular form or within some secondary 
basis (e.g., Radial Basis Functions or wavelets). This approach naturally provides greater .delity to 
measured data, but comes at the cost of larger datasets and more delicate interpolation and data processing. 
The Inverse ShadeTree (IST) framework [LBAD+06b] was the .rst project to explore a fully editable non-parametricrepresentationof 
spatially-varyingre.ectance. Thekey idea isto performahierarchical decompositionof measurementsofthe 
SVBRDF, shownatthe root of the tree diagram in Figure 3.2, into lower-dimensional components. This achieves 
the combined goal of compressing the data (the nodes at each level require less storage than their parent) 
and providing an editable representation (the leaf nodes of this tree re­veal intuitive and meaningful 
latent structure in the dataset that can be directly modi.ed bya designer). Figure 3.2: The Inverse 
ShadeTree framework [LBAD+06b] introduces techniques for decompos­ing measured SVBRDF data intoasetof(a) 
spatially-varying blending weight maps and(b) basis BRDFs. The basis BRDFs are factored into sampled 
2D functions corresponding to (c) specular and (d) diffuse components of re.ectance (we show lit spheres 
rendered with these factors, not the 2D factors themselves). These2D functionsare further decomposedinto(e&#38;f)1D 
curves. Inad­ditiontoproviding accurate interactiverenderingoftheoriginal SVBRDF,thisrepresentationalso 
supports editing either(a)the spatial distribution of the component materials or(b)individual materialproperties. 
The latteris accomplishedby editing(e &#38;f)the sampled curves. The concept of a shade tree was introduced 
by Rob Cook in 1984 [Coo84] as a way of assembling complex re.ectance functions by combining simple parametric 
and sampled functions using various combination operators, working from the leaf nodes up to the root. 
This IST framework essentially inverts this process: decomposing a complex re­.ectance function into 
a shade tree by working from the root down toward the leaves. The other important contribution of [LBAD+06b] 
was relating the decomposition at each level to factoring an appropriately constructed matrix. Computing 
a shade tree from measured SVBRDF data involves several steps. At the top level, measurements of the 
SVBRDF are .rst organized into a regularly spaced matrix by simply unrolling each image into a separate 
column. This preserves the spatial variation of this function along the rows of this matrix and the angular 
variation along its columns (color variation may be preserved along either rows or columns, although 
the original paper did not explore this trade-offand instead computed only colorized BRDFs). Fac­toring 
this matrix computes a sum ofproducts of functions that depend only on spatial position and angular position 
(both incoming and re.ected directions). These are esti­matesof thebasis BRDFs, storedin tabular form, 
and the blending weights,respectively. Many standardfactorization algorithms will not necessarily favor 
an intuitive or mean­ingful decomposition. Therefore, the key research challenge in this work is to design 
an algorithm that generates not only an accurate factorization, but one that is also intuitively editable. 
A common strategy for these types of problems is to place constraints on the result­ing factors that 
guarantee they are physically plausible. BRDFs and spatial blending weights have physical properties 
that can be translated into constraints. For example, re.ectance functions are non-negative. Therefore, 
we may restrict the factorization to be non-negative to guarantee it doesn t violate this property. Another 
property of natural materials that becomes a constraint on the optimization is that they are typically 
sparse: even though a dataset might be composed of seveal unique BRDFs, there are typically onlyafew 
blended togetheratanyone surface location. Finally,therearedomain-speci.c constraints such as enforcing 
the basis BRDFs conserve energy and are monotonically de­creasing along certain directions. There are 
a variety of algorithms available for computing a matrix factorization. We brie.y compare existing approaches 
and discuss the conditions under which they fail toprovidea meaningful decomposition. PCA/ICA: Two popular 
rank reduction algorithms are Principal Component Analysis (PCA) and Independent Component Analysis (ICA), 
along with extensions such as mul­tilinear tensor factorization [VT04]. The main advantage of PCA is 
that it yields a global minimum in the sense of total least squares. However, these algorithms recover 
a basis that is orthonormal (for PCA) or statistically independent (for ICA). These restrictions are 
not suf.cient to produce a meaningful description of the data. In particular, they allow negative values, 
resulting in a representation whose terms cannot be edited inde­pendently Clustering: One popular method 
for clustering data is the k-means algorithm [HW79]. Like all clustering algorithms, k-means partitionsthe 
input into disjoint sets, associating each point witharepresentative cluster center. This canbe interpretedasa 
factorizationof the SVBRDF. Although clustering performs well on input with a small basis that is well­separated 
over the surface, it typically fails to recover a useful basis when the SVBRDF exhibits blending of its 
component materials. Non-NegativeMatrixFactorization: Another matrix decomposition approach is Non-Negative 
Matrix Factorization (NMF) [LS99]. Together with similar algorithms such as Probabilistic Latent Semantic 
Indexing [Hof99], NMF guarantees that both resulting fac­tors contain only non-negative values. One motivation 
for this constraint is to encourage the algorithm to describe the input data as the sum of positive parts, 
thereby producing a more meaningful factorization. However, the character of the decomposition is sensitive 
to small changes in the data (including those due to measurement noise and misalign­ment), and the non-negativity 
constraint is not always enough to guarantee an editable separation. 3.3.1 Alternating Constrained Least 
Squares Lawrence et al. [LBAD+06b] introduce an algorithm for computing the factorization Z WH, subject 
to general linear constraints on W and H. This algorithm is built upon ef.cient numerical methods for 
solving convex quadraticprogramming (QP) problems of the form: minimize 1 b - Mx 2 subject to l = x = 
u (3.3) x.Rn 2 Ax The n-element vector x is called the vector of unknowns, M is called the least-squares 
ma­trix and b is the vector of observations. The vectors u and l provide the upper and lower bound constraints 
of both x andthe linearcombinations encodedinthe matrix A,called the general constraints. There are several 
algorithms available for solving these types of prob­lems. They usedan inertia-controlling method that 
maintainsa Choleskyfactorizationof the reduced Hessian of the objective function [GMSW84]. As with NMF, 
W and H are initialized to contain positive random values, and ACLS minimizes the Euclidean error of 
this approximation by alternately updating these two matrices. This problem is known to be convex in 
either W or H separately, but not si­multaneously in both. As a consequence, ACLS is guaranteed to recover 
only a local minimum. Without loss of generality, consider the case where bothV and W arerow vectors(v 
 wH). For .xed H,the current estimate ofw is updated to minimize the Euclidean distance with the corresponding 
row in V, subject to the linear constraint w = 0. This can be accomplished by solving a QP problem in 
Equation 3.3, with M = HT , b = vT, and x = wT.To constrain the solution to be non-negative, set l = 
0andu = 8. The entire matrix W is determined by computing the above solution for each of its rows in 
turn. Similarly, His computed one column at a time. Alternating between estimating W and H achieves a 
non-negative factorization of the input matrix V. Note these steps are guaranteed to never increase the 
Euclidean error, thus ACLS eventually converges to a stationary point. Sparsity is considered by modifying 
the objective function in a way that penalizes the sum of the non-maximum elements in w. Evaluating this 
measure of sparsity at several different coordinate positions showsit has the desiredeffectof favoringa 
solution thatis closely aligned to one of the basis axes. This penalty is weighted against the Euclidean 
error in the objective function through a user-speci.ed parameter. We compare the blending weights obtained 
from ACLS to other factorization algorithms for the wood+tape datasets (please consult the talk slides 
for comparisons). To visual­ize the output of SigularValue Decomposition (SVD), we mixed positive values 
into the green color channel and negative values into the red color channel. Although this is the optimalresultintermsofRMSerror,itfailstoprovidean 
editable characterizationofthis dataset. Non-Negative MatrixFactorization (NMF) [LS99] showsa better 
separation due to its non-negativity constraint, but it still shows signi.cant blending between the com­ponent 
materials. A k-means clustering algorithms provides the most sparse separation, but cannot recover a 
separate term for the Scotch tape in this dataset due to its inability to account for linear combinations 
of multiple terms. Finally, ACLS does a better job of disentangling the component materials in this dataset, 
but at the expected cost of larger numerical error. Each 4D BRDF is further factored into the sum of 
products of 2D functions which cor­respond to its dominant re.ectance lobes (i.e., backscattering lobe, 
specular lobe, diffuse lobe, etc.). Each of these are further factored into the product of 1D curves 
which cap­ture salient features of the BRDF such as the shape and size of its specular highlight (as 
seen in a curve of the half-angle) and brightening and color shifts toward grazing an­glesduetoFresneleffects(as 
seenina colorized curve de.ned overthedifference-angle). It is these curves that a designer can directly 
manipulate to control the scattering prop­erties of the component materials. We demonstrate three types 
of edits: modi.cations to the specular highlight of the metallic silver material in the dove dataset, 
replacing a subtree with curves computed from entries in the MERL/MIT database of isotropic BRDFs [MPBM03b], 
and interactive modi.cations to the spatial blending weights to alter the position of the Scotch tape. 
 3.4 Conclusion and Directions of Future Research To be useful in practice, representations of measured 
re.ectance data must be compact, accurate, and support editing. It s also important to consider how they 
may be inte­grated within existing rendering systems, although this topic was not discussed in detail 
(see [LRR04, CJAMJ05] for information on this topic). Representations based on basis function decomposition 
typically provide the greatest .­delity to measured data. Furthermore, approximating the input in a low-dimensional 
basis achieves signi.cant compression and reveal intuitive latent structure that allows editing the .nal 
result. Existing methods can be differentiated based on whether a para­metric or non-parametric model 
is used to represent the basis BRDFs. 3.4.1 Parametric vs. Non-Parametric It s important to understand 
the trade-offs involved in using a parametric or non­parametric model of the basis functions. Onereal 
practicalproblem concerns scattered data interpolation. Keepin mind that these datasets often consist 
of measurements scattered across a high-dimensional domain. A notoriously dif.cult task is to reconstruct 
a continuous approximation from these sam­ples, but is required to use them directly in a rendering system 
or, in the case of the IST framework, to generate the input matrix (see [LBAD+06b, Law06] for further 
details). On the other hand, .tting the parameters of an analytic model to scattered datais often easier 
and avoids any explicit interpolation stage (it can in fact be regarded as performing in­terpolation 
for a restricted set of continuous approximations). Generic data interpolation techniques have been applied 
in this context. Notable examples include the push-pull al­gorithm, .rst used with surface light .eld 
data [GGSC96], and techniques such as .tting Radial Basis Functions [ZERB05]. One area where non-parametric 
methods outperform parametric methods is in their in­herent .exibility and accuracy. Obviously, a tabulated 
tabulated grid of numbers can representa much wider rangeof functions than those capturedbyan analytic 
functionof a handful of parameters. In fact, we ve seen that analytic models are designed to model a 
speci.c class of materials, making a parametric approach particularly error-prone for datasets that include 
different types of materials (e.g., wood+tape dataset). The down­side, of course, is that there is more 
data to consider. Finally, please note that all of these techniques are susceptible to poor local minima. 
Non-parametric methodsareperhaps betterinthisregardsincetheydonot involvethe non-linear optimization 
that appears with parametric methods which can be unstable for multi-lobe models or particularly sparse 
and noisy input. Nevertheless, they are often very sensitive to their starting position and future work 
should consider optimal ways of collecting inputfroma human userto make this automatic separation morerobust. 
 3.4.2 Open Problems Perhaps the clearest direction of future research is to develop techniques that 
bring a wider range of appearance functions into this type of inverse shade tree framework or at least 
focus on the goal of editing. Examples include early work in developing repre­sentations for heterogeneous 
translucent materials [PvBM+06b] and time-varying mate­rials [GTR+06, WTL+06]. The fact that all of these 
basis decomposition techniques are fundamentally based on Expectation-Maximization suggests placing them 
in a unifying probabilistic framework. In particular, note the deep similarities between the hierarchical 
decomposition used in the IST framework and hierarchical probabilistic models [GaHSSR04] that are gain­ing 
traction with many problems in machine learning [Jor99, BNJ03] and computer vi­sion [FFP05]. This type 
of perspective may help clarify the assumptions existing repre­sentations make and set the stage for 
generalizing these algorithms to work with a wider range of datasets. Additionalresearchin measuring 
these datasetsisalso necessary.In particular,it s worth considering ways of performing synchronous measurement 
and appearance. Objects with opticalpropertiesthataredif.culttomodelbyhand(andthusjustifytakingadata-driven 
approach) also tend to be dif.cult to scan. What novel optical setups support collecting reliable measurements 
of both shape and scattering? Also, the calibration burden of these methods remains prohibitevlyhigh. 
What can be done with sparse and noisy measure­ments and what devices might eliminate the need for such 
fragile calibration procedures?  4 From BSSRDF to 8D Re.ectance Fields Hendrik Lensch, Max-Planck-Institut 
für Informatik In these notes we have so far concentrated on capturing surfacere.ectance whereincident 
light is scattered locally at the point of incidence. The models that have been presented so far ignore 
global effects such as subsurface scattering, transmission, or interre.ections in complicated surface 
geometry. 4.1 BTFs and Distant Light Re.ectance Fields Aquite general representation that can represent 
even non-local effects are so-called bi­directional texturefunctions (BTFs) [DvGNK97]. In principle,aBTF 
captures the apparent spatially-varying BRDF at a point (x,y) for parallel incident light: S(x,y,.i, 
.i,.o,.o), (4.1) i.e. it might contain data that cannot easily be described by a particular analytic 
BRDF model at each a point because the apparent re.ectance might be in.uenced by masking, shadowing, 
or interre.ections. To avoid the approximation errors by an analytic model, BTF data is quite often represented 
in tabulated form, i.e. tensors, and general compres­sion schemes such as wavelets, higher order PCA, 
clustering, or spherical harmonicsrep­resentations are frequently applied [MSK06]. The de.nition of a 
BTF is equivalent to the de.nition of a re.ectance .eld [DHT+00a] for distant lighting. The only difference 
being that in a BTF the points (x,y) are typically de.ned over the surface of a small material patch 
while in the re.ectance .eld approach (x,y) typically refer to locations in a camera image. For this 
reason, the same techniques for compressing and rendering of such re.ectance .elds can be applied. For 
acquiringBTFs,all incidentand outgoingdirectionsneedtobe sampled:Variousap­proaches have been proposed 
for acquiring BTFs from simulated data [TZL+02] or for measuring BTFs using moving robot gantries [DvGNK97, 
MMS+05, DHT+00a], static devices based on mirrors [HP03] for multiplexing a single camera, or multi-camera 
se­tups [MBK05]. A good survey on acquiring, processing and rendering BTFs ef.ciently can be found in 
[MMS+05]. 4.2 BSSRDFs BTFs and re.ectance .elds introduced so far assume that the light hitting the 
surface is parallel, originating from an in.nitely far away light source. If one wants to simulate a 
close by light source, or, equivalently, to project a light pattern into the scene, e.g. from a spot 
light or as the result of two objects interacting with each other, the far-.eld assump­tion is not valid. 
Instead, one needs to record the so-called near-.eld re.ectance .eld which couples incident to outgoing 
light rays. Here, the re.ectance depends both on the point of incidence and the re.ection point. For 
surfaces, this distinction of incident and re.ection point is necessary only if signif­icant interre.ections 
or subsurface scattering is observed. Subsurface scattering can be ef.ciently described by the Bi-direction 
Scattering Surface Re.ectance Distribution Func­tion (BSSRDF): dL(xo,yo,.o, .o) S(xi,yi,.i, .i,xo,yo,.o, 
.o)= (4.2) dF(xi,yi,.i, .i) Note that the BSSRDF is actually de.ned as the quotient of differential re.ected 
ra­diance over the incident .ux while a BRDF is the re.ected radiance over the irradiance. The de.nition 
of a BSSRDF is again equiva­lent to the de.nition of a full 8D re.ectance .eld [DHT+00a]. Because of 
their high dimensionality BSSRDFs arein generalhardtorepresentandto acquire. Asimple analytic BSSRDF 
model for homoge­neous materials has therefore been proposed by Jensen et al. [JMLH01a]. It has later 
been updated to incorporate homogeneous multi­ layer materials [DJ05a]. In the .rst paper, a simple 
measurement setup is presented in for al. [JMLH01a] for measuring homogeneous estimatingthe parametersofthe 
model:Acol- BSSRDFs. limated light beam hits the homogeneous slab at one point, and theresulting spatial 
distribu­tion of the re.ected light is measured using a camera. Asimpler device for instant measurement 
of a few samples has been proposed in [WMP+06a]. 4.3 Diffuse Subsurface Scattering For heterogeneous 
surfaces it is impractical to densely sample all eight dimension of the BSSRDF/re.ectance .elds. In order 
to allow for reasonable sampling effort, one strategy is to assume a less complex light transport. Goesele 
et al. [GLL+04a] present a measurement setup for acquiring the appearance of translucent objects witha 
high scattering albedo. In these cases,a photon travelling some distance through the material undergoes 
so many scattering events that the incident light direction has actually no in.uence on the outgoing 
light direction. Since the directional dependence can be dropped from the full 8D BSSRDF, the problem 
can be reduced to a 4D diffuse scattering R(xi,yi,xo,yo,) function that solely depends on the point where 
the light enters the material and the position where it leaves when being re.ected. For any pair of points 
Rindicates how much the incident irradiance at point (xi,yi) contributes to the outgoing radiosity at 
point (xo,yo). Asimple 4D tensor can be used to represent this 4D function. In order to measure its en­tries 
Goeseleetal.makeuseofalaserprojector,thatsweepsan individuallightpointover the surface of a translucent 
object (see Figure 4.2). Aset of HDR video cameras capture (a) (b) there.ectedlightateveryothersurfacepoint.Oneofthese 
measurementscorrespondto exactly one slice of the 4D tensor. Illuminating every surface point once eventually 
.lls the entire tensor. Because of occlusions and self-shadowing it is however likely that for some parts 
of the object no measurements areavailable, in which cases textureimpainting isappliedto.llthegaps.Foref.cientstorage,ahierarchicalrepresentationofthe 
tensor is chosen, providing a high sampling rates only close to the point of incidence, where the BSSRDF 
drops of quickly, while for distant points a coarser sampling is suf.cient. Recently,Peers at al. [PvBM+06c]presentedadifferent 
setupto acquirethe spatially vary­ing4DBSSRDFofaplanarslabof material.Inorderto acceleratethe acquisition,agridof 
points is swept over the surface. Ahigh compression on the captured tensor is achieved by aligning the 
main features, i.e. the point of incidence in every row, followed by a principle component analysis. 
 4.4 ArbitraryLightTransport For arbitrary materials and scenes, Masselus et al. [MPDW03] presented the 
.rst acqui­sition system for re.ectance .elds that are suitable for relighting with 4D incident light 
.elds, i.e. where the re.ected light depends on individual incident light rays. For the ac­quisition, 
a video projector swept a small block of light over the scene. In order to cope with the complexity of 
the acquisition problem the appearance was captured for a single viewpoint only. Additionally, the resolution 
of the incident light .eld was limited to a projectorresolutionof only16 × 16foracoupleofprojectorlocations. 
Thislowresolution resultsin clear block artifactsin therelit images. 4.4.1 Single View Single Projector 
In order to avoidthose artifacts, it is necessary to measure the re.ectance for every pair of rays between 
a camera and a projector, i.e. to acquire the re.ectance for every pair of camera and projector pixels, 
again resulting in a fourth order tensor. While in principle this high resolution re.ectance .eld could 
be acquired using scanning, it would be a too slow process. Sen et al. [SCG+05] exploited the fact that 
for quite a number of real-world scenes the lighttransportmatrix/re.ectance.eldisrathersparse,i.e.thatonlyasmall 
fractionofthe possible input rays actually contribute to the same re.ected ray. In this case, it is possible 
to exploit the sparseness by illuminating the scene and measuring the re.ected light rays for multiple 
illuminating light rays at once. It is possible to turn on two light rays/two projector pixels at the 
same time and tell their corresponding measurements apart when those two rays affect completely separated 
parts of the scene/the camera images. We call such two illumination rays radiometrically independent. 
In the same way one can also call two blocks of the projector pixels radiometrically independent if no 
camera pixel will be illuminated by both at the same time. Because of this property it is possible to 
measure the rays inside the independent block in parallel, i.e. to parallelize the exact acquisition 
of these two blocks. In their paper, Sen et al. propose a hierarchical approach for determining which 
sub-blocks are independent: Starting from a full white projector image, each block is subdivided into 
four children which again get subdivided. Initially, thiswillrequireone measurementperblockcorrespondingtoasequential 
acquisition.At some point in time the algorithm might however detect that at some level two blocks are 
now radiometrically independent, allowing for parallelized subdivision of these blocks inthe future. 
The neteffectofthis parallelizationis signi.cant,resultingina complexity that is O(log(n)) for n projector 
pixels. For quite a number of scenes the pixel-to-pixel re.ectance .eld betweenaone mega-pixelprojectoranda 
camera canbe acquiredinonly a couple of thousand images instead of a million. Once having acquired the 
pixel-to-pixel light transport one can apply Helmholtz reci­procity to invert the role of projectors 
and cameras. Helmholtz reciprocity states that the re.ectance measured for one path does not change no 
matter if one follows the path from the light source to the receiver or the other way around. One simply 
has to compute the transpose of the acquired tensor to obtain the re.ectance .eld from the camera (the 
new virtual projector) to the projector (which gets the new camera). The transpose corresponds to just 
a resorting of rays, and therefore can be computed very ef.ciently (see Figure 4.3). This dual imaging 
paradigm canbe usedtoef.ciently capturea6Dre.ectance .eldfrom a single viewpoint, i.e. to measure the 
projector to camera re.ectance .elds for multiple (a) (b) (c) Figure 4.3: Dual Photography: (a) Conventional 
photograph of a scene, illuminated by a projec­tor with all its pixels turned on. (b) After measuring 
the light transport between the projector and the camera using structured illumination, dual photography 
is able to synthesize a photorealistic image from the point of view of the projector. This image has 
the resolution of the projector and is illuminated by a light source at the position of the camera. The 
technique can capture subtle illumination effects such as caustics and self-shadowing. Note, for example, 
how the glass bottle in the primal image (a) appears as the caustic in the dual image (b) and vice-versa. 
Because we have determined the complete light transport between the projector and camera, it is easy 
to relight the dual image usinga synthetic light source (c). projectors.Theproblemisthatduringthe acquisitionthere.ectance.eldshavetobecap­tured 
sequentially for each projector because projectors areactivedevices. Their projected patterns might actually 
illuminate the same points in the scene causing dif.culties when trying to separate their contribution. 
If one uses the dual setup however, where the origi­nal cameraisreplacedbya singleprojectorandallprojectorarereplacedby 
cameras, one canverywellacquiretheprojector/camerare.ectance.eldsinparallelsince camerasare passive devices 
which do not interfere with each other. Applying Helmholtz reciprocity, this setup can virtually be transformed 
into the single camera/multiple projector con­.guration. By swapping camera and projectors one can capture 
a 6D re.ectance .eld at the same time cost as a 4D re.ectance .eld. The resulting data now allows to 
relight an arbitrary complex scenes with arbitrary incident light .elds, i.e. with high frequency illumination 
patterns from various virtual projector positions. 4.4.2 8D Re.ectance Fields The previous acceleration 
for measuring the light transport between a projector and a camera is however limited to scenes where 
the light transport tensor is rather sparse. This is often the case for an individual object in a black 
room where few interre.ections and little subsurface scattering take places. For more general cases, 
the light transport matrix is rather dense, i.e. every projector pixel indirectly affects every camera 
pixel due to multiple scattering events. The resulting light transport is however rather smooth for large 
blocksofthe tensor.For example, illuminatingonespotonawallwillhavea rather similar effect to all points 
on an opposite wall. While this smoothness might be partially destroyed by textures on both walls, the 
underlying light transport still has rather low complexity or low dimensionality it is called data sparse. 
In other parts in the ray space, however, for example for direct re.ections or refractions, the re.ectance 
.eld might not be smooth at all. H-matrices [Hac99]areanef.cientwayforrepresenting tensors whichare partially 
data­sparse. Ina H-matrix the original matrix is hierarchically subdivided into smaller blocks, e.g. 
usinga quad-tree fora2D matrix, and for every sub-blocka low-rank approximation is given, approximating 
the original matrix s entries. If the approximation error for one block is too large, the block is further 
subdivided. As H-matrices have been originally developed to solve integral equations more ef.ciently, 
and since the Rendering Equation which describes the light transport in arbitrary scenes is an integral 
equation, re.ectance .elds can be very ef.ciently described by this data structure. Besides resulting 
in a compact representation of a re.ectance .eld, H-matrices can ef.­ciently be evaluated during relighting, 
where the incident light .eld is simply multiplied with the tensor. H-matrices further open the way foref.cient 
acquisitionofre.ectance .eldof arbitrarily complex scenes where interre.ections and scattering cannot 
be neglected, as well as for the acquisitionof8Dre.ectance .elds. Garg et al. [GTLL06] have proposed 
a measurement setup that forces the captured re­.ectance tensor to be symmetric. In the setup, every 
camera is paired with one projector using a beam splitter in such a way that it is possible to emit light 
and to measure light exactly along the same ray. In the resulting transport tensor every off-diagonal 
sub-block isthereforerepresentedtwice, onceinits originalformand oncebeing transposed,i.e.we could capture 
the original and the dual image for one sub-block with just two images by fully illuminating the corresponding 
two projector blocks. Since one of the images corresponds to the sum along the rows of the block and 
the other image to the sum along the columns of the block, it is possible to obtain a rank-1 ap­proximation 
of this block with just these two images, simply as the tensor product of the two measurements obtained 
when .rst illuminating with one block of one projector, measuring the result in one block of some camera, 
and then measuring the transpose, i.e. measuring at the block of the .rst projector and emitting light 
from the .rst camera s block. Let slookataverysimple examplewheretheoff-diagonal block B2 has been determined 
to be rank-1: B1 B20 B2 B10 T ==+(4.3) B2 TB3B2 T 00 B3 In this case, we can determine all entries in 
B2 (and B2T)from just two images whileB1 and B3 might require additional investigation. From the intended 
solution one can subtract the already determined matrix 0 B2 , which leaves us with some very interesting 
rest which only contains the BT 0 2 remaining, yet unknown blocks. Those two blocks are however arranged 
in a very in­teresting con.guration: they are radiometrically independent, since they clearly effect 
completely different camera and projector regions. As a consequence, those two blocks can again be investigated 
further in parallel. It allows for the ef.cient and parallelized acquisition of even dense matrices as 
long as the matrices are data-sparse. InFigure4.4,weshowa low-resolution8Dre.ectance .eldfor3 × 3cameras 
and3 × 3 projectors. With the symmetric photography approach one can acquire the light trans­port of 
scenes as complicated as this glass of gummy bears and faithfully reproduce the appearance of the original 
object (Figure 4.5).  4.5 Conclusions and FutureWork In this section we have introduced the notion of 
re.ectance .elds for relighting with spa­tially varying illumination patterns: from the acquisition of 
heterogeneous translucent objects to methods for acquiring the ray-to-ray light transport in arbitrary 
materials and scenes. 4.5.1 Open Problems Onebigproblemof sampling BSSRDFsorre.ectance .eldssofaristhe 
limitedresolution withrespecttothe incidentand outgoingdirections. While solutionshavebeenproposed to 
increase the resolution of the incident illumination [HED05, FBLS07] by using special light source arrangement, 
the resolution of the viewing directions is still limited to the (a) (b) spacing between adjacent camera 
positions. Ascheme for adaptively controlling the res­olution in the viewing and the illumination direction 
still needs to be invented. Forrepresentingre.ectance .elds, various bases have beenproposed, wavelets, 
spherical harmonics, or the above mentioned H-matrices. It remains to be seen how to select the optimal 
representation, and how to determine the dimensionality of the light transport locally. Astill outstanding 
goal is the acquisition of re.ectance .elds for relighting with 4D in­cident light .elds for dynamic 
objects. While initial solutions to measuring time-varying far-.eldre.ectance .elds at interactive rates 
have been demonstrated [WGT+05, ECJ+06] the signi.cantly higher complexity of near-.eld re.ectance .elds 
currently requires too many images for every pair of viewing and illumination directions.  5 The HumanFace 
Scanner Project Tim Weyrich, Princeton University Thispartoftheclasspresentsaprojectthathas leveraged 
principlesof appearanceacqui­sition and representation to acquire digital models of human faces. Creating 
digital faces that are indistinguishable from real ones is one of the biggest challenges in computer 
graphics. Although general rendering quality in graphics often achieves photo-realistic appearanceof 
synthetic objects,renderingof human faces stillremainsademanding task. This is not onlybecause of the 
complexity of facial appearance, but also due to that fact that human observers are experts in judging 
whether a face is real or not. The process of capturing an actor s performance and likeness has accurately 
been named Digital Face Cloning. Digital face cloning has many applications in movies, games, medicine, 
cosmetics, computer vision, biometrics, and virtual reality. While recent feature .lms al­ready show 
authentic arti.cial renderings of character faces [BL03, Fee04], the datasets havebeen especially designedforan 
appearanceina particular scene.Theyare eitherthe result of extensive manual editing to make the model 
appear real under hand-optimized lighting design [BL03], or they merely tabulate an actor s lighting-dependent 
appearance fromasingle viewpoint [DHT+00b, HWT+04,Fee04].Uptorecently,therewasno generic procedure that 
allows for the acquisition of human faces, leading to a self-contained face model that can be used for 
renderings in an arbitrary context. Key requirements of such amodelare generality,to allowfora .exibleuse,and 
editability,thatis,it shouldbepos­sible to change the face s appearance using intuitive controls. Up 
to date, editing of face modelsrequires the handofa skilled artist witha deep technological understanding. 
The presented project addresses this issue, developing a face acquisition pipeline that allows for the 
automated acquisition of generic face models from individual subjects. This comprises the construction 
of a respective acquisition hardware, the development of a suitedmodelrepresentation,andtheanalysisoffacial 
appearanceacrossmultiplesubjects to derive meaningful controls for editing of the face model. This chapter 
sketches selected topics of the project as presented during the class. For more in-depth information 
and a more comprehensive discussion of related work, refer to [WMP+06b,Wey06]. 5.1 PreviousWork CapturingFace 
Appearance In accordance with the two major paradigms in appear­ance modeling, existing facial appearance 
modeling systems either use an explicit ap­proach, explicitly modeling facial geometry and surface texture[PHL+98], 
or they em­ploy image-based methods, such as re.ectance .elds over a rough impostor geome­try [DHT+00b, 
HWT+04]. In general, explicit models are more directly accessible to edit­ing operations, while image-based 
approach make it easier to achieve photo-realistic re­sults and are less sensitive to measurement errors. 
In our work, however, we target an explicit modeling approach to maintain full .exibility for editing. 
Skin Re.ectance Models Particular attention has to be payed to the modeling of skin re.ectance, as skin 
contributes the largest part of facial appearance. Previous work employs all major classes of re.ectance 
models, BRDF [HK93, MWL+99, Sta01], BSS-RDF [JMLH01b, DJ05b, DJ06], and BTF [CD02], see Chapter 2, to 
model skin re.ectance. Thepresentedproject usesa compound modelof BRDF and BSSRDFrepresentations. Appearance 
Editing Existing ap- Figure 5.1: Physiology of skin. On a micro-scale, human skin is a very heterogeneous 
tissue. How­ever, at scales relevant for rendering, it is suf.­cient to consider the two prominent layers 
epider­mis and dermis. The visual impact of scattering within the hypodermis is negligible. (Image from 
Wikipedia.) 5.2 Skin Appearance Acquisition proaches to realistically alter facial ap­pearance often 
focus on manual editing of facial textures, or use image-based techniques to alter skin appearance in 
photographs [TOS+03]. A higher-level editing technique is to use morphable face models to blend between 
shape and texture of multiple individual face mod­els [BV99, FLS05]. In terms of expres­siveness, we 
target similar high-level operations for ourrepresentation. Production Environment One of the most prominent 
areas of application of digital face cloning are.lm productions. Using latest techniques in face repre­sentation, 
they also use a signi.cant amount of manual labor to meet the requirements of a single shot [Wil05, Hér03, 
BL03, Sag04]. Increasingly, real­istic face rendering gains importance in computer games. Covering most 
of face, skin is the most important aspect of facial appearance. The domi­nanteffectinskinre.ectanceisdueto 
skin s translucentlayers,seeFigure5.1.Inarough approximation, skin consists of two optically active layers, 
the epidermis and the dermis underneathit.Light transportismainlyaffectedbytwoeffects:a)Surfacere.ectionatthe 
air/skin interface(thatis, surfacere.ectancethatcanbe describedbyaBRDFandrefrac­tion that propagates 
light into the skin); b) Scattering and absorption within epidermis and dermis that can be expressed 
as a BSSRDF. This two-fold separation is a commonly used simpli.cation and re.ects our goal to develop 
a skin model of high practical value. Moresophisticated, multi-layered, models exist. An excellent survey 
on the physiological and anatomical properties of skin and the state of the art in skin appearance modeling 
has been published by Igarashi et al. [INN05]. We built two custom re.ectance acquisition devices to 
capture surface and subsurface re.ectance independently. Acontact-device allows for sparse subsurface 
measurements across the face. Measuring pure surface re.ectance, however, is dif.cult. Hence, our sec­ond 
device captures the full skin re.ectance as a whole. Skin re.ectance being a sum of subsurface and surface 
components, surface re.ectance can then be derived by subtract­ing the measured subsurface component 
from the skin re.ectance measurements. 5.2.1 Subsurface Scattering Acquisition The .rst device, internally 
referred to as the BSSRDF Gun , is a contact-measurement device to measure purely subcutaneous light 
transport. The quantity we are interested in is the wavelength-dependent mean free path f, or skin s 
translucency 1/f, respectively. f is a measure of how far photons travel in average between two scattering 
events. The device feeds light into the skin using an optical .ber.Abundleof optical .bers arranged around 
the feeding .ber collects lightexiting the skin at different distances to the feeding .ber. Digitizing 
the radiant exitance using an HDR camera at the end of the .ber bundle (encased by a light-proof box) 
allows to measure the characteristic radial fall-off, that is, the diffusion kernel due to skin s subsurface 
scattering. We obtain f by .tting the dipole approximation of the diffusion model BSSRDF [JMLH01b] to 
the measured fall-off. As we will see in Section 5.5, translucency only varies minimally across the face. 
Hence, only a few measurements of the BSSRDF Gun are required to obtain representative translucency values 
of a face. Note that an even contact between the .ber probe and the skinisrequiredto eliminate surfacere.ectanceeffect.We 
ensure thisby gently evac­uating the sensor usinga suction pump. 5.2.2 Re.ectance Field Acquisition 
The second measurement device samples skin re.ectance as a whole, that is, the sum of subsurface and 
surface effects. The device captures a 150×16 re.ectance .eld of the face. There.ectance .eld [DHT+00b]isa 
tabulationof imagesfrom,in our case, sixteendiffer­ent viewing directions under 150 different illumination 
directions.To that end, we builta spherical dome containing sixteen 1300×1030 .rewirecameras and 150 
LED light sources (each of them being a disk-shaped panel of 150 LEDs). In order to be able to associate 
pixels in the re.ectance .eld with surface points on the face, we also acquire the facial geometryusinga 
commercial single-shot3D scannerbasedonstereoreconstructionofan IR random speckle projection. Our setup 
is completely synchronized, that is, the 150 light sources are sequentially triggered, while all cameras 
simultaneously acquire images at 12 frames per second. We are imaging each light condition under two 
different exposure timestoincreasethedynamicrangeofour measurements. Hence,afull acquisitiontakes about 
25 seconds. Theresultingre.ectance .eldhastobe radiometrically corrected, accordingtothe spatial location 
of each surface point, considering distance to the respective light source and differences between camera 
sensitivities andlight source characteristics. See section 5.3.1 for more details. Figure 5.2 shows a 
sample re.ectance .eld before correction.  5.3 Face Data Processing Starting from the acquired geometry 
and re.ectance data, synthetic face models have to be constructed. Before an actual skin re.ectance model 
can be .tted, extensive prepro­cessing of the input data is required. As the project aims at the construction 
of a face database, a large number of faces has to be processed. Hence, the processing is largely automated 
in a data processing pipeline that allows for an unsupervised face model con­struction. Figure 5.3 provides 
an overview over the processing pipeline. The raw geom­etry retrieved from the 3D scanner is cleaned, 
parameterized, and up-sampled to obtain a highly resolved model of the facial geometry. Based on this 
geometry, the acquired re­.ectance .eldisre-parameterized intoa lumitexel representation [LKG+01]. Thisrequires 
consideration of photometric and geometric calibrations as well as the computation of camera and light 
source visibilities for every vertex of the geometry. Based on the lu­mitexel representation, detailed 
surface normals are estimated using photometric stereo. The 3D scanner resolution itself does not suf.ce 
to capture .ne-scale normal variations. As an optional path in the processing pipeline, it is possible 
to use the normal estimates tore.ne vertexpositionsinthe source geometry,andtore-iteratetheprocessing 
pipeline. As a result of this processing pipeline, lumitexels and a re.ned normal map are used in a subsequent 
model .t. In the remainder, we are brie.y illuminating two speci.c aspects of this data re.nement that, 
although often receiving little attention, are essential for a radiometrically sound acquisition: system 
calibration and geometry re.nement. 5.3.1 System Calibration The acquisition hardwarerequires careful 
calibration.We generally follow the principles presented in Section 2.4. Here, we are listing the speci.c 
calibration steps necessary for each component within the face scanner hardware. Cameras The dome cameras 
require geometric and photometric calibration. The geo­metric parameters are intrinsics (focal length, 
principal point, aspect ratio, distortion co­ef.cients; determined using procedures from a standard vision 
library [OCV]) and ex­trinsics. The extrinsics of all sixteen cameras are simultaneously estimated using 
a Eu­clidean bundle adjustment optimization basedona 400-frameimage sequenceofanLED swept through the 
common viewing volume. Photometric calibration requires to model vignetting and the spectral sensitivity 
of the individual camera sensors. For vignetting calibration we acquire images of a large sheet of white 
tracing paper in front of a cloudy sky, for each camera .tting a fourth-degree bivariate polynomial to 
the images. For spec­tral (color) calibration, we take images of a color checker boardunder diffuse illumina­tion. 
It is crucial that each camera views the color checker under the exact same condi­tions. We color-correct 
all cameras using an af.ne color correction model, equalizing all cameras to have the same characteristics 
as one reference camera in the acquisition  dome. See [Dan92, FL00, Ami02] fora discussionof different 
color calibration models.A quantitative radiometric calibration is not explicitly acquired at this stage; 
it is implicitly performed by the light source calibration. Light Sources The desired parameters are 
the light source position, color (spectral in­tensity) and the light-cone fall-off. Light source positions 
are assumed to be .xed and are taken from the CAD model of the acquisition dome. The remaining parameters 
are jointly measuredusingaFluorilonTMre.ectancetargetthathasbeen calibratedtodiffusely re.ect 99.9% of 
the incident light uniformly across the spectrum. By acquiring complete re.ectance .elds of the target 
under different orientations, we collect enough data of the individual light sources to model their intensity 
distributions. We assume each intensity distribution to be directionally-dependent and to simply follow 
the 1/r2 law in radial direction (this holds for suf.ciently large distances to the light sources). Hence, 
the radio­metriclight source calibrationcanbe de.nedbyaplanar irradiancecross-sectionthrough its light 
cone.Wemodel each light cone s cross-section using three bivariate second-order polynomial that are independently 
.tted to the red, green, and blue observations of the FluorilonTM target, respectively. Using this simple 
model became possible, as we are us­ing diffuser plates in front of each light source that have experimentally 
been shown to producea near-quadratic intensity fall-off. 3D Scanner The employed commercial 3D scanner 
comes with its own intrinsic and extrinsic calibrationprocedure.In additiontotheseproprietary calibrations,weregister 
the 3D scanner s coordinate system with the coordinate system of the acquisition dome using the Procrustes 
algorithm to match corresponding points of a 3D target that we si­multaneously observe with both the 
3D scanner and the dome s cameras. BSSRDF-Gun The subsurface scattering measurement device .nally requires 
its own unique calibration procedure. We calibrate for the transmission from the sensing .bers to the 
HDR camera and for differences between the individual .bers by taking a white .eld measurement onalight 
table with an opal glass diffuser to ensure maximum uni­formity of the incident light. To calibrate for 
spill-light within the sensor, we acquire a black image (all sensing .bers covered by a black rubber 
sheet) with the feeding .ber turned on. The radiometric calibration .nally calibrates for the feeding 
.ber s irradiance and the spectral sensitivity of the camera sensor by measuring the diffusion kernel 
of a sample of skim milk using our device. Skim milk has previously been measured to high accuracy [JMLH01b] 
and alters only minimally between different vendors.We determine the HDR camera s color calibration so 
that the measured kernel meets the previously mea­sured scatteringpro.leofthemilk,inotherwords,weareusingskimmilkasasecondary 
standard. 5.3.2 GeometryRe.nement The geometry from the 3D scanner suffers from imprecisions, namely 
from a lack of high-frequency details and from high-frequency noise. Hence, additionalgeometric in­formation 
is obtained by estimating normals from the re.ectance .eld using a photo­metric stereo implementation 
after [BP01]. These estimated normals, however, are not free of errors, either. Due to small calibration 
errors normal estimations based on differ­ent cameras tend to be biased, which can lead to an inconsistent 
normal .eld. Nehab et al. [NRDR05] analyzethis generalproblemandproposea methodto combinethereliable 
low-frequency information of a 3D range scan with the high-frequency content of photo­metric normals, 
thereby removing the low-frequency bias from the normal orientations. The method by Nehab et al. is a 
two-step procedure. In a .rst stage, high-frequency details of the normal map are combined with the bias-free 
low-frequency normals of the smoothed geometry. Afterward, geometric detail is added to the smoothed 
geometry that is consistent with the new normals. We use this method to improve both normal maps and 
face geometry, adapting this method to our datasets. Figure 5.4 shows a schematic overviewof the geometryre.nement. 
 5.4 Re.ectance Model Fit We developed a re.ectance model that is capable of describing skin re.ectance 
based on the acquired data. We aimed at a representation that contains intuitive parameters that can 
be manually changed and that roughly re.ects the physiology of skin. These design goals leave a wide 
range of potential models that are more or less strictly aligned with physical reality [INN05]. Akey 
decision in our project, however, has been that the model should not provide more degrees of freedom 
than can be de.ned by the acquired data. A model that contains too many parameters would require ad-hoc 
de.nitions of quantities that have not been measured, which falsi.es the goal of a measurement-based 
skin model. On the other hand, physical quantities within the model should be well­exposed to further 
an elementary understanding of skin re.ectance. Skin Re.ectance Model As previously described, we use 
a simple two-layer model of skin which re.ects the coarse physiological classi.cation of skin into epidermis 
and dermis. On top of the epidermis, we assume a thin oil layer. This decomposition is sim­ple enough 
to be accessible to modeling from measured data. Our re.ectance model separates re.ectance into specular 
and diffuse re.ectance. Diffuse subsurface scattering is modeled using the dipole diffusion approximation 
[JMLH01b], see Section 2.3. This model assumes isotropic scattering (which, as we will show, holds for 
facial skin) and a homogeneous material. In order to achieve spatial variation in the diffuse term, we 
additionally modulate the diffuse term by a modulation texture [GLL+04b]. For the sec­ond (specular) 
term, we employ the widely-usedTorrance-Sparrow BRDF model, which in our experiments has proven to be 
best suited to model skin gloss at the oily skin/air interface. Model Fit During the model .t, we obtain 
model parameters for each surface point on the face.Westartbyestimating the diffuse albedoRdineachpointbyusingan 
extensionof a standardspecular/diffuse separation often used in theVision community. Our exten­sion trades 
the Lambertian model of diffuse re.ectance for a diffuse term that considers the transmissive Fresnel 
terms Ft(.,·), cf. Equation (2.3). (These terms effect an atten­uation for oblique viewing and lighting 
angles and model the fact that due to Fresnel re.ection, less light is able to pass the skin/air interface 
under oblique angles.) The same diffuse term occurs in the BRDF approximation of the dipole diffusion 
model [JMLH01b]. We parameterize our subsurface scattering BSSRDF to meet the average diffuse albedo 
Rd, while maintaining the translucency obtained using the BSSRDF Gun . The modu­lation texture is set 
to scale the underlying BSSRDF to meet Rd in every surface point. Finally, the parametersof theTorrance-Sparrow 
surface BRDF are obtainedby .tting this BRDF to the residual re.ectance samples after subtraction of 
the diffuse re.ectance in each point. See [Wey06] for an in-depth description of the .tting procedure. 
Reconstructions After the model .t, model parameters for each surface point on the faceareknown.Weencode 
these parametersin .oating-point textures overacommon uv­parameterization of the facial geometry. Using 
custom shaders that implement our skin modelwithinaMonte-Carlo raytracer,this enablesustorenderphoto-realisticimagesun­der 
arbitrary illumination from arbitrary vantage points. In particular, as our re.ectance .eldhasbeenacquired 
withinafully calibrated system,it becomes possibletoreplaythe exact illumination and viewing conditions 
of each re.ectance .eld image. This allows to directly evaluate our skin model in a side-by-side comparison 
with photographs from within the acquisition dome. Figure 5.5 shows such a comparison. It compares single 
in­put re.ectance images with synthetic images for different faces and different viewpoints. Note,thataslightblurinthe.ttedmodelre.ectsthefactthateach 
surfacepoint sparam­eters are .ttedto 2,400 input images simultaneously, which in the presence of noise 
and measurementimprecisions makesit impossibletoretrievethe exactinputimagefromthe model. 5.5 Re.ectance 
Analysis Acentral goal of the project has been to analyze skin re.ectance properties over a large group 
of subjects in order to obtain general insight in the variability of skin appearance across individuals. 
TheFace Database To this end, we scanned 149 subjects that were classi.ed by skin type,gender,age,andothertraits,andineachscanwemanually 
classi.edfacialregions, such as forehead, nose, chin, and others. This allows for a statistical analysis 
of charac­teristic variations in skin re.ectance for different populations and across different facial 
regions.Theskintypeis classi.ed accordingtothe Fitzpatricksystem[Fit88].Table5.1ex­plains the Fitzpatrick 
system and shows the distribution of our measurements. Figure 5.2 shows our faceregion classi.cation. 
TranslucencyVariance In an initial experiment on variation of skin translucency, we validated the isotropic 
assumption of our model s subsurface scattering term. By tak­ing subsurface scattering measurements under 
16 different orientations of the sensor, we measured the degree of anisotropy of the diffusion kernel. 
It turns out that light diffu­sion is not always isotropic; abdominal skin, for instance, shows a well-expressed 
scat­tering anisotropy. All facial measurements, however, show near-isotropic diffusion ker­nels, which 
justi.es an isotropic BSSRDF model for facial skin. We also analyzed spatial translucency variance measuring 
52 points in two subjects faces. As far as accessible by our sensor, all facialregions showeda very uniform 
translucency. Ultimately, we decided tomodelskin translucencytobe constantacrosseachfaceandreducethenumberofmea­surements 
per subject to three. Analyzing translucency variations across multiple subjects revealed a subtle difference 
between male and female subjects (females having a slightly Skin Skin Color Sun Exposure Subjects Type 
Reaction (M/F) I Very white Always burn II White Usually burn 8/6 III White to olive Sometimes burn 49 
/18 IV Brown Rarely burn 40 /8 V Dark brown Very rarely burn 13 /2 VI Black Never burn 4/1  Table 5.1: 
The Fitzpatrick skin type system and the number of Table 5.2: 10 face regions: subjects per skin type. 
(1) mustache, (2) forehead, (3) eyebrows, (4) eyelids, (5) eyelashes, (6) nose, (7) cheek, (8) lips, 
(9) chin, and (10) beard region. more translucent skin), while other traits did not correlate statistically 
signi.cantly with translucency. Spatial BRDFVariance Amore signi.cant variability, however, could be 
found in the surfacere.ectance. TherespectiveTorrance-Sparrow parameters .s and m vary signi.­cantly 
in dependence of the facial region. For each facial region, we perform principal component analysis (PCA) 
of these parameters, considering the BRDF .ts of all subjects in the database. It turns out that the 
parameters do not only vary between facial regions, but depending on the region, there is also a higher 
variability across subjects. Exem­plary observations are: the nose is quite specular, while the chin 
is rather non-specular; the BRDF variance on the forehead is extremely low and almost uniform across 
subjects, while re.ectance above the lip varies highly between subjects. This shows clearly that spatial 
BRDF variance is an important aspect of facial appearance. SkinTraitVariance In order to detect correlations 
betweenre.ectanceparameters and the traits associated with each subject, we perform canonical correlation 
analysis (CCA). It turns out that the surface BRDF parameters correlatethe most with skin type and gender. 
Less surprising, albedo is highly correlated with skin type. Apart from that, there is no signi.cant 
correlation of albedo with other traits. 5.6 AppearanceTransfer It is now possible to use the parameter 
observations within the face database to derive intuitive user controls to alter facial appearance. While 
the analysis performed in the pre­vious section can generally be used as a guideline when changing skin 
parameters, it is desirable to have higher-level controls. In the texture synthesis procedure by Heeger 
and Bergen [HB95] we founda powerful tool to transfer appearance parameters between sub­jects and to 
seamlessly blend between them [MZD05]. The texture synthesis is applicable to all model parameter types 
and can be used to add freckles, moles, gloss variations, and other individual effects. With the face 
database at hand, this provides a general appear­ance editing framework. Figure 5.6 shows examples where 
this method has been applied  to a face s diffuse re.ectance, where changes are most visible. Altering 
other model pa­rameters works analogously, although the effect appears to more subtle in renderings. 
 5.7 Conclusion Thispartoftheclasspresentedaprojectthat developedasimpleandpracticalskinmodel. An important 
featureof this modelis that all its parameters canberobustly estimated from measurements. This reduces 
the large amount of measured data to a manageable size, facilitates editing, and enables face appearance 
changes. Images from our model come close to reproducing photographs of real faces for arbitrary illumination 
and pose. We .t our model to data of a large and diverse group of people. The analysis of this data provides 
insight into the variance of face re.ectance parameters based on age, gender, or skin type. The database 
with all statistics is available to the research community for face synthesis and analysis [Mit]. In 
general, there are potential extensions to our model. For example, it would be inter­esting to measurewavelength-dependent 
absorption and scattering parameters. It would also be interesting to compare the results from the diffuse 
dipole approximation with a full Monte Carlo subsurface scattering simulation. Other important areas 
that require a different modeling approach arefacial hair (eyebrows, eyelashes, mustaches, and beards), 
hair, ears, eyes, and teeth. Very .ne facial hair also leads to asperity scattering and the important 
velvet look of skin near grazing angles [KP03]. Our model does not take this into account. We captured 
face re.ectance on static, neutral faces. Equally impor­tant are expressions and face performances. For 
example, it is well known that the blood .ow in skin changes based on facial expressions. Our setup has 
the advantage that such re.ectance changes could be captured in real-time.  List of Figures 1.1 Re.ectanceand 
subsurface scattering ...................... 1 1.2 Pointlight source emittinglightinadirection .................. 
3 1.3 Light emittedfroma surface,ina speci.cdirection. . . . . . . . . . . . . . . 4 1.4 Goniometricviewof 
slicesofaBRDF....................... 4 1.5 Anisotropicre.ection ................................ 6 
1.6 Synthesized images with and without subsurface scattering . . . . . . . . . 8 1.7 Taxonomy of scattering 
and re.ectance functions. . . . . . . . . . . . . . . . 9 3.1 Re.ectance measurementsofa dovegreeting 
card . . . . . . . . . . . . . . . 23 3.2 Inverse ShadeTree framework overview diagram . . . . . . . 
. . . . . . . . 26 4.1 Acquisitionsetupof Jensenetal. [JMLH01a] .................. 34 4.2 Acquisition 
setup of Goesele et al. [GLL+04a].................. 35 4.3 Dual Photography ................................. 
37 4.4 8Dre.ectance.eld ................................. 39 4.5 Symmetric Photography .............................. 
40 5.1 Physiologyofskin.................................. 42 5.2 Rawre.ectanceimages ............................... 
45 5.3 Overview overtheprocessing pipeline ...................... 46 5.4 Geometryre.nement work-.ow 
......................... 48 5.5 Comparisonofreal photographsto our model . . . . . . . . . . . . . 
. . . . 50 5.6 Appearanceediting ................................. 52  Bibliography [Ami02] Isaac Amidror. 
Scattered data interpolation methods for electronic imaging systems: a survey. Journal of Electronic 
Imaging, 11(2):157 176, April 2002. [BG01] S. Boivin and A. Gagalowicz. Image-based rendering of diffuse, 
specular and glossy surfaces from a single image. Proceedings of the 28th annual con­ference on Computer 
graphics and interactive techniques, pages 107 116, 2001. [BL03] George Borshukov and J. P. Lewis. Realistic 
human face rendering for "the matrix reloaded". In ACM SIGGRAPH 2003 Conference Abstracts and Appli­cations 
(Sketch), page 1, New York, NY, USA, 2003. ACM Press. [BNJ03] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet 
allocation. Journal of Machine Learning Research, 3:993 1022, January 2003. [BP01] Svetlana Barsky and 
Maria Petrou. Colour photometric stereo: Simultane­ous reconstruction of local gradient and colour of 
rough textured surfaces. In ICCV, pages 600 605, 2001. [BV99] Volker Blanz and Thomas Vetter. Amorphable 
model for the synthesis of 3d faces. In SIGGRAPH 99: Proceedings of the 26th annual conference on Computer 
graphics and interactive techniques, pages 187 194, New York, NY, USA, 1999. ACM Press/Addison-Wesley 
Publishing Co. [CCT] Camera calibration toolbox for matlab. http://www.vision.caltech.edu/ bouguetj/calib 
doc/. [CD02] Oana G. Cula and Kristin J. Dana. Image-based skin analysis. In Texture 2002, The Second 
International Workshop on Texture Analysis and Synthe­sis, pages 35 42, Copenhagen, Denmark, June 2002. 
[CJAMJ05] Petrik Clarberg, Wojciech Jarosz, Tomas Akenine-Möller, and Henrik Wann Jensen. Wavelet importance 
sampling: Ef.ciently evaluating products of complex functions. ACM Transactions on Graphics (Proc. SIGGRAPH 
2005), 24(3), 2005. [CLFS07] T. Chen, H.P.A. Lensch, C. Fuchs, and H.P. Seidel. Polarization and Phase-Shifting 
for 3D Scanning of Translucent Objects. Computer Vision and Pattern Recognition, 2007. CVPR 07. IEEE 
Conference on, pages 1 8, 2007. [Coo84] R. L. Cook. Shade trees. In Computer Graphics, volume 18 of SIGGRAPH 
84 Proceedings, pages 223 231, July 1984. [CT82] Robert L. Cook and Kenneth E. Torrance. Are.ection model 
for computer graphics. ACM Transactions On Graphics, 1(1):7 24, January 1982. 55 [Dan92] James L. Dannemiller. 
Spectral re.ectance of natural objects: how many basis functions are necessary? Journal of the Optical 
Society of America A, 9(4):507 515, April 1992. [DHT+00a] P. Debevec,T. Hawkins, C.Tchou, H.-P. Duiker,W. 
Sarokin, and M. Sagar. Acquiringthe Re.ectance FieldofaHuman Face.In Proc. ACM SIGGRAPH, pages 145 156, 
July 2000. ISBN 1-58113-208-5. [DHT+00b] Paul Debevec,Tim Hawkins, ChrisTchou, Haarm-Pieter Duiker,Westley 
Sarokin, and Mark Sagar. Acquiring the re.ectance .eld of a human face. In Computer Graphics, SIGGRAPH 
2000 Proceedings, pages 145 156, July 2000. [DJ05a] Craig Donner and HenrikWann Jensen. Light diffusion 
in multi-layered translucent materials. ACMTransactions on Graphics, 24(3):1032 1039, Au­gust 2005. [DJ05b] 
Craig Donner and HenrikWann Jensen. Light diffusion in multi-layered translucent materials. ACM Transactions 
on Graphics (Proc. SIGGRAPH 2005), 24(3):1032 1039, 2005. [DJ06] Craig Donnerand HenrikW. Jensen.Aspectralbssrdffor 
shadinghuman skin. In Proceedings of the 16th Eurographics Symposium on Rendering, pages 409 417. Eurographics 
Association, June 2006. [DM97] Paul Debevec and Jitendra Malik. Recovering high dynamic range radiance 
maps from photographs. In Computer Graphics, SIGGRAPH 97 Proceedings, pages 369 378, Los Angeles, CA, 
1997. [DvGNK97] Kristin J. Dana, Bram van Ginneken, Shree K. Nayar,and Jan J. Koenderink. Re.ectance 
and texture of real-world surfaces. In IEEE Conference on Com­puterVision and Pattern Recognition, pages 
151 157, 1997. [DvGNK99] Kristin J. Dana, Bram van Ginneken, Shree K. Nayar,and Jan J. Koenderink. Re.ectance 
and texture of real world surfaces. ACMTransactions on Graph­ics, 1(18):1 34, 1999. [DW04] K.J. Dana 
and J. Wang. Device for convenient measurement of spatially varying bidirectional re.ectance. Journal 
of the Optical Society of AmericaA, 21(1):1 12, 2004. [ECJ+06] Per Einarsson, Charles-Felix Chabert, 
Andrew Jones,Wan-ChunMa,Bruce Lamond,Tim Hawkins, Mark Bolas, Sebastian Sylwan, and Paul Debevec. Relighting 
human locomotion with .owed re.ectance .elds. In Rendering Techniques 2006: 17th Eurographics Workshop 
on Rendering, pages 183 194, June 2006. [FBLS07] Martin Fuchs,Volker Blanz, HendrikP.A. Lensch, and Hans-Peter 
Seidel. Adaptive sampling of re.ectance .elds. ACM Transactions on Graphics, 26(2), June 2007. Article 
10. [Fee04] Catherine Feeny. Servers, spydercams and Spider-Man 2 . In VFX Pro, http://www.uemedia.net/CPC/vfxpro/printer 
9050.shtml, July 2004. [FFP05] L. Fei-Fei andP. Perona.Abayesian hierarchical model for learning natural 
scene categories. In IEEE ComputerVision and Pattern Recognition, 2005. [Fit88] T. B. Fitzpatrick. The 
validity and practicality of sun-reactive skin typesI through VI. Arch. Dermatology, 124:869 871, 1988. 
[FL00] Brian V. Funt and Benjamin C. Lewis. Diagonal versus af.ne transfor­mations for color correction. 
Journal of the Optical Society of America A, 17(11):2108 2112, November 2000. [FLS05] Martin Fuchs, Hendrik 
P. A. Lensch, and Hans-Peter Seidel. Re.ectance from images: Amodel-based approach for human faces. IEEETransactions 
onVisualization andComputer Graphics, 11(3):296 305, 2005. Member-Volker Blanz. [GaHSSR04] Andrew Gelman, 
John B. Carlin ad Hal S. Stern, and Donald B. Rubin. Bayesian Data Analysis. Chapman and Hall, 2nd edition, 
2004. [GCHS03] Dan B. Goldman, Brian Curless, Aaron Hertzmann, and Steven M. Seitz. Shape and spatially-varying 
brdfs from photometric stereo. Technical Re­port 04-05-03, UniversityofWashington, 2003. [GCHS05a] Dan 
B Goldman, Brian Curless, Aaron Hertzmann, and Steven M. Seitz. Shape and spatially-varying BRDFs from 
photometric stereo. In IEEE Inter­national Conference on ComputerVision, 2005. [GCHS05b] D.B. Goldman, 
B. Curless, A. Hertzmann, and S.M. Seitz. Shape and spatially-varying brdfs from photometric stereo. 
IEEE International Confer­ence on ComputerVision, 2005. [Geo03] Athinodoros S. Georghiades. Recovering 
3-D shape and re.ectance from a small number of photographs. In Proceedings of the 14th Eurographics 
work­shop on Rendering, pages 230 240, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association. 
[GGSC96] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and MichaelF. Co­hen. The lumigraph. 
In Computer Graphics, SIGGRAPH 96 Proceedings, pages 43 54, New Orleans, LS, August 1996. [GLL+04a] Michael 
Goesele, HendrikP. A. Lensch, Jochen Lang, Christian Fuchs, and Hans-Peter Seidel. DISCO AcquisitionofTranslucent 
Objects. ACMTrans­actions on Graphics (Proceedings of SIGGRAPH 2004), 23(3), 2004. [GLL+04b] Michael 
Goesele, HendrikP. A. Lensch, Jochen Lang, Christian Fuchs, and Hans-Peter Seidel. DISCO Acquisition 
of translucent objects. ACMTrans­actions on Graphics (SIGGRAPH 2004), 24(3):835 844, 2004. [GMSW84] P. 
Gill,W. Murray, M. Saunders, and M.Wright. Procedures for optimiza­tion problems with a mixture of bounds 
and general linear constraints. In ACMTransactions on Mathematical Software, 1984. [GTLL06] Gaurav Garg, 
Eino-Ville Talvala, Marc Levoy, and Hendrik P. A. Lensch. Symmetric photography: Exploiting data-sparseness 
in re.ectance .elds. In Rendering Techniques 2006: 17th Eurographics Workshop on Rendering,pages 251 
262, June 2006. [GTR+06] Jinwei Gu, Chien-I Tu, Ravi Ramamoorthi, Peter Belhumeur, Wojciech Ma­tusik, 
and Shree Nayar. Time-varying surface appearance: Acquisition, modeling and rendering. ACM Transactions 
on Graphics (SIGGGRAPH 2006), 25(3), 2006. [Hac99] W. Hackbusch. A Sparse Matrix Arithmetic based on 
H-Matrices. Part I: Introduction to H-matrices. Computing, 62(2):89 108, 1999. [HB95] David J. Heeger 
and James R. Bergen. Pyramid-based texture analy­sis/synthesis. In Proceedings of SIGGRAPH 95,Computer 
Graphics Proceed­ings, Annual Conference Series, pages 229 238, August 1995. [Hea89] G. Healey. Using 
color for geometry-insensitive segmentation. J. Optical Society of America A, 6(6):920 937, 1989. [HED05] 
Tim Hawkins, Per Einarsson, and Paul Debevec. A dual light stage. In Rendering Techniques 2005: 16th 
Eurographics Workshop on Rendering, pages 91 98, June 2005. [Hér03] Christophe Héry. Face cloning at 
ILM. In SIGGRAPH 2003 Course Digital Face Cloning , 2003. [HK93] Pat Hanrahan and Wolfgang Krueger. Re.ection 
from layered surfaces due to subsurface scattering. In Computer Graphics, SIGGRAPH 93 Proceedings, pages 
165 174, Anaheim, CA, August 1993. [Hof99] Thomas Hofmann. Probabilistic latent semantic analysis. In 
Proceedings of Uncertainty in Arti.cial Intelligence, 1999. [HP03] Jefferson Y. Han and Ken Perlin. Measuring 
bidirectional texturere.ectance with akaleidoscope. ACM Transactions on Graphics,22(3):741 748, July 
2003. [HW79] J. A. Hartigan and M. A. Wong. Ak-means clustering algorithm. Applied Statistics, 28:100 
108, 1979. [HWT+04] Tim Hawkins, Andreas Wenger, Chris Tchou, Andrew Gardner, Fredrik Göransson, and 
Paul Debevec. Animatable facial re.ectance .elds. In Rendering Techniques 04 (Proceedings of the Second 
Eurographics Symposium on Rendering), pages 309 320, Norrköping, Sweden, June 2004. [INN05] Takanori 
Igarashi, Ko Nishino, and Shree K. Nayar. The appearance of hu­man skin. Technical report, Department 
of Computer Science, Columbia University CUCS-024-05, June 2005. [JMLH01a] Henrik Wann Jensen, Stephen 
R. Marschner, Marc Levoy, and Pat Hanra­han. Apractical model for subsurface light transport. In Proceedings 
of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Se­ries, pages 511 518, August 
2001. [JMLH01b] HenrikWann Jensen, StevenR. Marschner,MarcLevoy,andPat Hanrahan. Apractical model for 
subsurface light transport. InComputer Graphics, SIG-GRAPH 2001 Proceedings, pages 511 518, Los Angeles, 
CA, August 2001. [Jor99] Michael Jordan, editor. Learning in Graphical Models. MIT Press, 1999. [KP03] 
Jan Koenderink and Sylvia Pont. The secret of velvety skin. MachineVi­sion and Application, 14:260 268, 
2003. Special Issue on Human Modeling, Analysis, and Synthesis. [Law06] Jason Lawrence. Acquisition and 
Representation of Material Appearance for Editing and Rendering. PhD thesis, Department of Computer Science, 
Prince­ton University, 2006. [LBAD+06a] Jason Lawrence, Aner Ben-Artzi, Christopher DeCoro, Wojciech 
Matusik, Hanspeter P.ster, Ravi Ramamoorthi, and Szymon Rusinkiewicz. Inverse shade trees for non-parametric 
material representation and editing. ACM Transactions on Graphics (SIGGRAPH 2006), 25(3), 2006. [LBAD+06b] 
Jason Lawrence, Aner Ben-Artzi, Christopher DeCoro, Wojciech Matusik, Hanspeter P.ster, Ravi Ramamoorthi, 
and Szymon Rusinkiewicz. Inverse shade trees for non-parametric material representation and editing. 
ACM Transactions on Graphics (SIGGRAPH 2006), 25(3), 2006. [LBS90] H.C. Lee,E.J.Breneman, andC.P. Schulte. 
Modeling lightrelfectionfor computer color vision. IEEETrans. Pattern Analysis and Machine Intelligence, 
12(4):402 409, 1990. [LFTG97] EricP.F. Lafortune, Sing-Choong Foo, Kenneth E.Torrance, and DonaldP. Greenberg. 
Non-linear approximation of re.ectance functions. In SIG-GRAPH 97: Proceedings of the 24th annual conference 
on Computer graphics and interactive techniques, pages 117 126, NewYork, NY, USA, 1997. ACM Press/Addison-Wesley 
Publishing Co. [LFTW06] H. Li, S.C. Foo, K.E.Torrance, and S.H.Westin. Automated three-axis go­niore.ectometer 
for computer graphics applications. Optical Engineering, 45:043605, 2006. [LKG+01] HendrikP.A. Lensch, 
Jan Kautz, Michael Goesele,Wolfgang Heidrich, and Hans-Peter Seidel. Image-based reconstruction of spatially 
varying mate­rials. In Proceedings of the 12th Eurographics Workshop on Rendering, pages 104 115, June 
2001. [LKG+03] HendrikP.A. Lensch, Jan Kautz, Michael Goesele,Wolfgang Heidrich, and Hans-Peter Seidel. 
Image-based reconstruction of spatial appearance and geometric detail. ACMTransactions on Graphics, 22(2):234 
257, 2003. [LKK98] R.Lu,J. Koenderink,andA. Kappers. Opticalproperties (BidirectionalRe­.ectance Distribution 
Functions) of velvet. AppliedOptics,37(25):5974 5984, September 1998. [LRR04] Jason Lawrence, Szymon 
Rusinkiewicz, and Ravi Ramamoorthi. Ef.cient BRDF importance sampling using a factored representation. 
ACM Transac­tions on Graphics (SIGGRAPH 2004), 23(3):496 505, August 2004. [LS99] D. Lee and H. Seung. 
Learning the parts of objects by non-negative matrix factorization. Nature, 401:788 791, 1999. [MBK05] 
Gero Müller, GerhardH. Bendels, and ReinhardKlein. Rapid synchronous acquisition of geometry and appearance 
of cultural heritage artefacts. In VAST 2005: 6th International Symposium on Virtual Reality, Archaeology 
and Intelligent Cultural Heritage, pages 13 20, November 2005. [McA02a] David McAllister. AGeneralized 
Surface Appearance Representation for Com­puter Graphics. Ph.d. thesis, University of North Carolina 
(UNC), Chapel Hill, NC, 2002. [McA02b] David K. McAllister. AGeneralized Surface Appearance Representation 
for Com­puter Graphics. PhD thesis, University of North Carolina at Chapel Hill, NC, 2002. [Mer84] S. 
Mersch. Polarized lighting for machine vision applications. In Proc. RI/SME Third Annual Applied Machine 
Vision Conf., pages 40 54. Schaum­burg, IL, Feb. 1984. [Mit] Mitsubishi Electric Research Laboratories 
The MERL/ETH skin re.ectance database. facescanning/. (MERL), ETH Zurich. http://www.merl.com/ [MLH02] 
D.K. McAllister, A. Lastra, and W. Heidrich. Ef.cient rendering of spa­tial bi-directional re.ectance 
distribution functions. Proceedings ofthe ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware, 
pages 79 88, 2002. [MMS+05] G. Müller, J. Meseth, M. Sattler, R. Sarlette, and R. Klein. Acquisition, 
syn­thesis and rendering of bidirectional texture functions. Computer Graphics Forum, 24(1), 2005. [MN99] 
T. Mitsunaga and S.K. Nayar. Radiometric Self Calibration. In IEEE Con­ference on Computer Vision and 
Pattern Recognition (CVPR), volume 1, pages 374 380, Jun 1999. [MPBM03a] W. Matusik, H. P.ster, M. Brand, 
and L. McMillan. Ef.cient isotropic BRDF measurement. Proceedings of the 14th Eurographics workshop on 
Rendering, pages 241 247, 2003. [MPBM03b] Wojciech Matusik, Hanspeter P.ster, Matthew Brand, and LeonardMcMil­lan. 
Adata-driven re.ectance model. ACM Transactions on Graphics (SIG-GRAPH 2003), 22(3):759 770, July 2003. 
[MPDW03] Vincent Masselus, Pieter Peers, Philip Dutré, and Yves D. Willems. Relight­ing with 4d incident 
light .elds. ACM Transactions on Graphics, 22(3):613 620, July 2003.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401235</section_id>
		<sort_key>1030</sort_key>
		<section_seq_no>23</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Projectors for graphics]]></section_title>
		<section_page_from>23</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098854</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098855</person_id>
				<author_profile_id><![CDATA[81100150661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Aditi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Majumder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098856</person_id>
				<author_profile_id><![CDATA[81100504611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[P. A.]]></middle_name>
				<last_name><![CDATA[Lensch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098857</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401236</article_id>
		<sort_key>1040</sort_key>
		<display_label>Article No.</display_label>
		<pages>4</pages>
		<display_no>81</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Projectors for graphics]]></title>
		<page_from>1</page_from>
		<page_to>4</page_to>
		<doi_number>10.1145/1401132.1401236</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401236</url>
		<abstract>
			<par><![CDATA[<p>Modern digital projectors are a central part of large-format displays, non-intrusive augmented reality systems and computational illumination for 3D image-based modeling. High speed and high framerate projectors also support intriguing applications in optical communication. With a pocketsize form factor, projectors will be widely used for mobile applications. We survey this rapidly evolving landscape and the growing interest in experimenting with projectors. A novel class of applications is emerging, involving illumination+capture of complex 3D shapes as well as dynamic interaction via projection on movable surfaces. We provide a detailed survey of the several approaches for combining real-time computer graphics and computer vision methods for single and multi-projector systems. We cover topics in immersive rendering, projective geometry, reflectance field capture and spatial augmented reality. In addition, we will give practical insights, implementation details and case-studies for a variety of applications in research, art and industry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098858</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University, Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098859</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098860</person_id>
				<author_profile_id><![CDATA[81100150661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Aditi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Majumder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Irvine]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098861</person_id>
				<author_profile_id><![CDATA[81100504611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[P. A.]]></middle_name>
				<last_name><![CDATA[Lensch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Projectors for Graphics SIGGRAPH 2008 Course Ramesh Raskar, MIT Media Lab Aditi Majumder, University 
of California, Irvine Hendrik P. A. Lensch, MPI Informatik Oliver Bimber, Bauhaus-University Weimar 
 Course Abstract Modern digital projectors are a central part of large-format displays, non-intrusive 
augmented reality systems and computational illumination for 3D image-based modeling. High speed and 
high framerate projectors also support intriguing applications in optical communication. With a pocketsize 
form factor, projectors will be widely used for mobile applications. We survey this rapidly evolving 
landscape and the growing interest in experimenting with projectors. A novel class of applications is 
emerging, involving illumination+capture of complex 3D shapes as well as dynamic interaction via projection 
on movable surfaces. We provide a detailed survey of the several approaches for combining real-time computer 
graphics and computer vision methods for single and multi-projector systems. We cover topics in immersive 
rendering, projective geometry, reflectance field capture and spatial augmented reality. In addition, 
we will give practical insights, implementation details and case-studies for a variety of applications 
in research, art and industry. Prerequisites This course is appropriate for beginners in digital art 
and media. No programming or specific mathematical background is required. General knowledge of basic 
computer graphics techniques and 3D tools is helpful but not necessary. Course Syllabus Course Introduction 
and Overview (Bimber and Raskar, 10 minutes) Goals, Outline, Speakers, Schedule, Opportunities A. Large 
Format Displays (Majumder, 40 minutes) From CAVEs to large visualization centers, single and multi-projector 
displays are becoming easier to use due to novel camera-based maintenance systems. Overview and New 
opportunities  Planar, cylindrical, spherical and non-planar displays  Geometric and color calibration 
 Rendering strategies  Distributed self-calibrating displays  B. Visually Augmenting the real World 
with Projectors (Bimber, 50 minutes) Augmenting real world surfaces with projected images can be challenging. 
This module describes the fundamental concepts. Image correction techniques for geometric and radiometric 
(colored and textured) complex surfaces  Human vision adapted techniques  Global illumination compensation 
(scattering, inter-reflections, caustics)  Human vision adapted techniques  Defocus compensation (with 
multiple and single projectors)  Imperceptible coded projection  Superresolution and high-dynamic range 
projection  Applications (theatres, museums, historic sites, advertisement, games, television studios, 
movie sets, on-site visualization, etc.)  Questions and Answers (All, 5 minutes) Break (15 minutes) 
C. Mobile Projectors and Optical Communication (Raskar, 45 minutes) Pocket projectors allow novel human-computer 
interaction opportunities. Spatio-temporal modulation of light creates high speed optical communication 
which can be used in many tracking applications. Portable projectors, technology and issues  Single-handed 
interaction, Image stabilization and resizing  iLamps: Geometrically aware pocket projectors   Optical 
and Radio Frequency Tags  RFID for Augmented Reality: Location sensing RFID and automatic authoring 
 Optical communication for space labeling in robotics, games  Imperceptible projection for high speed 
motion capture   D. Computational Illumination for 3D Scene Modeling (Lensch, 45 minutes) The appearance 
of real world objects can be represented via a high-dimensional reflectance fields. Projectors are highly 
suitable for computational illumination to model such objects for rendering and computer vision applications. 
 Scene appearance as higher dimensional reflectance fields  Pattern projection for 3D geometry acquisition 
 Measuring appearance parameters  Capturing (and removing) global versus local illumination effects 
 Questions and Answers (All, 15 minutes)  Course Presenters Ramesh Raskar Associate Professor Media 
Lab, MIT 20 Ames Street, Cambridge, MA 02139 T +1.617.621.7533 F +1.617.621.7550 Email: raskar@merl.com, 
http://raskar.info Ramesh Raskar will join MIT full time in Spring 2008 and is currently a Senior Research 
Scientist at MERL. His research interests include projector-based graphics, projective geometry and non-photorealistic 
rendering. During his doctoral research at U. of North Carolina at Chapel Hill, he developed a framework 
for projector based 3D graphics, which can simplify the constraints on conventional immersive displays, 
and enable new projector-assisted applications. He has published several articles on immersive projector-based 
displays, spatially augmented reality and has introduced Shader Lamps, a new approach for projector-based 
augmentation. His technical papers have appeared in SIGGRAPH, EuroGraphics, IEEE VR, IEEE Visualization, 
CVPR and many other graphics and vision conferences. He was a course organizer and speaker for Siggraph 
2002 through 2007. He is a member of the ACM and IEEE. Aditi Majumder Assistant Professor University 
of California, Irvine Department of Computer Science 3029 Bren Hall, Irvine, CA 92697-3435 T +1 (949) 
824-8877 F +1 (949) 824-4056 E-Mail: majumder@ics.uci.edu, Web: http://www.ics.uci.edu/~majumder/ Aditi 
Majumder is an assistant professor at the Department of Computer Science in University of California, 
Irvine. She received her BE in Computer Science and Engineering from Jadavpur University, Calcutta, India 
in 1996 and PhD from Department of Computer Science, University of North Carolina at Chapel Hill in 2003. 
Her research is in the general area of computer graphics, vision and visualization. Her primary research 
contribution has been on using computer vision methodologies for easy and automatic assembly of commodity 
projector(s) and camera(s) to build large tiled multi-projector displays, exploiting human perception 
limitations for making them perceptually and functionally seamless. and devising human computer interaction 
modalities for such displays.She is the author of the recent book "Practical Multi-Projector Display 
Design" released by A.K.Peters in 2007. Hendrik P. A. Lensch MPI Informatik Campus E 1.4 66123 Saarbruecken 
Germany T +49 681 9325 428 F +49 681 9325 499 E-Mail: lensch@mpi-inf.mpg.de Web: http://www.mpi-inf.mpg.de/~lensch 
Hendrik P. A. Lensch is the head of an independent research group "General Appearance Acquisition and 
Computational Photography" at the MPI Informatik in Saarbrucken, Germany. The group is part of the Max 
Planck Center for Visual Computing and Communication. He received his diploma in computers science from 
the University of Erlangen in 1999, and after joining the computer graphics group at MPI he received 
his PhD from Saarland University in 2003. Dr. Lensch spent two years (2004-2006) as a visiting assistant 
professor at Stanford University, USA. His research interests include 3D appearance acquisition, image-based 
rendering and computational photography. For his work on reflectance measurement he received the Eurographics 
Young Researcher Award 2005. He was awarded an Emmy Noether Fellowship by the German Research Foundation 
in 2007. He has given several lectures and tutorials at various conferences including SIGGRAPH courses 
on realistic materials in 2002 and 2005. Oliver Bimber Associate Professor Media Faculty Bauhaus University, 
Bauhausstr. 11, 99423 Weimar, Germany T +49-3643-583724 F +49-3643-583709 Email: bimber@uni-weimar.de 
http://www.uni-weimar.de/medien/AR Oliver Bimber is a Junior Professor (associate level) for Augmented 
Reality at the Bauhaus University Weimar, Germany. He received a Ph.D. (2002) in Engineering at the Darmstadt 
University of Technology, Germany and a Habilitation degree (2007) in Computer Science (Informatik) at 
the Munich University of Technology. He is co-author of the book "Spatial Augmented Reality" and serves 
on the editorial board of the IEEE Computer magazine (graphics and multimedia editor). Bimber taught 
courses on Spatial Augmented Reality and projector-based display techniques at Eurographics 2003 and 
Eurographics 2004, ICAT 2005, ETD 2006, as well as at Siggraph 2005, Siggraph 2006 and Siggraph 2007. 
His research interests include real-time rendering, computer vision and human visual perception in the 
context of next-generation display technologies.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401237</article_id>
		<sort_key>1050</sort_key>
		<display_label>Article No.</display_label>
		<pages>103</pages>
		<display_no>82</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Large format displays]]></title>
		<page_from>1</page_from>
		<page_to>103</page_to>
		<doi_number>10.1145/1401132.1401237</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401237</url>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image displays</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098862</person_id>
				<author_profile_id><![CDATA[81100150661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aditi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Majumder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Irvine]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
             Method Cj can be related to D by a concatenation of camera homographies HCj 
.D = HR .D x HCk.R x .. HCj .Ck More than one path from Cj to R Minimum spanning homography tree  Hence, 
unique path  Method Projector Pi can be related to Cj HPi.Cj Hence, Pi can be related to D by concatenation 
of homographies HPi .D = HCj.D x HPi .Cj Errors can accumulate along a path of tree Global error diffusion 
 Non-Linear Method for Planar Display Projectors can have non-linearities  Rear projection systems 
 H is non-linear  Issues  Not easily invertible  Cannot be concatenated  Cannot scale to multiple 
cameras  Non-Linear Method for Planar Display H is a cubic polynomial Linear regression for polynomial 
fitting Issues Not perspective projection invariant  Assumes near rectangular array        
      Structured Light Pattern to be projected Use multiple frames 0010 0011 0100 Binary encode 
blob 0001 index  Each bit represents a frame  Each blob with the bit on is present in that frame  
 Structured Light Frame 1 (LSB) Use multiple frames  Binary encode blob 0001 0011 index  Each bit 
represents a frame  Each blob with the bit on is present in that frame   Structured Light Frame 2 
(2nd LSB) Use multiple frames 0010 0011 Binary encode blob index  Each bit represents a frame  Each 
blob with the bit on is present in that frame   Structured Light Frame 3 (3rd LSB) Use multiple frames 
 Binary encode blob 0100 index  Each bit represents a frame  Each blob with the bit on is present 
in that frame   Structured Light Frame 4 (MSB) Use multiple frames Binary encode blob index Each bit 
represents a frame 1001 1010 1011 1100 Each blob with the bit on is present in that frame  Non-planar 
Displays References: M. Harville, B. Culbertson, I. Sobel, D. Gelb, A. Futzhugh, D. Tanguay, Practical 
Methods for Geometric and Photometric Correction of Tiled Projector Displays on Curved Screens, IEEE 
CVPR Workshop on Projector-Camera Systems, 2006.  R. Raskar, J. VanBaar, T. Willwacher, S. Rao, Quadric 
Image Transfer for Immersive Curved Screen Displays, Eurographics 2004.  R. Yang, A. Majumder, M. S. 
Brown, Camera-Based Calibration Techniques for Seamless Multi-Projector Displays, IEEE Transactions on 
Visualization and Computer Graphics 11(2), Mar-Apr 2005  R. Raskar and G. Welch and M. Cutts and A. 
Lake, L. Stesin, H. Fuchs, The Office of the Future: A Unified Approach to Image Based Modeling and Spatially 
Immersive Display, ACM Siggraph, 1998.  R. Raskar, M.S. Brown, R. Yang, W. Chen, H. Towles, B. Seales, 
H. Fuchs, Multi Projector Displays Using Camera Based Registration, IEEE Visualization, 1999.    
          Common Bulb Using common bulb for all projectors  Limitations  Bulb is not the 
only cause of color variation  Not scalable  Expensive ($100,000 for 3x3 display)  Labor intensive 
 Can be used in abutting projector configuration only  Addresses only inter projector variation Common 
Bulb References: B. Pailthorpe, N. Bordes, W. Bleha, S. Reinsch, and J. Moreland, High-resolution display 
with uniform illumination, Proceedings Asia Display IDW, 1295-1298, 2001.     Optical Edge Blending 
Engineering feat Movements during insertion of the spatial filter Compensated by intercepting the analog 
signals from the computer to the projector  Details are not available  Edge Blending Scalable  Can 
be used in overlapping configuration only  Addresses only overlap variation Not enough control (Aperture 
and Optical)  No black attenuation (Software)  Assumes linearity of projector response  Can get rid 
of seams entirely if projectors are adjusted to be very similar   Gamut Matching Use a photometer 
to capture the color gamut One measurement per projector Find the common color gamut that all theprojectors 
can reproduce  Use linear transformations to achieve the matching   Gamut Matching Can be used in 
abutting configuration only Addresses only inter projector variation Not scalable to 40-50 projectors 
Due to algorithmic complexity  Seamless Displays Uniformity is not required  Addressing just the 
luminance variation can take us a long way  Spatial variation in chrominance is negligible  Humans 
are more sensitive to luminance variation than to chrominance variation   Desiderata Comprehensive 
and general framework Addresses intra, inter and overlap variations  Design general solutions  No 
special cases  Automated  Scalable  Explain and compare existing methods            Algorithm 
 Images of multiple projector inputs at multiple camera exposures  Setup system of equations and solve 
using least-squares  Zout = i + ln (t ln fc-1( ) ln fp()+ ln L(x,y) ln (tj )  Efficiency Issue Imagine 
a system with: 1024x768 pixel locations  32 colors  6 exposures  # of unknowns = 32 + 32 + (1024x768) 
 786,000 unknowns  # of equations = 1024x768x32x6  150,000,000 equations             
 Attenuation Map Display Attenuation Map (15 projector display) Projector Attenuation Map Per Projector 
Image Correction Channel Linearization Smoothing Maps Function X = -1 hl + Offset map Inverse of each 
projector s transfer Hl function Common Transfer Function f1    Summary Method Addresses.. Controls 
Manipulation Inter Luminance and Chrominance Common Bulb Inter Luminance and Chrominance Edge Blending 
Overlap Luminance Only Gamut matching Inter Luminance and Chrominance PRISM Intra + Inter + Overlap Luminance 
Only   Traditional Rendering Architecture Multi-pipe single machine Each pipe driving one projector 
Advantages Shared Memory  Shared Disk Space   PC Cluster Based Rendering Graphics card has similar 
performance  PC can drive only a single display  One PC per projector Problem Multiple PCs for the 
entire display  Content needs to be distributed      Can be found automatically camera Camera 
computes the mapping of each projector in the logical framebuffer Camera Image Logical Framebuffer Generate 
the Configuration file PC pipe_server (App) pipe_server (App) OpenGL lib OpenGL lib Graphics hardware 
Graphics hardware 1 2 PC pipe_server (App) pipe_server (App) OpenGL lib OpenGL lib Graphics hardware 
Graphics hardware 3 4 PC pipe_server (App) pipe_server (App) OpenGL lib OpenGL lib Graphics hardware 
Graphics hardware Camera ImageLogical Framebuffer PC pipe_server (App) pipe_server (App) OpenGL lib OpenGL 
lib Graphics hardware Graphics hardware COMPUTER GENERATED CONFIG FILE PC -Cluster 1 SERVER &#38; POSITION 
IN LOGICAL DISPLAY Server1.cs.ust.hk PC AppApp OpenGL libOpenGL lib Graphics hardwareGraphics hardware 
WIREGL 2 3 Position 0 0 Server2.cs.ust.hk Position 1000 0 size (1024 768) size (1024 768) Server3.cs.ust.hk 
4 Position 0 650 Server4.cs.ust.hk size (1024 768) Position 900 700 size (1024 768)     Limitations 
of Centralized Approach Educated User Difficult to deploy Not easy to add/remove projectors Not scalable 
(Limited by camera resolution) Not easy to rearrange projectors Not reconfigurable Not easy to tolerate 
faults Imagine A display that can calibrate itself with no user intervention  Can detect addition/removal 
and recalibrate itself  Can detect faults and function at a limited capability    Distributed Architecture 
 Data is pulled by each PPP  Data server does not know that these are displays  Acts like any other 
data client Each PPP manages its own pixels Asynchronous Distributed Calibration Each PPP runs asynchronous 
SPMD algorithm Each PPP discovers its neighbors  PPPs discovers the array configuration  Using camera-based-communication 
Self-calibrates accordingly Scalable  Reconfigurable  Fault-Tolerant     After Neighbor Discovery 
Each PPP knows The number of neighbors it has  Their location relative to self (top, bottom, etc.) 
 But does not know Total number of projectors  Projection array configuration  Its own coordinates 
in the array   Enabling Low Bandwidth Network Communication  Only camera-based communication till 
now  PPPs need to know the IP addresses of its neighbors  Each PPP broadcasts its IP address and coordinates 
  After Configuration Identification Each PPP knows Total size of display  The part of the display 
it is responsible for  IP address of neighbors  But does not know The relative orientation of its neighbor 
to warp the image to make a seamless display  Geometric Alignment. . .0.90 0.11 1024. 0.23 0.80 768 
.. . 0.13 0.21 1.00 Local Geometric Calibration . .. Find correspondences Hungarian Method Some Text 
No Pattern Sequences Find local homographies Global Geometric Calibration Distributed Homography Tree 
Distributed Homography Tree ROOT   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401238</article_id>
		<sort_key>1060</sort_key>
		<display_label>Article No.</display_label>
		<pages>76</pages>
		<display_no>83</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Visually augmenting the real world with projectors]]></title>
		<page_from>1</page_from>
		<page_to>76</page_to>
		<doi_number>10.1145/1401132.1401238</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401238</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098863</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University, Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
           O. Bimber Visually Augmenting the real World with Projectors 17/05/2008    
        O. Bimber Visually Augmenting the real World with Projectors 17/05/2008       
          O. Bimber Visually Augmenting the real World with Projectors 17/05/2008 O. Bimber 
Visually Augmenting the real World with Projectors 17/05/2008         O. Bimber Visually Augmenting 
the real World with Projectors 17/05/2008 O. Bimber Visually Augmenting the real World with Projectors 
17/05/2008 O. Bimber Visually Augmenting the real World with Projectors 17/05/2008  O. Bimber Visually 
Augmenting the real World with Projectors 17/05/2008  O. Bimber Visually Augmenting the real World 
with Projectors 17/05/2008 O. Bimber Visually Augmenting the real World with Projectors 17/05/2008 
        O. Bimber Visually Augmenting the real World with Projectors 17/05/2008   O. Bimber 
Visually Augmenting the real World with Projectors 17/05/2008 O. Bimber Visually Augmenting the real 
World with Projectors 17/05/2008               O. Bimber Visually Augmenting the real World 
with Projectors 17/05/2008
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401239</article_id>
		<sort_key>1070</sort_key>
		<display_label>Article No.</display_label>
		<pages>25</pages>
		<display_no>84</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[The visual computing of projector-camera systems]]></title>
		<page_from>1</page_from>
		<page_to>25</page_to>
		<doi_number>10.1145/1401132.1401239</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401239</url>
		<abstract>
			<par><![CDATA[<p>This article focuses on real-time image correction techniques that enable projector-camera systems to display images onto screens that are not optimized for projections, such as geometrically complex, colored and textured surfaces. It reviews hardware accelerated methods like pixel-precise geometric warping, radiometric compensation, multi-focal projection, and the correction of general light modulation effects. Online and offline calibration as well as invisible coding methods are explained. Novel attempts in super-resolution, high dynamic range and high-speed projection are discussed. These techniques open a variety of new applications for projection displays. Some of them will also be presented in this report.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[GPU rendering]]></kw>
			<kw><![CDATA[image-correction]]></kw>
			<kw><![CDATA[projector-camera systems]]></kw>
			<kw><![CDATA[virtual and augmented reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098864</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098865</person_id>
				<author_profile_id><![CDATA[81313481091]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar, Germany and Osaka University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098866</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098867</person_id>
				<author_profile_id><![CDATA[81100631719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Anselm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grundh&#246;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{AA01} Aggarwal M., Ahuja N.: Split Aperture Imaging for High Dynamic Range. In <i>Proc. of IEEE International Conference on Computer Vision (ICCV)</i> (2001), vol. 2, pp. 10--17.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153708</ref_obj_id>
				<ref_obj_pid>1153172</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{AOSS06} Ashdown M., Okabe T., Sato I., Sato Y.: Robust Content-Dependent Photometric Projector Compensation. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2006)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{ASOS07} Ashdown M., Sato I., Okabe T., Sato Y.: Perceptual Photometric Compensation for Projected Images. <i>IEICE Transaction on Information and Systems J90-D</i>, 8 (2007), 2115--2125. in Japanese.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{AU05} Allen W., Ulichney R.: Wobulation: Doubling the Addressed Resolution of Projection Displays. In <i>Proc. of SID Symposium Digest of Technical Papers</i> (2005), vol. 36, pp. 1514--1517.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1042326</ref_obj_id>
				<ref_obj_pid>1042196</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{BCK*05} Bimber O., Coriand F., Kleppe A., Bruns E., Zollmann S., Langlotz T.: Superimposing Pictorial Artwork with Projected Imagery. <i>IEEE MultiMedia 12</i>, 1 (2005), 16--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{BDD*04} Biehling W., Deter C., Dube S., Hill B., Helling S., Isakovic K., Klose S., Schiewe M.: LaserCave - Some Building Blocks for Immersive Screens -. In <i>Proc. of International Status Conference Virtual and Augmented Reality</i> (2004).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1137518</ref_obj_id>
				<ref_obj_pid>1137246</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{BE06} Bimber O., Emmerling A.: Multifocal Projection: A Multiprojector Technique for Increasing Focal Depth. <i>IEEE Transactions on Visualization and Computer Graphics (TVCG) 12</i>, 4 (2006), 658--667.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1042236</ref_obj_id>
				<ref_obj_pid>1042191</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{BEK05} Bimber O., Emmerling A., Klemmer T.: Embedded Entertainment with Smart Projectors. <i>IEEE Computer 38</i>, 1 (2005), 56--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1130430</ref_obj_id>
				<ref_obj_pid>1130237</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{BGZ*06} Bimber O., Grundh&#246;fer A., Zeidler T., Danch D., Kapakos P.: Compensating Indirect Scattering for Immersive and Semi-Immersive Projection Displays. In <i>Proc. of IEEE Virtual Reality (IEEE VR)</i> (2006), pp. 151--158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{Bim06} Bimber O.: Projector-Based Augmentation. In <i>Emerging Technologies of Augmented Reality: Interfaces and Design</i>, Haller M., Billinghurst M., Thomas B., (Eds.). Idea Group, 2006, pp. 64--89.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1313169</ref_obj_id>
				<ref_obj_pid>1313046</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{BJM07} Bhasker E. S., Juang R., Majumder A.: Registration techniques for using imperfect and par tially calibrated devices in planar multi-projector displays. <i>IEEE Trans. Vis. Comput. Graph. 13</i>, 6 (2007), 1368--1375.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{BM07} Bhasker E., Majumder A.: Geometric Modeling and Calibration of Planar Multi-Projector Displays using Rational Bezier Patches. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{BMS98} Batlle J., Mouaddib E. M., Salvi J.: Recent progress in coded structured light as a technique to solve the correspondence problem: a survey. <i>Pattern Recognition 31</i>, 7 (1998), 963--982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1042393</ref_obj_id>
				<ref_obj_pid>1042201</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{BMY05} Brown M., Majumder A., Yang R.: Camera Based Calibration Techniques for Seamless Multi-Projector Displays. <i>IEEE Transactions on Visualization and Computer Graphics (TVCG) 11</i>, 2 (2005), 193--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153682</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{BSC06} Brown M. S., Song P., Cham T.-J.: Image Pre-Conditioning for Out-of-Focus Projector Blur. In <i>Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i> (2006), vol. II, pp. 1956--1963.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1105179</ref_obj_id>
				<ref_obj_pid>1104996</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{BWEN05} Bimber O., Wetzstein G., Emmerling A., Nitschke C.: Enabling View-Dependent Stereoscopic Projection in Real Environments. In <i>Proc. of IEEE/ACM International Symposium on Mixed and Augmented Reality (ISMAR)</i> (2005), pp. 14--23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>279274</ref_obj_id>
				<ref_obj_pid>279270</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{CKS98} Caspi D., Kiryati N., Shamir J.: Range Imaging With Adaptive Color Structured Light. <i>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 20</i>, 5 (1998), 470--480.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1033707</ref_obj_id>
				<ref_obj_pid>1032652</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{CNGF04} Cotting D., N&#228;f M., Gross M. H., Fuchs H.: Embedding Imperceptible Patterns into Projected Images for Simultaneous Acquisition and Display. In <i>Proc. of IEEE/ACM International Symposium on Mixed and Augmented Reality (ISMAR)</i> (2004), pp. 100--109.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{CZGF05} Cotting D., Ziegler R., Gross M. H., Fuchs H.: Adaptive Instant Displays: Continuously Calibrated Projections using Per-Pixel Light Control. In <i>Proc. of Eurographics</i> (2005), pp. 705--714.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{DDS03} Dudley D., Duncan W. M., Slaughter J.: Emerging Digital Micromirror Device (DMD) Applications. In <i>Proc. of SPIE</i> (2003), vol. 4985, pp. 14--25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{DM97} Debevec P. E., Malik J.: Recovering High Dynamic Range Radiance Maps from Photographs. In <i>Proc. of ACM SIGGRAPH</i> (1997), pp. 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{DRW*06} Debevec P., Reinhard E., Ward G., Myszkowski K., Seetzen H., Zargarpour H., McTaggart G., Hess D.: High Dynamic Range Imaging: Theory and Applications. In <i>Proc. of ACM SIGGRAPH (Courses)</i> (2006).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{DSW*07} Damberg G., Seetzen H., Ward G., Heidrich W., Whitehead L.: High-Dynamic-Range Projection Systems. In <i>Proc. of SID Symposium Digest of Technical Papers</i> (2007), vol. 38, pp. 4--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{DVC07} Damera-Venkata N., Chang N. L.: Realizing Super-Resolution with Superimposed Projection. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1033700</ref_obj_id>
				<ref_obj_pid>1032652</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{EHH04} Ehnes J., Hirota K., Hirose M.: Projected Augmentation - Augmented Reality using Rotatable Video Projectors. In <i>Proc. of IEEE/ACM International Symposium on Mixed and Augmented Reality (ISMAR)</i> (2004), pp. 26--35.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1068998</ref_obj_id>
				<ref_obj_pid>1068507</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{FGN05} Fujii K., Grossberg M., Nayar S.: A Projector-Camera System with Real-Time Photometric Adaptation for Dynamic Environments. In <i>Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i> (2005), vol. I, pp. 814--821.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1166290</ref_obj_id>
				<ref_obj_pid>1166253</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{FR06} Flagg M., Rehg J. M.: Projector-Guided Painting. In <i>Proc. of ACM Symposium on User Interface Software and Technology (UIST)</i> (2006), pp. 235--244.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{GB07} Grundh&#246;fer A., Bimber O.: Real-Time Adaptive Radiometric Compensation. <i>To appear in IEEE Transactions on Visualization and Computer Graphics (TVCG)</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{GB08} Grosse M., Bimber O.: Coded aperture projection, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{GPNB04} Grossberg M., Peri H., Nayar S., Belhumeur P.: Making One Object Look Like Another: Controlling Appearance using a Projector-Camera System. In <i>Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i> (Jun 2004), vol. I, pp. 452--459.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1514356</ref_obj_id>
				<ref_obj_pid>1514339</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{GSHB07} Grundh&#246;er A., Seeger M., H&#228;ntsch F., Bimber O.: Dynamic Adaptation of Projected Imperceptible Codes. <i>Proc. of IEEE International Symposium on Mixed and Augmented Reality</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{HSM07} Habe H., Saeki N., Matsuyama T.: Inter-Reflection Compensation for Immersive Projection Display. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams) (poster)</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{JF07} Johnson T., Fuchs H.: Real-Time Projector Tracking on Complex Geometry using Ordinary Imagery. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{JGB*06} Jones A., Gardner A., Bolas M., Mcdowall I., Debevec P.: Simulating Spatially Varying Lighting on a Live Performance. In <i>Proc. of European Conference on Visual Media Production (CVMP)</i> (2006), pp. 127--133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{JM07} Juang R., Majumder A.: Photometric Self-Calibration of a Projector-Camera System. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276427</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{JMY*07} Jones A., McDowall I., Yamada H., Bolas M., Debevec P.: Rendering for an Interactive 360&#176; Light Field Display. In <i>Proc. of ACM SIGGRAPH</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{JR03} Jaynes C., Ramakrishnan D.: Super-Resolution Composition in Multi-Projector Displays. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2003).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>601698</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{JWS*01} Jaynes C., Webb S., Steele R., Brown M., Seales W.: Dynamic Shadow Removal from Front Projection Displays. In <i>Proc. of IEEE Visualization</i> (2001), pp. 175--555.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>977357</ref_obj_id>
				<ref_obj_pid>977247</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{JWS04} Jaynes C., Webb S., Steele R. M.: Camera-Based Detection and Removal of Shadows from Interactive Multiprojector Displays. <i>IEEE Transactions on Visualization and Computer Graphics (TVCG) 10</i>, 3 (2004), 290--301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{KKN*06} Kusakabe Y., Kanazawa M., Nojiri Y., Furuya M., Yoshimura M.: YC-separation Type Projector with Double Modulation. In <i>Proc. of International Display Workshop (IDW)</i> (2006), pp. 1959--1962.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{KN06} Kitamura M., Naemura T.: A Study on Position-Dependent Visible Light Communication using DMD for ProCam. In <i>IPSJ SIG Notes. CVIM-156</i> (2006), pp. 17--24. in Japanese.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{MB05} McDowall I. E., Bolas M.: Fast Light for Display, Sensing and Control Applications. In <i>Proc. of IEEE VR 2005 Workshop on Emerging Display Technologies (EDT)</i> (2005), pp. 35--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186180</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{MBHF04} McDowall I. E., Bolas M. T., Hoberman P., Fisher S. S.: Snared Illumination. In <i>Proc. of ACM SIGGRAPH (Emerging Technologies)</i> (2004), p. 24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1180549</ref_obj_id>
				<ref_obj_pid>1180495</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{MKO06} Mukaigawa Y., Kakinuma T., Ohta Y.: Analytical Compensation of Inter-reflection for Pattern Projection. In <i>Proc. of ACM Symposium on Virtual Reality Software and Technology (VRST) (short paper)</i> (2006), pp. 265--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383764</ref_obj_id>
				<ref_obj_pid>2383737</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{MW01} Majumder A., Welch G.: COMPUTER GRAPHICS OPTIQUE: Optical Superposition of Projected Computer Graphics. In <i>Proc. of Immersive Projection Technology - Eurographics Workshop on Virtual Environment (IPT-EGVE)</i> (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946766</ref_obj_id>
				<ref_obj_pid>946247</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{NB03} Nayar S. K., Branzoi V.: Adaptive Dynamic Range Imaging: Optical Control of Pixel Exposures over Space and Time. In <i>Proc. of IEEE International Conference on Computer Vision (ICCV)</i> (2003), vol. 2, pp. 1168--1175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{NBB04} Nayar S. K., Branzoi V., Boult T. E.: Programmable Imaging using a Digital Micromirror Array. In <i>Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i> (2004), vol. I, pp. 436--443.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141977</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{NKGR06} Nayar S., Krishnan G., Grossberg M. D., Raskar R.: Fast Separation of Direct and Global Components of a Scene using High Frequency Illumination. In <i>Proc. of ACM SIGGRAPH</i> (2006), pp. 935--944.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{NPGB03} Nayar S. K., Peri H., Grossberg M. D., Belhumeur P. N.: A Projection System with Radiometric Compensation for Screen Imperfections. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2003).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1100004</ref_obj_id>
				<ref_obj_pid>1099539</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{NSI05} Nii H., Sugimoto M., Inami M.: Smart Light-Ultra High Speed Projector for Spatial Multiplexing Optical Transmission. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{OS07} Oyamada Y., Saito H.: Focal Pre-Correction of Projected Image for Deblurring Screen Image. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>634285</ref_obj_id>
				<ref_obj_pid>634067</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{Pin01} Pinhanez C.: Using a Steerable Projector and a Camera to Transform Surfaces into Interactive Displays. In <i>Proc. of CHI (extended abstracts)</i> (2001), pp. 369--370.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1770106</ref_obj_id>
				<ref_obj_pid>1770090</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{PLJP07} Park H., Lee M.-H., Jin B.-K. S. Y., Park J.-I.: Content adaptive embedding of complementary patterns for nonintrusive direct-projected augmented reality. In <i>HCI International 2007</i> (2007), vol. 14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2163163</ref_obj_id>
				<ref_obj_pid>2163110</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{PLKP05} Park H., Lee M.-H., Kim S.-J., Park J.-I.: Specularity-Free Projection on Nonplanar Surface. In <i>Proc. of Pacific-Rim Conference on Multimedia (PCM)</i> (2005), pp. 606--616.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{PLKP06} Park H., Lee M.-H., Kim S.-J., Park J.-I.: Contrast Enhancement in Direct-Projected Augmented Reality. In <i>Proc. of IEEE International Conference on Multimedia and Expo (ICME)</i> (2006).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2173549</ref_obj_id>
				<ref_obj_pid>2173493</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{PLS*06} Park H., Lee M.-H., Seo B.-K., Shin H.-C., Park J.-I.: Radiometrically-Compensated Projection onto Non-Lambertian Surface using Multiple Overlapping Projectors. In <i>Proc. of Pacific-Rim Symposium on Image and Video Technology (PSIVT)</i> (2006), pp. 534--544.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{PPK03} Park S. C., Park M. K., Kang M. G.: Super-Resolution Image Reconstruction: A Technical Overview. <i>IEEE Signal Processing Magazine 20</i>, 3 (2003), 21--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{PS05} Pavlovych A., Stuerzlinger W.: A High-Dynamic Range Projection System. In <i>Proc. of SPIE</i> (2005), vol. 5969.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312160</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{Ras99} Raskar R.: Oblique Projector Rendering on Planar Surfaces for a Tracked User. In <i>Proc. of ACM SIGGRAPH (Sketches and Applications)</i> (1999).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015738</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{RBvB*04} Raskar R., Beardsley P., van Baar J., Wang Y., Dietz P., Lee J., Leigh D., Willwacher T.: RFIG Lamps: Interacting with a Self-Describing World via Photosensing Wireless Tags and Projectors. In <i>Proc. of ACM SIGGRAPH</i> (2004), pp. 406--415.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>319370</ref_obj_id>
				<ref_obj_pid>319351</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{RBY*99} Raskar R., Brown M., Yang R., Chen W., Welch G., Towles H., Seales B., Fuchs H.: Multi-Projector Displays using Camera-Based Registration. In <i>Proc. of IEEE Visualization</i> (1999), pp. 161--168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276422</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{RNdD*07} Raskar R., Nii H., de Decker B., Hashimoto Y., Summet J., Moore D., Zhao Y., Westhues J., Dietz P., Inami M., Nayar S. K., Barnwell J., Noland M., Bekaert P., Branzoi V., Bruns E.: Prakash: Lighting Aware Motion Capture using Photosensing Markers and Multiplexed Illuminators. In <i>Proc. of ACM SIGGRAPH</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311543</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{RPG99} Ramasubramanian M., Pattanaik S. N., Greenberg D. P.: A Perceptually Based Physical Error Metric for Realistic Image Synthesis. In <i>Proc. of ACM SIGGRAPH</i> (1999), pp. 73--82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{RvBWR04} Raskar R., van Baar J., Willwacher T., Rao S.: Quadric transfer for immersive curved screen displays. In <i>Proc. of Eurographics</i> (Aug 2004).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280861</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{RWC*98} Raskar R., Welch G., Cutts M., Lake A., Stesin L., Fuchs H.: The Office of the Future: A Unified Approach to Image-Based Modeling and Spatially Immersive Displays. In <i>Proc. of ACM SIGGRAPH</i> (1998), pp. 179--188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1208706</ref_obj_id>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{RWPD06} Reinhard E., Ward G., Pattanaik S., Debevec P.: <i>High Dynamic Range Imaging - Acquisition, Display and Image-Based Lighting</i>. Morgan Kaufmann, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073257</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{SCG*05} Sen P., Chen B., Garg G., Marschner S. R., Horowitz M., Levoy M., Lensch H. P. A.: Dual Photography. In <i>Proc. of ACM SIGGRAPH</i> (2005), pp. 745--755.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015797</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{SHS*04} Seetzen H., Heidrich W., Stuerzlinger W., Ward G., Whitehead L., Trentacoste M., Ghosh A., Vorozcovs A.: High Dynamic Range Display Systems. In <i>Proc. of ACM SIGGRAPH</i> (2004), pp. 760--768.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1097811</ref_obj_id>
				<ref_obj_pid>1097115</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{SMK05} Seitz S. M., Matsushita Y., Kutulakos K. N.: A Theory of Inverse Light Transport. In <i>Proc. of IEEE International Conference on Computer Vision (ICCV)</i> (2005), vol. 2, pp. 1440--1447.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{SMO03} Shirai Y., Matsushita M., Ohguro T.: HIEI Projector: Augmenting a Real Environment with Invisible Information. In <i>Proc. of Worhop on Interactive Systems and Software (WISS)</i> (2003), pp. 115--122. in Japanese.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{SPB04} Salvi J., Pag&#232;s J., Batlle J.: Pattern Codification Strategies in Structured Light Systems. <i>Pattern Recognition 37</i>, 4 (2004), 827--849.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{STJS01} Sukthankar R., Tat-Jen C., Sukthankar G.: Dynamic Shadow Elimination for Multi-Projector Displays. In <i>Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i> (2001), vol. II, pp. 151--157.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{TKH07} Takei J., Kagami S., Hashimoto K.: 3,000-fps 3-D Shape Measurement Using a High-Speed Camera-Projector System. In <i>Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i> (2007), pp. 3211--3216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1100005</ref_obj_id>
				<ref_obj_pid>1099539</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{VVSC05} Vieira M. B., Velho L., Sa A., Carvalho P. C.: A Camera-Projector System for Real-Time 3D Video. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338587</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{WB07} Wetzstein G., Bimber O.: Radiometric Compensation through Inverse Light Transport. <i>Proc. of Pacific Graphics</i> (2007).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1896343</ref_obj_id>
				<ref_obj_pid>1896300</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{WJV*04} Wilburn B., Joshi N., Vaish V., Levoy M., Horowitz M.: High-Speed Videography using a Dense Camera Array. In <i>Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i> (2004), vol. II, pp. 294--301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073259</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{WJV*05} Wilburn B., Joshi N., Vaish V., Talvala E.-V., Antunez E., Barth A., Adams A., Horowitz M., Levoy M.: High Performance Imaging using Large Camera Arrays. In <i>Proc. of ACM SIGGRAPH</i> (2005), pp. 765--776.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1100009</ref_obj_id>
				<ref_obj_pid>1099539</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{WSOS05} Wang D., Sato I., Okabe T., Sato Y.: Radiometric Compensation in a Projector-Camera System Based on the Properties of Human Vision System. In <i>Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams)</i> (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{WWC*05} Waschb&#252;sch M., W&#252;rmlin S., Cotting D., Sadlo F., Gross M. H.: Scalable 3D Video of Dynamic Scenes. <i>The Visual Computer 21</i>, 8--10 (2005), 629--638.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{YHS03} Yoshida T., Horii C., Sato K.: A Virtual Color Reconstruction System for Real Heritage with Light Projection. In <i>Proc. of International Conference on Virtual Systems and Multimedia (VSMM)</i> (2003), pp. 161--168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{YW01} Yang R., Welch G.: Automatic and Continuous Projector Display Surface Calibration using Every-Day Imagery. In <i>Proc. of International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG)</i> (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{ZB07} Zollmann S., Bimber O.: Imperceptible Calibration for Radiometric Compensation. In <i>Proc. of Eurographics (short paper)</i> (2007), pp. 61--64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{ZLB06} Zollmann S., Langlotz T., Bimber O.: Passive-Active Geometric Calibration for View-Dependent Projections onto Arbitrary Surfaces. <i>Proc. of Workshop on Virtual and Augmented Reality of the GI-Fachgruppe AR/VR 2006 (re-print to appear in Journal of Virtual Reality and Broadcasting 2007)</i> (2006).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141974</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{ZN06} Zhang L., Nayar S. K.: Projection Defocus Analysis for Scene Capture and Image Display. In <i>Proc. of ACM SIGGRAPH</i> (2006), pp. 907--915.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Visual Computing of Projector-Camera Systems Oliver Bimber1, Daisuke Iwai1,2, Gordon Wetzstein3 
and Anselm Grundhöfer1 1Bauhaus-University Weimar, Germany, {bimber, iwai, grundhoefer}@uni-weimar.de 
2Osaka University, Japan, iwai@sens.sys.es.osaka-u.ac.jp 3University of British Columbia, Canada, wetzste1@cs.ubc.ca 
 Abstract This article focuses on real-time image correction techniques that enable projector-camera 
systems to display images onto screens that are not optimized for projections, such as geometrically 
complex, colored and textured surfaces. It reviews hardware accelerated methods like pixel-precise geometric 
warping, radiometric compensation, multi-focal projection, and the correction of general light modulation 
effects. Online and of.ine calibration as well as invisible coding methods are explained. Novel attempts 
in super-resolution, high dynamic range and high-speed projection are discussed. These techniques open 
a variety of new applications for projection displays. Some of them will also be presented in this report. 
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Picture/Image Generation 
I.4.8 [Image Processing and Computer Vision]: Scene Analysis I.4.9 [Image Processing and Computer Vision]: 
Applications Keywords: Projector-Camera Systems, Image-Correction, GPU Rendering, Virtual and Augmented 
Reality 1. Introduction Their increasing capabilities and declining cost make video projectors widespread 
and established presentation tools. Be­ing able to generate images that are larger than the actual display 
device virtually anywhere is an interesting feature for many applications that cannot be provided by 
desktop screens. Several research groups discover this potential by applying projectors in unconventional 
ways to develop new and innovative information displays that go beyond simple screen presentations. Today 
s projectors are able to modulate the displayed im­ages spatially and temporally. Synchronized camera 
feedback is analyzed to support a real-time image correction that en­ables projections on complex everyday 
surfaces that are not bound to projector-optimized canvases or dedicated screen con.gurations. This article 
reviews current projector-camera-based image correction techniques. It starts in section 2 with a discus­sion 
on the problems and challenges that arise when project­ing images onto non-optimized screen surfaces. 
Geometric warping techniques for surfaces with different topology and . The Eurographics Association 
2007. re.ectance are described in section 3. Section 4 outlines ra­diometric compensation techniques 
that allow the projection onto colored and textured surfaces of static and dynamic scenes and con.gurations. 
It also explains state-of-the-art techniques that consider parameters of human visual percep­tion to 
overcome technical limitations of projector-camera systems. In both sections (3 and 4), conventional 
structured light range scanning as well as imperceptible coding schemes are outlined that support projector-camera 
calibration (ge­ometry and radiometry). While the previously mentioned sections focus on rather simple 
light modulation effects, such as diffuse re.ectance, the compensation of complex light modulations, 
such as specular re.ection, interre.ection, re­fraction, etc. are explained in section 5. It also shows 
how the inverse light transport can be used for compensating all measurable light modulation effects. 
section 6 is dedicated to a discussion on how novel (at present mainly experimental) approaches in high 
speed, high dynamic range, large depth of .eld and super-resolution projection can overcome the technical 
limitations of today s projector-camera systems in the future. Such image correction techniques have 
proved to be use­  Figure 1: Projecting onto non-optimized surfaces can lead to visual artifacts in 
the re.ected image (a). Projector-camera systems can automatically scan surface and environment properties 
(b) to compute compensation images during run-time that neutralize the measured light modulations on 
the surface (c). ful tools for scienti.c experiments, but also for real-world applications. Some examples 
are illustrated in .gures 25-29. (on the last page of this report). They include on-site archi­tectural 
visualization, augmentations of museum artifacts, video installations in cultural heritage sites, outdoor 
adver­tisement displays, projections onto stage settings during live performances, and ad-hoc stereoscopic 
VR/AR visualizations within everyday environments. Besides these rather individ­ual application areas, 
real-time image correction techniques hold the potential of addressing future mass markets, such as .exible 
business presentations with quickly approaching pocket projector technology, upcoming projection technol­ogy 
integrated in mobile devices -like cellphones, or game­console driven projections in the home-entertainment 
sector. 2. Challenges of Non-Optimized Surfaces For conventional applications, screen surfaces are optimized 
for a projection. Their re.ectance is usually uniform and mainly diffuse (although with possible gain 
and anisotropic properties) across the surface, and their geometrical topolo­gies range from planar and 
multi-planar to simple parametric (e.g., cylindrical or spherical) surfaces. In many situations, however, 
such screens cannot be applied. Some examples are mentioned in section 1. The modulation of the projected 
light on these surfaces, however, can easily exceed a simple diffuse re.ection modulation. In addition, 
blending with dif­ferent surface pigments and complex geometric distortions can degrade the image quality 
signi.cantly. This is outlined in .gure 1. The light of the projected images is modulated on the sur­face 
together with possible environment light. This leads to a color, intensity and geometry distorted appearance 
(cf. .gure 1a). The intricacy of the modulation depends on the com­ plexity of the surface. It can contain 
interre.ections, diffuse and specular re.ections, regional defocus effects, refractions, and more. To 
neutralize these modulations in real-time, and consequently to reduce the perceived image distortions 
is the aim of many projector-camera approaches. In general, two challenges have to be mastered to reach 
this goal: First, the modulation effects on the surface have to be measured and evaluated with computer 
vision techniques and second, they have to be compensated in real-time with computer graphics approaches. 
Structured light projection and synchronized camera feedback enables the required pa­rameters to be determined 
and allows a geometric relation between camera(s), projector(s) and surface to be established (cf. .gure 
1b). After such a system is calibrated, the scanned surface and environment parameters can be used to 
com­pute compensation images for each frame that needs to be projected during run-time. If the compensation 
images are projected, they are modulated by the surface together with the environment light in such a 
way that the .nal re.ected images approximate the original images from the perspective of the calibration 
camera/observer (cf. .gure 1c). The sections below will review techniques that compensate individual 
modulation effects. 3. Geometric Registration The amount of geometric distortion of projected images 
de­pends on how much the projection surface deviates from a plane, and on the projection angle. Different 
geometric projector-camera registration techniques are applied for in­dividual surface topologies. While 
simple homographies are suited for registering projectors with planar surfaces, projec­tive texture mapping 
can be used for non-planar surfaces of known geometry. This is explained in subsection 3.1. For geometrically 
complex and textured surfaces of unknown geometry, image warping based on look-up operations has frequently 
been used to achieve a pixel-precise mapping, as discussed in subsection 3.2. Most of these techniques 
require structured light projection to enable a fully automatic calibra­tion. Some modern approaches 
integrate the structured code information directly into the projected image content in such c . The Eurographics 
Association 2007. a way that an imperceptible calibration can be performed dur­ing run-time. They are 
presented in subsection 3.3. Note, that image warping techniques for parametric surfaces, such as explained 
in [RvBWR04] are out of the scope of this article. 3.1. Uniformly Colored Surfaces For surfaces whose 
re.ectance is optimized for projection (e.g., surfaces with a homogenous white re.ectance), a geo­metric 
correction of the projected images is suf.cient to pro­vide an undistorted presentation to an observer 
with known perspective. Slight misregistrations of the images on the sur­face in the order of several 
pixels lead to geometric artifacts that -in most cases-can be tolerated. This section gives a brief overview 
over general geometry correction techniques that support a single or multiple projectors for such surfaces. 
 (a) (b) Figure 2: Camera-based projector registration for untextured planar (a) and non-planar (b) surfaces. 
If multiple projectors (pro) have to be registered with a planar surface via camera (cam) feedback (cf. 
.gure 2a), collineations with the plane surface can be expressed as 3x3 camera-to-projector homography 
matrix H: h31 h32 h33 A homography matrix can be automatically determined numerically by correlating 
a projection pattern to its corre­sponding camera image. Knowing the homography matrix Hi for projector 
proi and the calibration camera cam, al­lows the mapping from camera pixel coordinates cam(x, y) to the 
corresponding projector pixel coordinates proi(x, y) with proi(x,y,1)= Hi · cam(x, y). The homographies 
are usu­ally extended to homogenous 4x4 matrices to make them compatible with conventional transformation 
pipelines and to consequently bene.t from single pass rendering [Ras99]: . . normalized camera coordinates 
into normalized projector co­ordinates. An observer located at the position of the (possibly off-axis 
aligned) calibration camera perceives a correct image in this case. Such a camera-based approach is frequently 
used for calibrating tiled screen projection displays. A sparse set of point correspondences is determined 
automatically using structured light projection and camera feedback [SPB04]. The correspondences are 
then used to solve for the matrix parameters of Hi for each projector i. In addition to a ge­ometric 
projector registration, a camera-based calibration can be used for photometric (luminance and chrominance) 
matching among multiple projectors. A detailed discussion on the calibration of tiled projection screens 
is out of the scope of this report. It does not cover multi-projector tech­niques that are suitable for 
conventional screen surfaces. The interested reader is referred to [BMY05] for a state-of-the-art overview 
over such techniques. Some other approaches ap­ply mobile projector-camera systems and homographies for 
displaying geometrically corrected images on planar surfaces (e.g., [RBvB*04]). Once the geometry of 
the projection surface is non-planar but known (cf. .gure 2b), a two-pass rendering technique can be 
applied for projecting the images in an undistorted way [RWC*98, RBY*99]: In the .rst pass, the image 
that has to be displayed is off-screen rendered from a target perspective (e.g. the perspective of the 
camera or an observer). In the second step, the geometry model of the display surface is texture-mapped 
with the previously rendered image while being rendered from the perspective of each projector pro. For 
computing the correct texture coordinates that ensure an undistorted view from the target perspective 
projective texture mapping is applied. This hardware accelerated technique dynamically computes a texture 
matrix that maps the 3D vertices of the surface model from the perspectives of the projectors into the 
texture space of the target perspective. . . h11 h12 h13 . h21 h22 h23 . A camera-based registration 
is possible in this case as well. H3x3 = For example, instead of a visible (or an invisible -as dis­cussed 
in section 3.3) structured light projection, features of the captured distorted image that is projected 
onto the surface can be analyzed directly. A .rst example was pre­sented in [YW01] that evaluates the 
deformation of the image content when projected onto the surface to reconstruct the surface geometry, 
and re.ne it iteratively. This approach as­sumes a calibrated camera-projector system and an initial 
rough estimate of the projection surface. If the surface geom­etry has been approximated, the two-pass 
method outlined above can be applied for warping the image geometry in such a way that it appears undistorted. 
In [JF07] a similar method is described that supports a movable projector and requires a h11 h12 0 h13 
stationary and calibrated camera, as well as the known sur- A4x4 = ... ... h21 h22 0 h23 0 010 face 
geometry. The projector s intrinsic parameters and all camera parameters have to be known in both cases. 
While the h31 h32 0 h33 method in [YW01] results in the estimated surface geometry, the approach of 
[JF07] leads to the projector s extrinsic pa-Multiplied after the projection transformation, they map 
rameters. The possibility of establishing the correspondence . The Eurographics Association 2007. between 
projector and camera pixels in these cases, however, depends always on the quality of detected images 
features and consequently on the image content itself. To improve their robustness, such techniques apply 
a predictive feature match­ing rather than a direct matching for features in projector and camera space. 
 However, projective texture mapping in general assumes a simple pinhole camera/projector model and normally 
does not take the lens distortion of projectors into account (yet, a technique that considers the distortion 
of the projector for planar untextured screens has been described in [BJM07]). This -together with .aws 
in feature matching or numerical minimization errors-can cause misregistrations of the pro­jected images 
in the range of several pixels even if other intrinsic and extrinsic parameters have been determined 
pre­cisely. These slight geometric errors are normally tolerable on uniformly colored surfaces. Projecting 
corrected images onto textured surfaces with misregistrations in this order causes -even with applying 
a radiometric compensation (see section 4)-immediate visual intensity and color artifacts that are well 
visible. Consequently, more precise registration techniques are required for textured surfaces. 3.2. 
Textured Surfaces Mapping projected pixels precisely onto different colored pigments of textured surfaces 
is essential for an effective ra­diometric compensation (described in section 4). To achieve a precision 
on a pixel basis is not practical with the registra­tion techniques outlined in section 3.1. Instead 
of registering projectors by structured light sampling followed by numer­ical optimizations that allow 
the computation of projector­camera correspondences via homographies or other projective transforms, 
they can be measured pixel-by-pixel and queried through look-up operations during runtime. Well known 
struc­tured light techniques [BMS98, SPB04] (e.g., gray code scan­ ning) can be used as well for scanning 
the 1-to-n mapping of camera pixels to projector pixels. This mapping is stored in a 2D look-up-texture 
having a resolution of the camera, which in the following is referred to as C2P map (cf. .gure 3). A 
corresponding texture that maps every projector pixel to one or many camera pixels can be computed by 
reversing the C2P map. This texture is called P2C map. It has the resolution of the projector. The 1-to-n 
relations (note that n can also become 0 during the reversion process) are .nally removed from both maps 
through averaging and interpolation (e.g., via a Delaunay triangulation of the transformed samples in 
the P2C map, and a linear interpolation of the pixel colors that store the displacement values within 
the computed triangles). Figure 3b illustrates the perspective of a camera onto a scene and the scanned 
and color-coded (red=x,green=y) C2P texture that maps camera pixels to their corresponding projector 
pixel coordinates. Note, that all textures contain .oating point numbers. (a) (b) Figure 3: Camera-based 
projector registration for textured surfaces (a). The camera perspective onto a scene (b-top) and the 
scanned look-up table that maps camera pixels to projector pixels. Holes are not yet removed in this 
example (b-bottom). These look-up textures contain only the 2D displacement values of corresponding projector 
and camera pixels that map onto the same surface point. Thus, neither the 3D surface ge­ometry, nor the 
intrinsic or extrinsic parameters of projectors and camera are known. During runtime, a fragment shader 
maps all pixels from the projector perspective into the camera perspective (via texture look-ups in the 
P2C map) to ensure a geometric con­sistency for the camera view. We want to refer to this as pixel displacement 
mapping. If multiple projectors are involved, a P2C map has to be determined for each projector. Projector­individual 
fragment shaders will then perform a customized pixel-displacement mapping during multiple rendering 
steps, as described in [BEK05]. In [BWEN05] and in [ZLB06], pixel-displacement map­ping has been extended 
to support moving target perspectives (e.g., of the camera and/or the observer). In [BWEN05] an image-based 
warping between multiple P2C maps that have been pre-scanned for known camera perspectives is applied. 
The result is an estimated P2C map for a new target perspec­tive during runtime. Examples are illustrated 
in .gures 27 and 28. While in this case, the target perspective must be measured (e.g., using a tracking 
device), [ZLB06] analyzes image features of the projected content to approximate a new P2C as soon as 
the position of the calibration camera has changed. If this is not possible because the detected features 
are too unreliable, a structured light projection is triggered to scan a correct P2C map for the new 
perspective. 3.3. Embedded Structured Light Section 3.1 has already discussed registration techniques 
(i.e., [YW01,JF07]) that do not require the projection of structured calibration patterns, like gray 
codes. Instead, they analyze the distorted image content, and thus depend on matchable image features 
in the projected content. Structured light techniques, however, are more robust because they generate 
such features c . The Eurographics Association 2007. synthetically. Consequently, they do not depend 
on the image content. Overviews over different general coding schemes are given in [BMS98,SPB04]. Besides 
a spatial modulation, a temporal modulation of projected images allows integrating coded patterns that 
are not perceivable due to limitations of the human visual sys­tem. Synchronized cameras, however, are 
able to detect and extract these codes. This principle has been described by Raskar et al. [RWC*98], 
and has been enhanced by Cotting et al. [CNGF04]. It is referred to as embedded impercepti­ble pattern 
projection. Extracted code patterns, for instance, allow the simultaneous acquisition of the scenes depth 
and texture for 3D video applications [WWC*05], [VVSC05]. These techniques, however, can be applied to 
integrate the calibration code directly into the projected content to enable an invisible online calibration. 
Thus, the result could be, for instance, a P2C map scanned by a binary gray code or an intensity phase 
pattern that is integrated directly into the pro­jected content. The .rst applicable imperceptible pattern 
projection tech­nique was presented in [CNGF04], where a speci.c time slot (called BIEP=binary image 
exposure period) of a DLP projec­tion sequence is occupied exclusively for displaying a binary pattern 
within a single color channel (multiple color chan­nels are used in [CZGF05] to differentiate between 
multiple projection units). Figure 4 illustrates an example. Figure 4: Mirror .ip (on/off) sequences 
for all intensity val­ues of the red color channel and the chosen binary image exposure period. c .2004 
IEEE [CNGF04] The BIEP is used for displaying a binary pattern. A camera that is synchronized to exactly 
this projection sequence will capture the code. As it can be seen in the selected BIEP in .g­ure 4, the 
mirror .ip sequences are not evenly distributed over all possible intensities. Thus, the intensity of 
each projected original pixel might have to be modi.ed to ensure that the mirror state is active which 
encodes the desired binary value . The Eurographics Association 2007. at this pixel. This, however, can 
result in a non-uniform in­tensity fragmentation and a substantial reduction of the tonal values. Artifacts 
are diffused using a dithering technique. A coding technique that bene.ts from re-con.gurable mirror 
.ip sequences using the DMD discovery board is described in section 6.4. Another possibility of integrating 
imperceptible code pat­terns is to modulate the intensity of the projected image I with a spatial code. 
The result is the code image Icod . In addition, a compensation image Icom is computed in such a way 
that (Icod + Icom)/2 = I. If both images are projected alternately with a high speed, human observers 
will perceive I due to the slower temporal integration of the human visual system. This is referred to 
as temporal coding and was shown in [RWC*98]. The problem with this simple technique is that the code 
remains visible during eye movements or code transitions. Both cannot be avoided for the calibration 
of projector-camera systems using structured light techniques. In [GSHB07] properties of human perception, 
like adaptation limitations to local contrast changes, are taken into account for adapting the coding 
parameters depending on local char­acteristics, such as spatial frequencies and local luminance values 
of image and code. This makes a truly imperceptible temporal coding of binary information possible. For 
binary codes, I is regionally decreased (Icod = I - . to encode a binary 0) or increased (Icod = I + 
. to encode a binary 1) in intensity by the amount of ., while the compensation im­age is computed with 
Icom = 2I - Icod . The code can then be reconstructed from the two corresponding images (Ccod and Ccom) 
captured by the camera with Ccod -Ccom <=> 0. Thereby, . is one coding parameter that is locally adapted. 
In [PLJP07]another technique for adaptively embedding complementary patterns into projected images is 
presented. In this work the embedded code intensity is regionally adapted depending on the spatial variation 
of neighbouring pixels and their color distribution in the YIQ color space. The .nal code contrast of 
. is then calculated depending on the estimated local spatial variations and color distributions. In 
[ZB07], the binary temporal coding technique was ex­ tended to encoding intensity values as well. For 
this, the code image is computed with Icod = I. and the compensation im­age with Icom = I(2 - .). The 
code can be extracted from the camera images with . = 2Ccod /(Ccod +Ccom). Using binary and intensity 
coding, an imperceptible multi-step calibration technique is presented in [ZB07] which is visualized 
in .gure 5, and is outline below. A re-calibration is triggered automatically if misregistra­tions between 
projector and camera are detected (i.e., due to motion of camera, projector or surface). This is achieved 
by continuously comparing the correspondences of embedded point samples. If necessary, a .rst rough registration 
is car­ried out by sampling binary point patterns (cf. .gure 5b) that leads to a mainly interpolated 
P2C map (cf. .gure 5f). This step is followed by an embedded measurement of the surface  Figure 5: Imperceptible 
multi-step calibration for radiometric compensation. A series of invisible patterns (b-e) integrated 
into an image (a) and projected onto a complex surface (g) results in surface measurements (f-i) used 
for radiometric compensation (j). .2007 Eurographics [ZB07]. re.ectance (cf. .gures 5c,g), which is explained 
in section 4.2. Both steps lead to quick but imprecise results. Then a more advanced 3-step phase shifting 
technique (cf. .gure 5e) is triggered that results in a pixel-precise P2C registration (cf. .gure 5i). 
For this, intensity coding is required (cf. .gure 5h). An optional gray code might be necessary for surfaces 
with discontinuities (cf. .gure 5d). All steps are invisible to the human observer and are executed while 
dynamic content can be projected with a speed of 20Hz. In general, temporal coding is not limited to 
the projection of two images only. Multiple code and compensation images can be projected if the display 
frame-rate is high enough. This requires fast projectors and cameras, and will be discussed in section 
6.4. An alternative to embedding imperceptible codes in the visible light range would be to apply infrared 
light as shown in [SMO03] for augmenting real environments with invisible information. Although it has 
not been used for projector­camera calibration, this would certainly be possible. 4. Radiometric Compensation 
For projection screens with spatially varying re.ectance, color and intensity compensation techniques 
are required in addition to a pixel-precise geometric correction. This is known as radiometric compensation, 
and is generally used to minimize the artifacts caused by the local light modulation between projection 
and surface. Besides the geometric map­ping between projector and camera, the surface s re.ectance parameters 
need to be measured on a per-pixel basis before using them for real-time image corrections during run-time. 
In most cases, a one-time calibration process applies visible structured light projections and camera 
feedback to establish the correspondence between camera and projector pixels (see section 3.2) and to 
measure the surface pigment s radiometric behavior. A pixel precise mapping is essential for radiometric 
com­pensation since slight misregistrations (in the order of only a few pixels) can lead to signi.cant 
blending artifacts -even if the geometric artifacts are marginal. Humans are extremely sensitive to even 
small (less than 2%) intensity variations. This section reviews different types of radiometric com­pensation 
techniques. Starting with methods that are suited for static scenes and projector-camera con.gurations 
in sub­section 4.1, it will then discuss more .exible techniques that support dynamic situations (i.e., 
moving projector-camera systems and surfaces) in subsection 4.2. Finally, most recent approaches are 
outlined that dynamically adapt the image content before applying a compensation based on pure ra­diometric 
measurements to overcome technical and physical limitations of projector-camera systems. Such techniques 
take properties of human visual perception into account and are explained in subsection 4.3.   4.1. 
Static Techniques In its most basic con.guration (cf. .gure 6a), an image is displayed by a single projector 
(pro) in such a way that it appears correct (color and geometry) for a single camera view (cam). Thereby, 
the display surfaces must be diffuse, but can have an arbitrary color, texture and shape. The .rst step 
is to determine the geometric relations of camera pixels and projector pixels over the display surface. 
As explained in section 3, the resulting C2P and P2C look-up textures support a pixel-precise mapping 
from camera space to projector space and vice versa.  (a) (b) Figure 6: Radiometric compensation with 
a single projector (a) and sample images projected without and with compensa­tion onto window curtains 
(b). c .2007 IEEE [BEK05] c . The Eurographics Association 2007. Once the geometric relations are known, 
the radiometric parameters are measured. One of the simplest radiometric compensation approaches is described 
in [BEK05]: With respect to .gure 6a, it can be assumed that a light ray with in­ tensity I is projected 
onto a surface pigment with re.ectance M. The fraction of light that arrives at the pigment depends on 
the geometric relation between the light source (i.e., the projector) and the surface. A simple representation 
of the form factor can be used for approximating this fraction: F = f * cos(a)/r2, where a is the angular 
correlation be­tween the light ray and the surface normal and r is the dis­tance (considering square 
distance attenuation) between the light source and the surface. The factor f allows scaling the in­tensity 
to avoid clipping (i.e., intensity values that exceed the luminance capabilities of the projector) and 
to consider the simultaneous contributions of multiple projectors. Together with the environment light 
E, the projected fraction of I is blended with the pigment s re.ectance M: R = EM + IFM. Thereby, R is 
the diffuse radiance that can be captured by the camera. If R, F, M, and E are known, a compensation 
image I can be computed with: I =(R - EM)/FM (1) In a single-projector con.guration, E, F, and M cannot 
be determined independently. Instead, FM is measured by projecting a white .ood image (I = 1) and turning 
off the entire environment light (E = 0), and EM is measured by projecting a black .ood image (I = 0) 
under environment light. Note, that EM also contains the black level of the pro­jector. Since this holds 
for every discrete camera pixel, R, E, FM and EM are entire textures and equation 1 can be com­puted 
together with pixel displacement mapping (see section 3.2) in real-time by a fragment shader. Thus, every 
rasterized projector pixel that passes through the fragment shader is displaced and color compensated 
through texture look-ups. The projection of the resulting image I onto the surface leads to a geometry 
and color corrected image that approximates the desired original image R = O for the target perspective 
of the camera. One disadvantage of this simple technique is that the op­tical limitations of color .lters 
used in cameras and projec­tors are not considered. These .lters can transmit a quite large spectral 
band of white light rather than only a small monochromatic one. In fact, projecting a pure red color, 
for instance, usually leads to non-zero responses in the blue and green color channels of the captured 
images. This is known as the color mixing between projector and camera, which is not taken into account 
by equation 1. Color mixing can be considered for radiometric compensa­tion: Nayar et al. [NPGB03], for 
instance, express the color transform between each camera and projector pixel as pixel­individual 3x3 
color mixing matrices: . The Eurographics Association 2007. .. vRR vRG vRB .. V = vGR vGG vGB vBR vBG 
vBB Thereby, vRG represents the green color component in the red color channel, for example. This matrix 
can be estimated from measured camera responses of multiple projected sam­ple images. It can be continuously 
re.ned over a closed feed­back loop (e.g., [FGN05]) and is used to correct each pixel during runtime. 
In the case the camera response is known while the projector response can remain unknown, it can be assumed 
that vii = 1. This corresponds to an unknown scaling factor, and V is said to be normalized. The off-diagonal 
val­ues can then be computed with vi j = .Cj/.Pi, where .Pi is the difference between two projected intensities 
(P1i -P2i) of primary color i, and .Cj is the difference of the correspond­ing captured images (C1 j 
-C2 j) in color channel j. Thus, 6 images have to be captured (2 per projected color channel) to determine 
all vi j. The captured image R under projection of I can now be expressed with: R = VI. Consequently, 
the compensation image can be computed with the inverse color mixing matrix: I = V -1R (2) Note, that 
V is different for each camera pixel and contains the surface re.ectance, but not the environment light. 
An­other way of determining V is to numerically solve equation 2 for V -1 if enough correspondences between 
I and R are known. In this case, V is un-normalized and vii is propor­tional to [FMR, FMG,FMB]. Consequently, 
the off-diagonal values of V are 0 if no color mixing is considered. Yoshida et al. [YHS03] use an un-normalized 
3x4 color mixing ma­ trix. In this case, the fourth column represents the constant environment light 
contribution. A re.ned version of Nayar s technique was used for controlling the appearance of two­and 
three-dimensional objects, such as posters, boxes and spheres [GPNB04]. Sections 4.2 and 4.3 also discuss 
varia­tions of this method for dynamic situations and image adap­tations. Note, that a color mixing matrix 
was also introduced in the context of shape measurement based on a color coded pattern projection [CKS98]. 
All of these techniques support image compensation in real­time, but suffer from the same problem: if 
the compensation image I contains values above the maximal brightness or below the black level of the 
projector, clipping artifacts will occur. These artifacts allow the underlying surface structure to become 
visible. The intensity range for which radiometric compensation without clipping is possible depends 
on the surface re.ectance, on the brightness and black level of the projector, on the required re.ected 
intensity (i.e., the desired original image), and on the environment light contribution. Figure 7 illustrates 
an example that visualizes the re.ection  Figure 7: Intensity range re.ected by a striped wall paper. 
c .2007 IEEE [GB07] properties for a sample surface. By analyzing the responses in both datasets (FM 
and EM), the range of intensities for a conservative compensation can be computed. Thus, only input pixels 
of the desired original image R = O within this global range (bound by the two green planes -from the 
max­imum value EMmax to the minimum value FMmin) can be compensated correctly for each point on the surface 
without causing clipping artifacts. All other intensities can potentially lead to clipping and incorrect 
results. This conservative in­tensity range for radiometric compensation is smaller than the maximum 
intensity range achieved when projecting onto optimized (i.e, diffuse and white) surfaces. Different 
possibilities exist to reduce these clipping prob­lems. While applying an amplifying transparent .lm 
material is one option that is mainly limited to geometrically sim­ple surfaces, such as paintings [BCK*05], 
the utilization of multiple projectors is another option. Figure 8: Radiometric compensation with multiple 
projec­tors. Multiple individual low-capacity projection units (a) are assumed to equal one singe high-capacity 
unit (b). The simultaneous contribution of multiple projectors in­creases the total light intensity that 
reaches the surface. This can overcome the limitations of equation 1 for extreme situa­tions (e.g., small 
FM values or large EM values) and can con­sequently avoid an early clipping of I. Therefore, [BEK05] 
presents a multi-projector approach for radiometric compen­sation: If N projectors are applied (cf. .gure 
8a), the measured radiance captured by the camera can be approximated with: R = EM + .N (IiFMi). One 
strategy is to balance the pro­ i jected intensities equally among all projectors i, which leads to: 
N Ii =(R - EM)/ .(IjFMj) (3) j Conceptually, this is equivalent to the assumption that a sin­gle high 
capacity projector (prov) produces the total intensity arriving on the surface virtually (cf. .gure 8b). 
This equa­ tion can also be solved in real-time by projector-individual fragment shaders (based on individual 
parameter textures FMi, C2Pi and P2Ci -but striving for the same .nal result R). Note, that EM also contains 
the accumulated black level of all projectors. If all projectors provide linear transfer func­tions (e.g., 
after a linearization) and identical brightness, a scaling of fi = 1/N used in the form factor balances 
the load among them equally. However, fi might be decreased further to avoid clipping and to adapt for 
differently aged bulbs. Note however, that the total black level increases together with the total brightness 
of a multiple projector con.guration. Thus, an increase in contrast cannot be achieved. Possibilities 
for dynamic range improvements are discussed in section 6.3. Since the required operations are simple, 
a pixel-precise radiometric compensation (including geometric warping through pixel-displacement mapping) 
can be achieved in real-time with fragment shaders of modern graphics cards. The actual speed depends 
mainly on the number of pixels that have to be processed in the fragment shader. For exam­ple, frame-rates 
of >100Hz can be measured for radiometric compensations using equation 1 for PAL-resolution videos projected 
in XGA resolution. 4.2. Dynamic Surfaces and Con.gurations The techniques explained in section 4.1 are 
suitable for purely static scenes and .xed projector-camera con.gurations. They require a one-time calibration 
before runtime. For many ap­plications, however, a frequent re-calibration is necessary because the alignment 
of camera and projectors with the sur­faces changes over time (e.g., due to mechanical expansion through 
heating, accidental offset, intended readjustment, mo­bile projector-camera systems, or dynamic scenes). 
In these cases, it is not desired to disrupt a presentation with visible calibration patterns. While 
section 3 discusses several online calibration methods for geometric correction, this section reviews 
online radiometric compensation techniques. Fujii et al. have described a dynamically adapted radio­metric 
compensation technique that supports changing pro­jection surfaces and moving projector-camera con.gurations 
[FGN05]. Their system requires a .xed co-axial alignment c . The Eurographics Association 2007.  Figure 
9: Co-axial projector-camera alignment (a) and re­.ectance measurements through temporal coding (b). 
of projector and camera (cf. .gure 9a). An optical registra­ tion of both devices makes a frequent geometric 
calibration unnecessary. Thus, the .xed mapping between projector and camera pixels does not have to 
be re-calibrated if either sur­face or con.guration changes. At an initial point in time 0 the surface 
re.ectance is determined under environment light (E0M0). To consider color mixing as explained in section 
4.1, this can be done by projecting and capturing corresponding images I0 and C0. The re.ected environment 
light E0 ata pigment with re.ectance M0 can then be approximated by E0M0 = C0 -V0I0, where V0 is the 
un-normalized color mix­ing matrix at time 0, which is constant. After initialization, the radiance Rt 
at time t captured by the camera under projection of It can be approximated with: Rt = Mt /M0(EtM0 +V0It 
). Solving for It results in: It = V0 -1(RtM0/Mt-1 - Et-1M0) (4) Thereby, Rt = Ot is the desired original 
image and It the corresponding compensation image at time t. The environ­ment light contribution cannot 
be measured during runtime. It is approximated to be constant. Thus, Et-1M0 = E0M0. The ratio M0/Mt-1 
is then equivalent to the ratio C0/Ct-1. In this closed feedback loop, the compensation image It at time 
t depends on the captured parameters (Ct-1) at time t - 1. This one-frame delay can lead to visible artifacts. 
Further­more, the surface re.ectance Mt-1 is continuously estimated based on the projected image It-1. 
Thus, the quality of the measured surface re.ectance depends on the content of the desired image Rt-1. 
If Rt-1 has extremely low or high values in one or multiple color channels, Mt-1 might not be valid in 
all samples. Other limitations of such an approach might be the strict optical alignment of projector 
and camera that might be too in.exible for many large scale applications, and that it does not support 
multi-projector con.gurations. Another possibility of supporting dynamic surfaces and projector-camera 
con.gurations that do not require a strict optical alignment of both devices was described in [ZB07]. 
As outlined in section 3.3, imperceptible codes can be em­ . The Eurographics Association 2007. bedded 
into a projected image through a temporal coding to support an online geometric projector-camera registration. 
The same approach can be used for embedding a uniform gray image Icod into a projected image I. Thereby, 
Icod is used to illuminate the surface with a uniform .ood-light image to measure the combination of 
surface re.ectance and projec­tor form factor FM, as explained in section 4.1. To ensure that Icod can 
be embedded correctly, the smallest value in I must be greater than or equal Icod . If this is not the 
case, I is transformed to I. to ensure this condition (cf. .gure 9b). A (temporal) compensation image 
can then be computed with Icom = 2I. - Icod . Projecting Icod and Icom with a high speed, one perceives 
(Icod + Icom)/2 = I.. Synchronizing a camera with the projection allows Icod and therefore also FM to 
be captured. In practice, Icod is approximately 3-5% of the to­tal intensity range -depending on the 
projector brightness and the camera sensitivity of the utilized devices. One other advantage of this 
method is, that in contrast to [FGN05] the measurements of the surface re.ectance do not depend on the 
projected image content. Furthermore, equations 1 or 3 can be used to support radiometric compensation 
with single or multiple projectors. However, projected (radiometric) com­pensation images I have to be 
slightly increased in intensity which leads to a smaller (equal only if FM = 1 and EM = 0) global intensity 
increase of R = O. However, since Icod is small, this is tolerable. One main limitation of this method 
in contrast to the techniques explained in [FGN05], is that it does not react to changes quickly. Usually 
a few seconds (approx. 5-8s) are required for an imperceptible geometric and radiometric re-calibration. 
In [FGN05] a geometric re­ calibration is not necessary. As explained in [GSHB07], a temporal coding 
requires a sequential blending of multiple code images over time, since an abrupt transition between 
two code images can lead to visible .ickering. This is another reason for longer calibration times. In 
summary we can say that .xed co-axial projector­camera alignments as in [FGN05] support real-time cor­ 
rections of dynamic surfaces for a single mobile projector­camera system. The re.ectance measurements 
quality de­pends on the content in O. A temporal coding as in [ZB07] allows unconstrained projector-camera 
alignments and sup­ports .exible single-or multi-projector con.gurations -but no real-time calibration. 
The quality of re.ectance measure­ments is independent on O in the latter case. Both approaches ensure 
a fully invisible calibration during runtime, and en­able the presentation of dynamic content (such as 
movies) at interactive rates (>=20Hz). 4.3. Dynamic Image Adaptation The main technical limitations 
for radiometric compensa­tion are the resolution, frame-rate, brightness and dynamic range of projectors 
and cameras. Some of these issues will be addressed in section 6. This section presents alternative techniques 
that adapt the original images O based on the hu­man perception and the projection surface properties 
before carrying out a radiometric compensation to reduce the effects caused by brightness limitations, 
such as clipping. All compensation methods described so far take only the re.ectance properties of the 
projection surface into account. Particular information about the input image, however, does not in.uence 
the compensation directly. Calibration is carried out once or continuously, and a static color transformation 
is applied as long as neither surface nor projector-camera con.guration changes -regardless of the individual 
desired image O. Yet, not all projected colors and intensities can be reproduced as explained in section 
4.1 and shown in .gure 7. Content dependent radiometric and photometric compen­sation methods extend 
the traditional algorithms by applying additional image manipulations depending on the current image 
content to minimize clipping artifacts while preserv­ing a maximimum of brightness and contrast to generate 
an optimized compensation image. Such a content dependent radiometric compensation method was presented 
by Wang et al. [WSOS05]. In this method, the overall intensity of the input image is scaled until clipping 
errors that result from radiometric compensation are below a perceivable threshold. The threshold is 
derived by using a perceptually-based physical error metric that was proposed in [RPG99], which considers 
the image luminance, spatial frequencies and visual masking. This early technique, however, can only 
be applied to static monochrome images and surfaces. The numerical minimization that is carried out in 
[WSOS05] requires a series of iterations that make real­ time rates impossible. Park et al. [PLKP06] 
describe a technique for increasing the contrast in a compensation image by applying a histogram equalization 
to the colored input image. While the visual quality can be enhanced in terms of contrast, this method 
does not preserve the contrast ratio of the original image. Consequently, the image content is modi.ed 
signi.cantly, and occurring clipping errors are not considered. A complex framework for computing an 
optimized photo­metric compensation for colored images is presented by Ash­down et al. [AOSS06]. In this 
method the device-independent CIE L*u*v color space is used, which has the advantage that color distances 
are based on the human visual perception. Therefore, an applied high dynamic range (HDR) camera has to 
be color calibrated in advance. The input images are adapted depending on a series of global and local 
parame­ters to generate an optimized compensated projection: The captured surface re.ectance as well 
as the content of the in­put image are transformed into the CIE L*u*v color space. The chrominance values 
of all input image s pixels are .tted into the gamut of the corresponding projector pixels. In the next 
step, a luminance .tting is applied by using a relaxation method based on differential equations. Finally, 
the compen­sated adapted input image is transformed back into the RGB color space for projection. (b) 
when being projected onto a colored surface (a). The pro­jection of an adapted compensation image (c) 
minimizes the visibility of these artifacts (d). c .2006 IEEE [AOSS06] This method achieves optimal compensation 
results for surfaces with varying re.ectance properties. Furthermore, a compensation can be achieved 
for highly saturated surfaces due to the fact that besides a luminance adjustment, a chromi­nance adaptation 
is applied as well. Its numerical complexity, however, allows the compensation of still images only. 
Figure 10 shows a sample result: An uncompensated projection of the input image projected onto a colored 
surface (a) results in color artifacts (b). Projecting the adapted compensation image (c) onto the surface 
leads to signi.cant improvements (d). Ashdown et al. proposed another .tting method in [ASOS07] that 
uses the chrominance threshold model of human vision together with the luminance threshold to avoid visible 
artifacts. Content-dependent adaptations enhance the visual quality of a radiometric compensated projection 
compared to static methods that do not adapt to the input images. Animated content like movies or TV-broadcasts, 
however, cannot be compensated in real-time with the methods reviewed above. While movies could be pre-corrected 
frame-by-frame in ad­vance, real-time content like interactive applications cannot be presented. In [GB07], 
a real-time solution for adaptive radiometric compensation was introduced that is implemented entirely 
on the GPU. The method adapts each input image in two steps: First it is analyzed for its average luminance 
that leads to an approximate global scaling factor which depends on the surface re.ectance. This factor 
is used to scale the input image s intensity between the conservative and the maximum c . The Eurographics 
Association 2007. intensity range (cf. .gure 7 in section 4.1). Afterwards, a com­ pensation image is 
calculated according to equation 1. Instead of projecting this compensation image directly, it is further 
an­alyzed for potential clipping errors. Errors are extracted and blurred in addition. In a .nal step, 
the input image is scaled globally again depending on its average luminance and on the calculated maximum 
clipping error. In addition, it is scaled locally based on the regional error values. The threshold map 
explained in [RPG99] is used to constrain the local image ma­ nipulation based on the contrast and the 
luminance sensitivity of human observers. Radiometric compensation (equation 1) is applied again to the 
adapted image, and the result is .nally projected. Global, but also local scaling parameters are adapted 
over time to reduce abrupt intensity changes in the projection which would lead to a perceived and irritating 
.ickering. .2007 IEEE [GB07] This approach does not apply numerical optimizations and consequently enables 
a practical solution to display adapted dynamic content in real-time and in increased quality (com­pared 
to traditional radiometric compensation). Yet, small clipping errors might still occur. However, especially 
for content with varying contrast and brightness, this adaptive technique enhances the perceived quality 
signi.cantly. An example is shown in .gure 11: Two frames of a movie (b,e) are projected with a static 
compensation technique [BEK05] (c,f) and with the adaptive real-time solution [GB07] (d,g) onto a natural 
stone wall (a). While clipping occurs in case (c), case (f) appears too dark. The adaptive method reduces 
the clipping errors for bright images (d) while maintaining details in the darker image (g). 5. Correcting 
Complex Light Modulations All image correction techniques that have been discussed so far assume a simple 
geometric relation between camera and projector pixels that can be automatically derived using homography 
matrices, structured light projections, or co-axial projector-camera alignments. . The Eurographics Association 
2007. When projecting onto complex everyday surfaces, how­ever, the emitted radiance of illuminated display 
elements is often subject to complex lighting phenomena. Due to dif­fuse or specular interre.ections, 
refractions and other global illumination effects, multiple camera pixels at spatially dis­tant regions 
on the camera image plane may be affected by a single projector pixel. A variety of projector-camera 
based compensation meth­ods for speci.c global illumination effects has been proposed. These techniques, 
as well as a generalized approach to com­pensating light modulations using the inverse light transport 
will be discussed in the following subsections. We start with discussions on how diffuse interre.ections 
(subsection 5.1) and specular highlights (subsection 5.2) can be compensated. The inverse light transport 
approach is introduced as the most general image correction scheme in subsection 5.3.   5.1. Interre.ections 
Eliminating diffuse interre.ections or scattering for projec­tion displays has recently gained a lot 
of interest in the computer graphics and vision community. Cancellation of interre.ections has been proven 
to be useful for improv­ing the image quality of immersive virtual and augmented reality displays [BGZ*06]. 
Furthermore, such techniques can be employed to remove indirect illumination from pho­tographs [SMK05]. 
For compensating global illumination ef­ fects, these need to be acquired, stored and processed, which 
will be discussed for each application. Seitz et al. [SMK05], for instance, measured an impulse scatter 
function (ISF) matrix B with a camera and a laser pointer on a movable gantry. The camera captured diffuse 
objects illuminated at discrete locations. Each of the samples centroid represents one row/column in 
the matrix as depicted in .gure 12. The ISF matrix can be employed to remove interre.ections from photographs. 
Therefore, an interre.ection cancellation operator C1 = B1B-1 is de.ned that, when multiplied to a captured 
camera image R, extracts its direct illumination. B-1 is the ISF matrix s inverse and B1 contains only 
direct illumination. For a diffuse scene, this can easily be extracted from B by setting its off-diagonal 
elements to zero. A related technique that quickly separates direct and indirect illumina­tion for diffuse 
and non-diffuse surfaces was introduced by Nayar et al. [NKGR06]. Experimental results in [SMK05] were 
obtained by sam­ pling the scene at approx. 35 locations in the camera image under laser illumination. 
Since B is in this case a very small and square matrix it is trivial to be inverted for computing B-1. 
However, inverting a general light transport matrix in a larger scale is a challenging problem and will 
be discussed in section 5.3. Compensating indirect diffuse scattering for immersive projection screens 
was proposed in [BGZ*06]. Assuming a known screen geometry, the scattering was simulated and corrected 
with a customized reverse radiosity scheme. Bim­ber et al. [Bim06] and Mukaigawa et al. [MKO06] showed 
that a compensation of diffuse light interaction can be per­formed in real-time by reformulating the 
radiosity equation as I =(1 - .F)O. Here O is the desired original image, I the projected compensation 
image, 1 the identity matrix and .F the precomputed form-factor matrix. This is equivalent to applying 
the interre.ection cancellation operator, introduced in [SMK05], to an image O that does not contain 
interre.ec­tions. The quality of projected images for a two-sided pro­jection screen can be greatly enhanced 
as depicted in .gure 13. All computations are performed with a relatively coarse patch resolution of 
about 128 × 128 as seen in .gure 13 (c). .2006 IEEE [BGZ*06] While the form factor matrix in [Bim06, 
MKO06] was precomputed, Habe et al. [HSM07] presented an algorithm that automatically acquires all photometric 
relations within the scene using a projector-camera system. They state also that this theoretically allows 
specular interre.ections to be compensated for a .xed viewpoint. However, such a compen­sation has not 
been validated in the presented experiments. For the correction, a form-factor matrix inverse is required, 
which again is trivial to be calculated for a low patch resolu­tion. 5.2. Specular Re.ections When projecting 
onto non-diffuse screens, not only diffuse and specular interre.ections affect the quality of projected 
imagery, but a viewer may also be distracted by specular highlights. Park et al. [PLKP05] presented a 
compensation approach that attempts to minimize specular re.ections using multiple overlapping projectors. 
The highlights are not due to global illumination effects, but to the incident illumination that is re.ected 
directly toward the viewer on a shiny surface. Usually, only one of the projectors creates a specular 
highlight at a point on the surface. Thus, its contribution can be blocked while display elements from 
other projectors that illuminate the same surface area from a different angle are boosted. For a view-dependent 
compensation of specular re.ections, the screen s geometry needs to be known and registered with all 
projectors. Displayed images are pre-distorted to create a geometrically seamless projection as described 
in section 3. The amount of specularity for a projector i at a surface point s with a given normal n 
is proportional to the angle .i between n and the sum of the vector from s to the projector s position 
pi and the vector from s to the viewer u: -1 -n · (pi + u) .i = cos (5) |pi + u| Assuming that k projectors 
illuminate the same surface, a weight wi is multiplied to each of the incident light rays for a photometric 
compensation: sin (.i) wi = .. (6).kj=1 sin .j Park et al. [PLS*06] extended this model by an addi­ tional 
radiometric compensation to account for the color modulation of the underlying projection surface (cf. 
.gure 14). Therefore, Nayar s model [NPGB03] was implemented. The required one-to-one correspondences 
between projector and camera pixels were acquired with projected binary gray codes [SPB04]. c . The Eurographics 
Association 2007.  .2006 IEEE [PLS*06] 5.3. Radiometric Compensation through Inverse Light Transport 
Although the previously discussed methods are successful in compensating particular aspects of the light 
transport between projectors and cameras, they lead to a fragmented understand­ing of the subject. A 
uni.ed approach that accounts for many of the problems that were individually addressed in previous works 
was described in [WB07]. The full light transport be- Figure 15: The light transport matrix between a 
projector and a camera. image with resolution m × n, i. is the projection pattern with a resolution of 
p×q, and e. are direct and global illumination effects caused by the environment light and the projector 
s black level captured from the camera. Each light transport matrix T .p (size: mn × pq) describes the 
contribution of a .c single projector color channel .p to an individual camera channel .c. The model 
can easily be extended for k projectors and l cameras: .. .. . . k 1TB R 1iR 11 1TR 1TG RR tween a projector 
and a camera was employed to compensate 1rR -1 eR ··· = ..... ..... ..... ..... 1iG . . . 11 k 1TR 1TG 
··· 1TB GG G direct and indirect illumination effects, such as interre.ec­ tions, refractions and defocus, 
with a single technique in .... .... 1rG -1 eG . . .. .. . . . . .. ..real-time. Furthermore, this also 
implies a pixel-precise geo­ metric correction. In the following subsection we refer to the . 11 k lrB 
-leB lTR lTG ··· lTB kiB BB B (8) For a generalized radiometric compensation the camera im­age r. is 
replaced by a desired image o. of camera resolution and the system can be solved for the projection pattern 
i. that approach as performing radiometric compensation. However, geometric warping is always implicitly 
included. In order to compensate direct and global illumination as well as geometrical distortions in 
a generalized manner, the needs to be projected. This accounts for color modulations full light transport 
has to be taken into account. Within a and geometric distortions of projected imagery. Due to the projector-camera 
system, this is a matrix T. that can be ac­matrix s enormous size, sparse matrix representations and 
quired in a pre-processing step, for instance as described by operations can help to save storage and 
increase performance. A customized clustering scheme that allows the light transport matrix s pseudo-inverse 
to be approximated is de- Sen et al. [SCG*05]. Therefore, a set of illumination patterns is projected 
onto the scene and recorded using HDR imag­ing techniques (e.g. [DM97]). Individual matrix entries can 
then be reconstructed from the captured camera images. As scribed in [WB07]. Inverse impulse scatter 
functions or depicted in .gure 15, a camera image with a single lit projec­ form-factor matrices had 
already been used in previous al­tor pixel represents one column in the light transport matrix. gorithms 
[SMK05,Bim06,MKO06,HSM07],butinamuch Usually, the matrix is acquired in a hierarchical manner by smaller 
scale, which makes an inversion trivial. Using the simultaneously projecting multiple pixels. For a single-projector-camera 
con.guration the forward light transport is described by a simple linear equation as light transport 
matrix s approximated pseudo-inverse, radio­metric compensation reduces to a matrix-vector multiplica­ 
tion: .. . rR - eR .. . iR i. = T +(o. - e.) , (9) . TR TG TB RR R TR TG TB GG G . rG - eG . = .. 
. iG . , (7) In [WB07], this was implemented on the GPU and yielded rB - eB TRTGTB iB BB B real-time 
frame-rates. where each r. is a single color channel . of a camera Figure 16 shows a compensated projection 
onto highly . The Eurographics Association 2007.  refractive material (f), which is impossible with 
conventional approaches (e), because a direct correspondence between projector and camera pixels is not 
given. The light transport matrix (cf. .gure 16b) and it s approximated pseudo-inverse (visualized in 
c) contain local and global illumination effects within the scene (global illumination effects in the 
matrix are partially magni.ed in b). It was shown in [WB07] that all measurable light mod­ ulations, 
such as diffuse and specular re.ections, complex interre.ections, diffuse scattering, refraction, caustics, 
defo­cus, etc. can be compensated with the multiplication of the inverse light transport matrix and the 
desired original image. Furthermore, a pixel-precise geometric image correction is implicitly included 
and becomes feasible -even for surfaces that are unsuited for a conventional structured light scanning. 
However, due to the extremely long acquisition time of the light transport matrix (up to several hours), 
this approach will not be practical before accelerated scanning techniques have been developed. 6. Overcoming 
Technical Limitations Most of the image correction techniques that are described in this report are constrained 
by technical limitations of pro­jector and camera hardware. An insuf.cient resolution or dynamic range 
of both devices leads to a signi.cant loss of image quality. A too short focal depth results in regionally 
cause the perception of temporally embedded codes. This section is dedicated to giving an overview of 
novel (at present mainly experimental) approaches that might lead to future improvements of projector-camera 
systems in terms of fo­cal depth (subsection 6.1), high resolution (subsection 6.2), dynamic range (subsection 
6.3), and high speed (subsection 6.4).   6.1. Increasing Focal Depth Projections onto geometrically 
complex surfaces with a high depth variance generally do not allow the displayed content to be in focus 
everywhere. Common DLP or LCD projec­tors usually maximize their brightness with large apertures. Thus, 
they suffer from narrow depths of .eld and can only generate focused imagery on a single fronto-parallel 
screen. Laser projectors, which are commonly used in planetaria, are an exception. These emit almost 
parallel light beams, which make very large depths of .eld possible. However, the cost of a single professional 
laser projector can exceed the cost of several hundred conventional projectors. In order to in­crease 
the depth of .eld of conventional projectors, several approaches for deblurring unfocused projections 
with a single or with multiple projectors have been proposed. Zhang and Nayar [ZN06] presented an iterative, 
spatially­ varying .ltering algorithm that compensates for projector defocus. They employed a coaxial 
projector-camera system to measure the projection s spatially-varying defocus. Therefore, dot patterns 
as depicted in .gure 17a are projected onto the screen and captured by the camera (b). The defocus kernels 
for each projector pixel can be recovered from the captured images and encoded in the rows of a matrix 
B. Given the environment light EM including the projector s black level and a desired input image O, 
the compensation image I can be computed by minimizing the sum-of-squared pixel difference between O 
and the expected projection BI + EM as argmin .BI + EM - O.2 , (10) I, 0=I=255 which can be solved with 
a constrained, iterative steepest gradient solver as described in [ZN06]. An alternative approach to 
defocus compensation for a sin­gle projector setup was presented by Brown et al. [BSC06]. Projector defocus 
is modeled as a convolution of a projected original image O and Gaussian point spread functions (PSFs) 
as R(x,y)= O (x, y) . H (x,y), where the blurred image that can be captured by a camera is R. The PSFs 
are estimated by projecting features on the canvas and capturing them with a camera. Assuming a spatially-invariant 
PSF, a compensation image I can be synthesized by applying a Wiener deconvolu­tion .lter to the original 
image: defocused image areas when projected onto surfaces with I (x,y)= F-1 H* (u,v)O (u,v) . (11) 2 
H (u,v) an essential depth variance. Slow projection frame-rates will + 1/SNR The signal-to-noise ration 
(SNR) is estimated a priori, O and H are the Fourier transforms of O and H, respectively, and H * is 
H s complex conjugate. F-1 denotes the inverse Fourier transform. Since the defocus kernel H is generally 
not spatially-invariant (this would only be the case for a fronto­parallel plane) Wiener .ltering cannot 
be applied directly. c . The Eurographics Association 2007.  Figure 17: Defocus compensation with a 
single projector: An input image (c) and its defocused projection onto a planar canvas (d). Solving equation 
10 results in a compensation image (e) that leads to a sharper projection (f). For this com­pensation, 
the spatially-varying defocus kernels are acquired by projecting dot patterns (a) and capturing them 
with a camera (b). c .2006 ACM [ZN06] Therefore, basis compensation images are calculated for each of 
the uniformly sampled feature points using equation 11. The .nal compensation image is then generated 
by interpolat­ing the four closest basis responses for each projector pixel. Oyamada and Saito [OS07] 
presented a similar approach to single projector defocus compensation. Here, circular PSFs are used for 
the convolution and estimated by comparing the original image to various captured compensation images 
that were generated with different PSFs. The main drawback of these single projector defocus com­pensation 
approaches is that the quality is highly dependent on the projected content. All of the discussed methods 
result in a pre-sharpened compensation image that is visually closer to the original image after being 
optically blurred by the de­focused projection. While soft contours can be compensated, this is generally 
not the case for sharp features. Inverse .ltering for defocus compensation can also be seen as the division 
of the original image by the projector s aperture image in frequency domain. Low magnitudes in the Fourier 
transform of the aperture image, however, lead to intensity values in spatial domain that exceed the 
displayable range. Therefore, the corresponding frequencies are not con­sidered, which then results in 
visible ringing artifacts in the .nal projection. This is the main limitation of the approaches discussed 
above, since in frequency domain the Gaussian PSF of spherical apertures does contain a large fraction 
of low Fourier magnitudes. As shown above, applying only small kernel scales will reduce the number of 
low Fourier mag­nitudes (and consequently the ringing artifacts) but will also lead only to minor focus 
improvements. To overcome this problem, a coded aperture whose Fourier transform has . The Eurographics 
Association 2007. initially less low magnitudes was applied in [GB08]. Con­ sequently, more frequencies 
are retained and more image details are reconstructed (cf. .gure 18). Figure 18: The power spectra of 
the Gaussian PSF of a spher­ical aperture and of the PSF of a coded aperture: Fourier magnitudes that 
are too low are clipped (black), which causes ringing artifacts. Image projected in focus, and with the 
same optical defocus (approx. 2m distance to focal plane) in three different ways: with spherical aperture 
 untreated and de­convolved with Gaussian PSF, with coded aperture and de­convolved with PSF of aperture 
code. An alternative approach that is less dependent on the ac­tual frequencies in the input image was 
introduced in [BE06]. Multiple overlapping projectors with varying focal depths illuminate arbitrary 
surfaces with complex geometry and re.ectance properties. Pixel-precise focus values Fi,x,y are automatically 
estimated at each camera pixel (x, y) for every projector. Therefore, a uniform grid of circular patterns 
is dis­played by each projector and recorded by a camera. In order to capture the same picture (geometrically 
and color-wise) for each projection, these are pre-distorted and radiometrically compensated as described 
in sections 3 and 4. Once the relative focus values are known, an image from multiple projector contributions 
with minimal defocus can be composed in real-time. A weighted image composition rep­resents a tradeoff 
between intensity enhancement and focus re.nement as: wi (R - EM) Fi,x,y Ii = , wi,x,y = , (12) .Nj wjFM 
j .Nj Fj,x,y where Ii is the compensation image for projector i if N projectors are applied simultaneously. 
Display contributions with high focus values are up-weighted while contributions of projectors with low 
focus values are down-weighted pro­portionally. A major advantage of this method, compared to single 
projector approaches, is that the focal depth of the entire projection scales with the number of projectors. 
An example for two projectors can be seen in .gure 19.  6.2. Super-Resolution Super-resolution techniques 
can improve the accuracy of geometric warping (see section 3) and consequently have the potential to 
enhance radiometric compensation (see section 4) due to a more precise mapping of projector pixels onto 
surface pigments. Over the past years, several researches have proposed super-resolution camera techniques 
to overcome the inherent limitation of low-resolution imaging systems by using signal processing to obtain 
super-resolution images (or image sequences) with multiple low-resolution devices [PPK03]. Using a single 
camera to obtain multiple frames of the same scene is most popular. Multi-camera approaches have also 
been proposed [WJV*05]. On the other hand, super-resolution projection systems are just beginning to 
be researched. This section introduces recent work on such techniques that can generally be categorized 
into two different groups. The .rst group proposes super­resolution rendering with a single projector 
[AU05]. Other approaches achieve this with multiple overlapping projectors [JR03, DVC07]. In single projector 
approaches, so-called wobulation tech­niques are applied: Multiple sub-frames are generated from an original 
image. An optical image shift displaces the projected image of each sub-frame by a fraction of a pixel 
[AU05]. Each sub-frame is projected onto the screen with slightly different positions using an opto-mechanical 
image shifter. This light modulator must be switched fast enough so that all sub-frames are projected 
in one frame. Consequently, ob­servers perceive this rapid sequence as a continuous and .icker-free image 
while the resolution is spatially enhanced. Such techniques have been already realized with a DLP sys­tem 
(SmoothPicture R ., Texas Instruments Incorporated). The goal of multi-projector super-resolution methods 
is to generate a high resolution image with the superposition of multiple low resolution sub-frames produced 
by different projection units. Thereby, the resolutions of each sub-frame differ and the display surfaces 
are assumed to be diffuse. Super-resolution pixels are de.ned by the overlapping sub­frames that are 
shifted on a sub-pixel basis as shown in .gure 20. Generally, the .nal image is estimated as the sum 
of the sub-frames. If N sub-frames Ii=1..N are displayed, this is modeled as: N R = .AiViIi + EM (13) 
i Note, that in this case the parameters R, Ii, and EM are im­ages, and that Ai and Vi are the geometric 
warping matrix and the color mixing matrix that transform the whole image (in contrast to sections 3 
and 4, where these parameters represent transformations of individual pixels). Figure 20c shows a close-up 
of overlapping pixels to il­ lustrate the problem that has to be solved: While I1[1..4] and I2[1..4] 
are the physical pixels of two projectors, k[1..4] rep­resent the desired super-resolution pixel structure. 
The goal is to .nd the intensities and colors of corresponding projector pixels in I1 and I2 that approximate 
k as close as possible by assuming that the perceived result is I1 + I2. This is ob­viously a global 
optimization problem, since k and I have different resolutions. Thus, if O is the desired original image 
and R is the captured result, the estimation of sub-frame Ii for projector i is in general achieved by 
minimizing ||O - R||: 2 Ii = argmin||O - R||(14) Ii c . The Eurographics Association 2007. Jaynes et 
al. .rst demonstrated resolution enhancement with multiple superimposed projections [JR03]. Homogra­ 
phies are used for initial geometric registration of multiple sub-frames onto a planar surface. However, 
homographic transforms lead to uniform two-dimensional shifts and sam­pling rates with respect to the 
camera image rather than to non-uniform ones of general projective transforms. To reduce this effect, 
a warped sub-frame is divided into smaller regions that are shifted to achieve sub-pixel accuracy. Initially, 
each such frame is estimated in the frequency do­main by phase shifting the frequencies of the original 
image. Then, a greedy heuristic process is used to recursively update pixels with the largest global 
error with respect to equation 14. The proposed model does not consider Vi and EM in equation 13 and 
a camera is used only for geometric correction. The iterations of the optimization process are terminated 
manually in [JR03]. Damera-Venkata et al. proposed a real-time rendering al­gorithm for computing sub-frames 
that are projected by su­perimposed lower-resolution projectors [DVC07]. In contrast to the previous 
method, they use a camera to estimate the geometric and photometric properties of each projector dur­ing 
a calibration step. Image registration is achieved on a sub-pixel basis using gray code projection and 
coarse-to-.ne multi-scale corner analysis and interpolation. In the proposed model, Ai encapsulates the 
effects of geometric distortion, pixel reconstruction point spread function and resample .l­tering operations. 
Furthermore, Vi and EM are obtained during calibration by analyzing the camera response for projected 
black, red, green, and blue .ood images of each projector. In principle, this model could be applied 
to a projection surface with arbitrary color, texture and shape. However, this has not been shown in 
[DVC07]. Once the parameters are estimated, equation 14 can be solved numerically using an iterative 
gradient descent algorithm. This generates optimal results but does not achieve real-time rendering rates. 
For real-time sub-frame rendering, it was shown in [DVC07] that near-optimal results can be produced 
with a non-iterative approximation. This is accomplished by intro­ducing a linear .lter bank that consists 
of impulse responses of the linearly approximated results which are pre-computed with the non-linear 
iterative algorithm mentioned above. The .lter bank is applied to the original image for estimating the 
sub-frames. In an experimental setting, this .ltering process is imple­mented with fragment shaders and 
real-time rendering is achieved. Figure 21 illustrates a close-up of a single pro­jected sub-frame (a) 
and four overlapping projections with super-resolution rendering enabled (b). In this experiment, the 
original image has a higher resolution than any of the sub-frames. . The Eurographics Association 2007. 
 (a) (b) c .2007 IEEE [DVC07] 6.3. High Dynamic Range To overcome the contrast limitations that are 
related to ra­diometric compensation (see .gure 7), high dynamic range (HDR) projector-camera systems 
are imaginable. Although there has been much research and development on HDR cam­era and capturing systems, 
little work has been done so far on HDR projectors. In this section, we will focus on state-of-the-art 
HDR pro­jector technologies rather than on HDR cameras and cap­turing techniques. A detailed discussion 
on HDR captur­ing/imaging technology and techniques, such as recovering camera response functions and 
tone mapping/reproduction is out of the scope of this report. The interested reader is referred to [RWPD06]. 
Note, that for the following we want to use the notation of dynamic range (unit decibel, dB) for cameras, 
and the notation of contrast ratio (unit-less) for projectors. The dynamic range of common CCD or CMOS 
chips is around 60 dB while recent logarithmic CMOS image sen­sors for HDR cameras cover a dynamic range 
of 170 dB (HDRC R ., Omron Automotive Electronics GmbH). Besides special HDR sensors, low dynamic rage 
(LDR) cameras can be applied for capturing HDR images. The most popular approach to HDR image acquisition 
involves taking multiple images of the same scene with the same camera using different exposures, and 
then merging them into a single HDR image. There are many ways for making multiple exposure mea­surements 
with a single camera [DM97] or with multiple coaxially aligned cameras [AA01]. The interested reader 
is referred to [NB03] for more information. As an alternative to merging multiple LDR images, the exposure 
of individ­ual sensor pixels in one image can be controlled with addi­tional light modulators, like an 
LCD panel [NB03] or a DMD chip [NBB04] in front of the sensor or elsewhere within the optical path. In 
these cases, HDR images are acquired directly.   The contrast ratio of DMD chips and LCoS panels (with­out 
additional optics) is about 2,000:1 [DDS03] and 5,000:1 (SXRD R ., Sony Corporation) respectively. Currently, 
a con­trast ratio of around 15,000:1 is achieved for high-end pro­jectors with auto-iris techniques that 
dynamically adjust the amount of the emitting light according to the image con­tent. Auto-iris techniques, 
however, cannot expand the dy­namic range within a single frame. On the other hand, a laser projection 
system achieved the contrast ratio of 100,000:1 in [BDD*04] because of the absence of light in dark regions. 
Multi-projector systems can enhance spatial resolution (see section 6.2) and increase the intensity range 
of projections (see section 4.1). However, merging multiple LDR projec­ tions does not result in an HDR 
image. Majumder et al., for example, have rendered HDR images with three overlapped projectors to demonstrate 
that a larger intensity range and resolution will result in higher quality images [MW01]. Al­ though 
the maximum intensity level is increased with each additional projector unit, the minimum intensity level 
(i.e., the black level) is also increased. The contrast of overlapping regions is never greater than 
the largest one of each individual projector. Theoretically, if the maximum and the minimum intensities 
of the ith projector are Imax and Imin, its contrast ratio is ii Imax/Imin i i : 1. If N projectors are 
overlapped, the contrast ratio Imax Imin of the .nal image is .N / .N : 1. For example, if two ii ii 
= 10,Imax projectors are used whose intensities are Imin = 11 100 and Imin = 100,Imax = 1000 (thus both 
contrast ratios 22 are 10 : 1), the contrast ratio of the image overlap is still 10 : 1 (10 =(Imax + 
Imax)/(Imin + Imin )). 12 12 Recently, HDR display systems have been proposed that combine projectors 
and external light modulators. Seetzen et al. proposed an HDR display that applies a projector as a backlight 
of an LCD panel instead of a .uorescent tube assembly [SHS*04]. As in .gure 22a, the projector is di­ 
rected to the rear of a transmissive LCD panel. The light that corresponds to each pixel on the HDR display 
is effectively modulated twice: .rst by the projector and then by the LCD panel. Theoretically, the .nal 
contrast ratio is the product of the individual contrast ratio of the two modulators. If a pro­jector 
with a contrast ratio of c1 : 1 and an LCD panel with a contrast ratio of c2 : 1 are used in this example, 
the contrast of the combined images is (c1 · c2) : 1. In an experimental setup, this approach achieved 
a contrast ratio of 54, 000 : 1 using an LCD panel and a DMD projector with a contrast ratio of 300 : 
1 and 800 : 1 respectively. The reduction of contrast is due to noise and imperfections in the optical 
path. The example described above does not really present a projection system since the image is generated 
behind an LCD panel, rather than on a projection surface. True HDR projection approaches are discussed 
in [DRW*06, DSW*07]. The basic idea of realizing an HDR projector is to combine a normal projector and 
an additional low resolution light modulating device. Double modulation decreases the black (a) (b) 
 (c)  Figure 22: Different HDR projection setups: using a projec­tor as backlight of an LCD (a), modulating 
the image path (b), and modulating the illumination path (c). level of the projected image, and increases 
the dynamic range as well as the number of addressable intensity levels. Thereby, LCD panels, LCoS panels, 
DMD chips can serve as light modulators. HDR projectors can be categorized into systems that mod­ulate 
the image path (cf. .gure 22b), and into systems that modulate the illumination path (22c). In the .rst 
case, an image is generated with a high resolution light modulator .rst, and then modulated again with 
an additional low reso­lution light modulator. In the latter case, the projection light is modulated 
in advance with a low resolution light modu­lator before the image is generated with a high resolution 
modulator. In each approach, a compensation for the optical blur caused by the low resolution modulator 
is required. The de­gree of blur can be measured and can described with a point spread function (PSF) 
for each low resolution pixel in relation to corresponding pixels on the higher resolution modulator. 
A division of the desired output image by the estimated blurred image that is simulated by the PSF will 
result in the neces­sary compensation mask which will be displayed on the high resolution modulator. 
Pavlovych et al. proposed a system that falls into the .rst category [PS05]. This system uses an external 
attachment (an LCD panel) in combination with a regular DLP projector (cf. .gure 22b). The projected 
image is resized and focused c . The Eurographics Association 2007. .rst on the LCD panel through a 
set of lenses. Then it is modulated by the LCD panel and projected through another lens system onto a 
larger screen. (a) (b) (c) Figure 23: Photographs of a part of an HDR projected image: image modulated 
with low resolution chrominance modula­tors (a), image modulated with a high resolution luminance modulator 
(b), output image (c). c .2006 ITE/SID [KKN*06] Kusakabe et al. proposed an HDR projector that applies 
LCoS panels that falls into the second category [KKN*06]. In this system, three low resolution (RGB) 
modulators are used .rst for chrominance modulation of the projection light. Finally, the light is modulated 
again with a high resolution luminance modulator which forms the image. The resolution of the panel that 
is applied for chrominance modulation can be much lower than the one for luminance modulation because 
the human visual system is sensitive only to a relatively low chrominance contrast. An experimental result 
is shown in .gure 23. The proposed projector has a contrast ratio of 1,100, 000 : 1. 6.4. High Speed 
High speed projector-camera systems hold the enormous po­tential to signi.cantly improve high frequent 
temporal coded projections (see sections 3.3 and 4.2). They enable, for in­ stance, projecting and capturing 
imperceptible spatial patterns that can be ef.ciently used for real-time geometric registra­tion, fast 
shape measurement and real-time adaptive radiomet­ric compensation while a .icker-free content is perceived 
by the observer at the same time. The faster the projection and the capturing process can be carried 
out, the more information per unit of time can be encoded. Since high speed capturing systems are well 
established, this section focuses mainly on the state-of-the-art of high speed projection systems. Both 
together, however, could be merged into future high speed projector-camera systems. For this reason, 
we .rst want to give only a brief overview of high speed capturing systems. . The Eurographics Association 
2007. Commercially available single-chip high speed cameras exist that can record 512x512 color pixels 
at up to 16,000 fps (FASTCAM SA1, Photron Ltd.). However, these systems are typically limited to storing 
just a few seconds of data directly on the camera because of the huge bandwidth that is necessary to 
transfer the images. Other CMOS devices are on the market that enable a 500 fps (A504k, Basler AG) capturing 
and transfer rates. Besides such single-camera systems, a high capturing speed can also be achieved with 
multi-camera arrays. Wilburn et al., for example, proposed a high speed video system for capturing 1,560 
fps videos using a dense array of 30 fps CMOS image sensors [WJV*04]. Their system captures and compresses 
images from 52 cameras in parallel. Even at ex­tremely high frame-rates, such a camera array architecture 
supports continuous streaming to disk from all of the cameras for minutes. In contrast to this, however, 
the frame-rate of commercially available DLP projectors is normally less than or equal to 120 fps (DepthQ 
R ., InFocus Corporation). Although faster projectors that can be used in the context of our projector­camera 
system are currently not available, we want to outline several projection approaches that achieve higher 
frame-rates -but do not necessarily allow the projection of high quality images. Raskar et al., for instance, 
developed a high speed optical motion capture system with an LED-based code projector [RNdD*07]. The 
system consists of a set of 1-bit gray code infrared LED beamers. Such a beamer array is effectively 
emitting 10,000 binary gray coded patterns per second, and is applied for object tracking. Each object 
to be tracked is tagged with a photosensor that detects and decodes the temporally projected codes. The 
3D location of the tags can be computed at a speed of 500 Hz when at least three such beamer arrays are 
applied. In contrast to this approach which does not intent to project pictorial content in addition 
to the code patterns, Nii et al. proposed a visible light communication (VLC) technique that does display 
simple images [NSI05]. They developed an LED-based high speed projection system (with a resolution of 
4x5 points produced with an equally large LED matrix) that is able to project alphabetic characters while 
applying an additional pulse modulation for coding information that is detected by photosensors. This 
system is able to transmit two data streams with 1 kHz and 2 kHz respectively at different locations 
while simultaneously projecting simple pictorial content. Although LEDs can be switched with a high speed 
(e.g., the LEDs in [NSI05] are temporally modulated at 10.7 MHz), such simple LED-based projection systems 
offer a too low spatial resolution at the moment. In principle, binary frame-rates of up to 16,300 fps 
can cur­rently be achieved with DMDs for a resolution of 1024x768. The DMD discovery board enables developers 
to implement their own mirror timings for special purpose application [DDS03]. Consequently, due to this 
high binary frame-rate some researchers utilized the Discovery boards for realizing high speed projection 
techniques. McDowall et al., for exam­ple, demonstrated the possibility of projecting 24 binary code 
and compensation images at a speed of 60 Hz [MBHF04]. Viewers used time-encoded shutter glasses to make 
individual images visible. Kitamura et al. also developed a high speed projector based on the DMD discovery 
board [KN06]. In their approach, photosensors can be used to detect temporal code patterns that are embedded 
into the mirror .ip sequence. In contrast to the approach by Cotting et al. [CNGF04] that was de­ scribed 
in section 3.3, the mirror .ip sequence can be freely re-con.gured. The results of an initial basic experiment 
with this system are shown in .gure 24a: The projected image is divided into 10 regions. Different on/off 
mirror .ip frequencies are used in each region (from 100 Hz to 1,000 Hz at 100 Hz intervals), while a 
uniformly bright image with a 50 % intensity appears in all regions -regardless of the locally applied 
frequencies. The intensity fall-off in the projection is mainly due to im­perfections in applied optics. 
The signal waves are received by photosensors that are placed within the regions. They can detect the 
individual frequency. (a) (b)  Figure 24: Regionally different mirror .ip frequencies and corresponding 
signal waves received by photosensors at dif­ferent image areas. The overall image appears mostly uni­form 
in intensity (a). Binary codes can be embedded into the .rst half of the exposure sequence while the 
second half can compensate the desired intensity (b). c .2006 IPSJ [KN06] Instead of using a constant 
on-off .ip frequency for each region, binary codes can be embedded into a projected frame. This is illustrated 
in .gure 24b: For a certain time slot of T , the .rst half of the exposure sequence contains a temporal 
code pattern (modulated with different mirror .ip states) that is compensated with the second half of 
the exposure sequence to modulate a desired intensity. Yet, contrast is lost in this case due to the 
modulated intensity level created by the code pattern. Here, the number of on-states always equals the 
number of off-states in the code period. This leads to a constant minimum intensity level of 25 %. Since 
also 25 % of the off states are used during this period, intensity values between 25 % and 75 % can only 
be displayed. All systems that have been outlined above, apply photo­sensors rather than cameras. Thus, 
they cannot be consid­ered as suitable projector-camera systems in our application context. Yet, McDowall 
et al. combined their high speed projector with a high speed camera to realize fast range scan­ning [MB05]. 
Takei et al. proposed a 3,000 fps shape mea­ surement system (shape reconstruction is performed off-line 
in this case) [TKH07]. In an image-based rendering context, Jones et al. pro­posed to simulate spatially 
varying lighting on a live perfor­mance based on a fast shape measurement using a high-speed projector-camsera 
system [JGB*06]. However, all of these approaches do not project pictorial image content, but rather 
represent encouraging examples of fast projector-camera tech­niques. The mirrors on a conventional DMD 
chip can be switched much faster than alternative technologies, such as ordinary LCD or LCoS panels whose 
refresh rate can be up to 2.5 ms (= 400 Hz) at the moment. LEDs are generally better suited for high-speed 
projectors than a conventional UHP lamp (we do not want to consider brightness issues for the moment), 
because three or more different LEDs that correspond to each color component can be switched at a high 
speed (even faster than a DMD) for modulating colors and intensities. Therefore, a combination of DMD 
and LED technologies seems to be optimal for future projection units. Let s assume that the mirrors of 
a regular DLP projector can be switched at 15µs (= 67,000 binary frames per second). For projecting 256 
different intensity levels (i.e., an 8 bit encoded gray scale image), the gray scale frame rate is around 
260 Hz (= 67,000 binary frames per second / 256 intensity levels). Consequently, the frame rate for full 
color images is around 85 Hz (= 260 gray scale frames per second / 3 color channels) if the color wheel 
consists of three .lter segments. Now, let s consider DLP projectors that apply LEDs in­stead of a UHP 
lamps and a color wheel. If, for example, the intensities of three (RGB) color LEDs can be switched between 
eight different levels (1,2,4,8,16,32,64,128,256) at a high speed, a full color image can theoretically 
be projected at around 2,800 Hz (= 67,000 binary frames per second / 8 (8-bit encoded) intensity levels 
/ 3 color channels). To overcome the bandwidth limitation for transferring c . The Eurographics Association 
2007. the huge amount of image data in high-speed, the MULE projector adopts a custom programmed FPGA-based 
cir­cuitry [JMY*07]. The FPGA decodes a standard DVI signal from the graphics card. Instead of rendering 
a color image, the FPGA takes each 24 bit color frame of video and dis­plays each bit sequentially as 
separate frames. Thus, if the incoming digital video signal is 60 Hz, the projector displays 60 × 24 
= 1,440 frames per second. To achieve even faster rates, the refresh rate of a video card is set at 180-240 
Hz. At 200 Hz, for instance, the projector can display 4,800 binary frames per second. 7. Conclusion 
This article reviewed the state-of-the-art of projector-camera systems with a focus on real-time image 
correction tech­niques that enable projections onto non-optimized surfaces. It did not discuss projector-camera 
related areas, such as camera supported photometric calibration of conventional projection displays (e.g., 
[BMY05], [JM07], [BM07]), real-time shadow removal techniques (e.g., [STJS01], [JWS*01], [JWS04]), or 
projector-camera based interaction approaches (e.g., [Pin01], [EHH04], [FR06]). While most of the presented 
techniques are still on a re­search level, others found already practical applications in theatres, museums, 
historic sites, open-air festivals, trade shows, and advertisement. Some examples are shown in .g­ures 
25-27. Future projectors will become more compact in size and will require little power and cooling. 
Re.ective technology (such as DLP or LCOS) will more and more replace trans­missive technology (e.g., 
LCD). This leads to an increased brightness and extremely high update rates. They will in­tegrate GPUs 
for real-time graphics and vision processing. While resolution and contrast will keep increasing, produc­tion 
costs and market prizes will continue to fall. Conven­tional UHP lamps will be replaced by powerful LEDs 
or multi-channel lasers. This will make them suitable for mobile applications. Imagining projector-camera 
technology to be integrated into, or coupled with mobile devices, such as cellphones or laptops, will 
support a truly .exible way for presentations. There is no doubt that this technology is on its way. 
Yet, one question needs to be addressed when thinking about mobile projectors: What can we project onto, 
without car­rying around screen canvases? It is clear that the answer to this question can only be: Onto 
available everyday surfaces. With this in mind, the future importance of projector-camera systems in 
combination with appropriate image correction techniques becomes clear. Acknowledgments We wish to thank 
the entire ARGroup at the Bauhaus-University Weimar who were involved in developing . The Eurographics 
Association 2007. projector-camera techniques over the last years, as well as the authors who gave permission 
to use their images in this article. Special thanks go to Stefanie Zollmann and Mel for proof-reading. 
Projector-camera activities at BUW were par­tially supported by the Deutsche Forschungsgemeinschaft (DFG) 
under contract numbers BI 835/1-1 and PE 1183/1-1. References [AA01] AGGARWAL M., AHUJA N.: Split Aperture 
Imag­ing for High Dynamic Range. In Proc. of IEEE Inter­national Conference on Computer Vision (ICCV) 
(2001), vol. 2, pp. 10 17. [AOSS06] ASHDOWN M., OKABE T., SATO I., SATO Y.: Robust Content-Dependent 
Photometric Projector Com­pensation. In Proc. of IEEE International Workshop on Projector-Camera Systems 
(ProCams) (2006). [ASOS07] ASHDOWN M., SATO I., OKABE T., SATO Y.: Perceptual Photometric Compensation 
for Projected Im­ages. IEICE Transaction on Information and Systems J90-D, 8 (2007), 2115 2125. in Japanese. 
[AU05] ALLEN W., ULICHNEY R.: Wobulation: Dou­bling the Addressed Resolution of Projection Displays. 
In Proc. of SID Symposium Digest of Technical Papers (2005), vol. 36, pp. 1514 1517. [BCK*05] BIMBER 
O., CORIAND F., KLEPPE A., BRUNS E., ZOLLMANN S., LANGLOTZ T.: Superimposing Pic­torial Artwork with 
Projected Imagery. IEEE MultiMedia 12, 1 (2005), 16 26. [BDD*04] BIEHLING W., DETER C., DUBE S., HILL 
B., HELLING S., ISAKOVIC K., KLOSE S., SCHIEWE M.: LaserCave -Some Building Blocks for Immersive Screens 
-. In Proc. of International Status Conference Virtual and Augmented Reality (2004). [BE06] BIMBER O., 
EMMERLING A.: Multifocal Pro­jection: A Multiprojector Technique for Increasing Focal Depth. IEEE Transactions 
on Visualization and Computer Graphics (TVCG) 12, 4 (2006), 658 667. [BEK05] BIMBER O., EMMERLING A., 
KLEMMER T.: Embedded Entertainment with Smart Projectors. IEEE Computer 38, 1 (2005), 56 63. [BGZ*06] 
BIMBER O., GRUNDHÖFER A., ZEIDLER T., DANCH D., KAPAKOS P.: Compensating Indirect Scat­tering for Immersive 
and Semi-Immersive Projection Dis­plays. In Proc. of IEEE Virtual Reality (IEEE VR) (2006), pp. 151 158. 
[Bim06] BIMBER O.: Projector-Based Augmentation. In Emerging Technologies of Augmented Reality: Interfaces 
and Design, Haller M., Billinghurst M., Thomas B., (Eds.). Idea Group, 2006, pp. 64 89. [BJM07] BHASKER 
E. S., JUANG R., MAJUMDER A.: Registration techniques for using imperfect and par tially calibrated devices 
in planar multi-projector displays. IEEE Trans. Vis. Comput. Graph. 13, 6 (2007), 1368 1375. [BM07] 
BHASKER E., MAJUMDER A.: Geometric Mod­eling and Calibration of Planar Multi-Projector Displays using 
Rational Bezier Patches. In Proc. of IEEE Interna­tional Workshop on Projector-Camera Systems (ProCams) 
(2007). [BMS98] BATLLE J., MOUADDIB E. M., SALVI J.: Re­cent progress in coded structured light as a 
technique to solve the correspondence problem: a survey. Pattern Recog­nition 31, 7 (1998), 963 982. 
[BMY05] BROWN M., MAJUMDER A., YANG R.: Cam­era Based Calibration Techniques for Seamless Multi-Projector 
Displays. IEEE Transactions on Visualization and Computer Graphics (TVCG) 11, 2 (2005), 193 206. [BSC06] 
BROWN M. S., SONG P., CHAM T.-J.: Image Pre-Conditioning for Out-of-Focus Projector Blur. In Proc. of 
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2006), vol. II, pp. 1956 1963. [BWEN05] 
BIMBER O., WETZSTEIN G., EMMERLING A., NITSCHKE C.: Enabling View-Dependent Stereo­scopic Projection 
in Real Environments. In Proc. of IEEE/ACM International Symposium on Mixed and Aug­mented Reality (ISMAR) 
(2005), pp. 14 23. [CKS98] CASPI D., KIRYATI N., SHAMIR J.: Range Imag­ing With Adaptive Color Structured 
Light. IEEE TRANS-ACTIONS ON PATTERN ANALYSIS AND MACHINE IN-TELLIGENCE 20, 5 (1998), 470 480. [CNGF04] 
COTTING D., NÄF M., GROSS M. H., FUCHS H.: Embedding Imperceptible Patterns into Projected Im­ages for 
Simultaneous Acquisition and Display. In Proc. of IEEE/ACM International Symposium on Mixed and Aug­mented 
Reality (ISMAR) (2004), pp. 100 109. [CZGF05] COTTING D., ZIEGLER R., GROSS M. H., FUCHS H.: Adaptive 
Instant Displays: Continuously Cali­brated Projections using Per-Pixel Light Control. In Proc. of Eurographics 
(2005), pp. 705 714. [DDS03] DUDLEY D., DUNCAN W. M., SLAUGHTER J.: Emerging Digital Micromirror Device 
(DMD) Applica­tions. In Proc. of SPIE (2003), vol. 4985, pp. 14 25. [DM97] DEBEVEC P. E., MALIK J.: Recovering 
High Dynamic Range Radiance Maps from Photographs. In Proc. of ACM SIGGRAPH (1997), pp. 369 378. [DRW*06] 
DEBEVEC P., REINHARD E., WARD G., MYSZKOWSKI K., SEETZEN H., ZARGARPOUR H., MC-TAGGART G., HESS D.: High 
Dynamic Range Imaging: Theory and Applications. In Proc. of ACM SIGGRAPH (Courses) (2006). [DSW*07] DAMBERG 
G., SEETZEN H., WARD G., HEI-DRICH W., WHITEHEAD L.: High-Dynamic-Range Pro­jection Systems. In Proc. 
of SID Symposium Digest of Technical Papers (2007), vol. 38, pp. 4 7. [DVC07] DAMERA-VENKATA N., CHANG 
N. L.: Re­alizing Super-Resolution with Superimposed Projection. In Proc. of IEEE International Workshop 
on Projector-Camera Systems (ProCams) (2007). [EHH04] EHNES J., HIROTA K., HIROSE M.: Projected Augmentation 
-Augmented Reality using Rotatable Video Projectors. In Proc. of IEEE/ACM International Sympo­sium on 
Mixed and Augmented Reality (ISMAR) (2004), pp. 26 35. [FGN05] FUJII K., GROSSBERG M., NAYAR S.: A Projector-Camera 
System with Real-Time Photometric Adaptation for Dynamic Environments. In Proc. of IEEE Conference on 
Computer Vision and Pattern Recognition (CVPR) (2005), vol. I, pp. 814 821. [FR06] FLAGG M., REHG J. 
M.: Projector-Guided Paint­ing. In Proc. of ACM Symposium on User Interface Soft­ware and Technology 
(UIST) (2006), pp. 235 244. [GB07] GRUNDHÖFER A., BIMBER O.: Real-Time Adap­tive Radiometric Compensation. 
To appear in IEEE Trans­actions on Visualization and Computer Graphics (TVCG) (2007). [GB08] GROSSE M., 
BIMBER O.: Coded aperture projec­tion, 2008. [GPNB04] GROSSBERG M., PERI H., NAYAR S., BEL-HUMEUR P.: 
Making One Object Look Like Another: Controlling Appearance using a Projector-Camera System. In Proc. 
of IEEE Conference on Computer Vision and Pat­tern Recognition (CVPR) (Jun 2004), vol. I, pp. 452 459. 
[GSHB07] GRUNDHÖER A., SEEGER M., HÄNTSCH F., BIMBER O.: Dynamic Adaptation of Projected Impercep­tible 
Codes. Proc. of IEEE International Symposium on Mixed and Augmented Reality (2007). [HSM07] HABE H., 
SAEKI N., MATSUYAMA T.: Inter-Re.ection Compensation for Immersive Projection Dis­play. In Proc. of IEEE 
International Workshop on Projector-Camera Systems (ProCams) (poster) (2007). [JF07] JOHNSON T., FUCHS 
H.: Real-Time Projector Tracking on Complex Geometry using Ordinary Imagery. In Proc. of IEEE International 
Workshop on Projector-Camera Systems (ProCams) (2007). [JGB*06] JONES A., GARDNER A., BOLAS M., MC-DOWALL 
I., DEBEVEC P.: Simulating Spatially Varying Lighting on a Live Performance. In Proc. of European Conference 
on Visual Media Production (CVMP) (2006), pp. 127 133. [JM07] JUANG R., MAJUMDER A.: Photometric Self-Calibration 
of a Projector-Camera System. In Proc. of IEEE International Workshop on Projector-Camera Sys­tems (ProCams) 
(2007). [JMY*07] JONES A., MCDOWALL I., YAMADA H., BO-LAS M., DEBEVEC P.: Rendering for an Interactive 
360° Light Field Display. In Proc. of ACM SIGGRAPH (2007). c . The Eurographics Association 2007. [JR03] 
JAYNES C., RAMAKRISHNAN D.: Super-Resolution Composition in Multi-Projector Displays. In Proc. of IEEE 
International Workshop on Projector-Camera Systems (ProCams) (2003). [JWS*01] JAYNES C., WEBB S., STEELE 
R., BROWN M., SEALES W.: Dynamic Shadow Removal from Front Projection Displays. In Proc. of IEEE Visualization 
(2001), pp. 175 555. [JWS04] JAYNES C., WEBB S., STEELE R. M.: Camera-Based Detection and Removal of 
Shadows from Interactive Multiprojector Displays. IEEE Transactions on Visualiza­tion and Computer Graphics 
(TVCG) 10, 3 (2004), 290 301. [KKN*06] KUSAKABE Y., KANAZAWA M., NOJIRI Y., FURUYA M., YOSHIMURA M.: 
YC-separation Type Pro­ jector with Double Modulation. In Proc. of International Display Workshop (IDW) 
(2006), pp. 1959 1962. [KN06] KITAMURA M., NAEMURA T.: A Study on Position-Dependent Visible Light Communication 
using DMD for ProCam. In IPSJ SIG Notes. CVIM-156 (2006), pp. 17 24. in Japanese. [MB05] MCDOWALL I. 
E., BOLAS M.: Fast Light for Dis­play, Sensing and Control Applications. In Proc. of IEEE VR 2005 Workshop 
on Emerging Display Technologies (EDT) (2005), pp. 35 36. [MBHF04] MCDOWALL I. E., BOLAS M. T., HOBER-MAN 
P., FISHER S. S.: Snared Illumination. In Proc. of ACM SIGGRAPH (Emerging Technologies) (2004), p. 24. 
[MKO06] MUKAIGAWA Y., KAKINUMA T., OHTA Y.: An­alytical Compensation of Inter-re.ection for Pattern Pro­jection. 
In Proc. of ACM Symposium on Virtual Reality Software and Technology (VRST) (short paper) (2006), pp. 
265 268. [MW01] MAJUMDER A., WELCH G.: COMPUTER GRAPHICS OPTIQUE: Optical Superposition of Pro­jected 
Computer Graphics. In Proc. of Immersive Pro­jection Technology -Eurographics Workshop on Virtual Environment 
(IPT-EGVE) (2001). [NB03] NAYAR S. K., BRANZOI V.: Adaptive Dynamic Range Imaging: Optical Control of 
Pixel Exposures over Space and Time. In Proc. of IEEE International Confer­ence on Computer Vision (ICCV) 
(2003), vol. 2, pp. 1168 1175. [NBB04] NAYAR S. K., BRANZOI V., BOULT T. E.: Pro­grammable Imaging using 
a Digital Micromirror Array. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
(2004), vol. I, pp. 436 443. [NKGR06] NAYAR S., KRISHNAN G., GROSSBERG M. D., RASKAR R.: Fast Separation 
of Direct and Global Components of a Scene using High Frequency Illumina­tion. In Proc. of ACM SIGGRAPH 
(2006), pp. 935 944. [NPGB03] NAYAR S. K., PERI H., GROSSBERG M. D., . The Eurographics Association 2007. 
BELHUMEUR P. N.: A Projection System with Radiomet­ric Compensation for Screen Imperfections. In Proc. 
of IEEE International Workshop on Projector-Camera Sys­tems (ProCams) (2003). [NSI05] NII H., SUGIMOTO 
M., INAMI M.: Smart Light-Ultra High Speed Projector for Spatial Multiplexing Opti­cal Transmission. 
In Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams) (2005). [OS07] OYAMADA 
Y., SAITO H.: Focal Pre-Correction of Projected Image for Deblurring Screen Image. In Proc. of IEEE International 
Workshop on Projector-Camera Sys­tems (ProCams) (2007). [Pin01] PINHANEZ C.: Using a Steerable Projector 
and a Camera to Transform Surfaces into Interactive Displays. In Proc. of CHI (extended abstracts) (2001), 
pp. 369 370. [PLJP07] PARK H., LEE M.-H., JIN B.-K. S. Y., PARK J.-I.: Content adaptive embedding of 
complementary patterns for nonintrusive direct-projected augmented reality. In HCI International 2007 
(2007), vol. 14. [PLKP05] PARK H., LEE M.-H., KIM S.-J., PARK J.-I.: Specularity-Free Projection on Nonplanar 
Surface. In Proc. of Paci.c-Rim Conference on Multimedia (PCM) (2005), pp. 606 616. [PLKP06] PARK H., 
LEE M.-H., KIM S.-J., PARK J.-I.: Contrast Enhancement in Direct-Projected Augmented Reality. In Proc. 
of IEEE International Conference on Multimedia and Expo (ICME) (2006). [PLS*06] PARK H., LEE M.-H., SEO 
B.-K., SHIN H.-C., PARK J.-I.: Radiometrically-Compensated Projection onto Non-Lambertian Surface using 
Multiple Overlapping Projectors. In Proc. of Paci.c-Rim Symposium on Image and Video Technology (PSIVT) 
(2006), pp. 534 544. [PPK03] PARK S. C., PARK M. K., KANG M. G.: Super-Resolution Image Reconstruction: 
A Technical Overview. IEEE Signal Processing Magazine 20, 3 (2003), 21 36. [PS05] PAVLOVYCH A., STUERZLINGER 
W.: A High-Dynamic Range Projection System. In Proc. of SPIE (2005), vol. 5969. [Ras99] RASKAR R.: Oblique 
Projector Rendering on Pla­nar Surfaces for a Tracked User. In Proc. of ACM SIG-GRAPH (Sketches and Applications) 
(1999). [RBvB*04] RASKAR R., BEARDSLEY P., VAN BAAR J., WANG Y., DIETZ P., LEE J., LEIGH D., WILLWACHER 
T.: RFIG Lamps: Interacting with a Self-Describing World via Photosensing Wireless Tags and Projectors. 
In Proc. of ACM SIGGRAPH (2004), pp. 406 415. [RBY*99] RASKAR R., BROWN M., YANG R., CHEN W., WELCH G., 
TOWLES H., SEALES B., FUCHS H.: Multi-Projector Displays using Camera-Based Registration. In Proc. of 
IEEE Visualization (1999), pp. 161 168. [RNdD*07] RASKAR R., NII H., DE DECKER B., HASHIMOTO Y., SUMMET 
J., MOORE D., ZHAO Y., WESTHUES J., DIETZ P., INAMI M., NAYAR S. K., BARNWELL J., NOLAND M., BEKAERT 
P., BRANZOI V., BRUNS E.: Prakash: Lighting Aware Motion Capture using Photosensing Markers and Multiplexed 
Illuminators. In Proc. of ACM SIGGRAPH (2007). [RPG99] RAMASUBRAMANIAN M., PATTANAIK S. N., GREENBERG 
D. P.: A Perceptually Based Physical Error Metric for Realistic Image Synthesis. In Proc. of ACM SIGGRAPH 
(1999), pp. 73 82. [RvBWR04] RASKAR R., VAN BAAR J., WILLWACHER T., RAO S.: Quadric transfer for immersive 
curved screen displays. In Proc. of Eurographics (Aug 2004). [RWC*98] RASKAR R., WELCH G., CUTTS M., 
LAKE A., STESIN L., FUCHS H.: The Of.ce of the Future: A Uni.ed Approach to Image-Based Modeling and 
Spatially Immersive Displays. In Proc. of ACM SIGGRAPH (1998), pp. 179 188. [RWPD06] REINHARD E., WARD 
G., PATTANAIK S., DE-BEVEC P.: High Dynamic Range Imaging -Acquisition, Display and Image-Based Lighting. 
Morgan Kaufmann, 2006. [SCG*05] SEN P., CHEN B., GARG G., MARSCHNER S. R., HOROWITZ M., LEVOY M., LENSCH 
H. P. A.: Dual Photography. In Proc. of ACM SIGGRAPH (2005), pp. 745 755. [SHS*04] SEETZEN H., HEIDRICH 
W., STUERZLINGER W., WARD G., WHITEHEAD L., TRENTACOSTE M., GHOSH A., VOROZCOVS A.: High Dynamic Range 
Display Systems. In Proc. of ACM SIGGRAPH (2004), pp. 760 768. [SMK05] SEITZ S. M., MATSUSHITA Y., KUTULAKOS 
K. N.: A Theory of Inverse Light Transport. In Proc. of IEEE International Conference on Computer Vision 
(ICCV) (2005), vol. 2, pp. 1440 1447. [SMO03] SHIRAI Y., MATSUSHITA M., OHGURO T.: HIEI Projector: Augmenting 
a Real Environment with Invisible Information. In Proc. of Workshop on Interac­tive Systems and Software 
(WISS) (2003), pp. 115 122. in Japanese. [SPB04] SALVI J., PAGÈS J., BATLLE J.: Pattern Codi­.cation 
Strategies in Structured Light Systems. Pattern Recognition 37, 4 (2004), 827 849. [STJS01] SUKTHANKAR 
R., TAT-JEN C., SUKTHANKAR G.: Dynamic Shadow Elimination for Multi-Projector Displays. In Proc. of IEEE 
Conference on Computer Vision and Pattern Recognition (CVPR) (2001), vol. II, pp. 151 157. [TKH07] TAKEI 
J., KAGAMI S., HASHIMOTO K.: 3,000­fps 3-D Shape Measurement Using a High-Speed Camera-Projector System. 
In Proc. of IEEE/RSJ International Con­ ference on Intelligent Robots and Systems (IROS) (2007), pp. 
3211 3216. [VVSC05] VIEIRA M. B., VELHO L., SA A., CARVALHO P. C.: A Camera-Projector System for Real-Time 
3D Video. In Proc. of IEEE International Workshop on Projector-Camera Systems (ProCams) (2005). [WB07] 
WETZSTEIN G., BIMBER O.: Radiometric Com­pensation through Inverse Light Transport. Proc. of Paci.c Graphics 
(2007). [WJV*04] WILBURN B., JOSHI N., VAISH V., LEVOY M., HOROWITZ M.: High-Speed Videography using 
a Dense Camera Array. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2004), 
vol. II, pp. 294 301. [WJV*05] WILBURN B., JOSHI N., VAISH V., TALVALA E.-V., ANTUNEZ E., BARTH A., 
ADAMS A., HOROWITZ M., LEVOY M.: High Performance Imaging using Large Camera Arrays. In Proc. of ACM 
SIGGRAPH (2005), pp. 765 776. [WSOS05] WANG D., SATO I., OKABE T., SATO Y.: Ra­diometric Compensation 
in a Projector-Camera System Based on the Properties of Human Vision System. In Proc. of IEEE International 
Workshop on Projector-Camera Sys­tems (ProCams) (2005). [WWC*05] WASCHBÜSCH M., WÜRMLIN S., COTTING D., 
SADLO F., GROSS M. H.: Scalable 3D Video of Dynamic Scenes. The Visual Computer 21, 8-10 (2005), 629 
638. [YHS03] YOSHIDA T., HORII C., SATO K.: A Virtual Color Reconstruction System for Real Heritage with 
Light Projection. In Proc. of International Conference on Virtual Systems and Multimedia (VSMM) (2003), 
pp. 161 168. [YW01] YANG R., WELCH G.: Automatic and Continu­ous Projector Display Surface Calibration 
using Every-Day Imagery. In Proc. of International Conference in Central Europe on Computer Graphics, 
Visualization and Com­puter Vision (WSCG) (2001). [ZB07] ZOLLMANN S., BIMBER O.: Imperceptible Cali­bration 
for Radiometric Compensation. In Proc. of Euro­graphics (short paper) (2007), pp. 61 64. [ZLB06] ZOLLMANN 
S., LANGLOTZ T., BIMBER O.: Passive-Active Geometric Calibration for View-Dependent Projections onto 
Arbitrary Surfaces. Proc. of Workshop on Virtual and Augmented Reality of the GI-Fachgruppe AR/VR 2006 
(re-print to appear in Journal of Virtual Real­ity and Broadcasting 2007) (2006). [ZN06] ZHANG L., NAYAR 
S. K.: Projection Defocus Analysis for Scene Capture and Image Display. In Proc. of ACM SIGGRAPH (2006), 
pp. 907 915. c . The Eurographics Association 2007.  . The Eurographics Association 2007.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Deutsche Forschungsgemeinschaft (DFG)</funding_agency>
			<grant_numbers>
				<grant_number>BI 835/1-1PE 1183/1-1</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>1401240</article_id>
		<sort_key>1080</sort_key>
		<display_label>Article No.</display_label>
		<pages>48</pages>
		<display_no>85</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Mobile projectors and optical communication]]></title>
		<page_from>1</page_from>
		<page_to>48</page_to>
		<doi_number>10.1145/1401132.1401240</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401240</url>
		<abstract>
			<par><![CDATA[<p>Micro and Pico portable projectors are still in development phase. Excellent review at http://www.economist.com/science/tq/displaystory.cfm?story_id=10789401. So called pocket projectors, e.g. from Mitsubishi Electric, are already available.</p> <p>We will look at research in mobile projectors and the interaction with real world. Then we will explore the opportunities in optical communication with projectors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>B.4.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>B.4.3</cat_node>
				<descriptor>Fiber optics</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010600.10010602.10010605</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Interconnect->Photonic and optical interconnect</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098868</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Lab, MIT, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mobile Projectors and Optical Communication  Mobility and Communication  Portable Projectors Technology 
and issues  Projector Interaction Single-handed interaction, Image stabilization and resizing  Research 
Prototypes iLamps: Geometrically aware pocket projectors  Optical communication for space labeling 
in robotics, games  Optical and Radio Frequency Tags  RFID for Augmented Reality: Location sensing 
RFID and automatic authoring   Imperceptible projection  High speed motion capture (Prakash system) 
Micro and Pico portable projectors are still in development phase. Excellent review at http://www.economist.com/science/tq/displaystory.cfm?story_id=10789401. 
So called pocket projectors, e.g. from Mitsubishi Electric, are already available. We will look at research 
in mobile projectors and the interaction with real world. Then we will explore the opportunities in optical 
communication with projectors.  Race for Space Smallest display device with Largest image Projective 
Devices Courtesy: Microvision, Inc.  Eye-worn display  Rollable Screens  Retinal Display   There 
are only a limited solution to create a large display from a small device. Projectors are showing the 
potential to create new ways of interacting with information in everyday life. Desktop screens, laptops 
and TVs have a basic constraint on their size they can never be smaller than the display area. Hand-helds 
such as PDAs are compact but the display size is too limited for many uses. In contrast, projectors of 
the near future will be compact, portable, and with the built-in awareness which will enable them to 
automatically create satisfactory displays on many of the surfaces in the everyday environment.  On 
the left is a standard DLP chip that you might find in a large-screen TV. On the right is a DLP for a 
projector phone. Although smaller, the chip can still project fairly large images. By putting these chips 
into phones, TI hopes to make watching video on phones more pleasing and, of course, sell more chips. 
A complete pico projector. It fits into a phone. The projector contains three lasers, a DLP chip and 
a power supply and measures about 1.5 inches in length. A mock-up of TI's functioning projector in a 
phone. Granted, the phone's a little larger than a lot of the phones on the market today, but those don't 
have projectors in them. http://www.news.com/TI-demos-its-movie-projector-in-a-phone/2100-1041_3­6170619.html?tag=ne.gall.related 
 Microvision s ultra-miniature full-color digital projection display approximately the size of a Thin 
Mint candy, designed to be embedded into handheld electronic devices such as cellphones, PDAs, or multimedia 
handhelds. (January 2007 Consumer Electronic Show). The projector developed at Microvision is composed 
of two main parts: a set of red, blue, and green lasers made of semiconductor material, such as gallium 
indium arsenide, and a mirror--one millimeter across--that tilts on two axes. The lasers shine on the 
mirror, and the mirror reflects the pixel of light onto a wall or other surface. The intensities of the 
lasers change to produce different colors: when all three are pumping out light full blast, the pixel 
is white; when all three are off, the pixel is black. Other colors are produced from various combinations 
in between. As the lasers flash on the mirror, the mirror gimbals on its two axes, flickering to produce 
30 million pixels a second, each illuminating a surface for 20 nanoseconds. Using this laser and single-mirror 
setup, the projector paints a scene onto a surface one pixel at a time, says Sprague. It does this so 
quickly that our eyes perceive a static image or a continuous movie. One of the challenges is to design 
a rapidly gyrating mirror that can coordinate with the lasers that turn on and off 100 million times 
a second. Integrated into the Microvision mirror are silicon mechanical structures that measure strain 
on the mirror, detecting what position it's in. This information is fed back into the laser modulator--the 
device that determines when a laser is emitting light or not--and the feedback loop allows the system 
to constantly adjust, depending on the demands of the projected image. The mirror, its mount, and the 
other mechanical components are all made of silicon, putting the projector in a class of device called 
MEMS (microelectromechanical systems). http://www.technologyreview.com/Biztech/17860/ LBO s approach 
to miniature projection has a range of differentiating features and benefits. The term holographic refers 
not to the projected image, but to the method of projection. A diffraction pattern of the desired 2D 
image, calculated using LBO s patented holographic algorithms, is displayed on a custom-designed phase-modulating 
reflective Liquid Crystal on Silicon (LCOS) microdisplay. When illuminated by coherent laser light, the 
desired 2D image is projected. The projector uses a reflective LCoS array to create a constantly varying 
diffraction pattern that is carefully calculated to produce the desired two-dimensional image when illuminated 
by red, green and blue lasers. The resulting image is in focus at all distances from the projector, 
and the projector has no moving parts. The drawback of this approach is that the LCoS array takes up 
more space than a single mirror. But the nature of the technology means that the size of the array can 
be kept to a minimum. Because the array displays a diffraction pattern, not the actual image, the resolution 
of the projected image can be higher than that of the array. LBO's current prototype projects a 1,600-by-1,024-pixel 
image using an 864-by-480-pixel array.  There has been significant research in modifying and interfacing 
projectors for novel scenarios and applications. Two of the projects we will discuss : iLamps and RFIG 
lamps. iLamps stands for Intelligent Locale-aware Mobile Projectors. The idea is to augment a mobile 
phone with a projector, camera and tilt sensor. Projectors are currently undergoing a transformation 
as they evolve from static output devices to portable, environment-aware, communicating systems. An enhanced 
projector can determine and respond to the geometry of the display surface, and can be used in an ad-hoc 
cluster to create a self-configuring display. Information display is such a prevailing part of everyday 
life that new and more flexible ways to present data are likely to have significant impact. This project 
examines geometrical issues for enhanced projectors, relating to customized projection for different 
shapes of display surface, object augmentation, and co­operation between multiple units. Consider interactive 
projection to allow a user to interact with projected information e.g. to navigate or update the projected 
information. A single-handed desktop-like interaction with projected illumination is possible.  Here 
we introduce interactive projection, allowing a user with a handheld projector to do mouse-style interaction 
with projected information. This is achieved by treating a projection as conceptually having two parts 
 a stabilized component that is static in the display surface, and a cursor that follows any user pointing 
motion of the projector effectively allowing the user to track a cursor across a projection. Accompanying 
mouse buttons are used to do selection. Interactive projection is not specific to a tagged environment, 
but the technologies meld well. In creating a handheld projector, an immediate problem that arises is 
that hand-jitter results in jitter in the projection. The core requirement to deal with this problem 
is to compute the pose of the projector relative to the display surface. Quasi-stabilization preserves 
the form of the projection up to an unknown translation in the plane i.e. it preserves projection shape, 
size and orientation. But the projection translates in the display plane in accordance with projector 
motion parallel to the plane. Absolute stabilization is possible when the camera is viewing four or 
more fixed surface points in general position, or the equivalent, on a planar display surface. Our goal 
is to find a homography H between the projector image plane and a fixed coordinate frame on the surface. 
Homography H specifies the mapping between each pixel on the projector image plane and the fixed surface 
coordinate frame. Hence we can use its inverse inv(H) to transform from a desired (fixed) projection 
on the surface to the projector image plane, thereby determining the required projector image to give 
the fixed projection. Video  A new way to play tic - tac - toe  Handheld projector.  Grid projects 
to a fixed position on the wall.  Cursor is guided by pointing the projector.  Playing game is a nice 
way to demonstrate various functionalities. Handheld devices will soon have the ability to project information 
onto any surface, thus enabling interfaces that are not possible with current handhelds. The authors 
in Toronto explore the design space of dynamically defining and interacting with multiple virtual information 
spaces embedded in a physical environment using a handheld projector and a passive pen tracked in 3D. 
They develop techniques for defining and interacting with these spaces. They extend the prior single-user 
research to co-located multi-user interaction using multiple handheld projectors. They present a set 
of interaction techniques for supporting co-located collaboration with multiple handheld projectors, 
and discuss application scenarios enabled by them. Handheld projectors provide interesting design challenges 
compared to other co-located collaborative settings such as a shared tabletop display. For example, users 
can create their individual displays with their projectors, allowing for easy support of personalized 
views, which is seldom the case in other settings.  Object Adaptive Projection  Method  Passive elements 
 Identification of objects  Pose of projector  Gesture interaction   Benefits  HMD: tracking 
issues  PDA: Last foot problem   Let us look at object augmentation using a hand-held projector, 
including a technique for doing mouse-style interaction with the projected data. Common to some previous 
approaches, we do object recognition by means of fiducials attached to the object of interest. Our fiducials 
are piecodes , colored segmented circles, which allow thousands of distinct colorcodings. As well as 
providing identity, these fiducials are used to compute camera pose (location and orientation) and hence 
projector pose since the system is fully calibrated. With projector pose known relative to a known object, 
content can be overlaid on the object as required.  AR Issues  Preprocessing: Authoring  Runtime: 
 Identification: Recognition of objects Using markers and visual tags  Registration: Finding relative 
pose of display device  Dynamic estimate of translation and rotation  Render/Warp images   Interaction: 
  Widgets, Gesture recognition, Visual feedback The pie-codes help us solve two critical run-time 
components of an AR system. Video A hand-held projector can use various aspects of its context when 
projecting content onto a recognized object. We use proximity to the object to determine level-of-detail 
for the content. Other examples of context for content control would be gestural motion, history of use 
in a particular spot, or the presence of other devices for cooperative projection. The main uses of object 
augmentation are (a) information displays on objects, either passive display, or training applications 
in which instructions are displayed as part of a sequence; (b) physical indexing in which a user is guided 
through an environment or storage bins to a requested object; (c) indicating electronic data items which 
have been attached to the environment. Related work includes the Magic Lens [Bier et al. 1993], Digital 
Desk [Wellner 1993], computer augmented interaction with real-world environments [Rekimoto and Nagao 
1995], and Hyper mask [Yotsukura et al. 2002]. AR Issues  Preprocessing: Authoring  Runtime:  Identification: 
Recognition of objects  Using markers and visual tags  Dynamic estimate of translation and rotation 
 Render/Warp images   Interaction:   Widgets, Gesture recognition, Visual feedback The pie-codes 
however can be replaced with radio frequency identification tags (RFID tags). The first large-scale 
use of passive-RFID tags is expected to be for inventory control as part of logistics (a US$900 billion 
industry), so we turn to a scenario in a warehouse -locating objects with a required property and annotating 
them. Mitsubishi Electric Research Laboratories Special Effects in the Real World Raskar 2007 Conventional 
Passive RFID Micro Memory Controller Micro Computer Memory Controller READER Conventional tag communication 
works by broadcast from an RFreader, with response from all in-range tags. Limiting the communication 
to a required tag is traditionally achieved using a shortrange tag-reader and close physical placement 
with the tag. Powered radio-frequency tags currently use a battery that is about the size of a watch-battery, 
have a lifetime of a few years, and have a cost of a few dollars. In contrast, passive RFID tags are 
unpowered, can be as small as a grain of rice, and cost tens of cents [Want 2003]. Prices of both are 
dropping but the price differential will remain. The size and cost properties are such that RFID is showing 
signs of being adopted as a mass-deployment technology. Current commercial applications including embedding 
of RFID tags in packaging for inventory control, non-contact access control, and ear tags for livestock. 
Despite the interest in RFID, the available functionality is very limited an RF-reader broadcasts a 
request, and in-range tags (collect energy from the RF pulse and) reply. The work is motivated by the 
observation that RFID is showing the potential to be a ground-breaking, pervasive technology, yet current 
functionality is limited. RF beacons or tag transmit devices references but without the ability to point 
and without visual feedback. A library scenario: Finding which books are on the shelf within the RF range 
is easy with a traditional (handheld) RF reader. But how can one find out which books are out of the 
alphabetically sorted order ? With passive photosensing RFID attached to each book we can find the exact 
location of each book. So, one can verify if there is a mismatch between the list of books sorted by 
title versus list of books sorted by position coordinates. The mismatch can also be indicated by projecting 
the arrows back on the shelf indicating the correct position. (Green arrows.) We can also find out if 
any book is placed upside down. We attach two tags, one at the top and one at the bottom. Books for which 
the location of the two tags is reversed is marked as upside down. This is indicated visually with red 
arrows. AR Issues  Preprocessing: Authoring  Runtime:  Dynamic estimate of translation and rotation 
 Render/Warp images  Widgets, Gesture   Interaction:  Let us see how a combination of photosensing 
RFID tags and pocket projectors allows us to solve AR problems in a new imperceptible way. We augment 
each tag with a photo-sensor to significantly extend the current functionality and support radio frequency 
identity and geometry (RFIG) discovery. The ability to address and wirelessly access distributed photosensors 
creates a unique opportunity. We recover geometric information, such as 3D location of tags or shape 
history of tagged objects, and exploit the associated geometric operations to bring the RF tags into 
the realm of computer vision and computer graphics. The key issue in evolving our active tag system 
to passive tags would be power. We only allow computation and sensing consistent with the size and power 
levels we felt were achievable on a passive RFID system. For example, (a) tags are not photo-sensing 
or computing until woken up by the RF reader and (b) we do not have a light emitting diode (LED) on the 
tag as a visual beacon to a human or camera-based system because it would be power-hungry. Also note 
that the tags incorporate a photo-sensor, so a passive version could draw power not just from the RF 
channel, but also from the incident light. Of course, there would be significant engineering challenges 
in moving from active to passive RFID. For example, a warehouse employee identifies food products that 
are close to expiry date and annotates an instruction to trash them. If these products were items on 
a computer desktop, this could be done with a few clicks. Our goal is to craft a scheme in the physical 
world that maintains the simplicity of the computer environment. In the computer, the items are files, 
and the interface is via keyboard, mouse, and display. In the physical world, the items are tagged objects, 
and the interface uses a handheld projector and user interaction directly through the projected information. 
 Video  RFID (Radio Frequency Identification) RFIG (Radio Frequency Id and Geometry)  Laser-guided 
robot. Guiding a robot to pick a certain object in a pile of objects on a moving conveyor belt, the projector 
locates the RFIG-tagged object, illuminating it with an easily identifiable temporal pattern. A camera 
attached to the robot arm locks onto this pattern, enabling the robot to home in on the object. The 
system built by Hideaki Nii and Masahiko Inami projects 3 different audio streams to different icons 
on the screen. They made a LED array board that was embedded in the camera and can project 3 different 
sounds simultaneously. If a subject changes a direction of the detector to a different icon, he/she can 
hear a different sound. The fundamental concept is to: 1) Embed optical sensors into the projection 
surface. 2) Project a series of Gray-coded binary patterns. 3) Decode the location of the sensors for 
use in a projected application. This video demonstrates this idea in the form of a target screen fitting 
application. It goes on to demonstrate how this approach can be used in multi-projector applications 
such as stitching (creating a large display using tiled projection) or layering (multiple versions of 
content on the same area for view dependent displays). Additionally, it can be used to automatically 
register the orientation of 3D surfaces for augmenting the appearance of physical objects. This technique 
is also useful for performing automatic touch calibration of interactive whiteboards or touch-tables. 
 Consider how optical communication can be used to built motion capture systems. For high speed tracking, 
the majority of optical motion capture systems use high speed cameras. These camera-based systems require 
special sensors and high bandwidth, are expensive and use a sanitized environment to maintain a high-contrast 
between the marker and its background. In this paper, we reverse the traditional approach. Instead of 
high speed cameras, we use high speed projectors to optically encode the space. Instead of retro-reflective 
or active light emitting diode (LED) markers, we use photosensitive tags to decode the optical signals. 
 Motion Capture ?  Building a human model  Dense sampling over surface  Geometry with Id at every 
millisecond  Bio parameters   Getting intimate  Cameras ..  Wearables  Second Skin (Sensor suit) 
 Tapping inside   Close the loop in bio-I/O  Remote monitoring: Elderly care, training  Robot observation:learning, 
worker safety  Feedback for biomech/neuro interfaces   Unlike previous methods that employ high speed 
cameras or scanning lasers, we capture the scene appearance using the simplest possible optical devices 
 a light-emitting diode (LED) with a passive binary mask used as the transmitter and a photosensor used 
as the receiver. We strategically place a set of optical transmitters to spatio-temporally encode the 
volume of interest. Photosensors attached to scene points demultiplex the coded optical signals from 
multiple transmitters, allowing us to compute not only receiver location and orientation but also their 
incident illumination and the reflectance of the surfaces to which the photosensors are attached. We 
use our untethered tag system, called Prakash, to demonstrate the new methods. Multi-LED projectors 
are light transmitters for space-labeling. Each beamer is simply a LED with a passive binary film (mask) 
set in front. The light intensity sequencing provides a temporal modulation, and the mask provides a 
spatial modulation. We use a rigid array of such beamers, called projectors. The binary masks of individual 
beamers are carefully chosen to exploit the epipolar geometry of the complete beamer arrangement. Each 
beamer projects invisible (near infrared) binary patterns thousands of times per second. Photosensing 
tags determine their location by decoding the transmitted space-dependent labels.  Let us look at the 
benefits of using multi-LED projectors for communication and photosensing markers. Expensive high-speed 
cameras pose several scalability issues. Bandwidth limits resolution as well as frame-rate. Higher frame-rate 
(i.e., shorter exposure time) requires either brighter controlled scene lighting for passive markers 
or the use of power hungry active LED markers. To robustly segment the markers from the background, these 
systems also use methods for increasing marker contrast. This usually involves requiring the actor to 
wear dark clothing under controlled lighting. The use of photosensing allows capture in natural settings. 
Since the photosensors are barely discernible, they can be embedded in a wide range of natural clothing 
so long as the photosensing element is exposed. The power of emitters is comparable to the IR emission 
from TV remote controls. Instead of high-power emission, we exploit high-frequency modulation to robustly 
communicate with the photosensors. Similar to photosensors in TVs, our sensors will work in many lighting 
conditions. So, in studio settings, the actor may wear the final costume, and he/she can be shot under 
theatrical lighting. Coded Illumination Motion Capture Clothing  500 Hz with Id for each Marker Tag 
 Capture in Natural Environment  Visually imperceptible tags  Photosensing Tag can be hidden under 
clothes  Ambient lighting is ok   Unlimited Number of Tags Light sensitive fabric for dense sampling 
 Non-imaging, complete privacy  Base station and tags only a few 10 s $  Full body scan + actions 
 Elderly, patients, athletes, performers  Breathing, small twists, multiple segments or people  Animation 
Analysis    Motion capture to date has been limited to exclusive and special purpose environments. 
Our low cost, easily portable, and visually imperceptible tracking system makes motion capture practical 
in a much wider application area. Interwoven tags on can help analyze patient rehabilitation progress 
after injuries. Tracking may be supportable in home video games and other casual virtual and augmented 
reality interfaces. The dream of filmmakers and game developers is on-set motion capture . One of the 
recent examples is the motion and appearance capture in the movie Pirates of the Caribbean . Our system 
can support this operation with unlimited number of imperceptible and interactive tags. Mobile projectors 
are allowing new opportunities thanks to the emerging small form factor. We can use projectors in a 
flexible way in everyday settings. The basic unit is a projector with sensors, computation, and networking 
capability. Singly or in a cluster, it can create a display that adapts to the surfaces or objects being 
projected on. As a hand-held, it allows projection of augmentation data onto a recognized object, plus 
mouse-style interaction with the projected data. Projectors can also behave as Smart Light, which can 
provide not only images but also optical information. The ideas provide geometric underpinnings for 
a new generation of projectors autonomous devices, adaptive to their surroundings and able to optically 
communicate with smart devices. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401241</article_id>
		<sort_key>1090</sort_key>
		<display_label>Article No.</display_label>
		<pages>85</pages>
		<display_no>86</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Projector-based illumination for 3D scene modeling]]></title>
		<page_from>1</page_from>
		<page_to>85</page_to>
		<doi_number>10.1145/1401132.1401241</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401241</url>
		<abstract>
			<par><![CDATA[<p>Overview</p> <p>&#8226; Scene appearance as higher dimensional reflectance fields</p> <p>&#8226; Capturing (and removing) global versus local illumination effects</p> <p>&#8226; Pattern projection for 3D geometry acquisition</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>3D/stereo scene analysis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Reflectance</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098869</person_id>
				<author_profile_id><![CDATA[81100504611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[P. A.]]></middle_name>
				<last_name><![CDATA[Lensch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Billy Chen and Hendrik P. A. Lensch. Light source interpolation for sparsely sampled reflectance fields. In G&#252;nther Greiner, Joachim Hornegger, Heinrich Niemann, and Marc Stamminger, editors, <i>Vision, Modeling, and Visualization 2005 (VMV'05)</i>, pages 461--469, Erlangen, Germany, November 2005. Aka.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tongbo Chen, Hendrik P. A. Lensch, Christian Fuchs, and Hans-Peter Seidel. Polarization and phase-shifting for 3D scanning of translucent objects. In <i>Proceedings of CVPR</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tongbo Chen, Hans-Peter Seidel, and Hendrik P. A. Lensch. Modulated phase-shifting for 3D scanning. In <i>Proceedings of CVPR</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344844</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yung-Yu Chuang, Douglas E. Zongker, Joel Hindorff, Brian Curless, David H. Salesin, and Richard Szeliski. Environment matting extensions: Towards higher accuracy and real-time capture. In <i>Proceedings of ACM SIGGRAPH 2000</i>, pages 121--130, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[P. Debevec, T. Hawkins, C. Tchou, H.-P. Duiker, W. Sarokin, and M. Sagar. Acquiring the Reflectance Field of a Human Face. In <i>Proc. SIGGRAPH</i>, pages 145--156, July 2000. ISBN 1-58113-208-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1243984</ref_obj_id>
				<ref_obj_pid>1243980</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Martin Fuchs, Volker Blanz, Hendrik P. A. Lensch, and Hans-Peter Seidel. Adaptive sampling of reflectance fields. <i>ACM Transactions on Graphics</i>, 26(2):10:1--10:18, June 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Martin Fuchs, Hendrik P. A. Lensch, Volker Blanz, and Hans-Peter Seidel. Superresolution reflectance fields: Synthesizing images for intermediate light directions. <i>Computer Graphics Forum</i>, 26(3):447--456, September 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383926</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gaurav Garg, Eino-Ville Talvala, Marc Levoy, and Hendrik P. A. Lensch. Symmetric photography: Exploiting data-sparseness in reflectance fields. In <i>Rendering Techniques 2006: 17th Eurographics Workshop on Rendering</i>, pages 251--262, June 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015807</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Michael Goesele, Hendrik P. A. Lensch, Jochen Lang, Christian Fuchs, and Hans-Peter Seidel. DISCO -- Acquisition of Translucent Objects. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2004)</i>, 23(3), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The lumigraph. In <i>Proceedings of SIGGRAPH 96</i>, Computer Graphics Proceedings, Annual Conference Series, pages 43--54, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Marc Levoy and Patrick M. Hanrahan. Light field rendering. In <i>Proceedings of SIGGRAPH 96</i>, Computer Graphics Proceedings, Annual Conference Series, pages 31--42, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882315</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Vincent Masselus, Pieter Peers, Philip Dutr&#233;, and Yves D. Willems. Relighting with 4d incident light fields. <i>ACM Transactions on Graphics</i>, 22(3):613--620, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566599</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Wojciech Matusik, Hanspeter Pfister, Addy Ngan, Paul Beardsley, Remo Ziegler, and Leonard McMillan. Image-based 3d photography using opacity hulls. <i>ACM Transactions on Graphics</i>, 21(3):427--437, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141977</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Shree K. Nayar, Gurunandan Krishnan, Michael D. Grossberg, and Ramesh Raskar. Fast separation of direct and global components of a scene using high frequency illumination. <i>ACM Transactions on Graphics</i>, 25(3):935--944, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882427</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Pieter Peers and Philip Dutr&#233;. Wavelet environment matting. In <i>Proceedings of the 14th Eurographics Symposium on Rendering</i>, pages 157--166, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383679</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Pieter Peers and Philip Dutr&#233;. Inferring reflectance functions from wavelet noise. In <i>Rendering Techniques 2005: 16th Eurographics Workshop on Rendering</i>, pages 173--182, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383271</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ravi Ramamoorthi and Pat Hanrahan. A signal-processing framework for inverse rendering. In Eugene Fiume, editor, <i>Proceedings of SIGGRAPH 2001</i>, Computer Graphics Proceedings, Annual Conference Series, pages 117--128. ACM Press / ACM SIGGRAPH, August 2001. ISBN 1-58113-292-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Nenanel Ratner and Yoav Y. Schechner. Illumination multiplexing within fundamental limits. In <i>Proceedings of CVPR</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566600</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Szymon Rusinkiewicz, Olaf Hall-Holt, and Marc Levoy. Real-time 3d model acquisition. <i>ACM Transactions on Graphics</i>, 21(3):438--446, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946723</ref_obj_id>
				<ref_obj_pid>946247</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Yoav Y. Schechner, Shree K. Nayar, and Peter N. Belhumeur. A theory of multiplexed illumination. In <i>Proceedings of IEEE ICCV</i>, volume 2, pages 808--815, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073257</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Pradeep Sen, Billy Chen, Gaurav Garg, Stephen R. Marschner, Mark Horowitz, Marc Levoy, and Hendrik P. A. Lensch. Dual photography. <i>ACM Transactions on Graphics</i>, 24(3):745--755, August 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311558</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Douglas E. Zongker, Dawn M. Werner, Brian Curless, and David H. Salesin. Environment matting and compositing. In <i>Proceedings of ACM SIGGRAPH 1999</i>, pages 205--214, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2008 Course on Projector-based Graphics Projector-based Illumination for 3D Scene Modeling 
Hendrik P.A. Lensch MPI Informatik Overview  Scene appearance as higher dimensional reflectance fields 
 Capturing (and removing) global versus local illumination effects  Pattern projection for 3D geometry 
acquisition  Hendrik Lensch In this part of the talk we will discuss the design and the analysis of 
illumination patterns for scene digitization tasks, for acquiring 3D geometry to capturing the appearance 
of a scene. In the latter, we will discuss techniques for measuring the light transport on a ray-to-ray 
basis which further allows to distinguish local illumination effects, where the incident light is directly 
reflected at the scene surface, from global effects where light might be scattered or interreflected 
multiple times before arriving at the observer. We will first address the problem of appearance acquisition 
and then discuss the implications of a separation into direct and global components for 3D range scanning. 
There are of course lots and lots of other fields where computational illumination plays a role, e.g. 
confocal microscopy, Digitizing Real-World Objects with a single picture 2D scene Hendrik Lensch The 
simplest way to digitize a scene is to acquire a 2D photograph. Unfortunately, besides zooming in and 
out, the operations that can be performed on a single image are rather limited. Lumigraphs or Light 
Fields represent a collection of photographs from a set of different view points. In essence, a fully 
sampled light field captures the outgoing radiance for any ray leaving the surface. Light Fields [Gortler96], 
[Levoy96] novel view synthesis but no relighting Hendrik Lensch With light fields applying view interpolation 
(maybe geometry assisted) arbitrary views of the object can be generated correctly as long as the virtual 
camera stays outside the visual hull. Direct and indirect reflections are reproduced in the rendered 
images, i.e. one can observe moving highlights, but the object will always be shown in the lighting that 
was present during acquisition. In order to obtain renderings in a novel lighting condition, one needs 
to capture a data set of images in a variety of illumination conditions. The simples is to illuminate 
the scene subsequently from a number of directions, e.g. from all positions on a hemisphere. Relighting 
 [Debevec2000] superposition Hendrik Lensch Due to the superposition principle of light these captured 
image can simply be added up to produce an image of the scene as if illuminated from all light sources 
at the same time. The data structure is called a reflectance field. 4D Reflectance Fields [Debevec2000] 
 distant light sources only Hendrik Lensch It is even possible to assign arbitrary weight to the virtual 
light sources by simply multiplying the individual images with a different color before adding up. This 
way the appearance of the scene can be reproduced in arbitrary environments. One restriction however 
is that the incident illumination is constrained to originate from the positions of the capturing light 
sources, for example from the sampling positions on a sphere. It is not possible to change the distance 
of the light source. As all light sources are assumed to be directional light sources, reflectance fields 
captured this way cannot reproduce the appearance according to a spatially varying illumination pattern 
formed for example by a spot light or by projected shadows. The direct reflection might be correctly 
reproduced, but the indirect reflections or subsurface scattering will be wrong.  This is demonstrated 
in these two slides. The projected light pattern alters the incident illumination for every point on 
the lamp shade. In the back on even sees how the incident light pattern is slightly blurred due to the 
scattering within the cloth. These effects cannot be reproduced with a far-field reflectance field. 6D 
Reflectance Fields [Masselus2003] 2D relighting with 4D incident light fields Hendrik Lensch In order 
to cover global illumination effects correctly one needs to acquire a reflectance field for individual 
incident rays. Now, one image is captured for every possible incident light ray, instead of for each 
light position The collection of incident rays form a 4D incident light field. Capturing one image per 
ray is of course expensive. Masselus et al. therefore only employed projectors with sixteen pixels. In 
the remainder of the course we will show how different illumination patterns can speed up this process. 
 The Holy Grail of appearance acquisition is to capture a full 8D reflectance field where the look of 
the object is captured from all possible direction from every ray in the incident light field. One can 
think of a reflectance field as an operator transforming the incident light field into the reflected 
light field. Main Problem  sampling an 8D function . spending 100 samples/dimension . 1016 samples 
 . hi-res 3D geometry: 108 vertices   coherence in reflectance fields . reduced data complexity  
no complete solution yet   Hendrik Lensch Sampling, storing or rendering an 8D function is a tough 
problem. Approaches therefore often reduce the dimensionality of the problem, e.g., assuming a single 
view point, distant illumination, etc. In addition, using structured illumination, it is possible to 
capture multiple samples in the same shot. Distant Illumination Reflectance Field one image for each 
light direction Hendrik Lensch Let s start with a reflectance field for distant illumination. As already 
said, one can reproduce the appearance of an object in the illumination of an environment map, assuming 
infinitely far away light sources. Light Stage [Debevec2000] single view point  assumes distant light 
sources  video  Hendrik Lensch The light stage is a device that is able to capture this. A dome of 
switchable light sources light sources illuminates the actor in sequence. The video demonstrates how 
the collected images can then be combined to reproduce the appearance in arbitrary environments. I would 
like to mention two properties of this particular setup: -The light source images are captured one at 
a time, i.e. one image per light source. This corresponds to scanning through the space of light sources 
and is relatively slow. In this setup. it has been accelerated using a high-speed camera. A positive 
point of this scanning illumination pattern is that the images can be directly used for rendering. In 
principle, no further analysis is necessary. -The second issue is that the sphere of position is relatively 
sparsely sampled from a set of fixed positions. As with any sampling process, one might observe sampling 
artifacts due to undersampling  Generating the appearance for a non-captured light source position therefore 
requires a modified acquisition system or additional processing in the form of light source interpolation. 
 Too few directional samples lead to artifacts most noticeable at high frequency effects such as specular 
highlights or shadows. Employing a non-linear upsampling scheme it is possible to obtain a much clearer 
reflection of the environment in the sphere. The problem gets clearer when taking a close up view on 
the scene: For a point on a glossy surface the reflectance function, the dependenc on the incident light 
ray direction (theta,phi) is rather smooth. A coarse sampling does not lead to any problems. Undersampled 
Highlight Region y  mirror direction might never be sampled . f x Hendrik Lensch On a mirroring 
sphere howerver, the reflection function changes drastically with the incident or the viewing angle. 
In a neighborhood of pixels on a sphere, it might be that only for a very view pixels, the coarse sampling 
locations actually contain a mirror direction. A highlight generated by a moving light source would generate 
an incorrect intensity variation. Prefiltering with Extended Light Sources y  mirror direction covered 
but at lower frequency . f x  [Fuchs et al. ACM TOG 2007] One can mitigate this problem by prefiltering 
the incident illumination, limiting the maximum frequency in the environment map. This has the effect 
the acquisition is done with extended light sources rather than point light sources. For each surface 
point a highlight will be captured. This can for example be achieved using an indirect light stage where 
disco spot lights project an extended spot onto the wall of a tent indirectly illuminating the scene 
in the middle. This can for example be achieved using an indirect light stage where disco spot lights 
project an extended spot onto the wall of a tent indirectly illuminating the scene in the middle. With 
this setup, one can even run an adaptive sampling pattern where the size of the light sources and the 
sampling density is adapted to meet different sampling requirements for different section of the illuminating 
sphere. This adaptive scheme provides a better sampling only at places where required. Compared to a 
sampling at full resolution a lot of sampling position can be well approximated from a sparser sampling. 
However, this adaptivity comes at the cost of a more complex acquisition setup, where the size and the 
location of the light sources need to be controllable. In addition, after each acquisition step the captured 
images need to be analyzed in order to predict, which samples to acquire next. Subdivision Hierarchy 
 take few input pictures  analyze the reflectance field  perform pair-wise upsampling, subdividing 
the mesh of light directions  Hendrik Lensch Another way is to upsample the acquired data by perfroming 
non-linear interpolation between the originally captured samples. Processing Pipeline  separate in different 
effects: . highlights . shadows  . low frequency effects   separate upsampling  refine result with 
texture priors    Hendrik Lensch We apply different upsampling schemes for highlights, shadows and 
low frequency effects. Result Videos  continuous highlights  smooth movement of reflections linear 
  superresolution  [Fuchs et al. EG 2007]  The benefit of this approach is that from the same set 
of input images much for faithful renderings can be produced. The environment is nicely reflected in 
the silver sphere. Shadows, highlights and even caustics move quite smoothly Result Videos artificial 
glare  some problemswith grazing angles  superresolution  [Fuchs et al. EG 2007] Here the performance 
on shadows is demonstrated. Note the correct effect of refraction in the glass. Some artifacts remain 
though. The choice of sampling pattern for the incident illumination depends on the scene properties. 
Prefiltering, adaptive sampling or an advanced interpolation scheme can significantly improve over a 
sparse sampling. Environment Matting for dense sampling one image for each monitor pixel [Zongker 
et al. SIGGRAPH 99] Environment mattes are designed to solve this task by densely sampling the light 
source positions. Especially geared to capture specular reflections or refractions. In principle, one 
could scan through all monitor pixels, but this would amount to 1 million images. Environment Matting 
 Extension of Alpha Matting capable of capturing transparent and specular objects for one view.  Allows 
for reproduction with arbitrary backdrops.  Traditional Alpha Matte C = F +(1-a) B  Composite 
color  Foreground color  Background color  Pixel coverage a  Acquired by blue/green screening  missing: 
dependence on light direction  C FB  Hendrik Lensch Environment Matting -Definition Add reflected 
and refracted rays (sum over backdrops) m C = F +(1-a)B +.RiM (Ti , Ai ) i=1 Reflectance R  Texture 
T  Axis-aligned area A  Averaging operator M (T, A)  Hendrik Lensch Environment matting extends traditional 
blue screen matting to incorporate a directionally dependent part. In this model, it is expressed as 
the dependence on a rectangular patch of the environment M(T,A). In order to acquire an environment matte 
one needs to determine the size and the location of the region of the backdrop/sides that influences 
each camera pixel. The process is visualized here. On each monitor side a set of hierarchical stripe 
patterns is displayed. From the recorded images for each individual displayed pattern the algorithm determines 
the direction of the sub-cone that contributes to the pixel s color. In addition, the algorithms determines 
the spread of the cone. Compared to scanning with individual pixels this hierarchical patterns are less 
precise as only an approximation of the actual beam is estimated. However, the acquisition time is significantly 
reduced. Environment Matte Extensions  Gaussian Filter kernel [Chuang et al. 2000]  Real-time acquisition 
[Chuang et al. 2000]  Wavelets in acquisition [Peers et al. 2003]  Multiple View Points (Opacity hulls) 
[Matusik 2002]  Hendrik Lensch Wavelet Environment Matting M hierarchical wavelet basis C =.aiCi i=1 
measure only where resolution is required Peers and Dutre suggested an adaptive approach based on wavelets. 
The illumination domain is only refined if there are pixels whose reflection show high frequency responses 
in this region. For rather diffuse reflection, sub-division is not required and a lot of images can be 
saved. On the other hand, very sharp reflections can be correctly reproduced as well. In principle, the 
output quality is as good as with a scanning approach, the number of images required is however drastically 
reduced. As can be seen in the flow diagram. the adaptive process requires to interleave pattern generation, 
acquisition and analysis. illumination (generate about 600 patterns) infer per-pixel reflection T(x, 
y)  measurements Ci (x, y) = T(x, y)Wi  Ci (x, y) -T (x, y)Wi minimize E =. 2 subject to a sparse T 
(x, y) i (compare to compressed sampling) [Peers&#38;Dutre EGSR 05] In their wavelet noise approach 
Peers and Dutre no longer illuminate with individual wavelet basis but rather combine a random collection 
(with random weights) together in order to form a fixed set of illumination patterns. From this set of 
patterns the reflection functions now can only be inferred. They are no longer measured directly but 
need to be determined based on an optimization process. In this particular case the reflectance function 
of each pixel is hierarchically estimated minimizing the error between the current prediction and the 
measured samples while keeping the structure of the wavelet tree as simple as possible. This process 
bears some resemblance with compressed sampling approaches. The benefit of this technique is, that one 
obtains a hierarchical basis representation from a fixed set of patterns. No explicit sampling or adaptation 
is necessary. The acquisition is significantly simplified but the analysis at the end is rather complicated. 
6D Reflectance Fields [Masselus2003] 2D relighting with 4D incident light fields Hendrik Lensch Now 
let s look at illumination pattern for a different type of reflectance fields where the illumination 
is recorded not for individual light source positions but rather for individual pairs of rays. Pixel-to-Pixel 
Transport  scene Hendrik Lensch Focusing on a single camera projector pair we need to determine for 
each projector pixel (p,q) what is the resulting camera image. The fourth-order tensor T stores the reflection 
coefficient for every pair of projector/camera pixel. It fully describes the scenes reflection properties. 
Pixel-to-Pixel Transport  linear transport: C = T L  measurement of the impulse response  This matrix 
can again be acquired by scanning, i.e. turning on each projector pixel individually, recording one image 
per projector pixel. This is of course extremely expensive, but doable.  Here, one sees a corresponding 
measurement setup. Since a single projector pixel is not very bright compared to the projectors black 
level, we apply a laser projector with galvanic mirrors. The laser projector provides a significantly 
better contrast compared to video projectors. The laser beam sweeps over the surface and a HDR video 
camera records every sample. When the laser beam hits a translucent surface, the area surrounding the 
laser point lights up due to subsurface scattering. The footprint varies drastically with the incident 
location. After one day and after capturing about one million images the entire matrix has been acquired. 
One can apply some compression for interactive rendering. Hole­filling is applied. The data set now allows 
for relighting with arbitrary light patterns from arbitrary directions. The very specific properties 
of the light transport in translucent objects is correctly captured and reproduced. One sees for example 
the drastic difference between front and back illumination as well as the light bleeding into shadowed 
regions. Adaptive Parallel Acquisition assumption: sparse matrix,hierarchical basis  We now apply an 
adaptive scheme to this problem, again using a hierarchical wavelet bases, or equivalent hierarchically 
nested blocks. In the case that the transport matrix is only sparsely populated, we can even parallelize 
the acquisition of multiple blocks. As long as they influence well separated camera regions only, they 
can be illuminated at the same time, and we can afterwards determine, based on the pixel location, which 
block in the reflectance tensor has been measured. The parallelization drastically reduces the acquisition 
time compared to a sequential adaptive scheme. Here you can see the acquisition sequence for a quite 
complex scene. Initially the subdivided regions are sequentially acquired. At some point the algorithm 
detects that the corresponding footprints (areas of influence) in the camera image are well separated. 
All subsequent measurements of radiometrically independent blocks can further be captured in parallel. 
This process starts at some specific level of subdivision. At the pixel-level, lots of samples can be 
acquired in parallel, especially in areas where direct reflections dominate the light transport. If the 
light transport is more complicated, as for example within the bottle, where refraction spreads the extent 
of the incident light rays, less samples can be captured at the same time. Still the acceleration is 
dramatic. For most scenes, the acquisition time is in the order of O(log N) for N projector pixels The 
recorded reflectance field can now be relit with arbitrary illumination patterns. The tensor captured 
all light transport paths: one can see direct diffuse and specular reflections, interreflections. The 
glas bottle furthermore features refraction and caustics.  In the Cornell box we demonstrate even diffuse 
color bleeding and mirror reflections. Helmholtz Reciprocity  dual aI aI    scene Hendrik Lensch 
 Given the reflectance measured on a ray-to-ray basis, i.e. along one path, we can apply Helmholtz reciprocity. 
It states that the reflectance along a path is the same no matter in which direction the path is followed. 
We can follow the light from the source to the receiver or we can swap their location and follow exactly 
the same path in the other direction, the observed reflectance coefficient will be exactly the same. 
Relighting with Dual Photography  scene Hendrik Lensch We can apply Helmholtz reciprocity has the 
tensor T captures the reflectance for any path between the projector and the camera. Helmholtz reciprocity 
holds for any collection of paths as long as the light source and the camera are within the same optical 
medium. Relighting with Dual Photography  scene Hendrik Lensch We can now swap the role of the camera 
and the projector, turning the original projector into a virtual camera, the original camera into a virtual 
projector. Given T one can easily compute an image from the point of view of the original projector as 
if illuminated from the camera. Here, one can even apply arbitrary patterns from the virtual camera. 
The process is surprisingly simple as the reflectance coefficients stay exactly the same. They just need 
to be reorder in order to account for the novel ray configuration. This can be done by simply transposing 
the captured tensor T. We can now generate different views from the same transport tensor, one from 
the original camera, illuminated from the original projector, and one from the projector illuminated 
from the camera. All light transport paths are correctly incorporated, leading to a faithful reproduction 
of the bottles refractions and the caustics of the teapot. Due to the duality, objects and shadows typically 
change place in the dual image. Another set of rather famous objects. Besides being a nice trick , 
Dual photography can be used to accelerate the acquisition of higher order reflectance fields. In this 
configuration, a set of projectors produce an incident 4D light field. In order to measure the corresponding 
reflectance field, the reflectance field of each projector needs to be captured in sequence. Otherwise 
the illumination patterns of multiple projectors would overlap. It would be rather difficult to disentangle 
the contributions of the individual projectors. With dual photography we can swap the role of cameras 
and projectors both during acquisition and during rendering. Since cameras are passive devices they can 
easily operate in parallel without interfering with each other. This way, the acquisition process of 
a 6D reflectance field for relighting with 4D incident light fields can be accelerated to the time cost 
of acquiring the reflectance field between a single camera/projector pair. Thus it is possible, to acquire 
a rather dense reflectance fields on a ray-to-ray basis in significantly reduced time. It allows for 
relighting with arbitrary patterns from arbitrary directions. Slightly extending the previous adaptive 
parallel acquisition approach the reflectance fields of arbitrarily complex scenes can be acquired at 
moderate time cost. Here you see the complex light pattern formed in a glass of gummy bears illuminated 
from two different directions with a high frequency pattern. One can acquire reflectance field that reproduce 
correctly all local and global illumination effects. Local vs. Global Reflections  Hendrik Lensch Looking 
at the structure of the resulting light transport tensors the difference between global and local illumination 
effects are clearly revealed. In this case, we just concentrate on the slice which is formed by illuminating 
every point along the white line and looking at how much light is reflected from all points along the 
line. In the left case, the reflections are due to the direct reflection off a planar, textured sheet 
of paper. As one can see, light will only be reflected at the place where the sheet is illuminated, leading 
to the (band) diagonal structure of the matrix. The tensor does not contain any contribution in off-diagonal 
elements. In the right case, the light transport between the two pages of an open book are captured. 
The transport is still dominated by the direct reflection, i.e. the entries along the diagonal. However, 
there is additional indirect light transport from the left to the right side of the book. as well as 
tertiary reflections. Those populate the off-diagonal elements in the matrix. Application: Getting Rid 
of Global Effects  remove off-diagonal components  diagonal entries might still contain global components. 
  Hendrik Lensch With this knowledge at hand, one can now try to separate direct and global illumination 
effects. Simply removing off-diagonal elements from the transport tensor removes most of the indirect 
reflections. However, even the will to some extent be influenced by global illumination effects, e.g. 
3rd order reflections back to the point of incidence. Fast Separation of Direct and Global Effects main 
idea: global illumination effects dampen high frequencies . illuminate with shifted high frequency patterns 
. only the local illumination will change . global illumination will be invariant to phase shifts  
 A principled method for a fast separation of direct from global illumination effects in images captured 
from a single view point has been proposed by Nayar et al. In this example, the direct component contains 
all specular highlights. The global component is strongest for the translucent surfaces and for the interreflections 
between the walls. For this problem again, structured illumination is applied. The idea is that only 
the direct reflection component will respond to high frequency variation in the incident illumination 
while the global component will remain fix. Fast Separation of Direct and Global Effects shifted periodic 
patterns (e.g. checker board)  record per-pixel minimum and maximum  approximation: Lg = Lmin Ld = 
Lmax - Lmin   [Nayar et al. SIGGRAPH 2006] This effect can be observed when illuminating the scene 
with a set of shifted periodic high frequency patterns. In the recorded image sequence one just need 
to determine the minimum and maximum per pixel in order to quickly compute the direct and global component. 
 In the paper a more precise approximation based on the projector contrast and black-level can be found. 
 This tool of quickly removing any global illumination effect from measurements performed with structured 
light can help to make computer vision tasks more robust. Traditional 3D Scanning  illumination with 
a swept point/line O(N)  detection of the brightest dot on surface  triangulation based on laser and 
camera  Hendrik Lensch For example, 3D range scanning, where structured illumination has been initially 
 been applied. Here, we see the principle of a traditional laser range scanner. Sweeping a line or a 
dot over the scene the camera records the brightest point along the scan line. This detection can be 
performed with sub-pixel precision. Given the location of the brightest point in the camera image and 
the rotation angle of the laser, one can now easily determine the distance between the camera and the 
surface point by triangulation.  Instead of sweeping a line, structured hierarchical codes can reduce 
the acquisition effort to O(log N). The recorded camera images now need to be analyzed in order to determine 
the projector row that illuminated the visible surface point in each camera pixel. Binary Encoded Stripes 
 determine the corresponding column in log(n) time. observations: wwbb ~ col. 3 wbbw ~ col. 6 bbbw ~ 
col. 14 Hendrik Lensch The easiest code simply encodes the column number in a set of bit planes, producing 
one illumination pattern for the most significant bit and so forth. By interpreting the recorded on/off 
patterns as a binary number one directly can read of the projector column again for each pixel. One limitation 
of this approach is however, that the encoding is constant within each bar of the highest resolution. 
Stripe Patterns -Extensions  Use Gray-code for more reliable detection  Different pattern for interactive 
scanning [Rusinkiewicz 02]  Color patterns for faster, texture invariant acquisition  Hendrik Lensch 
Various extensions have been proposed to make the encoding more robust or more efficient. Phase Shifting 
 0 2 4 6 8 101214 13579 111315   . precision of binary encoding limited by highest resolution . use 
shifted sine pattern to obtain subpixel precision . requires precise gray levels . period needs to 
be determined using coarser levels (phase unwrapping)  Hendrik Lensch One way to overcome the problem 
of the discrete locations it to apply sinusoidal functions shifted in phase for the last few iterations. 
Given some photometric calibration, the exact phase of a surface point in these shifted pattern can be 
reconstructed with subpixel precision. The output is the phase within one period of the highest frequency. 
The binary codes of coarser resolution are necessary to locate the period. Here, the constant location 
is sufficient. Robust 3D Range Scanning  use patterns that inherently filter out global illumination 
effects . high frequency sinusoids do this  use additional polarization multiply scattered light is 
typically unpolarized  Hendrik Lensch With the shifted sinusoidal patterns we can do two things at 
the same time: -3D range scanning using phase shifting -separation into direct and global reflections 
This produces a 3D range scanning approach that is robust to global illumination effects. Robust Sine 
Patterns  illuminate with phase shifted sinusoids: e.g. I = sin(x +di ), di .{-2p / 3,0,2p / 3} estimate 
phase -> 3D position: - .-.Li sindi . F= tan 1.. ..L cosd. . ii . or perform separation Lg = 23 (L0 + 
L1 + L2) - Ld , Ld = 32 3(L - L )2 + (2L - L - L )2 02 102 Hendrik Lensch The equations for obtaining 
the phase and for performing the separation are rather simple. It is worth noting that estimating the 
direct component using sine pattern using the above equations is much more robust than detecting the 
minimum and maximum from a sequence of shifted binary patterns. Using sinusoids all measurements are 
incorporated.  A commercial laser range scanner has sever difficulties in scanning translucent objects. 
The subsurface scattering offsets the location of the peak, resulting in a corrupted 3D scan. Removing 
the global effects a significantly better 3D scan can be obtained. Modulated Patterns in 2D use sinusoids 
in both dimensions . results in descattering in both dimensions I = sin(x +di ) ·sin( y +d j )  [Chen 
et al. CVPR 2008] Modulating a horizontal sine pattern with a vertical one and capturing M*N phase shifts 
for both directions, the separation capabilities can be drastically increased. Modulation and shifting 
in two dimensions outperforms even polarization difference imaging. Illumination Patterns and Codes 
 scan  periodic  hierarchical  multiplexed / Noise  modified by the properties of light . wavelength 
 . polarization . time of flight / phase (not discussed here)   Hendrik Lensch In the different fields 
we have seen the same codes and illumination patterns over and over again. Scanning is the slowest but 
delivers quite robust measurements. The benefit is that the acquired images can be used directly without 
any further processing. Periodic Patterns  O(1) constant number of phase shifts  requires some analysis 
 information within one period only  some robustness wrt. global illumination [Nayar et al. 2006, Chen 
et al. 2007/2008]  stripes 2D grid sinusoids  Hendrik Lensch Periodic patterns can be acquired much 
faster. As the number of phase shifts is fixed the acquisition effort is O(1). Of course, periodic patterns 
are note always applicable, e.g. not for measuring reflection properties. They require some analysis 
which is typically rather simple. The second benefit of periodic patterns is that the are inherently 
robust against global illumination effects if analyzed appropriately.  Hierarchical bases are used to 
compactly localize features globally. Furthermore, when acquiring reflectance fields or environment mattes 
the can be used to directly sample into a hierarchical representation of the otherwise large data structures. 
Adaptive Patterns  patterns adapted to scene content / signal  based on a hierarchical basis  . determine 
if subdivision is necessary optimal set of patterns requires analysis after each acquisition step Combined 
with an adaptive acquisition scheme, hierarchical bases can be used to accelerate the acquisition. Sub-trees 
that do not require further subdivision will be culled already during acquisition, unnecessary acquisition 
steps are saved. Multiplexing multiple basis functions, it is possible to either improve the SNR slightly 
 or to accelerate the acquisition sacrificing some quality. Multiplexed illumination requires some additional 
analysis to obtain the measurements for individual bases. Summary  computational illumination used 
for measuring . 3D shape . object appearance  choice of codes, acquisition and analysis are always 
interlinked  Hendrik Lensch Challenges robust acquisition under strong ambient illumination  dynamic 
scenes  large scenes  uncontrolled environments  Hendrik Lensch SIGGRAPH 2008 Course on Projector-based 
Graphics Projector-based Illumination for 3D Scene Modeling Hendrik P. A. Lensch  References [1] Billy 
Chen and Hendrik P. A. Lensch. Light source interpolation for sparsely sampled re.ectance .elds. In G¨ 
unther Greiner, Joachim Hornegger, Heinrich Niemann, and Marc Stamminger, editors, Vision, Modeling, 
and Visualiza­tion 2005 (VMV 05), pages 461 469, Erlangen, Germany, November 2005. Aka. [2] Tongbo Chen, 
Hendrik P. A. Lensch, Christian Fuchs, and Hans-Peter Sei­del. Polarization and phase-shifting for 3D 
scanning of translucent objects. In Proceedings of CVPR, 2007. [3] Tongbo Chen, Hans-Peter Seidel, and 
Hendrik P. A. Lensch. Modulated phase-shifting for 3D scanning. In Proceedings of CVPR, 2008. [4] Yung-Yu 
Chuang, Douglas E. Zongker, Joel Hindorff, Brian Curless, David H. Salesin, and Richard Szeliski. Environment 
matting extensions: Towards higher accuracy and real-time capture. In Proceedings of ACM SIGGRAPH 2000, 
pages 121 130, 2000. [5] P. Debevec, T. Hawkins, C. Tchou, H.-P. Duiker, W. Sarokin, and M. Sagar. Acquiring 
the Re.ectance Field of a Human Face. In Proc. SIGGRAPH, pages 145 156, July 2000. ISBN 1-58113-208-5. 
[6] Martin Fuchs, Volker Blanz, Hendrik P.A. Lensch, and Hans-Peter Seidel. Adaptive sampling of re.ectance 
.elds. ACM Transactions on Graphics, 26(2):10:1 10:18, June 2007. [7] Martin Fuchs, Hendrik P. A. Lensch, 
Volker Blanz, and Hans-Peter Sei­del. Superresolution re.ectance .elds: Synthesizing images for intermedi­ate 
light directions. Computer Graphics Forum, 26(3):447 456, September 2007. [8] Gaurav Garg, Eino-Ville 
Talvala, Marc Levoy, and Hendrik P. A. Lensch. Symmetric photography: Exploiting data-sparseness in re.ectance 
.elds. In Rendering Techniques 2006: 17th Eurographics Workshop on Rendering, pages 251 262, June 2006. 
[9] Michael Goesele, Hendrik P. A. Lensch, Jochen Lang, Christian Fuchs, and Hans-Peter Seidel. DISCO 
 Acquisition of Translucent Objects. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2004), 23(3), 
2004. [10] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Co­hen. The lumigraph. 
In Proceedings of SIGGRAPH 96, Computer Graphics Proceedings, Annual Conference Series, pages 43 54, 
August 1996. [11] Marc Levoy and Patrick M. Hanrahan. Light .eld rendering. In Proceedings of SIGGRAPH 
96, Computer Graphics Proceedings, Annual Conference Se­ries, pages 31 42, August 1996. [12] Vincent 
Masselus, Pieter Peers, Philip Dutr´e, and Yves D. Willems. Relight­ing with 4d incident light .elds. 
ACM Transactions on Graphics, 22(3):613 620, July 2003. [13] Wojciech Matusik, Hanspeter P.ster, Addy 
Ngan, Paul Beardsley, Remo Ziegler, and Leonard McMillan. Image-based 3d photography using opacity hulls. 
ACM Transactions on Graphics, 21(3):427 437, July 2002. [14] Shree K. Nayar, Gurunandan Krishnan, Michael 
D. Grossberg, and Ramesh Raskar. Fast separation of direct and global components of a scene using high 
frequency illumination. ACM Transactions on Graphics, 25(3):935 944, July 2006. [15] Pieter Peers and 
Philip Dutr´e. Wavelet environment matting. In Proceedings of the 14th Eurographics Symposium on Rendering, 
pages 157 166, 2003. [16] Pieter Peers and Philip Dutre.´Inferring re.ectance functions from wavelet 
noise. In Rendering Techniques 2005: 16th Eurographics Workshop on Ren­dering, pages 173 182, June 2005. 
[17] Ravi Ramamoorthi and Pat Hanrahan. A signal-processing framework for inverse rendering. In Eugene 
Fiume, editor, Proceedings of SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 
pages 117 128. ACM Press / ACM SIGGRAPH, August 2001. ISBN 1-58113­292-1. [18] Nenanel Ratner and Yoav 
Y. Schechner. Illumination multiplexing within fundamental limits. In Proceedings of CVPR, 2007. [19] 
Szymon Rusinkiewicz, Olaf Hall-Holt, and Marc Levoy. Real-time 3d model acquisition. ACM Transactions 
on Graphics, 21(3):438 446, July 2002. [20] Yoav Y. Schechner, Shree K. Nayar, and Peter N. Belhumeur. 
A theory of multiplexed illumination. In Proceedings of IEEE ICCV, volume 2, pages 808 815, 2003. [21] 
Pradeep Sen, Billy Chen, Gaurav Garg, Stephen R. Marschner, Mark Horowitz, Marc Levoy, and Hendrik P. 
A. Lensch. Dual photography. ACM Transactions on Graphics, 24(3):745 755, August 2005. [22] Douglas E. 
Zongker, Dawn M. Werner, Brian Curless, and David H. Salesin. Environment matting and compositing. In 
Proceedings of ACM SIGGRAPH 1999, pages 205 214, 1999. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401242</section_id>
		<sort_key>1100</sort_key>
		<section_seq_no>24</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Psychophysics 101: How to run perception experiments in computer graphics]]></section_title>
		<section_page_from>24</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098870</person_id>
				<author_profile_id><![CDATA[81100459651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ferwerda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401243</article_id>
		<sort_key>1110</sort_key>
		<display_label>Article No.</display_label>
		<pages>60</pages>
		<display_no>87</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Psychophysics 101]]></title>
		<subtitle><![CDATA[how to run perception experiments in computer graphics]]></subtitle>
		<page_from>1</page_from>
		<page_to>60</page_to>
		<doi_number>10.1145/1401132.1401243</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401243</url>
		<abstract>
			<par><![CDATA[<p>Psychophysical methods from experimental psychology can be used to quantify the relationships between the properties of images and what people perceive. The results of psychophysical experiments can be used to create predictive models of human perception that can guide the development of effective and efficient graphics algorithms and enabling graphical interfaces. This course will provide an introduction to the use of psychophysical methods in computer graphics and will teach attendees how to develop experiments that can be used to advance graphics research and applications. Throughout the presentation, graphics-relevant examples will be used so attendees will understand how to design and run their own experiments; analyze the results; and develop perceptually-based algorithms and applications. This course will be of interest to members of the graphics community who want to be able to interpret the results of perception psychology experiments and develop their own user studies of computer graphics techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.3.2</cat_node>
				<descriptor>Computer science education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527.10003531.10003533</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education->Computing education programs->Computer science education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098871</person_id>
				<author_profile_id><![CDATA[81100459651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Ferwerda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Munsell Color Science Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Psychophysics 101 How to run perception experiments in computer graphics SIGGRAPH 2008 Courses Program 
Course organizer and presenter James A. Ferwerda Munsell Color Science Laboratory Chester F. Carlson 
Center for Imaging Science Rochester Institute of Technology Course description Psychophysical methods 
from experimental psychology can be used to quantify the relationships between the properties of images 
and what people perceive. The results of psychophysical experiments can be used to create predictive 
models of human perception that can guide the development of effective and efficient graphics algorithms 
and enabling graphical interfaces. This course will provide an introduction to the use of psychophysical 
methods in computer graphics and will teach attendees how to develop experiments that can be used to 
advance graphics research and applications. Throughout the presentation, graphics-relevant examples will 
be used so attendees will understand how to design and run their own experiments; analyze the results; 
and develop perceptually-based algorithms and applications. This course will be of interest to members 
of the graphics community who want to be able to interpret the results of perception psychology experiments 
and develop their own user studies of computer graphics techniques. Prerequisites This course assumes 
a basic level understanding of issues in computer graphics and electronic imaging. Familiarity with freshman-level 
college math will be helpful. No specific knowledge of perception psychology or statistical methods will 
be required. All relevant concepts will be introduced in the class. Syllabus  -Welcome, Introductions, 
Schedule Review (5 mins) -Motivation / Orientation (20 mins) -Psychophysical Methods (50 mins) -Case 
Studies (25 mins) -Summary/Resources(5 mins) -Motivation/Orientation ( 20 mins) -Why does graphics need 
psychophysics? -What kinds of problems can be addressed by psyshophysics? - How does psychophysics relate 
to physical measurement? - How is psychophysics different from usability?  -What kinds of results can 
psychophysics produce? -Why don t we just mine the existing literature? -How do we make progress? - 
 -Psychophysical Methods ( 50 mins) -Introduction -need for objective metrics of subjective visual experience 
-fundamental psychophysical metrics: thresholds and scales -examples of threshold and scale metrics in 
computer graphics -Methods for measuring thresholds - the method of adjustment - the method of limits 
 - the method of constant stimuli  - Threshold models - psychometric functions - Sources of error in 
threshold measurements - Signal detection theory - SR matrices, ROC curves - error-free threshold measures 
 - Suprathreshold scaling methods - Types of psychophysical scales - nominal, ordinal, interval, ratio 
- Ordinal and interval scaling methods - Thurstonian scaling - Ratio scaling methods - Scaling models 
- Weber s law - Steven s power law - Multidimensional scaling (MDS) - theory, demo, data analysis -Practicalities 
of running psychophysical experiments in computer graphics - selecting test stimuli - selecting observers 
 - the importance of observer instructions - presentation/user interface issues - collecting data  
- Case Studies of Psychophysical Methods in Graphics (25 mins) - Brief descriptions and critique of past 
and current research in perceptually-based graphics. Topic areas: - realistic image synthesis - geometric 
simplification - image-based modeling - compression - virtual environments - visualization - Summary/Conclusions 
(5 mins) - utility and limits of psychophysical methods - relevance for computer graphics - Resources 
- books, papers, journals, web sites - software packages -organizations, conferences Resources Books 
Gescheider, G.A. (1997) Psychophysics: The Fundamentals, 3rd Edition. Erlbaum. Bartelson, C.J. and Grum, 
F. (Eds.) (1984) Optical Radiation Measurements, Vol 5: Visual Measurements. Academic Press, New York. 
Engeldrum, P.G. (2000) Psychometric scaling: A Toolkit for Imaging Systems Development. Imcotek Press. 
Guilford, J.P. (1954) Psychometric methods. Mcgraw-Hill. Torgerson, W.S. (1960) Theory and Methods of 
Scaling. Wiley. Green, D.M. and Swets, J.A. (1966) Signal Detection Theory and Psychophysics. Wiley. 
Fechner, G.T. (1966) Elements of Psychophysics. Holt, Rinehart &#38; Winston Papers/Standards ASTM (American 
Society for Testing and Materials), Standard Guide for Conducting Visual Experiments, E1808-96 ASTM (American 
Society for Testing and Materials), Standard Guide for Selection, Evaluation, and Training of Observers, 
E1499-94 CIE Technical Committee 1-34 Testing Color-Appearance Models: Guidelines for Coordinated Research 
- Alessi, P.J. (1994) Color Research and Applications,19, 48-58. Use of computers and cathode-ray-tube 
displays in visual psychophysics - special issues of the journal Spatial Vision 10(4) and 11(1) - http://www.hans.strasburger.de/ 
Software/Websites Psychophysics Toolbox (Matlab-based) http://psychtoolbox.org/ Psychophysica/Cinematica 
(Mathematica/Quicktime-based) http://vision.arc.nasa.gov/mathematica/psychophysica/ Strasburger s overview 
of psychophysics software http://www.hans.strasburger.de/  Course presenter biographical sketch James 
A. Ferwerda is an Associate Professor in the Munsell Color Science Laboratory in the Center for Imaging 
Science at the Rochester Institute of Technology. He received a B.A. in Psychology, M.S. in Computer 
Graphics, and a Ph.D. in Experimental Psychology, all from Cornell University. The focus of his research 
is on building computational models of human vision from psychophysical experiments, and developing advanced 
graphics algorithms based on these models. Current research interests include: high dynamic range imaging; 
perceptually­based rendering; perception of material properties; and low vision and assistive technologies. 
Prior to joining the Faculty at RIT he was a Research Associate in the Program of Computer Graphics at 
Cornell. In 1992 he received the IEEE Computer Society Paper of the Year Award, and in 2003 he was selected 
for the National Academy of Engineering Frontiers of Engineering Program. He is an Associate Editor of 
ACM Transactions on Applied Perception, was Guest Editor for a special edition of IEEE Computer Graphics 
and Applications on Applied Perception, and serves as a member of CIE Technical Committee TC8-08 on High 
Dynamic Range Imaging. Contact information James A. Ferwerda, Ph.D. Associate Professor Munsell Color 
Science Laboratory Chester F. Carlson Center for Imaging Science 18/1069 Lomb Memorial Drive Rochester 
Institute of Technology Rochester, NY 14623 585-475-4923 jaf@cis.rit.edu http://www.cis.rit.edu/jaf 
 One of the hallmarks of a science is the development of reliable techniques for measuring the phenomena 
of interest. For example, in physics there are standard techniques for measuring an object s mass or 
velocity. In chemistry there are techniques for measuring the strength of bonds or the energy given off 
by a reaction. In computer graphics, we would like to be able to measure what people perceive when they 
look at our images, so we can create more effective and compelling images, and create them more efficiently. 
However, while physical quantities can be measured with standard methods, our visual experiences are 
subjective and cannot be measured directly.  If computer graphics is to become a science of visualization 
then researchers need to be able to conduct experiments that quantify the relationships between the parameters 
of our algorithms, the images they produce, and the visual experiences the images create. Psychophysics 
provides a sound set of methodologies for conducting and analyzing these experiments. This tutorial provides 
a survey of the issues and methodologies of visual psychophysics.  graphics are generated to be useful 
to people  we need to be able to determine what factors contribute to visual experience  we need to 
be able to assess what methods produce an effective visual experience  realistic image synthesis how 
accurate does the input need to be?, what input is needed?, how accurate does the light transfer need 
to be?, how are the results in physical units transformed to displays?  data visualization how should 
data values be mapped to visual attributes?, how effective are different visual cues for conveying information 
about data, what are the interactions between these different cues?, how can we make sure that the images 
we create are faithful representations  virtual environments what trade-offs are acceptable to maintain 
real time performance?, what spatial representations are adequate?, what are the perceptual differences 
between screen-based and immersive displays?  compression what kinds of artifacts are visually acceptable 
in still images? In temporal sequences? In 3D geometric models?  human observers are responding to physical 
stimulus  depending on problem various physical measurements also needed object shape/material properties; 
light energy from real scenes/displays  focus is on visual perception and computing images , but overlap 
with these areas  more efficient graphics algorithms -- computing only what is necessary  more effective 
graphics methods -- choosing the right image to generate  graphics builds on psychophysical research 
(e.g. colorimetry)  goals of psychophysical research are different than graphics research  determining 
contrast sensitivity vs. designing a rendering method that uses a model of contrast sensitivity  adopt 
established experimental methods  build a literature of results relevant to graphics techniques  what 
is psychophysics?  branch of experimental psychology  concerned with developing experimental methods 
for objectively measuring our subjective perceptual experiences  why is this a problem? objects in 
the world have physical properties (length, weight, intensity)  can be measured directly with instruments 
 these objects create distinct perceptual impressions (size, heaviness, brightness)  but can t put 
electrodes in peoples brains to directly measure sensations, so we have to infer what people perceive 
from their responses  psychophysics provide a set of tools that allow us to make objective measurements 
of perception  quantifies the relationships between the dimensions of the external world of physical 
stimulation and our internal world of perceptual appearances  two fundamental quantities that psychophysical 
methods let us measure: thresholds and scales  thresholds describe the limits of perception  two kinds: 
absolute/detection threshold, difference/discrimination threshold  detection thresholds measure the 
absolute limits of perception for example: might ask how bright does a patch need to be to be seen, 
photometric efficiency of vision  difference threshold measures ability to discriminate between similar 
physical stimuli for example: how much brighter does the patch on the right need to be to be seen as 
different than the square on the left  this value is known as the just-noticeable-difference (JND) 
 JNDs are the basic units that form the foundation of many engineering applications of perception examples: 
compression: JPEG, MPEG, MP3, graphics: visible difference predictors in image quality, perceptually-based 
rendering  in addition to understanding absolute sensitivity and discrimination limits, we might want 
to know how what we perceive changes across some wide range of change in a physical parameter, this is 
the domain of scaling techniques  in scaling interest is in quantifying the suprathreshold appearances 
of things, and in measuring how appearance changes with changes in physical properties for example: 
which square on the bottom is twice as bright as the one on the top  scaling studies are also widely 
used in engineering application examples: Munsell color system, image quality metrics  development of 
psychophysical methods goes back over 150 years  earliest work attributed to Weber  experimented with 
weights, had subjects lift weights and judge differences  found that just-noticeable-differences (JNDs) 
were proportional to size of the weight (i.e. needed greater differences to tell heavier weights apart) 
 observations became formalized as Weber s law which says  delta I (JND) = k * I (magnitude of the standard) 
 different k s for different sensory modalities/judgments  Fechner regarded as the father of psychophysics: 
relations between physical and psychological worlds  basic question: how can we objectively measure 
subjective sensations?  to measure anything need a zero and a unit value, then can build a ruler and 
do objective measurements  Fechner took zero to be the absolute threshold (starting point of perception) 
 drawing on Weber s work, he took JND s as the unit of perception, and by measuring JND s at a range 
of physical values he could build up psychophysical scales to relate stimulation and perception  because 
of the proportionality of JNDs, scales were all logarithmic, formalized as Fechner s law says sensation 
is proportional to the log of the physical stimulation, different k s for different modalities, judgments 
 Fechner s lasting contribution was the development of a set of experimental methods for measuring psychophysical 
relations  Published in Elements of Psychophysics in 1860  rest of talk divided into two sections 
 first section on techniques for measuring thresholds  later will focus on techniques for developing 
psychophysical scales  Fechner introduced three experimental methods for measuring psychophysical threholds 
 method of adjustment  method of limits  method of constant stimuli   method of adjustment (MOA) 
 example of measuring difference threshold for brightness  start with fixed standard, continuously 
variable test set  experiment is run as a series of trials on each trial standard and test are presented 
together (side-by-side, sequentially, or binocularly)  subject s task: adjust intensity of the test 
square until just visibly different from standard  to control potential observer bias starting intensity 
varied randomly  both descending trials (start intensity above standard) and ascending trials (start 
intensity below standard) are presented   final intensities set are recorded for each trial  to determine 
the threshold, we first tabulate the data (trials, series, start intensity, final intensity for ascending,descending) 
 then calculate separate means and standard deviatons of the set values for the ascending and descending 
series  check for bias by making sure that means are not too different from each other using a t-test 
 If ok, can combine data and calculate overall mean and stdev for all trials  we can use these statistics 
to derive several threshold values  point of subjective equality (PSE) value where standard and test 
appear to match (usually physically equal)  just-noticeable-difference (JND) proportional to the standard 
deviation of the responses, talk later about why particular mulitplier  two difference thresholds  
upper value where test is just noticeably brighter  lower value where test is just noticeably darker 
  Interval of uncertainty range where subject is uncertain whether test is brighter or darker  can 
be used as measure of how much precision required in representation example: quantization in JPEG, as 
long as differences are within interval of uncertainty images will be indistinguishable  MOA is fast, 
easy/interactive, but requires continuously variable stim, not always possible  method of limits (MOL) 
addresses this, essentially a discrete stimulus version of MOA  will use the same example: determining 
brightness difference thresholds  fixed standard, test stimulus set with range that spans presumed threshold 
 experiment is run as a series of trials on each trial standard and test presented together, subject 
asked if test is brighter or darker than standard  if judged brighter, next darker stimulus is shown 
and the question is asked again  trial proceeds until response changes, on change, value midway between 
brighter/darker appearing stimuli is recorded  procedure similar to method of adjustment in terms of 
presenting a pattern of ascending, descending trials, random starting values  data is analyzed similarly 
to MOL  responses to each test intensity are recorded for each descending and ascending trial series 
 response crossover points are determined  series and overall means and stdev s are calculated and 
the various threshold values described earlier are determined  advantage of method of limits: works 
for discrete stimulus sets  disadvantage: inefficient method for finding threshold, requires many trials, 
lead to subject boredom, fatigue  to address efficiency issues variations on the MOL have been developed 
 staircase, PEST, QUEST  graph shows a series of method of limits trials  trial series repeatedly traverse 
threshold  trials far from threshold don t provide any information, waste time, effort one way to increase 
efficiency is to start next trial series where last one left off  called staircase method because of 
step-up, step-down look of the trial record  approach raises two issues 1) when to stop; 2) what values 
to include in threshold calculations  different conventions used  stopping: after some number of reversals 
 data values: include all vs. discard first n familiarization trials   staircase method improves the 
efficiency of the method of limits but stopping and data analysis criteria are somewhat ad-hoc  In last 
20 years two more principled improvements on the MOL have been introduced: PEST, QUEST  PEST: Parameter 
Estimation by Sequential Tracking  like staircase (step-up/down), but two differences  step size reduced 
on reversals  test intensity varied until a given performance level is achieved (e.g. 75% correct) 
        other method is known as QUEST test intensity chosen so data gives best fit to what 
s called a psychometric function model of observer performance, will explain later  particular advantage 
of QUEST is that it provides the maximum information gain per trial about the threshold value increases 
efficiency: threshold can be determined in the shortest number of trials  third of the Fechner s classical 
threshold methods is method of constant stimuli (MCS)  essentially randomized version of the method 
of limits  standard, discrete test set  experimental method is simple: series of trials where members 
of the test set are presented with the standard in pairs  subject is asked if test square is brighter 
or darker than the standard  random presentation controls for a variety of bias/errors that occur with 
ordered presentations like MOL  dat are analyzed by first tabulating how frequently each test patch 
was judged brighter or darker than the standard  test intensity 7: darker 19, brighter 1  test intensity 
12: darker 2, brighter 18  test intensity 10: darker 9, brighter 11 (uncertainty in response)  can 
convert frequencies to proportions by dividing by number of trials (in this case 20)  p s are the probability 
that a given test intensity will be seen as brighter than the standard  if we plot p s vs. test intensity 
we get what is known as a psychometric function (or frequency of seeing curve)  shows that the liklihood 
of seeing the test as brighter than the standard increases with the intensity of the standard  but where 
is the threshold?  to determine the thresholds for discriminating between the patches, we need to fit 
the data with a model that describes the observer s response behavior  there are a number of different 
mathematical models:  cumulative normal (a.k.a. probit analysis)  logistic function  Weibull function 
  subtle differences between these models, used by different communities, all basically S-shaped functions 
 for illustration we re going to use the cumulative normal model  function on the right shows the model 
 p is the integral of an exponential function  z is a parameter that is related to both the mean and 
the variance of the responses collected in the experiment  we can calculate the z values that correspond 
to the probabilities in the psychometric (cumulative normal) function (NORMSINV function in Excel)  
and we can plot these z values as a function of the test intensities  if the cumulative normal model 
is a good fit to the data, then the z values should fall along a straight line  by using linear regression 
( fit trendline in Excel) we can find both the slope and intercept of this line and the goodness of fit 
 R^2 of 0.99 shows that the normal model is a good fit to the data  slope (m) = 0.57  intercept (b) 
= -5.68  can use slope and intercept to calculate the mean and stdev of the cumulative normal function 
that fits the data  graph on left shows this S-shaped curve and its fit to the data  can use this model 
to determine the standard set of threshold values for the experiment  point of subjective equality corresponds 
to the mean of the data (here given by (p= 0.50))  equal probability of seeing test patch as brighter 
or darker  value is close to the standard so no observer/experimental bias   JND is again proportional 
to the standard deviation, but now we can see why the 0.67 factor is used:  because under the cumulative 
normal model adding this value to the PSE brings us to the (p=0.75) point on the curve (which by convention 
is taken as the upper threshold value)  reason for (p=.75): this is halfway between perfect performance 
(p=1.0) and the (p=0.5) performance that would be expected from just guessing  in practical terms p= 
0.75 means that a test intensity of 11.17 will be seen as brighter than the standard on 3 out of 4 trials 
  last two lines show that we can calculate the lower threshold and interval of uncertainty using similar 
logic  point of this exercise: in any analysis of experimental data some statistical model is always 
used  in the analyses of the first two methods we just assumed that a normal model fit the data and 
used means and standard deviations to calculate the threshold values  but in this example we were actually 
explicit about testing whether the model we used (in this case the cumulative normal) was actually a 
good description of the data  usually think of threshold as a step/ breakpoint between seeing and not 
seeing  shown in graph on left  increase stimulus intensity, some low range where it s undetectable 
(or indiscriminable) (probability 0.0)  but at some point the threshold is crossed and the stimulus 
is always seen (probability (1.0)  in reality we see that real observers perform differently  actual 
threshold data shows that in many experiments the probability of detecting the stimulus increases with 
intensity  but in reality there is no discrete step in the curve that corresponds to the theoretical 
notion of a threshold  why do observers behave this way?  what does this mean about our ideas of thresholds 
in perception?  to get a handle on variation in threshold measures we need to understand what s known 
as the signal detection problem  motivation for Signal Detection Theory (SDT)  SDT developed in the 
40 s  provides a framework for predicting observer s behavior on important perceptual tasks  such as 
radar operator  radiologist, x-ray  scientific visualization, data mining applications where judgment 
is based on subtle visual cues     20 trials/stimulus test int. f "darker" f "brighter" p "brighter" 
z 7 19 1 0.04 -1.75 8 18 2 0.12 -1.17 9 14 6 0.32 -0.47 10 9 11 0.56 0.15 11 6 14 0.69 0.50 12 2 18 0.89 
1.23 13 1 19 0.95 1.64  20 trials/stimulus test int. f "darker" f "brighter" p "brighter" z 7 19 1 0.04 
-1.75 8 18 2 0.12 -1.17 9 14 6 0.32 -0.47 10 9 11 0.56 0.15 11 6 14 0.69 0.50 12 2 18 0.89 1.23 13 1 
19 0.95 1.64  20 trials/stimulus test int. f "darker" f "brighter" p "brighter" z 7 19 1 0.04 -1.75 
8 18 2 0.12 -1.17 9 14 6 0.32 -0.47 10 9 11 0.56 0.15 11 6 14 0.69 0.50 12 2 18 0.89 1.23 13 1 19 0.95 
1.64  20 trials/stimulus test int. f "darker" f "brighter" p "brighter" z 7 19 1 0.04 -1.75 8 18 2 0.12 
-1.17 9 14 6 0.32 -0.47 10 9 11 0.56 0.15 11 6 14 0.69 0.50 12 2 18 0.89 1.23 13 1 19 0.95 1.64  20 
trials/stimulus test int. f "darker" f "brighter" p "brighter" z 7 19 1 0.04 -1.75 8 18 2 0.12 -1.17 
9 14 6 0.32 -0.47 10 9 11 0.56 0.15 11 6 14 0.69 0.50 12 2 18 0.89 1.23 13 1 19 0.95 1.64   original 
context imagine that you re a radar operator  looking at screen, trying to see blips of light that 
represent enemy planes  very important that you detect the planes, but very hard because the blips are 
sometimes faint  so imagine you re focused on a particular location on the screen  for each sweep of 
the beam (a trial) you re going to experience some intensity  however because of noise, the value of 
the intensity that you ll experience will vary from sweep to sweep  both external (physical) and internal 
(neural/psychological)sources of noise  over time, the probability that you ll experience a given intensity 
of sensation on a trial can be described by the noise distribution (N) (N) represents the variation 
in perceived intensity just due to noise in the system  applying a fixed intensity signal as input to 
the system will shift distribution up because of noise, range of sensations will remain the same (SN 
distribution)  if you re the radar operator in this situation, you ve got a problem  all you experience 
on each sweep is a some intensity of sensation  but as the graph shows, because of the overlap in the 
distributions, intensity could be caused by a real signal or could be due to noise alone  no way to 
objectively distinguish between these two situations  operator needs to make a DECISION that intensities 
greater than some level represent a true signal and values below are only noise outcome of this decision 
process is directly related to the threshold values we measure in psychophysical experiments to understand 
variation we see in threshold measures, need to look closer at operator s decision-making processes 
 consider that each sweep of the beam is a trial in a threshold experiment on each trial there is a 
stimulus  signal embedded in noise  noise alone   and on each trial observer has to give a response 
 yes there was a signal  no there wasn t a signal   for each trial, four possible outcomes  signal, 
yes hit  signal, no miss  no signal, yes false alarm   no signal, no correct reject over many 
trials we can calculate the probabilities that the observer will respond in a particular way  notation 
is shorthand, e.g. hit  p(yes|SN) probability of observer giving a yes response, given that a signal 
was actually present   experimental results tabulated this way form stimulus/response (SR) matrix 
 probabilities in the SR matrix correspond to areas under the noise and signal+noise distributions  
for example:  gray area under SN = hits  striped area under N = false alarms  unshaded areas correspond 
to misses and correct rejects respectively   boundary of these areas define a location along the intensity 
axis  location represents the observer s criterion for deciding whether a particular intensity represents 
a signal or not  notice:  responses above criterion are yes s (seen)  responses below criterion are 
no s (not seen)  yes/no responses independent of whether the signal was actually present  i.e misses 
and false alarms (observer s performance not perfect)   so how does the observer s decision criterion 
influence the thresholds we measure in psychophysical experiments  imagine an experiment where we present 
a fixed intensity signal on each trial  top figure, can see what happens when observer adopts a lax 
criterion for detection  willing to say yes when not sure, e.g. (going on alert in missle silo)  hit 
rate high, misses low, false alarms relatively high observer would report sensation intensity as seen 
  in experiment would be above threshold   in contrast, bottom figures shows what happens when observer 
adopts a strict criterion observer wants to be sure before saying yes , e.g. (firing missles!)  hit 
rate lower, misses higher, false alarms lower observer would report given sensation intensity as not 
seen  in experiment would be below threshold  many psychological and situational factors affecting 
observer s decision criterion  cost/payoff, expectation/frequency,, attention/vigilance/fatigue, learning/experience 
  important contribution of SDT  SDT allows us to derive two measures that separate the observer s 
underlying sensitivity from their decision making processes  called d and beta  beta = measure of the 
observers response bias propensity to say yes/no when uncertain = ratio of values of SN and N at criterion 
 d = measure of observer s true sensitivity independent of any decision making factors = distance between 
means of N / SN distributions scaled by variance in distributions = related to amount of overlap of 
distributions, potential for uncertainty whether sensation is signal or noise  measures allow us to 
tease apart the sensory and non-sensory factors that affect thresholds measured in experiments  got 
into this topic trying to understand variation seen in threshold measures  SDT makes several contributions 
to understanding  no true/sharp threshold cutoffs between seen/not seen  detection and discrimination 
processes near threshold are probabilistic due to noise (external/internal)  thresholds measured in 
experiments are a product of both sensory and non-sensory psychological factors (decision making)  effects 
of these factors can be teased apart using SDT methods (d , beta)  influence of SDT has led to the development 
of important variation on classical threshold method  known as two-alternative forced choice (2AFC) 
method  figure illustrates method for a difference threshold experiment on each trial two pairs of 
stimuli presented observer has to indicate which pair contained the difference  allows direct measurement 
of guessing under conditions of uncertainty, allows more direct measures of underlying sensitivity  
so this brings us to the end of the section on threshold methods  take home messages:  two kinds of 
thresholds: absolute/detection, difference/discrimination  variety of experimental methods for measuring 
thresholds:  Fechner  adjustment, limits, constant stimuli  SDT    next section will be talking 
about psychophysical scaling  whereas threshold methods concerned with measuring the limits of perception 
 scaling deals with everything else  goal is to quantify the relationships between the physical properties 
of objects and their perceptual appearances  typical scaling issue: which is twice as bright?  e.g. 
might use results of scaling experiments to build a scale relating physical radiance and perceived brightness 
 first thing to understand:  many different types of scales we can use to measure properties of objects 
 different scales have different mathematical properties   bike race as illustration  NOMINAL: simplest 
scale nominal = naming organize racers by teams (france, italy, us)  allows group/categorize like 
objects  doesn t allow us to say anything quantitative about relations between groups  ORDINAL: allows 
ordering of objects w.r.t some criterion  e.g. who finished 1st, 2nd,3rd, ordinal scales allow objects 
to be ranked but numbers are only labels, cannot be used in calculations  e.g. 2nd place finisher did 
not necessarily take twice as long as 1st place  INTERVAL: to measure distances (space or time) need 
to construct interval scale  e.g. relative finishing times (2nd finished 5 mins after 1st, 3rd finished 
4 mins after 2nd)  allows us to tell how far apart things are but not whether the differences are small 
or large in any absolute sense  e.g 5 mins difference between 1st and 2nd in Tour de France might be 
small, but in a 10 mile race would be large  RATIO: to understand magnitude of differences need a ratio 
scale  e.g absolute elapsed times of racers  comparing absolute times allows us to say 2nd rider took 
10% longer than 1st, 3rd place took twice as long as second  important feature of ratio scales: have 
a zero point  allows calculation of magnitudes   wide variety of methods have been developed to derive 
psychophysical scales  can be divided into two main categories: indirect and direct  indirect:  subjects 
are asked to make simple judgments about object properties (eg. grouping/sorting/ordering)  numerical 
scale values are derived using statistics on these judgments   direct:  subjects directly assign numerical 
values to their perceptions, or adjust stimuli to stand in some mathematical relation  numerical scales 
are a direct byproduct of the experiment, minimal statistical analysis needed   division relates to 
an ongoing debate in psychology about whether people use numbers linearly e.g. jelly bean problem: good 
at estimating if numbers (or range) are small, but accuracy decreases for larger magnitudes  first is 
rating  two forms: numerical, graphical numerical: subject shown examples of endpoints of scale  
asked: on a scale of 0 to 100 tell me how bright this patch is   graphical:  subject shown endpoint 
examples connected by a line segment  asked to make a mark representing where the test patch would fall 
on an imaginary scale connecting the endpoints  numerical ratings derived by measuring distance from 
one endpoint  leverages fact that over short distances real length and perceived length are directly 
related   rating methods are widely used, generally well understood, liked by subjects  but subject 
to a number of problems (range/frequency effects) that make them less than reliable for building psychophysical 
scales, effects illustrated in the graphs  each data point shows the mean rating value given to some 
physical stimulus in a scaling experiment, the red and green curves show the scales obtained for different 
experiments  first graph shows what are called range effects  range of intensities used in the green 
experiment was larger than in red experiment (0-100 vs. 0-60), differences in range lead to scales with 
different slopes  results of experiment affected by choice of range of test stimuli  shouldn t be the 
case if the method was accurately measuring the true relationship between physical stimulus intensity 
and its perceptual appearance   second graph shows frequency effects  red and green insets show how 
frequently each of the test stimuli was shown to the subject over the course of the experiment, red experiment: 
low intensities shown more frequently than hi, green: vice versa  differences in the shapes of the curves 
show that the scales are affected by how often subjects see the different stimuli  again this shouldn 
t be the case if the method is accurately measuring the psychophysical relationship   bottom graph 
shows distribution effects  in red experiment stimuli are mostly clustered near the low intensity range 
with a few high intensity values, green experiment opposite: mostly high intensity test stimuli with 
few low values  unequal distribution of test stimuli along the range leads to different scale estimates 
 again this shouldn t be the case if the rating method were reliable   doesn t mean that rating methods 
are all bad there are methods for controlling these effects  need to be aware of these problems if you 
decide to use rating methods in your experiments  second indirect scaling method is pair comparison 
 as name implies stimuli from the test set are organized into pairs as shown in the table on each trial 
one pair is presented and subject is asked (for example) to say which one is brighter  two important 
issues about choosing the test stimuli  number of pairs scales as (n)(n-1)/2 so if set is large number 
of trials can get large too  values need to be close enough together so subject is confused/makes mistakes 
(as in trial 4) since statistics for deriving scale values depend on some pair differences being sub-threshold 
  to derive a psychophysical scale from the results of this experiment, can use an analysis method 
called the law of comparative judgment  this and the law of categorical judgment were developed by Thurstone 
in the 20 s as part of his general theory of scaling  Thurstone is the father of modern scaling methods, 
similar in stature to Fechner  whole books written about Thurstonian scaling and the law of comparative 
judgment (some listed in bibliography)  will skip over theory and just describe the mechanics of the 
analysis  first tabulate the data into the frequency matrix  each cell indicates the number of times 
one patch was judged brighter than the other  e.g. (10 vs. 9, 35 vs. 15)  next convert these frequencies 
into proportions by dividing by the number of trials for each pair (in this case 50)  next, assuming 
that the data are normally distributed, can convert the proportions to z-scores  column means of the 
z-scores correspond to the values of the test stimuli on an interval scale  notice that this is an interval 
scale  distances between stimuli indicate how similar the test patches are perceived to be in brightness 
 e.g. 11 and 12 seen as very close  BUT scale is in terms of relative brightness (relative to mean of 
set)  no true zero, can t say 13 is perceived to be twice as bright as 10 (need a ratio scale)  take 
home message: by using this analysis (law of comparative judgment, normally distributed data) we re able 
to derive an interval scale of brightness from ordinal judgments (pair comparisons)  pair comparison 
is a good method, but can often lead to many trials, boring for subjects, problematic effects of fatigue 
 another indirect scaling method that can provide similar results is ranking  subject is shown entire 
test set in scrambled order  asked to place the patches in order (for example from dark to bright) 
 subjects generally like ranking experiments (puzzle-like)  happy subjects provide good data  figure 
on lower right shows the results produced by four subjects  notice that subjects are not complete accurate 
in their rankings (e.g. some reversals)  also not complete agreement between subjects  perceptual scale 
can be derived by drawing on Thurstone s law of comparative judgment  first step: tabulate the data 
showing how often each patch was placed in each rank position (data from 100 subjects)  notice that 
least intense consistently ranked lowest, but some spread, similar for increasing intensities  next: 
calculate the mean rank (7*78 + 6*11 + /100)  mean rank values can be used directly to derive an ordinal 
scale (no surprises, monotonic)  but mean ranks and ordinal scale say nothing about the perceived differences 
between the patches (monotonic not same as linear)  by again assuming a normal model and using Thurstone 
s law of comparative judgment we can derive an interval scale from the rank data  first derive proportions 
(# of ranks mean rank)/(# ranks 1)  then convert proportions to z scores  z-scores correspond to 
values of patches on an interval scale of relative brightness  notice again: equal physical spacing 
does not lead to equal perceived spacing  last of indirect scaling methods we ll look at is category 
scaling  experimental procedure is the same as categorical rating method discussed earlier  subject 
shown members of test set and asked to assign them to labeled categories  figure on lower right shows 
results from 3 subjects  again notice confusions/reversals/non-agreement in categorization  may have 
noticed that experimental procedure is the same as the categorical rating method I showed earlier that 
I said was problematic  difference between categorical rating and category scaling:  in categorical 
rating categories are assumed to be linear and equally spaced (not always true)  in category scaling 
can apply Thurstone s law of categorical judgement is applied to find the true shape and spacing of the 
scale   category scaling analysis:  first step is to tabulate the data  each cell indicates # of 
times a patch was placed into each category (200 trials/patch)  notice low intensity placed into dark 
categories, but some spread   next calculate cumulative frequencies for each patch e.g. patch 9 judged 
dark on 100 trials, dark or very dark on 138 trials,  next use cumulative frequencies to calculate cumulative 
proportions and z-scores that represent the boundaries between categories e.g. value in cell 1,1, represents 
an estimate of the boundary between the very dark and dark categories  finally, using the z-scores and 
the applying the law of categorical judgment  can calculate the values of the patches on an interval 
scale of relative brightness (shown in orange)  can also calculate the apparent locations of the boundaries 
between the different categories used in the experiment (shown in red)  notice categories not equally 
spaced or equally wide  e.g. patch values 10,11,12 all perceived as moderately bright , none as very 
bright  law of comparative judgment allows us to test the assumptions behind categorization methods 
and derive more meaningful scales  indirect methods all based on having subjects make simple judgments 
of object ordinal properties (categorizing, ranking, rating), interval scales derived through statistical 
analysis (Thurstone s laws)  in contrast, in direct methods subjects directly assign numbers to what 
they perceive or adjust stimuli to stand in particular numerical relationships  direct methods are associated 
with S.S. Stevens who introduced/championed them in the 50 s  controversial because they rest on assumptions 
about how people use numbers, but beneficial because they can be used to develop ratio scales (most powerful) 
 three direct methods:  EQUISECTION: method of adjustment-like procedure  test patch is bounded on 
either side by standards, subject is asked to adjust the brightness of the test patch until it falls 
halfway between the standards  in second trial, test patch becomes the lower standard, and so on  method 
used to develop the munsell color space  MAGNITUDE PRODUCTION: also an adjustment procedure  subject 
shown a standard, then asked to adjust a test patch until it s half as bright, twice as bright, etc 
 settings provide direct estimates of a ratio scale  MAGNITUDE ESTIMATION: most widely known of direct 
methods: magnitude estimation, most promoted by Stevens,  subject shown a standard, told has a brightness 
of 10 , then shown series of test patches, asked to assign numbers representing apparent brightness w.r.t 
standard  choice of range of numbers is up to the subject, but studies show that after some settling 
down, subjects are internally consistent   analysis of the magnitude estimation data is straightforward 
 simply involves averaging the estimates given by different subjects  table shows magnitude estimates 
assigned by different subjects to our standard test set  notice use of different numerical ranges by 
subjects  e.g. (subject A 1.1 to 788), (subject B 0.5 to 39)  to compensate for these differences, 
scale values are calculated using the geometric mean (average of logarithms of measured values)  graph 
on right shows scale derived using magnitude estimation (relating apparent brightness to patch intensity) 
 psychophysicists have tried to develop general mathematical models to describe psychophysical scales 
measured in experiments  earliest model was Fechner s law  based on Weber s law JNDs are proportional 
to intensity of standard  stacking up JNDs will produce a scale of perceptually uniform steps  Fechner 
s scales are all of the form S = k log I (because of proportionality of JNDs)   Steven s measured psychophysical 
scales for many different sensory modalities using magnitude estimation techniques  (brightness, loudness, 
weight, heat/cold, electric shock)  found that his scales could all be described by a power function 
S = k I ^b  known as Stevens power law  different k s and exponents for different sensory modalities 
 most are compressive functions  but some physically damaging ones like heat, electric shock have accelerating 
relations (protective mechanism?)    central issue: validity of direct vs. indirect scaling methods 
 final scaling method: multidimensional scaling  very useful tool for discovering psychophysical relations 
in complex stimuli (real objects, images)  MDS is a method for finding the hidden structure in a dataset 
 non-perceptual example  given a table that specifies distances between objects in a dataset example: 
table shows distances between major US cities, table gives proximities but not spatial relations  MDS 
will attempt to reconstruct a space that best describes it s overall structure  picture on lower left 
shows the result produced by an MDS algorithm which has placed all the cities in their proper locations 
on the US map  the graph on the right shows what is known as the stress diagram which indicates the 
goodness of fit of the MDS solution as dimensions are added to the solution. Notice the large drop in 
stress as we move from a 1-dimensional to a 2-dimensional solution because of the better fit to the data 
 3d is only slightly better (curvature of earth), so for most purposes 2d can be regarded as good solution 
 classical MDS solutions only specific up to rotations, reflections, necessary element of interpretation 
in understanding MDS results  MDS is useful for trying to understand the psychophysical dimensions of 
complex stimuli of the kind we often encounter in graphics  goal of psychophysics is to quantify the 
relationships between the physical properties of objects and their perceptual appearances  two psychophysical 
quantities: thresholds and scales  two types of thresholds: detection and discrimination  absolute 
sensitivity vs. ability to detect differences in visible stimuli  experimental methods introduced by 
Fechner: adjustment, limits, constant stimuli  table shows they are all really variations on the same 
theme, differ on  stimulus set: continuous or discrete  stimulus presentation: ordered or randomized 
   signal detection theory  contribution: thresholds are a product of sensory and psychological factors 
(cost/benefit, expectations)  SDT provides tools to tease these apart   second topic was scaling 
 four types of scales  nominal, ordinal, interval, ratio  different mathematical properties   two 
main classes of scaling methods: indirect, direct  table shows different scaling methods discussed, 
with the kinds of scales they re most often used to derive  finally, multidimensional scaling: method 
for finding the psychophysical dimensions in complex stimuli  for threshold studies need to select stimuli 
that span threshold range  for scaling studies need to select/generate stimuli that span a significant 
range of the parameter of interest can generally be selected by eyeballing or informal pretesting  
when generating stimuli need to be sure that all variables/parameters but the ones being tested are held 
constant or controlled for  need to select an experimental procedure that is appropriate to the stimulus 
set discrete, continuous, uni/multi dimensional, size of stimulus set  display viewing conditions must 
not interfere with/confound properties being tested/measured  user input/response methods must not limit/impede 
user response  running experiments is a time/labor intensive process for all involved  much time/effort/mistakes 
can be saved by running small pilot studies to test/refine stimuli/design/procedure  subject pool must 
be representative of find population of interest undergraduate psychology majors!, vision researchers! 
 many organizations require procedures to physically/legally protect human subjects  human subjects 
committees, independent review boards (IRB s)  ignore at your own peril  think long and hard about 
what task you re going to ask your subjects to do and how you re going to instruct them to do it  data 
collected often depends critically on user instructions  understand the effects of user learning, problem 
solving strategy development  learn from the masters by closely reading methods/procedures sections 
of papers in high quality journals (J. of Vis., Vis. Res., J. Opt. Soc. Am., Perception, SIGGRAPH, ACM 
TAP,...)            ongoing debate over the years about whether Fechner s or Steven s law 
is correct scaling model  stimulus set continuous discrete presentation ordered method of adjustment 
method of limits random method of constant stimuli  scale type nominal ordinal interval ratio indirect 
rating X X pair comparison X X ranking X X category scaling X X direct equisection X magnitude est./prod. 
X      
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401244</section_id>
		<sort_key>1120</sort_key>
		<section_seq_no>25</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Real time physics]]></section_title>
		<section_page_from>25</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098872</person_id>
				<author_profile_id><![CDATA[81100615490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098873</person_id>
				<author_profile_id><![CDATA[81100148921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098874</person_id>
				<author_profile_id><![CDATA[81100415142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[James]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098875</person_id>
				<author_profile_id><![CDATA[81335498663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nils]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Th&#252;rey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401245</article_id>
		<sort_key>1130</sort_key>
		<display_label>Article No.</display_label>
		<pages>90</pages>
		<display_no>88</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Real time physics]]></title>
		<subtitle><![CDATA[class notes]]></subtitle>
		<page_from>1</page_from>
		<page_to>90</page_to>
		<doi_number>10.1145/1401132.1401245</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401245</url>
		<abstract>
			<par><![CDATA[<p>Physically based simulation is a significant and active research field in computer graphics. It has emerged in the late eighties out of the need to make animations more physically plausible and to free the animator from explicitly specifying the motion of complex passively moving objects. In the early days, quite simple approaches were used to model physical behavior such as mass-spring networks or particle systems. Later, more and more sophisticated models borrowed from computational sciences were adopted. The computational sciences appeared decades before computer graphics with the goal of replacing real world experiments with simulations on computers. In contrast, the aim of using physical simulations in graphics was, and still is, the reproduction of the visual properties of physical processes for special effects in commercials and movies. Computer generated special effects have replaced earlier methods such as stop-motion frame-by-frame animation and offer almost unlimited possibilities.</p> <p>Meanwhile, the rapid growth of the computational power of CPUs and GPUs in recent years has enabled real time simulation of physical effects. This possibility has opened the door to an entirely new world, a world in which the user can interact with the virtual physical environment. Real time physical simulations have become one of the main next-generation features of computer games. Allowing user interaction with physical simulations also poses new challenging research problems. In this class we address such problems and present basic as well as state-of-the-art methods for physically based simulation in real time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.5</cat_node>
				<descriptor>Modeling methodologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010342.10010343</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Model development and analysis->Modeling methodologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098876</person_id>
				<author_profile_id><![CDATA[81100615490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[nVidia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098877</person_id>
				<author_profile_id><![CDATA[81100148921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Autodesk]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098878</person_id>
				<author_profile_id><![CDATA[81100415142]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[James]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098879</person_id>
				<author_profile_id><![CDATA[81335498663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nils]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Th&#252;rey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Zurich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073380</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{AN05} Alexis Angelidis and Fabrice Neyret. Simulation of smoke based on vortex filament primitives. <i>SCA '05: Proceedings of the 2005 ACM SIGGRAPH/ Eurographics symposium on Computer animation</i>, pages 87--96, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Ang07} Roland Angst. Control Algorithms for Interactively Animated Fluid Characters. Master Thesis, Computer Graphics Laboratory, ETH Zurich, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218068</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{ANSN06} Alexis Angelidis, Fabrice Neyret, Karan Singh, and Derek Nowrouzezahrai. A controllable, fast and stable basis for vortex based smoke simulation. <i>SCA '06: Proceedings of the 2006 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, pages 25--32, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276502</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{BBB07} Christopher Batty, Florence Bertails, and Robert Bridson. A fast variational framework for accurate solid-fluid coupling. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</i>, page 100, New York, NY, USA, 2007. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>195266</ref_obj_id>
				<ref_obj_pid>195261</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{Ber94} Jean-Pierre Berenger. A perfectly matched layer for the absorption of electromagnetic waves. <i>J. Comput. Phys.</i>, 114(2):185--200, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{BGK54} P. L. Bhatnagar, E. P. Gross, and M. Krook. A model for collision processes in gases. <i>Phys. Rev.</i>, 94:511--525, 1954.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272719</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{BT07} M. Becker and M. Teschner. Weakly Compressible SPH for Free Surface Flows. <i>ACM SIGGRAPH / Eurographics Symposium on Computer Animation</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{CCM92} Hudong Chen, Shiyi Chen, and William H. Matthaeus. Recovery of the Navier-Stokes equations using a lattice-gas Boltzmann method. <i>Phys. Rev. A</i>, 45(8):R5339--R5342, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618433</ref_obj_id>
				<ref_obj_pid>616046</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{CdVLHM97} Jim X Chen, Niels da Vitoria Lobo, Charles E. Hughes, and J. Michael Moshell. Real-time fluid simulation in a dynamic virtual environment. <i>IEEE Comput. Graph. Appl.</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272720</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{CFL &#60;sup&#62;+&#60;/sup&#62;07} Nuttapong Chentanez, Bryan E. Feldman, Franc&#231;ois Labelle, James F. O'Brien, and Jonathan R. Shewchuk. Liquid simulation on lattice-based tetrahedral meshes. In <i>SCA '07: Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, pages 219--228, Aire-la-Ville, Switzerland, Switzerland, 2007. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{CLT07} Keenan Crane, Ignacio Llamas, and Sarah Tariq. Real-time simulation and rendering of 3d fluids. In <i>GPU Gems 3</i>. Addison-Wesley Professional, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015733</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{CMT04} Mark Carlson, Peter John Mucha, and Greg Turk. Rigid fluid: Animating the interplay between rigid bodies and fluid. <i>ACM Trans. Graph.</i>, 23(3), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{Dur01} D. R. Durran. Open boundary conditions: Fact and fiction. In <i>IUTAM Symposium on Advances in Mathematical Modelling of Atmosphere and Ocean Dynamics</i>, pages 1--18. Kluwer Academic Publishers, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566645</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{EMF02} D. Enright, S. Marschner, and R. Fedkiw. Animation and Rendering of Complex Water Surfaces. <i>ACM Trans. Graph.</i>, 21(3):736--744, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{FdH&#60;sup&#62;+&#60;/sup&#62;87} Uriel Frisch, Dominique d'Humi&#232;res, Brosl Hasslacher, Pierre Lallemand, Yves Pomeau, and Jean-Pierre Rivert. Lattice Gas Hydrodynamics in Two and Three Dimensions. <i>Complex Systems</i>, 1:649--707, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{FF01} Nick Foster and Ronald Fedkiw. Practical animation of liquids. In <i>Proc. of ACM SIGGRPAH</i>, pages 23--30, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>244315</ref_obj_id>
				<ref_obj_pid>244304</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{FM96} N. Foster and D. Metaxas. Realistic Animation of Liquids. <i>Graphical Models and Image Processing</i>, 58, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882336</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{FOA03} Bryan E. Feldman, James F. O'Brien, and Okan Arikan. Animating suspended particle explosions. In <i>Proc. of ACM SIGGRAPH</i>, pages 708--715, Aug 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073281</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{FOK05} Bryan E. Feldman, James F. O'Brien, and Bryan M. Klingner. Animating gases with hybrid meshes. <i>ACM Trans. Graph.</i>, 24(3):904--909, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846284</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{GHDS03} Eitan Grinspun, Anil Hirani, Mathieu Desbrun, and Peter Schroder. Discrete shells. In <i>Proceedings of the ACM SIGGRAPH Symposium on Computer Animation</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276438</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{GHF&#60;sup&#62;+&#60;/sup&#62;07} Rony Goldenthal, David Harmon, Raanan Fattal, Michel Bercovier, and Eitan Grinspun. Efficient simulation of inextensible cloth. <i>ACM Trans. Graph.</i>, 26(3):49, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{GM97} S. F. Gibson and B. Mitrich. A survey of deformable models in computer graphics. <i>Technical Report TR-97-19, MERL</i>, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378530</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Hah88} James K. Hahn. Realistic animation of rigid bodies. In <i>SIGGRAPH '88: Proceedings of the 15th annual conference on Computer graphics and interactive techniques</i>, pages 299--308, New York, NY, USA, 1988. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{Hes07} Peter Hess. Extended Boundary Conditions for Shallow Water Simulations. Master Thesis, Computer Graphics Laboratory, ETH Zurich, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>178177</ref_obj_id>
				<ref_obj_pid>178173</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{Hig94} Robert L. Higdon. Radiation boundary conditions for dispersive waves. <i>SIAM J. Numer. Anal.</i>, 31(1):64--100, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{HL97} X. He and L.-S. Luo. Lattice Boltzmann model for the incompressible Navier-Stokes equations. <i>J. Stat. Phys.</i>, 88:927--944, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545288</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{HNC02} Damien Hinsinger, Fabrice Neyret, and Marie-Paule Cani. Interactive Animation of Ocean Waves. <i>Proc. of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer animation</i>, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{HSCD96} Shuling Hou, James D. Sterling, Shiyi Chen, and Gary Doolen. A Lattice Boltzmann Subgrid Model for High Reynolds Number Flow. <i>Fields Institute Communications</i>, 6:151--166, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{HTK&#60;sup&#62;+&#60;/sup&#62;04} B. Heidelberger, M. Teschner, R. Keiser, M. Mueller, and M. Gross. Consistent penetration depth estimation for deformable collision response. pages 339--346, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{HYP76} J. Hardy, O. De Pazzis Y., and Pomeau. Molecular dynamics of a classical lattice gas: Transport properties and time correlation functions. <i>Physical Review A</i>, 13:1949--1960, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{Jak01} T. Jakobsen. Advanced character physics the fysix engine. <i>www.gamasutra.com</i>, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311542</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{JP99} Doug L. James and Dinesh K. Pai. Artdefo, accurate real time deformable objects. In <i>Computer Graphics Proceedings</i>, Annual Conference Series, pages 65--72. ACM SIGGRAPH 99, August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{KC04} Pijush Kundu and Ira Cohen. <i>Fluid Mechanics</i>. Elsevier Academic Press, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141961</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{KFCO06} Bryan M. Klingner, Bryan E. Feldman, Nuttapong Chentanez, and James F. O'Brien. Fluid animation with dynamic meshes. <i>ACM Trans. Graph.</i>, 25(3):820--825, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97884</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{KM90} M. Kass and G. Miller. Rapid, Stable Fluid Dynamics for Computer Graphics. <i>ACM Trans. Graph.</i>, 24(4):49--55, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Lov03} J&#246;rn Loviscach. Complex Water Effects at Interactive Frame Rates. <i>Journal of WSCG</i>, 11:298--305, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{LvdP02} Anita T. Layton and Michiel van de Panne. A numerically efficient and stable algorithm for animating water waves. <i>The Visual Computer</i>, 18(1):41--53, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{LWK03} Wei Li, Xiaoming Wei, and Arie E. Kaufman. Implementing lattice Boltzmann computation on graphics hardware. <i>The Visual Computer</i>, 19(7--8):444--456, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846298</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{MCG03} Matthias M&#252;ller, David Charypar, and Markus Gross. Particle-based fluid simulation for interactive applications. <i>Proc. of the 2003 ACM Siggraph/Eurographics Symposium on Computer Animation</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{MDH07} Xing Mei, Philippe Decaudin, and Baogang Hu. Fast hydraulic erosion simulation and visualization on gpu. In <i>Pacific Graphics</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1006087</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{MG04} Matthias M&#252;ller and Markus Gross. Interactive virtual materials. In <i>GI '04: Proceedings of Graphics Interface 2004</i>, pages 239--246, School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada, 2004. Canadian Human-Computer Communications Society.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{MHR06} M. M&#252;ller, B. Heidelberger M. Hennix, and J. Ratcliff. Position based dynamics. <i>Proceedings of Virtual Reality Interactions and Physical Simulations</i>, pages 71--80, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{Mon92} J. Monaghan. Smoothed particle hydrodynamics. <i>Annu. Rev. Astron. Phys.</i>, 30:543--574, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073402</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{MSKG05} Matthias M&#252;ller, Barbara Solenthaler, Richard Keiser, and Markus Gross. Particle-based fluid-fluid interaction. <i>Proc. of the 2005 ACM Siggraph/Eurographics Symposium on Computer Animation</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015744</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{MTPS04} Antoine McNamara, Adrien Treuille, Zoran Popovic, and Jos Stam. Fluid control using the adjoint method. <i>ACM Trans. Graph.</i>, 23(3):449--456, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{MZ88} Guy R. McNamara and Gianluigi Zanetti. Use of the Boltzmann Equation to Simulate Lattice-Gas Automata. <i>Phys. Rev. Lett.</i>, 61(20):2332--2335, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566643</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{NFJ02} Duc Quang Nguyen, Ronald Fedkiw, and Henrik Wann Jensen. Physically based modeling and animation of fire. In <i>SIGGRAPH '02: Proceedings of the 29th annual conference on Computer graphics and interactive techniques</i>, pages 721--728, New York, NY, USA, 2002. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{NMK&#60;sup&#62;+&#60;/sup&#62;05} A. Nealen, M. M&#252;ller, R. Keiser, E. Boxerman, and M. Carlson. Physically based deformable models in computer graphics. <i>Eurographics 2005 state of the art report</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791474</ref_obj_id>
				<ref_obj_pid>791214</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{OH95} J. F. O'Brien and J. K. Hodgins. Dynamic simulation of splashing fluids. In <i>CA '95: Proceedings of the Computer Animation</i>, page 198, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{PKW&#60;sup&#62;+&#60;/sup&#62;03} T. Pohl, M. Kowarschik, J. Wilke, K. Iglberger, and U. R&#252;de. Optimization and Profiling of the Cache Performance of Parallel Lattice Boltzmann Codes in 2D and 3D. Technical Report 03--8, Department for System-Simulation, Germany, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{QdL92} Y. H. Qian, D. d'Humi&#232;res, and P. Lallemand. Lattice BGK Models for Navier-Stokes Equation. <i>Europhys. Lett.</i>, 17(6):479--484, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{RB07} Matthias Mueller-Fischer Robert Bridson. Fluid Simulation, SIGGRAPH 2007 Course, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882335</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{RNGF03} Nick Rasmussen, Duc Quang Nguyen, Willi Geiger, and Ronald Fedkiw. Smoke simulation for large scale phenomena. <i>ACM Trans. Graph.</i>, 22(3):703--707, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{Sma63} J. Smagorinsky. General circulation experiments with the primitive equations. <i>Mon. Wea. Rev.</i>, 91:99--164, 1963.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{Sta99} Jos Stam. Stable Fluids. <i>Proc. of ACM SIGGRAPH</i>, pages 121--128, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{Suc01} S. Succi. <i>The Lattice Boltzmann Equation for Fluid Dynamics and Beyond</i>. Oxford University Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1037965</ref_obj_id>
				<ref_obj_pid>1037957</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{SY05a} Lin Shi and Yizhou Yu. Controllable smoke animation with guiding objects. <i>ACM Trans. Graph.</i>, 24(1), 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073401</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{SY05b} Lin Shi and Yizhou Yu. Taming liquids for rapidly changing targets. <i>Proc. of the 2005 ACM Siggraph/Eurographics Symposium on Computer Animation</i>, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{Tes04} Jerry Tessendorf. Simulating Ocean Surfaces. <i>SIGGRAPH 2004 Course Notes 31</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{THM&#60;sup&#62;+&#60;/sup&#62;03} M. Teschner, B. Heidelberger, M. Mueller, D. Pomeranets, and M. Gross. Optimized spatial hashing for collision detection of deformable objects. pages 47--54, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218066</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{TKPR06} Nils Th&#252;rey, Richard Keiser, Mark Pauly, and Ulrich R&#252;de. Detail-Preserving Fluid Control. <i>Proc. of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{TKZ&#60;sup&#62;+&#60;/sup&#62;04} M. Teschner, S. Kimmerle, G. Zachmann, B. Heidelberger, Laks Raghupathi, A. Fuhrmann, Marie-Paule Cani, Fran&#231;ois Faure, N. Magnetat-Thalmann, and W. Strasser. Collision detection for deformable objects. In <i>Eurographics State-of-the-Art Report (EG-STAR)</i>, pages 119--139. Eurographics Association, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{TPBF87} D. Terzopoulos, J. Platt, A. Barr, and K. Fleischer. Elastically deformable models. In <i>Computer Graphics Proceedings</i>, Annual Conference Series, pages 205--214. ACM SIGGRAPH 87, July 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{TPR&#60;sup&#62;+&#60;/sup&#62;06} Nils Th&#252;rey, Thomas Pohl, Ulrich R&#252;de, Markus Oechsner, and Carolin K&#246;rner. Optimization and Stabilization of LBM Free Surface Flow Simulations using Adaptive Parameterization. <i>Computers and Fluids</i>, 35 {8--9}:934--939, September-November 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{TR04} Nils Th&#252;rey and Ulrich R&#252;de. Free Surface Lattice-Boltzmann fluid simulations with and without level sets. <i>Proc. of Vision, Modelling, and Visualization VMV</i>, pages 199--208, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218086</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{TRS06} Nils Th&#252;rey, Ulrich R&#252;de, and Marc Stamminger. Animation of Open Water Phenomena with coupled Shallow Water and Free Surface Simulations. <i>Proc. of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272716</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{TSS&#60;sup&#62;+&#60;/sup&#62;07} Nils Th&#252;rey, Filip Sadlo, Simon Schirm, Matthias M&#252;ller-Fischer, and Markus Gross. Real-time simulations of bubbles and foam within a shallow water framework. In <i>SCA '07: Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, pages 191--198, Aire-la-Ville, Switzerland, Switzerland, 2007. Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{WG00} Dieter A. Wolf-Gladrow. <i>Lattice-Gas Cellular Automata and Lattice Boltzmann Models</i>. Springer, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>968837</ref_obj_id>
				<ref_obj_pid>968723</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{WLMK04} Xiaoming Wei, Wei Li, Klaus M"uller, and Arie E. Kaufman. The Lattice-Boltzmann Method for Simulating Gaseous Phenomena. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 10(2):164--176, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{Wol02} Stephen Wolfram. <i>A New Kind of Science</i>. Wolfram Media, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1132460</ref_obj_id>
				<ref_obj_pid>1132452</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{WWXP06} Changbo Wang, Zhangye Wang, Tian Xia, and Qunsheng Peng. Real-time snowing simulation. <i>The Visual Computer</i>, pages 315--323, May 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846287</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{WZF&#60;sup&#62;+&#60;/sup&#62;03} Xiaoming Wei, Ye Zhao, Zhe Fan, Wei Li, Suzanne Yoakum-Stover, and Arie Kaufman. Natural phenomena: Blowing in the wind. <i>Proc. of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer animation</i>, pages 75--85, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{YGL05} H. Yu, S. S. Girimaji, and Li-Shi Luo. Lattice Boltzmann simulations of decaying homogeneous isotropic turbulence. <i>Phys. Rev. E</i>, 71, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15895</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{YUM86} Larry Yaeger, Craig Upson, and Robert Myers. Combining physical and visual simulation and creation of the planet jupiter for the film 2010. In <i>SIGGRAPH '86: Proceedings of the 13th annual conference on Computer graphics and interactive techniques</i>, pages 85--93. ACM Press, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real Time Physics Class Notes Matthias M¨uller, nVidia Jos Stam, Autodesk Doug James, Cornell University 
Nils Th¨urey, ETH Zurich  Contents 1 Introduction 5 1.1 Real-timevs.Off-linePhysics ........................ 
5 1.2 Biographiesof Authorsin Alphabetical Order . . . . . . . . . . . . . . . . 6 1.3 StructureoftheClassNotes 
......................... 7 2 Introduction to Solids 8 3 Mass Spring Systems 10 3.1 PhysicalFormulation 
............................. 11 3.2 Simulation................................... 11 3.3 Runge-KuttaIntegration 
........................... 13 3.4VerletIntegration ............................... 14 3.5 ImplicitIntegration 
.............................. 15 3.5.1 Newton-RaphsonSolver ....................... 16 3.6 Mesh Creation 
................................. 17 3.7 Collision Detection .............................. 19 3.8 
Collision Response .............................. 19 4 The Finite Element Method 21 4.1 Continuum Mechanics 
............................ 22 4.1.1 Strain................................. 23 4.1.2 Strain Examples 
........................... 24 4.1.3 Stress................................. 25 4.1.4 ConstitutiveLaws 
.......................... 25 4.1.5 Equationof Motion .......................... 26 4.2 Finite Element 
Discretization ......................... 27 4.2.1 Constant StrainTetrahedral Meshes . . . . . . . . 
. . . . . . . . . 28 4.2.2 LinearFEM.............................. 30 4.3WarpedStiffness ............................... 
32 4.3.1 Extractionof Rotation ........................ 32 4.3.2 Determining Element Rotation .................... 
34 5 Position Based Dynamics 36 5.1 PositionBased Simulation .......................... 37 5.2 TheSystemtobesolved 
........................... 38 2 5.3 The Non-Linear Gauss-SeidelSolver..................... 39 5.4 
Constraint Examples ............................. 40 5.4.1 Stretching............................... 
40 5.4.2 Bending................................ 40 5.4.3Triangle Collisions .......................... 
41 5.4.4Volume Conservation ......................... 41 5.4.5 Shape Matching ........................... 
42 6 Rigid Body Simulation 45 6.1 Linear Motion ................................. 46 6.2 Angular Motion 
................................ 46 6.3 Collision handling............................... 49 6.4 Resting 
Contacts ............................... 50 6.5 Dynamicand Static Friction ......................... 
51 6.6 RealTime Simulation usinga Gauss-Seidel Solver . . . . . . . . . . . . . 52 7 Reduced-order deformable 
models 53 8 User Interaction and Control 54 9 Introduction to Fluids 55 10 Grid Based Fluid Simulation 
57 10.1Navier-Stokes Equations ........................... 57 10.2 Lattice Boltzmann Methods .......................... 
59 10.3 TheBasic Algorithm ............................. 59 10.4 Implementation................................ 
62 10.5 Stability.................................... 65 11 ShallowWater Equations 67 11.1 Introduction.................................. 
67 11.2AbasicSolver................................. 68 11.3 Boundary Conditions ............................. 
70 12 Particles 76 13 Uni.ed Solver 77 I Appendix 78 A Derivationof the ShallowWater Equations 79 A.1 
IntegralForm ................................. 79 A.2 DifferentialForm ............................... 
83 Chapter1   Introduction Physically based simulation is a signi.cant and active research .eld in 
computer graphics. Ithas emergedinthelate eightiesoutoftheneedtomake animations morephysicallyplau­sible 
and to free the animator from explicitly specifying the motion of complex passively moving objects. In 
the early days, quite simple approaches were used to model physical behavior such as mass-spring networks 
or particle systems. Later, more and more sophis­ticated models borrowed from computational sciences 
were adopted. The computational sciences appeared decades before computer graphics with the goal of replacing 
real world experiments with simulations on computers. In contrast, the aim of using physical simu­lations 
in graphics was, and still is, the reproduction of the visual properties of physical processes for special 
effects in commercials and movies. Computer generated special ef­fects havereplaced earlier methods such 
as stop-motion frame-by-frame animation and offer almost unlimited possibilities. Meanwhile, the rapid 
growth of the computational power of CPUs and GPUs in recent years has enabled real time simulation of 
physical effects. This possibility has opened the door to an entirely new world, a world in which the 
user can interact with the virtual physical environment. Real time physical simulations have become one 
of the main next­generation featuresof computergames. Allowing user interaction withphysical simulations 
also poses new challenging research problems. In this class we address such problems and presentbasicaswellas 
state-of-the-art methodsforphysicallybased simulationinrealtime. 1.1 Real-time vs. Off-line Physics Inoff-linephysical 
simulation,thetop concernis visual quality. Computationalef.ciencyis important because the simulations 
are typically done in high resolution using .ne grids and large numbers of objectsbut performance is 
clearly not as important as the quality of the output. Often,farmsof computersworkfor hoursto producea 
short sequenceofamovie. Because off-line simulations are predictable, it is possible to re-run the process, 
adapt the time step in case of numerical instabilities or change parameters if the outcome does not meet 
the speci.cations or expectations. In contrast, interactive systems run at a .xed frame rate, typically 
between 30 and 60 Hertz to avoid jerking and to guarantee a smooth and experience. This leaves between 
15 and30 millisecondsper timestepforphysical simulations.Ingames,the mainpartof this 5 timebudget is 
used for the core features, e.g. the graphics or arti.cial intelligence com­putations. Thus, only a few 
milliseconds remain for physics. Staying within this limit is a must. The resolution and visual quality 
have to be adjusted to meet that constraint. Because of these speci.c requirements, methods used in off-line 
simulations cannot be used one to one in real time applications. Simply reducing the resolution often 
yields blobby results and removesallthe interesting detail.In computergamesandin interactiveenvironmentsit 
is also absolutely essential that simulations are unconditionally stable, i.e. stable under all circumstances. 
In contrast to off-line simulations, the outcome of interactive scenarios is non-predictable. In addition 
non-physical scenarios are common such as kinematic actors which move over long distances in a single 
time step, interpenetrating shapes or huge force .elds acting on the objects in the scene. Out of the 
need for specialized solutions for interactive environments emerged the new research .eldof real-timephysically-based 
simulationsin computer graphics. The lecturers of this class havemade central contributions in this .eld. 
In this class, each lecturer presents core ideas and state-of-the-art methods in his own .eld. 1.2 Biographies 
ofAuthors in Alphabetical Order DougJames holds three degrees in applied mathematics, including a Ph.D. 
from the University of British Columbia in 2001. In 2002 he joined the School of Computer Science at 
Carnegie Mellon University as an Assistant Professor, then in 2006 he be­came an Associate Professor 
of Computer Science at Cornell University. His research interests are physically based animation, computational 
geometry, scienti.c comput­ing, reduced-order modeling, and multi-sensory digital physics (including 
physics­basedsoundandhaptic force-feedback rendering).Heisa National ScienceFounda­tion CAREERawardee, 
anda fellowof the AlfredP. SloanFoundation.  MatthiasM ¨  uller received his Ph.D. on atomistic simulation 
of dense polymer sys­tems in 1999 from ETH Zurich. During his post-doc with the MIT Computer Graph­icsGroup 
1999-2001he changed .eldsto macroscopicphysically-based simulations. He has published papers on particle-based 
water simulation and visualization, Finite Element-based soft bodies, cloth and fracture simulation. 
In 2002 he co-founded thegamephysics middleware companyNovodeX.NovodeXwas acquiredin 2004 byAGEIA wherehewas 
headof research and responsible for theextensionof the physics simulation library PhysX by innovative 
new features. He has been head of the PhysX research team of nVidia since the acquisition in 2008. Jos 
Stamreceived dual BSc degrees in Computer Science and Mathematics from the University of Geneva. He got 
a MSc and Phd in Computer Science from the Univer­sityofToronto. Aftera postdocin France and Finlandhe 
joined Alias wavefront (now Autodesk). Stam s research spans several areas of computer graphics: natural 
phenomena, physics-based simulation, rendering and surface modeling. He has pub­lished papers in all 
of these. He has also participated in seven SIGGRAPH courses in these areas. He received the SIGGRAPHTechnical 
AchievementAward and has two AcademyAwards for his contributions to the .lm industry.  Nils Thuerey 
is a post-doctoral researcher at the Computer Graphics Laboratory at ETH Zurich, working together with 
Prof. Markus Gross. In March2007 he received his Phd (summa cum laude) in computer science from the University 
of Erlangen-Nuremberg. He has worked in the .eld of high-performance .uid simulations with free surfaces 
using, among others, lattice Boltzmannn methods. In addition, he has worked on the topic of real-time 
.uid simulations using height-.eld approaches, such as the shallowwater equations, together with the 
research groupofAGEIA.  CHAPTER 1. INTRODUCTION 1.3 Structure of the Class Notes The notes are divided 
into three parts. In the .rst part various real time methods for the simulation of solid rigid and deformable 
objects are presented. In addition, multiple ways to represent solids in simulations are discussed, such 
as particle systems, .nite element meshes or rigid bodies. The subject of the second part are .uids, 
i.e. liquids andgases. Simulation methods can be split into three main groups, 3-dimensional grid-based 
methods, 2.5-dimensional height .eld representations and particle based techniques. The last part discusses 
methods for handling interactions between objects of different types.  Chapter2 Introduction to Solids 
 For physically-based simulation, solid objects are typically divided into three main groups: Rigid bodies, 
soft bodies and cloth. From the point of view of the underlying physics, there is no such distinction. 
Completely rigid bodies do not exist in nature, every object is deformable to some extent. Cloth is a 
general three-dimensional soft body as well sinceithasa certain thickness.However,froman algorithmicand 
simulationpointofview, it makes sense to handle those three types separately. The assumption that objects 
made of stone are in.nitely rigid produces no visual artifactsbut simpli.es the handling and simu­lation 
of such objects signi.cantly. Also, simulating cloth as a 2d rather than a 3d object reduces simulation 
time and memory consumption. There is a large body of work concerning the simulation of solid objects 
in computer graphics. We refer the interested reader to the two surveys [GM97] and [NMK+05]. Since theearlyworkofTerzopoulosand 
others [TPBF87]inthe80 s,manytechniqueshavebeen proposed to simulate solid objects. In the early days 
physical simulations were typically done off-line. Because a single rigid body can be represented by 
just a few quantities, simulation of smallsetsof interactingrigid bodieswas possiblein real-timeinthe80 
salready ([Hah88]). One of the .rst systems that made real-time interaction with deformable objects possible 
was ArtDefo described in [JP99]. The system made use of the effective method of model reduction in connection 
withthe boundary element method (BEM) discussed in Chapter7 to speed up the simulation and achieve interactive 
frame rates. The Finite Element Methods (FEM) is one of the most widely used techniques in com­putations 
sciences for the simulation of solid objects. The method reduces general partial differential equations 
to systems of algebraic equations. In general, these equations are 8 CHAPTER 2. INTRODUCTION TO SOLIDS 
non-linear. Solvers for systems of non-linear equations are typically too slow for real-time performance. 
In certain scenarios, it is feasible to work with linear approximations, mainly whenthe deformations 
are small suchasinthe analysisofbuildings.However,inthe case of freely moving highly deformable objects, 
the artifacts are signi.cant. One way to make FEM-based simulationsfast enough for real-time applications 
is to decompose deforma­tions into linear and rotational parts [MG04] as detailed in Chapter 4. In real-time 
applications, solid objects are often represented by mass spring networks rather then FEM meshes. Mass 
spring systems will be discussed in Chapter 3. They are easier to program than FEM andfaster in general 
with disadvantage that they are harder to tune and do not converge to the true solution as the meshspacing 
goes to zero. Most of the time, this is not a signi.cant problem in real-time scenarios. Cloth is almost 
always repre­sented by mass-spring networks because these meshes can easily handle the 2-dimensional 
structure of cloth. In the FEM framework, special complex elements which can represent bending would 
have to be used for cloth, a solution that is unnecessarily complex and too slow in general. As mentioned 
in the introduction, it is essential that simulations are un­conditionally stable. Simple explicit integration 
schemes have no such guarantee. Implicit integration, on the other hand, are more complex to code, slower 
and introduce damping. Chapter5introducesthe conceptof position based integration which allowsto directly 
con­trol the behavior of the simulation [Jak01, MHR06]. Withthe assumptionofabodytobe rigid,its state 
canbe describedbya single position, orientation and a linear and angular velocity. This observation allows 
the simulation of a large number of rigid bodies in real time as described in Chapter 6. Rigid body simulators 
are the essential partofeveryphysics engineingames because mostof the objects present in a level can 
be considered to be rigid. Chapter3 Mass Spring Systems MatthiasM¨uller One of the simplest approaches 
to simulate solid deformable objects is to represent and simulate them as a mass spring system. Since 
this approach is so simple, it is an ideal framework to study various techniques for simulation and time 
integration. A mass spring system consists of a set of point masses that are connected by springs. The 
physics of such a system is straight forward and a simulator can be programmed on just a few pages. This 
simplicity comes at the price of a few drawbacks you have to be aware of: The behavior of the object 
depends on the way the spring network is set up  It can be dif.cult to tune the spring constants to 
get the desired behavior.  Mass spring networks cannot capturevolumetriceffects directly suchasvolume 
con­servation or prevention of volume inversions.  For various applications these drawbacks are not 
essential. In those cases, mass spring systems are the best choice because they are easy to implement 
andfast. If more accurate physics is needed, other approaches like the Finite Element Methods detailed 
in Chapter4 should be considered. 10 CHAPTER 3. MASS SPRING SYSTEMS 3.1 Physical Formulation A mass spring 
system is composed of a set of N particles with masses mi, positions xi and velocities vi, with i. 1...N. 
These particles are connected by a set S of springs (i, j,l0,ks,kd), where iand jare the indices of the 
adjacent particles, l0 the rest length, ks the spring stiffness and kd the damping coef.cient. The spring 
forces acting on the adjacent particles of a spring are xj- xi fi = fs(xi,xj)= ks (|xj- xi|- l0) (3.1) 
|xj- xi|fj= fs(xj,xi)= -f(xi,xj)= -fi (3.2) These forces conserve momentum(fi+ fj = 0)and are proportional 
to the elongation of the spring (|xj- xi|- l0). Alternatively, one can make them proportional to the 
relative elongation by replacing the constant ks with the constant ks/l0. The damping forces xj- xi fi 
= fd(xi,vi, xj,vj)= kd(vj- vi) · (3.3) |xj- xi|fj= fd(xj,vj, xi,vi)= -fi (3.4) are proportional to the 
velocity difference projected onto the spring and are momentum conserving as well. Let us combine the 
two forces into one uni.ed spring force as f(xi,vi, xj,vj)= fs(xi,xj)+ fd(xi,vi, xj,vj) (3.5) 3.2 Simulation 
Newton s second law of motion f= mx¨isthekeytoget fromthe de.nitionof forcestoa simulation algorithm. 
Solving for the acceleration yields x¨= f/m, where x¨is the second derivative of the position with respect 
to time. This formula can be used to compute the accelerations of the particles based on the forces acting 
on them. As a .rst step towards simulation, we separate this second order ordinary differential equation 
into two coupled .rst order equations as v.= f(x,v)/m (3.6) x.= v (3.7) The analytical solutions of these 
equations are t v(t)= v0+f(t)/mdt and (3.8) t0 t x(t)= x0+v(t)dt. (3.9) t0 Starting from the initial 
conditions v(t0)= v0 and x(t0)= x0, the integrals sum the in.nites­imal changes up to time t. Simulation 
is the same as computing x(t) and v(t) from time t0 on. Thus, the words simulation and time integration 
are often used interchangeably. The simplestwayto solvethe equations numericallyisto approximatethe derivatives 
with .nite differences vt+1- vt v.=+ O(.t2) and (3.10) .t xt+1- xt x.=+ O(.t2), (3.11) .t where .t is 
a discrete time step and t the frame number. Substituting these approximations into Eq. (3.6) and Eq. 
(3.7) yields two simple update rules t+1 t v= vt + .t f(x,vt)/m (3.12) t+1 t x= xt + .t v. (3.13) This 
is the explicit Euler integration scheme. It is an explicit scheme because the quan­tities of the next 
time step can directly be computed from the quantities at the current time step using explicit formulas. 
Asmall trick which makes the scheme more stable is to use vt+1 in stead of vt on the right hand side 
of Eq. (3.13) because the new velocity is already available at that point in time. We are now ready to 
write down an algorithm for the simu­lation of a mass spring system: // initialization (1) forallparticles 
i (2) initialize xi,vi and mi (3) endfor // simulation loop  (4) loop (5) forallparticles i fi . fg+ 
fcoll (6) i + . f(xi,vi, xj,vj) j,(i, j).S (7) endfor (8) forallparticles i (9) vi . vi+ .t fi/mi (10) 
xi . xi+ .t vi (11) endfor (12) display the system every nth time (13) endloop Here, fg is the gravity 
force and fcoll forces due to collisions. Explicit Euler integration is oneofthe simplest integration 
methodsbutithasan essentialdrawback.Itisonly stablefor relatively small steps. We will not go into the 
details of stability analysis. Just to give you an idea, the time step in a typical real-time scenario 
has to be of the order of 10-4 ...10-3 seconds meaning that several simulation steps are necessary between 
twovisualized frames. CHAPTER 3. MASS SPRING SYSTEMS This is why in the above algorithm step (12) is 
executed only every nth time. The main reason for the instability is that the Euler scheme steps blindly 
into the future. It assumes that the force is constant throughout an entire step. Consider the situation 
when a spring is slightly stretched and the adjacent particles move towards each other. With a large 
time step, the particles pass the equilibrium con.guration so the spring force changes sign during the 
time step. This is not accounted for if the force at the beginning is used throughout the entire time 
step. In this particular situation, the particles overshoot andgain energy which can result in an explosion 
eventually. One way to improve the situation is to use more accurate integration schemes. Popular choices 
are second and fourth order Runge-Kutta integrators. These schemes sample the forces multiple times within 
the time step to reduce the problem mentioned. 3.3 Runge-Kutta Integration The second order Runge-Kutta 
integrator replaces Eq. (3.12) and Eq. (3.13) with tt a1 = va2 = f(x,vt)/m .t .t .t b1 = vt+ a2 b2 = 
f(xt + a1,vt + a2)/m 2 22 t+1 t+1 x= xt + .tb1 v= vt + .tb2 to get from the position xt andvelocity vt 
at the current time steptothe position andvelocity xt+1 and vt+1atthenexttimestep.Hereisthe modi.edalgorithmwhichusesasecondorder 
RungeKutta integrator: // initialization (1) forallparticles i (2) initialize xi,vi and mi (3) endfor 
// simulation loop  (4) loop (5) forallparticles i (6) a1,i . vi  V fg+ fcoll (7) a2,i . + . f(xi,vi,xj,vj)/mi 
i j,(i, j).S (8) endfor (9) forallparticles i (10) b1,i . vi+ .ta2,i  V 2 .t .t .t .t fg+ fcoll (11) 
b2,i . i + . f(xi+ a1,i,vi+ a2,i,xj+ a1, j,vj+ a2, j)/mi 2222 j,(i, j).S (12) xi . xi+ .tb1,i (13) vi 
. vi+ .tb2,i  (14) endfor (15) display the system every nth time (16) endloop Asyou can see,the forceshavetobeevaluated 
twiceat each time step.So one second order Runge-Kutta step takes as much time as two Euler steps. However, 
Runge-Kutta is second order accurate in contrast to the .rst order accurate Euler scheme. This means 
that when you half the time step using Euler, you end up with half the error in contrast to one fourth 
of the errorintheRunge-Kutta case. Thereisa fourthorderRungeKuttaintegration scheme which looks like 
this: tt a1 = va2 = f(x,vt)/m .t .t .t b1 = vt + a2 b2 = f(xt + a1,vt + a2)/m 2 22 .t .t .t c1 = vt + 
b2 c2 = f(xt + b1,vt + b2)/m 2 22 d1 = vt + .tc2 d2 = f(xt + .tc1,vt + .tc2)/m .t .t t+1 t+1 x= xt +(a1+ 
2b1+ 2c1+ d1) v= vt +(a2+ 2b2+ 2c2+ d2) 66 This scheme is among the most popular integration techniques 
in computational sciences. The forces have to be evaluation four times which is rewarded by fourth order 
accuracy. Halving the time step results in one sixteenth of the error. 3.4 Verlet Integration The way 
used in the Runge-Kutta scheme to improve stability and accuracy was to sample the forces multiple times 
during one time step. Another idea is to use quantities evalu­ated in the past to increase the degree 
of the approximation of the derivatives in Eq. (3.10) Eq. (3.11).A varietyof schemes are basedon this 
idea.In this class,Verlet integrationis amongthe simplestand most popularin real-time applications. The 
basic ideaistokeepthe positions of time t- .t in an additional state variable and use this information 
for a more accurate prediction.Taylorexpansionofthe positionsinthetwotime directionsyields ... x(t+ .t)= 
x(t)+ x.(t).t+ 1 x¨(t).t2+ 1 x(t).t3+ O(.t4) (3.14) 26 1 1... x(t- .t)= x(t) - x.(t).t+ x¨(t).t2- x(t).t3+ 
O(.t4) (3.15) 26 Adding these two equations and rearranging terms gives x(t+ .t)= 2x(t) - x(t- .t)+ x¨(t).t2+ 
O(.t4) (3.16) = x(t)+[x(t) - x(t- .t)] + f(t).t2/m+ O(.t4). (3.17) (3.18) As you can see, the linear 
and cubic terms cancel out which makes this a fourth order integration scheme. The velocity does not 
show up explicitly. Instead, the position at the CHAPTER 3. MASS SPRING SYSTEMS previous time step has 
to be stored. We can bring the velocity back in by de.ning v(t)= [x(t) - x(t- .t)]/.t. Incorporating 
this idea and going back to the frame time notation yields t+1 t + v x= xt .t+ f(xt).t2/m (3.19) vt+1 
=(xt+1- xt)/.t. (3.20) Unlike in the related VelocityVerlet scheme, the velocity here is only .rst order 
accurate and not really used . It is just a different way of representing the position at the last time 
step. The interestingfact that this scheme operates on positions only is leveraged in the Position Based 
Dynamicsapproach introduced in Chapter 5. 3.5 Implicit Integration The integration schemes discussed 
sofar are only conditionally stable meaning that there is a certain range for the time step .t size for 
which the simulationis stable. This range depends mainly on the stiffness of the springs. The stiffer 
the springs, the smaller the time step requiredtokeepthe simulation stable.In real-time situation,e.g.inacomputergame,it 
is essential that an integration is unconditionally stable meaning stable in all circumstances andforthetimestepsizegivenbythe 
requiredframe rate.Onewaytoachievethisistouse an implicit integration scheme. Another possibility will 
be discussed in Chapter 5. The most popular implicit integration scheme used in computer graphics is 
implicit Euler. The explicit Euler scheme described in Eq. (3.12) and Eq. (3.13) has to be modi.ed only 
slightly to make it implicit: t+1 v= vt + .t f(xt+1)/m (3.21) t+1 t + .tt+1 x= xv. (3.22) Weremoved the 
friction force so the sum of all forces only depends on the positions, not the velocities. Friction forces 
stabilize the system so they can be added in an explicit step after the implicit solve. Second, implicit 
integration introduces quite a lot of numerical damping so in typical situations there is no need to 
add physical damping as well. The core change, however, is to use the positions and velocities of the 
new time step on the right hand side as well. It is not possible any more to directly and explicitly 
evaluate these two equations. Instead, we have two implicit equations that form a non-linear alge­braic 
system with the positions and velocities of the next time step as the unknowns. Non formally one could 
say that in contrast to explicit schemes, we don t move blindly into the futurebutmake surethatthe quantitieswearriveatarein 
accordancewithphysicallaws. As a .rst step towards solving these equations for the entire mass spring 
network we combine the positions, velocities and forces of all particles into two single vectors and 
a multidimensional force x =[x1T ,...,xT]T n TT v =[v1,...,v]T n f(x)=[f1(x1,...,xn)T ,...fn(x1,...xn)T]T 
We also construct a mass matrixM. R3n×3n which is diagonal with the values m1,m1,m1, m2,m2,m2, ...,mn,mn,mn 
along the diagonal. These de.nitions lead to the following sys­tem of equations t+1) Mvt+1 = Mvt + .tf(x(3.23) 
t+1 t+1 x= x+ .v(3.24) Substituting Eq. (3.24) into Eq. (3.23) results in a single system for the unknown 
velocities vt+1 at the next time step Mvt+1 = Mvt + .tf(xt + .tvt+1). (3.25) This system is non-linear 
because the forces are non-linear in the positions. 3.5.1 Newton-Raphson Solver The generalwayto solve 
sucha systemisto usetheNewton-Raphson method. This method starts at a guess for the unknown vt+1 and 
iteratively improves this guess. To this end, the equations are linearized atthe current stateandthe 
resulting linear systemis solvedto .nda better approximation. This processis repeated untilthe errorfalls 
belowa certain threshold. For real-time applications, linearizing multiple times per time step is too 
expensive. It is custom, therefore, to linearize once and use the current velocities vt as a guess for 
vt+1. Newton s.rstlawsaysthat withoutforces,velocitiesdonotchangewhichmakesthisguess a good one. Linearizing 
the forces at xt yields . t) t+1) Mvt+1 = Mvt + .t f(xt)+ f(x· (.tv(3.26) .x = Mvt + .tf(xt)+ .t2Kvt+1 
, (3.27) where K. R3n×3n is the Jacobian of f.It containsthederivativesofall3n force components w.r.t. 
all3n position components of the particles. This matrix is also called the tangent stiffness matrix in 
this context. While a standard stiffness matrix of an elastic element is evaluated at the equilibrium, 
K is evaluated at the current positions of the particles. We rearrange the terms to get a standard linear 
system for the unknown velocities t+1 M- .t2Kv= Mvt + .tf(xt) (3.28) Avt+1 = b, (3.29) where Ais a3n× 
3n dimensional matrix and ba3n dimensional vector. In real-time ap­plications, this system is typically 
solved using iterative methods like Conjugate Gradients (CG). Direct methods are not practical because 
Kchanges at each time step. The right hand side vector bcan directly be evaluated using known quantities 
only. Nowletushavea lookat K.Aspring force between particles iand jadds the four3 ×3 sub-matrices Ki,i, 
Ki, j, Kj,i and Kj, j to the global matrix K at positions (3i,3i), (3i,3j), (3j,3i) and (3j,3j) respectively.In 
ordertoevaluate these sub-matrices,we needto deduce CHAPTER 3. MASS SPRING SYSTEMS the derivatives of 
the spring force de.ned in Eq. (3.1) w.r.t. the positions xi and xj: Ki,i = . .xi fs(xi,xj) = ks . . 
xi (xj- xi) - l0 xj- xi |xj- xi| = ks -I+ l0 l I- (xj- xi)(xj- xi)T l2 (3.30) (3.31) (3.32) = -Ki, j 
= Kj, j = -Kj,i (3.33) where Iis the3 × 3 identity matrix and l = |xj- xi| the actual length of the 
spring. The tangent stiffness matrixis not constant throughout the simulation becauseit depends on the 
current positions x1...xn of the particles. So at each time step Kis set to zero. Then, for each spring 
the four sub-matrices Ki,i, Ki, j, Kj,i and Kj, j are added at the right positions in K. Next, the right 
hand side bof Eq. (3.29) is evaluated and the resulting linear system solved for vt+1. Finally the velocities 
can be updated using Eq. (3.24). Thetangentstiffnessmatrixissparseinthegeneral case.Itwouldonlybefullyoccupied 
ifeach particleswas connectedtoallother particleswhichis typicallynotthe case. Because all sub-matrices 
associatedwithaspringareequaluptotheirsign,onlyone3 × 3matrix has to be stored per string. Aswesawin 
this chapter, thereisa wide spectrumof integration methods ranging from very simplebut unstable methods 
to quite complex unconditionally stable schemes. Which choiceisoptimaldependsonthe application. Implicitintegrationhastwomainadvantages, 
its stability and the possibility of using a global solver such as Conjugate Gradients. Think of a long 
thin triangle mesh. If you pull on one side, the explicit Euler scheme can only propagate forces one 
triangle per integration step. Aglobal solver propagates errors much faster across the unknowns in the 
system. This way, deformable objects can be made stiff. This is important in connection with cloth simulation 
for instance. Cloth has low bending resistancebutisquitestiffw.r.t. stretching.Ontheotherhand,inordertoachieve 
real-time performance, the time step for an implicit integrator has to be chosen quite large because 
one integration step is so expensive. Also, implicit integration introduces substantial numerical damping. 
Both effects let interesting spatial and temporal detail disappear.  3.6 Mesh Creation Incomputergames, 
artistsmodel3dobjectsasgraphicalmeshesthat representtheirsurface. These meshes are often not well suited 
to be used for simulation directly for several reasons. First, theydo not represent the interior of the 
object and second, theycontain badly shaped triangles, duplicated vertices, intersecting triangles or 
are just triangle soups in the extreme case. In general, it is better to use two representations of the 
object, one for visualization and one for simulation. Sometimes a third representation for collision 
detection is needed as well. If the simulation and visual representations are not the same, a mechanism 
to move the visual mesh along with the physical mesh is needed. If the connectivity is based on a tetrahedral 
mesh, one can simply usethe barycentric coordinatesof each visualvertexw.r.t. its surrounding tetrahedron 
to move the vertex via skinning. There are many ways to create a simulation mesh from a graphical mesh. 
Let us have a look at a procedure that is quite simple. We create the mass spring system from a tetra­hedral 
mesh by turning eachvertexintoa particle and each tetrahedral edge intoa spring. What is missing are 
the masses of the particles and the stiffness and damping coef.cients of the springs. Givena user speci.ed 
density ., the mass of each tetrahedron can be computed as its volume multiplied by the density. Then, 
each tetrahedron distributes its mass evenly amongits fouradjacentvertices. Thisisa straight forwardprocedure. 
Finding reasonable spring coef.cients is not. No matter how you do it, the behavior of the mass spring 
sys­tem will always depend on the structure of the mesh. This problem can only be solved by considering 
volumetric tetrahedra for simulation (see Chapter 4), not just their one dimen­sional edges. Since there 
is no correct way, we might as well assign a common stiffness and damping coef.cient to all springs. 
If the lengths of the springs do not vary too much, this approach works quite well. Otherwise you might 
want to come up with some magic to compute the coef.cients from the edge lengths. The remaining question 
is how to go from a triangular surface mesh to a volumetric tetrahedral mesh. To this end, we distribute 
a certain number of particles pi evenly inside thevolume de.nedby the surface mesh and on the surface 
mesh itself.Todo this, we need an inside-outside test.The traditionalwayisto createaray originatingfromthe 
particleand count the number of surface triangle intersections. If the number is odd, the particle is 
inside the surface. This procedure requires the surface to be watertight. Then we run Delaunay tetrahedralization 
on these points and onlykeep tetrahedra with centers inside the surface. Anice feature of this approach 
is that the resulting tetrahedralization is independent of the triangulationofthesurface. Thereisaprettysimple 
formulationoftheDelaunay algorithm which constructs a tetrahedral mesh from a set of points: (1) create 
large tetrahedron that contains all points p1,..., pn (2) forallpoints pi (3) clearface list l (4) 
foralltetrahedra tjwhose circumsphere contains pi (5) forallfaces fk of tj (6) if lcontains fk remove 
fk from l (7) else add fk to l  (8) endfor (9) delete tj (10) endfor (11) forallfaces fk in l (12) 
create a tetrahedron from fk to pi  (13) endfor (14) endfor  (15) remove all tetrahedra that contain 
vertices of the large tetrahedron created in step (1) CHAPTER 3. MASS SPRING SYSTEMS 3.7 Collision Detection 
There is a large body of work on collision detection for deformable objects [TKZ+04]. In this section 
we will look at spatial hashing only [THM+03]. This method works well when all primitives in a scene 
have about the same size. An advantage of using a separate representation of the object for physics is, 
that this representation can be optimized for a speci.c simulation algorithm. In this case, we make sure 
that there is not much variation in the sizes of the tetrahedra. Spatial hashing works as follows: First, 
a grid spacing his chosen. The spacing should match the average size of the primitives. It de.nes a regular 
grid in space. Then, each tetrahedron is inserted into all grid cells it intersects. With this data structure 
in place, the collision detection algorithm can retrieve all objects in the neighborhood of a point in 
constant time. The integer coordinates of the grid cell a point plies in are px py pz (ix,iy,iz)=( , 
, ). (3.34) hhh Obviously the number of grid cells de.ned this way is not bounded. In order to store 
the grid, one could restrict the objects to remain in a .nite rectangular volume and then only work with 
the cells in this volume. A more .exible variant is to allocate a .xed number N ofbucketsandthenmapthecell 
coordinatestoabucket numberusingahash function. Sincethereareway morerealcellsthenbuckets, multiplecellswillbemappedtothe 
same bucket. This is not a problem though because even though a spatial query might return tetrahedra 
from other parts of the world, it will always return all true candidates as well. Thus, hash collisions 
slow down collision detectionbut do not make it less accurate. Here is an example of a hash function 
i= [(ix · 92837111) xor (ix · 689287499) xor (ix · 283923481)] mod N, (3.35) where i is thebucket number 
cell (ix,iy,iz) is mapped to. At each time step, the grid is recreated from scratch. If Nis substantially 
larger than the number of tetrahedra, clearing all thebucketsateachtimestepistooexpensive. Instead,eachbucket 
containsaframe counter. If this counter is smaller than the global frame counter, thebucket is de.ned 
to be empty andwheneveranobjectis inserted,the counterofthebucketissettotheglobal counter.At the beginning 
of collision detection, the global frame counter is increased which makes all bucketsemptyat once.Then,each 
tetrahedronisaddedtothebuckets correspondingtothe cells it overlaps. Finally, the algorithm iterates 
through all the particles on the surface of the meshand retrievesallthe tetrahedrainthe neighborhoodof 
that particle.Ifthe surface particle is inside a tetrahedron, a collision is reported. This method works 
for both, inter and intra object collision. It does not handle edge-edge collisions though. 3.8 Collision 
Response The basic event reported from collision detection is that a surface particle qof an object A 
is inside a tetrahedron t of an object B. In the case of a self collision, the two objects are identical. 
Itisfar fromtrivial to .nda stablewayof resolving sucha collision. The .rst problem is to decide where 
the penetrated vertex should go. One possibility is to move it to the closest surface point of object 
B. In general, this is not a very good choice. It is more natural and more stable to move qback to where 
it penetrated object B. A way to achieve this is to construct a ray from qin the opposite direction of 
the surface normal at q. Let us call the intersection of the ray with B s surface q/. This point lies 
insideafaceofa tetrahedron of Bwith adjacent vertices p1, p2 and p3. Let ß1, ß2 and ß3 be the barycentric 
coordinates of q/ such that / q= ß1p1+ ß2p2+ ß3p3. (3.36) We can now de.ne the collision response force 
fcoll = k(q/- q) (3.37) whichis proportionaltothe penetrationdepthandauserspeci.edstiffnesscoef.cient.This 
force is applied to vertex q. To make sure the momenta are conserved forces -ß1fcoll, -ß2fcoll and -ß2fcoll 
are applied to vertices p1, p1 and p1 respectively. The method de­scribedis quite simplistic.A more sophisticated 
and more robust techniqueis describedin [HTK+04].  Chapter4 The Finite Element Method MatthiasM ¨uller 
 Mass spring system cannot capture volumetric effects, as mentioned in the previous chapter.Also,theirbehaviordependsonthe 
structureofthemesh.To.xtheseproblems,we will now treat the deformable body as a continuous volume. To 
this end, we will look into the world of continuum mechanics. Of course, summarizing such a complex mathematical 
framework in its generality on just a few pages is not possible. Therefore, we will focus on the most 
important concepts needed for the simulation of deformable objects in computer graphics. Do not worry 
if you don tgrasp all the details of this section the .rst time. In Section 4.2 we will turn the resulting 
partial differential equations into a relatively simple formula for computing the forces that act on 
a tetrahedron based on the positions of its vertices. Given this formula, one can think of the tetrahedron 
as a generalized four-way spring andbuild larger structures fromit likein mass-spring systems. In the 
last section we will look at ways to evaluate the forces ef.ciently in order to enable simulation of 
large tetrahedral meshes in real time. 21 4.1 Continuum Mechanics In computational sciences, deformable 
objects are often modeled as continuous (three di­mensional) objects.For the descriptionof their behavior, 
the three quantities displacement, strain and stress playamajor role. In one dimensional problems, these 
quantities are all one dimensional and have intuitive interpretations. Let us consider a beam with cross 
sectional area AasshowninFig.4.1. Whena force fn is applied in the direction of the beam perpen­dicular 
to the cross section, the beam with original length lexpandsby .l. These quantities are related via Hooke 
s law as fn .l = E. (4.1) Al The constant of proportionality E isYoung s modulus. For steelE is in the 
order of 1011N/m2 while for rubber it lies between 107 and 108N/m2. The equation states that the stronger 
the force per area, the larger the relative elongation .l/l as expected. Also, the magnitude of the force 
per area needed to get a certain relative elongation increases with increasing EsoYoung smodulus describes 
the beam s stiffness. Hooke slawcan be written in a more compact form as s = Ee, (4.2) where s = fn/Ais 
the applied stress and e = .l/l the resulting strain or the other way around, s the resulting internal 
stress due to applied strain e. It follows that the unit of stress is force per area and that strain 
has no unit. This is true in three dimensions as well. A three dimensional deformable object is typically 
de.ned by its undeformed shape (also called equilibrium con.guration, original,restor initialshape)andbyasetof 
material parameters that de.ne how it deforms under applied forces. If we think of the rest shape as 
a continuous connected subset O of R3, then the coordinates x . O of a point in the object are called 
material coordinates of that point. When forces are applied, the object deforms and a point originally 
at location x (i.e. with material coordinates x)moves toa new locationp(x), the spatial or world coordinates 
of that point. Since new locations are de.ned for all material coordinates x, p(x) isa vector CHAPTER 
4. THE FINITE ELEMENT METHOD .eld de.ned on O. Alternatively,the deformation canalsobe speci.edbythe 
displacement .eld, which, in three dimensions, is a vector .eld u(x)= p(x) - x de.ned on O. 4.1.1 Strain 
In order to formulate Hooke s law in three dimensions, we have to .nd a way to measure strain, i.e. the 
relative elongation (or compression) of the material. In the general case, the strain is not constant 
inside a deformable body. In a bent beam, the material on the convex side is stretched while the one 
on the concave side is compressed. Therefore, strain is a function of the material coordinate e = e(x). 
If the body is not deformed, the strain is zero so strain must depend on the displacement .eld u(x). 
A spatially constant displacement .eld describes a pure translation of the object. In this situation, 
the strain should be zero as well. Thus, strain is derived from the spatial variation or spatial derivatives 
of the displacement .eld. In three dimensions the displacement .eld has three components u = u(x)=[u(x,y,z),v(x,y,z),w(x,y,z)]T 
and each component can be derived with respect to one of the three spatial variables x,y and z. Therefore, 
strain cannot be expressed by a singlescalaranymore.Thismakessensebecauseatasinglepointinsideathree dimensional 
object, the material can be stretched in one direction and compressed in another at the same time. Thus, 
strainis representedin three dimensionsbya symmetric3by3matrixor tensor . exx exy exz . e = . exy eyy 
eyz . (4.3) exz eyz ezz We consider two ways to compute the components of the strain tensor from the 
spatial derivatives of the displacement .eld eG = 1(.u+[.u]T +[.u]T.u) and (4.4) 2eC = 1(.u+[.u]T), (4.5) 
2 where the symmetric tensor eG . R3x3 is Green s nonlinear strain tensor (non-linear in the displacements) 
and eC . R3x3 its linearization, Cauchy s linear strain tensor. The gradient of the displacement .eldisa3by3matrix 
.. uuu ,x ,y ,z .u = . v,xv,yv,z ., (4.6) www ,x ,y ,z where the index after the comma represents a spatial 
derivative. Itis natural to derive strain from the spatial derivatives of the displacement .eld,but the 
speci.c de.nitions of eG and eC look quite arbitrary. There is a relatively easy way to motivate their 
de.nition though. Let us have a look at the the transformation p(x) in the neighborhood of a material 
point. Without loss of generality we can choose the material point 0. Close to that point, p(x) can be 
approximated with a linear mapping as p(x)= p(0)+ .p· x+ O(|x|2) (4.7) .. x .p .p .p = p(0)+ ,, · . y 
.+ O(|x|2) (4.8) .x .y .z z or .p . p .p p(x,y,z) p(0)+ · x+ · y+ · z (4.9) .x . y . z = p(0)+ p,x · 
x+ p,y · y+ p,z · z. (4.10) This shows that the local neighborhood of material point 0is moved into a 
new frame with origin p(0) and axes p,x,p,y and p,z. The neighborhood does not get distorted by the trans­lation 
along p(0). For the neighborhood not to get distorted by the entire transformation, the new axes must 
satisfy two conditions. All of them must have unit length, otherwise the neighborhood gets stretched 
or compressed along the axes. In addition, the axes must be perpendicular to each other, otherwise the 
neighborhood is sheared. These constraints can elegantlybeexpressedby requiring that [.p]T.p= I. Expanding 
this expression yields ... . pT ,x |p,x|2 p,x · p,y p,x · p,z [.p]T.p= . pT .[p,xp,yp,z]= . p,y · p,x 
|p,y|2 p,y · p,z .. (4.11) ,y pT ,z p,z · p,x p,z · p,y |p,z|2 Forthistobeequaltotheidentitymatrix,alldiagonal 
entriesmustbe1sayingthatallaxes must have unit length. The off-diagonal entries are0if all the dot products 
between pairs of axes are zero. This is the case when theyare perpendicular to each other. In other words, 
[.p]T.p= Imeans no strain. From this observation it makes sense to de.ne strain as the deviation of [.p]T.pfrom 
I, so e =[.p]T.p- I. (4.12) With this de.nition, the diagonal entries ofe are greater than zero if the 
material gets stretched and smaller if the material gets compressed. The off-diagonal entries indicate 
the amount of shearing. Now from u(x)= p(x)-x follows that .u= .p-I. Substituting into (4.12) yields 
e =(.u+ I)T(.u+ I) - I (4.13) = .u+[.u]T +[.u]T.u (4.14) whichis Green s strain tensoruptoafactorof 12. 
Cauchy s strainis simplythe linearization of Green s strain omitting the quadratic term [.u]T.u. 4.1.2 
Strain Examples Itisnowtimetolookattwospeci.cexamples. Considerthe displacement function u(x,y,z)= [x,y,z]T. 
It stretches material radially away from the origin. In this case .u = Iwith Ithe CHAPTER 4. THE FINITE 
ELEMENT METHOD 3 identity matrix. The two strain measures are eG = Iand eC = I. Even though the displace­ 
2 mentsgetlargerandlargerfor points furtherand furtheraway fromthe origin,the strainis constanteverywhere.Thetwo 
measuresdifferbutonlybya constant scalarfactor. Inthe secondexamplewe considera rotationof90degrees aboutthe 
z-axis.Asa rigid­body mode, such a displacement .eld should not generate anystrain. We have p(x,y,z)= 
[-y,x,z]T and u =[-y- x,x- y,z- z]T. This yields . .. . -1 -10 -2 00 .u = . 1 -10 ., eC = . 0 -20 ., 
eG = 0. (4.15) 000 000 In this case, only Green s non linear tensor yields the correct result, its linearization 
cannot capture the rotation correctly. This is an important observation we will discuss in Sec­tion 4.3. 
 4.1.3 Stress Now let us turn to the measurement of stress, the force per unit area. As strain, stress 
is representedin three dimensionsbya symmetric3by3matrix or tensor .. sxx sxy sxz s = . sxy syy syz . 
(4.16) sxz syz szz with the following interpretation: As we saw before, at a single material point the 
strain depends on the direction of measurement. The same is true for the stress. Let n be the normal 
vector in the direction of measurement. Then, df = s · n. (4.17) dA In other words, to get the force 
per area f/Awith respect to a certain plane with normal n, the stress tensor is multiplied by n. 4.1.4 
Constitutive Laws Aconstitutive law relates strain to stress. Hooke s law is a special case. It states 
that stress and strain are linearly related. This holds for so called Hookean materials under small deformations. 
In three dimensions, Hooke s law reads s = Ee. (4.18) Both stress and strain are symmetric tensors so 
they have only6 independent coef.­cients. The quantity E relating the two can, thus, beexpressedbya6by6 
dimensional matrix. For isotropic materials (with equal behavior in all directions), Hooke s law has 
the form . . .. . . sxx 1- .. . exx 0 0 0 0 0 0 0 0 0 ....... syy szz sxy syz ....... = E (1+ 
.)(1- 2.) ....... . 1- .. ....... ....... eyy ezz exy eyz ....... .. 1- . , 1- 2. 000 00 1- 
2. 0 0000 szx 0000 01- 2.ezx (4.19) where the scalar E isYoung s modulus describing the elastic stiffness 
and thescalar 1 . . [0... ) Poisson s ratio, a material parameter that describes to which amount volume 
is 2 conserved within the material. 4.1.5 Equation of Motion The conceptswesawsofar canbe usedto simulate 
a dynamic elastic object. First,we apply Newton s second law of motion f= mp¨to the in.nitesimal volumetric 
element dV at location x of the object (see Fig. 4.2). Since the mass of an in.nitesimal element is not 
de.ned, both sides of the equation of motion are divided by the volume dx· dy· dzof the element. This 
turns mass [kg] into density [kg/m3] and forces [N] into body forces [N/m3]. We get .p¨= f(x), (4.20) 
where . is the densityand f(x) the body force acting on the element at location x. This force is the 
sum of external forces (e.g. gravity or collision forces) and internal forces (due to deformation). The 
next step is to compute the internal elastic force at the center of the element due to the stress. To 
get this force, we are going to sum up the forces that act on eachofthesixfacesofthe in.nitesimal element.Letus.rstlookatthefaces 
perpendicular to the x-axis. The center of theface with normal [-1,0,0]T is located at [x,y,z]T and the 
one withface normal [1,0,0]T at position [x+ dx,y,z]T. According to Eqn. (4.17) the forces per unit area 
acting on thesefaces are . . . . sxx sxx . . . . sxy sxy - and (4.21) sxz sxz x,y,zx+dx,y,z 
CHAPTER 4. THE FINITE ELEMENT METHOD Toget forces,wehaveto multiplybytheface aready· dz. Finally, the 
body forces are the forces divided by dV = dx· dy· dz. This yieldsa total forceforthetwofacesof .. .... 
.. sxx sxx sxx,x .. f= .. sxy . - . sxy ../dx= . sxy,x ., (4.22) sxz x+dx,y,z sxz x,y,z sxz,x where the 
comma denotes spatial derivatives. If we take the forces acting on the other faces into account as well, 
we arrive at the .nal expression for the body forces acting on an in.nitesimal element due to internal 
stresses .. sxx,x + sxy,y+ sxz,z fstress = . · s = . syx,x + syy,y+ syz,z ., (4.23) szx,x + szy,y+ szz,z 
where, again, the comma represents a spatial derivative. We are now ready to write down the entire partial 
differential equation (PDE) governing dynamic elastic materials: .p¨= . · s + fext, (4.24) where fext 
areexternally applied body forces such as gravity or collision forces. Thishyper­bolic PDE can be used 
to compute the world coordinates pof all material points inside the elastic body at all times which is 
the same as simulating the deformable object. How would one do this? The density . and the external forces 
fext are known quantities. The process starts by de.ning the material coordinates xand theworld coordinates 
pof all points in the body (the initial condition). Then, at each time, the displacement .eld is u = 
p- x from which the strain .eld e can be computed. The stress .eld s is derived from the strains via 
Hooke s law. Finally, s is used to compute the acceleration of the world positions of the body p¨using 
Newton s law and these accelerations de.ne how the positions evolve. A linear dependency of the stresses 
on the strains such as in a Hookean material is called material linearity. Alinear measure of strain 
such as Cauchy s linear strain tensor de.ned in Eqn. (4.5) is called geometric linearity. Only with both 
assumptions, material and geometric linearity, Eqn. (4.24) becomesa linear PDE. Linear PDE s are easier 
to solve because discretizing them via Finite Differences of Finite Elements yields linear algebraic 
systems. However, for large deformations or rotations, the simpli.cation of geometric lin­earity causes 
signi.cant visual artifacts as we saw in one of the examples above.  4.2 Finite Element Discretization 
The unknown in Eq. (4.24)is pwhich is a continuous vector .eld. This rises the question of how one would 
write down the solution. A general continuous vector .eld cannot be described in compact form because 
it contains an uncountable set of vectors. A special class of vector .elds can be speci.ed by compact 
analytical formulas though. An example is the rotation .eld p(x,y,z)=[y,-x,z] we discussed earlier.Toendup 
withadescriptionin the formofa formula, onewouldhaveto solveEq. (4.24) analytically. Thisisonly possible 
for toyproblems with simple domains and boundary conditions. Fortunately thereis anotherwayto arriveat 
solutions that canbe writtendownexplicitly. Just restrict the possible solutions to a certain class of 
continuous vector .elds that can be describedbya .nite numberofvalues. From sucha restrictionit follows 
thatinthe general case, a solution computed in this way will only be an approximation of the true solution. 
Ideally the restricted solution is the one within the restricted class of vector .elds that is closest 
to the true solution. This is the core idea of the Finite Element Method (FEM). First, the domain is 
di­vided into a .nite set of (polygonal) elements of .nite size which cover the entire domain without 
overlaps. Within each element, the vector .eld is described by an analytical for­mula that depends on 
the positions of the vertices belonging to the element. More general parametrizations are possiblebut 
we willwork withvertex positions only. 4.2.1 Constant StrainTetrahedral Meshes Tokeep things simple, 
we use tetrahedra as .nite elements and represent the domain, i.e. the volume of the deformable body 
by a tetrahedral mesh. Within each tetrahedron, we use the simplest possible deformation .eld,a linear 
mapping.Aconstant deformation .eld withinthe elementwouldbeeven simplerbuta constant mappingwouldyield 
zero strain, and would therefore not be practical to simulate deformable bodies. Let x0,x1,x2,x3 be the 
the corners of the tetrahedron in the undeformed rest state and p0,p1,p2,p3 the same corners in the deformed 
state. We now have to .nd a linear vector .eld p(x) that maps points within the tetrahedron in the rest 
state to the points within the de­formed tetrahedron. Pure translation does not generate anyelastic forces 
so we can assume that x0 = 0and p0 = 0without loss of generality. In the general case you have to replace 
xi with xi-x0 and pi with pi-p0 in the following formulas. Let us describea point inside the undeformed 
tetrahedronbya weighted sumof the corner positions, i.e. x= x1b1+ x2b2+ x3b3 =[x1,x2,x3]b. (4.25) The 
transformed position p(x) willbea weighted sumof the deformed corner positions using the same weights: 
p(x)= p1b1+ p2b2+ p3b3 =[p1,p2,p3]b. (4.26) Solving Eq. (4.25) for band substituting into Eq. (4.26) 
yields p(x)=[p1,p2,p3][x1,x2,x3]-1x = Px (4.27) ¯ This is a linear mapping with Pa3× 3matrix. The part 
X=[x1,x2,x3]-1 is constant throughout the simulation and can be pre-computed. Because p(x) is linear, 
we have .p= Pand .u = P- I (4.28) independent of the position x within the tetrahedron. This means we 
will end up with constant strain and stress inside the tetrahedron. Using Green s stress tensor we have 
e = 1(.u+[.u]T +[.u]T.u) (4.29) 2 CHAPTER 4. THE FINITE ELEMENT METHOD and with the assumption of a Hookean 
material the stress is s = Ee (4.30) with Ede.nedinEq.(4.19). Multiplyingthe stress tensorbyanormalvectoryieldsthe 
elastic force per area so forface (0,1,2) of the tetrahedron the force is f0,1,2 = s · n0,1,2· A0,1,2 
= s[(p1- p0) × (p2- p0)] (4.31) Finally, we distributethisforceevenlyamongthevertices0,1and2anddothesameforall 
faces. Equations (4.27) through(4.31) yieldarecipe for computing the forcesf0,...f3 acting on the vertices 
of a tetrahedron based on the deformed positions p0,...p3. As you can see, thewhole processisquitesimple.Itisjustastep-by-stepof 
processof computing quantities basedonprevious ones.Also,wecouldeasilyreplaceHooke slawbyanynon-linear 
stress­strain relationship without increasing the complexity of the computation signi.cantly. The resulting 
forces -position relationship computed this way is highly non-linear. A simple simulation algorithm using 
explicit Euler integration could look like this // initialization (1) forallvertices i (2) pi = xi 
(3) initialize vi and mi  (4) endfor (5) foralltetrahedra i=(i0,i1,i2,i3) (6) X¯i =[xi1 - xi0,xi2 - 
xi0,xi3 - xi0]-1 (7) endfor // simulation loop (8) loop (9) froall vertices i = fg+ fcoll (10) fi  
i (11) endfor (12) foralltetrahedra i=(i0,i1,i2,i3) (13) P=[pi1 - pi0,pi2 - pi0,pi3 - pi0] · X¯i (14) 
.u= P- I 1 (15) e =(.u+[.u]T +[.u]T.u) 2 (16) s = Ee (17) forallfaces j=( j0, j1, j2) of tetrahedron 
i  (18) fface = s[(pj1 - pj0) × (pj2 - pj0)] (19) fj0 . fj0 + 13fface (20) fj1 . fj1 + 1fface (21) 
fj1 . fj1 + 1fface  3 3 (22) endfor (23) endfor  (24) forallvertices i (25) vi . vi+ .t fi/mi (26) 
pi . pi+ .t vi  (27) endfor (28) display the system every nth time (29) endloop This algorithmisvery 
similartothe algorithmfor simulating mass-spring systems.We basically replaced the one dimensional springs 
connecting pairsof mass pointsby three di­mensionalhyper-springs connecting four mass points. The computationofthe 
forces acting at the adjacent vertices of such a hyper-spring is more expensive than computing spring 
forces but it does not make the algorithm more dif.cult to understand or to implement. Also, replacing 
theexplicit Euler schemebyaVerlet or Runge-Kutta integratorwouldbe straight forward. All this comes from 
thefact, that all forceevaluations areexplicit. For unconditional stability and the useofa global solver, 
implicit integrationis needed. Inthiscase,thingsgetmoreinvolved. LinearizingthespringforceinEq.(3.33)wasnottoo 
hard. Doingthe sameforthe FEM-based forcesisabit more complicated.Youhavetowork throughallthestepsgiveninEq.(4.27)throughEq.(4.31).Theresulting 
formulasgetmore complexand more computationally intensivetoevaluate, especiallyfor general stress-strain 
relationships. Remember, this evaluation has to be done at every time step. For a spring force connecting 
points i and jwe derived local tangent stiffness matrices Ki,i, Ki, j, Kj,i and Kj, j.Inthe caseofa tetrahedron,the 
tangent stiffness matrix Kis 12×12 dimensional. It contains 4 × 4 submatrices Ki, j, where Ki, j describes 
the interaction between vertex i and vertex j. Because each vertex in the tetrahedron in.uences all others, 
the stiffness matrix of the tetrahedron is dense. With the stiffness matrix, the tetrahedral forces can 
be approximated as f(p+ .tv)= f(p)+ K|x(.tv)+ O(.t2), (4.32) whereallvectors are12 dimensional containingthe3componentsofthe4adjacentvertices 
of the tetrahedron, i.e. TTTT p=[p1,p2,p3,p4]T (4.33) TTTT v=[v1,v2,v3,v4]T and (4.34) f(p)=[f1(p1,p2,p3,p4)T 
,f2(p1,p2,p3,p4)T ,f3(p1,p2,p3,p4)T ,f4(p1,p2,p3,p4)T]T . (4.35) 4.2.2 Linear FEM If K was constantatalltimes,the 
simulation wouldobviouslybemuchfasterand more suitable for implicit integration in real time. Let us 
see how we can make it constant. First we have to decide where we want to evaluate K. The most natural 
choice is to evaluate it at the equilibrium con.guration so Kconst = K|x. Every other location would 
be biased in some way. This choice has a further advantage. The .rst term on the right hand side of Eq. 
(4.32) cancels out because the forces at the equilibrium con.guration are zero and we CHAPTER 4. THE 
FINITE ELEMENT METHOD get f(x+ .p)= f(x)+ K|x(.p) +O(|.p|2) (4.36) f(x+(p- x)) = K|x(p- x) +O(|p- x|2) 
(4.37) f(p)= K|x(p- x) +O(|p- x|2). (4.38) (4.39) By omitting the higher order terms and writing Kfor 
K|x from now on we have a very simple equation to compute the forces acting on the vertices of the tetrahedron 
given the displacements of the vertices, namely f(p)= K(p- x). (4.40) This approximation is only valid 
close to the equilibrium because that is where the forces were linearized. There are many applications 
in which the displacements remain small, for instance whenthe stress distributionina bridgeorbuildinghavetobe 
analyzed. In general, if the deformable body does not undergo large deformations it is reasonable to 
use this approximation. But how do we compute Kfor a tetrahedron? We could linearize Eq. (4.27) through 
Eq. (4.31) and evaluate what we get at p= x. There is a simpler more intuitive way to do it though. Three 
restrictions make the forces linear to begin with so linearization is not necessaryat all.For thiswehaveto 
1 use Cauchystrain e =(.u+[.u]T), 2 assume a linear stress-strain relationship, i.e. a Hookean material 
with s = Ee  and use the original positions of the vertices in Eq. (4.31) for the computation of the 
face normals andface areas.  We will not go through the whole process of the derivation. Here is the 
.nal result, the recipeto computethe stiffness matrixofa tetrahedron. Firstyou computethe four auxiliary 
vectors y0, y1,y2 and y3 as .. yT 1 . yT . = X-1 =[x1- x0,x2- x0,x3- x0]-1 (4.41) 2 yT 3 y0 = -y1- y2- 
y3 (4.42) With these, the3×3sub-matrix ofKconnectingvertex iand j(i, j. 1...4) can be directly computed 
as . .. .. . yi,x 0 0 a b b yj,x 0 0 Ki, j= . 0 yi,y 0 .. b a b .. 0 yj,y 0 . (4.43) . 0 0 yi,z .. b 
b a .. 0 0 yj,z . yi,y 0 yi,z c 0 0 yj,y yj,x 0 + . yi,x yi,z 0 .. 0 c 0 .. 0 yj,z yj,y ., (4.44) 0 yi,y 
yi,x 0 0 c yj,z 0 yj,x, where a = VE 1- . (1+ .)(1- 2.), (4.45) b= VE . (1+ .)(1- 2.), (4.46) c = VE 
1- 2. (4.47) (1+ .)(1- 2.), V = det(X), E isYoungs Modulus and . the Poisson ratio. The center matrix 
on line (4.43) is the upper left sub-matrix of Egiven in Eq. (4.19) and the matrices to the left and 
right of it represent normal strain. The center matrix on line (4.44) is the lower right sub-matrix of 
Eand the matrices to the left and right correspond to shear strain, where the subscripts after the comma 
are the spatial components of the vectors, not their derivatives.  4.3 Warped Stiffness In the framework 
of linear FEM discussed in the previous section, the stiffness matrices of all tetrahedra are constant, 
i.e. depend only on the rest con.guration of the vertices and canbe precomputed. Thismakesthe simulationfastandallows 
real-time simulationofhigh resolution tetrahedra meshes. However, we have seen in Section 4.1.2 that 
Cauchystrain cannot capture rotational deformations correctly (see Fig. 4.3). In graphics, this is a 
prob­lem, because only large deformations and free rotational motion yield visually interesting effects. 
The technique of stiffness warping which we will discuss in this section solves this prob­lembyexplicitlyextracting 
the rotational partof the deformation. The methodremoves the visual artifacts underlarge rotational deformations 
and allows to use the constant stiffness matrices of linear FEM. Let us see how this works. 4.3.1 Extraction 
of Rotation If we write Ke for the stiffness matrix of a tetrahedral element e to distinguish it from 
the stiffness matrix Kof the entire mesh, the computation of the linear forces acting on the four vertices 
of the element are f= Ke · (p- x). (4.48) CHAPTER 4. THE FINITE ELEMENT METHOD e ments R-1p- x are multiplied 
with the stiffness matrix yielding the forces Ke(R-1p- x) ee that are .nally rotated back to the frame 
of the deformed tetrahedron by multiplying them with Re. Now comes the trick. Let us assume that we know 
the rotational part Re of the deformation ofthe tetrahedron.Weusethis matrixto rotatetheworld coordinates 
pof the vertices back into an unrotated con.guration R-1p. For this to work, Re needs to be a 12 × 12 
matrix e containing four identical3 ×3rotation matricesalong its diagonal. After the transformation, 
the displacements are R-1p- x. The linear forces in this unrotated frame are Ke(R-1p- ee x). Finally, 
the forces have to be moved back into the rotated current con.guration of the tetrahedron yielding fwarped 
= ReKe(R-e 1p- x). (4.49) This computationisfast. Also remember that fora rotation matrix R-1 = RT.To 
use these ee force in an implicit Euler scheme we substitute them into Eq. (3.23) and Eq. (3.24) which 
yields Mvt+1 = Mvt + .tReKe(RTpt+1- x)+ fext(4.50) e t+1 t+1 p= pt + .tv. (4.51) Again, we substitute 
the second line into the .rst and get Mvt+1 = Mvt + .tReKe(RT(pt + .tvt+1) - x)+ fext(4.52) e t+1 (M- 
.t2ReKeRT)v= Mvt + .tReKe(RTpt - x)+ fext(4.53) ee t+1 (M- .t2K/e)v= Mvt + .tK/ept - f0+ fext, (4.54) 
where K/= ReKeRT and f0 = ReKex. The last line is the linear equation for the new ee velocities. All 
quantities except vt+1 are known at the current time step and form a linear system for the new velocities. 
This is the equation for a single tetrahedron. The one for the entire mesh is t+1 (M- .t2K/)v= Mvt + 
.t K/pt - f0+ fext . (4.55) Constructing the global stiffness matrix Kfrom the element matrices Ke and 
the global right hand side force offset is called element assembly. This process can be written as K= 
.ReKeRT e (4.56) e f0 = .ReKex. (4.57) e Thesearenotvalid equationsastheystandhere.Ontheleftwehave3n 
dimensional quan­tities, n being the number of mesh vertices, while on the right the quantities are 12 
dimen­sionalduetothe fourvertices froma tetrahedral element.Forthe formulastobevalid,the quantities on 
the right are made3n dimensional as well by placing in the sub matrices and vectors corresponding to 
the vertices of the tetrahedron at the right spots w.r.t to the global mesh numbering and .lling the 
rest with zeros. The remaining question is how to get the rotations Re of the tetrahedra. 4.3.2 Determining 
Element Rotation From Eq. (4.27) we know that the non-translational part of the mapping from the original 
con.guration (x0,x1,x2,x3) to the current con.guration (p0,p1,p2,p3) of a tetrahedron is the3 × 3dimensional 
matrix A=[p1- p0,p2- p0,p3- p0][x1- x0,x2- x0,x3- x0]-1 . (4.58) The rotational part of Ais the rotation 
we are looking for. There are several ways to con­struct a rotation matrix from A, also called orthonormalization 
of A. The simples is the Gram-Schmidt method. Let A=[a0,a1,a2] with ai the column vectors of Aand the 
axes ofthe transformation.Weneedtomake surethattheyallhaveunitlengthandthatthey are perpendicular to 
each other. The Gram-Schmidt method computes the three columns of the rotation matrix a0 r0 = (4.59) 
|a0| a1- r0· a1 r1 = (4.60) |a1- r0· a1| r2 = r0× r1. (4.61) The third line is non-Gram-Schmidt but simpler 
and makes sure that the resulting rotation matrix R=[r0,r1,r2] is right handed. This procedure is dependent 
on the order in which the axes are processed. The .rst axis of the rotation will always be aligned with 
the .rst axisof the transformation for instance. Thisfact might yield some artifacts.Typically, this 
isnotabig issue.Youjusthavetomake surethatyoukeeptheorderthe same throughout the simulation. Amathematically 
more correctand unbiasedwaytoextractthe rotationsisto .ndthe rotation matrix R that is closest to A in 
the least squares sense. It isa well knownfact CHAPTER 4. THE FINITE ELEMENT METHOD from linear algebra 
that each matrix canbefactored intoa rotation matrix anda symmetric matrix, i.e. A= RS. (4.62) Thisfactorizationis 
called polar decomposition and should be used for unbiased and more accurate results.  Chapter5 Position 
Based Dynamics MatthiasM¨uller The most popular approaches for the simulation of dynamic systems in 
computer graph­ics are force based. Internal and external forces are accumulated from which accelerations 
are computed based on Newton s second law of motion. A time integration method is then used to update 
the velocities and .nally the positions of the object. Afew simulation methods (most rigid body simulators) 
use impulse based dynamics and directly manipu­late velocities. In this chapter we present an approach 
which omits the velocity layer as well and immediately works on the positions. The main advantage of 
a position based ap­proach is its controllability. Overshooting problems of explicit integration schemes 
in force based systems can be avoided. In addition, collision constraints can be handled easily and penetrations 
canbe resolved completelyby projecting pointstovalid locations. 36 CHAPTER 5. POSITION BASED DYNAMICS 
5.1 Position Based Simulation The objectstobe simulated are representedbyasetof Nparticlesandasetof Mconstraints. 
Each particle ihas three attributes, namely mi mass xi position vi velocity Aconstraint jis de.ned by 
the .ve attributes nj cardinality Cj:R3nj . R scalar constraint function {i1,...inj}, ik . [1,...N] 
set of indices . kj. [0...1] stiffness parameter unilateral or bilateral type Constraint jwith type 
bilateral is satis.ed ifCj(xi1,...,xinj)= 0. If its type is unilateral then it is satis.ed if Cj(xi1,...,xinj) 
= 0. The stiffness parameter kj de.nes the strength of the constraint in a range from zero to one. Given 
this data and a time step .t, the simulation proceeds as follows: (1) forallparticles i (2) initialize 
xi = xi0 ,vi = vi0 ,wi = 1/mi  (3) endfor (4) loop  (5) forallparticles idovi . vi+ .twifext(xi) 
(6) forallparticles idopi . xi+ .tvi (7) forallparticles ido generateCollisionConstraints(xi . pi) 
(8) loop solverIterations times (9) projectConstraints(C1,...,CM+Mcoll ,p1,...,pN)  (10) endloop (11) 
forallparticles i (12) vi . (pi- xi)/.t (13) xi . pi  (14) endfor (15) endloop  Sincethealgorithm 
simulatesasystemwhichissecondorderintime,both,thepositions and the velocities of the particles need to 
be speci.ed in (1)-(3) before the simulation loop starts. Lines (5)-(6) perform a simple explicit forward 
Euler integration step on the veloci­ties and the positions. The new locations pi are not assigned to 
the positions directlybut are only used as predictions. Non-permanent external constraints such as collision 
constraints are generated at the beginning of each time step from scratch in line (7). Here the origi­nal 
and the predicted positions are used in order to perform continuous collision detection. The solver (8)-(10) 
then iteratively corrects the predicted positions such that theysatisfy the Mcoll external as well as 
the M internal constraints. Finally the corrected positions pi are used to update the positions and the 
velocities. It is essential here to update the velocities along with the positions. If this is not done, 
the simulation does not produce the correct behaviorofa second order system.Asyou can see,the integration 
schemeused hereisvery similartotheVerlet method describedinEq. (3.19)andEq. (3.20). 5.2 The System to 
be solved The goal of the solver step (8)-(10) is to correct the predicted positions pi of the particles 
suchthattheysatisfyall constraints.Theproblemthatneedstobesolved comprisesofaset of Mequations for the3Nunknown 
position components, where Mis now thetotal number of constraints. This system does not need to be symmetric. 
If M> 3N(M<3N) the system is over-determined (under-determined). In addition to the asymmetry, the equations 
are in general non-linear. The functionofa simple distance constraint C(p1,p2)=(p1-p2)2-d2 yieldsa non-linear 
equation. What complicates thingsevenfurtheristhefact that collisions produce inequalities rather than 
equalities. Solving a non-symmetric, non-linear system with equalities and inequalities is a tough problem. 
Let pbe the concatenation [pT 1,...,pTN]T and let all the constraint functions Cj take the concatenated 
vector pasinput while onlyusing the subset of coordinates they are de.ned for.Wecannow writethe systemtobesolvedas 
C1(p) > 0 ... CM(p) > 0, where the symbol > denotes either = or =. As we saw in Section 3.5.1, Newton-Raphson 
iteration is a method to solve non-linear symmetric systems with equalities only. The pro­cess starts 
with a .rst guess of a solution. Each constraint function is then linearized in the neighborhood of the 
current solution using C(p+ .p)= C(p)+ .pC(p) · .p+ O(|.p|2)= 0. (5.1) This yields a linear system for 
the global correction vector .p .pC1(p) · .p = -C1(p) ... .pCM(p) · .p = -CM(p), where .pCj(p) is the1× 
N dimensional vector containing the derivatives of the function Cj w.r.t. all its parameters, i.e. the 
N components of p. It is also the jth row of the linear CHAPTER 5. POSITION BASED DYNAMICS system. Both, 
the rows .pCj(p) and the right hand side scalars -Cj(p) are constant because they are evaluated at the 
location pbefore the system is solved. When M= 3N and only equalities are present, the system can be 
solved by any linear solver, e.g. PCG. Once it is solved for .pthe current solution is updated as p. 
p+ .p. A new linear system is generated by evaluating .pCj(p) and -Cj(p) at the new location after which 
the process repeats. If M = 3M the resulting matrix of the linear system is non-symmetric and not invert­ible. 
[GHF+07] solve this problemby using the pseudo-inverseof the system matrix which yields the best solution 
in the least-squares sense. Still, handling inequalities is not possible directly. 5.3 The Non-Linear 
Gauss-Seidel Solver In the position based dynamics appraoch, non-linear Gauss-Seidel is used. It solves 
each constraint equation separately. Each constraint yields a single scalar equation C(p) > 0 for all 
the particle positions associated with it. The subsystem is therefore highly under­determined. PBD solves 
this problem as follows. Again, given pwe want to .nd a cor­rection .psuch that C(p+ .p)= 0. It is important 
to notice that PBD also linearizes the constraint functionbut individually for each constraint. The constraint 
equationis approxi­mated by C(p+ .p) C(p)+ .pC(p) · .p= 0. (5.2) The problem of the system being under-determined 
is solved by restricting .ptobein the direction of .pC which conserves the linear and angular momenta. 
This means that only one scalar . -a Lagrange multiplier -has to be found such that the correction .p= 
..pC(p). (5.3) solves (5.2). This yields the following formula for the correction vector of a single 
particle i .pi = -s wi.piC(p), (5.4) where s = C(p) .jwj|.pjC(p)|2 (5.5) and wi = 1/mi. As mentioned 
above, this solver linearizes the constraint functions. However, in con­trast to the Newton-Raphson method, 
the linearization happens individually per constraint. Solving the linearized constraint function of 
a single distance constraint for instance yields the correct result in a single step. Because the positions 
are immediately updated after a constraint is processed, these updates will in.uence the linearization 
of the next constraint because the linearization depends on the actual positions. Asymmetry poses no 
problem because each constraint produces one scalar equation for one unknown Lagrange multiplier .. Inequalities 
are handled trivially by .rst checking whether C(p) = 0. If this is the case, the constraint is simply 
skipped. We have not considered the stiffnesskofthe constraintsofar. There areseveralwaysof incorporating 
the it. The simplest variant is to multiply the corrections .pby k. [0...1]. However, for multiple iteration 
loops of the solver, the effect of kis non-linear. The remain­ing error for a single distance constraint 
after ns solver iterations is .p(1- k)ns. To get a linear relationship we multiply the corrections not 
by kdirectlybut by k/ = 1- (1- k)1/ns. With this transformation the error becomes .p(1- k/)ns = .p(1- 
k) and, thus, becomes linearly dependent on k and independent of ns as desired. However, the resulting 
mate­rial stiffness is still dependent on the time step of the simulation. Realtime environments typically 
use .xed time steps in which case this dependencyis not problematic. 5.4 Constraint Examples 5.4.1 Stretching 
Togive anexample, let us consider the distance constraint functionC(p1,p2)= |p1- p2|- d. The derivative 
with respect to the points are .p1C(p1,p2)= n and .p2C(p1,p2)= -n p1-p2 |p1-p2|-d with n= . The scalingfactor 
s is, thus, s = and the .nal corrections |p1-p2| 1+1 w1 p1- p2 .p1 = - (|p1- p2|- d) (5.6) w1+ w2 |p1- 
p2|w2 p1- p2 .p2 =+ (|p1- p2|- d) (5.7) w1+ w2 |p1- p2| which are the formulas proposed in [Jak01] for 
the projection of distance constraints (see Figure 5.2). Theypop up as a special case of the general 
constraint projection method. 5.4.2 Bending In cloth simulation it is important to simulate bending 
in addition to stretching resistance. Tothis end, for each pair of adjacent triangles(p1,p3,p2) and (p1,p2,p4) 
abilateral bending constraint is added with constraint function Cbend (p1,p2,p3,p4)= (p2- p1) × (p3- 
p1)(p2- p1) × (p4- p1) acos ·- .0, |(p2- p1) × (p3- p1)||(p2- p1) × (p4- p1)| stiffness kbend . The 
scalar .0 is the initial dihedral angle between the two triangles and kbend is a global user parameter 
de.ning the bending stiffness of the cloth (see Figure 5.3). The advantage of this bending term over 
adding a distance constraint between points p3 and p4 CHAPTER 5. POSITION BASED DYNAMICS  oroverthe 
bending term proposedby [GHDS03]is thatitis independent of stretching. This is because the term is independent 
of edge lengths. 5.4.3 Triangle Collisions The handling of self collisions within cloth can be handled 
by an additional unilateral con­straint.For vertex qmoving through a triangle p1, p2, p3, the constraint 
function reads (p2- p1) × (p3- p1) C(q,p1,p2,p3)=(q- p1) ·- h, (5.8) |(p2- p1) × (p3- p1)| where his 
the cloth thickness. If the vertex enters from below with respect to the triangle normal, the constraint 
function has to be (p3- p1) × (p2- p1) C(q,p1,p2,p3)=(q- p1) ·- h (5.9) |(p3- p1) × (p2- p1)| 5.4.4 
Volume Conservation For tetrahedral meshes it useful to have a constraint that conserves the volume of 
single tetrahedra. Such a constraint has the form 1 C(p1,p2, p3,p4)= ((p2- p1) × (p3- p1)) · (p4- p1) 
-V0, (5.10) 6where p1, p2, p3 and p4 are the four corners of the tetrahedron and V0 its rest volume. 
 100 ,1). The bottom row shows (kstretching ,kbending) = 100 ,0). (1,0), (12,0) and ( 1 5.4.5 Shape 
Matching Shape Matching can be used to formulate a constraint for the entire set of particles. No connectivity 
is needed in this case. In correspondence to rest lengths, rest angles and rest volumes in the previous 
constraints, this constraint needs the original positions of all the particles x0. We do not formulate 
the constraint using a functionC(...) but describe the i projection of the points due to this constraint 
directly. In order to .nd the projected positions, a shape matching problem with a priori known correspondences 
has to be solved: Given two sets of points xi 0 and pi, .nd the rotation CHAPTER 5. POSITION BASED DYNAMICS 
matrix Rand the translation vectors tand t0 which minimize 0 .wi(R(xi - t0)+ t- pi)2 , (5.11) i where 
the wi are weights of individual points. The natural choice for the weights is wi = mi. The optimal translation 
vectors turn out to be the center of mass of the initial shape and the center of mass of the actual shape, 
i. e. .imix0 .imipi t0 = x0 = i , t= , (5.12) cm = xcm .imi .imi which is physically plausible. Finding 
the optimal rotation is slightly more involved. Let us de.ne the relative locations qi = xi 0- x0 and 
pi = xi- xcm of points with respect to cm their center of mass and let us relax the problem of .nding 
the optimal rotation matrix Rto .nding the optimal linear transformation A. Now, the term to be minimized 
is .imi(Aqi- pi)2. Setting the derivatives with respect to all coef.cients of Ato zero yields the optimal 
transformation TT A=(.mipiqi )(.miqiqi )-1 = ApqAqq. (5.13) ii The second term Aqq isa symmetric matrix 
and, thus, contains only scalingbut no rotation. Therefore, the optimal rotation Ris the rotational part 
of Apq which canbe foundviaapolar p decomposition Apq = RS,where the symmetric part isS= AT Apq and the 
rotational part pq is R= ApqS-1. Finally, the projected positions can be computed as 00 pi = R(xi - x)+ 
xcm. (5.14) cm Linear Deformations The method described sofar can only simulatesmalldeviations from the 
rigid shape.To extend the range of motion, the linear transformation matrix Acomputedin (5.13) can be 
used. This matrix describes the best linear transformation of the initial shape to match the actual shape 
in the least squares sense. Instead of using R in (5.14) to compute the gi, we use the combination ßA+(1- 
ß)R, where ß is an additional control parameter. This way,thegoalshapeisallowedtoundergoalinear transformation.The 
presenceof Rin the sum ensures that thereisstilla tendencytowardsthe undeformed shape.Tomake sure that 
c volume is conserved, we divide Aby 3 det(A) ensuring that det(A)= 1. For the standard approach we only 
need to compute Apq. Here, we also need the matrix Aqq =(.imiqiqTi )-1. Fortunately, this symmetric3× 
3matrix can be pre-computed. Quadratic Deformations Linear transformations can only represent shear 
and stretch.Toextend the rangeof motion by twist and bending modes, we move from linear to quadratic 
transformations. We de.ne a quadratic transformation as follows: gi =[AQM]q i, (5.15) Figure 5.7:Visualizationof 
all3 × 9 modes de.ned by the coef.cients of A =[AQM] de.ned in (5.15). 222 where gi . R3, q =[qx,qy,qz, 
qx,qy,qz, qxqy,qyqz,qzqx]T . R9, A. R3×3 contains the coef.cients for the linear terms, Q. R3×3 the coef.cients 
for the purely quadratic terms and M. R3×3 the coef.cients for the mixed terms. With A =[AQM] . R3×9 
we now have to minimize .imi(A q i- pi)2. The optimal quadratic transformation turns out to be TT A =(.mipiq 
i )(.miq iq i )-1 = A pqA qq. (5.16) ii Again, the symmetric A qq . R9×9 as well as the q i can be pre-computed. 
Analogous to the linear case, we use ßA +(1- ß)R to compute the goal shape, where R . R3×9 =[R00]. The 
algorithm based on quadratic deformations is a computationally cheap imitation of methods using modal 
analysis. The linear shear and stretch modes and the additional bend and twist modes are shown in Fig. 
5.7.   Chapter6 Rigid BodySimulation MatthiasM ¨uller Rigid bodies playa central rolein real-time 
applications such as computergames be­cause most objects around us can be considered as non-deformable. 
This is whythe core of mostgamephysicsenginesisa rigidbody solver. Representing objectsas rigid bodies 
is very effective, both in terms of simulation speed and memory footprint. Simulating rigid bodies in 
free .ight and handling dynamic collisions is not that hard. Surprisingly it is much hardertostablysimulate 
stacksofrigid bodiesatrestthanto simulateanexplosionin which manyobjects interact with each other dynamically. 
In the resting scenario the player will notice even tinyjittering so impulse exchanges between interacting 
bodies have to be computed accurately and consistently. In the explosion scenario is is much harder to 
see whether objects behave in a physically correct way or not. In the .rst part of this chapter, we will 
think of a rigid body as a set of point masses connected by springs of in.nite stiffness. This allows 
us to start from what we learned in Chapter3about massspring systems. Later, wewillgetridofthe particle 
representation. 45 6.1 Linear Motion We represent a rigid body by particles having massesmi, original 
positions x¯i, actual posi­tions xi and velocities vi. The original and current center of mass are given 
by the mass­weighted sum of the positions of the particles as x¯= 1 .mix¯i (6.1) M i x = 1 .mixi, (6.2) 
M i where M= .imi isthe total massofthebody.Wewillnow reorderthe termsabittoget Mx = .mixi (6.3) i Mx¨= 
.mix¨i (6.4) i Mx¨= .fi (6.5) i Mx¨= F, (6.6) where we have used Newton s second law and F= .ifi. Thus, 
for linear motion the entire rigid body can be represented by a single particle at the center of mass 
with the total mass of the particles on which the sum of all particle forces acts. 6.2 Angular Motion 
For linear motion, the assumption of rigidity let us replace individual particles by a single particle 
at the center of mass with the total mass of the body. The rigidity constraint also simpli.es angular 
motion of the particles about the center of mass. On a rigid body, all the particles have to have the 
same angular velocity. Let us .rst look at Newton s second law for the angular case. The angular quantity 
that corresponds to linear impulse pi = mix.is called angular momentum and is de.ned as. Li = ri× (mix.i)= 
ri× pi (6.7) where ri isthe current distancevector fromthe centerof masstothe particle, i.e. ri= xi-x. 
The cross product makes sure that only the part of the impulse perpendicular to the radius, i.e. alongthe 
tangentofthe rotationis considered. The quantity that correspondsto forceis called torque ti and de.ned 
as ti = ri× fi. (6.8) In correspondence with Eq. (6.6), Newton s second law for the angular case reads 
.ri× fi = d .ri× mix.i dti (6.9) . t = L. CHAPTER 6. RIGID BODY SIMULATION At this point we use thefact 
that all particleshave the same rotational velocity .. This allows us to express the angular part of 
their velocity as x.i = . × ri. (6.10) where . is the angular velocity of the entire body. Using this 
fact we can simplify the expression for the angular momentum L= .ri× mix. i = .ri× mi. × ri i = .-miri× 
ri× . (6.11) i = .-miskew(ri)skew(ri). i = J., where skew(r) . R3×3 is the matrix with the property skew(r)x 
= r× x. Because . is the same for all particles, it can be pulled out of the sum while the remaining 
part is called the moment of inertia or inertia tensor J. R3×3 = .-miskew(ri)skew(ri) (6.12) i Note that 
Jdepends on the current orientation of the rigid body. Given the current orienta­tion as a rotation matrix 
Rthe current inertia tensor can computed from the one related to the original pose by using J= R¯ JRT. 
There are four quantities that describe the state of a rigid body, namely the position of the center 
of mass x,  the orientation represented by a rotation matrix R,  the linear velocity of the center 
of mass x.and  the angular velocity . about the center of mass.  In rigid body simulators, the orientation 
is often represented by a quaternion. The orien­tation of a rigid body is a three dimensional quantity. 
Amatrix has six additional degrees of freedom while a quaternion only has one additional dimension. Due 
to numerical errors in the time integration, both representations drift away from representing true orientations. 
This problemis more severe for matrices. Here we stick with the matrix representation be­cause it is 
easier to understand. Also, one can always go back and forth from a quaternion to a matrix if needed. 
It is more convenient to use the linear momentum p= Mx.instead of x.and the angular momentum L= J. instead 
of .. The state of a rigid body can then be expressed by . . x(t) R(t) ... ... S(t)= . (6.13) 
p(t) L(t) In order to simulate the body we need to know how this state vector changes over time, i.e. 
we need to know its time derivative S.. First, we have Newton s second law saying that p.= F and from 
the de.nition of the linear momentum we have x.= M-1p. while in the angular case Newton s second law 
states that . L= t. The last piece is a bit more involved. What is R.? The angular velocity. rotates 
a vector around the origin yielding x.= . × x. Likewise . rotates the axes of R=[r1,r2,r3] so that R.=[. 
× r1,. × r2,. × r3]= skew(.)R. The angular velocity . can be computed via . = J-1Lwhile the inverse of 
the current inertia J-1 is related to the inverse of the original ¯ inertia tensor J-1 by J-1 = RJ¯-1RT. 
This yields the .nal equation of motion for the rigid body: . . . . M-1p ... x.. R p.. L ... = ... 
 skew(RJ¯-1RTL)R ... . S = . (6.14) F t Asimulator starts with an initial condition for the state 
of the rigid body and then integrates Eq. (6.14) in time. Using explicit Euler integration, a simulation 
algorithm would look like this: // initialization (1) M. .imi (2) x¯. (.ix¯i)/M (3) r¯i . x¯i- x¯¯ 
 (4) J-1 . (-.imiskew(r¯i)skew(r¯i))-1 (5) initialize x,v,R,L (6) J-1 . RJ¯-1RT (7) . . J-1L // simulation 
 (8) loop (9) F. .ifi (10) t . .iri× fi (11) x. x+ .tv (12) v. v+ .tF/M (13) R. R+ .tskew(.)R (14) 
L. L+ .tt (15) J-1 . RJ¯-1RT (16) . . J-1L (17) ri . Rr¯i (18) xi . x+ ri (19) vi . v+ . × ri  
CHAPTER 6. RIGID BODY SIMULATION (20) endloop 6.3 Collision handling There are two non-degenerate cases 
for collisions of two rigid bodies A and B, namely corner-face collisions and edge-edge collisions. The 
contact normal nis an important quan­tityinrigidbody collisionhandling.Inthecaseofacorner-face collision, 
nis perpendicular to the collisionface. When two edges collide, n points along the cross product of the 
di­rection of the two edges. Forces and impulses are always exchanged along n and only the component 
of contact velocities along n have to be considered. Therefore, all the quantities relevant for collision 
handling at a given contact point can be represented by scalars along n which simpli.es things quite 
a lot! In the real world, fully rigid bodies do not exist. In almost rigid objects, impacts cause tinydeformations 
at the contact point which, in turn, cause high stress forces that accelerate the bodies away from each 
other. Only after a .nite amount of time, the bodies are fully separated again. The stiffer the materials, 
the stronger the stresses and the shorter the re­sponse time. Sohypothetically, for in.nitely stiffmaterials, 
the response time is zero and thevelocities change immediately.We use the symbols u- and u+ for velocities 
before and after the collision along the contact normal, so - -- uA =(vA + wA × rA) · n (6.15) -- uB 
=(vB + .B -× rB) · n, where rA and rB are the positions of the collision point relative the center of 
mass of body Aand Brespectively. The relative velocity at the collision point before the collision is 
- -- urel = uA - u(6.16) B Instead of using a collision force we use a collision impulse p= np which 
changes this relative velocity immediately. Because it acts parallel to the contact normal it can be 
rep­resented by a scalar p. For a resting contact and a bouncycontact the impulse phas to be chosen such 
that the relative velocity after the collision + u rel = 0 (6.17) u+= -eu- rel rel respectively, where 
e is the coef.cientof restitution.For e = 1the collision is fully elastic while for e = 0it is fully 
inelastic. An impulsepchanges the linear and angular velocities of body Avia p= MA.vA (6.18) rA× p= JA..A 
which results in a change in uA of .uA =[.vA+ ..A× rA] · n (6.19) = pMA -1+(JA -1(rA× p)) × rA · n (6.20) 
= p nMA -1+(J-A 1(rA× n)) × rA · n (6.21) = p n· nM-1 A (rA× n)) × rAn (6.22) A +(J-1 · = p MA -1+(rA× 
n)TJ-A 1(rA× n) (6.23) = pwA (6.24) Fromstep(6.22)to (6.23),thefactthattheterm aftertheplussignisatriple 
productwas used. The formula for .ub is obviously analogous. The scalar wA can be computed from quantities 
known before the collision.We are now ready to solve for pusing Eq. (6.17): + - u= u rel rel + .uA- .uB 
- = urel + pwA- (-pwB) (6.25) -- = urel + p(wA+ wB)= ! -eurel It is important to notethat while pis applied 
to body A, -pis applied to body B. Solving for pyields - -(1+ e)urel p= (6.26) wA+ wB Once pis known, 
the kinematic quantities can be updated using + - v= vA + pnM-1 AA + - v= vB - pnM-1 BB (6.27) .A += 
.A - + pJ-A 1(rA× n) .B += .A -- pJ-B 1(rB× n) 6.4 Resting Contacts In a resting con.guration there 
are multiple (say N)contact points for which the solver + i has to make sure that urel >= 0. The change 
of the velocity .uA of body Aat contact iis in.uenced by all the impulses of contacts jthat in.uence 
body A: V i jii .uA = .pjMA -1+ .(J-A 1(rA× pj)) × rA · njj jM-1 j ii = . pjnA +(JA -1(rA× nj)) × r· 
n Aj (6.28) i jM-1 ii)TJ-1 jj) = . pjn· nA +(rA× nA (rA× nj = . pjwij A j CHAPTER 6. RIGID BODY SIMULATION 
This yields a linear equation for each contact i+ i- ii u= u rel rel + .uA- .uB i- (s j ij ij = urel 
+ . AwA - sBjwB) pj j i- (6.29) = urel + .aij pj j = 0 which is a system of N inequalities for the scalars 
pi. The coef.cient sAj is +1 if pj is applied to body Ain the direction of nj and -1ifpj is applied to 
body Ain the opposite direction of nj. It is zero if pjis not applied to body Aat all. The coef.cients 
sBjare de.ned similarly.We use the following de.nitions A=[aij] . RN×N p=[p1, p2,... pN]T . RN×1 (6.30) 
1- 2- N- u=[urel ,urel ,...,urel ]T . RN×1 c=[c1,c2,...cN]T . RN×1 , i- where ci = |urel| are the magnitudes 
of the pre impact relative velocities. Now we can state the problem as Ap+ u= 0 (6.31) p= 0, where the 
symbol = is used componentwise. All the vectors pwithin the polytope de.ned bythe inequalitiesare solutionstothisproblem.Togetaunique 
solutionweneed additional constraints. One additional constraint requires Ap+ u = c, i.e. that kinetic 
energy cannot begainedby the collision. Among all remaining solutionvectors pwe want the one that minimizes 
the quadratic function |Ap+ u|2 namely the magnitudes of the post velocities. This is a convex quadratic 
programming problem (QP) that can be formulated as a linear complementarity problem (LCP). 6.5 Dynamic 
and Static Friction For simulating friction, velocities perpendicular to the contact normaln have to 
be consid­ered. Let pbe the magnitude of the impulse along n computed as shown above. One way to do this 
is to add two additional constraints to each contact point with normals n. 1 and n. 2 perpendicular to 
n and to each other. The scalar impulses p. 2 along these two 1 and p. additional directions have to 
be solved for with the original impulses simultaneously. p Inthecaseofdynamic friction,the conditionmustholdthat 
p.12+ p.2= cdyn pwhere 2 cdyn is the dynamic friction constant. The three impulse magnitudes can be thought 
of as lying on a cone centered at the collision point. For static friction the situation is different. 
If p= pmax the solver has to make sure that the relative velocities along the tangential directions are 
zero by applying the appropriate impulses p. 1 and p. . The scalar pmax is a material parameter. It is 
non-trivial to include 2 these constraints into the non-frictional problem. Fortunately there is a very 
simple way to solve both, the original LCP as well as the additional friction constraints, namely the 
Gauss-Seidel technique. 6.6 Real Time Simulation using a Gauss-Seidel Solver The main drawback of a 
Gauss-Seidel solver (GS) is its slow convergence in comparison to global solvers. There are manyadvantages 
though. The most important one is the simplic­ity with which the complex LCP including static and dynamic 
friction constraints can be solved. In addition, ill posed and over constraint problems are handled in 
a robust manner. These properties are ideal for the usein computergames and other real time environments. 
The general idea is very simple. Instead of considering the global problem, GS iterates through each 
constraint one after the other, computes impulses locally using Eq. (6.26) and updates the kinematic 
quantities of the participating bodies immediately using Eq. (6.27). Before such a step, the relative 
velocity along the constraint is computed using the current linearandangularvelocitiesofthebodies.Ifthisvelocityisgreaterthanzero,the 
constraint is simply skipped. The solver also makes sure that the impulses applied are always larger 
than zero which canbe easily done when lookingat one single constraint.For static friction, the tangential 
constraints solve for zerovelocitiesbut onlyif the current normal impulseis above the static friction 
threshold, which can be different for the two directions resulting in an-isotropic friction. An important 
problem that comes with a velocity based approach is drift. Making sure that relative velocities are 
zero does not prevent penetrations. Once there is a penetration due to numerical errors for instance, 
the solver does not remove them. The solution to this problem is to move the bodies apart. One has to 
be careful to do this correctly. It is not correct to apply a simple translation. Rather, one has to 
solve for velocities that remove the penetration when multiplied by the time step. Chapter7  Reduced-order 
deformable models 53  Chapter8 User Interaction and Control 54 Chapter9 Introduction to Fluids Nils 
Thuerey As .uids are everywhere around us, they are an important part of physical simulations for virtual 
environments. In addition, correctly solving the underlying equations can result in complex and beautiful 
structures and motions. Numerical simulations of .uid .ow have been used for a long time in the .eld 
of computer graphics, although mainly for off-line animations(anexampleisshowninFig.9.1).In[YUM86],.uid 
simulationswere.rstused for the generation of animated textures. In 1990, Kass and Miller [KM90] demonstrated 
the practical use and ef.ciencyof height .eld .uids. First three dimensional .uid simulations were performedin 
[FM96]byFoster and Metaxas. The stable .uids approach, as presented by Jos Stam in [Sta99], made it possible 
to guarantee stability of the advection step, and lead to a variety of applications in the .eld. With 
this approach, level-set based simulations of liquids were made popular by the group of Ron Fedkiw, starting 
with [FF01, EMF02]. Since then, a variety of extensions and im­provements of the original algorithm have 
been proposed. Fluid simulations have been used for .re [NFJ02], large scale smoke [RNGF03], particle 
based explosions [FOA03], and were coupled to rigid bodies [CMT04, BBB07]. A .rst real-time GPU implementa­tion 
of a level-set based .uid solver, yielding a high performance, was demonstrated in [CLT07]. Due to the 
long run-times, control of off-line simulations also became an im­portant topic [MTPS04, SY05a, SY05b, 
TKPR06]. We will discuss eulerian .uid sim­ulations, based on this semi Lagrangian method, within the 
.eld of real-time applica- Figure 9.1:Atypicaloff-line .uid animation: liquidis poured intoa glass shaped 
obstacle. Animation and rendering require manyhours to compute. 55 tions in Chapter 10. In contrast 
to the previously mentioned work, the group of James O Brienworkedon algorithmstousedynamicmeshes insteadof 
equidistant Cartesiangrids [KFCO06, FOK05, CFL+07]. However, due to the complexity of these approaches, 
they arenot aimedat real-time simulations,andwillnotbe discussedinthe following.Adiffer­ent approach 
to guided .uid simulations was presented by Angelidis et al. in [AN05] and [ANSN06]. It is based on vortex 
.laments, and allows for real-time smoke simulations and control. Due to the limitations of an underlying 
grid, particle based approaches are especially popular for real-time simulations, as theydo not require 
a grid throughout the whole do­main. They are based on the so-called smoothed particle hydrodynamics 
(SPH) that were originally developed for astro-physics applications [Mon92]. The .rst SPH paper to target 
the real-time simulation of liquids was [MCG03]. This method was later extended to, e.g., handle multi-phase 
.uids in [MSKG05]. As SPH does not require a global correction step for the incompressibility of the 
.uid, the resulting compressibility artifacts are still a topic of current research activities [BT07]. 
Due to the simplicity and ef.ciency of the underlying algorithm, lattice Boltzmann methods (LBM) have 
become an interesting alternative to the approaches mentioned above. Theyhave been demonstrated, e.g., 
in [WZF+03,LWK03, WLMK04] for wind simulations with interacting bodies.In [WWXP06],theLBMwas usedto simulatefalling 
snowin real­time. It was also used for simulations of free surface .ows, e.g., in [TR04] and [TRS06]. 
We will explain how to implement a .uid solver with LBM later on in Section 10.2. For real-time applications, 
theshallow water equations (SWE) are especially interest­ing.TheSWEareareducedformoftheNavier-Stokes 
equations,andcanbesolvedveryef­.ciently due to their two-dimensional nature. In simpli.ed versions, such 
two-dimensional methods have been used, e.g., for generating splashes [OH95] or coupled to terrains and 
rigid bodies [CdVLHM97]. In [LvdP02], an algorithm to apply a semi Lagrangian advec­tion to the SWE with 
an implicit time integration scheme was introduced. Moreover, the underlying algorithmwasextendedtoachieveavarietyofdifferenteffectsin 
real-time,such asbubble dynamics [TSS+07], or terrain erosion [MDH07]. Adifferent class of methods forwave 
generation are the spectral approaches, e.g., as describedin [HNC02],[Lov03], or in [Tes04]. These approaches 
are especially suitable for deep water waves with dispersive effects,butwillnotbe discussedin more detailhere.InChapter11,wewillexplainhowto 
solvetheSWE,and describethe handlingofdifferent boundaryconditions,to achieve, e.g., .uid .owing through 
a terrain. Chapter 10 Grid Based Fluid Simulation Nils Thuerey 10.1 Navier-Stokes Equations The motion 
of a .uid is typically computed by solving the Navier-Stokes (NS) equations 1, which describe the .uid 
in terms of a continuous velocity .eld u and a pressure p. In the following, we will give an overview 
of how a simpli.ed form of these equations is com­monly solved in the .eld of computer graphics. The 
next section will discuss an alternative solution approach that is particularly well suitedfor parallel 
implementations such as GPUs. For more detailed information, e.g., on the derviation of the equations, 
there are manygood text books on the topic.Wecan, e.g., recommend [KC04]. Note thatavery detailed descrip­tion 
of how to solve the NS equations directly can be found in the notes of the SIGGRAPH course on .uid simulation 
[RB07]. The NS equations are basically two conservation equations, that have to be ful.lled in order 
to give the motion of an incompressible .uid. The .rst one is very simple -it ensures the conservation 
of mass in the velocity .eld. Assuming a constant density of the .uid, this can be written as .v= 0. 
(10.1) Itsimply meansthatwhensomequantityisadvectedinthevelocity.eld v,asmokedensity, for instance, the 
overall mass of this quantity will not change (assuming that we make no error computingthisadvection).For 
.uidsitisof coursehighly importanttokeepthe mass constant -otherwise it will look like the .uid evaporates 
over time, or that drops disappear while .ying through the air. The second, and more complicated part, 
ofthe NS equations 1The origins of the well established Navier-Stokes equations reach back to Isaac Newton, 
who, around 1700, formulated the basic equations for the theoretical description of .uids. These were 
used by L. Euler half a century later to develop the basic equations for momentum conservation and pressure. 
Amongst others, Louis M. H. Navier continued to work on the .uid mechanic equations at the end of the 
18th century, as did GeorgG. Stokes several years later. He was one of the .rst to analytically solve 
.uid problems for viscous media. The NS equations could not be practically used up to the middle of the 
20th century, when the numerical methods, that are necessary to solve the resulting equations, were developed. 
57 ensures that the momentum of the .uid is preserved. It can be written as .u . + u· .u + .P= .Du + 
.g. (10.2) .t '7P" '7P " '7P" ' 7P" pressure viscosity forces advection This equationisa lot more complicated,but 
canbe understoodby identifying its individual terms. The .rst one, the advection terms, deals with just 
that -the advection of the velocity .eld of the .uid with itself. The pressure term involves the gradient 
of the pressure, so it essentially makes sure the .uid tries to .ow away from regions with high pressure. 
The viscosity term, where . is a parameter to account for the different thicknesses of .uids (honey, 
e.g., has a much higher viscosity than water), involves the second derivates of the velocities, and as 
such, performs a smearing out of the information in the velocity .eld. As we are usually interested in 
computing .uids with a very low viscosity, such as water and air, this term is completely left out (the 
NS equations without this term are then called the Euler equations). We can do this because the method 
to solve the equations normally introducesafair amountof error, whichis noticeable asa viscosity. Likewise, 
the more accurately the equations are solved, the more lively (and less viscous) the .uid will look. 
Luckily, the different parts of this equation can be computed separately, which signif­icantly simpli.es 
the algorithm. As mentioned before, the viscosity term can be left out. Adding the forces (this could, 
e.g., include gravity) to the velocities is typically also very easy. Computingtheadvectionis somewhat 
moredif.cult.Averyelegantand stablewayto solve this is to use a semi-Lagrangian scheme [Sta99]. This 
step will be described in more detail later for solving the shallow water equations. It can be computed 
very ef.ciently and is unconditionally stable. This is very useful, as the nonlinear terms of the advection 
are usually very problematic for stability when using other schemes such as .nite differences. The last, 
and most complex term to handle in the equations is the pressure term. Given somethevelocity.eldcomputedfromtheprevioussteps,wecomputethe 
pressurevaluesin away that ensures that (10.1) holds. It turns out thata Poisson equation with corresponding 
boundary conditions has to be solved for the pressure. This is a well known equation in mathematics: 
it is a partial differential equation that requires a system of linear equations to be solved. Typically, 
this is done with an iterative method, such as a conjugate gradient solver.Itis usuallybyfarthe mostexpensivepartofthe 
algorithm. Oncea correct pressure is computed, the velocity .eld can be adjusted to be divergence free, 
and describes the state of the .uid for the next time step. These steps: adding forces, advecting the 
velocities, and computing a pressure correc­tion are repeated for each time step. They are used to compute 
the motion of a single .uid phase, and can be used to, e.g., simulate swirling smoke. For effects such 
as liquids, it is in addition to the basic algorithm necessaryto compute the motion and behavior of the 
in­terface between the liquid and its surrounding (usually air). We will only cover two-phase .owsinthe 
formofthe shallowwater equationsinChapter11.Inthenext sectionwe will explain a different approach to 
compute the motion of a .uid that is based on a simpler algorithm. CHAPTER 10. GRID BASED FLUID SIMULATION 
 10.2 Lattice Boltzmann Methods This section will describe the basic LBM algorithm. The lattice Boltzmann 
formulation is alsoa grid-based method,but originated fromthe .eldof statisticalphysics 2. Althoughit 
comes from a descriptions on the molecular level, the method can still be used to compute .uids on a 
larger scale. For simulations with LBM, the simulation region is typically rep­resented by a Cartesian 
and equidistant grid of cells. Each cell only interacts with cells in its direct neighborhood. While 
conventional solvers directly discretize the NS equations, the LBM is essentially a .rst order explicit 
discretization of the Boltzmann equation in a discrete phase-space. It can also be shown, that the LBM 
approximates the NS equations with good accuracy. Adetailed overview of the LBM and derivations of the 
NS equations from LBM can be found in [WG00] and [Suc01], among others. The LBM has several advantages, 
that make it interesting for real-time applications. First, a timestep with LBM only requires a single 
pass over the computational grid. So it maps very well to parallel architectures such as GPUs. On the 
other hand, as the LBM per­forms an explicit timestep, the allowed velocities in the simulation are typically 
limited to ensure stability. Another interesting aspect is that each cell in LBM contains more informa­tion 
thanjustthevelocityand pressureor density. Thismakesit possibleto handle complex boundaries with high 
accuracy even on relatively coarse grids. The following section will describe the standard LBM with simple 
boundary conditions, and a turbulence model to ensure stability. 10.3 The Basic Algorithm The basic 
lattice Boltzmann (LB) algorithm consists of two steps, the stream-step, and the collide-step. These 
are usually combined with no-slip boundary conditions for the domain boundaries or obstacles. The simplicity 
of the algorithm is especially evident when implementing it, which, for the basic algorithm, requires 
roughly a single page of C-code. Using a LBM, the particle movement is restricted to a limited number 
of direc­tions. Here, a three-dimensional model with 19 velocities (commonly denoted as D3Q19) will be 
used. Alternatives are models with 15 or 27 velocities. However, the latter one has no clear advantages 
over the 19 velocity model, while the model with 15 velocities has a decreased stability. The D3Q19 model 
is thus usually preferable as it requires less 2The Boltzmann equation itself has been known since 1872. 
It is named after the Austrian scientist Ludwig Boltzmann,andispartofthe classicalstatisticalphysicsthat 
describethebehaviorofagasonthe microscopic scale. The LBM follows the approach of cellular automata to 
model even complex systems with a set of simple and local rules for each cell [Wol02]. As the LBM computes 
macroscopic behavior, such as the motion of a .uid, with equations describing microscopic scales, it 
operates on a so-called mesoscopic level in between those two extremes. Historically,theLBMevolvedfrom 
methodsforthe simulationofgasesthat computedthe motionofeach molecule in thegas purely with integer operations. 
In [HYP76], there was a .rst attempt to perform .uid simulations with this approach. It took ten years 
to discover that the isotropyof the lattice vectors is crucial for a correct approximation of the NS 
equations [FdH+87]. Motivated by this improvement, [MZ88] developed the .rst algorithm that was actually 
called LBM by performing simulations with averaged .oating point values insteadofsingle.uid molecules.Thethird 
important contributiontothebasicLBMwasthe simpli.ed collision operator with a single time relaxation 
parameter [BGK54, CCM92, QdL92]. memorythanthe27velocity model.Fortwo dimensionstheD2Q9 modelwithnineveloc­ities 
is the most common one. The D3Q19 model withits lattice velocity vectors e1..19 is shown in Fig. 10.1 
(together with the D2Q9 model). The velocity vectors take the fol­lowing values: e1 =(0,0,0)T , e2,3 
=(±1,0,0)T , e4,5 =(0,±1,0)T , e6,7 =(0,0,±1)T , e8..11 =(±1,±1,0)T , e12..15 =(0,±1,±1)T, and e16..19 
=(±1,0,±1)T . As all formulas for the LBM usually only depend on the so-called particle distribution 
functions(DFs), all of these two-dimensional and three-dimensional models can be used with the method 
presented here.To increase clarity, the following illustrations will all use the D2Q9 model. For each 
of the velocities, a .oating point number f1..19, representing a blob of .uid moving with this velocity, 
needs to be stored. As the LBM originates from statistical physics, this blob is thought of as a collection 
of molecules or particles. Thus, in the D3Q19 model there are particles not moving at all( f1), moving 
with speed1(f2..7)and moving with speed v 2(f8..19). In the following, a subscript of i will denote the 
value from the inverse direction of a value with subscript i. Thus, fi and fi are opposite DFs with in­verse 
velocity vectors ei = -ei. During the .rst part of the algorithm (the stream step), all DFs are advected 
withtheir respective velocities. This propagation results in a movement of the .oating point values to 
the neighboring cells, as shown in Fig. 10.2. Formulated in terms of DFs the stream step can be written 
as * fi (x,t+ .t)= fi(x+ .t ei ,t). (10.3) Here, .x denotes the size of a cell and .t the time step size. 
Both are normalized by the condition .t/.x = 1, which makes it possible to handle the advection by a 
simple copying * operation, as described above. These post-streaming DFs fi have to be distinguished 
from the standard DFs fi, and are never really stored in the grid. The stream step alone is clearly not 
enough to simulate the behavior of incompressible .uids, which is governed by the on­going collisions 
of the particles with each other. The second part of the LBM, the collide step, amountsfor thisbyweightingtheDFsofa 
cell withtheso called equilibrium distribu­tion functions, denotedby fieq. These depend solely on the 
density and velocity of the .uid. Here, the incompressible model from [HL97] is used, which alleviates 
compressibility ef­fects of the standard model by using a modi.ed equilibrium DF and velocity calculation. 
The density and velocity can be computed by summation of all the DFs for one cell . = . fi u = .eifi 
. (10.4) Figure 10.2: This .gure gives an overview of the stream and collide steps for a .uid cell. 
The standardmodel,in contrasttotheoneusedhere,requiresanormalizationofthevelocity with the .uid density.Fora 
single direction i,the equilibrium DF feq can be computed with i feq 39 i = wi . + 3ei· u- u2+(ei· u)2 
, where (10.5) 22wi = 1/3 fori= 1, wi = 1/18 for i= 2..7, wi = 1/36 for i= 8..19. The equilibrium DFs 
represent a stationary state of the .uid. However, this does not mean that the .uid is not moving. Rather, 
the values of the DFs would not change, if the whole .uid was at such an equilibrium state. For very 
viscous .ows, such an equilibrium state (equivalent to a Stokes .ow) can be globally reached. In this 
case, the DFs will converge to constantvalues. The collisionsof the moleculesina real.uid are approximatedby 
linearly relaxing the DFs of a cell towards their equilibrium state. Thus, each fi is weighted with the 
corresponding feq using: i * fi(x,t+ .t)= (1- .) fi (x,t+ .t)+ . fieq . (10.6) Here, . is the parameter 
that controls the viscosity of the .uid. Often, t = 1/. is also used to denote the lattice viscosity. 
The parameter . is in the range of (0..2], where values close to0resultinvery viscous .uids, whilevalues 
near2resultin more turbulent.ows. Usually these are also visually more interesting. However, for values 
close to 2, the method can become instable. In Section 10.5, a method to stabilize the computations with 
a turbulence model will be explained. This alleviates the instabilities mentioned above. The parameter 
. is given by the kinematic viscosity of a .uid. The values computed with Eq. (10.6) are stored as DFs 
for time t+ .t. As each cell needs the DFs of the adjacent cells from the previous time step, two arrays 
for the DFs of the current and the last time step are usually used. The easiest way to implement the 
no-slip boundary conditions is the link bounce back rule that results in a placement of the boundary 
halfway between .uid and obstacle cells. If the neighboring cell at (x+ .t ei) is an obstacle cell during 
streaming, the DF from the inverse direction of the current cell is used. Thus, Eq. (10.3) changes to 
* fi (x,t+ .t)= fi (x,t). (10.7) Fig. 10.3 illustrates those basic steps for a cell next to an obstacle 
cell. 10.4 Implementation A2D implementationof the algorithm described sofar might consistofa .ag .eld 
g to distinguish .uid(g= FLUID)and obstacle cells(g= OBS), and two arrays of single­precision .oating 
pointvariables, f and f/, with9valuesfor each cellinthegrid.Foragrid with nx and ny cells along the x-and 
y-direction, respectively, a typical initialization step CHAPTER 10. GRID BASED FLUID SIMULATION might 
be to set the boundaries of the domain to be obstacles, and initialize a resting .uid volume: Init-lbm 
(1) for j= 0to ny do (2) fori= 0to nx do  (3) if(i== 0||i== nx - 1|| j== 0|| j== ny- 1) do (4) g[i, 
j]= OBS (5) else (6) g[i, j]= FLUID (5) endif (7) forl= 0to 9do  (7) f[i, j,l]= f/[i, j,l]= wl; (7) 
endfor  (8) endfor (9) endfor  The code above and below requires some globals arrays: the weights 
of the equilibrium DFs w, and the latticevectors.For the D2Q9 model, these are Lbm D2Q9 Globals (1) wl[9]= 
{1/3, 1/18,1/18,1/18,1/18, 1/18,1/18,1/18,1/18} (2) ex[9]= {0, 1,-1,0,0, 1,-1,1,-1} (3) ey[9]= {0, 
0,0,1,-1, 1,1,-1,-1}  During a loop over all cells in the current grid, each cell collects the neighboring 
DFs according to Eq. (10.3) or Eq. (10.7), for adjacent .uid and obstacle cells, respectively. Stream 
(1) for j= 0to ny do (2) fori= 0to nx do (3) forl= 0to 8do  (4) linv = invert(l) (5) if g[i+ ex[linv], 
j+ ey[linv]] == OBS do (6) f/[i, j,l]= f[i, j,linv] (7) else (8) f/[i, j,l]= f[i+ ex[linv], j+ ey[linv],l] 
(9) endif (10) endfor (11) endfor (12) endfor  Here, ex and ey denote the x-and y-components of a 
lattice vector ei, while invert(l) is a function to return the index of the lattice vector opposite to 
ei. The pseudo code above performs the normal streaming operation for all .uid neighbors, and uses a 
bounce-back boundary conditions for obstacle cells. Thenextstepistocomputethe collisionsofthe streamedDFs.Forthis,thedensityand 
velocity are computed and used to calculate the equilibrium DFs. These are weighted with the streamed 
DFs according to the viscosity parameter .. Collide (1) for j= 0to ny do (2) fori= 0to nx do  (3) if 
g[i, j] == OBS do continue; endif (4) (.,ux,uy)= getDensityAndVelocity(i, j,k)  (5) forl= 0to 8do (6) 
a = ex[l]ux + ey[l]uy 22 (7) feq = w(l)(. + 3(u+ u)+ 3a+ 29a2) 2xy (8) f/[i, j,l]=(1- .) f/[i, j,l]+ 
. feq (9) endfor (10) endfor (11) endfor  Here, getDensityAndVelocity(i, j) is a function to compute 
the density and velocity for a single cell. As described above, these values can be computed by summation 
over all the DFs of a cell: getDensityVelocity(i, j) (1) . = ux = uy = uz = 0 (2) forl= 0to 8do (3) .+= 
f/[i, j,l] (4) ux+= ex[l] · f/[i, j,l] (5) uy+= ey[l] · f/[i, j,l]  (6) endfor (9) return(.,ux,uy) 
 Once the Stream and Collide steps are completed, the f/ grid contains the updated state ofthe.uidforthenexttimestep.Typically,thetwogridsarenowswapped,and 
subsequent time steps alternate in streaming and colliding the DFs from one grid array to the other. 
Performing these two very simple steps already computes a solution of the Navier-Stokes equations.NotethatusingEq.(10.7)theDFsfor 
obstaclecellsarenever touched.Fora3D implementation, it is only necessary to loop over a third component, 
and include all 19 DFs in the inner loops over l (instead of only 9). The operations themselves are the 
same in 2D and 3D. In contrast to a standard .nite-difference NS solver, the implementation is much sim­pler, 
but also requires more memory. A typical NS solver as described in Section 10.1 usually requires at least7 
.oating pointvalues for each grid point (pressure, threeveloc­ity components, plus three temporaryvariables),but 
for some casesit might need higher CHAPTER 10. GRID BASED FLUID SIMULATION resolutionstoresolve obstacleswiththesame 
accuracy.Usingamore sophisticatedLBim­plementation with grid compression [PKW+03], the memory requirements 
can be reduced to almost half of the usual requirements. Furthermore, using an adaptive time step size 
is common practice for a NS solver, while the size of the time step in the LBM is, by default, .xed to1(however, 
[TPR+06] explains how to achieve the effect of a changing time step fora LBM solver). As the maximum 
latticevelocity may notexceed1/3, in order for the LBM to remain stable, it might still need several 
time steps to advance to the same time a NS solver would reach in a single step. However, each of these 
time steps usually requires a signi.cantly smaller amount of work, as the LBM can be computed very ef.ciently 
on modern CPUs. Moreover, it does not require additional global computations such as the pressure correction 
step. 10.5 Stability In order to simulate turbulent .ows with the LBM, the basic algorithm needs to 
be ex­tended, as its stability is limited once the relaxation parameter t approaches1/2(which is equivalent 
to . being close to 2). Here, the Smagorinskysub-grid model, as used in, e.g., [WZF+03,LWK03], will be 
applied. Its primary use is to stabilize the simulation, instead of relying on its ability to accurately 
model subgrid scale vortices in the simulation. The model requires slightly more computations per cell 
than the standard LBM,but signi.cantly increases stability whichmakesitavery useful addition. Note thatin 
recent years,avariety of collision operators with increased accuracyand stability have been developed 
(e.g., multi relaxation time models and cascaded lattice boltzmann),but we have have found that the Smagorinskymodel 
is easy to use and gives visually plausible results. The sub-grid turbulence model applies the calculation 
of the local stress tensor as de­scribed in [Sma63] to the LBM. The computation of this tensor is relatively 
easy for the LBM, as each cell already contains information about the derivatives of thehydrodynamic 
variables in each DF. The magnitude of the stress tensor is then used in each cell to mod­ify the relaxation 
time according to the so called eddy viscosity. For the calculation of the modi.ed relaxation time, the 
Smagorinsky constant C is used. For the simulations in the following, Cwillbe setto0.03.ValuesinthisrangearecommonlyusedforLB 
simulations, and were shown to yield good modeling of the sub-grid vortices [YGL05]. The turbulence model 
is integrated into the basic algorithm that was described in Section 10.3 by adding the calculationofthe 
modi.ed relaxationtimeafterthe streamingstep,andusingthisvalue in the normal collision step. The modi.ed 
relaxation time ts is calculatedby performingthe steps that are described in the following. First, the 
non-equilibrium stress tensor .a,ß is calculated for each cell with 19 fi- feq .a,ß = .eia eiß i , (10.8) 
i=1 using the notation from [HSCD96]. Thus, a and ß each run over the three spatial dimen­sions, while 
iis the index of the respective velocity vector and DF for the D3Q19 model. The viscosity correction 
uses an intermediate value Swhich is computed as p S= 1 .2+ 18C2.a,ß .a,ß - . . (10.9) 6C2 Now the modi.ed 
relaxation time is given by ts = 3(. +C2S)+ 1 . (10.10) 2 From Eq. (10.9) it can be seen that S will 
always have a positive value thus the local viscosity will be increased depending on the size of the 
stress tensor calculated from the non-equilibrium parts of the distribution functions of the cell to 
be relaxed. This effectively removes instabilities due to small values of t.We have found that with values 
of Caround 0.03 the LBM simulations are removed of all stability problems, if it is ensured that the 
velocities lie in the allowed range(|v| < 1/3). Chapter 11  ShallowWater Equations Nils Thuerey, Peter 
Hess 11.1 Introduction The shallow water equations (SWE) are a simpli.ed version of the more general 
Navier-Stokes (NS) equations, which are commonly used to describe the motion of .uids. The SWE reduce 
the problem of a three-dimensional .uid motion to a two-dimensional description with a height-.eld representation. 
From now on, we will use the following notation (it is also illustrated in Fig. 11.1): hdenotes the height 
of the .uid above zero-level. gis the height of the ground below the .uid (above zero-level). . denotes 
the height of the .uid above ground, . = h- g. v the velocity of the .uid in the horizontal plane. Abasic 
version of the SWE can be written as .. .t + (..)v= -.. · v (11.1) .v .t + (.v)v= an.h, (11.2) where 
an denotes a vertical acceleration of the .uid, e.g., due to gravity. This formulation canbe derived 
fromtheNS equationsby,most importantly,assumingahydrostatic pressure along the direction of gravity. 
Interested readers can .nd a detailed derivation of these euqations in Section A. In the following sections 
we will .rst explain how to solve these equations with a basic solver, and then extend this solver with 
more advanced techniques to handle open bound­aries, or free surfaces. 67 Figure 11.1:A.uidvolumeis represented 
asa height.eld elevationin normal direction n. The .uid velocity v has two components in horizontal directions. 
 11.2 Abasic Solver Abasic solver of the SWE has to compute the change of the height and velocity values 
of the water surface over time. According to Eq. (11.2) and Eq. (11.1) the equations for the water height 
. and the velocity v =(v1,v2) can be written as: ../.t+(..)v = -.. · v .v1/.t+(.v1)v = an.h .v2/.t+(.v2)v 
= an.h. (11.3) In this form, the two distinct parts of the equations can be identi.ed: the left side 
accounts for the advection within the velocity .eld v, while the right side computes an additional acceleration 
term. We will here use anexplicittime integration scheme, as this makes the solution of the SWE sign.cantly 
more simple. Alternatively, implicit schemes, as described in [LvdP02] couldbe used. These,however,requireasystemof 
linear equationstobe solved for the update, and, with simpler methods such as implicit Euler, introduce 
a signi.cant amount of damping. To compute a solution for these equations, we .rst discretize the domain 
withn1 cells in x-direction, and n2 cells in y-direction. For simplicity, we assume in the following 
that the cells have a square form with side length .x. The gravity force is assumed to act along the 
z-axis,andthesizeofasingletimestepisgivenby .t.To represent the three unknowns with this grid we use 
a staggered grid. This means that the pressure is located in the center of a cell, while the velocity 
components are located at the center of each edge, as shown in Fig. 11.1. The staggered grid is commonly 
used for .uid solvers, and prevents instabilities that would result from a discretization on a co-located 
grid. An update step of the shallow water solver consisits of the following parts: .rst all three .elds 
are advected with the current velocity .eld. Afterwards, the acceleration terms are computed for the 
height and velocity .elds. From now on,values willa prime, e.g., h/ will denote an updatedvalue. The 
following pseudo-code illustrates a single step of the simulation loop of a simple shallow water solver: 
Shallow-water-step(.,v,g) CHAPTER 11. SHALLOW WATER EQUATIONS (1) ./ = Advect(.,v) (2) v/= Advect(v1,v) 
 (3) v/= Advect(v2,v) (4) . = Update-height(./,v) (5) h= ./ + g (6) v= Update-velocities(h,v1/,v/2) 
 1 2 Fortheadvection,wecanusethe semi-Lagrangianmethod[Sta99]tocomputeastable solution. This algorithm 
computes the advection on a grid by essentially performing a backward trace of an imaginary particle 
at each grid location. Given a scalar .eld s to be advected, we have to compute a new value for a grid 
cell at position x. This is done by tracing a particle at this position backward in time, where it had 
the position xt-1 = x- .tv(x). We now update the value of s with the value at xt-1, so the new value 
is given by s(x)/ = s(xt-1). Note that although x is either the center or edge of a cell in our case, 
x/ can be located anywhere in the grid, and thus usually requires an interpolation to compute the value 
of s there. For this interpolation we can simply use a bi-linear one. This has the advantage of ensuring 
the stability of the advection step. It guarantees that the interpolated value is bounded by its source 
values from the grid, while any form of higher order interpolation could result in larger or smaller 
values, and thus cause stability problems. The advection step can be formulated as Advect(s,v) (1) for 
j= 1to n2- 1 (2) fori= 1to n1- 1 (3) x=(i· .x, j· .x) (4) x/ = x- .t· v(x) (5) s/(i, j)= interpolate(s,x/) 
 (6) endfor (7) endfor (8) return(s/)  Note that, due to the staggered grid, the lookup of v(x) above 
already might require an averaging of two neighboring velocity components to compute the velocity at 
the desired position. The divergence of the velocity .eld for the .uid height update can be easily computed 
with .nite differences on the staggered grid. So, according to Eq. (11.3), the height update is given 
by Update-height(.,g, v) (1) for j= 1to n2- 1 (2) fori= 1to n1- 1  (v1(i+1, j)-v1(i, j)) + (v2(i, 
j+1)-v2(i, j)) (3) ./(i, j)= -.(i, j) ·.x.t .x (5) endfor (6) endfor  (7) return(./) Similarly, the 
acceleration term for the velocity update is given by the gradient of the overall .uid height. Note that 
in this case, the total height above the zero-level is used instead of the .uid height above the ground 
level. This is necessary, to, e.g., induce an acceleration of the .uid on an inclined plane, even when 
the .uid height itself is constant (all derivatives of . would be zero in this case). The parameter a 
for the velocity update below is the gravity force. Update-velocities(h,v1,v2,a) (1) for j= 1to n2- 1 
 (2) fori= 1to n1- 1  (3) v1/(i, j)= a(h(i-1, j)-h(i, j)).t .x (3) v/2(i, j)= a(h(i, j-1)-h(i, j)).t 
.x (5) endfor (6) endfor (7) return(v/1,v/2)  This concludesasinglestepofa basic shallowwatersolver. 
Notethatthestepssofar do not update the values at the boundary of the simulation domain, as we cannot 
compute anyderivatives there. Instead, special boundary conditions are required at the border, and can 
be used to achieve a variety of effects. These will be the topic of the next section. 11.3 BoundaryConditions 
In the following, we will describe different types of boundary conditions: re.ecting and absorsorbing 
boundaries,aswellasaformoffreesurface boundary conditions.The former can be used to model a wall that 
re.ects incoming waves. The second type can be used to give the effect of an open water surface, as waves 
will simply leave the computational domain. Free surface boundary conditions can be used once the .uid 
should, e.g., .ow through a landscape. Although the boundary conditions will be described to handle the 
outermost region of the computational domain, they can likewise be used to, e.g., create a wall in the 
middle of the domain. We will not consider periodic boundary conditions here. They are commonly usedin 
engineering applications,butgive the visually unnaturaleffect that a wave leaving the domain at the right 
side re-enters it at the left. The results shown in this section where created by Peter Hess [Hes07]. 
Re.ecting Boundaries In the following, we will, without loss of generality, consider the boundary conditions 
for cells at the left boundary. Re.ecting boundary conditions are achievedby setting theveloc­itiesatthe 
boundaryto zero (afterall, there shouldbeno .ux throughthewall).In addition, we mirror the heightof the 
.uidin the outermost layer.We thus set: Figure 11.2: Example of a wave spreading in a basic shallow 
water simulation. h(0, j)/ = h(1, j) v1(1, j)/ = 0 v2(0, j)/ = 0. (11.4) Note that we do not modify the 
y-component v2 of the velocity .eld. The .uid is thus allowedtomovetangentiallytoawall. Theoretically,wecouldalso 
enforcedifferentbehav­iorsforthe tangentialvelocities,butin practicethisdoesnotmakea noticeabledifference. 
Also note, that we only set v1(1, j), as v1(0, j) is usually never accessed during an compu­tation step. 
 Absorbing Boundaries Surprisingly, it is more dif.cult to achieve absorbing boundaries than re.ecting 
ones. The problem of boundaries simulating an in.nite domain is already known for a long time (see, e.g.,[Dur01]for 
details).Acommonlyusedmethodtoachievethis,isthe perfectly matched layer introduced by [Ber94], requires 
an additional layer of computations around the actual domain. This is why we chose to use the Higdon 
boundary conditions [Hig94] which are less accuratebut can be more ef.ciently computed than PML. Below 
is thepth order Higdon boundary condition, where the velocities cj are chosen to span the range of incoming 
wave Figure 11.3:Acomparison between re.ecting boundary conditions (upperrowof pictures), and absorbing 
ones (lower row). velocities. p . j=1 . .t + cj . .x h= 0 (11.5)  This boundary condition canbe problematicfor 
higher order approximations,butasthe wave propagation speed in shallow water is known to be c = v g., 
this allows us to use to use the1st order boundary condition .. + ch= 0. (11.6) .t .x This boundary condition 
actually requires temporal derivatives, so we assume the cur­rent height.eld is given by ht, while the 
heights of the previous step are stored in ht-1. Hence, we can set the boundary values to: .xh(0, j)t-1+ 
.tc(1, j)th(1, j)t h(0, j)/ = .x+ .tc(1, j)t h(1, j)t - h(0, j)t v1(1, j)/ = v1(1, j)t-1- a .t .x v2(1, 
j)/ = 0 (11.7) Note that the update of v1 is essentially the same acceleration term on the left hand 
side of Eq. (A.16). To further suppress anyresidual re.ections at the boundary, we can apply a slight 
damping of the height .eld in a layer around the boundary. Fig. 11.3 shows the effect of these boundary 
conditions compared to re.ecting ones. For boundaries where .uid should .ow into or outof the domain, 
we can reuse thetwo types above. In.ow boundary conditions can be achieved by speci.ying re.ecting ones, 
with an additional .xed normal velocity. For out.ow boundary conditions, absorbing ones with free normal 
velocities are more suitable. Free Surfaces Often, shallow water simulations assume a completely .uid 
domain, since this makes solv­ingtheSWEquite straightforward.Once applicationslikeariver,or.uid.llinganarbitrary 
terrainareneeded,thisisnotsuf.cientanymore.Such applicationsrequirea distinctionbe­tween areas .lled 
with .uid, and empty or dry areas. An example can be seen in Fig. 11.4. In the following we will consider 
this as a problem similar to free surface handling for full .uid simulations. Shallow water simulations 
naturally have an interface, and thus a free surface,inthe simulationplanebetween.uidbelowandasecondgasphaseabovethe.uid. 
In addition, we will now prescribe boundary conditions with such a free surface within the simulation 
plane itself. From the mathematical point of view, a distinction between .uid and dry would not be necessary 
since the SWE still work if . is zero. Distinguishing .uid anddrycells,however,bringssomeadvantages.Foremost, 
computationaltimecanbesaved if large parts of the domain are dry. Therefore, we introduce cell .ags f(i, 
j), that deter­mine the type of each cell, and update them once per time step after updating the heights. 
This allows us to quickly identify wet and dry cells. Besides the computational advantage, controlling 
the transition between wet and dry cells also gives us some control over the spreadingvelocity.Without 
free surface tracking, the .uid boundarywouldexpandexactly one cell per time step, regardless of cell 
size and time step length. The height of this ad­vancingboundarywouldbeverysmall,butthisbehaviorisusuallynot 
desired.In addition, we will compute a .ll value r for each cell, as this allows us to track a smoothly 
moving surface line between the .uid and empty cells. Figure 11.4: Ashallow water simulation with free 
surface boundary conditions .lls a ter­rain. To determine the cell s .ag f we have to compute the minimal 
and maximal ground level hmin and hmax as well as the maximal .uid depth .max on the cell s edges. h(i, 
j)+ min h(p) hmin(i, j)= p. N (i, j) (11.8) 2 h(i, j)+ max h(p) hmax (i, j)= + eH p. N (i, j) (11.9) 
2 .(i, j)+ max .(p) .max (i, j)= p. N (i, j) (11.10) 2 where N (i, j) is the set of the four direct neighbors 
of cell (i, j). Note that we add a small value eh to hmax to prevent hmin to be equal to hmax in .at 
areas. With these three values we can nowdetermine f as well as the .ll ratio r which indicates the cell 
s.ll level in dependence of the local ground topology hmin(i, j) and hmax(i, j). r can be used to compute 
an isoline which de.nes the border of the rendered .uid surface for rendering the water surface. The 
following pseudo code shows how f and r are calculated: Compute-.ags(i, j) (1) ifh(i, j) = hmin(i, j)and.max(i, 
j) < e.max (2) f(i, j)= DRY (3) r(i, j)= 0  (4) else ifh(i, j) > hmax (5) f(i, j)= FLUID (6) r(i, 
j)= 1  (7) else (8) f(i, j)= FLUID (9) r(i, j)= h(i, j) - hmin(i, j) / hmax(i, j) - hmin(i, j)  (10) 
endif Acellismarkedasdryifitssurfaceheightisnot higherthanthelowestgroundvalue in the cell and if there 
is no neighbor cell from which .uid could .ow into this cell. The .ll ratio is then set to zero. e. can 
be seen as a threshold which allows in.ow from a neighbor cell only if this neighbor has a large enough 
amount of .uid. This effectively limits the spread of thin layers of .uid. So this could be seen as a 
simple way of simulating surface tension. A cell is completely .lled if its surface height is higher 
than the ground at any position in the cell. The .ll ratio is then set to one. The cell is also marked 
as .uid if the surface height is only in parts higher than the ground level. In this case however the 
.ll ratio is the ratio between minimal ground level, .uid surface height and maximal ground level. Note, 
that with this de.nition cells may have negative depth values . even if they are marked as .uid. There 
are cases were the cell center itself is dry, so the value of . is negative at this point, while the 
whole cell still contains .uid at the edges of a cell. CHAPTER 11. SHALLOW WATER EQUATIONS  Chapter 
12 Particles  Chapter 13 Uni.ed Solver PartI Appendix 78  AppendixA  Derivation of the ShallowWater 
Equations Nils Thuerey, Roland Angst Inthischapterwegothroughadetailedderivationofamoregeneralformofthe 
shallow water equations (SWE) from the underlying Navier-Stokes (NS) equations. It is based on [Ang07]. 
A.1 Integral Form The SWE are a specialized and simpli.ed version of the Navier-Stokes equations. The 
Navier-Stokes equations describe the dynamics of a .uid much like elasticity theory de­scribes the dynamics 
of deformable solids. The Navier-Stokes equations relate the dy­namics of the density .eld, the temperature 
.eld and the velocity .eld of a .uid to each other. The full Navier-Stokes equations in three dimensions 
are a system of partial differ­ential equations with .ve equations and unknowns (density, velocity and 
temperature). The Navier-Stokes equations as well as the SWE are derived from three balance equations 
which prescribe three conservation laws, namely the conservation of mass, momentum and energy. In addition, 
we will make use of the material speci.c constitutive relations for the deriva­tion. A common simpli.cation 
is to assume that the .uid .ow is incompressible which means that the .uid density is spatially constant. 
Likewise, we assume that the viscosity coef.cient, and the thermal conductivity are uniform. This assumption 
has important implications: The density is no longer an unknown of the equation system. Instead, the 
pressure becomes an unknown.  The pressure and the velocity .eld are decoupled from the energy conservation. 
This means that we can solve for the pressure and the velocity ignoring the temperature .eld.We thus 
assume an isotermal .uid.  These .rst assumptions reduce the Navier-Stokes equations to four equations 
in four un­knowns: three velocity components and the pressure. 79 Balance Equations The balance equations 
describe fundamental physical laws and are valid for any system described with continuum mechanics. Hence, 
theyare the same for the Navier-Stokes equa­tions and for the SWE. Since we assume that the .uid is incompressible 
and isothermal, the conservation of mass and the conservation of momentum are the only balance equations 
required for the derivation. The continuity equation or conservation of mass for a domain O with the 
boundary . O with density . reads like d .dV+ .v· ndA= 0. (A.1) dt O3 .O3 It means the temporal change 
of the .uid mass in the domain, which is computed by the integral of the density over the .uid volume 
(the .rst term), is given by the .ux along its boundary (the second term). Similarly, the conservation 
of momentum looks like d T .vdV+ .vv· ndA s · ndA= fdV (A.2) dt O3 .O3 .O3 O3 where v is the .uid velocity 
and fcaptures sources and sinks of the .uid. This last term is also known as body force and is a force 
density, in [N/m3], and the actual force acting on a given volume V is thus given by the volume integral 
over this force density. s denotes the stress tensor which is the sum of the pressure pand the viscous 
stress tensor T. Note that viscous forces are caused by spatial velocity differences in the .uid. We 
will assume that there are only small velocity variations, so that the viscosity coef.cient is small. 
In addition we will ignore the viscous stress tensor. This assumption justi.edbyobserving that water-like 
.uids have a very small viscosity and that the numerical integration introduces a certain amount of damping, 
which basically has the same effect as a viscosity larger than zero. Typically, this numerical viscosity 
is larger than that of water so that we rely on the solver to computea .uid motion witha viscosity as 
small as possible. Essentially, we solve for an inviscid .uid. Projection The derivationof the SWE now 
proceedsby making an important assumption: the pressure is assumedtobehydrostatic, i.e., p= .an.. (A.3) 
where an isthe accelerationinvertical directionand .. correspondstothevertical distance from the bottom 
of the .uid to its surface. Amongst others, the vertical acceleration takes care of the gravitational 
acceleration g= -9.81sm 2. Thishydrostatic pressure equation can bederivedbyassumingthattheverticalvelocityismuch 
smallerthanthe horizontalvelocity suchthatthe momentum conservationinvertical directionis dominatedbythe 
pressureterm and hencetheadvectionoftheverticalvelocity vn can safely be ignored. This assumption is 
justi.edby observing thatin areas where the .uidis shallow, theverticalvelocityis indeed orders of magnitude 
smaller than the horizontal velocity components. The implications of thehydrostatic pressure assumption 
are: APPENDIX A. DERIVATION OF THE SHALLOW WATER EQUATIONS The momentum equationforthevertical componentis 
static, i.e.,theverticalvelocity is temporally constant and equal to zero.  Thevelocityis basicallyatwodimensionaltangentialvector 
.eldwhichis orthogonal to the vertical (or normal) direction.  The pressure pacts as the sole vertical 
variable.  The .uid surface can be represented by a height.eld.  The assumptionofahydrostatic pressure 
thus allowsusto further reducethe numberof unknownsfromfourtothree -the.uidcannowbe describedbyvelocitiesinthe 
horizontal domain, and a height of the .uid. The tangentialvelocitycanbethoughtofasaveragedoverthe.uiddepth.The 
momen­tum conservation equation therefore reads ft d vt dV = . dV O3 fn dt O30 T vt vt + . · ndA+ pI· 
ndA (A.4) . O300 . O3 where the subscript t indicates horizontal (or tangential) components and the subscript 
n denotes the vertical (or normal) component. The following notation is introduced and will be used throughout 
all the chapters of this chapter: H: Height of .uid between zero-level and ground.  h: Height of .uid 
above zero-level.  .:Total .uid height, i.e. . = H+ h.  To achieve a height.eld representation of the 
.uid we have to get rid of the vertical di­mensioninthese equationsbyaprojectionoftheintegralstothetwodimensional 
horizontal domain. Assume the equations are computed over a .uid column with a square base area O2 t 
. Consider .rst the surface integral in the continuity equation .v· ndA= .v· ndA+ .v· ndA .O3 O2O2 t 
n where the surface integral was split up into two parts over the surfaces O2 vertical to the n horizontal 
projection plane O2 t and the remaining bottom and top surface. There is no mass .ux through the bottom 
because the bottom surface is impermeable and thus this surface integral vanishes. The slope of the .uid 
surface is assumed to be small. This implies that v· n is small, too and hence, the integral over the 
top of the .uid column is negligible. The mass .ux through the vertical sides O2 is reduced to a line 
integral by projecting the .uid n depth onto the boundaries .O2 t of the horizontal projection surface 
h .v· ndA= .v· nd.ds= .(h+ H)v· nds= ..v· nds. (A.5) O2 .O2-H .O2 .O2 n tt t Notethatthe normalsoftheverticalsurfaces 
O2 of the .uid column are equal to the normals n of the boundary curveof the horizontal projection surface. 
The projection of the momentum advection term is analog to the projection of the mass .ux, i.e., the 
integrals over the non­vertical areas vanish due to the same reasons. Consider now the pressure force 
acting on the .uid column pI· ndA= pI· ndA+ pI· ndA .O3 O2O2 t n where the surface integral was again 
split up into a horizontal and a vertical component. The surface integral over the top of the .uid surface 
vanishes because the pressure at this interface is negligible compared to the pressure inside the .uid. 
The surface integral over the bottom leads to a source term of the momentum, provided the bottom is non-.at. 
This pressure force makes the .uid .ow downwards. This force is computed by integrating the hydrostatic 
pressure p= .g.. over the bottom area pI· ndA= .an(-.)ndA O2O2 tt = .an(-H- h)(-.H)dA= .an(H+ h).HdA. 
(A.6) O2O2 tt Note that the normal n in this equation corresponds to the normal given by the negative 
gradient of the bottom height.eld -.H rather than the normal of the horizontal projection plane. Furthermore, 
the normalization of the gradient and the cosine for the area foreshort­ening cancel each other out. 
The projection of the pressure force due to the vertical sides of the .uid column trans­forms the surface 
integral over these sides again into a boundary integral over the curve .O2 t , which bounds the horizontal 
area. Thus,by integratingover the .uid depth, the .uxes through the vertical sides get concentrated at 
the boundary curve .O2 t h pI· ndA= .an(. - h)nd.ds O2 .O2-H n t = - 1.an(-H- h)2nds= - 1.an.2nds. (A.7) 
.O22.Ot22 t Integrating the vertical component in the volume integrals of Eq. (A.1) and Eq. (A.4) and 
inserting Eq. (A.5), Eq. (A.6), and Eq. (A.7) the SWE can be restated in the integral form d ..dA+ ..v· 
nds= 0 (A.8) dt O2t.O2t T d ..vdA+ ..vv· nds- 1.an.2nds dt Ot2.Ot2.O2t 2 = - .an..HdA+ .ft dA. (A.9) 
O2O2 tt From now on, the subscript t will be omitted in the equations. In the literature, these equations 
are often given in the so called .ux form d qdA+ F(q) · nds= .dA (A.10) dt O2 .O2 O2 APPENDIX A. DERIVATION 
OF THE SHALLOW WATER EQUATIONS where the equations were divided by the constant density and T q= ..u 
.v .. .u .v F(q)= f(q) g(q)= ..u2- 21an.2 .uv . .uv .v2- 21an.2 0 . = . -an..H+ .. ft The two components 
of the velocity are denoted here as v =(u,v)T. This notation clearly shows the non-linearity and the 
conservative nature of the equations due to the .ux across the boundary.  A.2 Differential Form Wenowturn 
to the derivation of the differential form of the SWE, although the integral form admits more general 
solutions than the differential form. Solutions to the integral form may exhibit discontinuities (which 
must still satisfy certain conditions) like for example shocks. Such solutions are also known as weak 
solutions. The differential form on the other hand is written as a partial differential equation (PDE) 
which only admits continuous solutions because the derivation requires a continuity assumption. Solutions 
of the differential form are known as strong (also called pointwise) solutions. But note that the true 
solution of the SWE with certain boundary conditions might indeed be discontinuous and in such cases, 
a solution method derived from the differential formfails to converge to the correct solution. However,in 
computer graphics, accuracyisnotthemost importantfactorandthus,astrong solution which is similar to the 
true weak solution might be adequate enough. As mentioned before, we will assume in this derivation that 
the solution to the SWE is continuous and differentiable. The divergence theorem can then be applied 
to the boundary integral of the integral form A.8 and A.9 and the time derivative can be moved inside 
the integral.We endup with qtdA+ . · F(q)dA= .dA. O2 O2 O2 This equation mustbevalidforanytwo dimensional 
domain O2 and therefore the following PDE must hold . f(q) .g(q) qt + . · F(q)= qt + qx + qy = .. (A.11) 
.q .q The chain-rule was used to expand the divergence into a product involving the Jacobian of the .ux 
functions fand g. Note that sucha PDEis calledhyperbolicif anylinear combina­ . f tion of the two Jacobians 
and .g has real eigenvalues and can be diagonalized. This is .q . q indeed the case for the SWE. Written 
out in detail, Eq. (A.11) looks like .t = -. · (.v) (A.12) ..v 1 . + . · (.vvT - Ian.2)= -an..H+ ft . 
(A.13) . dt 2. This latter equation can be simpli.ed even further. By pushing the differential operator 
inside the bracketed terms we get .vt + v.t + v. · (.v)+ .v· .u- an... - 1.2.an = -an..H+ . ft . 2. The 
second and third term on the left hand side correspond to the continuity equation times thevelocityandthusisequalto 
zero.Dividingby . and observing that H- . = -h.nally gives 11 vt + v· .v- an.h- ..an = ft (A.14) 2. The 
previous equation clearly highlights the advectional part of the momentum conserva­tion. The differential 
equation derived from the continuity equation can also be formulated usinga material derivativeby observing 
that .·(.v)= v·.. +..·v. The SWE then take the following form D. Dt = -.. · v (A.15) Dv Dt = an.h+ 1 
2..an + 1 . ft . (A.16) Note that when the balance equations were projected from three dimension to 
two dimen­sions, the vertical force component has been dropped. This term can now be reinserted as a 
vertical acceleration component, for example in addition to a constant acceleration due to gravity g= 
-9.81sm 2: fn an = g+ . Thanks to these external force terms ft and fn, the SWE can more easily interact 
with other objects. Note that the SWE are usually derived without these terms.  Bibliography [AN05] 
Alexis Angelidis and Fabrice Neyret. Simulation of smoke based on vor­tex .lament primitives. SCA 05: 
Proceedings of the 2005 ACM SIG­GRAPH/Eurographics symposium on Computer animation, pages 87 96, 2005. 
[Ang07] Roland Angst. Control Algorithms for Interactively Animated Fluid Char­acters. Master Thesis, 
Computer Graphics Laboratory, ETH Zurich, 2007. [ANSN06] Alexis Angelidis, Fabrice Neyret, Karan Singh, 
and Derek Nowrouzezahrai. Acontrollable, fast and stable basis for vortexbased smokesimulation.SCA 06: 
Proceedings of the 2006 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 25 32, 2006. 
[BBB07] Christopher Batty, Florence Bertails, and Robert Bridson. Afast variational framework for accurate 
solid-.uid coupling. In SIGGRAPH 07: ACM SIG-GRAPH 2007 papers, page 100, New York, NY, USA, 2007. ACM 
Press. [Ber94] Jean-Pierre Berenger. Aperfectly matched layer for the absorption of elec­tromagnetic 
waves. J. Comput. Phys., 114(2):185 200, 1994. [BGK54] P. L. Bhatnagar, E. P. Gross, and M. Krook. Amodel 
for collision processes in gases. Phys. Rev., 94:511 525, 1954. [BT07] M. Becker and M. Teschner. Weakly 
Compressible SPH for Free Surface Flows. ACM SIGGRAPH /Eurographics Symposium on Computer Anima­tion, 
2007. [CCM92] Hudong Chen, Shiyi Chen, and William H. Matthaeus. Recovery of the Navier-Stokes equations 
using a lattice-gas Boltzmann method. Phys. Rev. A, 45(8):R5339 R5342, 1992. [CdVLHM97] Jim X Chen, 
Niels da Vitoria Lobo, Charles E. Hughes, and J. Michael Moshell. Real-time .uid simulationinadynamic 
virtualenvironment. IEEE Comput. Graph. Appl., 1997. [CFL+07] Nuttapong Chentanez, Bryan E. Feldman, 
Franc¸ois Labelle, James F. O Brien, and Jonathan R. Shewchuk. Liquid simulation on lattice-based tetrahedral 
meshes. In SCA 07: Proceedings of the 2007 ACM SIG­GRAPH/Eurographics symposium on Computer animation, 
pages 219 228, Aire-la-Ville, Switzerland, Switzerland, 2007. Eurographics Association. 85 86 Real Time 
Physics Class Notes [CLT07] Keenan Crane, Ignacio Llamas, and Sarah Tariq. Real-time simulation and rendering 
of 3d .uids. In GPU Gems 3. Addison-WesleyProfessional, 2007. [CMT04] Mark Carlson, Peter John Mucha, 
and GregTurk. Rigid .uid: Animating the interplay between rigid bodies and .uid. ACM Trans. Graph., 23(3), 
2004. [Dur01] D.R. Durran. Open boundary conditions: Fact and .ction. In IUTAM Sym­posium on Advances 
in Mathematical Modelling of Atmosphere and Ocean Dynamics, pages 1 18. Kluwer Academic Publishers, 2001. 
[EMF02] D. Enright, S. Marschner, and R. Fedkiw. Animation and Rendering of Complex Water Surfaces. ACM 
Trans. Graph., 21(3):736 744, 2002. [FdH+87] Uriel Frisch, Dominique d Humi`eres, Brosl Hasslacher, Pierre 
Lallemand, Yves Pomeau, and Jean-Pierre Rivert. Lattice Gas Hydrodynamics in Two and Three Dimensions. 
Complex Systems, 1:649 707, 1987. [FF01] Nick Foster and Ronald Fedkiw. Practical animation of liquids. 
In Proc. of ACM SIGGRPAH, pages 23 30, 2001. [FM96] N. Foster and D. Metaxas. Realistic Animation of 
Liquids. Graphical Mod­els and Image Processing, 58, 1996. [FOA03] Bryan E. Feldman, James F. O Brien, 
and Okan Arikan. Animating sus­pended particle explosions. In Proc. of ACM SIGGRAPH, pages 708 715, Aug 
2003. [FOK05] Bryan E. Feldman, James F. O Brien, and Bryan M. Klingner. Animating gases with hybrid 
meshes. ACM Trans. Graph., 24(3):904 909, 2005. [GHDS03] Eitan Grinspun, Anil Hirani, Mathieu Desbrun, 
and Peter Schroder. Discrete shells. In Proceedings of the ACM SIGGRAPH Symposium on Computer Animation, 
2003. [GHF+07] Rony Goldenthal, David Harmon, Raanan Fattal, Michel Bercovier, and Eitan Grinspun. Ef.cient 
simulation of inextensible cloth. ACM Trans. Graph., 26(3):49, 2007. [GM97] S. F. Gibson and B. Mitrich. 
Asurveyof deformable models in computer graphics. Technical Report TR-97-19, MERL, 1997. [Hah88] James 
K. Hahn. Realistic animation of rigid bodies. In SIGGRAPH 88: Proceedings of the 15th annual conference 
on Computer graphics and in­teractive techniques, pages 299 308, New York, NY, USA, 1988. ACM. [Hes07] 
Peter Hess. Extended Boundary Conditions for Shallow Water Simulations. Master Thesis, Computer Graphics 
Laboratory, ETH Zurich, 2007. [Hig94] Robert L. Higdon. Radiation boundary conditions for dispersive 
waves. SIAM J. Numer. Anal., 31(1):64 100, 1994. BIBLIOGRAPHY [HL97] X. He and L.-S. Luo. Lattice Boltzmann 
model for the incompressible Navier-Stokes equations. J. Stat. Phys., 88:927 944, 1997. [HNC02] Damien 
Hinsinger,FabriceNeyret, and Marie-Paule Cani. Interactive Ani­mationof OceanWaves. Proc.of the 2002ACM 
SIGGRAPH/Eurographics Symposium on Computer animation, July 2002. [HSCD96] Shuling Hou, James D. Sterling, 
Shiyi Chen, and Gary Doolen. ALattice Boltzmann Subgrid Model for High Reynolds Number Flow. Fields Insti­tute 
Communications, 6:151 166, 1996. [HTK+04] B. Heidelberger,M.Teschner,R.Keiser,M. Mueller,andM. Gross. 
Consis­tent penetration depth estimation for deformable collision response. pages 339 346, 2004. [HYP76] 
J.Hardy,O.DePazzisY.,and Pomeau. Molecular dynamicsofa classical latticegas:Transport properties and 
time correlation functions. Physical ReviewA, 13:1949 1960, 1976. [Jak01] T. Jakobsen. Advanced character 
physics the fysix engine. www.gamasutra.com, 2001. [JP99] Doug L. James and Dinesh K.Pai. Artdefo, accurate 
real time deformable objects. In Computer Graphics Proceedings, Annual Conference Series, pages 65 72.ACM 
SIGGRAPH 99, August 1999. [KC04] PijushKundu and Ira Cohen. Fluid Mechanics. Elsevier Academic Press, 
2004. [KFCO06] BryanM. Klingner, BryanE. Feldman, Nuttapong Chentanez, and JamesF. O Brien. Fluid animation 
with dynamic meshes. ACM Trans. Graph., 25(3):820 825, 2006. [KM90] M. KassandG. Miller. Rapid, Stable 
FluidDynamicsfor Computer Graph­ics. ACMTrans. Graph., 24(4):49 55, 1990. [Lov03] J¨ornLoviscach. ComplexWaterEffectsat 
Interactive Frame Rates. Journal of WSCG, 11:298 305, 2003. [LvdP02] AnitaT. Layton and MichielvandePanne.Anumericallyef.cient 
and sta­ble algorithm for animating water waves. TheVisual Computer, 18(1):41 53, 2002. [LWK03] Wei Li, 
XiaomingWei, and Arie E. Kaufman. Implementing lattice Boltz­mann computation on graphics hardware. The 
Visual Computer, 19(7­8):444 456, 2003. M¨ .uid simulation for interactive applications. Proc. of the 
2003ACM Sig­ graph/Eurographics Symposium on Computer Animation, 2003. [MCG03] Matthias uller, David 
Charypar, and Markus Gross. Particle-based 88 Real Time Physics Class Notes [MDH07] Xing Mei, Philippe 
Decaudin, and Baogang Hu. Fast hydraulic erosion simulation and visualization on gpu. In Paci.c Graphics, 
2007. [MG04] Matthias M ¨uller and Markus Gross. Interactive virtual materials. In GI 04: Proceedings 
of Graphics Interface 2004, pages 239 246, School of Com­puter Science, University of Waterloo, Waterloo, 
Ontario, Canada, 2004. Canadian Human-Computer Communications Society. [MHR06] M. M¨uller, B. Heidelberger 
M. Hennix, and J. Ratcliff. Position based dy­namics. Proceedings of Virtual Reality Interactions and 
Physical Simula­tions, pages 71 80, 2006. [Mon92] J. Monaghan. Smoothed particle hydrodynamics. Annu. 
Rev. Astron. Phys., 30:543 574, 1992. [MSKG05] Matthias M ¨uller, Barbara Solenthaler, Richard Keiser, 
and Markus Gross. Particle-based .uid-.uid interaction. Proc. of the 2005 ACM Sig­graph/Eurographics 
Symposium on Computer Animation, 2005. [MTPS04] Antoine McNamara, Adrien Treuille, Zoran Popovic, and 
Jos Stam. Fluid control using the adjoint method. ACM Trans. Graph., 23(3):449 456, 2004. [MZ88] Guy 
R. McNamara and Gianluigi Zanetti. Use of the Boltzmann Equation to Simulate Lattice-Gas Automata. Phys. 
Rev. Lett., 61(20):2332 2335, 1988. [NFJ02] Duc Quang Nguyen, Ronald Fedkiw, and Henrik Wann Jensen. 
Physically based modeling and animation of .re. In SIGGRAPH 02: Proceedings of the 29th annual conference 
on Computer graphics and interactive tech­niques, pages 721 728, New York, NY, USA, 2002. ACM Press. 
[NMK+05] A. Nealen, M. M¨uller, R. Keiser, E. Boxerman, and M. Carlson. Physically based deformable models 
in computer graphics. Eurographics 2005 state of the art report, 2005. [OH95] J. F. O Brien and J. K. 
Hodgins. Dynamic simulation of splashing .uids. In CA 95: Proceedings of the Computer Animation, page 
198, 1995. [PKW+03] T. Pohl, M. Kowarschik, J. Wilke, K. Iglberger, and U. R¨ude. Optimization and Pro.ling 
of the Cache Performance of Parallel Lattice Boltzmann Codes in 2D and 3D. Technical Report 03 8, Department 
for System-Simulation, Germany, 2003. [QdL92] Y. H. Qian, D. d Humi `eres, and P. Lallemand. Lattice 
BGK Models for Navier-Stokes Equation. Europhys. Lett., 17(6):479 484, 1992. [RB07] Matthias Mueller-Fischer 
Robert Bridson. 2007 Course, 2007. Fluid Simulation, SIGGRAPH BIBLIOGRAPHY [RNGF03] Nick Rasmussen, 
Duc Quang Nguyen, Willi Geiger, and Ronald Fed­kiw. Smoke simulation for large scale phenomena. ACMTrans. 
Graph., 22(3):703 707, 2003. [Sma63] J. Smagorinsky. General circulation experiments with the primitive 
equa­tions. Mon.Wea. Rev., 91:99 164, 1963. [Sta99] Jos Stam. Stable Fluids. Proc.ofACM SIGGRAPH, pages 
121 128, 1999. [Suc01] S. Succi. The Lattice Boltzmann Equation for Fluid Dynamics and Beyond. Oxford 
University Press, 2001. [SY05a] Lin Shi andYizhouYu. Controllable smokeanimation with guiding objects. 
ACMTrans. Graph., 24(1), 2005. [SY05b] Lin Shi andYizhouYu.Taming liquids for rapidly changing targets. 
Proc. of the 2005ACMSiggraph/Eurographics Symposiumon Computer Animation, 2005. [Tes04] JerryTessendorf. 
Simulating Ocean Surfaces. SIGGRAPH 2004 Course Notes 31, 2004. [THM+03] M.Teschner, B. Heidelberger, 
M. Mueller, D. Pomeranets, and M. Gross. Optimized spatial hashing for collision detection of deformable 
objects. pages 47 54, 2003. [TKPR06] Nils Th¨urey, Richard Keiser, MarkPauly, and Ulrich R¨ude. Detail-Preserving 
Fluid Control. Proc.ofthe2006ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 2006. [TKZ+04] 
M. Teschner, S. Kimmerle, G. Zachmann, B. Heidelberger, Laks Raghu­pathi, A. Fuhrmann, Marie-Paule Cani, 
Franc¸ois Faure, N. Magnetat-Thalmann, and W. Strasser. Collision detection for deformable objects. In 
Eurographics State-of-the-Art Report (EG-STAR), pages 119 139. Eu­rographics Association, 2004. [TPBF87] 
D.Terzopoulos, J. Platt, A. Barr, and K. Fleischer. Elastically deformable models. In Computer Graphics 
Proceedings, Annual Conference Series, pages 205 214.ACM SIGGRAPH 87, July 1987. [TPR+06] Nils Th¨ude, 
Markus Oechsner, and Carolin urey, Thomas Pohl, UlrichR ¨K¨Optimization and Stabilization of LBM Free 
Surface Flow Sim­ orner. ulations using Adaptive Parameterization. Computers and Fluids, 35 [8­9]:934 
939, September-November 2006. [TR04] Nils Th ¨ude. Free Surface Lattice-Boltzmann .uid simu­ ureyand 
UlrichR ¨lations with and without level sets. Proc. ofVision, Modelling, andVisual­ization VMV, pages 
199 208, 2004. 90 Real Time Physics Class Notes [TRS06] Nils Th¨urey, Ulrich R ¨ude, and Marc Stamminger. 
Animation of Open Wa­ter Phenomena with coupled Shallow Water and Free Surface Simulations. Proc. of 
the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 2006. [TSS+07] Nils Th¨urey, Filip 
Sadlo, Simon Schirm, Matthias M¨uller-Fischer, and Markus Gross. Real-time simulations of bubbles and 
foam within a shal­low water framework. In SCA 07: Proceedings of the 2007 ACM SIG­GRAPH/Eurographics 
symposium on Computer animation, pages 191 198, Aire-la-Ville, Switzerland, Switzerland, 2007. Eurographics 
Association. [WG00] Dieter A. Wolf-Gladrow. Lattice-Gas Cellular Automata and Lattice Boltz­mann Models. 
Springer, 2000. [WLMK04] Xiaoming Wei, Wei Li, Klaus M uller, and Arie E. Kaufman. The Lattice-Boltzmann 
Method for Simulating Gaseous Phenomena. IEEE Transactions on Visualization and Computer Graphics, 10(2):164 
176, 2004. [Wol02] Stephen Wolfram. ANew Kind of Science. Wolfram Media, 2002. [WWXP06] Changbo Wang, 
Zhangye Wang, Tian Xia, and Qunsheng Peng. Real-time snowing simulation. The Visual Computer, pages 315 
323, May 2006. [WZF+03] Xiaoming Wei, Ye Zhao, Zhe Fan, Wei Li, Suzanne Yoakum-Stover, and Arie Kaufman. 
Natural phenomena: Blowing in the wind. Proc. of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer 
animation, pages 75 85, July 2003. [YGL05] H. Yu, S.S. Girimaji, and Li-Shi Luo. Lattice Boltzmann simulations 
of decaying homogeneous isotropic turbulence. Phys. Rev. E, 71, 2005. [YUM86] Larry Yaeger, Craig Upson, 
and Robert Myers. Combining physical and visual simulation and creation of the planet jupiter for the 
.lm 2010. In SIGGRAPH 86: Proceedings of the 13th annual conference on Computer graphics and interactive 
techniques, pages 85 93. ACM Press, 1986. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401246</section_id>
		<sort_key>1140</sort_key>
		<section_seq_no>26</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Realistic hair simulation: animation and rendering]]></section_title>
		<section_page_from>26</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098880</person_id>
				<author_profile_id><![CDATA[81100421152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sunil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadap]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098881</person_id>
				<author_profile_id><![CDATA[81407594555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marie-Paule]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098882</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098883</person_id>
				<author_profile_id><![CDATA[81365596229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tae-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098884</person_id>
				<author_profile_id><![CDATA[81100393298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Florence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bertails]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098885</person_id>
				<author_profile_id><![CDATA[81100238316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marschner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098886</person_id>
				<author_profile_id><![CDATA[81450592780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kelly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098887</person_id>
				<author_profile_id><![CDATA[81320490688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaci&#263;-Alesi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401247</article_id>
		<sort_key>1150</sort_key>
		<display_label>Article No.</display_label>
		<pages>154</pages>
		<display_no>89</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Realistic hair simulation]]></title>
		<subtitle><![CDATA[animation and rendering]]></subtitle>
		<page_from>1</page_from>
		<page_to>154</page_to>
		<doi_number>10.1145/1401132.1401247</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401247</url>
		<abstract>
			<par><![CDATA[<p>The last five years have seen a profusion of innovative solutions to one of the most challenging tasks in character synthesis: hair simulation. This class covers both recent and novel research ideas in hair animation and rendering, and presents time tested industrial practices that resulted in spectacular imagery.</p> <p>The course is aimed at an intermediate level and addresses the special-effects developers and technical directors who are looking for innovation as well as proven methodologies in hair simulation. The audience will get a good grasp of the state of the art in hair simulation and will have plenty of working solutions that they can readily implement in their production pipelines. The course will also be a bootcamp for aspiring computer graphics researchers interested in physically based modeling in computer graphics.</p> <p>The class addresses the special-effects developers and technical directors who are looking for innovation as well as proven methodologies in hair simulation. The audience will get a good grasp of the state of the art in hair simulation and will have plenty of working solutions that they can readily implement in their production pipelines. The class will also be a boot-camp for aspiring computer graphics researchers interested in physically based modeling.</p> <p>The class covers two crucial tasks in hair simulation: animation and rendering. For hair animation, we first discuss recent successful models for simulating the dynamics of individual hair strands, before presenting viable solutions for complex hair-hair and hair-body interactions. For rendering, we address issues related to shading models, multiple scattering, and volumetric shadows. We finally demonstrate how hair simulation techniques are nowadays developed and applied in the feature films industry to produce outstanding visual effects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098888</person_id>
				<author_profile_id><![CDATA[81100393298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Florence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bertails]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Rh&#244;ne-Alpes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098889</person_id>
				<author_profile_id><![CDATA[81100421152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sunil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadap]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adobe Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098890</person_id>
				<author_profile_id><![CDATA[81407594555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marie-Paule]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INP Grenoble]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098891</person_id>
				<author_profile_id><![CDATA[81452602436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098892</person_id>
				<author_profile_id><![CDATA[81365596229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tae-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm&Hues Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098893</person_id>
				<author_profile_id><![CDATA[81100238316]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marschner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098894</person_id>
				<author_profile_id><![CDATA[81450592780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kelly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098895</person_id>
				<author_profile_id><![CDATA[81320490688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ka&#269;i&#263;-Alesi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light+Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{ALS98} C. Adler, J. Lock, and B. Stone. Rainbow scattering by a cylinder with a nearly elliptical cross section. <i>Applied Optics</i>, 37(9):1540--1550, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{AP07} B. Audoly and Y. Pomeau. <i>Elasticity and Geometry: from hair curls to the nonlinear response of shells</i>. Oxford University Press, &#192; para&#238;tre en 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134021</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{AUK92a} K. Anjyo, Y. Usami, and T. Kurihara. A simple method for extracting the natural beauty of hair. In <i>Proceedings of the 19th annual conference on Computer graphics and interactive techniques (SIGGRAPH)</i>. ACM SIGGRAPH, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134021</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{AUK92b} K. Anjyo, Y. Usami, and T. Kurihara. A simple method for extracting the natural beauty of hair. In <i>Computer Graphics Proceedings (Proceedings of the ACM SIGGRAPH'92 conference)</i>, pages 111--120, August 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142012</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{BAC&#60;sup&#62;+&#60;/sup&#62;06} F. Bertails, B. Audoly, M.-P. Cani, B. Querleux, F. Leroy, and J.-L. L&#233;v&#234;que. Super-helices for predicting the dynamics of natural hair. In <i>ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH'06 conference)</i>, pages 1180--1187, August 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192246</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{Ban94} David C. Banks. Illumination in diverse codimensions. <i>Proc. of ACM SIGGRAPH</i>, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{BAQ&#60;sup&#62;+&#60;/sup&#62;05} F. Bertails, B. Audoly, B. Querleux, F. Leroy, J.-L. L&#233;v&#234;que, and M.-P. Cani. Predicting natural hair shapes by solving the statics of flexible rods. In J. Dingliana and F. Ganovelli, editors, <i>Eurographics'05 (short papers)</i>. Eurographics, August 2005. Eurographics'05 (short papers).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{Bar92} David Baraff. <i>Dynamic simulation of non-penetrating rigid bodies</i>. PhD thesis, Department of Computer Science, Cornell University, March 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237226</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Bar96} David Baraff. Linear-time dynamics using lagrange multipliers. <i>Proceedings of SIGGRAPH 96</i>, pages 137--146, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{BCN03} Y. Bando, B-Y. Chen, and T. Nishita. Animating hair with loosely connected particles. <i>Computer Graphics Forum</i>, 22(3):411--418, 2003. Proceedings of Eurographics'03.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{BCP96} K. E. Brenan, S. L. Campbell, and L. R. Petzold. <i>Numerical Solution of Initial-Value Problems in Differential-Algebraic Equations</i>. Classics in Applied Mathematics. SIAM, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{Ber06} F. Bertails. <i>Simulation de chevelures naturelles</i>. PhD thesis, Institut National Polytechnique de Grenoble, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846305</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{BKCN03a} F. Bertails, T-Y. Kim, M-P. Cani, and U. Neumann. Adaptive wisp tree - a multiresolution control structure for simulating dynamic clustering in hair motion. In <i>ACM SIGGRAPH - EG Symposium on Computer Animation</i>, pages 207--213, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846305</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{BKCN03b} F. Bertails, T-Y. Kim, M-P. Cani, and U. Neumann. Adaptive wisp tree: a multiresolution control structure for simulating dynamic clustering in hair motion. In <i>2003 ACM SIGGRAPH / Eurographics Symposium on Computer Animation</i>, aug 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089521</ref_obj_id>
				<ref_obj_pid>1089508</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{BMC05} F. Bertails, C. M&#233;nier, and M-P. Cani. A practical self-shadowing algorithm for interactive hair animation. In <i>Proc. Graphics Interface</i>, pages 71--78, May 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846281</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{BMF03} R. Bridson, S. Marino, and R. Fedkiw. Simulation of clothing with folds and wrinkles. In <i>Eurographics/SIGGRAPH Symposium on Computer Animation</i>, San Diego, California, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826497</ref_obj_id>
				<ref_obj_pid>826028</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{Bru99} A. Bruderlin. A method to generate wet and broken-up animal fur. In <i>Computer Graphics and Applications</i>, 1999. Proceedings. Seventh Pacific Conference, pages 242--249, October 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{BS91} H. Bustard and R. Smith. Investigation into the scattering of light by human hair. <i>Applied Optics</i>, 24(30):3485--3491, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{BW92} D. Baraff and A. Witkin. Dynamic simulation of non-penetrating flexible bodies. In <i>Computer Graphics Proceedings (Proceedings of the ACM SIGGRAPH'92 conference)</i>, pages 303--308, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{BW98a} David Baraff and Andrew Witkin. Large steps in cloth simulation. <i>Proc. of ACM SIGGRAPH</i>, pages 43--54, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{BW98b} David Baraff and Andrew P. Witkin. Large steps in cloth simulation. In <i>Proceedings of SIGGRAPH 98</i>, Computer Graphics Proceedings, Annual Conference Series, pages 43--54, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073389</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{CCK05a} B. Choe, M. Choi, and H-S. Ko. Simulating complex hair with robust collision handling. In <i>ACM SIGGRAPH - EG Symposium on Computer Animation</i>, pages 153--160, August 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073389</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{CCK05b} Byoungwon Choe, Min Gyu Choi, and Hyeong-Seok Ko. Simulating complex hair with robust collision handling. In <i>2005 ACM SIGGRAPH / Eurographics Symposium on Computer Animation</i>, pages 153--160, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545273</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{CJY02a} J. Chang, J. Jin, and Y. Yu. A practical model for hair mutual interactions. In <i>ACM SIGGRAPH - EG Symposium on Computer Animation</i>, pages 73--80, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545273</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{CJY02b} Johnny Chang, Jingyi Jin, and Yizhou Yu. A practical model for mutual hair inteactions. In <i>Proceedings of Symposium on Computer Animation</i>. ACM SIGGRAPH, San Antonio, USA, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1042390</ref_obj_id>
				<ref_obj_pid>1042201</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{CK05} B. Choe and H-S. Ko. A statiscal wisp model and pseudophysical approaches for interactive hairstyle generation. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 11(2):153--160, March 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{CPS92} R. W. Cottle, J. S. Pang, and R. E. Stone. <i>The linear complementarity problem</i>. Academic Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{CSDI99} L. Chen, S. Saeyor, H. Dohi, and M. Ishizuka. A system of 3d hairstyle synthesis based on the wisp model. <i>The Visual Computer</i>, 15(4):159--170, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344795</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{DKY&#60;sup&#62;+&#60;/sup&#62;00} Y. Dobashi, K. Kaneda, H. Yamashita, T. Okita, and T. Nishita. A simple, efficient method for realistic animation of clouds. In <i>Computer Graphics Proceedings (Proceedings of the ACM SIGGRAPH'00 conference)</i>, pages 19--28. ACM Press/Addison-Wesley Publishing Co., 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{DMTKT93} A. Daldegan, N. Magnenat-Thalmann, T. Kurihara, and D. Thalmann. An integrated system for modeling, animating and rendering hair. <i>Computer Graphics Forum</i>, 12(3):211--221, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{Dur04} D. Durville. Modelling of contact-friction interactions in entangled fibrous materials. In <i>Procs of the Sixth World Congress on Computational Mechanics (WCCM VI)</i>, September 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>576516</ref_obj_id>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{Fea87} Roy Featherstone. <i>Robot Dynamics Algorithms</i>. Kluwer Academic Publishers, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882358</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{GBF03} E. Guendelman, R. Bridson, and R. Fedkiw. Nonconvex rigid bodies with stacking. <i>ACM Transactions on Graphics (SIGGRAPH Proceedings)</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258807</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[{Gol97} D. Goldman. Fake fur rendering. In <i>Proceedings of ACM SIGGRAPH'97</i>, Computer Graphics Proceedings, Annual Conference Series, pages 127--134, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826598</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[{GZ02} Y. Guang and H. Zhiyong. A method of human short hair modeling and real time animation. In <i>Proceedings of Pacific Graphics'02</i>, pages 435--438, September 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[{Had03} Sunil Hadap. <i>Hair Simulation</i>. PhD thesis, MIRALab, CUI, University of Geneva, January 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>295582</ref_obj_id>
				<ref_obj_pid>295565</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[{HKS98} T. Hou, I. Klapper, and H. Si. Removing the stiffness of curvature in computing 3-d filaments. <i>J. Comput. Phys.</i>, 143(2):628--664, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[{HMT01a} S. Hadap and N. Magnenat-Thalmann. Modeling dynamic hair as a continuum. <i>Computer Graphics Forum</i>, 20(3):329--338, 2001. Proceedings of Eurographics'01.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[{HMT01b} Sunil Hadap and Nadia Magnenat-Thalmann. Modeling dynamic hair as a continuum. <i>Computer Graphics Forum</i>, 20(3):329--338, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[{HS98} W. Heidrich and H.-P. Seidel. Efficient rendering of anisotropic surfaces using computer graphics hardware. <i>Proc. of Image and Multi-dimensional Digital Signal Processing Workshop (IMDSP)</i>, pages 315--318, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383840</ref_obj_id>
				<ref_obj_pid>2383815</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[{JLD99} H. Jensen, J. Legakis, and J. Dorsey. Rendering of wet material. <i>Rendering Techniques</i>, pages 273--282, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846278</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[{KANB03} Z. Ka&#269;i&#263;-Alesi&#263;, M. Nordenstam, and D. Bullock. A practical dynamics system. In <i>SCA '03: Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, pages 7--16. Eurographics Association, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[{KAT93} T. Kurihara, K. Anjyo, and D. Thalmann. Hair animation with collision detection. In <i>Proceedings of Computer Animation'93</i>, pages 128--138. Springer, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[{Ken04} Erleben Kenny. <i>Stable, Robust, and Versatile Multibody Dynamics Animation</i>. PhD thesis, Department of Computer Science, University of Copenhagen, November 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[{KH00} C. Koh and Z. Huang. Real-time animation of human hair modeled in strips. In <i>EG workshop on Computer Animation and Simulation (EG CAS'00)</i>, pages 101--112, September 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776362</ref_obj_id>
				<ref_obj_pid>776350</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[{KH01} C. Koh and Z. Huang. A simple physics model to animate human hair modeled in 2D strips in real time. In <i>EG workshop on Computer Animation and Simulation (EG CAS'01)</i>, pages 127--138, September 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1009600</ref_obj_id>
				<ref_obj_pid>1009379</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[{KHS04} M. Koster, J. Haber, and H-P. Seidel. Real-time rendering of human hair using programmable graphics hardware. In <i>Computer Graphics International (CGI'04)</i>, pages 248--256, June 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>959918</ref_obj_id>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[{Kim02} Tae-Yong Kim. <i>Modeling, Rendering, and Animating Human Hair</i>. PhD thesis, University of Southern California, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[{KK89} J. Kajiya and T. Kay. Rendering fur with three dimensional textures. In <i>Computer Graphics Proceedings (Proceedings of the ACM SIGGRAPH'89 conference)</i>, Computer Graphics Proceedings, Annual Conference Series, pages 271--280, New York, NY, USA, 1989. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791572</ref_obj_id>
				<ref_obj_pid>791217</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[{KN99} W. Kong and M. Nakajima. Visible volume buffer for efficient hair expression and shadow generation. In <i>Computer Animation</i>, pages 58--65. IEEE, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872905</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[{KN00} T.-Y. Kim and U. Neumann. A thin shell volume for modeling human hair. In <i>Computer Animation 2000</i>, IEEE Computer Society, pages 121--128, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732282</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[{KN01} T-Y. Kim and U. Neumann. Opacity shadow maps. In <i>Rendering Techniques 2001</i>, Springer, pages 177--182, July 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566627</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[{KN02} T-Y. Kim and U. Neumann. Interactive multiresolution hair modeling and editing. <i>ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH'02 conference)</i>, 21(3):620--629, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[{Kok04} Evangelos Kokkevis. Practical physics for articulated characters. In <i>Proceedings of Game Developers Conference 2004</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[{LFHK88} B. Lindelof, B. Forslind, MA. Hedblad, and U. Kaveus. Human hair form. morphology revealed by light and scanning electron microscopy and computer aided three-dimensional reconstruction. <i>Arch. Dermatol.</i>, 124(9):1359--1363, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[{LGLM00} E. Larsen, S. Gottschalk, M. Lin, and D. Manocha. Distance queries with rectangular swept sphere volumes. <i>Proc. of IEEE Int. Conference on Robotics and Automation</i>, pages 24--48, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508127</ref_obj_id>
				<ref_obj_pid>508126</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[{LK01a} D-W. Lee and H-S. Ko. Natural hairstyle modeling and animation. <i>Graphical Models</i>, 63(2):67--85, March 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508127</ref_obj_id>
				<ref_obj_pid>508126</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[{LK01b} Doo-Won Lee and Hyeong-Seok Ko. Natural hairstyle modeling and animation. <i>Graphical Models</i>, 63(2):67--85, March 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[{LTT91} A. M. LeBlanc, R. Turner, and D. Thalmann. Rendering hair using pixel blending and shadow buffers. <i>The Journal of Visualization and Computer Animation</i>, 2(3):92--97, -- 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[{LV00} T. Lokovic and E. Veach. Deep shadow maps. In <i>Computer Graphics Proceedings (Proceedings of the ACM SIGGRAPH'00 conference)</i>, pages 385--392. ACM Press/Addison-Wesley Publishing Co., 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[{Mar74} D. Marcuse. Light scattering from elliptical fibers. <i>Applied Optics</i>, 13:1903--1905, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[{Mir96} Brian Mirtich. <i>Impulse-based Dynamic Simulation of Rigid Body Systems</i>. PhD thesis, University of California, Berkeley, December 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237265</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[{Mit96} D. Mitchell. Consequences of stratified sampling in graphics. In <i>Computer Graphics Proceedings (Proceedings of the ACM SIGGRAPH'96 conference)</i>, pages 277--280, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[{MJC&#60;sup&#62;+&#60;/sup&#62;03a} S. Marschner, H. Jensen, M. Cammarano, S. Worley, and P. Hanrahan. Light scattering from human hair fibers. <i>ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH'03 conference)</i>, 22(3):281--290, July 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882345</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[{MJC&#60;sup&#62;+&#60;/sup&#62;03b} S. Marschner, H. W. Jensen, M. Cammarano, S. Worley, and P. Hanrahan. Light scattering from human hair fibers. <i>ACM Transactions on Graphics</i>, 22(3):780--791, July 2003. Proceedings of ACM SIGGRAPH 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383558</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[{MKBR04} T. Mertens, J. Kautz, P. Bekaert, and F. Van Reeth. A self-shadow algorithm for dynamic hair using density clustering. In <i>Proceedings of Eurographics Symposium on Rendering</i>, pages 173--178, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[{MTM98} C. Mount, D. Thiessen, and P. Marston. Scattering observations for tilted transparent fibers. <i>Applied Optics</i>, 37(9):1534--1539, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[{NdP98} I. Neulander and M. Van de Panne. Rendering generalized cylinders with paintstrokes. In <i>Graphics Interface</i>, pages 233--242, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614392</ref_obj_id>
				<ref_obj_pid>614269</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[{Ney98} F. Neyret. Modeling animating and rendering complex scenes using volumetric textures. <i>IEEE Transaction on Visualization and Computer Graphics</i>, 4(1):55--70, Jan-Mar 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776359</ref_obj_id>
				<ref_obj_pid>776350</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[{NR01} O. Nocent and Y. Remion. Continuous deformation energy for dynamic material splines subject to finite displacements. In <i>EG workshop on Computer Animation and Simulation (EG CAS'01)</i>, pages 88--97, September 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[{OBKA03} H. Ono, S. Benza, and Z. Ka&#269;i&#263;-Alesi&#263;. Bringing digital crash dummies to life for 'the hulk'. In <i>SIGGRAPH Sketches and Applications</i>, San Diego, California, 2003. ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>113575</ref_obj_id>
				<ref_obj_pid>113571</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[{Ove91} C. Van Overveld. An iterative approach to dynamic simulation of 3D rigid-body motions for real-time interactive computer animation. <i>The Visual Computer</i>, 7:29--38, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[{Pai02a} D. Pai. Strands: Interactive simulation of thin solids using cosserat models. <i>Computer Graphics Forum</i>, 21(3):347--352, 2002. Proceedings of Eurographics'02.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[{Pai02b} D. K. Pai. Strands: Interactive simulation of thin solids using cosserat models. <i>Computer Graphics Forum</i>, 21(3):347--352, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776363</ref_obj_id>
				<ref_obj_pid>776350</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[{PCP01a} E. Plante, M-P. Cani, and P. Poulin. A layered wisp model for simulating interactions inside long hair. In <i>EG workshop on Computer Animation and Simulation (EG CAS'01)</i>, Computer Science, pages 139--148. Springer, September 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776363</ref_obj_id>
				<ref_obj_pid>776350</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[{PCP01b} Eric Plante, Marie-Paule Cani, and Pierre Poulin. A layered wisp model for simulating interactions inside long hair. In <i>Proceedings of Eurographics Workshop, Computer Animation and Simulation</i>. EUROGRAPHICS, Manchester, UK, September 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>639423</ref_obj_id>
				<ref_obj_pid>639420</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[{PCP02} E. Plante, M-P. Cani, and P. Poulin. Capturing the complexity of hair motion. <i>Graphical Models (Academic press)</i>, 64(1):40--58, january 2002. submitted Nov. 2001, accepted, June 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[{PK96} F. C. Park and I. G. Kang. Cubic interpolation on the rotation group using cayley parameters. In <i>Proceedings of the ASME 24th Biennial Mechanisms Conference</i>, Irvine, CA, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[{PS03} Jong-Shi Pang and David E. Stewart. Differential variational inequalities. Technical report, http://www.cis.upenn.edu/davinci/publications, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>230636</ref_obj_id>
				<ref_obj_pid>230627</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[{PT96} J. S. Pang and J. C. Trinkle. Complementarity formulations and existence of solutions of dynamic multi-rigid-body contact problems with coulomb friction. <i>Mathematical Programming</i>, 73:199--226, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614331</ref_obj_id>
				<ref_obj_pid>614261</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[{QT96} H. Qin and D. Terzopoulos. D-nurbs: A physics-based framework for geometric design. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 2(1):85--96, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1761412</ref_obj_id>
				<ref_obj_pid>1761408</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[{RCFC03} L. Raghupathi, V. Cantin, F. Faure, and M.-P. Cani. Real-time simulation of self-collisions for virtual intestinal surgery. In Nicholas Ayache and Herv&#233; Delingette, editors, <i>Proceedings of the International Symposium on Surgery Simulation and Soft Tissue Modeling</i>, number 2673 in Lecture Notes in Computer Science, pages 15--26. Springer-Verlag, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[{RCT91a} R. Rosenblum, W. Carlson, and E. Tripp. Simulating the structure and dynamics of human hair: Modeling, rendering and animation. <i>The Journal of Visualization and Computer Animation</i>, 2(4):141--148, October-December 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{RCT91b} R. Rosenblum, W. Carlson, and E. Tripp. Simulating the structure and dynamics of human hair: Modeling, rendering, and animation. <i>The Journal of Visualization and Computer Animation</i>, 2(4):141--148, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073294</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{RGL05a} S. Redon, N. Galoppo, and M. Lin. Adaptive dynamics of articulated bodies. <i>ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH'05 conference)</i>, 24(3):936--945, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073294</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{RGL05b} Stephane Redon, Nico Galoppo, and Ming C. Lin. Adaptive dynamics of articulated bodies. <i>ACM Transactions on Graphics</i>, 24(3):936--945, aug 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{RJFdJ04} Jos&#233; Ignacio Rodr&#237;guez, Jos&#233; Manuel Jim&#233;nez, Francisco Javier Funes, and Javier Garc&#237;a de Jal&#243;n. Recursive and residual algorithms for the efficient numerical integration of multi-body systems. <i>Multi-body System Dynamics</i>, 11(4):295--320, May 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{Rob94} Clarence R. Robbins. <i>Chemical and Physical Behavior of Human Hair</i>. Springer-Verlag, New York, third edition, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{Rob02} C. Robbins. <i>Chemical and Physical Behavior of Human Hair</i>. 4th ed. Springer, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{Rub00} M. B. Rubin. <i>Cosserat Theories: Shells, Rods and Points</i>. Springer, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{SGF77} Robert F. Stamm, Mario L. Garcia, and Judith J. Fuchs. The optical properties of human hair i. fundamental considerations and goniophotometer curves. <i>J. Soc. Cosmet. Chem.</i>, 28:571--600, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{Sha01} Ahmed A. Shabana. <i>Computational Dynamics</i>. Wiley-Interscience, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{Tri70} R. Tricker. <i>Introduction to Meteorological Optics</i>. Mills &amp; Boon, London, 1970.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1263263</ref_obj_id>
				<ref_obj_pid>1263129</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{WBK&#60;sup&#62;+&#60;/sup&#62;07} K. Ward, F. Bertails, T.-Y. Kim, S. Marschner, M.-P. Cani, and M. Lin. A survey on hair modeling: styling, simulation, and rendering. <i>IEEE Transactions on Visualization and Computer Graphics</i>, 13(2):213--234, Mar/Apr. 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{WGL04} K. Ward, N. Galoppo, and M. Lin. Modeling hair influenced by water and styling products. In <i>Proceedings of Computer Animation and Social Agents (CASA'04)</i>, pages 207--214, May 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1130445</ref_obj_id>
				<ref_obj_pid>1130237</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{WGL06} K. Ward, N. Galoppo, and M. Lin. A simulation-based vr system for interactive hairstyling. In <i>IEEE Virtual Reality - Application and Research Sketches</i>, pages 257--260, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1246983</ref_obj_id>
				<ref_obj_pid>1246981</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[{WGL07} K. Ward, N. Galoppo, and M. Lin. Interactive virtual hair salon. In <i>PRESENCE to appear (June 2007)</i>, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946947</ref_obj_id>
				<ref_obj_pid>946250</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[{WL03} K. Ward and M. Lin. Adaptive grouping and subdivision for simulating hair dynamics. In <i>Proceedings of Pacific Graphics'03</i>, pages 234--243, September 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791860</ref_obj_id>
				<ref_obj_pid>791221</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[{WLL&#60;sup&#62;+&#60;/sup&#62;03} K. Ward, M. Lin, J. Lee, S. Fisher, and D. Macri. Modeling hair using level-of-detail representations. In <i>Proceedings of Computer Animation and Social Agents (CASA'03)</i>, pages 41--47, May 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>320042</ref_obj_id>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{Wol99} S. Wolfram. <i>The Mathematica book (4th edition)</i>. Cambridge University Press, New York, NY, USA, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617720</ref_obj_id>
				<ref_obj_pid>616021</ref_obj_pid>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[{WS92} Y. Watanabe and Y. Suenaga. A trigonal prism-based method for hair image generation. <i>IEEE Computer Graphics and Applications</i>, 12(1):47--53, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>565650</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[{WW90} A. Witkin and W. Welch. Fast animation and control of non-rigid structures. In <i>Computer Graphics Proceedings (Proceedings of the ACM SIGGRAPH'90 conference)</i>, pages 243--252, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345147</ref_obj_id>
				<ref_obj_pid>345140</ref_obj_pid>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[{YXYW00} X. Yang, Z. Xu, J. Yang, and T. Wang. The cluster hair model. <i>Graphics Models and Image Processing</i>, 62(2):85--103, March 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[{ZFWH04} C. Zeller, R. Fernando, M. Wloka, and M. Harris. Programming graphics hardware. In <i>Eurographics - Tutorials</i>, September 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[{Zvi86} C. Zviak. <i>The Science of Hair Care</i>. Marcel Dekker, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 REALISTIC HAIR SIMULATION ANIMATION AND RENDERING SIGGRAPH 2008 Course Notes May 19, 2008 Course Organizer: 
Florence Bertails (INRIA) Presenters: Sunil Hadap Marie-Paule Cani Ming Lin Adobe Systems INP Grenoble 
University of North Carolina sunilhadap@acm.org Marie-Paule.Cani@imag.fr lin@cs.unc.edu Tae-Yong Kim 
Florence Bertails Steve Marschner Rhythm&#38;Hues Studio INRIA Rh one-Alpes Cornell University tae@rhythm.com 
Florence.Bertails@inrialpes.fr srm@cs.cornell.edu Kelly Ward Zoran Ka.c-Alesi´ci´c Walt Disney Animation 
Studios Industrial Light+Magic Kelly.Ward@disney.com zoran@ilm.com  Course Abstract The last .ve years 
have seen a profusion of innovative solutions to one of the most challenging tasks in character synthesis: 
hair simulation. This class covers both recent and novel research ideas in hair animation and rendering, 
and presents time tested industrial practices that resulted in spectacular imagery. The course is aimed 
at an intermediate level and addresses the special-effects developers and technical directors who are 
looking for innovation as well as proven methodologies in hair simulation. The audience will get a good 
grasp of the state of the art in hair simulation and will have plenty of working solutions that they 
can readily implement in their production pipelines. The course will also be a boot­camp for aspiring 
computer graphics researchers interested in physically based modeling in computer graphics. The class 
addresses the special-effects developers and technical directors who are looking for innovation as well 
as proven methodologies in hair simulation. The audience will get a good grasp of the state of the art 
in hair simulation and will have plenty of working solutions that they can readily implement in their 
production pipelines. The class will also be a boot-camp for aspiring computer graphics researchers interested 
in physically based modeling. The class covers two crucial tasks in hair simulation: animation and rendering. 
For hair animation, we .rst discuss recent successful models for simulating the dy­namics of individual 
hair strands, before presenting viable solutions for complex hair-hair and hair-body interactions. For 
rendering, we address issues related to shading models, multiple scattering, and volumetric shadows. 
We .nally demon­strate how hair simulation techniques are nowadays developed and applied in the feature 
.lms industry to produce outstanding visual effects. Prerequisites: Familiarity with fundamentals of 
computer graphics, physical simulation and physically based rendering is strongly recommended but not 
manda­tory. Understanding of numerical linear algebra, differential equations, numerical methods, rigid-body 
dynamics, collision detection and response, physics-based illumination models, .uid-dynamics would be 
a plus. Target audiences include special effects developers, technical directors, game developers, researchers, 
or any one interested in physically based modeling for computer graphics. Contents Introduction 5 VirtualHair:Motivations&#38;Challenges 
. . . . . . . . . . . . . . . . . 5 Presentationofthespeakers........................ 7 CourseSyllabus 
.............................. 10 1 Dynamics of Strands 13 1.1 Introduction ............................. 
13 1.2 Hairstructureandmechanics .................... 14 1.3 Oriented Strands: a versatile dynamic 
primitive . . . . . . . 15 1.3.1 IntroductionandRelatedWork. . . . . . . . . . . . . . . 16 1.3.2 Differential 
Algebraic Equations . . . . . . . . . . . . . 18 1.3.3 Recursive Residual Formulation . . . . . . . . 
. . . . . . 19 1.3.4 AnalyticConstraints .................... 22 1.3.5 CollisionResponse .................... 
27 1.3.6 ImplementationDetails .................. 29 1.3.7 Simulation Parameters and Work.ow . . . 
. . . . . . . . 29 1.3.8 Stiffness Model and External Forces . . . . . . . . . . . . 30 1.3.9 Accurate 
Acceleration of Base Joint . . . . . . . . . . . . 31 1.3.10 TimeScaleandLocalDynamics . . . . . . . 
. . . . . . . 31 1.3.11 PosableDynamics ..................... 31 1.3.12 Zero-gravityRestShape .................. 
32 1.3.13 Strand-strandInteraction .................. 32  1.4 Super-Helices: a compact model for thin 
geometry . . . . . 33 1.4.1 TheDynamicsofSuper-Helices . . . . . . . . . . . . . . 33 1.4.2 ApplicationsandValidation 
. . . . . . . . . . . . . . . . 40 1.4.3 Conclusion ......................... 45 1.4.4 Parametervaluesfornaturalhair 
. . . . . . . . . . . . . . 47 2 Hair Interactions 49 2.1 Introduction............................. 49 
 2.2 Hair animation and interaction processing . . . . . . . . . . . . . 51 2.2.1 Continuous versus wisp-based 
hair models . . . . . . . . 51 2.2.2 Processing hair interactions with the body . . . . . . . . . 51 
2.2.3 The issues raised by hair self-interactions . . . . . . . . . 52 2.3 A volumetric approach for 
real-time self-interactions . . . . . . . 54 2.3.1Avolumetric structureforhair ............... 54 2.3.2 
Applicationto hair interaction . . . . . . . . . . . . . . . 54 2.4 Detecting guide-strand interactions 
. . . . . . . . . . . . . . . . . 55 2.4.1 Deformable versus cylindrical hair wisps . . . . . . . . . 
56 2.4.2 Handling curly hair and exploiting temporal coherence . . 58 2.5 Response to guide-strand interactions 
. . . . . . . . . . . . . . . 60 2.5.1 Anisotropic response in wisp-based methods . . . . . . . 60 2.5.2 
Setting up realistic penalty and friction forces . . . . . . . 61 2.6 Conclusion ............................. 
63 3 Multi-Resolution Hair Modeling 65 3.1 Geometric Representations ..................... 67 3.1.1 
TheBaseSkeleton ..................... 67 3.1.2 Strips ............................ 68 3.1.3 Clusters........................... 
68 3.1.4 Strands ........................... 69 3.2 Hair Hierarchy ........................... 69 
3.2.1 Stripand ClusterSubdivision ............... 70 3.2.2 StrandGroupSubdivision ................. 
71 3.3 Runtime SelectionofHair ..................... 72 3.3.1Visibility .......................... 
74 3.3.2Viewing Distance ...................... 74 3.3.3 Hair Motion ......................... 75 
3.3.4 Combining Criteria ..................... 76 3.4 Level-of-DetailTransitions ..................... 
76 3.4.1 AdaptiveSubdivision .................... 77 3.4.2 AdaptiveMerging ..................... 77 
 3.5 InteractiveDynamic Simulation .................. 78 3.5.1 ImplicitIntegration ..................... 
79 3.5.2 Collision Detection and Response . . . . . . . . . . . . . 82 3.5.3 Simulation Localization 
.................. 87 3.5.4 Resultsand Discussion ................... 90 3.6 Conclusion ............................. 
92 4 Hair Rendering 94 4.1 RepresentingHairfor Rendering .................. 94 4.1.1 Explicit Representation 
................... 95 4.1.2 Implicit Representation ................... 95 4.2 Light ScatteringinHair 
....................... 96 4.2.1 HairOptical Properties ................... 96 4.2.2 Notation and Radiometry 
of Fiber Re.ection . . . . . . . 97 4.2.3 Re.ection and Refraction in Cylinders . . . . . . . . . . . 
99 4.2.4 Measurementsof Hair Scattering . . . . . . . . . . . . . . 99 4.2.5 ModelsforHair Scattering 
................. 100 4.2.6 Light ScatteringonWetHair ................ 102 4.3 Hair Self-Shadowing 
........................ 103 4.3.1 Ray-casting throughaVolumetric Representation . . . . . 104 4.3.2 
ShadowMaps ........................ 104 4.4 Rendering AccelerationTechniques . . . . . . . . . . . . 
. . . . 106 4.4.1 ApproximatingHair Geometry ............... 107 4.4.2 InteractiveVolumetric Rendering 
. . . . . . . . . . . . . 107 4.4.3 GraphicsHardware ..................... 108 5 Hair in Feature Production 
110 5.1 Strand and Hair Simulation at PDI/DreamWorks -Madagas­carandShrekTheThird ................. 110 
5.1.1 Conclusion, Limitations and FutureWork . . . . . . . . . 114 5.1.2 Acknowledgments ..................... 
115 5.2 StrandandHair SimulationatILM ................. 116 5.2.1 Overview .......................... 
116 5.2.2 DynamicsofHairand Strands ............... 117 5.2.3 Challenges ......................... 
121 5.2.4 Examples.......................... 124 5.2.5 Conclusions......................... 129 5.3 
Hair Simulation at Rhythm and Hues -Chronicles of Narnia . . . 130 5.3.1 Thehair simulator ..................... 
131 5.3.2 Layering and mixing simulations . . . . . . . . . . . . . 138 5.3.3 Simulating .exible objects 
for crowd characters . . . . . . 138    Introduction Virtual Hair: Motivations &#38; Challenges Hair 
is an essential element for the plausibility of virtual humans. It was however neglected for tenths of 
years, being considered as almost impossible to animate both ef.ciently and with some visual realism. 
Virtual characters were thus mainly modelled with short, rigid hair, represented by a plain surface, 
or sometimes with a pony-tail represented by a generalized cylinder with around some simple dynamic skeleton 
(such as a chain of masses and springs). Recently, modelling, animating, and rendering realistic hair 
has drawn a lot of interest, and impressive new models were introduced. This course presents these advances 
and their applications in recent productions. For a full state of the art on the domain, the reader should 
refer to [WBK+07]. 1. Nature of hair and challenges The great dif.culty in modelling and animating realistic 
hair comes from the com­plexity of this speci.c mater: human hair is made of typically 100 000 to 200 
000 strands, whose multiple interactions produce the volume and the highly damped and locally coherent 
motion we observe. Each hair strand is itself an inextensible, elastic .bre. As such, it tends to recover 
its rest shape in terms of curvature and twist when no external force is applied. Hair strands are covers 
by scales, mak­ing their frictional behaviour, as well as the way they interact with light, highly anisotropic. 
Lastly, the ellipticity of their cross-section which varies from an elongated ellipse for African hair 
to a circular shape for Asian hair is responsi­ble for the different kinds of curls, from quasi-uniform 
curliness to the classical European locks, quite straight at the top but helicoidal at the bottom. Reproducing 
these features in virtual is clearly a challenge. A typical example is the number of interactions that 
one would have to process at each time step, if a naive model for hair was used, with the extra dif.culty 
of preventing crossing between very thin objects. Even if these interactions, though responsible for 
most of the emerging behavior of hair, were neglected, a full head of 100 000 hair would still be dif.cult 
to animate in a reasonable computational time [SLF08]. 2. From individual strands to a full head of 
hair: elements of methodology Most hair models proposed up to now use a speci.c methodology to cope with 
the complexity of hair, in terms of the number of strands: they simulate, at each time step, the motion 
of a relatively small number of guide strands (typically, a few hundreds), and use either interpolation 
or approximation to add more strands at the rendering stage. See Figure 1. More precisely, three strategies 
car be used for generalizing a set of guide strands to a full head of hair : 1. Using the hypothesis 
that hair is a continuous set of strands, one can inter­polate between three guide strands which are 
neighbours of the scalp; this works well for straight, regular hair styles; 2. One can on the opposite 
add extra stands around each guide strand to form a set of independent wisps; This has proved successful 
for curly hair for which hair clustering is more relevant; 3. A hybrid strategy, which consists in interpolating 
between guide strands near the scalp while extrapolating to generate wisps at the bottom of hair, was 
introduced recently [BAC+06]. This has the advantage of capturing the aspect of any type of hair.  Using 
this methodology, the main challenges in terms of animation are to .nd good models for animating individual 
strands, and then modify their dynamics to take into account the interactions that would take place in 
the corresponding full head of hair. 3. Overview The contents of the course notes is organized as follows: 
chapter 1 .rst presents and compares the models for animating individual strands. Chapter 2 deals with 
the generalization to a full head of hair by reviewing the different methods for pro­cessing hair interactions. 
Chapter 3 presents recent multiresolution schemes for Figure 1: Animating guide strands (left). More 
hair strands are added before rendering, using methods that range from interpolation to approximation 
(right, from [BAC+06]). computing the geometry and dynamics of hair at interactive frame rates. Chapter 
4 is devoted to the important problem of hair rendering. Finally, chapter 5 presents the current hair 
models used in the feature .lm industry to produce outstanding visual effects. Presentation of the speakers 
Florence Bertails, INRIA Rh one-Alpes, France Florence Bertails is a young tenured researcher at INRIA 
Rh one-Alpes in Greno­ble, France. A graduate from the Telecommunication Engineering School of INP Grenoble, 
she received in 2002 a MSc in Image, Vision and Robotics, and com­pleted in 2006 a PhD on hair simulation 
at INP Grenoble. She worked at the Uni­versity of British Columbia as a postdoctoral researcher, before 
joining INRIA in September 2007 as a permanent researcher. Her research interests deal with the modeling 
and the simulation of complex mechanical objects, mainly for graph­ics applications. She presented her 
work at international conferences such as the ACM-EG Symposium of Computer Animation, Eurographics, and 
SIGGRAPH. Sunil Hadap, Adobe Systems (formerly at PDI/DreamWorks), USA Sunil Hadap is Manager at Advanced 
Technology Labs, Adobe. Formerly, he was a member of R&#38;D Staff at PDI/DreamWorks, developing next 
generation dynam­ics tools for use in .lm productions. His research interests include wide range of physically 
based modeling aspects such as clothes, .uids, rigid body dynamics and deformable models, and recently 
computational imaging. Sunil Hadap has completed his PhD in Computer Science from MIRALab, University 
of Geneva under the guidance of Prof. Nadia Magnenat-Thalmann. His PhD thesis work is on Hair Simulation. 
Sunil further developed strand and hair simulation techniques at PDI/DreamWorks. The resulting system 
is extensively used in production of Madagascar, Shrek The Third and Bee Movie. Marie-Paule Cani, INP 
Grenoble, France Marie-Paule Cani is a Professor at the Institut National Polytechnique de Greno­ble, 
France. A graduate from the Ecole Normale Sup´erieure, she received a PhD in Computer Science from the 
University of Paris Sud in 1990. She was awarded membership of the Institut Universitaire de France in 
1999. Her main research in­terest is to explore the use of geometric and physically-based models for 
designing complex interactive scenes. Recent applications include the animation of natural scenes (lava-.ows, 
ocean, meadows, wild animals, human hair) and interactive sculpting or sketching techniques. Marie-Paule 
Cani has served in the program committee of the major CG conferences. She co-chaired IEEE Shape Modelling 
&#38; Applications in 2005 and was paper co-chair of EUROGRAPHICS 2004 and SCA 2006. Ming Lin, University 
of North Carolina, USA Ming Lin received her Ph.D. in EECS from the University of California, Berkeley. 
She is currently Beverly Long Distinguished Professor of Computer Science at the University of North 
Carolina, Chapel Hill. She received several honors and six best-paper awards. She has authored over 170 
refereed publications in physically­based-modeling, haptics, robotics, and geometric computing. She has 
served as the chair of over 15 conferences and the steering committee member of ACM SIGGRAPH/EG Symposium 
on Computer Animation, IEEE VR, and IEEE TC on Haptics and on Motion Planning. She is also the Associate 
EIC of IEEE TVCG and serves on 4 editorial boards. She has given many lectures at SIGGRAPH and other 
international conferences. Tae-Yong Kim, Rhythm &#38; Hues Studios, USA Tae-Yong Kim is a software developer 
in Rhythm and Hues Studios. He actively develops and manages the company s proprietary dynamics software, 
including simulation of cloth, hair, and rigid body. He is also part of .uid dynamics simu­lation team 
there and has contributed to the company s Academy Award winning .uid system. He holds a Ph.D degree 
in computer science from the University of Southern California where he did researches on hair modeling 
and rendering techniques. His work was published in SIGGRAPH 2002 as well as other confer­ences. He has 
been a lecturer in recent SIGGRAPH courses (2003, 2004, 2006, 2007). Zoran Kaci.´c, Industrial Light 
&#38; Magic, USA c-Alesi´ Zoran Ka.ci´c-Alesi´c is a principal R&#38;D engineer at Industrial Light &#38; 
Magic, leading a team responsible for structural simulation and sculpting/modeling tools. His movie credits 
span Jurassic Park, Star Wars, Harry Potter, and the Pirates of the Caribbean. He received a Scienti.c 
and Engineering Academy Award for the development of the ViewPaint 3D Paint System. Zoran holds a BEng 
degree in electrical engineering from the University of Zagreb, Croatia; a MSc in computer science from 
the University of Calgary, Canada; and an honorary doctorate in .ne arts from the University of Lethbridge, 
Canada. Steve Marschner, Cornell University, USA Steve Marschner is Assistant Professor of Computer Science 
at Cornell Univer­sity, where he is conducting research into how optics and mechanics determine the appearance 
of materials. He obtained his Sc.B. from Brown University in 1993 and his Ph.D. from Cornell in 1998. 
He held research positions at Hewlett-Packard Labs, Microsoft Research, and Stanford University before 
joining Cor­nell in 2002. He has delivered numerous presentations, including papers at IEEE Visualization, 
the Eurographics Rendering Workshop, and SIGGRAPH, and SIG-GRAPH courses every year from 2000 to 2005. 
For contributions in rendering translucent materials, he is co-recipient with Henrik Wann Jensen and 
Pat Hanra­han of a 2003 Academy Award for technical achievement. Kelly Ward, Disney Animation, USA Kelly 
Ward is currently a senior software engineer at Walt Disney Animation Stu­dios, where she develops hair 
simulation tools for animated .lms. She received her M.S. and Ph.D. degrees in Computer Science from 
the University of North Carolina, Chapel Hill in 2002 and 2005, respectively. She received a B.S. with 
honors in Computer Science and Physics from Trinity College in Hartford, CT in 2000, where she was named 
the President s Fellow in Physics in 1999-2000. Her research interests include hair modeling, physically-based 
simulation, colli­sion detection, and computer animation. She has given several presentations and invited 
lectures on her hair modeling research at international venues. Course Syllabus Introduction. Virtual 
Hair: motivations and challenges (Marie-Paule Cani) Hair is essential towards realistic virtual humans. 
However, it can be con­sidered as one of the most challenging material to be modeled, being made of a 
huge number of individual .bers which interact both mechanically and optically. This talk presents the 
basic methodology for generating a full head of hair from a reasonable number of animated strands and 
introduces the main problems in hair animation and rendering which will be developed in this class. Session 
1. Dynamics of Strands Oriented Strands a versatile dynamic primitive (Sunil Hadap) The simulation of 
strand like primitives modeled as dynamics of serial branched multi-body chain, albeit a potential reduced 
coordinate formulation, gives rise to stiff and highly non-linear differential equations. We introduce 
a recursive, linear time and fully implicit method to solve the stiff dynamical problem arising from 
such a multi-body system. We augment the merits of the proposed scheme by means of analytical constraints 
and an elaborate collision response model. We .nally show how this technique was successfully used for 
animating ears, foliage and hair in the feature productions Shrek The Third and Madagascar. Super Helices 
 dynamics of thin geometry (Florence Bertails) We introduce the mechanical model based on Super Helix. 
This model is de.ned as a piece-wise helical rod, and can represent the essential modes of de­formation 
(bending and twisting) of a strand, as well as a complex rest geometry (straight, wavy, curly) in a very 
compact form. We develop the kinematics of the model, as we derive the dynamic equations from the Lagrange 
equations of motion. Finally, we provide a rigorous validation for the Super Helix model by comparing 
its behavior against experiments performed on real hair wisps. Session 2: Hair-obstacle and Hair-hair 
Interaction Strategies for hair interactions (Marie-Paule Cani) This talk presents the two main approaches 
developed to tackle hair interac­tions: the haircontinuum methods, which generate forces that tend to 
restore the local density of hair, possibly in real time; and the methods based on pair-wise in­teractions 
between hair clusters. The latter raise the problem of ef.cient collision detection, leading to solutions 
which either adapt the number of hair clusters over time or exploit temporal coherence. We also discuss 
the generation of adequate, anisotropic response forces between wisp volumes. Multi-resolution hair-hair 
and hair-obstacle interaction (Ming Lin, Kelly Ward) We present novel geometric representations, simulation 
techniques, and nu­merical methods to signi.cantly improve the performance of hair dynamics com­putation, 
hair-object and hair-hair interactions. These approaches focus on bal­ancing visual .delity and performance 
to achieve realistic appearance of animated hair at interactive rates. In addition, we discuss application 
and system require­ments that govern the selection of appropriate techniques for interactive hair mod­eling. 
Session 3 : Hair Rendering (Steve Marschner) In this session, we cover the state-of-the-art in hair rendering. 
We present a comprehensive yet practical theory behind physically based hair rendering, in­cluding light 
scattering through hair volume and self-shadowing, and provide ef­.cient algorithms for solving these 
issues. Session 4 : Hair Simulation in Feature Productions Hair Simulation at Walt Disney Animation Studios 
(Kelly Ward) We present hair simulation techniques and work-.ows used in production on the up-coming 
animated feature Bolt. Hair Simulation at ILM (Zoran Ka.ci´c-Alesi´c) We provide an overview of hair 
and strand simulation techniques used in the production environment at ILM. Examples include highly dynamic 
long hair (Vampire Brides in Van Helsing), full body medium length fur (werewolves, wolves, Wookies in 
Van Helsing, The Day After Tomorrow, and Star Wars Episode 3), dig­ital doubles (Jordan in The Island 
and Sunny in Lemony Snicket s A Series of Un­fortunate Events), articulated tentacles simulations (Davy 
Jones in Pirates of the Caribbean 2 and 3), as well as recent examples from The Spiderwick Chronicles 
and Indiana Jones and the Kingdom of the Crystal Skull. Commonalities between hair, cloth, .esh, and 
rigid body simulations are explored, along with situations in which they can be used together or interchangeably. 
Hair Simulation at Rhythm and Hues (Tae-Yong Kim) Since the old Polar bear commercial, hair simulation 
techniques at Rhythm and Hues Studios have experienced dramatic changes and improvements over last decade. 
In this presentation, we provide an overview of hair simulation techniques used in R&#38;H, including 
short hair/fur (gar.eld, alvin and the chipmunks), medium hair/fur (The Chronicles of Narnia, The Night 
at the Museum) and more human­like long hair (The Incredible Hulk). We also provide a brief description 
of the new mass spring simulation system we developed over past couple of years. Session 5 : Questions 
and Discussions Chapter1 Dynamics of Strands Sunil Hadap, Florence Bertails, Basile Audoly, Marie-Paule 
Cani 1.1 Introduction Realistic hair simulation is one of the most dif.cult issues when animating vir­tual 
humans. Human hair is a very complex material, consisting of hundreds of thousands of very thin, inextensible 
strands that interact with each other and with the body. Unlike solids or .uids, which have been studied 
for over a century and well modeled by now classical equations, hair remains a largely unsolved problem 
described by no well accepted model. Finding a representation that provides an accurate simulation of 
hair motion remains a challenge. Modeling hair dynamics raises a number of dif.culties. The very .rst 
one is due to the fact that each individual strand has a complex nonlinear mechanical be­havior, strongly 
related to the thinness of its cross section as well as its natural shape: smooth, wavy, curly or fuzzy. 
In this chapter, after a brief report about the mechanical structure and properties of hair strands, 
we present two innova­tive models that allow to capture the main dynamic features of thin geometry. The 
.rst model, called Oriented Strands, is based on a stable integration of serial body dynamics, and nicely 
incorporates external constraints. The second model, called Super-Helices, provides a compact high-order 
representation for a strand of arbitrary geometry (straight, wavy, curly), and captures the essential 
modes of deformation of a real .ber; this model was strongly validated against experiments made on real 
hair. The dif.cult problem of interactions and contacts to be accounted for when simu­lating a full head 
of hair, and the question of ef.ciency, will be adressed in chap­ter2 and chapter3, respectively. 1.2 
Hair structure and mechanics Achieving realistic simulations of hair motion requires some understanding 
of hair structure. This section gives a summary of the existing knowledge on individual hair strands, 
mostly issued from the .eld of cosmetics. Further details can be found in [LFHK88, Rob02]. A human hair 
.ber is a thin structure (about 0.1 mm in diameter) with either a circular or oval cross section. The 
active part, called the follicle, is located under the skin and produces the keratin proteins that compose 
the hair material. The second part, the visible and dead part of hair, is called the hair shaft, and 
corresponds to the hair strand we are seeking to animate. The hair shaft is entirely synthesized by the 
associated follicle, which acts as a mold for shaping the strand [LFHK88]. It thus has almost uniform 
cross sec­tion, natural twist and natural curvatures all along. These geometric parameters are associated 
with commonsense notions of straight, curly, or fuzzy hair. Their values are characteristic of the ethnic 
group from which the hair comes [Rob02]. Africans have follicles with a helical form and an oval cross 
section, whereas Asians have follicles that are completely straight with a larger and circular cross 
section. As a result, Asian hair is thicker, with no natural curliness. It makes it look smooth and regular. 
In contrast, African hair looks frizzy and irregular. Caucasian hair stands between these two extremes. 
The internal structure of the shaft consists of three concentric layers from the core to the periphery: 
a central canal called medulla; the cortex, i.e. cells .lled with keratin, contributing 90% of the total 
weight; and the cuticle, a thin coating covered by tilted scales. Keratin is a remarkably stiff material, 
making the shaft extremely dif.cult to shear and stretch. However, because its cross section is very 
small, it can be easily bent and twisted. Deformations of a hair strand involve rotations that are not 
in.nitely small and so can only be described by nonlinear equations [AP07]. Physical effects arising 
from these nonlinearities include instabilities called buckling. For example, when a thin hair wisp is 
held between two hands that are brought closer to each other (see Figure 1.1, right), it reacts by bending 
in a direction perpendicular to the applied compression. If the hands are brought even closer, a second 
instability occurs and the wisp suddenly starts to coil (the bending deformation is converted into twist). 
1.3 Oriented Strands: a versatile dynamic primitive The simulation of strand like primitives modeled 
as dynamics of serial branched multi-body chain, albeit a potential reduced coordinate formulation, gives 
rise to stiff and highly non-linear differential equations. We introduce a recursive, linear time and 
fully implicit method to solve the stiff dynamical problem arising from such a multi-body system. We 
augment the merits of the proposed scheme by means of analytical constraints and an elaborate collision 
response model. We .nally discuss a versatile simulation system based on the strand primitive for character 
dynamics and visual effects. We demonstrate dynamics of ears, braid, long/curly hair and foliage. 1.3.1 
Introduction and RelatedWork The simulation of ears, tails, braids, long wavy/curly hair, foliage, jewelry 
is pe­culiar in nature. The .exible shape is characterized by a thin and long geom­etry, which typically 
has a non-straight rest con.guration. The dynamics pre­dominantly includes the bend and the torsional 
components, and very rarely the length-wise stretch component. Even though being one-dimensional in nature, 
the intricate rendering aspects of these primitives, along with potentially highly anisotropic physical 
properties, demand a consistent/stable curvilinear coordinate system all along the geometry. Here, we 
would like to present a versatile dy­namic primitive that spans the stated characteristics and applications. 
We name the system as Oriented Strands, to clearly conveythe picularity to the user. Cosserat Models 
discussed in [Rub00] and .rst introduced to computer graphics community by [Pai02b] give an elaborate 
continuum theory behind the dynamics of thin deformable objects such as strand and shells. The discrete 
approximation of the strand model come strikingly close to the strand-as-serial-multi-body-chain model, 
.rst proposed by [HMT01b, Had03]. Since then the paradigm is suc­cessfully used for hair simulation by 
[CJY02b, CCK05b]. We too model strand as serial chain of rigid segments connected by spherical joints. 
Previously, the hair was typically modeled using mass-spring-hinge system, as individual hair strands 
[RCT91a, AUK92a] or as wisps [PCP01b]. However, these models are not effective in representing consistent 
coordinates and the twist dynamics. An exhaustive overview of various hair simulation techniques is given 
in [Had03]. [Fea87] developed one of the .rst multi-body reduced coordinate formulations that hasa linear 
complexity. [Mir96,Kok04] furtherdevelopedef.cient and com­prehensive impulse and constraint formulations 
to it. [RGL05b] extended the for­mulation to achieve interesting sub-linear complexity, and also gives 
a thorough overview of the other reduced coordinate formulations. [Bar96, Sha01] gives max­imal coordinate 
formulations which also are known to have linear complexity us­ing sparse-matrix solution methods. The 
typical multi-body system resulting from the strand model,gives rise to stiff and highly non-linear differential 
equations. The numerical dif.culties stem from small rotational inertia along the axis due to thin geometry, 
large bend and torsional stiffness-to-mass-ratio and intricate non­straight rest shape. The non-linearity 
is due to velocity terms corresponding to Coriolis forces and the speci.c choice of the non-linear elastic 
model in our im­plementation to limit unrealistic high deformations. These dif.culties call for an implicit 
integration scheme. Even though the reduced coordinate formulation is ef.cient for multi-body systems 
with large number of segments and relatively small number of DOFs, it is dif.cult to realize an implicit 
integration scheme, as pointed out by [Had03]. Instead, [CCK05b] use a traditional maximal coordinate 
formulation with (soft) constraints [Bar96, Sha01], followed by an implicit inte­gration. Complex collision 
response models with static and dynamic friction can be integrated into the maximal coordinate framework, 
with relative ease, using impact and contact velocity constraints [CCK05b]. Nevertheless, the reduced 
coordinate formulation has certain advantages. The generalized coordinates directly facilitate the parameterization 
for bending and torsional stiffness dynamics. Further, they have the exact same form as the pa­rameterization 
used in articulated character animation. Thus the rest shape of the multi-body system can be conveniently 
de.ned in terms of some animated local, e.g. a hairstyle can be de.ned in terms of the frame associated 
with the head joint. Even the dynamics is often expressed in terms of successive local coordi­nates starting 
from the base link. One can thus interpret the dynamic motion of the strand as overall rigid-body transformation 
of the strand at the base link, followed by secondary dynamics from the rest shape expressed in terms 
of successive local frames. This paradigm gives a tremendous advantage in terms of overall simula­tion 
work.ow. Typically the base link is constrained to an animation path. Using the paradigm, it is trivial 
to kick-start the simulation from an arbitrary starting po­sition and orientation of the base link. Moreover, 
certain concepts such as posable dynamics, local dynamics, time-scale and zero-gravity rest shape make 
the strand simulation system versatile. As discussed subsequently, they are often trivial to realize 
in the paradigm of reduced coordinate formulation. Ultimately, the choice of reduced coordinate formulation 
proved very rewarding for us. The speci.c contributions of the methodology are as follows. In Section 
1.3.2 and Section 1.3.3 we develop a linear time, implicit and fully recursive scheme for reduced coordinate 
formulation of general branched open-chain multi-body system, using Differential Algebraic Equations 
(DAE). In Section 1.3.4, we dis­cuss how to realize external bilateral and unilateral constraints on 
the formulated multi-body dynamics. We also discuss the numerical issues associated with the solution 
of Linear Complementarity Problem (LCP) arising from the formulation. In Section 1.3.5, we develop an 
elaborate collision response model with inelastic impact and static/dynamic friction, using unilateral 
constraints. Finally, in Sec­tion 1.3.6, we introduce the Oriented Strands system, implemented as dynamics 
of serial multi-body chain. We develop some novel concepts and give important implementation details 
that makes the dynamic strand primitive versatile. In Sec­tion 5.1, we present some illustrative examples 
of dynamics of ears, braid, hair and foliage. 1.3.2 Differential Algebraic Equations Typically, unconstrained 
dynamical problems such as cloth [BW98b] and general deformable models are formulated as the following 
explicit Ordinary Differential Equation (ODE) of degree two. q¨= M-1Q(t, q, q.) (1.1) Constrained dynamical 
problems such as dynamics of multi-body systems [Sha01, Bar96] are formulated as the following semi-explicit 
ODE of degree two. M(q)q¨= Q(t, q, q.) - Fq T. F(t, q)= 0 (1.2) where, M is generalized mass matrix. 
The force function Q and the constraint function F are typically non-linear and stiff . In order to integrate 
the state vector [q.T , q T]T, in a traditional way, one can try and solve for the derivatives of the 
state t vector [q¨T , q.T]T , which often turns out to be complex. Fortunately, the direct t+1 computation 
of derivatives is not the only way, neither it is the most ef.cient way, of solving the differential 
equations. Differential Algebraic Equations solvers are TTT TTT remarkable, they advance the solution 
[q., q ]. [q., q ], as they estimate tt+1 the derivatives [q¨T , q.T]T at the same time. t+1 As far as 
we can track, Differential Algebraic Equations (DAE) are new to com­puter graphics. In this section wewould 
like togivea gentle introduction toDAE. For thorough discussion and associated theory we would like to 
refer to [BCP96]. DAE solvers work on the system of differential equations in its most implicit form. 
Consider the followingDAE of degree one. F(y, y., t)= 0 (1.3) The implicit function F in differential 
variables y and free variable t may be non­linear and stiff . Let the set {y, y.}t be the solution of 
the DAE, i.e. it satis.es equation 1.3 at time t. Then the DAE solvers use an extrapolation method, e.g. 
Backward Difference Formula (BDF) of an appropriate order, to extrapolate the solution to y 1 and while 
making a numerical estimate of the derivative y.1 t+1 t+1. 11 The estimates yt+1, y.typically would not 
satisfy equation 1.3. The solver then t+1 successively corrects the solution and associated derivative 
estimate by number of Newton-Raphson iterations. Let the residue associated with the estimate of kth 
iteration be k kk rs = F(yy + 1) (1.4) t+1 t+1, .t+1, t The Newton-Rapson iteration takes the following 
form -1 . F k+1 k k yt+1 = y - rs t+1 . y t+1 k+1 k+1 y.t+1 =(y - yt)/.t (1.5) t+1 Thus, in order to 
useDAE integrator such asDASPK [BCP96], one has to provide the integrator with a residual function rs 
that computes the residue from the esti­mates of the solution the integrator provides. One may also provide 
the Jacobian of the residue function . F/. y, or optionally the integrator can compute the Jaco­bian 
numerically. The highlight of the solution method is the residue function rs and the Jacobian . F/. 
y are often simple to evaluate. In the next section, we formulate a fully recursive method to evaluate 
the residual function for solution ofa sitff multi-body system. 1.3.3 Recursive ResidualFormulation 
To describe the dynamics of the multi-body system, we use Spatial Algebra and associated notation developed 
by [Fea87]. Consider a serial branched multi-body system (MBS) having n links connected by n single DOF 
joints as shown in Fig­ure 1.2. There are no loops in the kinematic chain. The base link is denoted by 
link0 and is typically constrained to a motion path. The other links are numbered in a breadth .rst manner. 
The velocity v j, the ac­celeration a j and the inertia matrix I j of linkj are de.ned in the frame F 
j, which is rigidly attached to the link s principal inertia axis. The joint variable qj de.nes the spatial 
transformation X j that transforms the spatial quantities de.ned in the parent s frame F i to the frame 
F j of linkj. The derivatives of joint variables q.j and q¨j relate the velocity and acceleration of 
the parent to the velocity and acceleration of linki via the spatial joint axis s i. v j = X j v i + 
s jq.j a j = X j a i + s jq¨j+ v j× s jq.j (1.6) The set of joint variables qt and their derivative 
q.t forms the system state vector yt =[q.T , q T]T, at time t. We would like to solve the forward dynamical 
prob­ t lem given the base velocity v 0 and the base acceleration a 0, integrate the state from yt to 
yt+1. In what follows we will develop a recursive residual formulation based on DAE methodology discussed 
in the previous section. The discussion is rather a free physical interpretation, for the rigorous proof 
refer to [RJFdJ04]. The procedure is surprisingly simple as compared to the traditional methods such 
as Articulated Body Method [Fea87], where one computes the state derivative y.t+1 explicitly. The solution 
set {yt, y.t} at time t forms the input to theDAE integrator. As high­ k lighted in the previous section, 
the integrator then estimates the new state yt+1, and k it s derivative y.t+1 in succession. It is our 
responsibility to compute the associated residue rst+1(y k , y.k). Given v 0 and a 0, we .rst compute 
the spatial velocities v j and spatial accelerations a j for each link linkj, j= 1..n using forward kinematic 
relation, equation 1.6. The residue associated with accelerations can be computed using the force balance 
condition. Starting with the outer most link linkn, the forces acting on linkn are spatial body force 
I n a n, combined spatial centripetal and Coriolis force v n× I n v n, external spatial force fn. The 
force balance equation for the spatial forces is rs n = I n a n + v n× I n v n - f n (1.7) We still have 
to relate the force residue rs n which is a spatial vector to the residue in joint acceleration which 
is a scalar. We project the force residue on to the joint s motion sub-space de.ned by the spatial joint 
axis s n. S rsn = s rs n - Qn (1.8) n where, s S is spatial transpose of the joint axis and Qn is scalar 
joint actuation force n associated with the stiffness model. The force residue projected on the joint 
s motion space rsn vanishes when the estimated state and derivative vector is the solution ofDAE. For 
simplicity, .rst consider a multi-body system with no branches. Thus, the only parent of linkn would 
be linkn-1. In computing the force balance equation for linkn-1, along with all the obvious forces described 
for linkn, we need to add the residual force from linkn that gets applied via the jointn. In order to 
do that, we need to transform the force residue rs n into the frame of linkn-1, using inverse -1 transformation 
matrix X . The resulting force balance equation for linkn-1 is n rs n-1 = I n-1 a n-1 + v n-1× I n-1v 
n-1 - fn-1 -1 + X rs n n s rsn-1 = s n-1rs n-1 - Qn-1 (1.9) We repeat this process for each linki till 
we reach the .rst link link1. The residue associated with the joint velocities is trivially the difference 
in joint velocities k* kk k* k q.- q., where q.belongs to y k and .q belongs to y.. iii i Algorithm 1 
lists the fully-recursive algorithm for computing the residue, for a general multi-body system that have 
branches. It is possible to compute the analytic Jacobians for the recursive residue formula­tion [RJFdJ04]. 
Alternatively, we can let the DAE solver compute the Jacobians numerically. We particularly commend the 
ef.cient and smart evaluations of Jacobians inDASPK, theDAE solver we used for the implementation. The 
solver uses modi.ed Newton-Rapson iteration, where the Jacobians are evaluated only when the solver observes 
a large change in the system s state. In practice, we found that the numerical evaluation of Jacobians 
is not only adequate, but also versatile. Thus, we can implement any complex stiffness model and associate 
general external .elds to the multi-body system, as discussed in Section 1.3.6. It may not be possible 
to evaluate analytic Jacobians for these. Algorithm1 rest+1(y k , y.k , v 0, a 0) k k* k q k q. Require: 
y =k, y.=k q.q¨ 1: n . dim(q) 2: for j= 1 to n do 3: i. parent(linkj) 4: v j . X j v i + s jq.j 5: a 
j . X j a i + s jq¨j+ v j× s jq.j 6: endfor 7: 8: rs . 0 . R6n 9: rs . 0. Rn 10: for i= n to1 do 11: 
rs i . I i a i + v i× I iv i - fi 12: for all j. child(linki) do -1 13: rs i . rs i + X rs j j 14: endfor 
s 15: rsi . s irs i - Qi 16: endfor 17: q.k*- q.k 18: return rs 1.3.4 Analytic Constraints The base 
joint in the multi-body system in Figure 1.2 is typically constrained to some prescribed motion path. 
In practice, we would want to impose some additional constraints on the system, e.g. constraining a point 
on some other link to a motion path, or constraining a joint to an animated value in time. These constraints 
are transient in nature and often introduce cyclic dependancy in the system. Thus they are treated as 
external constraints, as opposed to de.ning them implicitly as part of reduced coordinate formulation. 
Initially, we enthusiastically tried the DAE s natural way of de.ning constraints via algebraic slack 
variables. The general form of a DAE with algebraic con­straints is F(y, y., x, t)= 0 (1.10) where x 
is the set of algebraic variables. For each constraint, we formulated a scalar valued constraint function 
fi(y, y., t) and inserted an algebraic variable asso­ciated with the residue corresponding to the constraint 
condition x = fi(y, y., t)= 0 into theDAE. However, we soon abandoned this line of thinking for the following 
reasons The constraints are transient in nature. We either have to adjust the dimen­sion of algebraic 
variables x according to the number of active constraints, or represent all the possible constraints 
and activate or deactivate them al­gebraically.  We found theDAE solver s convergence rate deteriorates 
rapidly with each additional algebraic constraint. Further, if the constraint can not be satis.ed, the 
integrator does not converge.  The algebraic constraints can only represent bilateral constraints. The 
con­straints arising from collisions are unilateral. We would have to extend our scope to Differential 
Variational Inequalities [PS03], which are extension ofDAE that handle inequality constraints on differential 
and algebraicvari­ables.  Instead, we augment theDAE based multi-body formulation inspiredby method­ologies 
proposed by [Mir96] and recently by [Kok04] on impulse dynamics and analytical constraints. We would 
like to give a brief overview of the methodology, along with the details on how we integrate it with 
the DAE framework and some interesting implementation issues. Consider a point constraint pj as depicted 
in Figure 1.3. The trajectory of pj, starting with the initial conditions, can be uniquely de.ned by 
the time varying d acceleration ajnj. As discussed in the previous section, we do not directly eval­uate 
the state derivative vectors y.t+1 in order to integrate the system yt . yt+1. Therefore, we can not 
simply enforce the acceleration constraint, by directly alter­ing the state derivatives y.t+1 as proposedbyKokkevis.We 
enforce the constraint by applying an external force instead. However, we don t use a penalty like force 
formulation. Before every DAE integration step, we analytically determine the precise nature of the force 
fjnj, using the similar methodology as in [Kok04]. The unit constraint direction nj is treated as constant 
and is updated for every in­tegration step. There is a linear relationship between the magnitude of the 
applied d force fj and the resulting desired acceleration a j d 0 aj = .. afjj fj+ aj (1.11) 0 where, 
a is the acceleration in the direction nj before the force is applied. If we j have another constraint 
point pi with force having magnitude fi in the direction ni, the resulting accelerations adi and adj 
will be given by the following linear system d 0 a . ai/. fi . ai/. fjfi a i =+ i (1.12) d 0 a . aj/. 
fi . aj/. fjfj a jj Generalizing, for m such constraints we need to determine the vector of f . Rm unknown 
force magnitudes by solving the following linear system. 0d Kf+ a -a = 0 (1.13) a The Jacobian K. Rm×m 
can be evaluated by applying unit test force at each con­straint and evaluating the changes in accelerations 
at every constraint. An ef.cient procedure to evaluate the Jacobian using the framework of Featherstone 
s Artic­ulated Body Method is given in [Kok04]. The constraint forces thus determined are applied to 
the multi-body system over the next integration step via the external force term f, as discussed in the 
previous section. We replace the constraint directionn by a spatial vector n to generalize the type of 
the constraint that can be represented, including the joint acceleration constraint. We further extend 
the constraint system to include the unilateral constraints such as collisions, friction and joint limits 
by posing it as a Linear Complementarity Problem (LCP). 0 Kf+ a -a d = 0 . f= 0 (1.14) a The LCP states 
that forces f, applied only in positive constraint direction, would strive to make the resulting constraint 
accelerations a equal to desired acceleration a d . However, force fi will be zero if the resulting constraint 
acceleration ai is d greater than desired acceleration ai . We will discuss the signi.cance of the LCP 
formulation when we develop the collision response model in the next section. At .rst, the solution of 
a LCP might appear as a daunting task. However, the iterative LCP methods [CPS92] are surprisingly simple 
and adequate for our pur­pose. [Ken04] gives a gentle introduction to the solution methods based on vari­ous 
matrix splitting techniques. Apart from the simplicity of the implementation, the iterative LCP solvers 
have other advantages as compared to pivoting meth­ods. As we will discuss in the next section, we often 
need to apply multiple constraints on a single link. In this case, the Jacobian K will have linearly 
depen­dent columns. The iterative methods try to distribute the required forces evenly on the link, when 
multiple solutions exists in this case. Secondly, the LCP may not have a solution. The LCP problems arising 
from friction models are often non­convex [Bar92, PT96], particularly for high friction values. Further, 
the Jacobian can be singular or near singular if the limited DOFs of multi-body system does not allow 
motion in a constraint directions. In all these cases, we can bailout early in the solution process and 
still have a .nite and a well distributed solution for the forces. We list an iterative LCP solver in 
Algorithm 2. Apart from the lines 14 and 12 it is a standard successive-over-relaxation linear system 
solver. Line 14 ensures the inequality condition. We add e to the diagonal term in line 12 to make A 
positive de.nite, from potentially positive semi-de.nite, and guard against potentially zero Algorithm2 
sor lcp(A, x, b, . , e , Kiter) Require: A is symmetric, positive semi-de.nite Ensure: w = Ax- b = 0, 
x = 0, x T w = 0 1: x . 0 2: n . dim(x) 3: for k = 1 to Kiter do 4: for i= 1 to n do 5: d . 0 6: for 
j= 1 to i- 1 do 7: d = d + Ai, jxj 8: endfor 9: for j= i+ 1 to n do 10: d = d + Ai, jxj 11: endfor 12: 
d =(bi - d )/(Ai,i + e ) 13: xi =(1- . )xi + .d 14: xi = 0 if xi < 0 15: endfor 16: endfor or near zero 
diagonal terms in the Jacobian K. Further, instead of any elaborate convergence check, we simply make 
.xed number of iterations Kiter, as we know that the solution may not exist. Using forces for enforcing 
the constraints has an advantage here. If the forces are indeterminate, they get projected into the multi-body 
s motion null-space, thus always giving valid con.guration, without any pops in the simulation. Further, 
as the forces are determined analytically, as compared to, say, penalty based formulation, they are small 
for most types of the constraints. Thus they are well within the stability zone of the integrator taking 
4­8 time-steps per frame. The only exception to this is velocity impulse constraint, we will discuss 
this case in detail in the next section. As the constraint may not be satis.ed accurately, we augment 
the constraint acceleration by a proportional­derivative form. To exemplify, for a positional constraint, 
the constraint desired acceleration and the constraint direction be ddd d a = p¨+ Kp(pi - pi)+ Kd(p.i 
- vi) ii dd dd a =. ai ., ni = ai /a (1.15) ii ddd where, p¨i , p.i , p are acceleration, velocity and 
position of the constraint path, and i pi, vi are the current position and velocity of the constraint. 
It is important to remove the effect of the constraint forces applied to multi-body system from the previous 
integration step, and adjust the initial constraint accel­erations a 0 accordingly, before we determine 
the next set of constraint forces. We can use the same procedure that determines the Jacobian Kby method 
of applying test forces for this. 1.3.5 Collision Response We use the unilateral position constraints 
discussed in the previous section to de­velop collision response model for the multi-body system. Collision 
Detection is a mature subject in computer graphics. For brevity, we only enlist the require­ments from 
the collision detection system for our purpose. Between the current con.guration given by the state vector 
yt and extrapolated con.guration using derivative vector y.t and next integration time step h, we .nd 
all the points on the multi-body system that would collide with the obstacles. Figure 1.4 shows two such 
collision positions point pi is already penetrated the obstacle and point pj is about to collide. There 
may be more than one colliding point for a link. Let ni be the collision normal, direction directly away 
from the obstacle, and ai and vi be collision accelerations and velocities respectively, relative to 
the obstacle. We apply collision response in two steps contacts and impacts. We .rst compute the unilateral 
constraints that would prevent collision points from accelerating towards the obstacle. Followed by computation 
of velocity impulses that would prevent collision points from moving towards the obstacle. contacts: 
We decompose the collision acceleration and the collision velocity into the normal components ani, vni 
and tangential components ati, vti. To prevent any acceleration towards the obstacle, we insert a unilateral 
constraint along the col­lision normal direction ni. The unilateral constraint corresponding to the friction 
acts in the tangent plane de.ned by the collision normal. We could sample the tangent plane into discrete 
set of tangents to formulate a complex and numerically expensive friction model based of the discrete 
frictional cone. Instead, taking inputs from [Kok04], we formulate a novel technique as follows. We set 
the uni­lateral constraint direction corresponding to friction as ti = unit( ati + vti/h ) (1.16) If 
both ati and vti is zero, we use previously determined tangent vector for the con­tact. Finally, the 
LCP formulation corresponding to the two unilateral constraints in the direction ni and ti at collision 
position pi is d ani - ani = 0 . fni = 0 µifni - fti = 0 . .i = 0 d (ati - ati)+ .i = 0 . fti = 0 (1.17) 
d We set the desired normal accelerationani proportional to the penetration depth di if the point is 
penetrating, see equation 1.15, or zero if the collision point is outside d the obstacle. The desired 
tangential acceleration a is set to (-. vti . /h). The ti LCP formulation will compute required amount 
of normal force fni to remove the normal acceleration ani. The tangential force fti will be at most µifni, 
and try to remove any tangential non-zero velocity component the dynamic friction case, or if the tangential 
velocity is zero it will try to remove anytangential acceleration the static friction case. impacts: 
We use impulses to arrest the collision normal velocity vni. Only those contacts are considered that 
have the normal velocity component vni < 0. For the impulse computations we can use the same acceleration 
constraints discussed in d the previous section by setting a = -(1+ . )vni, where . is collision restitution. 
ni Instead of applying potentially large forces, we alter the joint velocities q.t. This would invalidate 
the consistent solution set {yt, y.t}. We should correct q¨corre­spondingly. In reality, we found that 
the solver is tolerant to the error. 1.3.6 Implementation Details Having developed the theoretical framework 
in the last three sections, in this sec­tion we would like to give a brief overview of the Oriented Strands 
system mod­eled as dynamics of multi-body system. It is implemented as a plug-in to Maya, as well as 
plug-in to our proprietary animation system.We useDASPK [BCP96] for our implementation. Along with the 
robust formulation, any physically based simulation system to be successful in production environment 
needs to have an important aspect to be able to art direct. In the following subsections, we develop 
some novel concepts towards that, along with few important implementation details. In our opinion, choice 
of reduced coordinate formulation and dynamics expressed in local frames makes some of these concept 
easier to implement. 1.3.7 SimulationParameters andWork.ow The dynamic strand is composed from a hierarchy 
of input curves de.ned in a local frame, that de.nes the initial rest shape of the corresponding multi-body 
system. We provide the user with high-level control over the direct numerical simulation by means of 
relevant physical parameters of the dynamic strand, such as mass per unit length, strand radius, bend 
stiffness/damping, torsional stiff­ness/damping, gravity, air drag. The user can animate all the parameters 
and specify their length-wise anisotropic variation. The collision parameters collision restitution and 
static/dynamic friction are de.ned per obstacle surface. The strand may have additional anisotropic weights 
over collision parameters, along with their length-wise variation. 1.3.8 Stiffness Model and ExternalForces 
In Section 1.3.3, whiledeveloping theDAE based formulation, we assumed single-DOF joints for the simplicity 
of discussion. However, we use three-DOF spher­ical joint in the implementation of Oriented Strands. 
The joint variable of ith joint is expressed as a quaternion qi . R4 and the joint velocity as a vector 
wi . R3. [Had03, Fea87] gives details on how to formulate multiple-DOF joints. We decompose the quaternion 
de.ning the relative transformation between two links into a twist component .t around the local y-axis 
and a pure bend component .b around a bend axis b. We provide a nonlinear stiffness model as follows 
Qb = Kb(b) b tan((.b - .b0)/2) Qt = Kt y tan((.t - . 0)/2) (1.18) t where .b 0 and . 0 correspond to 
the rest con.guration. Kt is torsional stiffness t coef.cient and Kb(b) is anisotropic bend stiffness 
coef.cient. The response model is almost linear for small deformations. However, the non-linear response 
prevents excessive deformations and potentially joints snapping. We support general external force .elds 
using the plug-in architecture of Maya and that of our proprietary animation system. The user can attach 
any complex combination of time-varying .elds such as wake, turbulence, .uid simulations and general 
event driven scripted force .elds. The user can further specify length-wise anisotropic weights for the 
external force .elds. The user can optionally include these .elds in computing the Jacobians numerically 
discussed in Section 1.3.3. 1.3.9 Accurate Acceleration of BaseJoint In the reduced coordinate formulation 
it is critical to compute and supply the accurate velocities and accelerations of the base joint s prescribed 
motion path. We could have evaluated them numerically, however that would mean making repetitive evaluations 
of motion system at sub-frame interval, which is typically very expensive. Instead we interpolate the 
rigid-body transformation from four successive frames. Constructing a C2 continuous curve that interpolates 
a number of prescribed rotations is a well studied problem. We use the method developed by [PK96], where 
we construct a piecewise cubic curve whose coef.cients ai, bi, ci are members of so(3). The rotation 
is evaluated by taking the matrix exponential of this polynomial. 1.3.10 Time Scale and Local Dynamics 
Often the dynamical simulation are encountered with very extreme and brisk an­imated motions. Although 
a robust formulations will be able to cope with the scenario, often the directors would want the motion 
to be selectively less violent. We introduce time scale ß to control the amount of energy pumped in the 
system. It is a factor by which velocity and acceleration of the base joint get scaled and is typically 
between zero and one, however the user can set it more than one to accentuate the motion. The local dynamics 
. is another similar parameter which blends out velocity and acceleration of some local dynamics reference 
frame. a 0 = ß (a 0 - . a ref ) v 0 = ß (v 0 - . v ref ) (1.19) One scenario that is frequent is, a 
braid or long hair that .y away when character starts running or rides a horse. The local dynamics reference 
frame is simply set to the character s hip joint, and with appropriate local dynamics parameter one can 
control the amount of .yaway the user wants. 1.3.11 Posable Dynamics Ears and tail, often have free 
secondary dynamic motion when the animator lets them loose . However, animator would want to hit a certain 
pose at a certain time to make the character expressive. We tried different techniques that are based 
on the motion control principle. However, it did not give desired results. For high values of Kp and 
Kd in the PID controller (Equation 1.15), the constraint follows the goal rather exactly. If we reduce 
Kp and Kd, due to slew rate, the PID controller gave a large error in achieving pose and the solution 
oscillated a lot before coming to rest to the animated pose. Surprisingly a very simple model worked 
for this speci.c application. We insert a spring between the dynamic strand and the desired animated 
pose at tip of each segment, to give a penalty based soft constraint. The user can animate the stiffness 
and damping, namely pose strength and pose damping to achieve the Posable Dynamics. 1.3.12 Zero-gravity 
Rest Shape The rest shapes of the dynamic strands are typically modeled taking the gravity into account. 
Intricate hairstyle is a good example of this. Unfortunately, when we start simulating hair, the strands 
would sag under the gravity before they .nally settles down. This would change the original art directed 
hair shape depending on the stiffness. Increasing the stiffness to preserve the shape would give unre­alistic 
motion. One can go back and try to edit the rest shape so as to achieve desired shape under gravity. 
However, this would be very laborious and iterative process. We developed a technique to automatically 
compute the equivalent rest shape, without gravity, so that under gravity we would get the desired shape. 
The problem is a straight forward inverse dynamics problem in robotics. Given a set of external forces 
(gravity) and given the desired con.guration of multi-body sys­tem, inverse dynamics problem .nds set 
of joint forces required to achieve certain joint accelerations, zero in our case. We refer to [Fea87] 
for the details. We would like to highlight that it would be dif.cult to formulate this in the case of 
maximal coordinate formulation. 1.3.13 Strand-strand Interaction Strand-strand interaction is not important 
in some simulation scenarios such as foliage motion, braids and ears, whereas it is critical in certain 
cases such hair simulation. We have implemented a modular plug-in .eld to Maya that computes the .uid 
like forces on a continuum, that can be attached to the Oriented Strands system to realize the strand-strand 
interactions as introduced by [HMT01b]. The other interesting approaches to handle strand-strand interactions 
include wisp level interactions [PCP01b, BKCN03b], layers [LK01b] and strips [CJY02b]. We demonstrate 
the effectiveness of the proposed Oriented Strand methodology, through impressive results in production 
of Madagascar and Shrek The Third at PDI/DreamWorks, in Section 5.1. 1.4 Super-Helices: a compact modelfor 
thin geometry The Super-Helix model is a novel mechanical model for hair, dedicated to the ac­curate 
simulation of hair dynamics. In the spirit of work by Marschner et al. in the .eld of hair rendering 
[MJC+03a], we rely on the structural and mechanical fea­tures of real hair to achieve realism. This leads 
us to use Kirchhoff equations for dynamic rods. These equations are integrated in time thanks to a new 
deformable model that we call Super-Helices: A hair strand is modeled as a C1 continuous, piecewise helical1 
rod, with an oval to circular cross section. We use the degrees of freedom of this inextensible rod model 
as generalized coordinates, and derive the equations of motion by Lagrangian mechanics. As our validations 
show, the resulting model accurately captures the nonlinear behavior of hair in motion, while ensuring 
both ef.ciencyand robustness of the simulation. This work was published at SIGGRAPH in 2006 [BAC+06], 
and results from a collaboration with Basile Audoly, researcher in mechanics at Universite Pierre et 
Marie Curie,Paris6, France. 1Ahelix is a curve with constant curvatures and twist. Note that this de.nition 
includes straight lines (zero curvatures and twist), so Super-Helices can be used for representing anykind 
of hair. 1.4.1 The Dynamics of Super-Helices  We shall .rst present the model that we used to animate 
individual hair strands (guide strands). This model has a tunable number of degrees of freedom. It is 
built upon the Cosserat and Kirchhofftheories of rods. In mechanical engineering literature, a rod is 
de.ned as an elastic material that is effectively one dimensional: its length is much larger than the 
size of its cross section. Kinematics We consider an inextensible rod of length L. Let s . [0, L] be 
the curvilinear abscissa along the rod. The centerline, r(s, t), is the curve passing through the center 
of mass of every cross section. This curve describes the shape of the rod at a particular time t but 
it does not tell how much the rodtwists around its centerline. In order to keep track of twist, the Cosserat 
model introduces a material frame ni(s, t) at every point of the centerline2. By material, we mean that 
the frame .ows along with the surrounding material upon deformation. By convention, n0 is the tangent 
to the centerline: ' r (s, t)= n0(s, t), (1.20a) 2By convention, lowercase Latin indices such as i are 
used for all spatial directions and run over i = 0, 1, 2 while Greek indices such as a are for spatial 
directions restricted to the plane of the cross section, a = 1, 2. while (na )a =1,2 span the plane of 
the cross section, see Figure 1.6, left. We use ' primes to denote space derivatives along the center 
line, f = . f/. s, while the overstruck notation is for time derivatives, f.= df/dt. The Kirchhoff model 
for elastic rod starts from this mathematical description of a Cosserat curve and adds the physical requirement 
of inextensibility and unsheara­bility. In this case, the frame (ni(s))i=0,1,2 is orthonormal for all 
s, and there exists a vector O (s, t), called the Darboux vector, such that: ' ni(s, t)= O (s, t) × ni(s, 
t) for i= 0, 1, 2. (1.20b) Appropriate boundary conditions must be speci.ed: one end of the hair strand, 
s = 0, is clamped into the head while the other end, s = L, is free. The position of the clamped end, 
together with the orientation of the initial frame, are imposed by head motion (an input in the simulations): 
 r(0, t)= rc(t) (1.20c) ni(0, t)= ni,c(t) for i= 0, 1, 2, where subscript c refers to the clamped end 
of the rod, s = 0. The rod s material curvatures (.a (s, t))a =1,2 with respect to the two directions 
of the cross section and the twist t (s, t) are de.ned as the coordinates of the vector O (s, t) in the 
local material frame: O (s, t)= t (s, t) n0(s, t)+ .1(s, t) n1(s, t)+ .2(s, t) n2(s, t). (1.21) By introducing 
a redundant notation for the twist, .0 = t , we can refer to these parameters collectively as (.i(s, 
t))i=0,1,2. Reconstruction, generalized coordinates The degrees of freedom of a Kirchhoff rod are its 
material curvatures and twist (.i(s, t))i=0,1,2. A continuous model being of little use for computer 
animation, we introduce a spatial discretization as follows. Let us divide the strand s . [O, L] into 
N segments SQ indexed by Q(1= Q= N). These segments may have different lengths, and N is an arbitrary 
integer, N = 1. We de.ne the material curvatures and twist of our deformable model with piecewise constant 
functions over these segments. We write qi,Q(t) the constant value of the curvature .i (for i = 1, 2) 
or twist .0 = t (for i = 0) over the segment SQ at time t. Therefore, an explicit formula for the material 
curvatures and twist reads N .i(s, t)= .Q=1qi,Q(t) .Q(s) (1.22) where .Q(s) is the characteristic function 
of segment Q, equal to 1 if s . SQ and 0 otherwise. We collect the numbers qi,Q(t) into a vector q(t) 
of size 3N, which we call the generalized coordinates of our model. These generalized coordinates q(t) 
can be used to reconstruct the rod shape at any given time. Indeed, plugging equation (1.22) into equation 
(1.21), and then equa­tion (1.21) into equations (1.20a c) yields a differential equation with respect 
to s. By integration of this equation, one obtains the centerline r(s) and the material frames ni(s) 
as a function of s and q(t). This process, called the reconstruction, can be carried out analytically; 
as explained in Appendix 1.4.3, the integration with respect to s has a symbolic solution over every 
segment SQ. By patching these solutions, we .nd that our model deforms as a helix over every segment 
SQ and, moreover, is C1-smooth (between adjacent helices, both the centerline and the material frames 
are continuous). This is why we call this model a Super- SHSH Helix. We write r (s, q) and ni (s, q) 
as the parameterization of the Super-Helix in terms of its generalized coordinates q. In Appendix 1.4.3, 
we explain how these SH functions r SH and n can be obtained in symbolic form. i Imposing a uniform value 
to the material curvatures and twist over the hair length would make it deform as a plain helix. This 
is indeed what happens when one chooses the coarsest possible spatial discretization, that is N = 1. 
For other values of N, the rod is made of several helices patched together. Large values of N yield arbitrarily 
.ne space discretizations. Dynamic equationsfora Super-Helix Given a deformable body whose con.guration 
depends on generalized coordinates q(t), Lagrangian mechanics provides a systematic method for deriving 
its equa­tions of motion, q¨= a(q, q., t). This is done by feeding the Lagrangian equations of motion: 
. T lL d . T .U . D - ++ = JiQ(s, q, t) · F(s, t) ds (1.23) dt. q.iQ. qiQ . qiQ . q.iQ 0 with the expressions 
for the kinetic energy T(q, q., t), for the internal energyU(q, t) and for the dissipation potential 
D(q, q., t) that describe the physics of the system at hand. The right-hand side of equation (1.23) is 
the generalized force fiQ de­riving from the lineic density F(s, t) of physical force applied on the 
rod, and JiQ SH de.nes the Jacobian matrix, JiQ = . r (s, q)/. qiQ. We consider three force con­tributions, 
namely hair weight, viscous drag from ambient air (considered at rest for simplicity) with coef.cient 
. , and interaction forces with surrounding strands and body: SH F(s, t)= . Sg- . r.(s, q)+ Fi(s, t), 
(1.24a) where F(s, t) is the total external force applied to the rod per unit length, . S is the mass 
of the rod per unit length, and gis the acceleration of gravity. The interaction forces Fi are computed 
using the model presented shortly in Section 1.4.2. The three energies in the equations of motion (1.23) 
that are relevant for an elastic rod are: l L 1 (SH)2 T(q, q., t)= . Sr.(s, q)ds (1.24b) 20 l L 2 U(q, 
t)= 1 .2 (EI)i(. SH(s, q) - .i n(s))ds (1.24c) 20 i=0i l L 1 2 (. SH )2 D(q, q., t)= . ..(s, q)ds. (1.24d) 
20 i=0 i The kinetic energy T is de.ned in terms of the rod velocity, r.= dr/dt in the classical way. 
The internal energy U in equation (1.24c) is the elastic energy of a rod, as derived, for instance, in 
[AP07] and used in [BAQ+05]. The coef.cients (EI)i are the principal bending stiffness of the rod in 
the directions ni (for i= 1, 2) while (EI)0 is the torsional stiffness, classically written µ J (for 
i = 0). These parameters are given by textbook formulas in terms of the material properties (Young s 
modulus and Poisson s ratio) and of the geometry of the cross-section. The quantities .i n(s) are called 
the natural curvatures (i = 1, 2)and twist(i= 0)of the rod. Theycharacterize the shape of the rod in 
the absence of external force: for .i(s)= . n(s) the elastic energy is vanishing and therefore minimum. 
Vanishing i natural curvatures(.a n = 0 for a = 1, 2) model straight hair. Nonzero values will result 
in wavy, curly or fuzzy hair. In practice, tuning these parameters allows one to choose for the desired 
hair style, as explained in Section ??. Overall, the mechanical properties of the rod are captured by 
only six entities, the stiffnesses (EIi)i=0,1,2 and the natural twist and curvatures (. n(s))i=0,1,2. 
We neglect the i dependence of the stiffnesses on s,but not that of the natural twist and curvatures: 
we found that slight variations of (. n(s))i with s allow for more realistic hair i styles. Finally, 
we choose for the dissipation energy Din equation (1.24d) a simple heuristic model for capturing visco-elastic 
effects in hair strands, the coef.cient . being the internal friction coef.cient. All the terms needed 
in equation (1.23) have been given in equations (1.24). By plugging the latter into the former, one arrives 
at explicit equations of motion for the generalized coordinate q(t). Although straightforward in principle, 
this calculation is involved3. It can nevertheless be worked out easily using a symbolic calculation 
language such as Mathematica [Wol99]: the .rst step is to implement the reconstruction of Super-Helices 
as given in Appendix 1.4.3; the second step is to work out the right-hand sides of equations (1.24), 
using symbolic integration whenever necessary; the .nal step is to plug everything back into equation 
(1.23). This leads to the equation of motion of a Super-Helix: l L n M[s, q] · q¨+ K · (q- q )= A[t, 
q, q.]+ JiQ[s, q, t] · Fi(s, t) ds. (1.25) 0 In this equation, the bracket notation is used to emphasize 
that all functions are given by explicit formula in terms of their arguments. In equation (1.25), the 
inertia matrix M is a dense square matrix of size 3N, which depends nonlinearly on q. The stiffness matrix 
K has the same size, is diagonal, and is .lled with the bending and torsional stiffnesses of the rod. 
The vector q n de.nes the rest position in generalized coordinates, and is .lled with the natural twist 
or curvature . n of the rod over element labelled Q. Finally, i the vector A collects all remaining terms, 
including air drag and visco-elastic dissipation, which are independent of q¨and may depend nonlinearly 
on q and q.. Time discretization The equation of motion (1.25) is discrete in space but continuous in 
time. For its time integration, we used a classical Newton semi-implicit scheme with .xed time step. 
Both the terms q¨and q in the left-hand side are implicited. Every time step involves the solution of 
a linear system of size 3 N. The matrix of this linear system is square and dense, like M, and is different 
at every time step: a 1 .. '' 3The elements of M, for instance, read MiQ,i ' Q ' = 2 JiQ(s, q) · Ji ' 
Q ' (s , q) dsds where Jis the gradient of r SH(s, q) with respect to q. conjugate-gradient algorithm 
is used. The density of M is the price to be paid for incorporating the inextensibility constraint into 
the parameterization. It results in degrees of freedom that are non local in physical space. Super-Helicesfor 
solving the Kirchhoff equations The equations of motion for dynamic elastic rods were derived by Kirchhoff 
in 1859. A modern derivation of these equations can be found, for instance, in [AP07]: it follows the 
same principles as the one for a Super-Helix. The main difference is that we have constrained the material 
curvatures and twists to be piecewise constant functions of s in equation (1.22); these functions may 
depend arbitrarily on s for regular Kirchhoff rods. Apart from this difference, the Super-Helix model 
is based on the same physical assumptions as the Kirchhoff equa­tions. Therefore, the Super-Helix method 
provides a discrete model for solving the Kirchhoff equations. We derived the Super-Helix model after 
we extensively tested existing integra­tion schemes for the Kirchhoff equations, and eventually realized 
that they were not well suited for computer graphics applications. We implemented an elegant algorithm, 
due to [HKS98], based on a discretization of these equations by .­nite differences. In this paper, Hou 
et al. discuss very clearly the dif.culties associated with the numerical integration of the Kirchhoff 
equations, which are numerically very stiff. They propose an attempt for removing this stiffness. It 
bringsavery signi.cant improvementover previous methodsbut we found thatit was still insuf.cient for 
hair animation purposes: there remain quite strong con­straints on the time steps compatible with numerical 
stability of the algorithm. For instance, simulation of a 10 cm long naturally straight hair strand using 
the algorithm given in [HKS98] remained unstable even with 200 nodes and a time step as low as 10-5 s. 
The stiffness problems in nodal methods have been ana­lyzed in depth by [BW92] who promoted the use of 
Lagrangian deformable mod­els (sometimes called global models as opposed to nodal ones). This is indeed 
the approach we used above to derive the Super-Helix model, in the same spirit as [WW90, BW92, QT96]. 
We list a few key features of the Super-Helix model which contribute to realis­tic, stable and ef.cient 
hair simulations. All space integrations in the equations of motion are performed symbolically off-line, 
leading to a quick and accurate evaluation of the coef.cients in the equation of motion at every time 
step. The inextensibility constraint, enforced by equations (1.20a 1.20b), is incorporated into the reconstruction 
process. As a result, the generalized coordinates are free of any constraint and the stiff constraint 
of inextensibility has been effectively re­moved from the equations. Moreover, the method offers a well-controlled 
space discretization based on Lagrangian mechanics, leading to stable simulations even for small N. For 
N . 8, the Kirchhoff equations are recovered, making the sim­ulations very accurate. By tuning the parameter 
N, one can freely choose the best compromise between accuracy and ef.ciency, depending on the complexity 
of hair motion and on the allowed computational time. We are aware of another Lagrangian model4 used 
in computer graphics that provides an adjustable number of degrees of freedom, namely the Dynamic NURBS 
model [QT96], studied in the 1D case by [NR01]. Finally, external forces can have an arbitrary spatial 
de­pendence and do not have to be applied at speci.c points such as nodes, thereby facilitating the combination 
with the interaction model. 1.4.2 Applications andValidation In this section, we provide a validation 
of our physical model against a series of experiments on real hair, and demonstrate that the Super-Helix 
model accurately simulates the motion of hair. Images and videos showing our set of results are available 
at http://www-evasion.imag.fr/Publications/2006/BACQLL06/. Choosing the parameters of the model In our 
model, each Super-Helix stands for an individual hair strand placed into a set of neighboring hair strands, 
called hair clump, which is assumed to deform continuously. To simulate the motionofagiven sampleof hair, 
which can either be a hair wisp or a full head of hair, we .rst deduce the physical and geometric parameters 
of each Super-Helix from the structural and physical properties of the hair strands composing the clump. 
Then, we adjust friction parameters of the model according to the damping observed in real motion of 
the clump. Finally, interactions are set up between the Super-Helices to account for contacts occurring 
4In this model, geometric parameters, de.nedby the NURBS control points and the associated weights, are 
used as generalized coordinates in the Lagrangian formalism. In contrast, we opt here for mechanically-based 
generalized coordinates: they are the values of the material curvatures and twist, which are the canonical 
unknowns of the Kirchhoff equations. between the different animated hair groups. In this section, we 
explain how we set all the parameters of the Super-Helix model using simple experiments performed on 
real hair. Hair mass and stiffness: We set the density . to be equal to a typical value for hair,1.3g· 
cm -3. The mean radius r and the ellipticity e = of the Super-Helix rmin cross-section are deduced by 
direct microscopic observation of real hair .bers (see Figure 1.7, left) whereasYoung s modulus and Poisson 
s ratio are taken from existing tables, which report values for various ethnic origins [Rob02]. These 
pa­rameters are then used to compute the bending and torsional stiffnesses (EI)i=0,1,2 of the Super-Helix, 
as given by textbook formulas. Natural curliness: The natural curvatures and twist parameters of the 
Super-Helix model are set by: .h .1n = 1/rh . n = 0 t n = , 2 2 2p r h where rh is the radius and .h 
the step of the approximate helical shape of the real hair clump, measured near the tips (see Figure 
1.7, right). Indeed, the actual curvatures and twist should be equal to their natural value at the free 
end of the rod, where the role of gravity becomes negligible. In practice, we add small random variations 
to these values along each Super-Helix to get more natural results. We have noted that in reality, most 
hair types have an almost zero natural twist t n, except African hair (see Appendix 1.4.4). Internal 
friction . : This parameter measures the amount of internal dissipa­tion within a Super-Helix during 
motion. It especially accounts for the hair-hair dissipative interactions occurring inside the hair clump 
whose motion is guided by the Super-Helix. We found that, in practice, the internal friction can be easily 
adjusted by comparing the amplitude of deformation between the real and the sim­ulated hair clump when 
a vertical oscillatory motion is imposed, see Figure 1.8. , 5.10-113 -1 Typically, we obtained best results 
with . . [5.10-10 ] kg· m · s. Air-hair friction coef.cient: Once parameter . is chosen, the air-hair 
friction parameter can be .tted by comparing the damping duration between the real and the simulated 
hair clump, for example when imposing a pendulum motion. We noted the air-hair friction parameter is 
strongly related to the local alignment of neighboring hair strands, called the hair discipline in the 
.eld of cosmetics. As one can observe in the real world, fuzzy hair is more subject to air damping than 
regular, disciplined hair. In practice, we chose the air-hair friction coef.cient . between 5.10-6 kg· 
(m· s)-1 (disciplined hair) and 5.10-5 kg· (m· s)-1 (fuzzy hair). Friction with another object: Contacts 
between hairs, and between our hair model and external objects (such as the body) are performed through 
penalty forces which include a normal elastic response together with a tangential viscous friction force. 
For simulating realistic contacts between hair and external objects, we use an anisotropic friction force, 
which accounts for the oriented scales cov­ering individual hair .bers. The friction parameter is directly 
adjusted from real observations of sliding contacts between the hair clump and a given material, and 
then multiplied by a cosine function to account for the orientation of hair .bers with respect to their 
sliding motion over the external object. Visual comparisons With simulation we have reproduced a series 
of real experiments on smooth and wavy hair clumps to show that our model captures the main dynamic features 
of 3 for different values of . . In this case, . = 1.10-10 kg· m · s -1 gives qualitatively similar 
results. natural hair. We used the technique presented previously to .t the parameters of the Super Helix 
from the real manipulated hair clump. As shown in Figure 1.9, left, our Super-Helix model adequately 
captures the typical nonlinear behavior of hair (buckling, bending-twisting instabilities), as well as 
the nervousness of curly hair when submitted to high speed motion (see Figure 1.8, left). Figure 1.9, 
right, shows the fast motion of a large hair, which is realistically simulated using 3 interacting Super-Helices. 
All these experiments also allowed us to check the stability of the simulation, even for high speed motion. 
Finally, Figure 1.10 demonstrates that our model convincingly captures the com­plex effects occurring 
in a full head of hair submitted to a high speed shaking motion. Results and simulation performance 
Figure 1.5 shows three examples of motion for a full head of hair. Different hair types were simulated, 
from long to short and curly to straight. To set up our simulations, we used typical parameter values 
for real hair of different ethnic ori­gins. These parameters are given in Appendix 1.4.4. We used one 
hundred guide strands for the wavy and curly hairstyles, and two hundred for the smooth Asian hairstyle. 
For all hair types, even long or curly ones, we found it to be unnecessary to use more than 5 to 10 helical 
elements per guide hair strand. For higher values of N, the increase in accuracybecomes imperceptible. 
Our model was tested on a 3 GHz Pentium 4 processor. Up to 10 strands can be simulated in real-time. 
When simulating a full head of hair, we obtained a very reasonable mean computational timeof0.3sto3 sper 
frame. The performanceof our implementation is thus as good as other recent approaches, such as [CCK05a]. 
This is due to the stability of the Super-Helix model, which allows time steps of 1/30 s, even during 
high speed motion, and to the high order of interpolation providedby the helices, which helps tokeep 
N small while offering a good accu­racy. Limitations and future work The Super-Helix model remains stable 
for any number N of helical elements in guide strands. However, the matrix M used in the dynamic computation 
is dense, and as a result, the computation time increases quickly with N, as O(N2). This quadratic time 
complexity prevents the use of Super-Helices for a very .ne sim­ulation. However, this proves to be a 
minor concern for hair animation purposes, as we .nd N doesnothavetobeverylargefor generating pleasant 
visual results. Moreover, once the number of helical parts is chosen, the complexity of the whole simulation 
remains linear with respect to the number of guide strands. Besides this, constraints are currently treated 
using penalty methods. Analytical methods would be very useful, as they would allow solid friction to 
be handled. This is one of the planned future extensions of the model. Although we could advance in the 
understanding on collective hair behavior, not enough data were available for us to set up the really 
strong model we would have dreamed of. Indeed, processing non-simulated hair strands by a simple interpo­lating 
scheme between a .xed set of sparse guide hair strands may lose .ne-scale details; moreover, when thin 
objects interact with such sparse hair strands, the coarse granularity of hair may become obvious and 
distracting. Quantifying the tendencyof hair to cluster and separate according to the hair type as well 
as to the collisions occurring between hair and external objects would be a very interesting avenue for 
future work. The relationship between this and the intuitive notions of curliness and discipline could 
be investigated. 1.4.3 Conclusion We have introduced a deformable model able to simulate hair dynamics 
for a wide range of hair types, capturing the complex motions observed in real hair motions. In particular, 
the simulation of curly hair, a notoriously dif.cult problem, has been demonstrated. Super-Helices are 
based on Kirchhoff equations for elastic, inextensible rods and on Lagrangian dynamics, and provide a 
freely adjustable number of degrees of freedom. They take into account important hair features such as 
the natural curvature and twist of hair strands, as well as the oval shape of their cross section. To 
stress on the powerful representation of moving hair by Super-Helices, we have presented a rigorous validation 
of this model, supported by a series of comparative experiments on real hair. We also noted that Super-Helices 
are able to achieve realistic motions at a very reasonable computational cost: this is permitted by the 
stability of the method, which enables large time steps, and by the high order of interpolation provided 
by the helices. An interesting direction for future research would be to adapt our hair model to a real-time 
framework, in order to perform interactive hair-styling operations or to use it for character animation 
in video-games. We could think of setting up an adaptive version of the Super-Helices model, where the 
number of helical parts would automatically vary over time according to the current deformation and to 
the available computational power, following work in articulated body dynam­ics [RGL05a]. Appendix Helical 
solution We show here that the reconstruction of the rod can be carried out over any particular LR element 
SQ =[sQ, s ] of the Super-Helix, over which the functions (.i(s))i are constant Q ' by construction. 
By equations (1.20), O ' = .i.i ni + O × O = 0, which means that the Darboux vector is constant along 
each element. For a given element Q, let us therefore introduce O the norm of the vector O and . = O 
/O the unit vector aligned with O (the case O = 0 is considered separately, see below). Finally, we write 
a . =(a · . ). and . a = a- a . as the projection of an arbitrary vector a parallel to and perpendicular 
to the axis spanned by . , respectively. Since O is constant, integration of equation (1.20b) over an 
element is straightforward. The material frame rotates around . with a constant rate of rotation O per 
unit of curvi­linear length. Therefore, the material frame at coordinate s . SQ is obtained from the 
material frame n Q = ni(s L) given on the left-hand side of the interval SQ, by a rotation i,L Q L with 
angle O (s- sQ) and axis parallel to . : Q. Q. QQ. Q ni(s)= n + n cos(O (s- s )) + . × n sin(O (s- s 
)). (1.26a) i,L i,LLi,LL By equation (1.20a), the centerline r(s) is then found by spatial integration 
of n0(s): QQ sin(O (s- s )) 1- cos(O (s- s )) QQ. QQ. LQ. L r(s)= r + n (s-s )+ n +. ×n , (1.26b) L0,LL0,L0,L 
OO Q L where rL = r(sQ) is the prescribed position of the centerline on the left-hand side of the interval. 
Equations (1.26) provide the explicit reconstruction of an element. Its centerline is a helix with axis 
parallel to . . An equivalent derivation based on Rodrigues formula is given in [Pai02a]. Two degenerate 
cases are possible and must be considered separately: the curve is an arc of circle when t = 0 and .1 
0 or .2 0; it is a straight line when == .1 = .2 = 0, which can be twisted(t = 0, implying O = = 0) or 
untwisted(t 0). Equations (1.26) can be used to propagate the centerline and the material frame from 
the LR left-hand side s of the element to its right-hand side sQ. The whole rod can then be re- Q constructed 
by applying this procedure over every element successively, starting from the scalp where r and ni are 
prescribed by equation (1.20c). This yields explicit formulae for SHSH the functions r (s, q) and ni 
(s, q), which have the form of equation (1.26) over each el­ement. The integration constants are determined 
by continuity at the element boundaries. 1.4.4 Parameter valuesfor natural hair Radius (µ m) Asian (smooth) 
50 Caucasian 1 (wavy) 35 Caucasian 2 (curly) 50 African (fuzzy) 50 Ellipticity 1 1.1 1.1 1.2 Helix radius 
(cm) 0 1 0.6 0.1 Helix step (cm) 0 0.5 0.5 1 Young s mod. (GPa) 1 2 1.5 0.5 Poisson s ratio 0.48 0.48 
0.48 0.48 Chapter2 Hair Interactions Florence Bertails, Basile Audoly, Marie-Paule Cani 2.1 Introduction 
Human hair is a composite, deformable material made of more than 100 000 in­dividual .bers called hair 
strands. As mentioned in the previous chapter, these thin tubular structures are elastic: after motion, 
they tend to come back to a rest shape, which is related to their individual natural curliness and to 
the set of exter­nal forces applied to them. This chapter deals with the dif.cult problem of hair interactions, 
which plays a major role in the motion of a full head of hair, and even on the shape hair takes at rest: 
collisions and contacts between hair strands of different orientations cause hair to occupy a pretty 
high volume, especially in the case of irregular, curly or fuzzy hair. Due to this larger volume, tangled 
or fuzzy hair in motion is much more subject to air damping than smooth and disciplined hair. The nature 
of interactions between hair strands is very complex. This is largely due to the surface of individual 
hair strands, which is not smoothbut composed of tilted scales (see Figure 1.1, left). This irregular 
surface causes anisotropic fric­tion inside hair, with an amplitude that strongly depends on the orientation 
of the scales and of the direction of motion [Zvi86]. Moreover, hair is very triboelectric, meaning it 
can easily release static charges by mere friction. This phenomenon, which has been measured in the case 
of combed hair, most probably impacts the hair-hair friction rates. Because of the extremely large number 
of strands that compose a full head of hair, processing hair interactions is known as one of the main 
challenges in hair anima­tion. Until the late nineties, most hair animation methods tackled hair collisions 
with the body,but were not processing self-interactions at all. This often resulted into an obvious lack 
of hair volume. The .rst methods that detected interactions between hair wisps spent more than 80% of 
the simulation time in this process. More recently, several interesting solutions that make hair interactions 
much more practical were developed: some of them mimic the effect of hair interactions glob­ally, using 
a structure that stores the volumetric density of hair. Others achieve more accurate results by developing 
ef.cient algorithms for detecting collisions between hair-wisps and by setting up realistic models for 
response and friction forces. This chapter presents those of these recent advances in which the authors 
partic­ipated: Section 2.2 brie.y reviews the two main approaches for animating hair, namely modeling 
hair as a continuum or as a set of individual hair wisps. The as­sociated methods for processing hair 
interactions with the body are presented and the issues raised by hair self-interactions are introduced. 
Section 2.3 presents a practical real-time solution, applicable in any hair animation system, which gives 
hair a volumetric behavior without requiring to detect individual interactions be­tween the animated 
guide-strands. We then focus on more accurate methods, ap­plicable for generating high quality animation 
of long hair: Section 2.4 reviews some recent methods for ef.ciently, yet robustly detecting the interactions 
be­tween guide-strands. Section 2.5 discusses the anisotropic models that were set up to model response 
to these interactions. In particular, we describe a validated model for friction forces. In conclusion, 
we emphasize the steps forwards made in the last few years, but also the issues that were not tackled 
yet, showing that improving the ef.ciencyand visual realism of hair animation is going to remain a hot 
research topic for a while. 2.2 Hair animation and interaction processing 2.2.1 Continuous versus wisp-based 
hair models Hair animation was made practical in the early nineties [RCT91b] by the idea of animating 
only a sub-set of the hair strands (typically one or two hundreds), which we will call here the guide-strands. 
This is made possible by the local spatial coherence of hair motion. Once the guide-strands have been 
animated (using for instance spring and masses, projective dynamics or chains of articulated rigid bodies), 
their position is used to generate the remaining hair strands at the rendering stage. More precisely, 
two main families of approaches were developed for modeling hair: The .rst ones, more appropriate for 
smooth, .uid hair, consider hair as a continuum [AUK92b, DMTKT93, HMT01a, CJY02a, BCN03] and thus use 
interpolation between the animated guide-strands for generating a full head of hair. The second ones, 
which achieve their best results for wavy of curly hair, model hair as a set of disjoint wisps [CSDI99, 
KN00, PCP01a, KH01, BKCN03a, WL03, CCK05a]. The animated guide-strands are assimilated to wisp skeletons 
and extrapolation is used for generating extra hair-strands within each wisp. Re­cently, Bertails [BAC+06] 
bridged the gap between the two kinds of approaches by allowing the guide-strands to be used both for 
interpolation or approximation depending on the type of hair and on the current distance between neighboring 
guide-strands. This model captures hair that looks like a continuum near the head while well identi.ed 
wisps can be observed at the tip. In the remainder of this chapter, we will discuss hair interactions 
independently of the hair model used among the approaches above: hair will be considered as a set of 
individual hair guides, each of them more or less explicitly modeling a volume of hair around it. Interactions 
will be detected and treated based on the position and motion of these guide-strands. 2.2.2 Processing 
hair interactions with the body The .rst step towards processing hair interactions is to adequately model 
hair col­lisions and contacts with obstacles, starting with the body of the animated char­acter. Since 
hair is animated using guide-strands, the latter and the wisp volumes around them (if any) should be 
prevented from penetrating inside the body. The latter is often approximated using sets of ellipsoids 
or stored in a spatial partition­ing grid to accelerate this detection. Since hair is a very soft material, 
modeling a one way response is suf.cient: the body can be considered as in.nitely rigid and heavy compared 
with hair, so the collision has no effect on the subsequent body shape and motion. Moreover, hair is 
a very soft and light material: it does not bounce after collision, but rather experiment a strong static 
friction with the parts of the body it is in contact with. Collision response can thus be treated using 
methods set up for other very light material, such as clothing: when a penetration is detected, the guide-strand 
or the associated wisp volume is re-positioned as to be in resting contact with the body. The guide-strand 
is either given the velocity of this body part, or a static friction force is set up between them. The 
remainder of the chapter focuses on the part of interaction processing most speci.c to hair and much 
more dif.cult to handle than collisions with obstacles: we are now addressing the challenging problem 
of self-interactions. 2.2.3 The issues raised by hair self-interactions The interactions that occur between 
hair-strands arevery dif.cult to simulate,For the following reasons: Firstly, in real hair, the friction 
between neighboring strands of similar orientation plays an important part: it dissipates some kinetic 
energy and damps the overall motion. This phenomenon cannot be simulated properly in virtual hair, where 
only a few guide-hair distributed on the scalp are animated. The only way to capture thispartof self-interactionistoadd 
some internaldamping -which should depend on the type of hair and is quite dif.cult to tune -on the individual 
motion of a guide strand. Secondly, strands are very thin, so standard collision detection methods based 
on penetration cannot be used: strands or even small wisps of hair of different orientations might cross 
each other between two simulations steps and go to rest in the wrong positions, this interaction remaining 
un-noticed. Lastly, once a collision between hair guides or hair wisps of different orientation have 
been detected, the response model should account for the complex state of surface of a hair strand: the 
tilted scales that cover a strand result in strongly anisotropic static friction. Moreover, these friction 
forces are dominant: due to the lightness on a hair strand, the colliding strands will most probably 
remain in contact. One of the challenges of hair self-interactions it thus to de.ne a response model 
that prevents strands from crossing each other while avoiding to generate any bouncing. The latter, often 
noticeable in hair animation systems, gives an overall unstable behavior to the full hair, due to the 
extremely large number of local collisions that occur at each time step, even when hair is at rest. Historically, 
the continuous and wisp-based approaches have tackled hair self­interactions in dramatically different 
ways: -Volumetric interactions: Continuum approaches such as Hadap s and Bando s methods relied on .uid-like 
internal viscosity to model hair friction and to prevent self-intersections is a rather global way [HMT01a, 
BCN03]: no collision is detected between individual hair strands, but the latter interact (as .uid particles 
would do), depending on the local hair density and on the relative hair motion around them. -Guide-strands 
interactions: In contrast, processing hair self-collision in dis­continuous, wisp-based approaches has 
been done through the actual de­tection of penetration between moving hair wisps [PCP01a]. This allows 
a more accurate modeling of the discontinuities that can be observed during fast motion of long, human 
hair: in these approaches, wisps of hair de.ned around a guide-strand are prevented from crossing each 
other and two wisps of different orientations can be in resting contact. We believe that the general 
approach chosen for handling hair interactions can be chosen quite independently from the hair model, 
would it be a continuum model, an disjoint set of hair wisps, or something inbetween. The remainder of 
this chapter presents the speci.c solution the authors have de­veloped for tackling the problem of hair 
interactions. This chapter is not aimed as providing a state of the art in the area: the interested reader 
can .nd a recent survey on hair animation and rendering techniques in [WBK+07]. The volumet­ric method 
for hair interactions presented in Section 2.3 belongs to the volumetric interactions approach: it provides 
a real-time alternative to .uid-like interactions when a coarser approximation is suf.cient. Methods 
for improving the ef.ciency of collision detection and the realism of collision response in the interacting 
guide­strands approach are detailed in Sections 2.4 and 2.5. 2.3 Avolumetric approachforreal-time self-interactions 
The work presented in this section was .rst introduced in [BMC05], as a side application of a method 
for handling hair self-shadowing in real-time. We detail here the application of this approach to hair 
self-interactions. 2.3.1 Avolumetric structurefor hair An acceptable approximation of hair self-interaction 
consists of considering that internal collisions mainly result into the preservation of hair volume [LK01a]. 
Starting from this assumption, hair density information is very useful: If the local density of hair 
is over a .xed threshold (corresponding to the maximum quan­tity of hair that can be contained within 
a cell), the hair strands should undergo external forces that spread them out. Bertails et al. [BMC05] 
use a light-oriented voxel grid to store hair density values. This enables them to ef.ciently compute 
both lighting and mechanical interac­tions inside the hair volume in real-time. Though very simple, this 
method yields convincing interactive results for animated hair, is very simple to implement, ef.­cient 
and can easily be parallelized to increase performance. More precisely, the volumetric structure used 
is based on a 3D light-oriented den­sity map, which combines an optimized volumetric representation of 
hair with a light-oriented partition of space. This voxel structure stores the local hair den­sity in 
space, computed from the number of guide-strand segments within a given cell. It is used to approximate 
the light attenuation through each cell of the grid: since the cells are sorted along the light direction, 
computing the accumulated translucencyfor each cell through the hair volume becomes straightforward. 
2.3.2 Application to hair interaction At each animation step, all guide-strand are moved to their new 
position and the density map is updated. Then, hair self-collisions are taken into account for the next 
simulation step by adding density-based interaction forces where needed: repulsive forces directed from 
the center to the border of a grid cell are generated. 54 They are applied to each hair-guide element 
located in a cell whose density if over a threshold. This threshold value depends on the desired level 
of hair fuzziness. Although this interaction method is extremely simple, it yields convincing results. 
In practice, it was tested with an accordingly simple, yet robust algorithm for an­imating the guide-strands: 
hair is composed of approximately a hundred wisps, each of which being simulated using three guide-strands 
modeled as chains of rigid links. The latter are animated using a fast and robust but non-accurate method 
[Ove91]. The rendering technique is a hybrid between continuum and wisp-based methods: interpolation 
between the three guide-strands is used to generate a continuum of hair inside each deformable wisps. 
The overall method results into interactive hair animations that include self-interactions as well as 
self-shadowing, and generate visually convincing hair volume (see Figure 4.6). Furthermore, with this 
technique, handlinghair self-collisions only requires 2.5% of the whole processing time. 2.4 Detecting 
guide-strand interactions Volumetric methods as the simple solution presented above are not suf.cient 
for generating high quality animation of non-smooth hair: two hair wisps of different orientations may 
cross each other during motion despite of the volumetric forces they undergo. Most hair animation methods 
have thus relied on the distance be­tween pairs of guide-strands or on the penetration between wisps 
of hair de.ned around them for accurately detecting hair self-interactions. In this chapter, we call 
these more accurate approaches guide-strand interactions. A naive implementation of guide-strand interactions 
would lead to O(n 2) tests, where n is the total number of guide-strand segments (or wisp segments) in 
the hair model. Following Plante [PCP01a], most methods use a pre-detection based on a regular 3D grid 
data structure, built around the character and its hair, to quickly get rid of most non-intersecting 
cases. Each grid cell contains a list of hair-guide elements (or wisp segments) whose bounding box intersects 
the cell. At each animation step, the grid is used for quickly determining a shorter list of segments 
susceptible to intersect. A mailbox parameter indicates the last time step when a given pair of such 
segments has been tested, ensuring that each pair is tested only once. The 3D grid data structure can 
also be used for optimizing collision detection between hair and the character model: to achieve this, 
each cell also references the polygons of the character model that intersect it. 2.4.1 Deformable versus 
cylindrical hair wisps To account for the complex interactions observed in real hair during fast mo­tion, 
Plante et al. represented hair using a .xed set of deformable, volumetric wisps [PCP01a, PCP02]. Each 
wisp is structured into three hierarchical layers: a skeleton curve (called here guide-strand) that de.nes 
its large-scale motion and deformation, a deformable volumetric envelope that coats the skeleton and 
ac­counts for the deformation due to hair interaction within a wisp, and a given num­ber of hair strands 
distributed inside the wisp envelope and which are generated only at the rendering stage (see Figure 
2.2). More precisely, the deformable sec­tions that shape a wisp of hair around its guide-strand are 
animated using 4 1D damped springs, attempting to capture the way a wisp of hair deforms when its moves 
and most often comes back to its initial size at rest. The wisp volume was de.ned as a quadratic surface 
envelop controlled by these cross-sections. Using such a complex deformable wisp model for the detection 
of guide-strand interactions proved very time consuming: more than 70% of the simulation time was used 
in collision detection between hair wisps, despite of the space grid used to accelerate the process. 
In total, without taking hair rendering into account, about 3 hours of computations were required, in 
2001, to compute 3 seconds of animation. Bertails et al. [BKCN03a] introduced an adaptive animation control 
structure, called the Adaptive Wisp Tree (AWT), which enables the dynamic splitting and merging of hair 
wisps. The AWT depends on a full hierachical structure for the hair, which can either be precomputed 
-for instance using a hierarchical hairstyle [KN02] -or computed on the .y. The AWT represents at each 
time step the wisps segments (or guide-strand segments) of the hierarchy that are ac­tually simulated 
(called active segments). Considering that hair should always be more re.ned near the tips than near 
the roots, the AWT dynamically splits or merges hair wisps while always preserving a tree-like structure, 
in which the root coincides with the hair roots and the leaves stand for the hair tips. In addition to 
limiting the number of active hair-wisp segments, one of the key bene.ts of the AWT for collision detection 
is that the splitting behavior of the wisps models their deformation: there is no need for the complex 
deformable wisp geometry used in [PCP01a]. For collision processing, active wisp segments of the AWT 
are thus represented by cylinders, which greatly simpli.es collision detection tests: detecting interactions 
simpli.es into detecting the local minima of the distance between guide-strand and comparing its value 
to the sum of the wisp radii. With this method, ten seconds of animations could be computed, in 2003, 
in less than .ve minutes. 2.4.2 Handling curly hair and exploiting temporal coherence The Super-Helix 
model, which was recently introduced at SIGGRAPH [BAC+06], and presented in chapter 1, is the .rst model 
that accurately simulates the dynam­ics of curly hair: unlike previous approaches, curly hair wisps are 
not modeled using a straight mass-spring skeleton around which wavy strands are drawn at the rendering 
stage, but are instead accurately modeled using wavy to fuzzy guide­strands, which have a piece-wise 
helical shape. Detecting interactions between such complex helical guide-strands is indeed more costly. 
To handle collisions between hair clumps guided by Super-Helices in a both accu­rate and ef.cient way, 
our strategy is based on the two following ideas: 1) the use of adaptive cylindrical bounding envelopes 
around each hair wisp, whose number and size can automatically adapt during motion, depending on the 
geometry of the wisp, and 2) the tracking of the closest points between the skeletons(i.e., the principal 
axes) of the bounding cylinders. 1. Adaptive bounding envelopes: the bounding volume of a helical element 
Qi of the guide hair strand is composed of a single, large cylinder if the helix s spires are tight enough. 
In other cases(i.e. for straighter strands), we use one or two cylinders, oriented along the mean local 
tangent of the element, to approximate the volume of the wisp (see Figure 2.3). 2. Tracking pairs of 
the closest points: we adapted the algorithm of Raghu­pathi et al., originally designed for detecting 
self-collisions in long and thin deformable objects [RCFC03], to the collision detection between guide 
hair volumes. Since guide hair volumes are composed of a set of cylinders, the method amounts to computing 
minimal distances between pairs of segments (the principal axes of the cylinders), as in [RCFC03]. For 
each pair of guide­strands, we .rst initialize a closest point pair near the root. At each time step, 
each closest point pair is updated by letting the closest points slide along the associated wisp, from 
the positions they had at the last time step. They stopin a location that locally minimizes the distance 
between the two wisp volumes. When this distance is under a threshold, new pairs of points are created 
at both sides of the initial pair, to track the possible multiple local minima. When two closest point 
pairs slide to the same location, they are merged together. At each time step, because of temporal coherence, 
only very few of these pairs need to be moved, so advancing them is very fast. Each time the distance 
between two guide volumes is locally smaller than the sum of their radii, collision is detected.  This 
algorithm ensures that at least one pair of closest points is maintained be­tween two guide volumes, 
while keeping the number of tracked pairs between guide volumes low (merging occurs when two different 
pairs slide towards the same place). The algorithm has thus a n 2 complexity where n is the number of 
guide hair strands composing the hairstyle instead of the total number of segments composing hair, as 
it would be when using a naive algorithm. The same adaptive wisp volumes and temporal coherence technique 
are used for detecting collisions between the hair and the body of the character. Distance tests are 
computed between segments and spheres, as the body is approximated by a unions of spheres. Using this 
technique, we obtained a total frame rate of only 3 seconds per frame for a dynamic hair style composed 
of a hundred of guide hair strands, including self-interactions and interactions with the body. 2.5 Response 
to guide-strand interactions As already mentioned hair is a very soft and light material. Seen as a whole, 
it deforms rather than bouncing when it collides with a relatively rigid obstacle such as the character 
s body. Indeed, hair self-collisions should be very soft as well, and result into frictional rather than 
bouncing behaviors. Therefore, response to guide-strands interactions have been modeled using soft penalty 
forces together with friction forces. 2.5.1 Anisotropic response in wisp-based methods As noted by Plante 
et al. [PCP01a, PCP02], accounting for collisions between hair wisps is fairly different from modelling 
collisions between standard deformable bodies. Wisps are highly anisotropic, since they are just a virtual 
representation for a group of hair strands. While two perpendicular colliding wisps should be compressed 
in order to avoid intersection, interpenetration can be allowed be­tween neighbouring wisps moving roughly 
in the same plane. In consequence, the authors proposed an anisotropic model for the interactions between 
hair wisps: Wisps of similar orientations are mostly submitted to viscous friction and pene­trate each 
other, whereas wisps of different orientations actually collide in a very dissipative way. As illustrated 
in Figure 2.4, this approach yields convincing results, even for fast motions: the model adequately captures 
the discontinuities that can be observed in long, thick hair, preserves hair volume and prevents crossing 
between hair wisps. Nevertheless, the high number of contacts that needed to be computed between the 
different wisps at rest caused some noticeable artifacts such as unstabilities when hair comes to rest. 
The previous anisotropic collision response model was re-used and improved by the Adaptive Wisp Tree 
(AWT) method [BKCN03a]: an AWT implicitly models some of the mutual hair interactions, since neighboring 
wisps with similar mo­tions merge, thus ef.ciently yet robustly mimicking the static friction in real 
hair. This merging behavior also avoids subsequent collision processing between these wisps, thus increasing 
ef.ciency as well as gaining stability from the reduced number of primitives. Typically, an AWT simulation 
starts with a reduced num­ber of hair wisps. While the character moves, these wisps re.ne where and when 
needed (see Figure 2.5), to merge again as soon as they can. When the character is back at rest, the 
simulation eventually ends up a single large hair wisps. This totally avoids the local unstabilities 
noted in previous approaches. 2.5.2 Settinguprealistic penalty and frictionforces The recent work on 
Super-Helices tackled the problem of setting up more accu­rate response forces between interacting guide-strands 
[BAC+06]. Interactions between guide-hairs, and between hair and external objects (such as the body) 
are performed through penalty forces which include a normal elastic response to­gether with a tangential 
friction force. As in [Dur04], the normal penalty force is stabilized thanks to a quadratic reg­ularization 
for small penetrations. From a regularization depth dreg (arbitrarily chosen), the normal reaction force 
R N exerted between the two closest points of interacting guide-strands is computed as follows: if (gap= 
0) R N = 0 kc gap 2 if (0= gap= dreg) R N = n c 2dreg dreg else = kc (gap- )n c R N 2 where n c is 
the unitary vector giving the direction of collision (calculated as the cross product of the vectors 
de.ning the two closest segments), and kc an arbitrary constant value. . .. ..  To simulated friction 
between wisps in contact or friction with an obstacle, the method extends viscous friction law in [CK05], 
de.ned as : RT = -. (vrel - (vrel.nc) nc) To account for the oriented scales covering individual hair 
.bers, the friction coef­.cient . is multiplied by a sine function to account for the orientation of 
hair .bers with respect to their sliding motion over the external object: . = .0(1+ sin(. /2)), where 
angle . is de.ned in Figure 2.6. The parameters of interaction forces, as well as the other parameters 
of the Super-Helices model, can be set up using the actual study of real wisps of hair: The friction 
parameter .0 between hair and a given material is directly adjusted from real observations of sliding 
contacts between the hair clump and the material. As Figures 2.7 and 1.10 show, the Super-Helices model 
results in realistic simu­lations which can be compared side by side with videos of real hair in motion. 
 2.6 Conclusion As we have shown, processing hair interactions requires a dedicated set of meth­ods, 
due to the very speci.c nature of the hair material. Impressive advances were made in the last six years, 
from the .rst models able to handle hair self-collisions to ef.cient, robust and even partly validated 
methods. This chapter has detailed several speci.c solutions that range from the use of a volumetric 
approach when a very quick solution is required to realistic models that stillkeep the computational 
load to an acceptable rate. In spite of all these advances, there still remains very challenging issues 
in the modeling of hair self-interactions: these interactions are indeed the origin of the complex collective 
behavior of hair. Especially they cause hair to group into clus­ters during motion; this phenomenon has 
never been accounted before (except in very simpli.ed models, such as the AWT), as previous models usually 
assume that hair granularity is .xed by the number of simulated guide-strands. Moreover, hair interactions 
vary a lot according to external conditions such as moisture (wet hair being the extreme case), combing, 
or the use of cosmetic products. Lastly, hair tribo-electricity has never been modelled in an accurate 
way. Future research should include attempts to make volumetric methods such as the one presented in 
section 2.3 more accurate at low cost, by taking local hair di­rectional distribution into account while 
setting up the response force. The ap­proaches that seek for realism should probably extract the internal 
damping inside a hair wisp from the preliminary study of hair chunks actually modeled using a full set 
of interacting hair strands. This study should also bring more accurate criteria for splittinga wisp 
into sub-wisps or merging them, and couldhelp char­acterizing the number of hair guides required according 
to the natural curliness and smoothness of a given hair type. Chapter3 Multi-Resolution Hair Modeling 
In this chapter, we present the basic framework for level-of-detail hair modeling. These methods determine 
on the .y which hairs are of most signi.cance to the simulation and provide novel techniques to allocate 
the majority of the computa­tional resources towards modeling these hairs. This process then accelerates 
the simulation of hairs deemed less important, thereby accelerating the overall hair simulation while 
maintaining the desired visual quality of the total simulated hair. Traditional hair modeling techniques 
have viewed hair in different manners. Hair is typically seen as individual strands, or one-dimensional 
curves in three-dimensional space. Sometimes, hair is modeled as groups of strands, or wisps, where multiple 
rendered strands were animated and styled as larger groups. These disjoint hair groups are also modeled 
as strips of hair through two-dimensional surfaces. Hair at times is also perceived as one large volume; 
animation and styling is controlled through a continuous medium while either one-dimensional strands 
or surfaces can be used to render the volume of hair. These separate hair modeling representations have 
typically mandated a choice between simulation quality and simulation speed. The impetus of this research 
is to dynamically create a balance between quality and speed for hair modeling. To attain this goal, 
it is necessary to allow the hair model to adapt to the changing simulation, .nding the balance between 
simulation speed and simulation qual­ity. In this chapter, we will describe the three representations 
we use to model hair. These representations, which we also refer to as the discrete levels-of-detail 
for modeling hair include individual strands, clusters, and strips, see Figure 3.1. The individual strands 
provide the .nest level-of-detail, are modeled with one­dimensional subdivision curves, and can be grouped 
to follow traditional wisp animation schemes. The clusters are a new representation formed from general­ized 
swept volumes created with subdivision surfaces to model a volume of hair. The strips are the lowest 
level-of-detail and are created from .at two-dimensional subdivision surfaces. These representations 
were .rst introduced by Ward et al. [WLL+03]. This chapter will also introduce the base skeleton used 
to control the motion and shape of each level-of-detail. The base skeleton is the underlying control 
struc­ture for each LOD representation and dictates the placement of control vertices used for subdivision. 
The base skeleton plays an important role in level-of-detail transitioning; it is intentionally selected 
to maintain a global, consistent, macro­scopic physical behavior as LOD switches take place. Using the 
same base control structure for each LOD helps to drastically simplify many transition dif.culties typically 
present during LOD switching. It automatically reduces a fairly high degree-of-freedom dynamical system 
down to a lower degree-of-freedom dynam­ical system without any extra expensive computations other than 
performing the LOD switching tests. Hair simulation is controlled through the use of the base skeleton. 
We introduce a collision detection method that ef.ciently and correctly handles hair-object and hair-hair 
interactions. We will also explain how the different framework entities work together to model hair and 
show how the LOD framework can model hair more ef.ciently than pre­vious methods, while maintaining a 
high visual quality. We have developed meth­ods for choosing the appropriate LOD for modeling a section 
of hair based on a number of criteria, including visibility, viewing distance, and hair motion. In this 
chapter, we will discuss how these criteria are used together to choose the .nal representation for hair, 
transition between the different representations, and show results and discuss the performance of these 
methods. 3.1 Geometric Representations Using a base skeleton, the three discrete hair representations 
can then be created. Each LOD representation has varying simulation complexity and visual .delity for 
modeling hair and they have been chosen to be used together due to these variations. 3.1.1 The Base Skeleton 
The base skeleton controls both the shape and the motion of each hair represen­tation at any given point 
during simulation. Based on the idea for modeling each individual hair strand [AUK92b, KAT93], a structure 
has been employed for the base skeleton that forms the core of the proposed set of LOD representations. 
This base skeleton is comprised of n control points, or nodes. This value is de­cided based on criteria 
involving the length of the hair, the waviness or curliness speci.ed for the hair, and the desired smoothness 
for motion. The higher the number of control points, the higher the complexity of the system and the 
.ner the detail is. The skeleton is modeled as an open chain of rigid line segments that connect these 
nodes. The shape of the skeleton is controlled by polar coordinate angles between each node. The Eulerian 
distance between each node is .xed, thus preventing the length of the hair from changing during simulation. 
3.1.2 Strips The strip model in Figure 3.1(a) and (b) uses a single base skeleton model as its foundation 
for motion. The structure for this model is inspired by the strips representation presented by [KH00, 
KH01]. The skeleton is the center of the strip and for each node in the skeleton there are two control 
points that are used to de.ne the strip. These two strip control points and the skeleton node point are 
collinear. A skeleton with n nodes will result in a subdivision surface created froma control polygon 
consistingof2n control points. A strip is typically used to represent the inner most layers of hair or 
parts of hair that are not fully visible to the viewer and, therefore, are often not rendered. It is 
the coarsest (lowest) level-of-detail used for modeling hair. It is mainly used to maintain the global 
physical behavior and the volume of the hair during the simulation. While the strip representation gives 
better visual results for straight hair, it can alsobeusedtomodelwavyandcurlyhair,butnotinas.nea detailasthe 
clusters or strands. Strips are only used when the viewer cannot observe .ne detail, such as when the 
hair is at distances far from the viewer, or when the hair is not in sight. Thus, while the strip cannot 
depict all hairstyles as accurately as the other two LODs, it is typically not visible to the viewer. 
Criteria for choosing an LOD is discussed in further detail in Section 3.3. 3.1.3 Clusters The clusters 
are represented as generalized cylinders created with texture-mapped subdivision surfaces, as shown in 
Figure 3.1(c) and (d). Each cluster is formed from one skeleton that is located at the center of the 
cluster. A radius is speci.ed at the top and the bottom of each cluster. The radius is then linearly 
interpolated at each skeleton node point; this allows the thickness to vary down the length of the cluster. 
At each skeleton node, a circular cross-section, made up of m control points, is created based on the 
radius value at that node. Thus, a skeleton made up of n points will create a cluster of mn control points. 
Typically having m=4 is enough detail to de.ne the cross-section. A cluster is used to model the intermediate 
layers of hair and often makes up the majority of the body of semi-visible hair. Whenever appropriate, 
it is far less costly to represent a group of hair using the cluster model, instead of a large number 
of individual strands. The cluster is able to give more detail than the strip representation because 
it more accurately represents a given volume of hair since it is rendered as a textured cylindrical surface. 
However, the cluster requires more control points than the strip making the complexity to both simulate 
and render it more costly. A single cluster though can approximate a large number of strands, considerably 
decreasing the number of base skeletons required for simulation and the number of control points for 
rendering in comparison to strands alone. 3.1.4 Strands Each individual strand is modeled as a subdivision 
curve using 1D subdivision with n control points, as shown in Figure 3.1(e) and (f). A single control 
ver­tex is created for each node in the skeleton. Strands capture the most detail in comparison to the 
other representations; nevertheless they also require the most computation. Multiple strands are grouped 
to follow the same skeleton to create strand groups or wisps. This process captures many realistic behaviors 
of hair since real hair tends to group together due to static electricity, oils in the hair, or other 
substances in the hair such as water or styling products. The observation that hair strands near each 
other behave similarly allow for a more ef.cient modeling of individual strands. Still, these groups 
of strands are more expensive to simulate than the clusters or strip representations; moreover each strand 
is still rendered making the strands more costly for rendering in comparison to clusters and strips. 
A strand group containing jstrands will then comprise jncontrol vertices before subdivision. 3.2 Hair 
Hierarchy The previous sections introduced the basic components for level-of-detail hair modeling. In 
this section, we introduce the hair hierarchy, a control structure that provides further re.nement to 
the LOD hair framework. The hair hierarchy increases control over the resolution as it contains various 
numbers and sizes of each discrete representation. The hair hierarchy was .rst presented by Ward and 
Lin [WL03]. Using the hair hierarchy the coarsest representation for a given volume of hair is stilla 
single strip.Togain more resolution,however, the hair hierarchyallows the volume to transition into multiple 
smaller strips before reaching the cluster level. Likewise, in cluster form, the volume of hair can now 
be represented with various numbers of clusters that differ in size as well as visual .delity and performance 
speed. As the number of clusters that are used to model a volume of hair increases so does the visual 
.delity of the simulation. Finally, rather than using groups of strands of static sizes, the hair hierarchy 
allows these strand groupings to merge and split on-the-.y, simplifying or adding detail to the simulation 
in the process. The hair hierarchy is created through the continual subdivision of strips, clusters, 
and strand groups and upon completion, contains varying resolutions of each dis­crete representation. 
As a result, near continuous level-of-detail control over the simulation is provided. Ahair hierarchyis 
traversed on-the-.y during the simula­tion to not only select the appropriate discrete representations 
for a section of hair, but also the appropriate resolutions of the representations. In addition to providing 
further level-of-detail control, the hair hierarchy actually captures a behavior of hair that numerous 
hair modeling techniques ignore; this effect is the dynamic clustering of hair strands often exhibited 
in long hair. While strands of hair in close proximity with each other do tend to follow similar mo­tions 
(an underlying assumption of most hair modeling techniques), strands can often collect into large disjoint 
groups of strands that remain separate from the majority of the hair volume (a property continuum-based 
approaches often lack). These large disjoint groups of strands can actually break into smaller groups, 
or strands can leave one group and join another group under large motions, a behav­ior referred to as 
dynamic clustering, which static wisp-based approaches fail to capture. The hair hierarchy can simulate 
dynamic clustering effects as it simulates groups of hairs split and merge as simulationfactors change. 
In this section, we explain the construction and storage of the hair hierarchy, which is performed as 
a pre-process to the simulation. 3.2.1 Strip and Cluster Subdivision Beforea hierarchyof strips or clusters 
canbebuilt, the initial top-level strip must be created. A top-level strip is created by choosing a location 
on the scalp for the origin of the skeleton (the .rst node point of the skeleton). Next, a user-de.ned 
width is speci.ed controlling the thickness of the initial strip. Because the strip is a two-dimensional 
surface, its subdivision is restricted such that it may only be split into two equal parts. Strip subdivision 
is simply the degenerate case to cluster or strand group subdivision, using a degenerate quad­tree, or 
a binary tree, instead of the quad-tree data structure that is used for cluster and strand group hierarchies. 
The subdivision ends once the width of the current strip is below a user-de.ned threshold; these strips 
then become the leaves of the strip hierarchy. To create the cluster hierarchies, leaf strips are divided 
into two equal-sized clus­ters, which become the root clusters of the cluster hierarchies. The cluster 
subdi­vision starts with the circular cross-section that de.nes the cluster. This circular cross-section 
is then split into four equal parts. The four sub-clusters have the same radius value but represent four 
different quadrants of the original cluster. The subdivision of a cluster always results in four children, 
so its information is held in a quad-tree. Clusters stop subdividing once their radius is below a user­de.ned 
threshold value. At this point, further detail is created in the strand group hierarchies. 3.2.2 Strand 
Group Subdivision A strand group cross-section is illustrated in Figure 3.2a. The individual hair strands 
are randomly placed within the group and follow the dynamics of the skeleton. The circular shape of the 
strand groups is used for its simplicity in collision detection. Aquad-tree data structure contains the 
hierarchyinformation. It follows therefore, that each strand group is split into four equal sections, 
as shown in Figure 3.2b. The subdivision of a strand group into four sections creates a tight .tting 
circular cross-section for each subgroup, as in Figure 3.2c and Figure 3.2d. Once the strand group is 
divided, the number of strands in each quadrant is calcu­lated. If a quadrant has no strands within its 
boundaries then the child associated with that quadrant is set to null (see Figure 3.2e). A strand group 
will have be­tween zero and four children. A strand group that contains only one strand will have zero 
children and becomes a leaf in the tree. It may not be necessary for the strand hierarchies to reach 
the individual strands in a simulation if the user Figure 3.2: Strand group subdivision. The subdivision 
process of a strand group into multiple strand groups. (a) The cross-section of a single strand group. 
(b) Strand group is divided into4 equal quadrants and the strands are separatedby the quadrantin which 
they lie (designated by different colors). (c) Circular cross-section is .t around each quadrant, or 
child, of original strand grouping. (d) Four new strand groups are created which are children of the 
original strand group. (e) Continual subdivision process is repeated on each child. Tinted squares show 
empty quadrants that contain no strands, these quadrants are set to null. does not desire that much detail. 
In that case, as an alternative the user can de­cide a minimum number of strands in a group. When a strand 
group contains the minimum number, or less, the subdivision stops. 3.3 Runtime Selection of Hair Now 
that the level-of-detail framework for hair has been discussed, this section will explain how they work 
together to accelerate the simulation and rendering of hair. The primary goal of this framework is to 
measure the importance of a section of hair to the application and use that importance to determine the 
appropriate LOD to model the section of hair. At any given time during the simulation, a head of hair 
can be comprised of an assortment of LOD representations, meaning strands, clusters, and strips are used 
together to balance the visual quality and simulation performance for a head of hair. Using this method, 
the majority of computational resources are used to model the hair that is most signi.cant to the application. 
The importance of a section of hair is measured based on how much detail there is for the viewer to observe. 
The less observable detail there is for a section of hair, then the less important it is deemed for the 
viewer and it is then simulated and rendered with a coarser LOD. A section of hair, in this context, 
is de.ned to be a given volume of hair that can be modeled using any level of the created hair hierarchy. 
 We have developed three criteria that measure the importance of a section of hair to the application. 
These criteria include the following: Visibility -Measures if the viewer can see the section of hair; 
 Viewing distance-Measures howfar the section of hair is from the viewer, which correlates to how much 
screen space the hair covers;  Hair motion -Measures the velocity of the section of hair to determine 
how much simulation accuracywill be needed to model the hair;  The rest of this section will explain 
these criteria in more detail including why they are important to the viewer, how they are measured, 
and then how they work together to choose the .nal hair representation. 3.3.1 Visibility If a viewer 
cannot see a section of hair, that section does not need to be simulated or rendered at its highest resolution. 
The viewer cannot see hair if it is not in the .eld of view of the camera or if it is completely occluded 
by the head or other objects in the scene. If a section of hair in strand representation is normally 
simulated using s number ofskeletonsbutis occludedby other objects, that sectionof hairis simulated using 
one larger strip, and therefore, one skeleton. When that section of hair comes back into view, it is 
important that the placement and action of the hair are consistent with the case when no levels-of-detail 
are used at all; therefore, it continues to be simulated. In addition, when a hair section is occluded, 
it does not need to be rendered at all. Therefore, when a section of hair is occluded, the hair that 
might normally be represented as either clusters or strands is simulated as strips using fewer skeletons 
and these sections are not rendered. 3.3.2 Viewing Distance Hair thatisfar fromtheviewer cannotbe seenin 
great detail. The amountof detail that will be seen by the viewer can be estimated by computing the screen 
space area that the hair covers. As the distance from the viewer to the hair increases, the amount of 
pixels covered by the hair gets smaller and less detail is viewable. The amount of pixels covered by 
the hair is calculated to choose the appropriate LOD. Each level in a hair hierarchyis designed to cover 
a similar amount of world space, thus the root strip can be used as an estimate to the amount of screen 
space area a given hair section occupies. By calculating the amount of pixel coverage the hair will have 
at its current dis­tance, an appropriate LOD can be chosen. The number of pixels of error for the system 
is projected into world space to calculate the world space error at the hair s location; this conversion 
is based on the distance of the hair from the viewer using the following equations: wR - wL wT - wB dPP= 
0.5* max( , ) WH d* allowedPixelsOfError * dPP WSE = Near Here, wR, wL, wT , and wB are the right, left, 
top, and bottom coordinates of the viewing plane, respectively, and W and H are the width and height 
of the of the viewing window in pixels. Near is the distance to the near plane, and d is the distance 
from the camera to the hair section that is currently being tested. The value dPP is the distance per 
pixel, or amount of object space that a single pixel represents. It is calculated based on the setup 
of the viewing camera. The world space error calculated, WSE, is then tested against the error values 
that have been assigned to each LOD. A representation is chosen by .nding the LOD with the maximum error 
that is still less than the allowable world space error amount. The pre-determined maximum allowable 
error for each LOD is decided experimentally based on the viewer s preference; it can be easily altered 
by the viewerina linearfashion. 3.3.3 Hair Motion If the hair is not moving at all, then a large amount 
of computation is not needed to animate it and a lower level-of-detail can be used. When the avatar makes 
sudden movements, e.g. shaking his or her head, or a large gust of wind blows through the hair, a higher-detailed 
simulation is used. When a large force is applied to the hair, such as wind, often individual strands 
can be seen even by a person who is normallytoofarawaytoseetheindividual strandsofhairthatarenotin motion. 
A particular LOD is chosen based on hair motion by .rst determining the skele­ton node in the current 
representation that has the largest velocity. This value is compared to certain thresholds de.ned for 
each level of the hierarchy. If the force acting on the skeleton is not high enough to be represented 
as either strands or clusters, then the hair can be modeled as a strip. The threshold values are based 
on the thickness of each LOD group. The thicker the group the more easily it should break into smaller 
groups. 3.3.4 Combining Criteria At any given time during a simulation, a head of hair is represented 
by multiple LODs. Each section of hair uses its own parameter values to trigger a transition. The sections 
of hair that have a root location at the top of the head, and therefore typically more viewable, remain 
at the strands level longer than the sections of hair that are located at the base of the neck. Thus, 
even if these two sections are at the same distance from the camera and have the same motion, it is more 
important that the top layer be represented in more detail since it is in direct view. When determining 
an appropriate LOD to use, a section of hair is .rst tested for occlusion. If the hair is not visible 
to the viewer then it is automatically simulated as a strip and is not rendered. In this case, no other 
transition tests are needed. If the section of hair is visible, we perform the motion and distance tests 
described above. The LOD representation is chosen based on whichever of these two tests requires higher 
detail. The use of different representations for the hair is virtually unnoticeable to the viewer. 3.4 
Level-of-DetailTransitions The hair hierarchy allows the simulation to choose the appropriate discrete 
rep­resentation and resolution for the hair dynamically. The hierarchy is simply tra­versed selecting 
the desired hair assemblage. As the simulation moves to a differ­ent level in the hair hierarchy either 
a hair group is divided into multiple groups or several groups are combined into one larger group of 
hair. The base skeleton makes these transitions smooth and straightforward. Because each hair represen­tation 
uses the same underlying skeleton for positioning and dynamics, the tran­sitioning algorithm is generalized 
so that it can be applied at any location in the hierarchy. A transition is identi.ed following the criteria 
explained in the previous section. When these tests determine a transition is to occur, the hierarchy 
either performs adaptive subdivision or adaptive merging of the appropriate hair groups. 3.4.1 Adaptive 
Subdivision Using the pre-computed hierarchy, a group of hair can be divided into multiple groups by 
moving a level down the hierarchy. This becomes a simple process through the use of the base skeleton. 
Each hair group s skeleton has the same number of control points as its parent skeleton. Furthermore, 
all of the style prop­erties are the same from parent to child. Accordingly, when a transition to a hair 
group s children occurs, the child skeletons inherit the dynamic state of their par­ent skeleton. Each 
control point in a child skeleton corresponds to a control point in its parent skeleton. When the child 
groups are created from the parent group, the offset of each child from the parent is stored. When the 
parent transitions into its children these offsets are used to position the children accordingly. Figure 
3.4 shows two skeletons dynamically subdivide into multiple skeletons as a gust of wind blows through 
the hair. 3.4.2 Adaptive Merging Merging multiple child skeletons back into their parent skeleton is, 
again, rather straightforward. The dynamic states of the children are averaged, including posi­tion and 
velocity values, and the average is then assigned to the parent skeleton. In order to alleviate visual 
artifacts that can appear by merging children into a parent skeleton, a transition may only occur if 
all of the children are ready to tran­sition back into the parent. Furthermore, when merging multiple 
groups of hair, it is important to avoid a sudden jump in the position of the hair; thus, a positional 
constraint is imposed on the children for the transition, illustrated in Figure 3.5. First, after the 
control point positions in the child skeletons are averaged, the dis­tance of the child control points 
from their corresponding parent control point is calculated (see Figure 3.5b). If this distance for anycontrol 
point is greater than a certain threshold, the transition will not occur. It is advantageous to merge 
groups of hair when possible since it helps to alleviate excess computations. Therefore, if skeletons 
are near each other but not close enough to merge, the skeletons are subtly pulled closer together so 
the transition can eventually take place. In this case, control points that fall outside of the .rst 
distance threshold are tested against a second, slightly larger, threshold (see Figure 3.5c).Ifthe control 
pointsfall withinthe second threshold, a spring forceis used to subtly pull the children into place so 
a smooth transition may occur (see Figure 3.5d). 3.5 Interactive Dynamic Simulation In this section, 
we discuss additional simulation acceleration techniques for hair including an implicit integration scheme, 
collision detection, and a simulation lo­calization technique based on spatial decomposition that is 
used to rapidly locate the areas of highest activity. These areas, de.ned based on various conditions 
(such as on the user s interaction with the hair during interactive hairstyling), are subsequently simulated 
with high detail while the simulation resolution of the remaining hair sections is signi.cantly reduced. 
This process accelerates the dy­namic simulationofhairby allocatingthe majorityofthe computational resources 
towards areas of highest importance to the simulation. Simulation and rendering levels can thenbe achieved 
that arefast enoughto allowa userto actually interact with dynamic hair. Figure 3.5: Adaptive Merging. 
Positional constraints placed on child skeletons merg­ing into parent (a) Parent skeleton (in red) potential 
position determined by averaging position of child skeletons (in yellow). (b) Distance of child nodes 
measured from parent nodeand comparedagainst distancethreshold(in blue).(c)Two nodes havegreater dis­tance 
than .rst threshold, tested against second distance threshold. (d) Nodes are within second threshold, 
spring force placed between nodes and potential parent position to pull them into place. 3.5.1 Implicit 
Integration Although explicit methods such as Euler or fourth-order Runge-Kutter can be used for this 
integration, an implicit integration provides greater stability for the simu­lation. Moreover, manyhairstyles, 
or hair types, require stiff angular springs with high spring constants, for example due to the application 
of hairspray. Explicit integration schemes are inherently poor for such systems because a very low time 
step is necessary to avoid instability. The development of this implicit integration scheme not only 
offers greater stability, but also provides a generality to mod­eling more diverse hairstyles over the 
aforementioned explicit techniques. This approach is similar to cloth simulations that use implicit integration 
for greater stability [BW98a]. This implicit derivation for hair modeling was .rst presented inWard and 
Lin [WL03]. Starting from the basic dynamics model for simulating hair that was .rst proposed by [AUK92b, 
KAT93], we use the torque equations due to spring forces calculated by: M. i = -k. (.i - .i0) (3.1) Mf 
i = -kf (fi - fi0), (3.2) where k. and kf are the spring constants for . and f , respectively. Furthermore, 
.i0 and fi0 are the speci.ed rest angles and .i and fi are the current angle values. We will .rst show 
how the implicit scheme is derived for the . -component. Be­cause the bending motion is measured in polar 
coordinates, the equations will display angular positions, . and f , angular velocities, .. and .f , 
and angular accelerations, a. and af . Rewriting Equation 3.1 as a second-order differential equation 
returns: ¨¨ . (t)= f(. (t), . (t)) = -k. (.i - .i0). (3.3) This can be rewritten as a .rst-order differential 
equation by substituting the vari­¨. ables a. = . and .. = . . The resulting set of .rst-order differential 
equations is: .. d . d . .. = = = . (3.4) . a. dt . dt .. f(. , .. ) The following formulations for .. 
and ... are derived when using the explicit forward Euler method, where .. = . (t0 + h) -. (t0) and ... 
= .. (t0 + h) ­.. (t0) and h is the time step value: .. .. 0 = h . (3.5) ... -k. (. - .0) Instead, an 
implicit step is used, which is often thought of as taking a backwards Euler step since f(. , .. ) is 
evaluated at the point being aimed for rather than at the point it was just at. In this case, the set 
of differential equations changes to the form: .. .. 0 + ... = h . (3.6) ... f(.0 + .. , .. 0 + ... ) 
80 ATaylor series expansion is applied to f to obtain the .rst-order approximation: . f . f .. + ... 
f(.0 + .. , .. 0 + ... ) f0 + .. ... -k. (. - .0) - k. .. + 0(... ) -k. (. - .0) - k. .. (3.7) Substituting 
the approximation of f back into the differential equation of Equation  3.6 yields: .. .. 0 + ... = 
h . (3.8) ... -k. (. - .0) - k. .. Focusing on the angular velocity ... alone and substituting .. = h(.. 
0+ ... ) delivers: ... = h(-k. (. - .0) - k. h(.. 0 + ... )) Rearranging this equation gives: (1+ k. 
h2)... = -hk. (. - .0) - k. h2.. 0 -hk. (. - .0) - h2k. .. 0 ... = . (3.9) 1+ h2k. The change in angular 
velocity for the . -component of a skeleton node point, ... , is thus given in Equation 3.9, where h 
is the time step, and .. 0 = .. (t0) is the angular velocity at time t0. Once ... has been calculated, 
the change in angular position, .. , can be calculated from .. = h(.. 0 + ... ). The same process is 
applied to the f -component of the angular position and angular velocity for each control point of a 
skeleton. Implicit integration allows the use of stiffer springs when warranted, for example, when simulating 
the bristles of a brush which have different spring constants than the hair on a human head. Using stiffsprings 
with explicit integration on the other hand, requires much smaller time steps to ensure a stable simulation. 
3.5.2 Collision Detection and Response Collision detection and response is typically the most time consuming 
process for the overall simulation; it can constitute up to 90% of the total animation time. Its intrinsic 
ability to accelerate collision detection is one of the most appealing con­tributions of the level-of-detail 
hair modeling framework. Using a lower level-of­detail to model a section of hair entails using fewer 
and larger geometric objects, e.g. a single strip versus multiple strands. It is computationally less 
expensive to check for and handle collisions between a few large objects in comparison to many smaller 
ones. The LOD system provides an automatic method for using lower LODs whenever possible, thereby accelerating 
collision detection among other features. Furthermore, the algorithms developed for computing collisions 
are especially designed for the LOD hair representations giving an accurate and ef.cient overall collision 
detection method. In the rest of this section, we will describe the novel selection of appropriate bounding 
volumes for each LOD representation. Then, we will explain the process for detecting collisions for both 
hair-object and hair-hair interactions, including the collision response methods for each type of interaction. 
Swept SphereVolumes Manytechniques have been introduced for collision detection. Common practices have 
used bounding volumes (BVs) as a method to encapsulate a complex object within a simpler approximation 
of said object. Wehave chosento utilizethefamilyof swept spherevolumes (SSVs) [LGLM00] to surround the 
hair. SSVs comprise a family of bounding volumes de.ned by a core skeleton grown outward by some offset. 
The set of core skeletons may in­clude a point, line, or ngon. Figure 3.6 shows examples of some SSVs, 
namely a point swept sphere (PSS), a line swept sphere (LSS), and a rectangular swept sphere (RSS). To 
calculate an SSV, let C denote the core skeleton and S be a sphere of radius r, the resulting SSV is 
de.ned as: B= C. S= {c+ r| c . C, r . S}. (3.10) To detect an intersection between a pair of arbitrary 
SSVs a distance test is per­ Figure 3.6:Familyof Swept SphereVolumes. (a)Point swept sphere (PSS); (b) 
Line swept sphere (LSS); (c) Rectangle swept sphere (RSS). The core skeleton is shown as a bold line 
or point. formed between their corresponding core skeletons and then the appropriate off­sets, i.e. the 
radius of each SSV, are subtracted. Swept SphereVolumesfor Hair We have chosen to use the family of SSVs 
to encapsulate the hair because the shape of the SSVs closely matches the geometry of the hair representations. 
The SSVs that correspond to the three geometric representations for hair are line swept spheres (LSSs) 
for the strands and cluster levels, and rectangular swept spheres (RSSs) for the strip level. These SSVs 
can be used in combination to detect colli­sions between different representations of hair. For each 
rigid segment of the skeleton model, that is, each line segment between two nodes, an SSV bounding volume 
is pre-computed. For a skeleton with n nodes, there are n-1segments, and thus n-1single SSVs. The variable 
thickness of each segment de.nes the radius of the SSV along its length. In order to compute a BV for 
a strip, the four control points of the strip that outline a skeletal segment de.ne the area for a RSS 
to enclose. This is performed for each of the n - 1 segments along the skeleton. The geometry of the 
strip is different from the other two representations in that the strip is a surface while the clusters 
and a collection of strands are volumes. In order to allow the transition from a strip into multiple 
clusters remain faithful to the volume of hair being depicted an RSS is created for a strip section by 
surrounding each strip section with a box of certain thickness. Each strip is given a thickness equal 
to that of its cluster and strand grouping counterparts. While the strip is rendered as a surface, it 
acts physically as a volume. Thus, when a transition from a strip into clusters occurs, the volume of 
hair being represented remains constant throughout this process. For the cluster representation, an LSS 
is created around the2m control points that de.ne a segment(m control points, as de.ned in Section 3.1.3, 
from the cross­section at the top of the segment and m control points at the bottom of the seg­ment). 
The line segment between the two skeleton control points of each section is used as the core line segment 
of the line swept sphere. For individual strands, collision detection is performed for each strand or 
group of strands, depending on implementation, in a manner similar to that of the clusters. An LSS is 
computed around the skeleton that de.nes each segment with a radius de.ning the thickness. The radius 
of each LSS is varied based on the thickness of the group of strands. Hair-Hair Interactions Because 
hair is in constant contact with surrounding hair, interactions among hair are important to capture. 
Ignoring this effect can cause visual disturbances since the hair will not look as voluminous as it should 
and observing hair passing straight through other hairs creates a visual disruption to the simulation. 
The typi­cal human head has thousands of hairs. Consequently, testing the n- 1sections of each hair group 
against the remaining sections of hair would be too overwhelming for the simulation even using wisp or 
LOD techniques. Instead, spatial decompo­sition is used to create a three-dimensional grid around the 
area containing the hair and avatar. The average length of the rigid line segments of the skeletons is 
used as the height, width, and depth of each grid cell. Every time a section of hair moves or the skeleton 
for simulation is updated, its line swept spheres (LSSs) or rectangular swept spheres (RSSs) are inserted 
into the grid. An SSV is inserted into the grid by determining which cells .rst contain the core shape 
of the SSV (line or rectangle), then the offset of the SSVs are used to determine the remain­ing inhabited 
cells. Subsequently, collisions only need to be tested against SSVs that fall within the same cell, re.ning 
the collision tests to SSVs with the highest potential for collision. Itis possiblefora singleSSVtofall 
into multiple cells.Asa result,two separate SSVs can overlap each other in multiple grid cells. To prevent 
calculating a col­lision response more than once for the same pair of SSVs, each SSV keeps track of the 
other SSVs it has encountered in a given time step. Multiple encounters of the same pair of SSVs are 
ignored. For each pair of SSVs thatfalls into the same grid cell the distance between their corresponding 
core skeletons, s1 and s2, are determined. This distance, d, is subtracted from the sum of the radii 
of the two SSVs, r1 and r2, to determine if there is an intersection. Let overlap = d- (r1+ r2) (3.11) 
If overlap is positive then the sections of hair do not overlap and no response is calculated. Figure 
3.7 shows the calculation of the overlap of two LSSs. If there is an intersection, their corresponding 
velocities are set to the average of their initial velocities. This minimizes penetration in subsequent 
time steps because the sections of hair will start to move together. Next, followingthe formulation proposedby 
[PCP01a],the orientationsofthetwo hair sections will determine how the collision response is handled. 
The cross prod­uct between the core skeletons, s1ands2, is computed to determine the orientation of the 
skeletons in relation to each other. If s1ands2are near parallel, the velocity averaging will be enough 
to combat their collision, similar to [PCP01a]. Whereas [PCP01a] solely relies on modifying velocities 
in different manners based on the orientation of the hair sections, using the SSVs to compute collisions 
makes it straightforward to determine the amount of penetration between corresponding hair sections. 
As a result, intersecting hair sections that are not of similar orienta­tions are pushed apart based 
on their amount of overlap. The extra force exerted to remove hair penetrations help this system to capture 
.ner collision detail than other systems, including intricate braiding or twisting details. The direction 
to move each hair section is determined by calculating a vector from the closest point on s1 to the closest 
point on s2. Each section is moved by half the overlap value and in opposite directions along the vector 
from s1 to s2. Figure 3.8 shows the effects of hair-hair interactions in comparison to no hair-hair interactions. 
 Hair-Object Interactions Hair can interact with any object in the scene, such as the head or body of 
the character, where the object is a solid body that allows no penetration. Throughout the rest of this 
section we will use the terms head and object interchangeably since the collision detection algorithm 
used for hair-head interactions is applicable to all hair-object interactions. The spatial decomposition 
scheme that is used for .nding hair-hair interactions is also used to determine potential collisions 
between the hair and objects in the scene. Therefore, both the hair and the objects must be represented 
in the grid. The polygons of the avatar, or other objects, are placed into the grid to determine potential 
collisions with the hair. Object positions only need to be updated within the grid if the object is moving 
otherwise the initial insertion is suf.cient. Grid­cells that contain both impenetrable triangles and 
hair geometry are marked to be checked for hair-object collision; only these cells contain a potentially 
colliding pair. A collision is checked by calculating the distance between the SSV core shape and the 
triangles and then subtracting the offset of the SSV. If a section of hair is colliding with the object, 
the position of the hair section is adjusted so that it is outside of the object. The amount by which 
to push the hair section is determined by calculating the amount of penetration of the hair section into 
the object. Then the skeleton is pushed in the direction normal to the object in the amount of the penetration. 
The section of hair is now no longer colliding withthe object.In addition,thevelocityofthe sectionof 
hairissetto zerointhe direction towards the object (opposite the direction of the normal), so that the 
hair is restricted to only move tangential to and away from, the object. In the next time step, the hair 
will still be in close proximity to the object. If there is no intersection between the object and the 
hair it is determined whether the hair is still within a certain distance threshold. If it is within 
this threshold, then the hair is still restricted so that its velocity in the direction of the object 
is zero. If it is not within this threshold, then the hair can move about freely. When hair interacts 
with an object, a frictional force must be applied. The friction force is calculated by projecting the 
acceleration of the hair from force onto the plane tangential to the object at the point of contact. 
The result is the accelera­tion component that is tangent to the object. The friction force is applied 
in the opposite direction to oppose the motion of the hair. The magnitude of this force is based on the 
acceleration of the hair and the frictional coef.cient, µ f, which is dependent upon the surface of the 
object, where0 < µ f < 1. The resulting friction force, Ff, becomes: Ff = -µ f(F- (F· N)N) (3.12) where 
F is the force on the hair and N is the normal direction. 3.5.3 Simulation Localization Interactive hair 
simulation and rendering is necessary for many applications, in­cluding virtual hairstyling tools. An 
intuitive virtual hairstyling tool needs to take into account user interaction with dynamic hair. Until 
recently, the complexity of animating and rendering hair had been too computationally costly to accurately 
model hair s essential features at desired rates. As a result, manyhairstyling meth­ods ignore dynamic 
simulation and/or user interaction, which creates an unnatural styling process in comparison to what 
would be expected in practice. In this sec­tion, we discuss a simulation localization technique that 
was originally introduced byWard et al. [WGL06, WGL07] for the creation of an interactive virtual hair 
sa­lon system. This interactive styling system supports user interaction with dynamic hair through several 
common hair salon applications, such as applying water and styling products [WGL04]. Spatial decomposition 
is used to rapidly determine the high activity areas of the hair; these areas are then simulated with 
.ner detail. A uniform grid consisting of axis-aligned cells that encompass the area around the hair 
and human avatar is employed. This spatial decomposition scheme was previously described for hair-hair 
and hair-object collision detection. Here, this process is extended to all features of hair simulation, 
not just collision detection. Insertion into the Grid The polygons of the avatar, or other objects, are 
placed into the grid to determine potential collisions with the hair. Object positions only need to be 
updated within the grid if the object is moving otherwise the initial insertion is suf.cient. The hair 
is represented in the grid by inserting each SSV of the hair; every time a section of hair moves, or 
the skeleton for simulation is updated, its line swept spheres (LSSs) or rectangular swept spheres (RSSs) 
positions are updated in the grid. An SSV is inserted into the grid by determining which cells .rst contain 
the core shape of the SSV (line or rectangle), then the offset of the SSVs are used to determine the 
remaining inhabited cells. Figure 3.9(a) shows the grid cells that contain hair geometry. When dealing 
with user interaction with virtual hair, as the user employs an appli­cation (e.g. spraying water, grabbing 
the hair) the grid is used to indicate which portions of the hair are potentially affected by the user 
s action. As the user moves his or her attention, such as through the use of a PHANToM stylus, its position 
and orientation are updated. Each application has an area of in.uence that de.nes where in space its 
action will have an effect. This area is de.ned as a triangle for the cutting tool and a cone for the 
remaining tools. The cone of in.uence is de­.ned by the application s position, orientation (or direction 
pointed), length (how far it can reach), and cutoff angle (determining its radius along its length). 
These properties de.ne the cone s position in the grid. Inserting the cone becomes simi­larto insertinganLSS,buttheoffset 
becomesavariableof distancealongthe core line (an SSV has a constant offset along its core shape). The 
triangle for cutting is de.ned by the space between the open blades of a pair of scissors. Retrieval 
from the Grid Once information has been inserted or updated in the grid, it is retrieved to deter­mine 
where to check for potential collisions and user interaction. To locate user interactions, the grid maintains 
a list of grid-cells where the user interaction cone or triangle has been inserted. Anyof these grid 
cells that contain hair geometry are returned and the sections of hair within the cell are independently 
checked to see if theyfall within the area of in.uence, see Figure 3.9. Using the grid, much fewer sectionsof 
hairhavetobe checked than withoutit,buttheexact hair positions are still checked against the cone or 
triangle to maintain accuracy. Multi-Resolution Simulation with the Grid The grid aids the system to 
localize the simulation towards the areas of high­est importance to the model. Following the criteria 
discussed earlier, a section of hair s signi.cance is measured by its visibility, motion and viewing 
distance. These factors are used to choose the resolution and representation of a section of hair via 
the hair hierarchy. The simulation localization technique expands upon the motion criterion and adds 
the user s interaction with the hair to further re.ne the simulation. The motion of a section of hair 
is highly pertinent to the amount of detail needed to simulate it. In the case of interactive styling, 
most applications performed on hair are localized to a small portion of the hair; the majority of hair 
thus lies dormant. The sections of hair that are dormant are modeled with a lower LOD representation 
and resolution, determined by comparison against velocity thresh­olds as discussed earlier,but here wegoa 
step furtherbyeffectively turning-off simulation for areas where there is no activity. Each grid cellkeeps 
track of the activity within the cell, tracking the hair sections that enter and exit the cell. When 
the action in a given cell has ceased and the hair sections in the cell have a zero velocity, there is 
no need to compute dynamic simulation due to gravity, spring forces, or collisions. The positions of 
the hair sections are thus frozen until they are re-activated. The cell is labeled as dormant and does 
not become active again until either the user interacts with the cell or until a new hair section enters 
the cell. When a hair section is active, full simula­tion is performed on it including dynamics of spring 
forces, gravity, and collision detection and response. Rapid determination of the active cells and hair 
sections allows the system to allocate the computational resources towards dynamic simu­lation for the 
hairs of highest interest to the user. 3.5.4 Results and Discussion To test the interactive dynamic simulation 
process described in this section, a vir­tual hair salon system was implemented, which allows a user 
to create a hairstyle by directly manipulating dynamic virtual hair. The system allows for a user to 
dynamically alter the properties through several common hair salon applications (such as cutting, wetting, 
drying). Further implementation details can be found in Ward et al. [WGL07]. Figure 3.10 shows a comparison 
of real hair under the in.uence of common hair modeling applications with the virtual salon results under 
the same conditions. Level-of-detail representations coupled with the simulation localization scheme 
have accelerated the animation of hair so that a user can actually interact with it. Dynamic simulation, 
including implicit integration, LOD selection, hair appli­cations (wetting, cutting, etc.), and collision 
detection, to create a hairstyle ran at an average of 0.092 seconds per frame. This .gure comprised between 
37 to 296 skeleton models, determined on-the-.y throughout the simulation, with an average of 20 control 
points each. At the .nest resolution, the model contained 8,128 rendered strands; throughout the simulation 
the rendering LOD contained between 6K and 1,311K rendered vertices. Lighting and shadow computations 
on the GPU were performed in 0.058 seconds/frame on average. The user applica­tions performed to create 
this style included wetting, cutting and blow-drying. The benchmarks were . measured on a desktop PC 
equipped with an Intel RXeonTM 2.8 Ghz processor with 2.0 GB RAM and an NVIDIA R6800 graph­ . GeForceTM 
ics card. Figure 3.11 shows a detailed performance comparison over the course of an entire simulation 
between wisps (which are used as the baseline of comparison), LODS alone, and our LODs coupled with simulation 
localization. The performance of the LODs with simulation localization varies over time due to the user 
perform­ing different applications on the hair. However, it is clear that the LODs with simulation localization 
are able to outperform wisps alone as well as LODs alone. Figure 3.11: Simulation Performance Comparison. 
Shows the factor of speed-up for LODs with simulati on localization and LODs over wisps alone. Here, 
the average runtimeofthewispsisusedasthe baselinefor comparison(valueof1 onthischart). Over the course 
ofthis simulation, the camera remained at a consistent distance from the .gure and the viewer primarily 
faced the back of the avatar -making distance and occlusion tests have a small overall impact on the 
LOD choice. Note the LODs with simulation localization overall outperforms both wisps and LODs alone, 
though the simulation varies over time as the user employs different applications. 3.6 Conclusion In 
this chapter, we have presented the basic features of level-of-detail hair mod­eling and simulation. 
These techniques can dynamically change a hair model, al­lowing a simulation to balance between the visual 
.delity and performance speed of the animated hair. To illustrate the effectiveness of the level-of-detail 
frame­work for modeling hair, an interactive system for styling hair was implemented using these techniques. 
The interactive virtual hair salon provides a user interface that obtains 3D input from the user for 
direct manipulation of dynamic hair and allocates the majority of computational powers towards the hairs 
that are most signi.cant towards the application. As a result, the hair is simulated at a speed that 
actually allows a user to directly interact with it. KellyWard, Ming Lin Chapter4 Hair Rendering Steve 
Marchner Realistic rendering of human hair requires the handling of both local and global hair properties. 
To render a full hairstyle, it is necessary to choose an appropriate global representation for hair. 
Implicit and explicit representations are presented and discussed in Section 4.1. Local hair properties 
de.ne the way individual hair .bers are illuminated. Section 4.2 describes the scattering properties 
of hair and reviews the different models that have been proposed to account for those proper­ties. Global 
hair properties also include the way hair .bers cast shadows on each other; this issue of self-shadowing, 
handled in Section 4.3, plays a crucial role in volumetric hair appearance. Rendering hair typically 
requires time-consuming computations, Section 4.4 reviews various rendering acceleration techniques. 
4.1 Representing Hairfor Rendering Choices of hair rendering algorithms largely depend on the underlying 
represen­tations for modeling hair geometry. For example, explicit models require line or triangle-based 
renderers, whereas volumetric models need volume renderers, or rendering algorithms that work on implicit 
geometry. 4.1.1 Explicit Representation With an explicit representation, one has to draw each hair .ber. 
Ahair .ber is nat­urally represented with a curved cylinder. The early work by Watanabe and Sue­naga 
[WS92] adopted a trigonal prism representation, where each hair strand is represented as connected prisms 
with three sides. This method assumes that varia­tion in color along the hair radius can be well approximated 
by a single color. Oth­ers use ribbon-like connected triangle strips to represent hair, where each triangle 
alwaysfacestowardsthe camera. Ivan Neulander [NdP98] introduceda technique that adaptively tessellates 
a curved hair geometry into polygons depending on the distance to the camera, curvature of hair geometry, 
etc. At large distances, a hair strand often resembles many hairs. Kong and Nakajima [KN99] exploited 
this property to reduce the number of rendered hairs by adaptively creating more hairs at the boundary. 
Dif.culties arise with explicit rendering of tesselated hair geometry due to the unique nature of hair 
 a hair strand is extremely thin in diameter (0.1 mm). In a normal viewing condition, the projected thickness 
of a hair strand is much smaller than the size of a pixel. This property causes severe undersamplingproblems 
for rendering algorithms for polygonal geometry. Any point sample-based renderer determines a pixel s 
color (or depth) by a limited number of discrete samples. Undersampling creates abrupt changes in color 
or noisy edges around the hair. Increasing the number of samples alleviates the problem,but only at slow 
conver­gence rates [Mit96] and consequently at increased rendering costs. LeBlanc et al. [LTT91] addressed 
this issue by properly blending each hair s color usingapixel blendingbuffer technique.In this method, 
each hair strandisdrawn as connectedlinesandtheshadedcolorisblendedintoapixelbuffer.Whenusing alpha-blending, 
one should be careful with the drawing order. Kim and Neu­mann [KN02] also use an approximate visibility 
ordering method to interactively draw hairs with OpenGL s alpha blending. 4.1.2 Implicit Representation 
Volumetric textures (or texels) [KK89, Ney98] avoid the aliasing problem with pre-.ltered shading functions. 
The smallest primitive is a volumetric cell that can be easily mip-mapped to be used at multiple scales. 
The cost of ray traversal is relatively low for short hairs, but can be high for long hairs. Also when 
hair animates, such volumes should be updated for every frame, making pre-.ltering inef.cient. The rendering 
method of the cluster hair model [YXYW00] also exploits implicit geometry. Each cluster is .rst approximated 
by a polygonal boundary. When a ray hits the polygonal surface, prede.ned density functions are used 
to accu­mulate density. By approximating the high frequency detail with volume density functions, the 
method produces antialiased images of hair clusters. However, this method does not allow changes in the 
density functions, making hairs appear as if theyalways stay together. 4.2 Light Scattering in Hair The 
.rst requirement for anyhair rendering system is a model for the scattering of light by individual .bers 
of hair. This model plays the same role in hair rendering as a surface re.ection, or local illumination, 
model does in conventional surface rendering. 4.2.1 Hair Optical Properties The composition and microscopic 
structure of hair are important to its appearance. A hair .ber is composed of three structures: the cortex, 
which is the core of the .ber and provides its physical strength, the cuticle, a coating of protective 
scales that completely covers the cortex several layers thick (see Figure 1.1 in chapter 1), and the 
medulla, a structure of unknown function that sometimes appears near the axis of the .ber. As already 
mentioned in chapter 1, the cross sectional shape varies from circular to elliptical to irregular [Rob94]. 
Much is known about the chemistry of hair, but for the purposes of optics it suf­.ces to know that it 
is composed of amorphous proteins that act as a transparent medium with an index of refraction . = 1.55 
[Rob94, SGF77]. The cortex and medulla contain pigments that absorb light, often in a wavelength-dependent 
way; these pigments are the cause of the color of hair. 4.2.2 Notation and Radiometry of Fiber Re.ection 
 Our notation for scattering geometry is summarized in Figure 4.1. We refer to the plane perpendicular 
to the .ber as the normal plane. The direction of illumination is .i, and the direction in which scattered 
light is being computed or measured is .r; both direction vectors point away from the center. We express 
.i and .r in spherical coordinates. The inclinations with respect to the normal plane are denoted .i 
and .r (measured so that 0 degree is perpendicular to the hair). The azimuths around the hair are denoted 
fi and fr, and the relative azimuth fr - fi, which is suf.cient for circular .bers, is denoted .f . 
Because .bers are usually treated as one-dimensional entities, light re.ection from .bers needs to be 
described somewhat differently from the more familiar surface re.ection. Light scattering at a surface 
is conventionally described using the bidi­rectional re.ectance distribution function (BRDF), fr(.i, 
.r). The BRDF gives the density with respect to the projected solid angle of the scattered .ux that re­sults 
from a narrow incident beam from the direction .i. It is de.ned as the ratio of surface radiance (intensity 
per unit projected area) exiting the surface in di­rection .r to surface irradiance (.ux per unit area) 
falling on the surface from a differential solid angle in the direction .i: dLr(.r) fr(.i, .r)= . dEi(.i) 
Under this de.nition, the radiance due to an incoming radiance distribution Li(.i) is l Lr(.r)= fr(.i, 
.r)Li(.i) cos .id.i H2 where H2 is the hemisphere of directions above the surface. Light scattering from 
.bers is described similarly,but the units for measuring the incident and re.ected light are different 
because the light is being re.ected from a one-dimensional curve [MJC+03b]. If we replace surface with 
curve and area with length in the de.nition above we obtain a de.nition of the scattering function fs 
for a .ber: the ratio of curve radiance (intensity per unit projected length) exiting the curve in direction 
.r to curve irradiance (.ux per unit length) falling on the curve from a differential solid angle in 
the direction .i. The curve radiance due to illumination from an incoming radiance distribution Li is 
l Lc (.r)= D fs(.i, .r)Li(.i) cos .id.irH2 where D is the diameter of the hair as seen from the illumination 
direction. This transformation motivated Marschner et al. [MJC+03b] to introduce curve radiance and curve 
irradiance. Curve radiance is in some sense halfway between the concepts of radiance and intensity, and 
it describes the contribution of a thin .ber to an image independent of its width. Curve irradiance measures 
the radiant power intercepted per unit length of .ber and therefore increases with the .ber s width. 
Thus, given two .bers with identical properties but different widths, both will have the same scattering 
function but the wider .ber will produce a brighter curve in a rendered image because the wider .ber 
intercepts more incident light. This de.nition is consistent with the behavior of real .bers: very .ne 
hairs do appearfainter when viewed in isolation. Most of the hair scattering literature does not discuss 
radiometry, but the above de.nitions formalize the common practice, except that the diameter of the hair 
is normally omitted since it is just a constant factor. The factor of cos.i is often included in the 
model, as was common in early presentations of surface shading models. 4.2.3 Re.ection and Refraction 
in Cylinders For specular re.ection, a hair can be modeled, to a .rst approximation, as a trans­parent 
(if lightly pigmented) or purely re.ecting (if highly pigmented) dielectric cylinder. The light-scattering 
properties of cylinders have been extensively stud­ied in order to inversely determine the properties 
of optical .bers by examining their scattering [ALS98, Mar74, MTM98]. As .rst presented in graphics by 
Kajiya and Kay [KK89] (their scattering model is presentedin Section 4.2.5),if we considerabundleof parallel 
rays that illuminates a smooth cylinder, each ray will re.ect across the local surface normal at the 
point where it strikes the surface. These surface normals are all perpendicular to the .ber axis they 
lie in the normal plane. Because the direction of each re.ected ray is symmetric to the incident direction 
across the local normal, all the re.ected rays will make the same angle with the normal plane. This means 
that the re.ected distribution from a parallel beam due to specular re.ection from the surface lies in 
a cone at the same inclination as the incident beam. For hairs that are not darkly pigmented, the component 
of light that is refracted and enters the interior of the hair is also important. As a consequence of 
Bravais s Law [Tri70], a corrolary of Snell s Law that is often used to describe refractions through 
crystals with a cylinder-like structure, the directions of the rays that are refracted through thecylinder 
surface alsofall ona cone centered on thecylinder axis. The same holds for the refractions as the rays 
exit the cylinder. Therefore all specularly re.ected light from a smooth cylinder will emit on the same 
cone as the surface re.ection, no matter what sequence of refractions and internal re.ections it may 
have taken. 4.2.4 Measurements of Hair Scattering In cosmetics literature, some measurements of incidence-plane 
scattering from .bers have been published. Stamm et al. [SGF77] made measurements of re.ec­tion from 
an array of parallel .bers. Theyobserved several remarkable departures from the expected re.ection into 
the specular cone: there are two specular peaks, one on either side of the specular direction, and there 
is a sharp true specular peak that emerges at grazing angles. The authors explained the presence of the 
two peaks using an incidence-plane analysis of light re.ecting from the tilted scales that cover the 
.ber, with the surface re.ection and the .rst-order internal re.ection explaining the two specular peaks. 
A later paper by Bustard and Smith [BS91] reported additional measurements of single .bers, including 
measuring the four combinations of incident and scattered linear polarization states. They found that 
one of the specular peaks was mainly depolarized while the other preserved the polarization. This discovery 
provided additional evidence for the explanation of one lobe from the surface re.ection and one from 
the internal re.ection. Bustard and Smith also discussed preliminary results of an azimuthal measure­ment, 
performed with illumination and viewing perpendicular to the .ber. They reported .nding bright peaks 
in the azimuthal distribution and speculated that they were due to caustic formation,but theydid not 
report anydata. Marschner et al. [MJC+03b] reported measurements of single .bers in more gen­eral geometries. 
In addition to incidence plane measurements, they presented normal plane measurements that show in detail 
the peaks that Bustard and Smith discussed and how they evolve as a strand of hair is rotated around 
its axis. The authors referred to these peaks as glints and showed a simulation of scattering from an 
elliptical cylinder that predicts the evolution of the glints; this clearly con­.rmed that the glints 
are caused by caustic formation in internal re.ection paths. They also reported some higher-dimensional 
measurements that show the evolu­tion of the peaks with the angle of incidence, which showed the full 
scattered distribution for a particular angle of incidence. 4.2.5 Modelsfor Hair Scattering The earliest 
and most widely used model for hair scattering is Kajiya and Kay s model which was developed for rendering 
fur [KK89]. This model includes a diffuse component and a specular component: cos p(.r + .i) S(.i, fi, 
.r, fr)= kd + ks . cos(.i) Kajiya and Kay derived the diffuse component by integrating re.ected radiance 
across the width of an opaque, diffuse cylinder. Their specular component is sim­ply motivated by the 
argument from the preceding section that the ideal specular re.ection from the surface will be con.ned 
to a cone and therefore the re.ection from a non-ideal .ber should be a lobe concentrated near that cone. 
Note that neither the peak value nor the width of the specular lobe changes with . or f . Banks [Ban94] 
later re-explained the same model based on more minimal geo­metric arguments. For diffuse re.ection, 
a differential piece of .ber is illuminated by a beam with a cross section proportional to cos .i and 
the diffusely re.ected power emits uniformly to all directions.1 For specular re.ection, Fermat s princi­ple 
requires that the projection of the incident and re.ected rays onto the .ber be the same. In another 
paper on rendering fur, Goldman [Gol97], among a number of other re.nements to the aggregate shading 
model, proposed a re.nement to introduce azimuthal dependence into the .ber scattering model. He multiplied 
both terms of the modelbyafactor fdir that can be expressed in the current notation as: fdir = 1+ acos 
.f . Setting a > 0 serves to bias the model toward backward scattering, while setting a < 0 biases the 
model towards forward scattering.2 Tae-Yong Kim [Kim02] proposed another model for azimuthal dependence, 
which accounts for surface re.ection and transmission using two cosine lobes. The sur­ 1Banks does not 
discuss why uniform curve radiance is the appropriate sense in which the scattered light should be uniform. 
2In Goldman s original notation a =(.reflect - .transmit)/(.reflect + .transmit). A factor of 1 (.reflect 
+ .transmit) can be absorbed into the diffuse and specular coef.cients. 2 face re.ection lobe derives 
from the assumption of mirror re.ection with con­stant re.ectance (that is, ignoring the Fresnel factor), 
and the transmission lobe is designed empirically to give a forward-focused lobe. The model is built 
on Kajiya-Kay in the same way Goldman s is, de.ning: -pp cos f < f < g(f )=22 0 otherwise This model 
is Kajiya and Kay s model multiplied by: fdir = ag(.f /2)+ g(k(.f - p )) where a is used to balance forward 
and backward scattering and k is a parameter to control how focused the forward scattering is. The .rst 
term is for backward (surface) scattering and the second term is for forward (transmitted) scattering. 
Marschner et al. [MJC+03b] proposed the most complete physically based hair scattering model to date. 
Their model makes two improvements to Kajiya and Kay s model: it predicts the azimuthal variation in 
scattered light based on the ray optics of a cylinder, and it accounts for the longitudinal separation 
of the highlight into surface-re.ection, transmission, and internal-re.ection components that emerge 
at different angles. The azimuthal component of the model is based on a ray analysis that accounts for 
focusing and dispersion of light, absorption in the interior, and Fresnel re.ection at each interaction. 
The longitudinal component models the shifts of the .rst three orders of re.ection empirically using 
lobes that are displaced from the specular cone by speci.c angles. 4.2.6 Light Scattering onWet Hair 
The way light scatters on hair is changed when hair becomes wet. Jensen et al. [JLD99] noted that when 
objects become wet theytypically appear darker and shinier; hair behaves the same way. Bruderlin [Bru99] 
and Ward et al. [WGL04] altered previous light scattering models to capture the effects of wet fur and 
wet hair, respectively. As hair becomes wet, a thin .lm of water is formed around the .bers, forming 
a smooth, mirror-like surface on the hair. In contrast to the naturally rough, tiled surface of dry hair, 
this smoother surface creates a shinier appearance of the hair due to increased specular re.ections. 
Furthermore, light rays are subject to total internal re.ection inside the .lm of water around the hair 
strands, contributing to the darker appearance wet hair has over dry hair. Moreover, water is absorbed 
into the hair .ber, increasing the opacity value of each strand leading to more aggressive self-shadowing 
(see Section 4.3). Bruderlin [Bru99] and Ward et al. [WGL04] modeled wet strands by increasing the amount 
of specular re.ection. Furthermore, by increasing the opacity value of the hair, the .bers attain a darker 
and shinier look, resembling the appearance of wet hair. 4.3 Hair Self-Shadowing Hair .bers cast shadows 
onto each other, as well as receiving and casting shad­ows from and to other objects in the scene. Self-shadowing 
creates crucial visual patterns that distinguish one hairstyle from another, see Figure 4.3. Unlike solid 
objects, a dense volume of hair exhibits complex light propagation patterns. Each hair .ber transmits 
and scatters rather than fully blocks the incoming lights. The strong forward scattering properties as 
well as the complex underlying geometry make the shadow computation dif.cult. One can ray trace hair 
geometry to compute shadow, whether hair is represented by implicit models [KK89] or explicit models 
[MJC+03b]. For complex geom­etry, the cost of ray traversal can be expensive and many authors turn to 
caching schemes for ef.ciency. Two main techniques are generally used to cast self­shadows into volumetric 
objects: ray casting through volumetric densities and shadow maps. 4.3.1 Ray-casting throughaVolumetric 
Representation With implicit hair representations, one can directly ray trace volume density [YXYW00], 
or use two-pass shadowing schemes for volume density [KK89]; the .rst pass .lls volume density with shadow 
information and the second pass renders the volume density. 4.3.2 Shadow Maps LeBlanc [LTT91] introduced 
the use of the shadow map, a depth image of hair rendered from the light s point of view. In this technique, 
hair and other objects are rendered from the light s point of view and the depth values are stored. Each 
point to be shadowed is projected onto the light s camera and the point s depth is checked against the 
depth in the shadow map. Kong and Nakijima [KN99] ex­tendedthe principleof shadow cachingtothe visiblevolumebuffer, 
where shadow information is stored in a 3D grid. In complex hair volumes, depths can vary radically over 
small changes in image space. The discrete nature of depth sampling limits shadow buffers in handling 
hair. Moreover, lights tend to gradually attenuate through hair .bers due to for­ward scattering. The 
binary decision in depth testing inherently precludes such light transmission phenomena. Thus, shadow 
buffers are unsuitable for volumet­ric hair. The transmittance t (p) of a light to a point p can be written 
as: l l '' t (p)= exp(-O), where O = st(l )dl . 0 l is the length of a path from the light to p, st is 
the extinction (or density) function along the path. O is the opacity thickness (or accumulated extinction 
function). 104 In the deep shadow maps technique [LV00], each pixel stores a piecewise linear approximation 
of the transmittance function instead of a single depth, yielding more precise shadow computations than 
shadow maps, see Figure 4.4 for an il­lustration. The transmittance function accounts for two important 
properties of hair. FractionalVisibility: In the context of hair rendering, the transmittance function 
can be regarded as a fractional visibility function from the light s point of view. If more hair .bers 
are seen along the path from the light, the light gets more attenu­ated (occluded), resulting in less 
illumination (shadow). As noted earlier, visibil­ity can change drastically over the pixel s extent. 
To handle this partial visibility problem, one should accurately compute the transmission function by 
correctly integrating and .ltering all the contributions from the underlying geometry. Translucency: 
A hair .ber not only absorbs, but also scatters and transmits the incoming light. Assuming that the hair 
.ber transmits the incoming light only in a forward direction, the translucencyis also handled by the 
transmittance function. Noting that the transmittance function typically varies radically over image 
space, but gradually along the light direction, one can accurately approximate the trans­mittance function 
with a compact representation. Deep shadow maps [LV00] use a compressed piecewise linear function for 
each pixel, along with special han­dling for discontinuities in transmittance. Figure 4.3 shows a comparison 
of hair geometry with and without shadows using the deep shadow maps algorithm. Opacity shadow maps 
[KN01] further assume that such transmittance functions always vary smoothly, and can thus be approximated 
with a set of .xed image caches perpendicular to the lighting direction (see Figure 4.5). By approximating 
the transmittance function with discrete planar maps, opacity maps can be ef.­ciently generated with 
graphics hardware (see Section 4.4.3). Linear interpolation from such mapsfacilitatesfast approximation 
to hair self-shadows. 4.4 Rendering AccelerationTechniques Accurately rendering complex hairstyles can 
take several minutes for one frame. Many applications, such as games or virtual reality, require real-time 
rendering of hair. These demands have initiated recent work to accelerate precise rendering algorithmsby 
simplifyingthe geometric representationof hair,bydevelopingfast volumetric rendering, or by utilizing 
recent advances in graphics hardware. 4.4.1 Approximating Hair Geometry Section 4.2 explained the structure 
of hair and showed that hair .bers are actually quite complex. Simplifying this geometry, using fewer 
vertices and rendering fewer strands, is one strategy for accelerating hair rendering. Removing large 
portions of hair strands can be distracting and unrealistic, therefore surfaces and strips have been 
used for approximating large numbers of hair strands [KH00, KH01, GZ02, KHS04]. These two-dimensional 
representations resemble hair by texture mapping the sur­faces with hair images and using alpha mapping 
to give the illusion of individual hair strands. Curly wisps can be generated by projecting the hair 
patch onto a cylindrical surface [KHS04]. Level of detail (LOD) representations used by Ward et al. [WLL+03, 
WL03] (see chapter 3) for accelerating the dynamic simulation of hair, also accelerates hair rendering. 
Using a coarse LOD to model hair that cannot be seen well by the viewer requires rendering fewer vertices 
with little loss in visual .delity. As a result, the time required to calculate light scattering and 
shadowing effects is diminished by an order of magnitude. 4.4.2 InteractiveVolumetric Rendering Bando 
et al. [BCN03] modeled hair as a set of connected particles, where particles represent hairvolume density. 
Their rendering methodwas inspiredbyfast cloud rendering techniques [DKY+00] where each particle is rendered 
by splatting a textured billboard, both for self-shadowing computation and .nal rendering. This method 
runs interactively,butit does not castvery accurate shadows inside hair. Bertails et al. [BMC05] use 
a light-oriented voxel grid to store hair density values, which enables them to ef.ciently compute accumulative 
transmittance inside the hair volume. Transmittance values are then .ltered and combined with diffuse 
and specular components to calculate the .nal color of each hair segment. Though very simple, this method 
yields convincing interactive results for animated hair (see Figure 4.6). Moreover, it can easily be 
parallelized to increase performance.  4.4.3 Graphics Hardware Many impressive advances have been made 
recently in programmable graphics hardware. Graphics processor units (GPUs) now allow programming of 
more and more complex operations through dedicated languages, such as Cg. For exam­ple, various shaders 
can directly be implemented on the hardware, which greatly improves performance. Currently, the major 
drawback of advanced GPU pro­gramming is that new features are neither easy to implement nor portable 
across different graphics cards. Heidrich and Seidel [HS98] ef.ciently render anisotropic surfaces by 
using OpenGL texture mapping. Anisotropic re.ections of individual hair .bers have also been implemented 
with this method for straightforward ef.ciency. As for hair self-shadowing, some approaches have recently 
focused on the ac­celeration of the opacity shadow maps algorithm (presented in Section 4.3.2), by using 
the recent capabilities of GPUs. Koster et al. [KHS04] exploited graph­ics hardware by storing all the 
opacity maps in a 3D texture, to have the hair self-shadow computation done purely in graphics hardware. 
Using textured strips to simplify hair geometry (as seen in Section 4.4.1), they achieve real-time per­formance. 
Mertens et al. [MKBR04] explored ef.cient hair density clustering schemes suited for graphics hardware, 
achieving interactive rates for high quality shadow generation in dynamically changing hair geometry. 
Finally, a real-time demonstration showing long hair moving in the sea was presented by NVidia in 2004 
[ZFWH04] to illustrate the new capabilities of their latest graphics cards (see http://www.nzone.com/object/nzone 
nalu home.html). Chapter5 Hair inFeature Production Sunil Hadap, Zoran Ka ci.´c,Tae-Yong Kim c-Alesi´ 
5.1 Strand and Hair Simulation at PDI/DreamWorks -Madagascar and Shrek The Third In this section, we 
would like to exemplify the versatile use of the dynamic strand primitive, introduced in section 1.3, 
for character dynamics and visual effects. The proposed methodology is extensively used in the production 
of Madagascar and Shrek The Third. With each example, we would like to highlight the aspects of the formulation 
that is most relevant. The methodology and the tools based on it are continually being effective for 
simulation of strands and long hair in upcoming productions at DreamWorks Animation such as Bee Movie. 
The example of lemurs dancing with fancy .ower lanterns in (Figure 5.1), high­lights the importance of 
stable and consistent coordinate frame along the strand. The tip of the strand is made heavy by using 
the length-wise variation of mass per unit length parameter, and the .ower geometry is simply parented 
to the coordi­nate frame at the tip of the strand. Subtle twist dynamics adds to the realism. The foliage 
simulation is a great example of branched dynamics. The individual trees are composed of hierarchy of 
strands, some of the segments being very stiff towards the trunk. One can follow the physically believable 
motion of trees under   the in.uence of (strong) external force .eld such as wake and turbulence. 
It is also evident that the strand system is very scalable. Each tree typically has 50-100 segments and 
there are around 1000 trees in the Blown by Horn shot (video 3). Donkey s ear exempli.es posable dynamics. 
Animators precisely control the sub­tle secondary dynamic motion around the animated poses, using time-varing 
pose strength parameter. Thebun followedby the long braidis composedofa single strand. Thevery stiff 
intial sectiongives the subtle interesting bounce and twist to thebun. The .exible section corresponds 
to the braid. The local dynamics parameter is used to control the amount of .oppyness the braid exibits. 
The simulation of curly bangs illustrate the ability of the system to handle stiff equations arising 
from the intricate rest shape. The rest shape is adjusted to ac­count for the shape change due to gravity. 
The long hair simulations highlight the effectiveness of the collision response model. The accurate computation 
of velocity and acceleration of the base joint results in highly realistic hair motion, where as time 
scale parameter gives con­trol. We have not done a comprehensive performance analysis and optimization 
of the   Oriented Strands system yet. Nevertheless, we would like to state the typical performance 
numbers for the hair simulations, as they represent the most of the dynamic complexities. The simulation 
of curly bangs uses 9 strands having an average 15 segments, each. The simulation runs at interactive 
rate of 2-4 Hz. The long hair simulations use 190 strands with 20-25 segments each. The simulations take 
less than 20 seconds per animation frame. The complexity of the strand dy­namics is linear time in the 
total number of segments n. Whereas, the collision response is O(m 2) in m number of collision points. 
Recently, we tried to analyze the convergence characteristics of the solver. We found that the solver 
uses so­phisticated error control and heuristics, which result in a very wide variation in the number 
of non-linear iteration the solver takes. For the long hair simulations, the number varies from 2 to 
213 iterations, with mean at 18.3 iterations. In the advent of multi-core and multi-cpu workstations, 
we would like to note that the computations of individual strands are embarrassingly parallel. 5.1.1 
Conclusion, Limitations and FutureWork The simulation system of Oriented Strands has found widespread 
applications in feature animation and visual effects. We would like to attribute the successful usage 
of Oriented Strands to the robustness coming from the implicit formulation and the comprehensive collision 
response model, the intuitive work.ow coming from local space formulation, physically based stiffness 
and collision models. In addition, innovative concepts such as time scale, local dynamics, posable dynam­ics, 
zero-gravity rest shape make Oriented Strands system art directable . In this methodology, our focus 
has been stiff dynamics of serial open-chain multi-body systems with constraints and collision response.Fortunately, 
theDAE based formulation can be extended to include closed-loops [RJFdJ04]. Unfortu­nately, the analytical 
constraints and collision response model discussed sofar do not readily .t the framework of closed-loops. 
Thus, in future we would like to extend, or develop new, methodologies to include closed-loops. Intricate 
jewelry on animated characters is our main motivation. Other limitations of the proposed methodology 
are The approach is computationally expensive as compared to previous meth­ods in [Had03, CCK05b]. It 
would not scale well to do e.g. fur dynamics.  Although one can incorporate stretch in the strand system 
by animating lengths of rigid segments, the system does not handle stretch dynamically.  Developing 
and implementing constraints and collision response model is not as straightforward as compared to maximal 
coordinate formulation [CCK05b].  5.1.2 Acknowledgments Iwould like to take this opportunity to acknowledge 
the great teamwork at PDI/DreamWorks. Iwould like to thank Dave Eberle and DeepakTolani for their great 
collaboration during the development of the system, Scott Singer, Arnauld Lamorlette, Scott Peterson, 
Francois Antoine, Larry Cutler, Lucia Modesto, Terran Boylan, Jeffrey Jay, Daniel Dawson, Lee Graft, 
Damon Riesberg, David Caeiro Cebrian, Alain De Hoe and everyone who directly in.uenced the development 
and tirelessly worked towards the successful use of the system in production, my supervisors Lisa Mary-Lamb, 
Andrew Pearce and Sanjay Das for supporting me all along, and anyone I forgot to mention. 5.2 Strand 
and Hair Simulation at ILM The quest for ever increasing realism in visual effects has reached a point 
where many viewers, including experts, frequently cannot tell what aspects of a live ac­tion movie were 
created using computer graphics techniques. Often, only the im­probability that a scene was shot using 
practical means provides a clue. It is now commonly expected that hair, skin, muscles, clothing, jewelry, 
and accessories of digital characters look and move in a way indistinguishable from real ones. And without 
exceptions, these aspects of digital characters are not supposed to attract attention unless intended 
so by the movie maker. Dynamics simulations are a signi.cant contributor to this apparent success. At 
Industrial Light&#38;Magic we use them extensively to achieve believable motion of digital characters. 
Simulation of hair and strands is an integral part of a collection of techniques that we commonly refer 
to as structural dynamics. As much as we like to celebrate our achievements in the area of simulation, 
the other main point that we hope to conveyis that we still have a long way to go in this .eld. In many 
ways, recent successes have just opened the door to a widespread use of simulations. We want to describe 
not only what has been done, but also whatwewouldliketodobuthavenotbeenable,sofar. Therearemanyinteresting 
and challenging problems left to be solved. We hope this presentation provides motivation to tackle some 
of these issues. 5.2.1 Overview The survey paper [WBK+07] provides a very good overview of many techniques 
used in our hair pipeline at ILM.Typically, we think of our pipeline as consisting of four distinct stages: 
Hair placement and styling: an artist driven, interactive modeling task done in our proprietary 3D application 
called Zeno. Our general free-form sur­face sculpting tools were augmented to satisfy hair speci.c needs 
such as hair length and rooting preservation, twisting and curling, interpolated placement, hair extension 
and clipping. These tools are used to create a set of guide hairs (B-spline curves) representative of 
the overall hair style, and are also very useful for touching up simulation results. The number of guide 
hairs can vary widely, depending on the complexity and coverage, from several hundred to manythousands. 
Although still subject to modi.cations, this is a mature and well established component of our pipeline, 
with nearly a decade of production use. Simulation: interactive tools and scripts for creation and editing 
of simu­lation meshes/rigs, tuning simulation controls and parameters, and running simulations in Zeno. 
This is among the most complex parts of our pipeline and is the main topic of our discussion.  Hairgeneration: 
a procedural technique for creating a complete set of hair strands (a full body of hair) from a relatively 
small number of guide hairs. The number of generated hairs typically ranges from mid tens of thousands 
to several million. Finer details of hair tufting and wisping, jitter, and irreg­ularities are almost 
exclusively handled at this level. This is the oldest component of our pipeline, having roots in the 
techniques developed for Jumanji in 1995. It still undergoes frequent show and crea­ture speci.c modi.cations, 
and has recently been completely overhauled.  Lighting and rendering. We used to render hair using our 
own in-house particle and hair renderer called Prender,but for several years now wehave relied on RenderMan 
exclusively to render hairs as B-spline curves. Setting up the lighting is done using our proprietary 
tools inside Zeno.  5.2.2 Dynamics of Hair and Strands Animation of hair and strands can be achieved 
using the same combination of keyframe techniques, inverse kinematics, procedural deformation, and motion 
capture that is applied to the body and skin of a digital character. As long as hair follows along and 
remains properly oriented and rooted on the underlying surface, the results may look convincing. This 
is often suf.cient for short hair and fur, and for background characters. In other cases, particularly 
when appearance of compliance with the laws of physics is desired, traditional and manual animation techniquesfall 
short or are at best te­dious. To a degree this is also true for other aspects of digital character animation, 
but it is particularly problematic for fast moving long and medium length hair. Physically based simulations 
have provided a solution that we increasingly de­pend on at ILM. We have traditionally relied on our 
own proprietary simulation tools, although our artists have access to and use when appropriate dynamics 
tools inside vendor packages such as Maya. Over the years we have made several major revisions to our 
software -our hair and cloth simulation tools are currently in their third generation, and rigid body 
and .esh/skin tools are in their second. Over the past several years we have collaborated closely with 
Ron Fedkiw and his graduate students at Stanford, and have migrated our simulation engines to use PhysBAM, 
which is also our .uid simulation engine. With regard to a physical model used as representative of hair 
dynamics, we have been .rmly in the mass-spring camp. This is also true for our cloth, skin, and .esh 
simulation systems. Rigid body dynamics has also been one of the mainstays of our pipeline -simulations 
of ropes, chains, and accessories that dangle from digital characters and mechanical structures have 
long been employed in production. In [OBKA03] we describe how our rigid body dynamics system was used 
to create extremely complex, convincing character performances for The Hulk (2003). And in [KANB03] we 
describe how fundamentally similar or identical techniques can be used for deformable and rigid body 
simulations. Sometimes, it is not obvious what types of simulation would be the most appro­priate or 
the most cost effective for a particular character. For digital doubles and furry creatures simulation 
of thin strands of hair is often the obvious choice. But when the hairis braided, asitis for someWookies 
(.gure 5.7), or madeofkelp and other types of seaweed (.gure 5.9), as it is for some pirates of the Caribbean, 
or the strands are actually octopus-like tentacles, as they are on the Davy Jones char­acter (.gures 
5.8 and 5.17), techniques commonly associated with cloth, .esh, and articulated rigid body simulations 
become equally or more attractive.  Treating all these types of simulation as just different .avors 
of structural dynam­ics is useful from both the developer s and the artist s perspective. Algorithms, 
data structures, and work.ows developed for one type of simulation are often directly applicable to the 
others. Similarly, an artist s expertise in one area of structural simulations is useful for most of 
them. Over the years we have suc­cessfully blurred the distinction between the .avors of structural simulations 
at every level, from the simulation engine, to the UI, setup, and work.ows used by the production. Simulations 
that were once incompatible can now be used inter­changeably, or can be layered upon each other with 
one way interactions, or can be used together. The surveypaper [WBK+07] describes manyoptions when it 
comes to the choice of a dynamic mesh/rig, that is, the con.guration of point-masses and springs that 
best represents the motion of hair and strands. Similarly, we do not have a single answer as well. A 
simple 1D rig was used with great success in 1998 for the hair on the Mighty Joe Young. Although lacking 
sophisticated collision and hair-hair interaction handling, and despite thefact that today we can simulate 
thousands of hairs like that at interactive speeds, the visual impact of those particular shots has not 
been signi.cantly surpassed to date. 2D strips of cloth have also been used successfully to represent 
tufts of hair. While still the best solution for hair madeofkelp, cloth strips cannot support their weight 
or resist wind equally in all directions and are, thus, not a good general solution. We now commonly 
use 3D meshes of various con.gurations and complexities to surround and support the guide hairs during 
simulation. Given that we simulate only a subset of .nal hairs, mainly just the guide hairs, it is important 
to note that these meshes are more representative of a tuft of hair than of a single strand. In this 
context, hair-body and hair-hair interactions are closer to tuft-body and tuft-tuft interactions. As 
mentioned before, .ne details of tufting and wisping are handled procedurally at the hair generation 
stage. Finally, if absolute control and support is needed because strands become a part of and artistically 
driven performance, as it is the case with Davy Jones tentacles, articulated rigs with rigid constraints 
and motorized torque joints are used. With regard to the choice of numerical integration technique, the 
common wisdom seems to point towards implicit methods. Their unconditional stability is greatly appreciated 
when simulations must not fail, as is the case in the games industry and real time systems, or when, 
for whatever reason, taking small timesteps is not an option. Still, implicit methods come at a very 
signi.cant cost, as discussed in the next section. For manyyears we have relied exclusively on explicit 
integration methods, such as Verlet or Newmark central differencing [KANB03]. The visual quality of simulations 
that ILM has achieved using strictly explicit methods is still unsurpassed in each category: hair, cloth, 
.esh, and rigid dynamics. For our deformable simulations,wenowuseahybridmethodthatisstillexplicitin nature 
but uses full implicit integration of damping forces [BMF03]. Finite elements methods are also available 
to our users with a switch of a button. Their promise is in more accurate modeling of physical material 
properties. But they are still signi.cantly slower than other methods and are rarely used in production. 
Our lack of commitment to any particular solution is most evident in handling collisions and interactions 
between dynamic objects. Collisions are often the most expensive part of simulation. Our users often 
need the cheapest method that does the job for anyparticular shot or effect. We therefore offer: Force 
based point-point and point-face repulsions, including self repulsion. They are reasonably cheap (with 
a judicious use of bounding box trees and k-d trees)but not robust.  Collisions of points against discretized 
level set volumes, with the option to handle stacking of rigid bodies [GBF03]. They are fairly robust 
and very fast once the volumes are computed. But computation of collision volumes can be arbitrarily 
slow, depending on the desired level of detail.  Geometric collisions between moving points and moving 
faces, and be­tween moving edges, with iterations to resolve multiple collisions. This is the most robust 
andbyfar the slowest option.  Each of these methods presents its own implementation challenges, particularly 
when it comes to achieving robustness and ef.ciency, but also in dealing with static friction and the 
details ofthe time differencing method. This topic could .ll a course on its own and is, sadly, beyond 
the scope of this presentation. Anycombination of the above methods can be used in our system to handle 
inter­actions of hair meshes with other objects in the scene, including other hair meshes. 5.2.3 Challenges 
The main consideration in feature production is whether the results look good. No matter how physically 
correct a simulation is, if it falls short of a speci.c artistic goal, it is not good enough. Fortunately, 
this does not imply that the results always have to be perfect. It means that the artists need a variety 
of tools and that some combination of these tools can be used to achieve the desired effect. It is actually 
quite rare that the results of a simulation end up in the movie theater without being touched in some 
way by the artist. An equally important consideration is that of the economy: whether the bene.ts of 
using a particular technique outweigh the costs. In an ever more competitive environment movies are done 
on very tight schedules and budgets. Our ability to meet the time and resource constraints depends on 
tools that are not only capable of achievingavarietyof desired resultsbut candoitina predictable, reliable, 
and controllablefashion. Most of our tools are designed for a long term use -they are supposed to outlive 
any particular production and work on the next movie, and the one after. When we are fortunate to work 
on effects that have never been done before, for which no tools exist, our initial attempts can be somewhat 
raw, requiring unusual amounts of manual labor from the artists, and still be useful to production. But 
to remain viable, the use of these new tools has to become more or less routine by the next production 
cycle. Our dynamics, hair styling, and lighting tools are modes inside Zeno, ILM s large 3D application 
framework. There are great bene.ts to having such an integrated applications, from code sharing to the 
commonality of the UI and the work.ows. Fitting our solutions into this large framework and making them 
work nicely with the other components also requires great care. Here are some of the problems related 
to our hair simulation pipeline, but not limited to it, that weface daily: Control: Animators need tools 
to take the animation in anydesired direction without destroying the illusion of physical correctness. 
When on occasion they are asked to do the physically impossible, it is a toolmaker s job as much as artist 
s to make the impossible look good. This is a control issue and is as important as the physical and numerical 
soundness of algorithms at the core of our engine. As described in [KANB03], control is ideally achieved 
with spring-like forces. Ideally because our system is already very well tuned to deal with such forces. 
However, when precise control is needed or when we know the desired simulation outcome, forces are often 
insuf.cient as they are balanced by the solver against all other events in the scene. In such cases we 
need to adjust the state of the system (positions, velocities, orientations, and angular momenta) directly. 
And in doing so we need to preserve the stability of the system and as much of physical correctness as 
possible. It is really interesting when multiple controls want to modify the state si­multaneously -this 
usually con.icts with our other strong desire to keep controls modular and independent from each other. 
 Robustness: This is primarily about stability and accuracy. While a dis­tinction between these two terms 
is extremelyimportant for the developers, it is completely lost on the users. Whether the simulation 
fails because of a numerical over.ow (stability of explicit integration methods) or it looks completely 
erratic or overdamped because of large errors (accuracy of im­plicit methods) is not nearly as important 
to the users as is thefact that they have nothing to show in the dailies. This problem is compounded 
by our need to provide simulation controls. Stability of our integration methods holds only as long as 
we do not violate the premises on which it is based. Stability analysis is dif.cult enough for a well 
formulated set of differential equations. How do we analyze stability of 10 lines of C++ code?  Work.ow 
and setup issues: Structural dynamics is just a subset of tools that artists responsible for creature 
development use daily. It is common that a creature requires several layers of cloth simulations, hair 
simulation, skin and .esh simulation, and rigid simulations for all the accessories. And there could 
be a dozen of them in a shot. All this in an environment where models and animation are developed concurrently 
and change daily. Zeno, our 3D application, provides a framework that makes this possible. It is by 
no means easy.  Simulation time: No one has yet complained that simulations run too fast. Quick turnaround 
is extremely important to our users productivity and to their ability to deliver a desired effect. Simulation 
speeds ranging from in­teractive to several seconds per frame are ideal. Several minutes per frame is 
nearthe limitof tolerance. Structural simulations thatfailto .nishovernight for the entire length of 
the shot are not acceptable.  Cost of development and support: Algorithms that are very dif.cult to 
get right or that work only under very speci.c hard to meet conditions are usu­ally less than ideal for 
a production environment. We do not take this ar­gument too far because it would disqualify all of dynamics 
-it is a balance that we strive for. Systems based on implicit integration methods tend to be considerably 
more expensive in this regard.  Requirement for very specialized knowledge or experience, limiting the 
pool of available artists and developers.  Manyof the above issues could motivate a Siggraph course 
on their own. It is not without regret that we touch upon them so lightly in this presentation. Obviously, 
dynamics systems also provide great bene.ts by enabling creation of a physically believable animation 
that would be dif.cult, tedious, or impractical to do otherwise. This cost-bene.t analysis may put us 
on a somewhat different course from the purely research and academic community. A proof of concept, a 
functional pro­totype, a research paper is often just a starting point for us. Majority of our effort 
is spent on building upon proven techniques and turning them into production worthy tools -on bridging 
the gap between successful technology and the arts. Practical and engineering challenges of doing that 
easily match those of inventing the techniques in the .rst place. Working on these issues in a production 
envi­ronment, continuously balancing the needs and the abilities, has been a humbling experience for 
everyone involved. 5.2.4 Examples The extreme motion of the creatures and the transformations between 
human and fantastic forms, e.g. werewolves tearing out from inside the skin of their human characters 
and vice versa, were the two biggest challenges for digital hair in the making of Van Helsing(2004). 
It was also the .rst time that we modeled very long curly human hair (.gure 5.10) and simulated it through 
a full range of motion and well into the realm of humanly impossible (.gures 5.11 and 5.12). Modeling 
(styling) and simulation of hair was done on a smaller number of guide hairs up to several hundred on 
a human head and almost nine thousand on the Werewolves. Before rendering, the full body of hair was 
created by a complex interpolation technique that also added irregularities and took care of tufting 
and .ne wisping. These generated hairs, numbering in the mid tens of thousands for human hair and up 
to several million for the wolves (.gure 5.13), were then passed to RenderMan for rendering as B-spline 
curves. We relied heavily on numerical simulations to achieve a believable motion of hair. Slow-moving 
creatures and motion-captured humans presented very few prob­lems. Fast moving werewolves and vampire 
brides were more dif.cult, particu­larly for long hair. The creatures were often animated to the camera 
and did not necessarily follow physically plausible 3D trajectories. In many cases the script just asked 
for the physically improbable. Consequently, our simulations also em­ployed controls that were not based 
on physics. Particularly useful were those for matching the animation velocities in a simulation. Still, 
the animation some­times had to be slowed down, re-timed, or edited to remove sharp accelerations. Wind 
sources with fractal variation were also invaluable for achieving realistic fast motion of hair and for 
matching live action footage. Our proprietary hair pipeline was reimplemented for Van Helsing to allow 
for close integration of interactive hair placement and styling tools, hair simulation tools, and hair 
interpolation algorithms. The hair and (tearing) cloth dynamics systems were merged for the needs of 
Werewolf transformation shots in which the hair was simulated either on top of violently tearing skin, 
or just under it.  This integration enabled the artists to style the hair, set up and run skin and hair 
simulations, and sculpt post-simulation corrective shapes in a single application framework. These same 
tools were simultaneously used in the making of The Day After To­morrow (2004) for the CG wolves (.gures 
5.14, 5.15) and have since been used on many other shows for digital doubles (for example .gure 5.16), 
a CG baby, hairy and grassy creatures, lose threads of clothing, etc. Pirates of the Caribbean: Dead 
Man s Chest features Davy Jones (.gures 5.8 and 5.17), an all CG character, whose most outstanding feature 
is his beard, designed to look like an octopus with dozens of tentacles. The beard presented multiple 
 Figure 5.13:AWerewolf from Van Helsing (2004) challenges for animation, simulation, and deformation. 
To make this character believable, it was critical that the tentacle behaved like that of an octopus, 
but still presented the dynamic motion of the character s per­formance. ILM utilized Stanford s PhysBAM 
simulation system as a base for the articulated rigid body simulation that drives the performance of 
the tentacles. Along with Stanford s solver, ILM created animation controllers that allowed the artists 
to direct the performance of the simulation. By incorporating the Stanford solver into our proprietary 
animation package, Zeno, we made it possible for a team of artists to quickly produce the animation for 
the 200+ shots required by production. An articulated rigid body dynamics engine was utilized to achieve 
the desired look. Each tentacle was built as a chain of rigid bodies, and articulated point joints served 
as a connection between the rigid bodies. This simulation was per­formed independently of all other simulations, 
and the results were placed back on an animation rig that would eventually drive a separate .esh simulation. 
Since Davy s beard had 46 tentacles with a total of 492 rigid bodies and 446 point joints, a controller 
system was needed in order to make the simulation manageable for an artist. Each tentacle had a controller 
to de.ne parameters to achieve the desired dynamic behavior, which was mainly in.uenced by the head motion 
and any col­liding objects. Another controller, which served as a multiplier for all individual controllers, 
helped the artist to in.uence the behavior of the whole beard at once. To make the tentacles curl, the 
connecting point joints were motorized using a sine wave that was controlled by attributes like amplitude, 
frequency and time. Most dynamic parameters were set along the length of the tentacle. So, in order to 
automate the setting of these parameters, a 2D curve represented the length of the tentacle on the x-axis 
and the value of the parameter on the y-axis. Periodically some tentacles required key framed animation 
in order to manipulate objects in the scene. When speci.c interactions were required from the animation 
pipeline, the rigid bodies were set to follow the animation and used as collision geometry for the simulated 
tentacles. The control for each joint on a tentacle was accomplished using a force-based targeting system. 
The targeting system calculated torques between rigid objects constrained by a joint. The goal of the 
targeting system was to minimize the differ­ence between the joint s target angle and its current angle. 
During the progression of the simulation, the target angles for the joints were modi.ed by the animation 
controller. For each joint, the targeting system calculated the difference between the target orientation 
and current orientation. The resulting difference produced an axis of rotation that de.ned a torque around 
which the connected rigid objects rotated. The .nal step was to apply the calculated torque back onto 
the connected rigid objects.  A real tentacle has numerous suction cups that allow the tentacle to momen­tarily 
grab onto surfaces and let go at any time. A functionality was required, termed Stiction, which would 
automatically create connecting springs between rigid bodies, to correctly mimic this momentary grab 
and release. The Stiction­spring interface was implemented through a set of area maps on the underside 
of the tentacle, de.ning regions on the tentacle where springs could be created. Properties of the Stiction 
interface de.ned distances at which springs could be created or destroyed, thus displaying the correct 
motion of the grab and release. 5.2.5 Conclusions Simulation of hair and strands, and dynamics in general, 
has been very successful but is far from a solved problem. Many of the remaining challenges are not as 
much about possibility but more about practicality. How do we make simulations more controllable, more 
intuitive, more robust, more detailed but less expensive, much faster but no less accurate? Using Pirates 
of the Caribbean: Dead Man s Chest as a point of reference, the amount of detail that we can model and 
render today exceeds what can be simulated by an order of magnitude. And modeling and rendering are far 
from solved problems on their own. As computers get more powerful and our techniques get better, the 
demand for simulations seems to in­crease with the increased possibilities -or at least, that has been 
our experience over the past decade. We see no indication of this changing any time soon. The prospect 
of solving some of the remaining practical problems should be wonder­fully exciting to the industry practitioners 
and researchers alike. 5.2.6 Acknowledgments The ILM hair pipeline and the examples presented in this 
session are a result of collaboration of many people across several departments. The ILM R&#38;D De­partment 
and particularly David Bullock, Stephen Bowline, Brice Criswell, Don Hatch, Rachel Weinstein, Charlie 
Kilpatrick, Christophe Hery, and Ron Fedkiw; the ILM Creature Development Department and particularly 
Karin Cooper, Renita Taylor, Eric Wong, Keiji Yamaguchi, Tim McLaughlin, Andy Anderson, Vijay Myneni, 
Greg Killmaster, Aaron Ferguson, Jason Smith, Nigel Sumner, Steve Sauers, Sunny Lee, Greg Weiner, James 
Tooley, and Scott Jones; ILM Model­ers Ken Bryan, Mark Siegel, Lana Lan, Frank Gravatt, Andrew Cawrse, 
Corey Rosen, Michael Koperwas, Sunny Li-Hsien Wei, Jung-Seung Hong, Geoff Camp­bell, and Giovanni Nakpil; 
ILM Technical Directors Pat Conran, Craig Hammack, Doug Smythe, and Tim Fortenberry. 5.3 Hair Simulation 
at Rhythm and Hues -Chroni­cles of Narnia Rhythm and Hues have been well known for various works on animal 
characters dating back to the Babe movie. Along with each show, the whole hair pipeline have been constantly 
revised, enhanced, and sometimes completely rewritten, to meet the ever increasing demand of production 
in dealing with hair. For the movie, The Chronicles of Narnia, the Lion, the Witch and the Wardrobe, 
the whole hair pipeline had to be revamped in many aspects. The movie had many talking animal characters, 
including the majestic lion, aslan. Dealing with fur of each character presented enormous challenges 
on every side of pipeline. Animating fur -espe­cially longer hairs like the mane of a lion -presented 
a challenge that the studio had not dealt with before. A new hair dynamics solution as well as many other 
tools had to be developed and the tools were extensively used to simulate motion of the hair of many 
such mythological characters. When the crews had a chance to see and interact with wild animals (such 
as a real lion!), two things were pointed out. Most animal fur is very stiff.  Animal fur almost always 
move in clumps, apparently due to hair-hair in­  teraction This meant that we needed to have a stiff 
hair dynamics system with full con­trol over hair-hair interaction. As any experienced simulation software 
developer would .nd, this is not a particularly pleasant situation to be in to hear something is stiff 
in a simulation. 5.3.1 The hair simulator From the literature, one would .nd a number of choices for 
dealing with hair-like objects. Among those are articulated rigid body method, mass-spring (lumped particle), 
and continuum approach, as surveyed throughout this course note. Each method has pros and cons and one 
could argue one method s advantages over others. We decided to start with the mass-spring system since 
we had a working code from the in-house cloth simulator. There we started by adapting the existing particle­based 
simulator to hair. mass-spring structurefor hair In our simulator, each hair would be represented by 
a number of nodes, each node representing the (lumped) mass of certain portion of hair. In practice, 
each CV of guide hairs (created at the grooming stage) was used as the mass node. Such nodes are connected 
by two types of springs -linear and angular springs. Linear springs maintain the length of each hair 
segment and angular springs maintain the relative orientation between hair segments. Linear spring was 
simply taken from the standard model used for cloth simulator, but a new spring force had to be developed 
for the angular springs. We considered the use of so-called .exion springs that are widely used in cloth 
simulation. With this scheme, each spring connects nodes that are two (or more) nodes apart. However, 
after initial tests, it was apparent that this spring would not serve our purpose since there are a lot 
of ambiguities in this model and angles are not always preserved. This ambiguity would result in some 
unwanted wrinkles in the results (in the images below, all three con.gurations are considered the same 
from linear spring´s point of view). Eventually, the hair angle preservation had to be modeled directly 
from angles. We derived the angle preservation force by .rst de.ningan energy term de.ned on two relative 
angles between hair segments, and then by taking variational deriva­tives to derive forces. A matching 
damping force was added as well. 133 Derivation on angles are usuallyfar more dif.cult thanworking on 
positions, and it would also require additional data such as anchor points attached to the root such 
that angles could be computed at the root point as well. To compute a full angle around each node, each 
node would have an attached axis that was gener­ated at the root and propagated to each node . We simulated 
the motion of each hair along with external forces such as gravity, wind forces. The time integra­tion 
was performed with a full implicit integration scheme. As a consequence, the simulator was very stable 
dealing with the stiffhair problem. Extremely stiffhairs (such as wire) needed some numerical treatment 
such as modi.cation of jacobian matrices, etc., but in general, this new approach was made very stable 
and could handle very stiff hairs (almost like a wire) in a .xed time stepping scheme. In the absence 
of collision and hair-hair interaction, each hair could be solved independently, and solved very fast 
if a direct numerical method was employed (thousands of guide hairs could be simulated in less than a 
second per frame). In practice, the simulation time was dominated by collision detection and hair hair 
interaction. Overall integrator time was only a small fraction (less than 3%). Collision Handling There 
are two types of collision in hair simulation -Hair would collide against the character body, but would 
also collide against other hairs. These two cases were separately handled, and each case presented challenges 
and issues. Collision handling between hair and characters. For collision handling, each character was 
assigned as a collision object and col­lision of each hair against the collision object was performed 
using the standard collision detection techniques (such as AABB, Hashing, OBB, etc.) with some speed 
optimizations (e.g. limiting distance query to the length of hair, etc.) added. If a CV was found to 
be penetrating the collision object, it was pushed out by a projection scheme that was tied to our implicit 
integrator. For most cases, the simple scheme worked very well, even in some situations where collision 
objects are pinched between themselves. However, in character animation, some amount of pinching is unavoidable 
(es­pecially when characters are rolling or being dragged on the ground), and the simulator had to be 
constantly augmented and modi.ed to handle such special case handling of user error in collision handling. 
For example, in some scenes, hair roots often lie deep under the ground. In such cases, applying standard 
collision handler would push things out to the surface, but hair had to be pulled back since the root 
had to lie under the ground. This would eventually result in oscillations and jumps in the simulation. 
Our simulator had additional routines to detect such cases and provided options to freeze the simulation 
for the spot or to blend in simulation results. In addition, animations were adjusted (mostly for aesthetical 
reasons) and other deformation tools were also employed to sculpt out the problem area in collision. 
Hair-hair interaction Early on, it was determined that the ability to simulate hair-hair interaction 
was a key feature that we wanted to have. Without hair-hair interaction, hairs would simply penetrate 
through each other, and one would lose the sense of volume in hair simulation. This was especially important 
since our hair simulation was run on guide hairs, and each guide hair could represent a certain amount 
of volumes around it. So, the sense of two volumes interacting with each other was as important as simulating 
each guide hair accurately. Having based our simulator on a mass-spring model, we added the hair interaction 
effect as additional spring force acting on hair segments. Whenever a hair is close to another hair, 
a spring force was temporarily added to prevent nearby hairs from further approaching each other, and 
also to repel too close ones away from each other. The amountof spring forcewas scaledby suchfactors 
as distance, relative velocity, and user-speci.ed strength of hair interaction. Adding wind effect In 
the movie, many scenes were shot in extremely windy environment. There was almost always some amount 
of wind in the scene, whether it was a mild breeze or gusty wind. Once we had a working simulator, the 
next challenge was to add these wind effects with full control. In general, hair simulation was .rst 
run on (only) thousands of guide hairs and then the guide hairs would drive motion of millions of hairs 
that were .nally rendered. Correspondingly, there were two controls over the wind effects. First, dynamics 
had a wind force that applies random and directional noise-driven force that would move guide hairs. 
Second, a tool called pelt wind was developed and added on top of the dynamics motion, providing subtle 
control over motion in every rendered hair Bad inputs/ user errors Throughout the production, we would 
battle issues with bad inputs to the simu­lator. In practice, inputs to simulation are never perfect 
sometimes there would be two hairs stemming from exactly the same root position, sometimes hair shape 
was too crooked. In some cases, hairs were too tangled to begin with, and hair interaction alone could 
not handle the situation. Additional hair model processing tools were then used to tame the input such 
as untangling hair orientation more evenly or straightening out crooked hairs. In the end, the simulator 
was also used as a draping tool that users could use to process and clean up some of the hand modeled 
hair geometry. 5.3.2 Layering and mixing simulations Often, digital counterpart of real actors were used 
and mixed into the real scenes. Simulations were also used for clothing of such characters (such as cape, 
skirt, etc.) and even skins of winged characters. At times, cloth simulation and hair simulation had 
to work together. Cloth would collide against hairs, but hair would in turn collide with cloth. In such 
cases, a proxy geometry was built to represent the outer surface of hair volume. Cloth would then collide 
against the proxy geometry and then served as collision object for hair simulation. This concept of simulation 
layering was used all over. For some hair simulation, cloth was .rst simulated as the proxy geometry 
for hair, and then hair simulation was run, roughly following the overall motion driven by the proxy 
geometry, and then individual hair motion and hair interaction was added. 5.3.3 Simulating .exible objectsfor 
crowd characters In addition to hero characters that had 2-3 hair/ cloth simulations per character, the 
whole army of characters had to be animated, and consequently their cloth, hair, and anything soft had 
to be simulated. As an example, the scene in Figure 5.23 shows 20+ hero characters, and all the background 
(crowd) characters were given another pass of simulation, to give their banner, armor, .ag, and hair 
.owing looks. The simulator used for crowd characters was adapted from our in-house cloth simulator, 
and modi.ed to meet the new requirements. For distance characters, geometry used for crowd simulation 
was of relatively low resolution (¡200 polys). The simulator had to not only run fast, but also had to 
give convincing results on such low resolution geometry. Many characters in crowd shots are not visible 
until certain moments in frames, and also change its visual importance as they move in and out of the 
camera. This fact was extensively exploited in our simulation level of detail system. In contrast to 
conventional simulation system where a simulator computes an end­to-end frame calculation, we simulated 
all the characters at each frame, and con­stantly examined whether some characters were moving out of 
the camera frus­trum. For such invisible characters, the lowest level of detail of used in simulation 
. On the other hand, as characters move closer to the camera, the detail was pro­moted and more time 
was spent on simulating higher resolution version of the geometry. Thisway, we couldkeep the desired 
.delityin motion, while minimiz­ing the requirements for computational resources. The framework required 
that simulation had to be interchangeable between dif­ferent resolutions, so special attention and care 
had to be paid to ensure that the solver´s state carries over from lower resolution to higher resolution 
without no­ticeable jump or discontinuity in motion. Typically, several cloth simulations were run per 
each character, some cloth patches representing a strip of hair (we did not run hair simulator directly 
on any crowd character) that actual hair geometrywouldbe attachedat the render time. About3 to4different 
resolutions were used and switched during simulation.Forexample, a character´ s hair would be simulated 
as a simple polygon strip at the lowest level, and then re.ned all the way up to 20-100 strips representing 
the same geometry in much higher detail. Bibliography [ALS98] C. Adler, J. Lock, and B. Stone. Rainbow 
scattering by a cylinder with a nearly elliptical cross section. Applied Optics, 37(9):1540 1550, 1998. 
[AP07] B. Audoly and Y. Pomeau. Elasticity and Geometry: from hair curls to the nonlinear response of 
shells. Oxford University Press, ` A para itre en 2007. [AUK92a] K. Anjyo, Y. Usami, and T. Kurihara. 
A simple method for extract­ing the natural beauty of hair. In Proceedings of the 19th annual conference 
on Computer graphics and interactive techniques (SIG-GRAPH). ACM SIGGRAPH, 1992. [AUK92b] K. Anjyo, Y. 
Usami, and T. Kurihara. A simple method for extract­ing the natural beauty of hair. In Computer Graphics 
Proceedings (Proceedings of the ACM SIGGRAPH 92 conference), pages 111 120, August 1992. [BAC+06] F. 
Bertails, B. Audoly, M.-P. Cani, B. Querleux, F. Leroy, and J.-L. L´ev eque. Super-helices for predicting 
the dynamics of natural hair. In ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH 06 conference), 
pages 1180 1187, August 2006. [Ban94] David C. Banks. Illumination in diverse codimensions. ACM SIGGRAPH, 
1994. Proc. of [BAQ+05] F. Bertails, B. Audoly, B. Querleux, F. Leroy, J.-L. L´ev eque, and M.-P. Cani. 
Predicting natural hair shapes by solving the statics of .exible rods. In J. Dingliana and F. Ganovelli, 
editors, Eurograph­ ics 05 (short papers). Eurographics, August 2005. Eurographics 05 (short papers). 
[Bar92] David Baraff. Dynamic simulation of non-penetrating rigid bodies. PhD thesis, Department of Computer 
Science, Cornell University, March 1992. [Bar96] David Baraff. Linear-time dynamics using lagrange multipliers. 
Proceedings of SIGGRAPH 96, pages 137 146, August 1996. [BCN03] Y. Bando, B-Y. Chen, and T. Nishita. 
Animating hair with loosely connected particles. Computer Graphics Forum, 22(3):411 418, 2003. Proceedings 
of Eurographics 03. [BCP96] K. E. Brenan, S. L. Campbell, and L. R. Petzold. Numerical Solu­tion of Initial-Value 
Problems in Differential-Algebraic Equations. Classics in Applied Mathematics. SIAM, 1996. [Ber06] F. 
Bertails. Simulation de chevelures naturelles. PhD thesis, Institut National Polytechnique de Grenoble, 
2006. [BKCN03a] F. Bertails, T-Y. Kim, M-P. Cani, and U. Neumann. Adaptive wisp tree -a multiresolution 
control structure for simulating dynamic clustering in hair motion. In ACM SIGGRAPH -EG Symposium on 
Computer Animation, pages 207 213, July 2003. [BKCN03b] F. Bertails, T-Y. Kim, M-P. Cani, and U. Neumann. 
Adaptive wisp tree: a multiresolution control structure for simulating dynamic clustering in hair motion. 
In 2003 ACM SIGGRAPH /Eurographics Symposium on Computer Animation, aug 2003. [BMC05] F. Bertails, C. 
M´enier, and M-P. Cani. A practical self-shadowing algorithm for interactive hair animation. In Proc. 
Graphics Inter­face, pages 71 78, May 2005. [BMF03] R. Bridson, S. Marino, and R. Fedkiw. Simulation 
of clothing with folds and wrinkles. In Eurographics/SIGGRAPH Symposium on Computer Animation, San Diego, 
California, 2003. [Bru99] A. Bruderlin. A method to generate wet and broken-up animal fur. In Computer 
Graphics and Applications, 1999. Proceedings. Sev­enth Paci.c Conference, pages 242 249, October 1999. 
142 [BS91] H. Bustard and R. Smith. Investigation into the scattering of light by human hair. Applied 
Optics, 24(30):3485 3491, 1991. [BW92] D. Baraff and A. Witkin. Dynamic simulation of non-penetrating 
.exible bodies. In Computer Graphics Proceedings (Proceedings of the ACM SIGGRAPH 92 conference), pages 
303 308, 1992. [BW98a] David Baraff and Andrew Witkin. Large steps in cloth simulation. Proc. of ACM 
SIGGRAPH, pages 43 54, 1998. [BW98b] David Baraffand Andrew P. Witkin. Large steps in cloth simulation. 
In Proceedings of SIGGRAPH 98, Computer Graphics Proceedings, Annual Conference Series, pages 43 54, 
July 1998. [CCK05a] B. Choe, M. Choi, and H-S. Ko. Simulating complex hair with robust collision handling. 
In ACM SIGGRAPH-EG Symposium on Computer Animation, pages 153 160, August 2005. [CCK05b] Byoungwon Choe, 
Min Gyu Choi, and Hyeong-Seok Ko. Simu­lating complex hair with robust collision handling. In 2005 ACM 
SIGGRAPH / Eurographics Symposium on Computer Animation, pages 153 160, 2005. [CJY02a] J. Chang, J. Jin, 
and Y. Yu. A practical model for hair mutual inter­actions. In ACM SIGGRAPH -EG Symposium on Computer 
Ani­mation, pages 73 80, July 2002. [CJY02b] Johnny Chang, Jingyi Jin, and Yizhou Yu. A practical model 
for mutual hair inteactions. In Proceedings of Symposium on Computer Animation. ACM SIGGRAPH, San Antonio, 
USA, July 2002. [CK05] B. Choe and H-S. Ko. A statiscal wisp model and pseudophysical approaches for 
interactive hairstyle generation. IEEE Transactions on Visualization and Computer Graphics, 11(2):153 
160, March 2005. [CPS92] R.W. Cottle, J. S. Pang, and R.E. Stone. The linear complementarity problem. 
Academic Press, 1992. [CSDI99] L. Chen, S. Saeyor, H. Dohi, and M. Ishizuka. A system of 3d hairstyle 
synthesis based on the wisp model. The Visual Computer, 15(4):159 170, 1999. 143 [DKY+00] Y. Dobashi, 
K. Kaneda, H. Yamashita, T. Okita, and T. Nishita. A simple, ef.cient method for realistic animation 
of clouds. In Computer Graphics Proceedings (Proceedings of the ACM SIG-GRAPH 00 conference), pages 19 
28.ACM Press/Addison-Wesley Publishing Co., 2000. [DMTKT93] A. Daldegan, N. Magnenat-Thalmann, T. Kurihara, 
and D. Thal­mann. An integrated system for modeling, animating and rendering hair. Computer GraphicsForum, 
12(3):211 221, 1993. [Dur04] D. Durville. Modelling of contact-friction interactions in entangled .brous 
materials. In Procs of the SixthWorld Congress on Compu­tational Mechanics (WCCM VI), September 2004. 
[Fea87] RoyFeatherstone. Robot Dynamics Algorithms. Kluwer Academic Publishers, 1987. [GBF03] E. Guendelman, 
R. Bridson, and R. Fedkiw. Nonconvex rigid bod­ies with stacking. ACM Transactions on Graphics (SIGGRAPH 
Proceedings), 2003. [Gol97] D. Goldman. Fake fur rendering. In Proceedings of ACM SIG-GRAPH 97, Computer 
Graphics Proceedings, Annual Conference Series, pages 127 134, 1997. [GZ02] Y. Guang and H. Zhiyong. 
Amethod of human short hair modeling and real time animation. In Proceedings of Paci.c Graphics 02, pages 
435 438, September 2002. [Had03] Sunil Hadap. Hair Simulation. PhD thesis, MIRALab, CUI, Uni­versity 
of Geneva, January 2003. [HKS98] T. Hou, I. Klapper, and H. Si. Removing the stiffness of curvature in 
computing 3-d .laments. J. Comput. Phys., 143(2):628 664, 1998. [HMT01a] S. Hadap and N. Magnenat-Thalmann. 
Modeling dynamic hair as a continuum. Computer Graphics Forum, 20(3):329 338, 2001. Proceedings of Eurographics 
01. [HMT01b] Sunil Hadap and Nadia Magnenat-Thalmann. Modeling dynamic hair as a continuum. Computer 
Graphics Forum, 20(3):329 338, 2001. [HS98] W. Heidrich and H.-P. Seidel. Ef.cient rendering of anisotropic 
surfaces using computer graphics hardware. Proc. of Image and Multi-dimensional Digital Signal Processing 
Workshop (IMDSP), pages 315 318, 1998. [JLD99] H. Jensen, J. Legakis, and J. Dorsey. Rendering of wet 
material. Rendering Techniques, pages 273 282, 1999. [KANB03] Z. Ka.ci´c-Alesi ´c, M. Nordenstam, and 
D. Bullock. A practical dy­namics system. In SCA 03: Proceedings of the 2003 ACM SIG­GRAPH/Eurographics 
symposium on Computer animation, pages 7 16. Eurographics Association, 2003. [KAT93] T. Kurihara, K. 
Anjyo, and D. Thalmann. Hair animation with col­lision detection. In Proceedings of Computer Animation 
93, pages 128 138. Springer, 1993. [Ken04] Erleben Kenny. Stable, Robust, and Versatile Multibody Dynamics 
Animation. PhD thesis, Department of Computer Science, Univer­sity of Copenhagen, November 2004. [KH00] 
C. Koh and Z. Huang. Real-time animation of human hair modeled in strips. In EG workshop on Computer 
Animation and Simulation (EG CAS 00), pages 101 112, September 2000. [KH01] C. Koh and Z. Huang. A simple 
physics model to animate hu­man hair modeled in 2D strips in real time. In EG workshop on Computer Animation 
and Simulation (EG CAS 01), pages 127 138, September 2001. [KHS04] M. Koster, J. Haber, and H-P. Seidel. 
Real-time rendering of human hair using programmable graphics hardware. In Computer Graph­ics International 
(CGI 04), pages 248 256, June 2004. [Kim02] Tae-Yong Kim. Modeling, Rendering, and Animating Human Hair. 
PhD thesis, University of Southern California, 2002. [KK89] J. Kajiya and T. Kay. Rendering fur with 
three dimensional textures. In Computer Graphics Proceedings (Proceedings of the ACM SIG-GRAPH 89 conference), 
Computer Graphics Proceedings, Annual Conference Series, pages 271 280, New York, NY, USA, 1989. ACM 
Press. 145 [KN99] W.Kong andM. Nakajima. Visiblevolumebuffer foref.cient hair expression and shadow 
generation. In Computer Animation, pages 58 65. IEEE, 1999. [KN00] T.-Y. Kim and U. Neumann. A thin shell 
volume for modeling human hair. In Computer Animation 2000, IEEE Computer Society, pages 121 128, 2000. 
[KN01] T-Y. Kim and U. Neumann. Opacity shadow maps. In Rendering Techniques 2001, Springer, pages 177 
182, July 2001. [KN02] T-Y. Kim and U. Neumann. Interactive multiresolution hair mod­eling and editing. 
ACM Transactions on Graphics (Proceedings of theACM SIGGRAPH 02 conference), 21(3):620 629, July 2002. 
[Kok04] EvangelosKokkevis. Practicalphysics for articulated characters. In Proceedings of Game Developers 
Conference 2004, 2004. [LFHK88] B. Lindelof, B. Forslind, MA. Hedblad, and U. Kaveus. Human hair form. 
morphology revealed by light and scanning electron microscopy and computer aided three-dimensional reconstruction. 
Arch. Dermatol., 124(9):1359 1363, 1988. [LGLM00] E. Larsen, S. Gottschalk, M. Lin, and D. Manocha. Distance 
queries with rectangular swept sphere volumes. Proc. of IEEE Int. Confer­ence on Robotics andAutomation, 
pages 24 48, 2000. [LK01a] D-W. Lee and H-S.Ko. Natural hairstyle modeling and animation. Graphical Models, 
63(2):67 85, March 2001. [LK01b] Doo-Won Lee and Hyeong-Seok Ko. Natural hairstyle modeling and animation. 
Graphical Models, 63(2):67 85, March 2001. [LTT91] A. M. LeBlanc, R.Turner, and D. Thalmann. Rendering 
hair using pixel blending and shadow buffers. The Journal of Visualization and Computer Animation, 2(3):92 
97, 1991. [LV00] T. Lokovic and E. Veach. Deep shadow maps. In Computer Graphics Proceedings (Proceedings 
of the ACM SIGGRAPH 00 conference), pages 385 392.ACM Press/Addison-WesleyPublish­ing Co., 2000. 146 
 [Mar74] D. Marcuse. Light scattering from elliptical .bers. Applied Optics, 13:1903 1905, 1974. [Mir96] 
Brian Mirtich. Impulse-based Dynamic Simulation of Rigid Body Systems. PhD thesis, University of California, 
Berkeley, December 1996. [Mit96] D. Mitchell. Consequences of strati.ed sampling in graphics. In Computer 
Graphics Proceedings (Proceedings of the ACM SIG-GRAPH 96 conference), pages 277 280, 1996. [MJC+03a] 
S. Marschner, H. Jensen, M. Cammarano, S. Worley, and P. Hanra­han. Light scattering from human hair 
.bers. ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH 03 conference), 22(3):281 290, July 
2003. [MJC+03b] S. Marschner, H. W. Jensen, M. Cammarano, S. Worley, and P. Han­rahan. Light scattering 
from human hair .bers. ACM Transactions on Graphics, 22(3):780 791, July 2003. Proceedings of ACM SIG-GRAPH 
2003. [MKBR04] T. Mertens, J. Kautz, P. Bekaert, and F. Van Reeth. A self-shadow algorithm for dynamic 
hair using density clustering. In Proceedings of Eurographics Symposium on Rendering, pages 173 178, 
2004. [MTM98] C. Mount, D. Thiessen, and P. Marston. Scattering observations for tilted transparent .bers. 
Applied Optics, 37(9):1534 1539, 1998. [NdP98] I. Neulander and M. Van de Panne. Rendering generalized 
cylinders with paintstrokes. In Graphics Interface, pages 233 242, 1998. [Ney98] F. Neyret. Modeling 
animating and rendering complex scenes using volumetric textures. IEEE Transaction on Visualization and 
Com­puter Graphics, 4(1):55 70, Jan-Mar 1998. [NR01] O. Nocent and Y. Remion. Continuous deformation 
energy for dy­namic material splines subject to .nite displacements. In EG work­shop on Computer Animation 
and Simulation (EG CAS 01), pages 88 97, September 2001. [OBKA03] H. Ono, S. Benza, and Z. Ka.ci´c-Alesi 
´c. Bringing digital crash dum­mies to life for the hulk . In SIGGRAPH Sketches and Applica­tions, San 
Diego, California, 2003. ACM SIGGRAPH. 147 [Ove91] C.VanOverveld.An iterative approachto dynamic simulationof3­Drigid-body 
motions for real-time interactive computer animation. TheVisual Computer, 7:29 38, 1991. [Pai02a] D.Pai. 
Strands: Interactive simulation of thin solids using cosserat models. Computer Graphics Forum, 21(3):347 
352, 2002. Pro­ceedings of Eurographics 02. [Pai02b] D. K. Pai. Strands: Interactive simulation of thin 
solids using cosserat models. Computer GraphicsForum, 21(3):347 352, 2002. [PCP01a] E. Plante, M-P. Cani, 
andP. Poulin. A layered wisp model for sim­ulating interactions inside long hair. In EG workshop on Computer 
Animation and Simulation (EG CAS 01), Computer Science, pages 139 148. Springer, September 2001. [PCP01b] 
Eric Plante, Marie-Paule Cani, and Pierre Poulin. A layered wisp model for simulating interactions inside 
long hair. In Proceedings of Eurographics Workshop, Computer Animation and Simulation. EUROGRAPHICS, 
Manchester, UK, September 2001. [PCP02] E. Plante, M-P. Cani, and P. Poulin. Capturing the complexity 
of hair motion. Graphical Models (Academic press), 64(1):40 58, jan­uary 2002. submitted Nov. 2001, accepted, 
June 2002. [PK96] F. C.Park and I. G. Kang. Cubic interpolation on the rotation group using cayleyparameters. 
In Proceedings of the ASME 24th Biennial Mechanisms Conference, Irvine, CA, 1996. [PS03] Jong-Shi Pang 
and David E. Stewart. Differ­ential variational inequalities. Technical report, http://www.cis.upenn.edu/davinci/publications, 
2003. [PT96] J.S.Pang andJ.C.Trinkle. Complementarity formulations andex­istence of solutions of dynamic 
multi-rigid-body contact problems with coulomb friction. Mathematical Programming, 73:199 226, 1996. 
[QT96] H. Qin and D. Terzopoulos. D-nurbs: A physics-based frame­work for geometric design. IEEE Transactions 
on Visualization and Computer Graphics, 2(1):85 96, 1996. 148 [RCFC03] L. Raghupathi, V. Cantin, F. 
Faure, and M.-P. Cani. Real-time sim­ulation of self-collisions for virtual intestinal surgery. In Nicholas 
Ayache and Herv ´e Delingette, editors, Proceedings of the Interna­tional Symposium on Surgery Simulation 
and Soft Tissue Modeling, number 2673 in Lecture Notes in Computer Science, pages 15 26. Springer-Verlag, 
2003. [RCT91a] R. Rosenblum, W. Carlson, and E. Tripp. Simulating the structure and dynamics of human 
hair: Modeling, rendering and animation. The Journal of Visualization and Computer Animation, 2(4):141 
148, October-December 1991. [RCT91b] R. Rosenblum, W. Carlson, and E. Tripp. Simulating the structure 
and dynamics of human hair: Modeling, rendering, and animation. The Journal of Visualization and Computer 
Animation, 2(4):141 148, 1991. [RGL05a] S. Redon, N. Galoppo, and M. Lin. Adaptive dynamics of articu­lated 
bodies. ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH 05 conference), 24(3):936 945, 
2005. [RGL05b] Stephane Redon, Nico Galoppo, and Ming C. Lin. Adaptive dy­namics of articulated bodies. 
ACM Transactions on Graphics, 24(3):936 945, aug 2005. [RJFdJ04] Jos´e Ignacio Rodr´iguez, Jos ´e Manuel 
Jim´enez, Francisco Javier Fu­nes, and Javier Garc´ia de Jal´on. Recursive and residual algorithms for 
the ef.cient numerical integration of multi-body systems. Multi­body System Dynamics, 11(4):295 320, 
May 2004. [Rob94] Clarence R. Robbins. Chemical and Physical Behavior of Human Hair. Springer-Verlag, 
New York, third edition, 1994. [Rob02] C. Robbins. Chemical and Physical Behavior of Human Hair. ed. 
Springer, 2002. 4th [Rub00] M.B. Rubin. Cosserat Theories: Shells, Rods and Points. Springer, 2000. [SGF77] 
Robert F. Stamm, Mario L. Garcia, and Judith J. Fuchs. The optical properties of human hair i. fundamental 
considerations and gonio­photometer curves. J. Soc. Cosmet. Chem., 28:571 600, 1977. 149 [Sha01] Ahmed 
A. Shabana. Computational Dynamics. Wiley-Interscience, 2001. [Tri70] R. Tricker. Introduction to Meteorological 
Optics. Mills &#38; Boon, London, 1970. [WBK+07] K. Ward, F. Bertails, T.-Y. Kim, S. Marschner, M.-P. 
Cani, and M. Lin. A survey on hair modeling: styling, simulation, and render­ing. IEEE Transactions on 
Visualization and Computer Graphics, 13(2):213 234, Mar/Apr. 2007. [WGL04] K. Ward, N. Galoppo, and M. 
Lin. Modeling hair in.uenced by water and styling products. In Proceedings of Computer Animation and 
Social Agents (CASA 04), pages 207 214, May 2004. [WGL06] K. Ward, N. Galoppo, and M. Lin. A simulation-based 
vr system for interactive hairstyling. In IEEE Virtual Reality -Application and Research Sketches, pages 
257 260, 2006. [WGL07] K.Ward, N. Galoppo, and M. Lin. Interactive virtual hair salon. In PRESENCE to 
appear (June 2007), 2007. [WL03] K.Ward and M. Lin. Adaptive grouping and subdivision for simu­lating 
hair dynamics. In Proceedings ofPaci.c Graphics 03, pages 234 243, September 2003. [WLL+03] K. Ward, 
M. Lin, J. Lee, S. Fisher, and D. Macri. Modeling hair using level-of-detail representations. In Proceedings 
of Computer Animation and Social Agents (CASA 03), pages 41 47, May 2003. [Wol99] S.Wolfram. The Mathematica 
book (4th edition). Cambridge Uni­versity Press, NewYork, NY, USA, 1999. [WS92] Y. Watanabe and Y. Suenaga. 
A trigonal prism-based method for hair image generation. IEEE Computer Graphics and Applications, 12(1):47 
53, 1992. [WW90] A. Witkin and W. Welch. Fast animation and control of non-rigid structures. In Computer 
Graphics Proceedings (Proceedings of the ACM SIGGRAPH 90 conference), pages 243 252, 1990. 150 [YXYW00] 
X. Yang, Z. Xu, J. Yang, and T. Wang. The cluster hair model. Graphics Models and Image Processing, 62(2):85 
103, March 2000. [ZFWH04] C. Zeller, R. Fernando, M. Wloka, and M. Harris. Programming graphics hardware. 
In Eurographics -Tutorials, September 2004. [Zvi86] C. Zviak. The Science of Hair Care. Marcel Dekker, 
1986.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401248</section_id>
		<sort_key>1160</sort_key>
		<section_seq_no>27</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Complexity and Accessibility: Sorting in space: multidimensional, spatial, and metric data structures for CG applications]]></section_title>
		<section_page_from>27</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098896</person_id>
				<author_profile_id><![CDATA[81365590796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hanan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Samet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401249</article_id>
		<sort_key>1170</sort_key>
		<display_label>Article No.</display_label>
		<pages>106</pages>
		<display_no>90</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Sorting in space]]></title>
		<subtitle><![CDATA[multidimensional, spatial, and metric data structures for computer graphics applications]]></subtitle>
		<page_from>1</page_from>
		<page_to>106</page_to>
		<doi_number>10.1145/1401132.1401249</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401249</url>
		<abstract>
			<par><![CDATA[<p>The representation of spatial data is an important issue in game programming, computer graphics, visualization, solid modeling, and related areas including computer vision and geographic information systems (GIS). A wide number of representations is currently in use. Recently, there has been much interest in hierarchical data structures such as quadtrees, octrees, and pyramids which are based on image hierarchies, as well methods that make use of bounding boxes which are based on object hierarchies. The key advantage of these representations is that they provide a way to index into space. In fact, they are little more than multidimensional sorts. They are compact and depending on the nature of the spatial data they save space as well as time and also facilitate operations such as search.</p> <p>In this course we provide a brief overview of hierarchical spatial data structures and related algorithms that make use of them. We describe hierarchical representations of points, lines, collections of small rectangles, regions, surfaces, and volumes. For region data, we point out the dimension-reduction property of the region quadtree and octree, as how to navigate between nodes in the same tree, thereby leading to the popularity of these representations in ray tracing applications. We also demonstrate how to use these representations for both raster and vector data. In the case of nonregion data, we show how these data structures can be used to compute nearest objects in an incremental fashion so that the number of objects need not be known in advance. We also review a number of different tessellations and show why hierarchical decomposition into squares instead of triangles or hexagons is preferred. In addition a demonstration of the SAND spatial browser based on the SAND spatial database system and of the VASCO JAVA applet illustrating these methods (found at http://www.cs.umd.edu/~hjs/quadtree/index.html is presented.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Graphics data structures and data types</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Sorting and searching</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>E.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003152.10003161</concept_id>
				<concept_desc>CCS->Information systems->Information storage systems->Record storage systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10010031.10010033</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Data structures design and analysis->Sorting and searching</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010070.10010111.10011710</concept_id>
				<concept_desc>CCS->Theory of computation->Theory and algorithms for application domains->Database theory->Data structures and algorithms for data management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011024.10011028</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language features->Data types and structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10010031</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Data structures design and analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10002971</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Data structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010394</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics file formats</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098897</person_id>
				<author_profile_id><![CDATA[81100139629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hanan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Samet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Maryland, College Park, MD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1076819</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Same06} H. Samet. &lt;u&gt;Foundations of Multidimensional and Metric Data Structures&lt;/u&gt;. Morgan-Kaufmann, San Francisco, CA, USA, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77589</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Same90a} H. Samet. &lt;u&gt;The Design and Analysis of Spatial Data Structures&lt;/u&gt;, Addison-Wesley, Reading, MA, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77587</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{Same90b} H. Samet. &lt;u&gt;Applications of Spatial Data Structures: Computer Graphics, Image Processing, and GIS&lt;/u&gt;, Addison-Wesley, Reading, MA, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sorting in Space Multidimensional, Spatial, and Metric Data Structures for Computer Graphics Applications 
 Hanan Samet hjs@cs.umd.edu http://www.cs.umd.edu/ hjs Department of Computer Science University of 
Maryland College Park, MD 20742, USA Unless explicitly stated otherwise, the upper-left corner of These 
notes may not be reproduced by any means each slide indicates the page numbers in Foundations of (mechanical 
or electronic or any other) or posted on any Multidimensional and Metric Data Structures by H. Samet, 
web site without the express written permission of Hanan Morgan-Kaufmann, San Francisco, 2006, where 
more Samet details on the topic can be found Copyright @2008 Hanan Samet Sorting in Space p.1/3 TITLE: 
Sorting in Space: Multidimensional, Spatial, and Metric Data Structures for Com­puter Graphics Applications 
Hanan Samet Computer Science Department Center for Automation Research Institute for Advanced Computer 
Studies University of Maryland College Park, Maryland 20742 e-mail: hjs@cs.umd.edu url: http://www.cs.umd.edu/~hjs 
 SUMMARY STATEMENT: We show how to represent spatial data using techniques that sort the data with respect 
to the space that it occupies. These techniques include quadtrees, octrees, and bounding volume hierarchies 
and are useful for speeding-up operations involving search in all computer graphics applications including 
games, ray tracing, and solid modeling. COURSE ABSTRACT: The representation of spatial data is an important 
issue in game programming, computer graphics, visualization, solid modeling, and related areas including 
computer vision and geographic informa­tion systems (GIS). A wide number of representations is currently 
in use. Recently, there has been much interest in hierarchical data structures such as quadtrees, octrees, 
and pyramids which are based on image hierarchies, as well methods that make use of bounding boxes which 
are based on object hierarchies. The key advantage of these representations is that they provide a way 
to index into space. In fact, they are little more than multidimensional sorts. They are compact and 
depend­ing on the nature of the spatial data they save space as well as time and also facilitate operations 
such as search. In this course we provide a brief overview of hierarchical spatial data structures and 
related algorithms that make use of them. We describe hierarchical representations of points, lines, 
col­lections of small rectangles, regions, surfaces, and volumes. For region data, we point out the dimension-reduction 
property of the region quadtree and octree, as how to navigate between nodes in the same tree, thereby 
leading to the popularity of these representations in ray tracing applica­tions. We also demonstrate 
how to use these representations for both raster and vector data. In the case of nonregion data, we show 
how these data structures can be used to compute nearest objects in an incremental fashion so that the 
number of objects need not be known in advance. We also review a number of different tessellations and 
show why hierarchical decomposition into squares instead of triangles or hexagons is preferred. In addition 
a demonstration of the SAND spatial browser based on the SAND spatial database system and of the VASCO 
JAVA applet illustrating these methods (found at http://www.cs.umd.edu/~hjs/quadtree/index.html is presented. 
1  PREREQUISITE: Afamiliarity with computer terminology and some programming experience. COURSE LEVEL: 
Beginner INTENDED AUDIENCE: Practitioners working in computer graphics will be given a different perspective 
on data structures found to be useful in most applications Game developers and technical managers will 
appreciate the presentation and methods described herein. COURSE SYLLABUS: 1. Introduction a. Sorting 
de.nition b. Sample queries c. Spatial Indexing d. Sorting approach e. Minimum bounding rectangles 
(e.g., R-tree) f. Disjoint cells (e.g., R+-tree, k-d-B-tree) g. Uniform grid h. Region quadtree i. 
Space ordering methods j. Pyramid k. Region quadtrees vs: pyramids  2. Points a. Point quadtree b. 
PR quadtree c. Sorting points d. K-d tree e. PR k-d tree  3. Lines a. Strip tree b. MX quadtree 
for regions c. PM1 quadtree d. PM2 quadtree e. PM3 quadtree f. PMR quadtree g. Triangulations  
2 4. Regions a. Region quadtree b. Dimension reduction c. Tessellations d. Bintree e. Generalized 
bintree f. XY-tree, treemap, puzzletree g. BSP tree  5. Bounding Box Hierarchies a. Overview b. Minimum 
bounding rectangles (e.g., R-trees) c. Searching in an R-tree d. Node over.ows e. Examples of node 
over.ow policies  6. Rectangles a. MX-CIF quadtree b. Loose quadtree or coverage .eldtree c. Partition 
.eldtree  7. Surfaces and Volumes a. Restricted quadtree b. Region octree c. PM octree  8. Metric 
Data a. vp-tree b. gh-tree c. mb-tree  9. Operations a. Incremental nearest object location b. Region 
quadtree Boolean set operations c. Region quadtree nearest neighbor .nding  3 10. Example system a. 
SAND internet browser b. JAVAspatial data applets  COURSE MATERIALS Participants receive a copy of 
the slides. In addition, there is a web site at http://www.cs.umd.edu/~hjs/quadtree/index.html where 
applets demonstrating much of the material in the course are available. Participants are referred to 
the text: H.Samet, Founda­tions of Multidimensional and Metric Data Structures, Morgan-Kaufmann, San 
Francisco, 2006. Participants also have the opportunity to obtain the book at a discount of 20% as re.ected 
at http://www.cs.umd.edu/~hjs/multidimensional-book-flyer.pdf  SPEAKER BIOGRAPHY Hanan Samet received 
the B.S. degree in engineering from the University of California, Los Ange­les, and the M.S. Degree in 
operations research and the M.S. and Ph.D. degrees in computer science from StanfordUniversity, Stanford,CA.Heisa 
Fellowofthe IEEE,ACM,and IAPR (International Association for Pattern Recognition). In 1975 he joined 
the Computer Science Department at the University of Maryland, Col­lege Park, where he is now a Professor. 
He is a member of the Computer Vision Laboratory of the Center for Automation Research and also has an 
appointment in the University of Mary­land Institute for Advanced Computer Studies. At the Computer Vision 
Laboratory he leads a number of research projects on the use of hierarchical data structures for geographic 
informa­tion systems. His research group has developed the QUILT system which is a GIS based on hierarchical 
spatial data structures such as quadtrees and octrees, the SAND system which inte­grates spatial and 
non-spatial data, the SAND Spatial Browser which enables browsing through a spatial database using a 
graphical user interface, the VASCO spatial indexing applet (found at http://www.cs.umd.edu/~hjs/quadtree/index.html), 
a symbolic image database system, and the STEWARD spatio-textual document search engine. His research 
interests are data structures, computer graphics, geographic information systems, computer vision, robotics, 
and database management systems. He is the au­thor of the recent book titled Foundations of Multidimensional 
and Metric Data Structures (http://www.mkp.com/multidimensional)published by Morgan-Kaufmann,an imprint 
of El­sevier, in 2006, and of the .rst two books on spatial data structures titled Design and Analysis 
of Spatial Data Structures, and Applications of Spatial Data Structures: Computer Graphics, Image Processing, 
and GIS, both published by Addison-Wesley in 1990. He is an Area Editor of Graph­ical Models and Image 
Processing and on the Editorial Board of Image Understanding, Journal of Visual Languages, and GeoInformatica. 
He is the co-general chair of the 15th and 16th ACM International Symposium on Advances in Geographic 
Information Systems (ACM GIS 2007 and ACM GIS 2008) and is the founding chair of the ACM Special Interest 
Group on Spatial Informa­tion (SIGSPATIAL). In 1989-1991 he served as the Capital Region Representative 
to the ACM and 4 was a member of the ACM Council. He received a best paper award in the 2008 ACM SIGMOD 
Conference. 5  Outline 1. Introduction 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. 
Bounding Box Hierarchies 6. Rectangles 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. 
Example system  Copyright .2008 Hanan Samet Sorting in Space p.2/3 Why Sorting of Spatial Data is Important 
 Most operations invariably involve search  Search is sped upby sorting the data  sort -De.nition: 
verb  1. toputina certain placeorrank accordingto kind, class,or nature 2. to arrange according to 
characteristics  Examples 1. Warnock algorithm: sorting objectsfor display vector: hidden-line elimination 
 raster: hidden-surface elimination 2. Back-to-front and front-to-back algorithms 3. BSP treesfor visibility 
determination 4. Acceleratingray tracing andray castingby .ndingray-object intersections 5. Bounding 
box hierarchies arrange space according to whether occupied or unoccupied  Copyright 2008 by Hanan Samet 
Sorting Implies the Existence of an Ordering 1. Finefor one-dimensional data sort peopleby weight and 
.nd  closest in weight to Bill and can also .nd closest in weight to Larry  sort citiesby distance 
from Chicago and .nd closest to Chicagobut can­not .nd closest to New York unless resort 2. Hardfortwo-dimensions 
as higheras notionof ordering doesnotexist unless a dominance relation holds point a = {ai |1 = i = 
d}dominates point b = {bi |1 = i = d}if ai = bi , 1 = i = d.  a does not dominate b but dominates c 
 3. Only solution is to linearize data as in a space-.lling curve sort is explicit  need implicit 
sort so no need to resort if reference point changes  Copyright 2008 by Hanan Samet hi28 PRINCE GEORGES 
COUNTY 464-466-Representative point methods Copyright &#38;#169; 2007 by Hanan Samet 464-466-Representative 
point methods hi27 EXAMPLE QUERIES ON LINE SEGMENT DATABASES Queries about line segments 1. All segments 
that intersect a given point or set of points 2. All segments that have a given set of endpoints 3. 
All segments that intersect a given line segment 4. All segments that are coincident with a given line 
segment  Proximity queries 1. The nearest line segment to a given point 2. All segments within a given 
distance from a given point (also known as a range or window query)  Queries involving attributes of 
line segments 1. Given a point, find the closest line segment of a particular type  2. Given a point, 
find the minimum enclosing polygon whose constituent line segments are all of a given type 3. Given 
a point, find all the polygons that are incident on it  Copyright &#38;#169; 2007 by Hanan Samet 464-466-Representative 
point methods gs10 WHAT MAKES CONTINUOUS SPATIAL DATA DIFFERENT 1. Spatial extent of the objects is the 
key to the difference 2. A record in a DBMS may be considered as a point in a multidimensional space 
  a line can be transformed (i.e., represented) as a point in 4-d space with (x1 , y1 , x2 , y2 )  
good for queries about the line segments  not good for proximity queries since points outside the object 
are not mapped into the higher dimensional space  representative points of two objects that are physically 
close to each other in the original space (e.g., 2-d for lines) may be very far from each other in the 
higher dimensional space (e.g., 4-d)  Ex:  problem is that the transformation only transforms the space 
occupied by the objects and not the rest of the space (e.g., the query point)  can overcome by projecting 
back to original space  3. Use an index that sorts based upon spatial occupancy (i.e., extent of the 
objects)  Copyright &#38;#169; 2007 by Hanan Samet 464-466-Representative point methods hi29.1 SPATIAL 
INDEXING REQUIREMENTS 1. Compatibility with the data being stored 2. Choose an appropriate zero or reference 
point 3. Need an implicit rather than an explicit index  impossible to foresee all possible queries 
in advance  cannot have an attribute for every possible spatial relationship a. derive adjacency relations 
 b. 2-d strings capture a subset of adjacencies  all rows  all columns  implicit index is better as 
an explicit index which, for example, sorts two-dimensional data on the basis of distance from a given 
point is impractical as it is inapplicable to other points  implicit means that don't have to resort 
the data for queries other than updates  Copyright &#38;#169; 2007 by Hanan Samet 264-265-Hierarchical 
object repesentation overview gs11 SORTING ON THE BASIS OF SPATIAL OCCUPANCY Decompose the space from 
which the data is drawn into regions called buckets (like hashing but preserves order)  Interested in 
methods that are designed specifically for the spatial data type being stored  Basic approaches to decomposing 
space  1. minimum bounding rectangles e.g., R-tree or AABB (axis-aligned) and OBB (arbitrary orientation) 
 good at distinguishing empty and non-empty space  drawbacks: a. non-disjoint decomposition of space 
 may need to search entire space b. inability to correlate occupied and unoccupied space in two maps 
 2. disjoint cells drawback: objects may be reported more than once  uniform grid a. all cells the 
same size b. drawback: possibility of many sparse cells  adaptive grid quadtree variants a. regular 
decomposition b. all cells of width power of 2  partitions at arbitrary positions a. drawback: not 
a regular decomposition b. e.g., R+-tree  Can use as approximations in filter/refine query processing 
strategy  Copyright 2008 by Hanan Samet 270-296-R-tree  h i 3 1  gz rb MINIMUM BOUNDING RECTANGLES 
Objects grouped into hierarchies, stored in a structure similar to a B-tree Drawback: not a disjoint 
decomposition of space Object has single bounding rectangle, yet area that it spans may be included in 
several bounding rectangles Examples include the R-tree and the R*-tree Order (m,M ) R-tree 1. between 
m M/2 and M entries in each node except root 2. at least 2 entries in root unless a leaf node R1: 
R2: R3: R4: R5: R6: anan C op y r i gh t &#38;#169; 2007 b y H an a n S a m e t 270-296-R-tree  h 
i 3 2  gzr vb SEARCHING FOR A POINT OR LINE SEGMENT IN AN R-TREE Drawback is that may have to examine 
many nodes since a line segment can be contained in the covering rectangles of many nodes yet its record 
is contained in only one leaf node (e.g., i in R2, R3, R4, and R5) Ex: Search for a line segment containing 
point Q  Q is in R0 Q can be in both R1 and R2 Searching R1 first means that R4 is searched but this 
leads to failure even though Q is part of i which is in R4 Searching R2 finds that Q can only be in 
R5 C op y r i gh t &#38;#169; 2007 b y H an a n S a m e t 311-R-+-tree h i 3 3 DISJOINT CELLS gz rb 
 Objects decomposed into disjoint subobjects; each subobject in different cell  Techniques differ in 
degree of regularity  Drawback: in order to determine area covered by object, must retrieve all cells 
that it occupies  R+-tree (also k-d-B-tree) and cell tree are examples of this technique  R3: op gh 
2007 C opy r i ght &#38;#169; 2007 b y H an a n S a m e t 304-311-K-d-B-tree h i 3 3 . 1 K-D-B-TREES 
 gz rb  Rectangular embedding space is hierarchically decomposed into disjoint rectangular regions 
 No dead space in the sense that at any level of the tree, entire embedding space is covered by one of 
the nodes  Blocks of k-d tree partition of space are aggregated into nodes of a finite capacity  When 
a node overflows, it is split along one of the axes  Originally developed to store points but may be 
extended to non-point objects represented by their minimum bounding boxes  Drawback: in order to determine 
area covered by object,  must retrieve all cells that it occupies  R1: R2: R3: R4: R5: 2007 an C 
op y r i gh t &#38;#169; 2007 b y H ana n S a m e t 210-Uniform grid hi34 UNIFORM GRID Ideal for uniformly 
distributed data  Supports set-theoretic operations  Spatial data (e.g., line segment data) is rarely 
uniformly distributed  Copyright &#38;#169; 2007 by Hanan Samet 211-Region quadtree hi35 QUADTREES 
 Hierarchical variable resolution data structure based on regular decomposition  Many different decomposition 
schemes and applicable to different data types:  1. points 2. lines 3. regions 4. rectangles 5. 
surfaces 6. volumes 7. higher dimensions including time  changes meaning of nearest a. nearest in 
time, OR b. nearest in distance  Can handle both raster and vector data as just a spatial index  Shape 
is usually independent of order of inserting data  Ex: region quadtree  A decomposition into blocks 
 not necessarily a tree! Copyright &#38;#169; 2007 by Hanan Samet 211-Region quadtree hi36 REGION QUADTREE 
 Repeatedly subdivide until obtain homogeneous region  For a binary image (BLACK = 1 and WHITE = 0) 
 Can also use for multicolored data (e.g., a landuse class map associating colors with crops)  Can 
also define data structure for grayscale images  A collection of maximal blocks of size power of two 
and placed at predetermined positions  1. could implement as a list of blocks each of which has a unique 
pair of numbers:  concatenate sequence of 2 bit codes correspond­ing to the path from the root to the 
block s node  the level of the block s node  2. does not have to be implemented as a tree tree good 
for logarithmic access  A variable resolution data structure in contrast to a pyramid (i.e., a complete 
quadtree) which is a multiresolution data structure  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 
1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 B F G H I J 37 38 
N O 39 40 L M 57 58 Q 59 60 A 37383940 57585960 Copyright &#38;#169; 2007 by Hanan Samet 199-201--Space 
ordering methods Ordering Space Many ways of laying out the addresses cor­ responding to the lo­ cations 
in space of the cells each having its own mapping function row order row-prime order morton order Can 
use one of many possible space-.lling curves Important to dis­ tinguish between address and location 
or cell Address of a location or cell . physical lo­ peano-hilbert order cantor-diagonal order spiral 
order cation (e.g., in mem­ ory, on disk, etc.), if any, where some of the information asso­ ciated with 
the loca­ tion or cell is stored gray code double gray order u order Chapter 2: Copyright 2007 Hanan 
Samet bg4 CONVERTING BETWEEN POINTS AND CURVES  Need to know size of image for all but the Morton 
order  Relatively easy for all but the Peano-Hilbert order which is difficult (although possible) to 
decode and encode to obtain the corresponding x and y coordinate values  Morton order  1. use bit interleaving 
of binary representation of the x and y coordinates of the point 2. also known as Z-order 3. Ex: Atlanta 
(6,1)  Copyright &#38;#169; 2008 by Hanan Samet  bg5  zr b STABILITY OF SPACE ORDERING METHODS 
An order is stable if the relative order of the individual pixels is maintained when the resolution (i.e., 
the size of the space in which the cells are embedded) is doubled or halved  Morton order is stable 
while the Peano-Hilbert order is not  Ex: Morton: Peano-Hilbert:  Result of doubling the resolution 
(i.e., the coverage) in which case the circled points do not maintain the same relative order in the 
Peano-Hilbert order while they do in the Morton order Copyright &#38;#169; 2008 by Hanan Samet 199-201--Space 
ordering methods bg6 DESIRABLE PROPERTIES OF SPACE FILLING CURVES 1. Pass through each point in the 
space once and only once  2. Two points that are neighbors in space are neighbors along the curve and 
vice versa  impossible to satisfy for all points at all resolutions 3. Easy to retrieve neighbors 
of a point 4. Curve should be stable as the space grows and contracts by powers of two with the same 
origin   yes for Morton and Cantor orders  no for row, row-prime, Peano-Hilbert, and spiral orders 
 5. Curve should be admissible at each step at least one horizontal and one vertical neighbor must 
have already been encountered  used by active border algorithms -e.g., connected component labeling 
algorithm  row, Morton, and Cantor orders are admissible  Peano-Hilbert order is not admissible  row-prime 
and spiral orders are admissible if permit the direction of the horizontal and vertical neighbors to 
vary from point to point  6. Easy to convert between two-dimensional data and the curve and vice-versa 
 easy for Morton order  difficult for Peano-Hilbert order  relatively easy for row, row-prime, Cantor, 
and spiral orders  Copyright 2008 by Hanan Samet 266-270-Pyramid hi37 PYRAMID Internal nodes contain 
summary of information in nodes below them  Useful for avoiding inspecting nodes where there could be 
no relevant information  Copyright &#38;#169; 2007 by Hanan Samet 266-270-Pyramid hi38 QUADTREES VS. 
PYRAMIDS Quadtrees are good for location-based queries 1. e.g., what is at location x? 2. not good if 
looking for a particular feature as have to examine every block or location asking are you the one I 
am looking for?  Pyramid is good for feature-based queries e.g., 1. does wheat exist in region x? if 
wheat does not appear at the root node, then impossible to find it in the rest of the structure and the 
search can cease 2. report all crops in region x just look at the root 3. select all locations where 
wheat is grown  only descend node if there is possibility that wheat is in one of its four sons implies 
little wasted work  Ex: truncated pyramid where 4 identically-colored sons are merged  Can represent 
as a list of leaf and nonleaf blocks (e.g., as a linear quadtree)  Copyright &#38;#169; 2007 by Hanan 
Samet Outline 1. Introduction 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. Bounding Box 
Hierarchies 6. Rectangles 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. Example system 
 Copyright .2008 Hanan Samet Sorting in Space p.2/3 028-037--Point quadtree POINT QUADTREE (Finkel/Bentley) 
z vgvgz rb Marriage between a uniform grid and a binary search tree  Buffalo Atlanta Miami op 2007 
anan C opy r i gh t &#38;#169; 2007 b y H a nan S a m e t 042-047--PR quadtree PR QUADTREE (Orenstein) 
r zgvgz rb 1. Regular decomposition point representation 2. Decomposition occurs whenever a block contains 
more than one point 3. Useful when the domain of data points is not discrete but finite 4. Maximum 
level of decomposition depends on the minimum separation between two points  if two points are very 
close, then decomposition can be very deep  can be overcome by viewing blocks as buckets with capacity 
c and only decomposing the block when it contains more than c points  Ex: c = 1 anan C op y r i gh 
t &#38;#169; 2007 b y H a nan S a m e t  5 4 3 2 1  042-047--PR quadtree h p 10 REGION SEARCH v g 
z r b Ex: Find all points within radius r of point A search the quadrants of p specified by l 1. SE 6. 
NE 11. All but SW  2. SE, SW 7. NE, NW 12. All but SE  3. SW 8. NW 13. All  5. SW, NW 10. All but 
NE 4. SE, NE 9NWtublAl. Copyright 2008 by Hanan Samet  zk 24 FINDING THE NEAREST OBJECT r zvgzrb Ex: 
find the nearest object to P 042-047--PR quadtree 12 E 8 7 C 6 13 9 1 4 B 5 D P 2 A 3 10 11 F new F 
 Assume PR quadtree for points (i.e., at most one point per block)  Search neighbors of block 1 in 
counterclockwise order  Points are sorted with respect to the space they occupy which enables pruning 
the search space  Algorithm:  1. start at block 2 and compute distance to P from A 2. ignore block 
3 whether or not it is empty as A is closer to P than any point in 3 3. examine block 4 as distance 
to SW corner is shorter than the distance from P to A; however, reject B as it is further from P than 
A 4. ignore blocks 6, 7, 8, 9, and 10 as the minimum distance to them from P is greater than the distance 
from Pto A  5. examine block 11 as the distance from P to the southern border of 1 is shorter than the 
distance from P to A; however, reject F as it is further from P than A  If F was moved, a better order 
would have started with block 11, the southern neighbor of 1, as it is closest 2007 anan C op y r i gh 
t &#38;#169; 2007 b y H a nan S a m e t 048-057--K-d tree K-D TREE (Bentley) v r gvgz rb Test one 
attribute at a time instead of all simultaneously as in the point quadtree  Usually cycle through all 
the attributes  Shape of the tree depends on the order in which the data is encountered  Chicago 
 op 2007 C opy r i gh t &#38;#169; 2007 b y H a nan S a m e t 071-072--PR k-d tree PR K-D TREE (Knowlton) 
vr gzvgz rb A region contains at most one data point  Analogous to EXCELL with bucket size of 1  (0,100) 
(100,100) (0,0) (100,0)  C op y r i gh t &#38;#169; 2007 b y H anan S a m e t  Outline 1. Introduction 
 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. Bounding Box Hierarchies 6. Rectangles 
 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. Example system  Copyright .2008 Hanan Samet 
Sorting in Space p.2/3 382-386--Strip tree, arc tree, BSPR  c d 4  vgz r b STRIP TREE (Ballard, Peucker) 
 Top-down hierarchical curve approximation Rectangle strips of arbitrary orientation Assume curve is 
continuous  Contact points = where the curve touches the box 1. not tangent points 2. curve need not 
be differentiable -just continuous  Terminate when all rectangles are of width W C op y r i gh t &#38;#169; 
2007 b y H anan S a m e t 382-386--Strip tree, arc tree, BSPR  c d5 SPECIAL CASES zrb 1. Closed curve 
 2. Curve extends beyond its endpoints     enclosed by a rectangle split into two rectangular strips 
C op y r i gh t &#38;#169; 2007 b y H anan S a m e t 382-386--Strip tree, arc tree, BSPR  c d6 APPLICATIONS 
gz rb 1. Curve intersection  NULL CLEAR POSSIBLE 2. Union of two curves  not possible as the result 
may fail to be continuous 3. Others length area of a closed curve intersection of curves with areas etc. 
 C op y r i gh t &#38;#169; 2007 b y H anan S a m e t 357-359--MX quadtree  h p1 3  rb MX QUADTREE 
FOR REGIONS (Hunter) Represent the boundary as a sequence of BLACK pixels in a region quadtree  Useful 
for a simple digitized polygon (i.e., non­intersecting edges)  Three types of nodes  1. interior -treat 
like WHITE nodes 2. exterior -treat like WHITE nodes 3. boundary -the edge of the polygon passes through 
them and treated like BLACK nodes  Disadvantages 1. a thickness is associated with the line segments 
 2. no more than 4 lines can meet at a point  Copyright 2008 by Hanan Samet  vgzrvgz rb PM1 QUADTREE 
 Vertex-based (one vertex per block) 365-369-PM quadtree DECOMPOSITION RULE: Partitioning occurs when 
a block contains more than one segment unless all the segments are incident at the same vertex which 
is also in the same block Shape independent of order of insertion Copyright &#38;#169; 2007 by Hanan 
Samet  cd 3 3  vgzrvgz rb PM2 QUADTREE Vertex-based (one vertex per block) 365-369-PM quadtree 
DECOMPOSITION RULE: Partitioning occurs when a block contains more than one line segment unless all the 
segments are incident at the same vertex (the vertex can be in another block!) Shape independent of 
order of insertion Copyright &#38;#169; 2007 by Hanan Samet  cd 3 4  vgzrvgz rb PM3 QUADTREE Vertex-based 
(one vertex per block) 365-369-PM quadtree DECOMPOSITION RULE: Partitioning occurs when a block contains 
more than one vertex (i.e., a PR quadtree with edges)  Shape independent of order of insertion Copyright 
&#38;#169; 2007 by Hanan Samet 374-377-PMR quadtree cd3 5 PMR QUADTREE vgz r vgz rb  Edge-based 
  Avoids having to split many times when two vertices or lines are very close as in PM1 quadtree  Probabilistic 
splitting and merging rules  Uses a splitting threshold value say N  DECOMPOSITION RULE: Split a 
block once if upon insertion the number of segments intersecting a block exceeds N Merge a block with 
its siblings if the total number of line segments intersecting them is less than N Merges can be performed 
more than once  Does not guarantee that each block will contain at most N line segments  Splitting 
threshold is not the same as bucket capacity  Shape depends on order of insertion  Copyright &#38;#169; 
2007 by Hanan Samet Triangulations  PM2 quadtree is quite useful vis-a-vis PM1 quadtree  Given a triangle 
table, only need to store at most a single vertex with each cell and can reconstruct mesh with the aid 
of clipping  Example triangular mesh   PM1 quadtree PM2 quadtree Can also formulate a PM-triangle 
quadtree variant Copyright 2008 by Hanan Samet Outline 1. Introduction 2. Points 3. Lines 4. Regions, 
Volumes, and Surfaces  5. Bounding Box Hierarchies 6. Rectangles 7. Surfaces and Volumes 8. Metric 
Data 9. Operations 10. Example system  Copyright .2008 Hanan Samet Sorting in Space p.2/3 211-Region 
quadtree hi36 REGION QUADTREE Repeatedly subdivide until obtain homogeneous region  For a binary 
image (BLACK = 1 and WHITE = 0)  Can also use for multicolored data (e.g., a landuse class map associating 
colors with crops)  Can also define data structure for grayscale images  A collection of maximal blocks 
of size power of two and placed at predetermined positions  1. could implement as a list of blocks each 
of which has a unique pair of numbers:  concatenate sequence of 2 bit codes correspond­ing to the path 
from the root to the block s node  the level of the block s node  2. does not have to be implemented 
as a tree tree good for logarithmic access  A variable resolution data structure in contrast to a 
pyramid (i.e., a complete quadtree) which is a multiresolution data structure  0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 
0 0 0 B F G H I J 37 38 N O 39 40 L M 57 58 Q 59 60 A  Copyright &#38;#169; 2007 by Hanan Samet bg7 
 357-359-MX-quadtree SPACE REQUIREMENTS 1. Rationale for using quadtrees/octrees is not so much for 
saving space but for saving execution time 2. Execution time of standard image processing algorithms 
that are based on traversing the entire image and performing a computation at each image element is proportional 
to the number of blocks in the decomposition of the image rather than their size  aggregation of space 
leads directly to execution time savings as the aggregate (i.e., block) is visited just once instead 
of once for each image element (i.e., pixel, voxel) in the aggregate (e.g., connected component labeling) 
3. If want to save space, then, in general, statistical image compression methods are superior drawback: 
statistical methods are not progressive as need to transmit the entire image whereas quadtrees lend themselves 
to progressive approximation  quadtrees, though, do achieve compression as a result of use of common 
subexpression elimination techniques a. e.g., checkerboard image b. see also vector quantization  
 4. Sensitive to positioning of the origin of the decomposition for an n x n image, the optimal positioning 
requires an O(n2 log2n) dynamic programming algorithm (Li, Grosky, and Jain) Copyright &#38;#169; 2007 
by Hanan Samet 357-359-MX-quadtree   r gz rb DIMENSION REDUCTION 1. Number of blocks necessary to 
store a simple polygon as a region quadtree is proportional to its perimeter (Hunter) implies that many 
quadtree algorithms execute in O(perimeter) time as they are tree traversals  the region quadtree is 
a dimension reducing device as perimeter (ignoring fractal effects) is a one­dimensional measure and 
we are starting with two­dimensional data  generalizes to higher dimensions a. region octree takes 
O (surface area) time and space (Meagher) b. d-dimensional data take time and space proportional to 
a O (d-1)-dimensional quantity (Walsh)  2. Alternatively, for a region quadtree, the space requirements 
double as the resolution doubles in contrast with quadrupling in the array representation  for a region 
octree the space requirements quadruple as the resolution doubles  ex. region quadtree  easy to see 
dependence on perimeter as decomposition only takes place on the boundary as the resolution increases 
  C op y r i gh t &#38;#169; 2007 b y H anan S a m e t    t l 1  ALTERNATIVE DECOMPOSITION METHODS 
r b A planar decomposition for image representation should be: 1. infinitely repetitive 2. infinitely 
decomposable into successively finer patterns  Classification of tilings (Bell, Diaz, Holroyd, and Jackson) 
1. isohedral all tiles are equivalent under the symmetry group of the tiling (i.e., when stand in one 
tile and look around, the view is independent of the tile) NO   [36] [32.4.3.2] [3.4.6.4] [34.6] [3.6.3.6] 
[4.6.12] [33.42] 196-198--Tiling methods [3.122] means 3 edges at the first vertex of the polygonal 
tile followed by 12 edges at the next two vertices gh 200 an C o p y r i ght &#38;#169; 2007 b y H ana 
n S a m e t 196-198--Tiling methods  t l 2  PROPERTIES OF TILINGS SIMILARITY r b Similarity a 
tile at level k has the same shape as a tile  [44] YES YES NO  Limited = NOT similar (i.e., cannot 
be decomposed infinitely into smaller tiles of the same shape)  Unlimited: each edge of each tile lies 
on an infinitely straight line composed entirely of edges  Only 4 unlimited tilings [44], [63], [4.82], 
and [4.6.12]   [4.82] [4.6.12] Two additional hierarchies:  Note: [4.82] and [4.6.12] are not regular 
 gh ana C o p y r i ght &#38;#169; 2 00 7 b y H anan S a m e t 196-198--Tiling methods  t l 3  PROPERTIES 
OF TILINGS ADJACENCY r b  Adjacency two tiles are neighbors if they are adjacent along an edge or 
at a vertex  Uniform adjacency = distances between the centroid of one tile and the centroids of all 
its neighbors are the same  Adjacency number of a tiling (A) = number of different adjacency distances 
  A=1 A=2 A=3  gh 2007 an C o p y r i ght &#38;#169; 2007 b y H an an S a m e t 196-198--Tiling methods 
 YES NO YES Conclusion: [44] has a lower adjacency number than [63]  [44] has a uniform orientation 
while [63] does not  [44] is unlimited while [36] is limited Use [44]!  07 ana C o p y r i g h t &#38;#169; 
2 00 7 b y H anan S a m e t 221-222--Bintree Bintree  Regular decomposition k-d tree Cycle through 
attributes A1 A2 A3 W4 W5 W6 C1 C2 B1 W3 W7 W8 W9 W2 W1 C3 B2  W2 B1 W8 C1 W9 C2 Chapter 2: Copyright 
2007 Hanan Samet 221-222--Bintree Generalized Bintree  Regular decomposition k-d tree but no need to 
cycle through attributes Need to record identity of partition axis at each nonleaf node Chapter 2: Copyright 
2007 Hanan Samet 225-230--X-Y tree, treemap, and puzzletree X-Y Tree, Treemap, and Puzzletree  Split 
into two or more parts at each partition step Implies no two successive partitions along the same attribute 
as they are combined Implies cycle through attributes in two dimensions Chapter 2: Copyright 2007 Hanan 
Samet Three-Dimensional X-Y Tree, Treemap, and Puzzletree   No longer require cycling through dimensions 
as this results in losing some perceptually appealing block combinations y x  L  225-230--X-Y tree, 
treemap, and puzzletree Chapter 2: Copyright 2007 Hanan Samet Bintree compared with X-Y Tree, Treemap, 
Puzzletree  225-230--X-Y tree, treemap, and puzzletree Much more decomposition in bintree W3 W8  X-Y 
Tree  W2 A3 W5 B2 W6 B3 W7 C3 W10 Chapter 2: Copyright 2007 Hanan Samet 233-237--BSP tree ar2 BSP 
TREES (Fuchs, Kedem, Naylor) Like a bintree except that the decomposition lines are at arbitrary orientations 
(i.e., they need not be parallel or orthogonal)  For data of arbitrary dimensions  In 2D (3D), partition 
along the edges (faces) of a polygon (polyhedron)  Ex: arrows indicate direction of positive area  
Usually used for hidden-surface elimination  1. domain is a set of polygons in three dimensions 2. 
position of viewpoint determines the order in which the BSP tree is traversed  A polygon s plane is 
extended infinitely to partition the entire space Copyright &#38;#169; 2007 by Hanan Samet 233-237--BSP 
tree  a r 3 zrb DRAWBACKS OF BSP TREES  A polygon may be included in both the left and right subtrees 
of node  Same issues of duplicate reporting as in representations based on a disjoint decomposition 
of the underlying space  Shape of the BSP tree depends on the order in which the polygons are processed 
and on the polygons chosen to serve as the partitioning plane  Not based on a regular decomposition 
thereby complicating the performance of set-theoretic operations  Ex: use line segments A in two dimensions 
B  D C 1. partition B induced by choosing B as the root 2. partition C induced by choosing C as the 
root    1 2  gh 200 an C o p y r i g ht &#38;#169; 2 007 b y H an a n S a m e t Outline 1. Introduction 
 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. Bounding Box Hierarchies 6. Rectangles 
 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. Example system  Copyright .2008 Hanan Samet 
Sorting in Space p.2/3 Bounding Box Hierarchies 1. Axis-aligned bounding boxes (AABB)  2. Oriented 
bounding boxes (OBB)  Arbitrary orientation for bounding hyperrectangles 3. Minimum bounding hyperspheres 
(sphere tree, SS-tree) 4. Combination of hyperspheres and hyperrectangles (SR-tree) 5. 3-dimensional 
pie slices (BOXTREE)  6. Truncated tetrahedra (prism tree)   Copyright 2008 by Hanan Samet Bounding 
Box Hierarchies 1. Axis-aligned bounding boxes (AABB)  2. Oriented bounding boxes (OBB)  Arbitrary 
orientation for bounding hyperrectangles 3. Minimum bounding hyperspheres (sphere tree, SS-tree) 4. 
Combination of hyperspheres and hyperrectangles (SR-tree) 5. 3-dimensional pie slices (BOXTREE)  6. 
Truncated tetrahedra (prism tree)   Copyright 2008 by Hanan Samet Bounding Box Hierarchies 1. Axis-aligned 
bounding boxes (AABB)  2. Oriented bounding boxes (OBB)  Arbitrary orientation for bounding hyperrectangles 
 3. Minimum bounding hyperspheres (sphere tree, SS-tree) 4. Combination of hyperspheres and hyperrectangles 
(SR-tree) 5. 3-dimensional pie slices (BOXTREE)  6. Truncated tetrahedra (prism tree)   Copyright 
2008 by Hanan Samet Bounding Box Hierarchies 1. Axis-aligned bounding boxes (AABB)  2. Oriented bounding 
boxes (OBB)  Arbitrary orientation for bounding hyperrectangles 3. Minimum bounding hyperspheres (sphere 
tree, SS-tree) 4. Combination of hyperspheres and hyperrectangles (SR-tree) 5. 3-dimensional pie slices 
(BOXTREE)  6. Truncated tetrahedra (prism tree)   Copyright 2008 by Hanan Samet Bounding Box Hierarchies 
 1. Axis-aligned bounding boxes (AABB)  2. Oriented bounding boxes (OBB)  Arbitrary orientation for 
bounding hyperrectangles 3. Minimum bounding hyperspheres (sphere tree, SS-tree) 4. Combination of 
hyperspheres and hyperrectangles (SR-tree) 5. 3-dimensional pie slices (BOXTREE)  6. Truncated tetrahedra 
(prism tree)  Copyright 2008 by Hanan Samet Bounding Box Hierarchies 1. Axis-aligned bounding boxes 
(AABB)  2. Oriented bounding boxes (OBB)  Arbitrary orientation for bounding hyperrectangles 3. Minimum 
bounding hyperspheres (sphere tree, SS-tree) 4. Combination of hyperspheres and hyperrectangles (SR-tree) 
 5. 3-dimensional pie slices (BOXTREE)  6. Truncated tetrahedra (prism tree)  Copyright 2008 by Hanan 
Samet 270-296--R-tree r c 1 3  MINIMUM BOUNDING RECTANGLES gz r b  Rectangles grouped into hierarchies, 
stored in another structure such as a B-tree  Drawback: not a disjoint decomposition of space  Rectangle 
has single bounding rectangle, yet area it spans may be included in several bounding rectangles  May 
have to visit several rectangles to determine the presence/absence of a rectangle Order (m,M ) R-tree 
  1. between m M/2 and M entries in each node except root  2. at least 2 entries in root unless a leaf 
node  Ex: order (2,3) R-tree op gh anan C opy r i ght &#38;#169; 2007 b y H a nan S a m e t 270-296--R-tree 
  r c 1 5 SEARCHING FOR A RECTANGLE gzr vb CONTAINING A POINT IN AN R-TREE Drawback is that may have 
to examine many nodes since a rectangle can be contained in the covering rectangles of many nodes yet 
its record is contained in only one leaf node (e.g., D in R0, R1, R2, R3, and R5) Ex: Search for the 
rectangle containing point Q   Q is in R0 Q can be in both R1 and R2 Searching R1 first means that 
R3 is searched but this leads to failure even though Q is in a part of D which is in R3 Searching R2 
finds that Q can only be in R5 op 2007 C opy r i gh t &#38;#169; 2007 b y H a nan S a m e t 270-296--R-tree 
Dynamic R-Tree Construction Differbyhowto splitover.owing node p upon insertion Con.icting goals 1. 
Reduce likelihood that each node q is visitedby the search achieveby minimizing total area spannedby 
bounding box of q (coverage)  2. minimize number of children of p thatmustbe visitedby search  operations 
achieveby minimizing area common to children so that the area that they span is not visited a multiple 
number of times (overlap) Rectangles Goal 1 Goal 2 Copyright 2008 by Hanan Samet   270-296--R-tree 
vr8 EXAMPLE DYNAMIC SPLITTING METHODS 1. Methods based on reducing coverage: exhaustive search  quadratic 
 linear  2. R*-tree minimize overlap in leaf nodes  Minimize coverage in nonleaf nodes  also reduces 
coverage by minimizing perimeter of bounding boxes of resulting nodes when effect on coverage is the 
same  when node overflows, first see if can avoid problem by reinserting a fraction of the nodes (e.g., 
30%)  3. Ang/Tan: linear with focus on reduction of overlap 4. Packed methods that make use of an ordering 
  usually order centroids of bounding boxes of objects and build a B+-tree a. Hilbert packed R-tree: 
Peano-Hilbert order  b. Morton packed R-tree: Morton order  node overflow  a. goals of minimizing 
coverage or overlap are not part of the splitting process b. do not make use of spatial extent of bounding 
boxes in determining how to split a node  Copyright 2008 by Hanan Samet 270-296--R-tree rc16  R-TREE 
OVERFLOW NODE SPLITTING POLICIES Could use exhaustive search to look at all possible partitions  Usually 
two stages:  1. pick a pair of bounding boxes to serve as seeds for resulting nodes ( seed-picking ) 
 2. redistribute remaining nodes with goal of minimizing the growth of the total area ( seed-growing 
)  Different algorithms of varying time complexity 1. quadratic: find two boxes j and k that would 
waste the most area if they were in the same node  for each remaining box i, determine the increase 
in area dij and dik of the bounding boxes of j and k resulting from the addition of i and add the box 
r for which |drj drk| is a maximum to the node with the smallest increase in area  rationale: find 
box with most preference for one of j, k  2. linear: find two boxes with greatest normalized separation 
along all of the dimensions  add remaining boxes in arbitrary order to box whose area is increased the 
least by the addition  3. linear (Ang/Tan) minimizes overlap  for each dimension, associate each 
box with the closest face of the box of the overflowing node  pick partition that has most even distribution 
 a. if a tie, minimize overlap b. if a tie, minimize coverage  Copyright &#38;#169; 2007 by Hanan 
Samet 270-296--R-tree rc17  R*-TREE Tries to minimize overlap in case of leaf nodes and minimize increase 
in area for nonleaf nodes  Changes from R-tree:  1. insert into leaf node p for which the resulting 
bounding box has minimum increase in overlap with bounding boxes of p s brothers compare with R-tree 
where insert into leaf node for which increase in area is a minimum (minimizes coverage) 2. in case 
of overflow in p, instead of splitting p as in R­tree, reinsert a fraction of objects in p known as 
forced reinsertion and similar to deferred splitting or rotation in B-trees  how do we pick objects 
to be reinserted? possibly sort by distance from center of p and reinsert furthest ones  3. in case 
of true overflow, use a two-stage process determine the axis along which the split takes place a. sort 
bounding boxes for each axis to get d lists b. choose the axis having the split value for which the 
sum of the perimeters of the bounding boxes of the resulting nodes is the smallest while still satisfying 
the capacity constraints (reduces coverage)  determine the position of the split a. position where 
overlap between two nodes is minimized b. resolve ties by minimizing total area of bounding boxes (reduces 
coverage)  Works very well but takes time due to reinsertion  Copyright &#38;#169; 2007 by Hanan Samet 
270-296--R-tree rc18 EXAMPLE OF R-TREE NODE SPLITTING POLICIES Sample collection of 1700 lines using 
m=20 and M=50 Collection of lines R*-tree Linear Quadratic Copyright &#38;#169; 2007 by Hanan Samet 
Outline 1. Introduction 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. Bounding Box Hierarchies 
 6. Rectangles 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. Example system  Copyright 
.2008 Hanan Samet Sorting in Space p.2/3 5 4 3 2 1  466-474-MX-CIF quadtree h p 1 4 MX-CIF QUADTREE 
(Kedem) z v g r b  1. Collections of small rectangles for VLSI applications 2. Each rectangle is associated 
with its minimum enclosing quadtree block  3. Like hashing: quadtree blocks serve as hash buckets 4. 
Collision = more than one rectangle in a block   resolve by using two one-dimensional MX-CIF trees 
to store the rectangle intersecting the lines passing through each subdivision point  one for y-axis 
  if a rectangle intersects both x and y axes, then associate it with the y axis  one for x-axis 
 Binary tree for y­axis through A Y2 Y7 2 Binary tree for x­axis through A X1 X3 9 X4 X5 X2 X6 7 
 C op y r i gh t &#38;#169; 2007 b y H anan S a m e t 24 466-474-MX-CIF quadtree Loose Quadtree (Octree)/Cover 
Fieldtree Overcomes drawback of MX-CIF quadtree that the width w of the minimum enclosing quadtree block 
of a rectangle o is not a function of the size of o Instead, it depends on the position of the centroid 
of o and often  considerably larger than o Solution: expand size of space spanned by each quadtree block 
of width w by expansion factor p (p> 0) so expanded block is of width (1 + p)w 1. p = 0.3 2. p = 1.0 
 Maximum w (i.e., minimum depth of minimum enclosing quadtree block) is a function of p and radius r 
of o and in­  dependent of position of centroid of o 1. Range of possible ratios w/2r : 1/(1 + p) · 
w/2r< 2/p 2. For p = 1, restricting w and r to powers of 2, w/2r takes on at most 2 values and usually 
just 1   Copyright 2008 by Hanan Samet 466-474-MX-CIF quadtree Partition Fieldtree  Alternative to 
loose quadtree (octree)/cover .eldtree at overcoming drawback of MX-CIF quadtree that the width w of 
the minimum enclosing quadtreeblockofa rectangle o is not a function of the size of o  Achieves similar 
resultby shifting positions of the centroid of quadtree blocks at successive levels of the subdivisionby 
one half of the width of theblock that is being subdivided  Subdivision rule guarantees that width of 
minimum enclosing quadtree blockfor rectangle o is boundedby8 times the maximumextent r of o  Same 
ratio is obtained for the loose quadtree (octree)/cover .eld­tree when p =1/4, and thus partition .eldtree 
is superior to the cover .eld­tree when p<1/4   Summary: cover .eldtree expands the width of the quadtreeblocks 
while the partition .eldtree shifts the posi­tions of their centroids  Copyright 2008 by Hanan Samet 
Outline 1. Introduction 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. Bounding Box Hierarchies 
 6. Rectangles 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. Example system  Copyright 
.2008 Hanan Samet Sorting in Space p.2/3 402-408-Hierarchical rectangular surface decomposition sf2 
 HIERARCHICAL RECTANGULAR DECOMPOSITION Similar to triangular decomposition  Good when data points 
are the vertices of a rectangular grid  Drawback is absence of continuity between adjacent patches of 
unequal width (termed the alignment problem)  Overcoming the presence of cracks   1. use the interpolated 
point instead of the true point (Barrera and Hinjosa) 2. triangulate the squares (Von Herzen and Barr) 
  can split into 2, 4, or 8 triangles depending on how many lines are drawn through the midpoint  if 
split into 2 triangles, then cracks still remain  no cracks if split into 4 or 8 triangles  Copyright 
&#38;#169; 2007 by Hanan Samet 402-408-Hierarchical rectangular surface decomposition vgz rb RESTRICTED 
QUADTREE (VON HERZEN/BARR) All 4-adjacent blocks are either of equal size or of ratio 2:1  Note: also 
used in finite element analysis to adptively refine an element as well as to achieve element compatibility 
(termed h-refinement by Kela, Perucchio, and Voelcker)  8-triangle decomposition rule  1.  decompose 
each block into 8 triangles (i.e., 2 triangles per edge) 2. unless the edge is shared by a larger block 
 3.  in which case only 1 triangle is formed  4-triangle decomposition rule 1.  decompose each block 
into 4 triangles (i.e., 1 triangle per edge) 2. unless the edge is shared by a smaller block 3.  in 
which case 2 triangles are formed along the edge  Prefer 8-triangle rule as it is better for display 
applications (shading) C op y r i gh t &#38;#169; 2007 b y H anan S a m e t 369-370--PM octree td3 
 OCTREES 1. Interior (voxels) analogous to region quadtree  approximate object by aggregating similar 
voxels  good for medical images but not for objects with planar faces  2. Boundary (PM octrees) adaptation 
of PM quadtree to three-dimensional data  decompose until each block contains a. one face  b. more 
than one face but all meet at same edge c. more than one edge but all meet at same vertex  impose 
spatial index on a boundary model (BRep)   Copyright 2008 by Hanan Samet   Outline 1. Introduction 
 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. Bounding Box Hierarchies 6. Rectangles 
 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. Example system  Copyright .2008 Hanan Samet 
Sorting in Space p.2/3 598-600--Distance-based indexing Basic De.nitions 1. Oftenonlyinformationavailableisa 
distance function indicatingdegreeof similarity (or dis-similarity) between all pairs of N data objects 
 2. Distance metric d: objects must reside in .nite metric space (S, d) where for o1 , o2 , o3 in S, 
d must satisfy  d(o1 ,o2 )= d(o2 ,o1 ) (symmetry)  d(o1 ,o2 ) = 0, d(o1 ,o2 )=0 iff o1 = o2 (non-negativity) 
 d(o1 ,o3 ) = d(o1 ,o2 )+ d(o2 ,o3 ) (triangle inequality)  3.Triangle inequalityisakey propertyforpruning 
search space Computing distance is expensive 4. Non-negativity property enables ignoring negative values 
in derivations Copyright 2008: Hanan Samet Similarity Searching for Multimedia Databases Applications 
 p.2/3 598-600--Distance-based indexing Pivots Identify a distinguished object or subset of the objects 
termed pivots or vantage points 1. sort remaining objects based on a. distances from the pivots, or 
 b. which pivot is the closest  2. and build index  3. use index to achieve pruning of other objects 
during search  S ' Given pivot p . S , for all objects o .. S , we know: 1. exact value of d(p, o), 
 2. d(p, o) lies within range [rlo ,rhi ] of values (ball partitioning) or  drawback is asymmetry 
of partition as outer shell is usually narrow 3. o is closer to p than to some other object p2 . S (generalized 
hyperplane partitioning) Distances from pivots are useful in pruning the search Copyright 2008: Hanan 
Samet Similarity Searching for Multimedia Databases Applications p.3/3 604-607--Pivot-based distance 
indexing (vp-tree) vp-Tree (Metric Tree; Uhlmann|Yianilos)  Ball partitioning method \ and let b 
Pick from o be median of distances of other objects from Partition into two sets and where: \w \w \ 
 \ \ E E   bb EE o) o)  oo  \8\8  Apply recursively, yielding a binary tree with pivot and 
radius values at internal nodes Choosing pivots 1. simplest is to pick at random  2. choose a random 
sample and then select median  p  Copyright 2007: Hanan Samet p.35/113  .  vp-Tree Example  d 
raq {}mg kwe  Copyright 2007: Hanan Samet Similarity Searching for Multimedia Databases Applications 
 p.36/113 .  objects in can also move hyperplane, by using  Apply recursively, yielding a binary tree 
with two pivots at internal nodes  Copyright 2007: Hanan Samet Similarity Searching for Multimedia Databases 
Applications p.39/113 . ... . .  and partition into two sets and where: . .  Pick and from .. 
..  Objects in are closer to than to (or equidistant from both), and are closer to than to .. .... 
 .. . . . . . . . . . ... ... . . .. . . . .. . . ... .  . ... . . .  .. . . . 
 gh-Tree (Metric Tree; Uhlmann)  Generalized hyperplane partitioning method . . . . .  hyperplane 
corresponds to all points satisfying .  . . . . . . .  .. . . . . . .. .. ........ . . 
 .. .. .. . .. . .. .... . .   613-616--Cluster-based distance indexing (gh-tree)  gh-Tree Example 
  (b)  Copyright 2007: Hanan Samet Similarity Searching for Multimedia Databases Applications p.40/113 
618-622--Mb-tree mb-Tree (Dehne/Noltemeier) regionpartitions areimplicit(de.nedbypivot objects)instead 
of explicit b  1. Inherit onepivotfromancestornode 2. Fewerpivotsandfewerdistancecomputationsbutperhapsdeepertree 
  ... .  3. Likebucket( )PRk-dtreeassplit wheneverregionhas objectsbut  (a) (b) Copyright 2007: 
Hanan Samet Similarity Searching for Multimedia Databases Applications p.44/113 618-622 -- Mb-tree Comparison 
of mb-tree (BSP tree) and PR k-d tree PR k-d tree Partition of underlying space Buffalo analogous 
to that of BSP tree for points (0,0) (100,0) x  mb-tree Copyright 2008 Hanan Samet Outline 1. Introduction 
 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. Bounding Box Hierarchies 6. Rectangles 
 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. Example system  Copyright .2008 Hanan Samet 
Sorting in Space p.2/3 490-499--Incremental nearest neigbhor finding Incremental Nearest Neighbors (Hjaltason/Samet) 
 Motivation 1. oftendon tknowinadvancehowmany neighborswill need 2. e.g.,want nearest city toChicagowithpopulation>1 
million  Several approaches 1. guesssomearearangearoundChicagoand checkpopulationsof citiesin range 
 if .nd acity withpopulation>1 million,mustmakesurethatthere are no other cities that are closer withpopulation 
>1 million  inef.cient ashave toguess size of area to search  problem withguessingis we may choose 
too small a region or too large a region a. if size too small, area may not contain any cities with 
right population and need to expand the search region b. if sizetoolarge,maybeexamining many citiesneedlessly 
 2. sort all thecitiesbydistancefromChicago  impractical as we need to re-sort them each timepose a 
similar query with respect to another city  alsosortingisoverkill whenonly need .rstfewneighbors  
3. .nd . closest neighbors and checkpopulation condition Copyright 2007: Hanan Samet Similarity Searching 
for Multimedia Databases Applications p.98/113 490-499--Incremental nearest neigbhor finding Mechanics 
of Incremental Nearest Neighbor Algorithm  Make use of a searchhierarchy(e.g., tree) where 1. objectsatlowestlevel 
 2. object approximationsareat nextlevel(e.g.,boundingboxesinan R-tree) 3. nonleaf nodesinatree-basedindex 
  Traversesearchhierarchyina best-.rst mannersimilartoA*-algorithm instead of more traditionaldepth-.rst 
orbreadth-.rst manners 1. at each step,visit element with smallestdistancefromquery object among all 
unvisited elementsin the searchhierarchy i.e., all unvisited elements whoseparentshavebeen visited 2. 
useagloballist of elements,organizedby theirdistancefromquery object use apriorityqueue asit supports 
necessaryinsert anddelete minimum operations tiesindistance: priority tolower type numbers if still 
tied,priority to elementsdeeperin searchhierarchy Copyright 2007: Hanan Samet Similarity Searching for 
Multimedia Databases Applications p.99/113 490-499--Incremental nearest neigbhor finding Incremental 
Nearest Neighbor Algorithm Algorithm: INCNEAREST(q , S , T ) 1 Q+ NEWPRIORITYQUEUE() 2 e t+ root of 
the search hierarchy induced by q , S , and T 3ENQUEUE(Q , e , 0) 4 while not ISEMPTY(Q ) do 5 e e+ DEQUEUE(Q 
) 6 if tt 0 then /* e is an object */ 7 Report e as the next nearest object 8 else 9 for each child element 
e , of e do 10 ENQUEUE(Q , e , , d , (q e ,t) ) 1. Lines 1-3 initialize priority queue with root 2. 
In main loop take element ee closest to q off the queue  report e as next nearest object if ee is an 
object otherwise, insert child elements of e into priority queue Copyright 2007: Hanan Samet Similarity 
Searching for Multimedia Databases Applications p.100/113 490-499--Incremental nearest neigbhor finding 
Example of INCNEAREST Initially, algorithm descends tree to leaf n node containing q expand n  expand 
n .  Start growing search region  expand n.  report e as nearest neighbor  front queue  Copyright 
2007: Hanan Samet Similarity Searching for Multimedia Databases Applications p.101/113 490-499--Incremental 
nearest neigbhor finding VASCO Spatial Applet  http://www.cs.umd.edu/ hjs/quadtree/index.html Copyright 
2007: Hanan Samet Similarity Searching for Multimedia Databases Applications p.102/113 490-499--Incremental 
nearest neigbhor finding Complexity Analysis  AlgorithmisI/O optimal no nodes outside search region 
are accessed  betterpruning thanbranch andbound algorithm  Observationsfor .nding . nearest neighborsfor 
uniformly-distributed two-dimensionalpoints  expected# ofpoints onpriorityqueue: .......  expected# 
ofleaf nodesintersecting search region: .........  In worst case,priorityqueue willbe aslarge as entiredata 
set e.g., whendata objects are all nearly equidistantfromquery object  probability of worst case verylow, 
asit depends on aparticular con.guration of both thedata objects and thequery object (but: curseofdimensionality!) 
  Copyright 2007: Hanan Samet Similarity Searching for Multimedia Databases Applications p.103/113 
Section-6.3.2-Applications of Spatial Data Structures t f 1 SET OPERATIONS ON QUADTREES z r v z g r 
b UNION(S,T) : traverse S and T in tandem  1. GRAY(S) : GRAY(T) : recursively process subtrees and 
 merge if all resulting sons are BLACK BLACK(T) : result is T WHITE(T) : result is S 2. BLACK(S) 
 : result is S 3. WHITE(S) : result is T  11 12 1314 15 16 1718  INTERSECTION: interchange roles 
of BLACK and WHITE in UNION  Execution time is bounded by sum of nodes in two input trees but may be 
less if don't create a new copy as really just the sum of the minimum of the number of nodes at corresponding 
levels of the two quadtrees  More efficient than vectors as make use of global data  1. vectors require 
a sort for efficiency 2. region quadtree is already sorted  C op y r i gh t &#38;#169; 2007 b y H anan 
S a m e t 217-Quadtree neighbor finding  n f 1  r vgzr b NEIGHBOR FINDING OPERATIONS USING QUADTREES 
 Many image processing operations involve traversing an image and applying an operation to a pixel and 
some of its neighboring (i.e., adjacent) pixels  For quadtree/octree representations replace pixel/voxel 
by block  Neighbor is defined to be an adjacent block of greater than or equal size  Desirable to 
be able to locate neighbors in a manner that  1. is position-independent 2.  is size-independent  
3.  makes no use of additional links to adjacent nodes (e.g., ropes and nets a la Hunter) 4. just uses 
the structure of the tree or configuration of the blocks  Some block configurations are impossible, 
thereby simplifying a number of algorithms 1.  impossible for a node A to have two larger neighbors B 
and C on directly opposite sides or touching corners  2.  partial overlap of two blocks B and C with 
A is impossible since a quadtree is constructed by recursively splitting blocks into blocks that have 
side lengths that are powers of 2    C op y r i gh t &#38;#169; 2007 b y H anan S a m e t 217-Quadtree 
neighbor finding  n f 2  gz r bz r b  FINDING LATERAL NEIGHBORS OF EQUAL SIZE Algorithm: based on 
finding the nearest common ancestor 1. Ascend the tree if the node is a son of the same type as the direction 
of the neighbor (ADJ) 2.  Otherwise, the father F is the nearest common ancestor and retrace the path 
starting at F making mirror image moves about the edge shared by the neighboring blocks  Ex: E neighbor 
of A (i.e., G) NE A G1 NE NW NW NW 2 FB 6 5 NE C E 3 4 D    node procedure EQUAL_LATERAL_NEIGHBOR(P,D); 
/* Find = size neighbor of P in direction D */  D begin value pointer node P; value direction D;  EQUAL_LATERAL_NEIGHBOR(FATHER(P),D) 
end; B B NW NE SW SE NW NE SW SEA A N N ADJ(A,B) E REFLECT(A,B) E S S W W C op y r i gh t &#38;#169; 
2007 b y H anan S a m e t 217-Quadtree neighbor finding nf4  ANALYSIS OF NEIGHBOR FINDING 1. Bottom-up 
random image model where each pixel has an equal probability of being black or white  probability of 
the existence of a 2x2 block at a particular position is 1/8  OK for a checkerboard image but inappropriate 
for maps as it means that there is a very low probability of aggregation  problem is that such a model 
assumes independence  in contrast, a pixel s value is typically related to that of its neighbors  
2. Top-down random image model where the probability of a node being black or white is p and 1-2p for 
being gray model does not make provisions for merging  uses a branching process model and analysis 
is in terms of extinct branching processes  3. Use a model based on positions of the blocks in the decomposition 
 a block is equally likely to be at any position and depth in the tree  compute an average case based 
on all the possible positions of a block of size 1x1, 2x2, 4x4, etc.  1 case at depth 0, 4 cases at 
depth 1, 16 cases at depth 2, etc.  this is not a realizable situation but in practice does model the 
image accurately  Copyright &#38;#169; 2007 by Hanan Samet 217-Quadtree neighbor finding  n f 5  
gzbvgz r b ANALYSIS OF FINDING LATERAL NEIGHBORS 23·(23 1) neighbor pairs of equal sized nodes in direction 
E NCA = nearest common ancestor 1 8 have NCA at level 3 9 24 have NCA at level 2 25 56 have NCA at level 
1 Theorem: average number of nodes visited by EQUAL_LATERAL_NEIGHBOR is = 4 Proof: Let node A be at 
level i (i.e., a 2i ×2i block)  Thereare2n i·(2n i 1)possiblepositions for node A such that an equal 
sized neighbor exists in a given horizontal or vertical direction  2n i rows 2n i 1 adjacencies per 
row 2n i·20have NCA atlevel n 2n i·21have NCA atlevel n 1  2n i·2n i 1have NCA atlevel i+1 j at leve 
j 2·(j i) nodes are visited in locating an equal-sized neighbor at level i For node A veltael irection 
D, and the NCA n- 1n 2n-i · 2n - j .. · 2 · (j - i) i = 0j =i + 1 n-1 . 2n - i · (2n- i - 1) i  A 
 i = 0 nodes are visited on the average = 4 C op y r i gh t &#38;#169; 2007 b y H anan S a m e t Outline 
 1. Introduction 2. Points 3. Lines 4. Regions, Volumes, and Surfaces  5. Bounding Box Hierarchies 
 6. Rectangles 7. Surfaces and Volumes 8. Metric Data 9. Operations 10. Example system  Copyright 
.2008 Hanan Samet Sorting in Space p.2/3 490-499--Incremental nearest neigbhor finding VASCO Spatial 
Applet  http://www.cs.umd.edu/ hjs/quadtree/index.html Copyright 2007: Hanan Samet Similarity Searching 
for Multimedia Databases Applications p.102/113 SAND Internet Browser  http://www.cs.umd.edu/~brabec/sandjava/ 
 Copyright 2008 by Hanan Samet References 1. [Same06] H. Samet. Foundations of Multidimensional and 
Metric Data Structures. Morgan-Kaufmann, San Francisco, CA, USA, 2006. 2. [Same90a] H. Samet. The Design 
and Analysis of Spatial Data Structures, Addison-Wesley, Reading, MA, 1990. 3. [Same90b] H. Samet. Applications 
of Spatial Data Structures: Computer Graphics, Image Processing, and GIS, Addison-Wesley, Reading, MA, 
1990.  Copyright .2008 Hanan Samet Sorting in Space p.3/3  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401250</article_id>
		<sort_key>1180</sort_key>
		<display_label>Article No.</display_label>
		<pages>15</pages>
		<display_no>91</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A sorting approach to indexing spatial data]]></title>
		<page_from>1</page_from>
		<page_to>15</page_to>
		<doi_number>10.1145/1401132.1401250</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401250</url>
		<abstract>
			<par><![CDATA[<p>Spatial data is distinguished from conventional data by having extent. Therefore, spatial queries involve both the objects and the space that they occupy. The handling of queries that involve spatial data is facilitated by building an index on the data. The traditional role of the index is to sort the data, which means that it orders the data. However, since generally no ordering exists in dimensions greater than 1 without a transformation of the data to one dimension, the role of the sort process is one of differentiating between the data and what is usually done is to sort the spatial objects with respect to the space that they occupy. The resulting ordering is usually implicit rather than explicit so that the data need not be resorted (i.e., the index need not be rebuilt) when the queries change (e.g., the query reference objects). The index is said to order the space and the characteristics of such indexes are explored further.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.3.1</cat_node>
				<descriptor>Indexing methods</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Sorting and searching</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Spatial databases and GIS</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.4</cat_node>
				<descriptor>Performance evaluation (efficiency and effectiveness)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003317.10003359</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Evaluation of retrieval results</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003145.10003147.10010887</concept_id>
				<concept_desc>CCS->Human-centered computing->Visualization->Visualization application domains->Geographic visualization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10010031.10010033</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Data structures design and analysis->Sorting and searching</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003236</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Spatial-temporal systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003365.10003366</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Search engine architectures and scalability->Search engine indexing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003318</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Document representation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098898</person_id>
				<author_profile_id><![CDATA[81100139629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hanan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Samet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Maryland, College Park, Maryland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. J. Abel and J. L. Smith. A data structure and algorithm based on a linear key for a rectangle retrieval problem. <i>Computer Vision, Graphics, and Image Processing</i>, 24(1):1--13, October 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. J. Abel and J. L. Smith. A data structure and query algorithm for a database of areal entities. <i>Australian Computer Journal</i>, 16(4):147--154, November 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>847361</ref_obj_id>
				<ref_obj_pid>846219</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Aboulnaga and J. F. Naughton. Accurate estimation of the cost of spatial selections. In <i>Proceedings of the 16th IEEE International Conference on Data Engineering</i>, pages 123--134, San Diego, CA, February 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[W. G. Aref and H. Samet. Uniquely reporting spatial objects: yet another operation for comparing spatial data structures. In <i>Proceedings of the 5th International Symposium on Spatial Data Handling</i>, pages 178--189, Charleston, SC, August 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>191307</ref_obj_id>
				<ref_obj_pid>191246</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[W. G. Aref and H. Samet. Hashing by proximity to process duplicates in spatial databases. In <i>Proceedings of the 3rd International Conference on Information and Knowledge Management (CIKM)</i>, pages 347--354, Gaithersburg, MD, December 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>291374</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[K. Arnold and J. Gosling. <i>The JAVA</i>#8482; <i>Programming Language</i>. Addison-Wesley, Reading, MA, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>94794</ref_obj_id>
				<ref_obj_pid>94788</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. Arvo and D. Kirk. A survey of ray tracing acceleration techniques. In <i>An Introduction to Ray Tracing</i>, A. S. Glassner, ed., chapter 6, pages 201--262. Academic Press, New York, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>3975</ref_obj_id>
				<ref_obj_pid>3973</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. Ayala, P. Brunet, R. Juan, and I. Navazo. Object representation by means of nonminimal division quadtrees and octrees. <i>ACM Transactions on Graphics</i>, 4(1):41--59, January 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[G. Barequet, B. Chazelle, L. J. Guibas, J. S. B. Mitchell, and A. Tal. BOXTREE: a hierarchical representation for surfaces in 3D. In <i>Proceedings of the EUROGRAPHICS'96 Conference</i>, J. Rossignac and F. X. Sillion, eds., pages 387--396, 484, Poitiers, France, August 1996. Also in <i>Computer Graphics Forum</i>, 15(3):387--396, 484, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>98741</ref_obj_id>
				<ref_obj_pid>93597</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R*-tree: an efficient and robust access method for points and rectangles. In <i>Proceedings of the ACM SIGMOD Conference</i>, pages 322--331, Atlantic City, NJ, June 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>356797</ref_obj_id>
				<ref_obj_pid>356789</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. L. Bentley and J. H. Friedman. Data structures for range searching. <i>ACM Computing Surveys</i>, 11(4):397--409, December 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>719937</ref_obj_id>
				<ref_obj_pid>647249</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[F. Brabec and H. Samet. The VASCO R-tree JAVA#8482; applet. In <i>Visual Database Systems (VDB4). Proceedings of the IFIP TC2//WG2.6 Fourth Working Conference on Visual Database Systems</i>, pages 147--153, Chapman and Hall, L'Aquila, Italy, May 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>719808</ref_obj_id>
				<ref_obj_pid>647249</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[F. Brabec and H. Samet. Visualizing and animating R-trees and spatial operations in spatial databases on the worldwide web. In <i>Visual Database Systems (VDB4). Proceedings of the IFIP TC2//WG2.6 Fourth Working Conference on Visual Database Systems</i>, pages 123--140, Chapman and Hall, L'Aquila, Italy, May 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[F. Brabec and H. Samet. Visualizing and animating search operations on quadtrees on the worldwide web. In <i>Proceedings of the 16th European Workshop on Computational Geometry</i>, pages 70--76, Eilat, Israel, March 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>356776</ref_obj_id>
				<ref_obj_pid>356770</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D. Comer. The ubiquitous B-tree. <i>ACM Computing Surveys</i>, 11(2):121--137, June 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[L. De Floriani, M. Facinoli, P. Magillo, and D. Dimitri. A hierarchical spatial index for triangulated surfaces. In <i>Proceedings of the Third International Conference on Computer Graphics Theory and Applications (GRAPP 2008)</i>, J. Braz, N. Jardim Nunes, and J. Madeiras Pereira, eds., pages 86--91, Funchal, Madeira, Portugal, January 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>847395</ref_obj_id>
				<ref_obj_pid>846219</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J.-P. Dittrich and B. Seeger. Data redundancy and duplicate detection in spatial join processing. In <i>Proceedings of the 16th IEEE International Conference on Data Engineering</i>, pages 535--546, San Diego, CA, February 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>502524</ref_obj_id>
				<ref_obj_pid>502512</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J.-P. Dittrich and B. Seeger. GESS: a scalable similarity-join algorithm for mining large data sets in high dimensional spaces. In <i>Proceedings of the 7th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i>, pages 47--56, San Francisco, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A. Frank. Problems of realizing LIS: storage methods for space related data: the fieldtree. Technical Report 71, Institute for Geodesy and Photogrammetry, ETH, Zurich, Switzerland, June 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>89292</ref_obj_id>
				<ref_obj_pid>89284</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[A. U. Frank and R. Barrera. The Fieldtree: a data structure for geographic information systems. In <i>Design and Implementation of Large Spatial Databases---1st Symposium, SSD'89</i>, A. Buchmann, O. G&#252;nther, T. R. Smith, and Y.-F. Wang, eds., vol. 409 of Springer-Verlag Lecture Notes in Computer Science, pages 29--44, Santa Barbara, CA, July 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[H. Fuchs, Z. M. Kedem, and B. F. Naylor. On visible surface generation by a priori tree structures. <i>Computer Graphics</i>, 14(3):124--133, July 1980. Also in <i>Proceedings of the SIGGRAPH'80 Conference</i>, Seattle, WA, July 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237244</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[S. Gottschalk, M. C. Lin, and D. Manocha. OBBTree: a hierarchical structure for rapid interference detection. In <i>Proceedings of the SIGGRAPH'96 Conference</i>, pages 171--180, New Orleans, LA, August 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>602266</ref_obj_id>
				<ref_obj_pid>602259</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[A. Guttman. R-trees: a dynamic index structure for spatial searching. In <i>Proceedings of the ACM SIGMOD Conference</i>, pages 47--57, Boston, June 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[A. Henrich. A distance-scan algorithm for spatial access structures. In <i>Proceedings of the 2nd ACM Workshop on Geographic Information Systems</i>, N. Pissinou and K. Makki, eds., pages 136--143, Gaithersburg, MD, December 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>718930</ref_obj_id>
				<ref_obj_pid>647224</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[G. R. Hjaltason and H. Samet. Ranking in spatial databases. In <i>Advances in Spatial Databases---4th International Symposium, SSD'95</i>, M. J. Egenhofer and J. R. Herring, eds., vol. 951 of Springer-Verlag Lecture Notes in Computer Science, pages 83--95, Portland, ME, August 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>320255</ref_obj_id>
				<ref_obj_pid>320248</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[G. R. Hjaltason and H. Samet. Distance browsing in spatial databases. <i>ACM Transactions on Database Systems</i>, 24(2):265--318, June 1999. Also University of Maryland Computer Science Technical Report TR-3919, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>908681</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[G. M. Hunter. <i>Efficient computation and data structures for graphics</i>. PhD thesis, Department of Electrical Engineering and Computer Science, Princeton University, Princeton, NJ, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>654159</ref_obj_id>
				<ref_obj_pid>645475</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[A. Hutflesz, H.-W. Six, and P. Widmayer. The R-file: an efficient access structure for proximity queries. In <i>Proceedings of the 6th IEEE International Conference on Data Engineering</i>, pages 372--379, Los Angeles, February 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1206056</ref_obj_id>
				<ref_obj_pid>1206049</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[E. Jacox and H. Samet. Spatial join techniques. <i>ACM Transactions on Database Systems</i>, 32(1):7, March 2007. Also an expanded version in University of Maryland Computer Science Technical Report TR-4730, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253347</ref_obj_id>
				<ref_obj_pid>253260</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[N. Katayama and S. Satoh. The SR-tree: an index structure for high-dimensional nearest neighbor queries. In <i>Proceedings of the ACM SIGMOD Conference</i>, J. Peckham, ed., pages 369--380, Tucson, AZ, May 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>809229</ref_obj_id>
				<ref_obj_pid>800263</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[G. Kedem. The quad-CIF tree: a data structure for hierarchical on-line algorithms. In <i>Proceedings of the 19th Design Automation Conference</i>, pages 352--357, Las Vegas, NV, June 1982. Also University of Rochester Computer Science Technical Report TR--91, September 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[A. Klinger. Patterns and search statistics. In <i>Optimizing Methods in Statistics</i>, J. S. Rustagi, ed., pages 303--337. Academic Press, New York, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280635</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[D. E. Knuth. <i>The Art of Computer Programming: Sorting and Searching</i>, vol. 3. Addison-Wesley, Reading, MA, second edition, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[D. Meagher. Geometric modeling using octree encoding. <i>Computer Graphics and Image Processing</i>, 19(2):129--147, June 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[S. M. Omohundro. Five balltree construction algorithms. Technical Report TR-89-063, International Computer Science Institute, Berkeley, CA, December 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[J. A. Orenstein. Multidimensional tries used for associative searching. <i>Information Processing Letters</i>, 14(4):150--157, June 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>66954</ref_obj_id>
				<ref_obj_pid>67544</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[J. A. Orenstein. Redundancy in spatial databases. In <i>Proceedings of the ACM SIGMOD Conference</i>, pages 294--305, Portland, OR, June 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>28002</ref_obj_id>
				<ref_obj_pid>28001</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[J. Ponce and O. Faugeras. An object centered hierarchical representation for 3d objects: the prism tree. <i>Computer Vision, Graphics, and Image Processing</i>, 38(1):1--28, April 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>4333</ref_obj_id>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[F. P. Preparata and M. I. Shamos. <i>Computational Geometry: An Introduction</i>. Springer-Verlag, New York, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[D. R. Reddy and S. Rubin. Representation of three-dimensional objects. Computer Science Technical Report CMU-CS-78-113, Carnegie-Mellon University, Pittsburgh, PA, April 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1300094</ref_obj_id>
				<ref_obj_pid>1299946</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[A. A. G. Requicha and H. B. Voelcker. Solid modeling: a historical summary and contemporary assessment. <i>IEEE Computer Graphics and Applications</i>, 2(2):9--24, March 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>582321</ref_obj_id>
				<ref_obj_pid>582318</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[J. T. Robinson. The K-D-B-tree: a search structure for large multidimensional dynamic indexes. In <i>Proceedings of the ACM SIGMOD Conference</i>, pages 10--18, Ann Arbor, MI, April 1981.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360831</ref_obj_id>
				<ref_obj_pid>360827</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[J. B. Rothnie Jr. and T. Lozano. Attribute based file organization in a paged memory environment. <i>Communications of the ACM</i>, 17(2):63--69, February 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77587</ref_obj_id>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[H. Samet. <i>Applications of Spatial Data Structures: Computer Graphics, Image Processing, and GIS</i>. Addison-Wesley, Reading, MA, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>77589</ref_obj_id>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[H. Samet. <i>The Design and Analysis of Spatial Data Structures</i>. Addison-Wesley, Reading, MA, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1042052</ref_obj_id>
				<ref_obj_pid>1042046</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[H. Samet. Decoupling partitioning and grouping: overcoming shortcomings of spatial indexing with bucketing. <i>ACM Transactions on Database Systems</i>, 29(4):789--830, December 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1076819</ref_obj_id>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[H. Samet. <i>Foundations of Multidimensional and Metric Data Structures</i>. Morgan-Kaufmann, San Francisco, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[H. Samet and J. Sankaranarayanan. Maximum containing cell sizes in cover fieldtrees and loose quadtrees and octrees. Computer Science Technical Report TR-4900, University of Maryland, College Park, MD, October 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>282966</ref_obj_id>
				<ref_obj_pid>282957</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[H. Samet and R. E. Webber. Storing a collection of polygons using quadtrees. <i>ACM Transactions on Graphics</i>, 4(3):182--222, July 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>758354</ref_obj_id>
				<ref_obj_pid>645915</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[B. Seeger and H.-P. Kriegel. Techniques for design and implementation of efficient spatial access methods. In <i>Proceedings of the 14th International Conference on Very Large Databases (VLDB)</i>, F. Bachillon and D. J. DeWitt, eds., pages 360--371, Los Angeles, August 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>671636</ref_obj_id>
				<ref_obj_pid>645914</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[T. Sellis, N. Roussooulos, and C. Faloutsos. The <i>R</i>
&#60;sup&#62;+&#60;/sup&#62;-tree: a dynamic index for multi-dimensional objects. In <i>Proceedings of the 13th International Conference on Very Large Databases (VLDB)</i>, pages 71--79, Brighton, United Kingdom, September 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>673466</ref_obj_id>
				<ref_obj_pid>645922</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[K. Sevcik and N. Koudas. Filter trees for managing spatial data over a range of size granularities. In <i>Proceedings of the 22nd International Conference on Very Large Data Bases (VLDB</i>), T. M. Vijayaraman, A. P. Buchmann, C. Mohan, and N. L. Sarda, eds., pages 16--27, Mumbai (Bombay), India, September 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>653419</ref_obj_id>
				<ref_obj_pid>645473</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[H.-W. Six and P. Widmayer. Spatial searching in geometric databases. In <i>Proceedings of the 4th IEEE International Conference on Data Engineering</i>, pages 496--503, Los Angeles, February 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628015</ref_obj_id>
				<ref_obj_pid>627323</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[J.-W. Song, K.-Y. Whang, Y.-K. Lee, M.-J. Lee, and S.-W. Kim. Spatial join processing using corner transformation. <i>IEEE Transactions on Knowledge and Data Engineering</i>, 11(4):688--695, July/August 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[S. L. Tanimoto and T. Pavlidis. A hierarchical data structure for picture processing. <i>Computer Graphics and Image Processing</i>, 4(2):104--119, June 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[J. K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. <i>Information Processing Letters</i>, 40(4):175--179, November 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[T. Ulrich. Loose octrees. In <i>Game Programming Gems</i>, M. A. DeLoura, ed., pages 444--453. Charles River Media, Rockland, MA, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[W. Wang, J. Yang, and R. Muntz. PK-tree: a spatial index structure for high dimensional point data. In <i>Proceedings of the 5th International Conference on Foundations of Data Organization and Algorithms (FODO)</i>, pages 27--36, Kobe, Japan, November 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[J. E. Warnock. A hidden line algorithm for halftone picture representation. Computer Science Technical Report TR 4--5, University of Utah, Salt Lake City, UT, May 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[J. E. Warnock. A hidden surface algorithm for computer generated half tone pictures. Computer Science Technical Report TR 4--15, University of Utah, Salt Lake City, UT, June 1969.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>655573</ref_obj_id>
				<ref_obj_pid>645481</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[D. A. White and R. Jain. Similarity indexing with the SS-tree. In <i>Proceedings of the 12th IEEE International Conference on Data Engineering</i>, S. Y. W. Su, ed., pages 516--523, New Orleans, LA, February 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[G. Wyvill and T. L. Kunii. A functional model for constructive solid geometry. <i>Visual Computer</i>, 1(1):3--14, July 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Sorting Approach to Indexing Spatial Data* HANAN SAMET Center for Automation Research Institute for 
Advanced Computer Studies Computer Science Department University of Maryland College Park, Maryland 20742, 
USA hjs@cs.umd.edu http://www.cs.umd.edu/~hjs May 18, 2008 Abstract Spatial data is distinguished from 
conventional data by having extent. Therefore, spatial queries involve both the objects and the space 
that they occupy. The handling of queries that involve spatial data is facil­itated by building an index 
on the data. The traditional role of the index is to sort the data, which means that it orders the data. 
However, since generally no ordering exists in dimensions greater than 1 without a transformation of 
the data to one dimension, the role of the sort process is one of differentiating between the data and 
what is usually done is to sort the spatial objects with respect to the space that they occupy. The resulting 
ordering is usually implicit rather than explicit so that the data need not be resorted (i.e., the index 
need not be rebuilt) when the queries change (e.g., the query reference objects). The index is said to 
order the space and the characteristics of such indexes are explored further. 1 Introduction The representation 
of multidimensional data is an important issue in solid modeling as well as in manyother diverse .elds 
including computer-aided design (CAD), computational geometry, .nite-element analysis, and computer graphics 
(e.g., [44, 45, 47]). The main motivation in choosing an appropriate representation is to facilitate 
operations such as search. This means that the representation involves sorting the data in some manner 
to make it more accessible. In fact, the term access structure or index is often used as an alternative 
to the term data structure in order to emphasize the importance of the connection to sorting. The most 
common de.nition of multidimensional data is a collection of points in a higher dimensional space (i.e., 
greater than 1). These points can represent locations and objects in space as well as more general records 
where each attribute (i.e., .eld) corresponds to a dimension and only some, or even none, of the attributes 
are locational. As an example of nonlocational point data, consider an employee record that has attributes 
corresponding to the employee s name, address, gender, age, height, weight, and social security number 
(i.e., identity number). Such records arise in database management systems and can be treated as points 
in, for this example, a seven-dimensional space (i.e., there is one dimension for each attribute), although 
the different dimensions have different type units (i.e., name and address are strings of characters; 
gender is binary; while age, height, weight, and social security number are numbers some of which have 
are associated with different units). Note that the address attribute could also be interpreted in a 
locational sense using positioning coordinates such as latitude and longitude readings although the stringlike 
symbolic representation isfar more common. * This work was supported in part by the National Science 
Foundation under Grants EIA-00-91474, CCF-05-15241, and IIS-07­13501, Microsoft Research, NVIDIA, and 
the University of Maryland General Research Board. 1 When multidimensional data corresponds to locational 
data, we have the additional property that all of the attributes usually have the same unit (possibly 
with the aid of scaling transformations), which is distance in space. In this case, we can combine the 
distance-denominated attributes and pose queries that involve proximity. For example, we may wish to 
.nd the closest city to Chicago within the two-dimensional space from which the locations of the cities 
are drawn. Another query seeks to .nd all cities within 50 miles of Chicago. In contrast, such queries 
are not very meaningful when the attributes do not have the same type. Nevertheless, other queries such 
as range queries that seek, for example, all individuals born between 1940 and 1960 whose weight ranges 
between 150 and 200 pounds are quite common and can be posed regardless of the nature of the attributes. 
When the range of multidimensional data spans a continuous physical space (i.e., an in.nite collection 
of locations), the issues become more interesting. In particular, we are no longer just interested in 
the locations of objects, but, in addition, we are also interested in the space that they occupy (i.e., 
their extent). Some example objects with extent include lines (e.g., roads, rivers), intervals (which 
can correspond to time as well as space),regionsofvaryingshapeand dimensionality(e.g.,lakes, counties,buildings,crop 
maps, polygons, polyhedra), and surfaces. The objects (when they are not points) may be disjoint or could 
even overlap. The fact that the objects have extent has a direct effect on the type of indexes that we 
need. This can be best understood by examining the nature of the queries that we wish to support. For 
example, consider a database of objects. There are three types of queries that can be posed to such a 
database. The .rst is the set of queries about the objects themselves such as .nding all objects that 
contain a given point or set of points, have a non-empty intersection with a given object, have a partial 
boundary in common, have a boundary in common, have any points in common, contain a given object, included 
in a given object, etc. The second consists of proximity queries such as the nearest object to a given 
point or object, and all objects within a given distance of a point or object (also known as a range 
or window query). The third consists of queries involving non-spatial attributes of objects such as given 
a point or object, .nding the nearest object of a particular type, the minimum enclosing object of a 
particular type, or all the objects of a particular type whose boundary passes through it. Being able 
to support the different types of queries described above has a direct effect on the type of indexes 
that are useful for such data. In particular, recall our earlier observation that a record in a conventional 
database may be considered as a point in a multidimensional space. For example, a straight line segment 
object having endpoints (x1, y1 ) and (x2 , y2 ) can be transformed (i.e., represented) as the point 
(x1 , y1 , x2 , y2 ) in a 4-d space (termed a corner transformation [50])1. This representation is good 
for queries about the line segments (the .rst type), while it is not good for proximity queries (i.e., 
the second and third type) since points outside the object are not mapped into the higher dimensional 
space. In particular, the representative points of two objects that are physically close to each other 
in the original space (e.g., 2-d for lines) may be veryfar from each other in the higher dimensional 
space (e.g., 4-d), thereby leading to large search regions. This is especially true if there is a great 
difference in the relative size of the two objects (e.g., a short line in proximity to a long line as 
in Figure 1). On the other hand, when the objects are small (e.g., their extent is small), then the method 
works reasonably well as the objects are basically point objects. The problem is that the transformation 
only transforms the space occupied by the objects and not the rest of the space (e.g., the query point). 
Proponents of the transformation method argue that this problem can be overcome by projecting back to 
original space and indexing on the projection (e.g., [54]). However, at this point, it is not unreasonable 
to ask why we bother to make the transformation in the .rst place. Figure 1: Example of two objects 
that are close to each other in the original space but are not clustered in the same region of the transformed 
space when using a transformation such as the corner transformation. 1 Although for ease of visualization, 
our discussion and examples are in terms of line segment and rectangle objects, it is applicable to data 
of arbitrary dimension such as polyhedra and hyperrectangles. It is important to observe that our notion 
of sorting spatial objects is more one of differentiating between the objects which is different from 
the conventional one which is intimately tied to the notion of providing an ordering. As we know, such 
an ordering implies a linearization which restricts the underlying data to one dimension, and such an 
ordering usually does not exist in dimensions d higher than one save for a dominance relationship (e.g., 
[39]) where point a = {ai |1 = i = d} is said to dominate point b = {bi |1 = i = d} if bi =ai , 1=i =d. 
On the other hand, it is clear that the rationale for our discussion is that the data in which we are 
interested is of dimension greater than one. This leads to the conclusion that what is needed is an index 
that sorts (i.e., differentiates) between objects on the basis of spatial occupancy(i.e., their spatial 
extent). In other words, it sorts the objects relative to the space that they occupy, and this is the 
focus of the rest of this paper. Before choosing a particular index we should also make sure that the 
following requirements are satis.ed. First of all, the index should be compatible with the type of data 
(i.e., spatial objects) that is being stored. In other words, it should enable users to distinguish between 
different objects as well as render the search ef.­cient in terms of pruning irrelevant objects from 
further consideration. Second, we must have an appropriate zero or reference point. In the case of spatial 
occupancy, this is usually some easily identi.ed point or object (e.g., the origin of the multidimensional 
space from which the objects are drawn). Most importantly, given our observation about the absence of 
an ordering, it is best to have an implicit rather than an explicit index. In particular, an implicit 
index is needed because it is impossible to foresee all possible queries in advance. For example, in 
the case of spatial relationships such as left, right, up, down, etc. it is impractical to have a data 
structure which has an attribute for every possible spatial relationship. In other words, the index should 
support the ability to derive the spatial relationships between the objects. It should be clear that 
an implicit index is superior to an explicit index, which, for example in the case of two-dimensional 
data such as the locations of cities, sorts the cities on the basis of their distance from a given point. 
The problem is that this sorting order is inapplicable to other reference points. In other words, having 
sorted all of the cities in the US with respect to their distance from Chicago, the result is useless 
if we want to .nd the closest city to New Orleans that satis.es a particular condition like having a 
population greater than 50,000 inhabitants. Therefore, having an implicit index means that we don thave 
to resort the data for queries other than updates. 2 Methods Based on Spatial Occupancy  g h a b e 
d i f c  (a) (b) (c) Figure 2: (a) Example collection of straight line segments embedded in a 4×4 
grid, (b) the object hierarchy for the R-tree corresponding to the objects in (a), and (c) the spatial 
extent of the minimum bounding rectangles corresponding to the object hierarchy in (b). Notice that the 
leaf nodes in (b) also store bounding rectangles although this is only shown for the nonleaf nodes. The 
indexing methods that are based on sorting the spatial objects by spatial occupancy essentially de­compose 
the underlying space from which the data is drawn into regions called buckets in the spirit of classical 
hashing methods, with the difference that the spatial indexing methods preserve order. In other words, 
objects in close proximity should be placed in the samebucket or at least inbuckets that are close to 
each other in the sense of the order in which they would be accessed (i.e., retrieved from secondary 
storage in case of a false hit, etc.). There are two principal methods of representing spatial data. 
The .rst is to use an object hierarchy that initially aggregates objects into groups, preferably based 
on their spatial proximity, and then uses proximity to further aggregate the groups thereby forming a 
hierarchy, where the number of objects that are aggregated in each node of the hierarchy is permitted 
to range between parameters m =.M/2.and M. The rationale for choosing this type of a range is for the 
hierarchy to mimic the behavior of a B-tree (e.g., [15]), where each element of the hierarchy acts like 
a disk page and thus is guaranteed to be half full, provided that m = .M/2.. Note that the object hierarchy 
is not unique as it depends on the manner in which the objects were aggregated to form the hierarchy 
(e.g., minimizing overlap between objects or coverage of the underlying space). Queries are facilitated 
by also associating a minimum bounding box with each object and group of objects as this enables a quick 
way to test if a point can possibly lie within the area spanned by the object or group of objects. A 
negative answer means that no further processing is required for the object or group while a positive 
answer means that further tests must be performed. Thus the minimum bounding box serves to avoid wasting 
work. Equivalently, it serves to differentiate (i.e., sort ) between occupied and unoccupied space. Data 
structures that make use of axis-aligned bounding boxes (AABB) such as the R­tree [23]and theR* -tree 
[10]illustrate the use of this method, as well as the more general oriented bounding box (OBB) where 
the sides are orthogonal, while no longer having to be parallel to the coordinate axes (e.g., [22, 40]). 
In addition, some data structures use other shapes for the bounding boxes such as spheres (e.g., SS-tree 
[35, 61]), combinations of hyperrectangles and hyperspheres (e.g.,SR-tree [30]), truncated tetrahedra 
(e.g., prism tree [38]), as well as triangularpyramids which are 5-sided objects with two parallel triangular 
faces and three rectangular faces forming a three-dimensional pie slice (e.g., BOXTREE [9]). These data 
structures differ primarily in the properties of the bounding boxes, and their interrelationships, that 
they use to determine how to aggregate the bounding boxes, and, of course, the objects. Aggregation is 
an issue when the data structure is used in a dynamic environment, where objects are inserted and removed 
from the hierarchy thereby leading to elements that are full or sparse vis-a-vis the values of m and 
M. As an example of an R-tree, consider the collection of straight line segment objects given in Figure 
2(a) shown embedded in a 4 ×4 grid. Figure 2(b) is an example of the object hierarchy induced by an R-tree 
for this collection, with m = 2 andM = 3. Figure 2(c) shows the spatial extent of the bounding rectangles 
of the nodes in Figure 2(a), with heavy lines denoting the bounding rectangles corresponding to the leaf 
nodes, and broken lines denoting the bounding rectangles corresponding to the subtrees rooted at the 
nonleaf nodes. The drawback of the object hierarchy approach is that from the perspective of a space 
decomposition method, the resulting hierarchy of bounding boxes often leads to a non-disjoint decomposition 
of the under­lying space. This means that if a search fails to .nd an object in one path starting at 
the root, then it is not necessarily the case that the object will not be found in another path starting 
at the root. This is the case in Figure 2(c) when we search for the line segment object that contains 
Q. In particular, we .rst visit nodes R1 and R4 unsuccessfully, and thus need to visit nodes R2 and R5 
in order to .nd the correct line segment object i. The second method is based on a decomposition (usually 
recursive) of the underlying space into disjoint blocks so that a subset of the objects is associated 
with each block. There are several ways to proceed. The .rst is to simply rede.ne the decomposition and 
aggregation associated with the object hierarchy method so that the minimum bounding boxes are decomposed 
into disjoint boxes, thereby also implicitly partitioning the underlying objects that theybound. In this 
case, the partition of the underlying space is heavily dependent on the data and is said to be at arbitrary 
positions. The k-d-B-tree [42]and theR+-tree [51] are examples of such an approach, with the difference 
being that in the k-d-B-tree, the entire space which contains the objects is decomposed into subspaces 
and it is these subspaces that are aggregated, while in the R+-tree, it is the bounding boxes that are 
decomposed and subsequently aggregated. Figure 3 is an example of one possible R+ -tree for the collection 
of line segments in Figure 2(a). This particular tree is of order (2,3) although in general it is not 
possible to guarantee that all nodes save for the root node willalwayshavea minimumof2entries.In particular,theexpected 
B-tree performance guarantees are not necessarily valid (i.e., pages are not guaranteed to be m/M full) 
unless we are willing to perform very complicated record insertion and deletion procedures. Notice that 
in this example line segment objects c, h, and i appearintwodifferent nodes.Of course, othervariants 
are possible sincetheR+-tree is not unique.  Figure 3: (a) R+-tree for the collection of line segments 
in Figure 2(a) with m=2 and M=3, and (b) the spatial extents of the bounding rectangles. Notice that 
the leaf nodes in the index also store bounding rectangles although this is only shown for the nonleaf 
nodes. The second way is to partition the underlying space into cells (i.e., blocks) at .xed positions 
so that all resulting cells are of uniform size, which is the case when using the uniform grid (e.g., 
[11, 33, 43]), also the standard indexing method for maps. Figure 2(a) is an example of a 4 × 4 uniform 
grid in which a collection of straight line segments has been embedded. One drawback of the uniform grid 
is the possibility of a large number of empty or sparsely-.lled cells when the objects are not uniformly 
distributed, as well as the possibility that most of the objects will lie in a small subset of the cells. 
This is resolved by making use of a variable resolution representation such as one of the quadtree variants 
(e.g., [47]) where the subset of the objects that are associated with the cells is de.ned by placing 
an upper bound on the number of objects that can be associated with each cell. The cells that comprise 
the underlying space are recursively decomposed into congruent sibling cells whenever this upper bound 
is exceeded. Therefore, the upper bound serves as a stopping condition for the recursive decomposition 
process. An alternative, as exempli.ed by the PK-tree [46, 58], makes use of a lower bound on the number 
of objects that can be associated with each cell (termed an instantiation or aggregation threshold). 
Depending on the underlying representation that is used, the result can also be viewed as a hierarchy 
of congruent cells (see, e.g., the pyramid structure [55] whichisafamilyof representationsthatmakeuseof 
multiple resolutionwhichcanbe characterizedasimage hierarchies [47]). The PR quadtree [36, 45]is one 
example of a variable resolution representation for point objects where the underlying space in which 
a set of point objects lie is recursively decomposed into four equal-sized square­shaped cells until 
each cell is empty or contains just one object. For example, Figure 4 is the PR quadtree for the set 
of point objects A F and P. The PR quadtree represents the underlying decomposition as a tree although 
our .gure only illustrates the resulting decomposition of the underlying space into cells (i.e., the 
leaf nodes/blocks of the PR quadtree). Turning to more complex such objects such as line segments, which 
have extent, we consider the PM1 quadtree [49]. It is an example of a variable resolution representation 
for a collection of straight line segment objects such as the polygonal subdivision given in Figure 2(a). 
In this case, the stopping condition of its decomposition rule stipulates that partitioning occurs as 
long as a cell contains more than one line segment unless the line segments are all incident at the same 
vertex, which is also in the same cell (e.g., Figure 5(a)), The PM1 quadtree and its variants are ideal 
for representing polygonal meshes as they provide an access structure to enable the quick determination 
of the polygon that contains a given point (i.e., a point location operation). In particular, the PM2 
quadtree [49], which differs from the PM1 quadtree by permitting a cell c to contain several line segments 
as long as they are incident at the same vertex v regardless of whether or not v is in c (e.g., Figure 
5(b)), is particularly suitable for representing triangular meshes [16]. A similar representation to 
the PM1 quadtree has been devised for collections of three-dimensional objects such as 12 E 10 6 C 7 
11 9 1 3 B 5 D P 2 A 4 13 8 F Figure 4: Block decomposition induced by the PR quadtree for the point 
objects A F and P. polyhedra images (e.g., [8] and the references cited in [47]). The decomposition criteria 
are such that no cell contains more than one face, edge, or vertex unless the faces all meet at the same 
vertex or are adjacent to the same edge.  (a) (b) Figure 5: (a) PM1 quadtree and (b) PM2 quadtree for 
a collection of straight line segment objects that form a triangulation. The above variants of the PM 
quadtree and PM octree represent an object by its boundary. The region quadtree [32] and region octree 
[27, 34] are variable resolution representations of objects by their interiors. In particular, the environment 
containing the objects is recursively decomposed into four or eight, respec­tively, rectangular congruent 
blocks until each block is either completely occupied by an object or is empty. For example, Figure 6(b) 
is the block decomposition for the region quadtree corresponding to the result of embedding the two-dimensional 
object in Figure 6(a) in an 8 × 8 grid, while Figure 7(b) is the block decom­position for the region 
octree corresponding to the three-dimensional staircaselike object in Figure 7(a). Region octrees are 
also known as volumetric or voxel representations and are useful for medical ap­plications. They are 
to be contrasted with procedural representations such as constructive solid geometry (CSG) [41] where 
primitive instances of objects are combined to form more complex objects by use of geo­metric transformations 
and regularized Boolean set operations (e.g., union, intersection). A disadvantage of the CSG representation 
is that it is not unique. In particular, there are frequently several ways of constructing an object 
(e.g., from different primitive elements). In addition, there is no overall notion of geometry except 
of the primitives that form each of the objects and thus there is no easy correlation between the objects 
and (a) Figure 6: (a) Sample object, and (b) its region quadtree block decomposition with the blocks 
of the object being shaded, assuming that it is embedded in an 8 × 8 grid. the space in which they are 
embedded unless techniques such as the PM-CSG tree [62] are used.  (a) (b) Figure 7: (a) Example three-dimensional 
object, and (b) its region octree block decomposition. The principal drawback of the disjoint method 
is that when the objects have extent (e.g., line segments, rectangles, and any other non-point objects), 
then an object is associated with more than one cell when the object has been decomposed. This means 
that queries such as those that seek the length of all objects in a particular spatial region will have 
to remove duplicate objects before reporting the total length. Nevertheless, methods have been developed 
that avoid these duplicates by making use of the geometry of the type of the data that is being represented 
(e.g., [4, 5, 17]). Note that the result of constraining the positions of the partitions means that there 
is a limit on the possible sizes of the resulting cells (e.g., a power of 2 in the case of a quadtree 
variant). However, the result is that the underlying representation is good for operations between two 
different data sets as their representations are in registration (i.e., it is easy to correlate occupied 
and unoccupied space in the two data sets, which is not easy when the positions of the partitions are 
not constrained as is the case with methods rooted in representations based an object hierarchy even 
though the resulting decomposition of the underlying space is disjoint). The PR, PM, and region quadtrees 
make use of a space hierarchy of where each level of the hierarchy contains congruent cells. The difference 
is that in the PR quadtree, each object is associated with just one cell, while in the PM and region 
quadtrees, the extent of the objects causes them to be decomposed into subobjects and thereby possibly 
be associated with more than one cell, although the cells are disjoint. At times, we want to use a space 
decomposition method that makes use of a hierarchy of congruent cells while still not decomposing the 
objects. In this case, we relax the disjointness requirement by stipulating that only the cells at a 
given level (i.e., depth) of the hierarchy must be disjoint. In particular, we recursively decompose 
the cells that comprise the underlying space into congruent sibling cells so that each object is associated 
with just one cell, and this is the smallest possible congruent cell that contains the object in its 
entirety. Assuming a top-down subdivision process that decomposes each cell into four square cells (i.e., 
a quadtree) at each level of decomposition, the result is that each object is associated with its minimum 
enclosing quadtree cell. Subdivision ceases whenever a cell contains no objects. Alternatively, subdivision 
can also cease once a cell is smaller than a predetermined threshold size. This threshold is often chosen 
to be equal to the expected size of the objects. We use the term MX-CIF quadtree [1, 31](see also the 
multilayer grid .le [53], R-.le [28], .lter tree [52], and SQ-histogram [3]) to describe such a decomposition 
method. In order to simplify our presentation, we assume that the objects stored in the MX-CIF quadtree 
are rectangles, although the MX-CIF quadtree is applicable to arbitrary objects in arbitrary dimensions 
in which caseitkeeps trackof their minimum boundingboxes.Forexample,Figure8bisthe tree representationofthe 
MX-CIF quadtree for a collection of rectangle objects given in Figure 8a. Note that objects can be associated 
with both terminal and non-terminal nodes of the tree. (a) (b) (c) (d) Figure 8: (a) Collection of rectangle 
objects and the cell decomposition induced by the MX-CIF quadtree; (b) the tree representation of (a); 
the binary trees for the y axes passing through the root of the tree in (b), and through (d) the NE son 
of the root of the tree in (b). Since there is no limit on the number of objects that are associated 
with a particular cell, an additional decomposition rule is sometimes provided to distinguish between 
these objects. For example, in the case of the MX-CIF quadtree, a one-dimensional analog of the two-dimensional 
decomposition rule is used. In particular, all objects that are associated with a given cell b are partitioned 
into two sets: those that intersect (or whose sides are collinear) with the vertical axis passing through 
the center of b, and those that intersect (or whose sides are collinear) with the horizontal axis passing 
through the center of b. Objects that intersect with the center of b are associated with the horizontal 
axis. Associated with each axis is a one-dimensional MX-CIF quadtree (i.e., a binary tree), where each 
object o is associated with the node that corresponds to o s minimum enclosing interval. For example, 
Figure 8c and Figure 8d illustrate the binary trees associated with the y axes passing through the root 
and the NE son of the root, respectively, of the MX-CIF quadtree of Figure 8b. Thus we see that the two-dimensional 
MX-CIF quadtree acts like a hashing function with the one-dimensional MX-CIF quadtree playing the role 
of a collision resolution technique. The MX-CIF quadtree can be interpreted as an object hierarchy where 
the objects appear at different levels of the hierarchy and the congruent cells play the same role as 
the minimum bounding boxes. The difference is that the set of possible minimum bounding boxes is constrained 
to the set of possible congruent cells. Thus, we can view the MX-CIF quadtree as a variable resolution 
R-tree. An alternative interpretation is that the MX-CIF quadtree provides a variable number of grids, 
each one being at half the resolution of its immediate successor, where an object is associated with 
the grid whose cells have the tightest .t. Infact, this interpretation forms the basis of the .lter tree 
[52]and the multilayer grid .le [53]where the only difference from the MX-CIF quadtree is the nature 
of the access structure for the cells (i.e., a hierarchy of grids based on a regular decomposition for 
the .lter tree and based on a grid .le for the multilayer grid .le, and a tree structure for the MX-CIF 
quadtree). One of the main drawbacks of the MX-CIF quadtree is that the size (i.e., width w)of the cell 
c corre­sponding to the minimum enclosing quadtree cell of object o s minimum enclosing bounding box 
b is nota function of the size of b or o. Instead, it is dependent on the position of o. In fact, c is 
often considerably larger than b thereby causing inef.ciency in search operations due to a reduction 
in the ability to prune ob­jects from further consideration. This situation arises whenever b overlaps 
the axes lines that pass through the center of c, and thus w can be as large as the width of the entire 
underlying space. There are several ways of overcoming this drawback. One easy way is to introduce redundancy 
(i.e., representing the object several times thereby replicating the number of references to it) by decomposing 
the quadtree cell c into smaller quadtree cells, each of which minimally encloses some portion of o (or, 
alternatively, some portion of o s minimum enclosing bounding box b)and contains a reference to o. The 
expanded MX-CIF quadtree [2] is a simple example of such an approach where c is decomposed once into 
four subblocks ci , which are then decomposed further until obtaining the minimum enclosing quadtree 
cell si for the portion of o, if any, that is coveredby ci . A more general approach. used in spatial 
join algorithms [29], sets a bound on the number of replications, (termed a size bound [37] and used 
in the GESS method [18]) or on the size of the covering quadtree cells resulting from the decomposition 
of c that contain the replicated references (termed an error bound [37]). Replicating the number of references 
to the objects is reminiscent of the manner in which the non­disjointness of the decomposition of the 
underlying space resulting from the use of an object hierarchy was overcome, and thus has the same shortcoming 
of possibly requiring the application of a duplicate object re­moval step prior to reporting the answer 
to some queries. The cover .eldtree [19, 20], and the equivalent loose quadtree (loose octree in three 
dimensions) [57], adopt a different approach at overcoming the independence of the sizes of c and b drawback. 
In particular, they do not replicate the objects. Instead, they expand the size of the space that is 
spanned by each quadtree cell c of width w by a cell expansion factor p (p> 0) so that the expanded cell 
is of width (1+ p) · w. In this case, an object is associated with its minimum enclosing expanded quadtree 
cell. It has been shown that given a quadtree cell c of width w and cellexpansionfactor p, the radius 
r of the minimum bounding box b of the smallest object o that could possibly be associated with c must 
be greater than pw/4 [57]. However, the utility of the loose quadtree is best evaluated in terms of the 
inverse of this relation (i.e., the maximum possible width w of c given an object o with minimum bounding 
box b of radius r)as reducing w is the primary motivation for the development of the loose quadtree as 
an alternative to the MX-CIF quadtree. It has been shown [48]that the maximum possible width w of c given 
an object o with minimum bounding box b of radius r is just a function of r and p and is independent 
of the position of o. More precisely, taking the ratio of cell to bounding box width w/(2r), we have 
[48]: 1/(1+ p) = w/(2r) = 1/ p. In particular, the range of possible ratios of width w/(2r) as a function 
of p for p= 1 takes on at most two values, and usually just one value [48]. The ideal value for p is 
1 [57]. The rationale is that using cell expansion factors much smaller than 1 increases the likelihood 
that the minimum enclosing expanded quadtree cell is large (as is the case for the MX-CIF quadtree, where 
p= 0), and that letting p be much larger than1 resultsin the areas spannedby the expanded quadtree cells 
being too large, thereby having much overlap. For example, letting p= 1, Figure9 is the loose quadtree 
corresponding to the collection of objects in Figure 8(a) and its MX-CIF quadtree in Figure 8(b). In 
this example, there are only two differences between the loose and MX-CIF quadtrees: 1. Rectangle object 
E is associated with the SW child of the root of the loose quadtree instead of with the root of the MX-CIF 
quadtree. 2. Rectangle object B is associated with the NW child of the NE child of the root of the loose 
quadtree instead of with the NE child of the root of the MX-CIF quadtree.  Note that the loose quadtree 
(cover .eldtree) is not the only approach at overcoming the drawback of the MX-CIF quadtree. In particular, 
the partition .eldtree [19, 20] is an alternative method of overcoming the drawback of the MX-CIF quadtree. 
The partition .eldtree proceeds by shifting the positions of the centroids of cells at successive levels 
of subdivision by one-half the width of the cell that is being subdivided. Figure 10 shows an example 
of such a subdivision. This subdivision rule guarantees that the width w of the minimum enclosing quadtree 
cell for the minimum bounding box bfor object o is boundedby eight times the maximum extent r of b[20, 
47]. The same ratio is obtained for the cover .eldtree when p= 1/4, and thus the partition .eldtree is 
superior to the cover .eldtree when p< 1/4[47]. (a) (b) Figure 9: (a) Cell decomposition inducedby the 
loose quadtree for a collection of rectangle objects identical to those in Figure 8(a), and (b) its tree 
representation.  3 Examples of the Utility of Sorting As an example of the utility of sorting spatial 
data suppose that we want to determine the nearest object to a given point (i.e., a pick operation in 
computer graphics). In order to see how the search is facilitated by sorting the underlying data, consider 
the set of point objects A F in Figure 4 which are stored in a PR quadtree [36, 45], and let us .nd the 
nearest neighbor of P. The search must .rst determine the leaf that contains the location/object whose 
nearest neighboring object is sought (i.e., P). Assuming a tree-based index, this is achieved by a top-down 
recursive algorithm. Initially, at each level of the recursion, we explore the subtree that contains 
P. Once the leaf node containing P has been found (i.e., 1), the distance from P to the nearest object 
in the leaf node is calculated (empty leaf nodes have a value of in.nity). Next, we unwind the recursion 
so that at each level, we search the subtrees that represent regions overlapping a circle centered at 
Pwhose radius is the distance to the closest object that has been found sofar. When more than one subtree 
must be searched, the subtrees representing regions nearer to P are searched before the subtrees that 
are farther away (since it is possible that an object in them might make it unnecessary to search the 
subtrees that arefarther away). In our example, the order in which the nodes are visited is given by 
their labels. We visit the brothers of the node 1 containing the query point P (and all remaining nodes 
at each level) in the order of the minimum distance from P to their borders (i.e., SE, NW, and NE for 
node 1). Therefore, as we unwind for the .rst time, we visit the eastern brother of node 1and its subtrees 
(nodes 2 and 3followed by nodes 4and 5), node 6, and node 7. Note that once we have visited node 2, there 
is no need to visit node 4 since node 2 contains A. However, we must still visit node 3 containing point 
B (closer than A), but now there is no need to visit node 5. Similarly, there is no need to visit nodes 
6and 7as they are toofaraway from Pgiven our knowledge of A. Unwinding one more level reveals that due 
to the distance between P and A, we must visit node 8 as it could contain a point that is closer to P 
than A;however, there is no need to visit nodes9, 10, 11, 12, and 13. The algorithm that we described 
can also be adapted to .nd the k nearest neighbors in which case the pruning of objects that cannot serve 
as the k nearest neighbors is achieved by making use of the distance to the kth nearest object that has 
been found so far. Having retrieved the k closest objects, should we be interested in retrieving an additional 
object (i.e., the k + 1th nearest object), then we have to reinvoke the algorithm again to .nd the k 
+ 1 nearest objects. An alternative approach is incremental and makes use of a priority queue [24, 25, 
26] so that there is no need to look again for the neighboring objects that have been reported sofar. 
There are many other applications where the sorting of objects is useful, and below we review a few that 
arise in computer graphics. For example, sorting forms the basis of all operations on z buffers, visibility 
calculations (e.g., BSP trees [21]), as well as back-to-front and front-to-back display algorithms. It 
also forms the basis of Warnock s hidden-line [59] and hidden-surface [60] algorithms that repeatedly 
subdivide the picture area into successively smaller blocks while simultaneously searching it for areas 
that are suf.ciently simple to be displayed. It is also used to accelerate ray tracing by .nding ray-object 
intersections (e.g., [7]). 4 Concluding Remarks An overview has been given of the rationale for sorting 
spatial objects in order to be able to index them thereby facilitating a number of operations involving 
search in the multidimensional domain. A distinction has been made between spatial objects that couldbe 
representedby traditional methods thathave been applied to point data and those that have extent thereby 
rendering the traditional methods inapplicable. Sorting is also used as the basis of an index in an environment 
where the data is drawn from a metric space rather than a vector space. In this case, the only information 
that we have is a distance function d (often a matrix) that indicates the degree of similarity (or dissimilarity) 
between all pairs of objects, given a set of N objects. Usually, it is required that d obeythe triangle 
inequality, be nonnegative, and be symmetric, in which case it is known as a metric and also referred 
to as a distance metric. Indexes in such an environment are based on either picking one distinguished 
object p and a value r, and then recursively subdividing the remaining objects into two classes depending 
on a comparison of their distance from p with r, or by choosing two distinguished objects p1 and p2 and 
recursively subdividing the remaining objects into two classes depending on which of p1 or p2 is closer 
(e.g., [47, 56]). The difference between these methods and those for data that lies in a vector space 
is that the subdivision lines in the embedding space from which the objects are drawn are explicit for 
the vector space while they are implicit for the metric space (see [47]for more details). The functioning 
of these various spatial sorting methods can be experienced by trying VASCO [12, 13, 14],a system forVisualizing 
and Animating Spatial Constructs and Operations.VASCO consists ofa set of spatial indexJAVATM (e.g., 
[6]) applets that enable users on the worldwide web to experiment with a number of hierarchical representations 
(e.g., [44, 45, 47]) for different spatial data types, and see animations of how they supporta numberof 
search queries (e.g., nearest neighbor and range queries). TheVASCO system can be found at http://cs.umd.edu/~hjs/quadtree/. 
Acknowledgments I am deeply grateful to Jagan Sankaranarayanan for help with the .gures and preparation 
for publication. References [1] D.J. AbelandJ.L. Smith.A data structureand algorithm basedona linearkeyfora 
rectangle retrieval problem. Computer Vision, Graphics, and Image Processing, 24(1):1 13, October 1983. 
[2] D. J. Abel and J. L. Smith. A data structure and query algorithm for a database of areal entities. 
Australian Computer Journal, 16(4):147 154, November 1984. [3] A. AboulnagaandJ.F. Naughton. Accurate 
estimationofthe costof spatial selections.In Proceedings of the 16th IEEE International Conference on 
Data Engineering,pages 123 134, San Diego, CA, February 2000. [4] W. G. Aref and H. Samet. Uniquely reporting 
spatial objects: yet another operation for comparing spatial data structures. In Proceedings of the 5th 
International Symposium on Spatial Data Handling, pages 178 189, Charleston, SC, August 1992. [5] W. 
G. Aref and H. Samet. Hashing by proximity to process duplicates in spatial databases. In Proceed­ings 
of the 3rd International Conference on Information and Knowledge Management (CIKM), pages 347 354, Gaithersburg, 
MD, December 1994. [6] K. Arnold and J. Gosling. The JAVATM Programming Language. Addison-Wesley, Reading, 
MA, 1996. [7] J. Arvo and D. Kirk. A surveyof ray tracing acceleration techniques. In An Introduction 
to Ray Tracing, A.S. Glassner, ed., chapter6, pages 201 262. Academic Press,NewYork, 1989. [8] D. Ayala, 
P. Brunet, R. Juan, and I. Navazo. Object representation by means of nonminimal division quadtrees and 
octrees. ACM Transactions on Graphics, 4(1):41 59, January 1985. [9] G. Barequet, B. Chazelle, L. J. 
Guibas, J. S. B. Mitchell, and A. Tal. BOXTREE: a hierarchical rep­resentation for surfaces in 3D. In 
Proceedings of the EUROGRAPHICS 96 Conference, J. Rossignac andF. X. Sillion, eds., pages 387 396, 484, 
Poitiers, France, August 1996. Also in Computer Graphics Forum, 15(3):387 396, 484, August 1996. [10] 
N. Beckmann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R* -tree: an ef.cient and robust access 
method for points and rectangles. In Proceedings of the ACM SIGMOD Conference, pages 322 331, Atlantic 
City, NJ, June 1990. [11] J. L. Bentley and J. H. Friedman. Data structures for range searching. ACM 
Computing Surveys, 11(4):397 409, December 1979. [12] F. Brabec and H. Samet. The VASCO R-tree JAVATM 
applet. In Visual Database Systems (VDB4). Proceedings of the IFIP TC2//WG2.6 Fourth Working Conference 
on Visual Database Systems, pages 147 153, Chapman and Hall,L Aquila, Italy, May 1998. [13] F. BrabecandH. 
Samet.Visualizingand animating R-treesand spatial operationsin spatial databaseson the worldwide web. 
In Visual Database Systems (VDB4). Proceedings of the IFIP TC2//WG2.6 Fourth Working Conference on Visual 
Database Systems, pages 123 140, Chapman and Hall, L Aquila, Italy, May 1998. [14] F. Brabec and H. Samet. 
Visualizing and animating search operations on quadtrees on the worldwide web. In Proceedings of the 
16th European Workshop on Computational Geometry, pages 70 76, Eilat, Israel, March 2000. [15] D. Comer. 
The ubiquitous B-tree. ACM Computing Surveys, 11(2):121 137, June 1979. [16] L. De Floriani, M. Facinoli, 
P. Magillo, and D. Dimitri. A hierarchical spatial index for triangulated surfaces. In Proceedings of 
the Third International Conference on Computer Graphics Theory and Applications (GRAPP 2008), J. Braz, 
N. Jardim Nunes, and J. Madeiras Pereira, eds., pages 86 91, Funchal, Madeira, Portugal, January 2008. 
[17] J.-P. Dittrich and B. Seeger. Data redundancy and duplicate detection in spatial join processing. 
In Proceedings of the 16th IEEE International Conference on Data Engineering, pages 535 546, San Diego, 
CA, February 2000. [18] J.-P. Dittrich and B. Seeger. GESS: a scalable similarity-join algorithm for 
mining large data sets in high dimensional spaces. In Proceedings of the 7thACM SIGKDD Conference on 
Knowledge Discovery and Data Mining, pages 47 56, San Francisco, August 2001. [19] A. Frank. Problems 
of realizing LIS: storage methods for space related data: the .eldtree. Technical Report 71, Institute 
for Geodesy and Photogrammetry, ETH, Zurich, Switzerland, June 1983. [20] A. U. Frank and R. Barrera. 
The Fieldtree: a data structure for geographic information systems. In Design and Implementation of Large 
Spatial Databases 1st Symposium, SSD 89, A. Buchmann, O.G¨unther,T.R. Smith,andY.-F.Wang, eds.,vol.409of 
Springer-Verlag Lecture Notesin Computer Science, pages 29 44, Santa Barbara, CA, July 1989. [21] H. 
Fuchs, Z. M. Kedem, and B. F. Naylor. On visible surface generation by a priori tree structures. Computer 
Graphics, 14(3):124 133, July 1980. Also in Proceedings of the SIGGRAPH 80 Conference, Seattle,WA, July 
1980. [22] S. Gottschalk, M. C. Lin, and D. Manocha. OBBTree: a hierarchical structure for rapid interference 
detection. In Proceedings of the SIGGRAPH 96 Conference, pages 171 180, New Orleans, LA, August 1996. 
[23] A. Guttman. R-trees: a dynamic index structure for spatial searching. In Proceedings of the ACM 
SIGMOD Conference, pages 47 57, Boston, June 1984. [24] A. Henrich. A distance-scan algorithm for spatial 
access structures. In Proceedings of the 2nd ACM Workshop on Geographic Information Systems, N. Pissinou 
and K. Makki, eds., pages 136 143, Gaithersburg, MD, December 1994. [25] G. R. Hjaltason and H. Samet. 
Ranking in spatial databases. In Advances in Spatial Databases 4th International Symposium, SSD 95, M. 
J. Egenhofer and J. R. Herring, eds., vol. 951 of Springer-Verlag Lecture Notes in Computer Science, 
pages 83 95, Portland, ME, August 1995. [26] G. R. Hjaltason and H. Samet. Distance browsing in spatial 
databases. ACMTransactions on Database Systems, 24(2):265 318, June 1999. Also University of Maryland 
Computer ScienceTechnical Report TR 3919, July 1998. [27] G. M. Hunter. Ef.cient computation and data 
structures for graphics. PhD thesis, Department of Electrical Engineering and Computer Science, Princeton 
University, Princeton, NJ, 1978. [28] A. Hut.esz, H.-W. Six, andP.Widmayer. The R-.le: an ef.cient access 
structure for proximity queries. In Proceedings of the 6th IEEE International Conference on Data Engineering, 
pages 372 379, Los Angeles, February 1990. [29] E. Jacox and H. Samet. Spatial join techniques. ACM Transactions 
on Database Systems, 32(1):7, March 2007. Also anexpandedversionin Universityof Maryland Computer ScienceTechnical 
Report TR 4730, June 2005. [30] N. Katayama and S. Satoh. The SR-tree: an index structure for high-dimensional 
nearest neighbor queries. In Proceedings of the ACM SIGMOD Conference, J. Peckham, ed., pages 369 380, 
Tucson, AZ, May 1997. [31] G. Kedem. The quad-CIF tree: a data structure for hierarchical on-line algorithms. 
In Proceedings of the 19th Design Automation Conference, pages 352 357, Las Vegas, NV, June 1982. Also 
University of Rochester Computer ScienceTechnical Report TR 91, September 1981. [32] A. Klinger. Patterns 
and search statistics. In Optimizing Methods in Statistics, J. S. Rustagi, ed., pages 303 337. Academic 
Press, NewYork, 1971. [33] D. E. Knuth. The Art of Computer Programming: Sorting and Searching, vol. 
3. Addison-Wesley, Reading, MA, second edition, 1998. [34] D. Meagher. Geometric modeling using octree 
encoding. Computer Graphics and Image Processing, 19(2):129 147, June 1982. [35] S. M. Omohundro. Five 
balltree construction algorithms. Technical Report TR 89 063, International Computer Science Institute, 
Berkeley, CA, December 1989. [36] J. A. Orenstein. Multidimensional tries used for associative searching. 
Information Processing Letters, 14(4):150 157, June 1982. [37] J. A. Orenstein. Redundancy in spatial 
databases. In Proceedings of the ACM SIGMOD Conference, pages 294 305, Portland, OR, June 1989. [38] 
J. Ponce and O.Faugeras. An object centered hierarchical representation for 3d objects: the prism tree. 
Computer Vision, Graphics, and Image Processing, 38(1):1 28, April 1987. [39] F. P. Preparata and M. 
I. Shamos. Computational Geometry: An Introduction. Springer-Verlag, New York, 1985. [40] D. R. Reddy 
and S. Rubin. Representation of three-dimensional objects. Computer Science Technical Report CMU CS 78 
113, Carnegie-Mellon University, Pittsburgh,PA, April 1978. [41] A. A. G. Requicha and H. B.Voelcker. 
Solid modeling: a historical summary and contemporary assess­ment. IEEE Computer Graphics and Applications, 
2(2):9 24, March 1982. [42] J. T. Robinson. The K-D-B-tree: a search structure for large multidimensional 
dynamic indexes. In Proceedings of the ACM SIGMOD Conference, pages 10 18, Ann Arbor, MI, April 1981. 
[43] J. B. Rothnie Jr. and T. Lozano. Attribute based .le organization in a paged memory environment. 
Communications of the ACM, 17(2):63 69, February 1974. [44] H. Samet. Applications of Spatial Data Structures: 
Computer Graphics, Image Processing, and GIS. Addison-Wesley, Reading, MA, 1990. [45] H. Samet. The Design 
and Analysis of Spatial Data Structures. Addison-Wesley, Reading, MA, 1990. [46] H. Samet. Decoupling 
partitioning and grouping: overcoming shortcomings of spatial indexing with bucketing. ACM Transactions 
on Database Systems, 29(4):789 830, December 2004. [47] H. Samet. Foundations of Multidimensional and 
Metric Data Structures. Morgan-Kaufmann, San Francisco, 2006. [48] H. Samet and J. Sankaranarayanan. 
Maximum containing cell sizes in cover .eldtrees and loose quadtrees and octrees. Computer Science Technical 
Report TR 4900, University of Maryland, Col­legePark, MD, October 2007. [49] H. Samet and R. E. Webber. 
Storing a collection of polygons using quadtrees. ACM Transactions on Graphics, 4(3):182 222, July 1985. 
[50] B. Seeger and H.-P. Kriegel. Techniques for design and implementation of ef.cient spatial access 
meth­ods. In Proceedings of the 14th International Conference on Very Large Databases (VLDB),F. Bachillon 
and D. J. DeWitt, eds., pages 360 371, Los Angeles, August 1988. [51] T. Sellis, N. Roussooulos, and 
C. Faloutsos. The R+-tree: a dynamic index for multi-dimensional objects. In Proceedings of the 13th 
International Conference on Very Large Databases (VLDB), pages 71 79, Brighton, United Kingdom, September 
1987. [52] K.SevcikandN.Koudas. Filter treesformanagingspatialdataoverarangeofsize granularities.In Pro­ceedings 
of the 22nd International Conference on Very Large Data Bases (VLDB),T. M.Vijayaraman, A. P. Buchmann, 
C. Mohan, and N. L. Sarda, eds., pages 16 27, Mumbai (Bombay), India, September 1996. [53] H.-W. Six 
and P. Widmayer. Spatial searching in geometric databases. In Proceedings of the 4th IEEE International 
Conference on Data Engineering, pages 496 503, Los Angeles, February 1988. [54] J.-W. Song, K.-Y. Whang, 
Y.-K. Lee, M.-J. Lee, and S.-W. Kim. Spatial join processing using corner transformation. IEEE Transactions 
on Knowledge and Data Engineering, 11(4):688 695, July/August 1999. [55] S. L. Tanimoto and T. Pavlidis. 
A hierarchical data structure for picture processing. Computer Graphics and Image Processing, 4(2):104 
119, June 1975. [56] J. K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. 
Information Process­ing Letters, 40(4):175 179, November 1991. [57] T. Ulrich. Loose octrees. In Game 
Programming Gems, M. A. DeLoura, ed., pages 444 453. Charles River Media, Rockland, MA, 2000. [58] W. 
Wang, J. Yang, and R. Muntz. PK-tree: a spatial index structure for high dimensional point data. In Proceedings 
of the 5th International Conference on Foundations of Data Organization and Algorithms (FODO), pages 
27 36, Kobe, Japan, November 1998. [59] J. E. Warnock. A hidden line algorithm for halftone picture representation. 
Computer Science Technical Report TR 4 5, University of Utah, Salt Lake City, UT, May 1968. [60] J. E. 
Warnock. A hidden surface algorithm for computer generated half tone pictures. Computer Science Technical 
Report TR 4 15, University of Utah, Salt Lake City, UT, June 1969. [61] D. A. White and R. Jain. Similarity 
indexing with the SS-tree. In Proceedings of the 12th IEEE International Conference on Data Engineering, 
S. Y. W. Su, ed., pages 516 523, New Orleans, LA, February 1996. [62] G. Wyvill and T. L. Kunii. A functional 
model for constructive solid geometry. Visual Computer, 1(1):3 14, July 1985.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation</funding_agency>
			<grant_numbers>
				<grant_number>EIA-00-91474CCF-05-15241IIS-07-13501</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
	<section>
		<section_id>1401251</section_id>
		<sort_key>1190</sort_key>
		<section_seq_no>28</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[ABCDE: The art of proposal writing]]></section_title>
		<section_page_from>28</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098899</person_id>
				<author_profile_id><![CDATA[81365593679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Helfer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098900</person_id>
				<author_profile_id><![CDATA[81365596247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cunningham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098901</person_id>
				<author_profile_id><![CDATA[81540838756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGrath]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098902</person_id>
				<author_profile_id><![CDATA[81100492996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosenblum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401252</article_id>
		<sort_key>1200</sort_key>
		<display_label>Article No.</display_label>
		<pages>39</pages>
		<display_no>92</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The art of proposal writing]]></title>
		<page_from>1</page_from>
		<page_to>39</page_to>
		<doi_number>10.1145/1401132.1401252</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401252</url>
		<abstract>
			<par><![CDATA[<p>This class covers general proposal writing for academic projects in two broad categories: research and education. It reviews the project concept, the search for an appropriate funding program, and development of a proposal based on a program announcement. Attendees develop a solid understanding of the structure of a competitive proposal, learn the different ways a proposal may be reviewed, and discuss the essential factors that determine whether or not a project gets funded.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944.10011122</concept_id>
				<concept_desc>CCS->General and reference->Document types</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011122</concept_id>
				<concept_desc>CCS->General and reference->Document types</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1098903</person_id>
				<author_profile_id><![CDATA[81365593679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Barb]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Helfer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Leapfrog]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098904</person_id>
				<author_profile_id><![CDATA[81365596247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cunningham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown Cunningham Associates]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098905</person_id>
				<author_profile_id><![CDATA[81540838756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGrath]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Colorado School of Mines]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098906</person_id>
				<author_profile_id><![CDATA[81100492996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Larry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosenblum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Science Foundation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Art of Proposal Writing SIGGRAPH 2008 August 15, 2008, 8:30 10:15 am Organizer: Barb Helfer, Leapfrog 
Speakers: Steve Cunningham, Brown Cunningham Associates Mike McGrath, Colorado School of Mines Larry 
Rosenblum, National Science Foundation Syllabus Introduction: Helfer General concepts: McGrath Research: 
Rosenblum Education: Cunningham This class covers general proposal writing for academic projects in two 
broad categories: research and education. It reviews the project concept, the search for an appropriate 
funding program, and development of a proposal based on a program announcement. Attendees develop a solid 
understanding of the structure of a competitive proposal, learn the different ways a proposal may be 
reviewed, and discuss the essential factors that determine whether or not a project gets funded. The 
presenters all have a significant history with the US National Science foundation. Cunningham and McGrath 
are former program officers (DUE and CISE, respectively), and Rosenblum is currently a program officer 
with CISE whose portfolio includes computer graphics research funding. The course will thus unavoidably 
have a slant towards NSF programs, but will include general concepts and other funding agencies as well. 
These notes were frozen well before the conference and some of the presenters slides were not ready at 
that time. It is likely that there will be additions and expansions before the actual conference presentation. 
An updated copy of these notes will be available after the SIGGRAPH 08 conference at http://www.cs.csustan.edu/~rsc/S08-proposal-course.pdf 
 Biographies of Speakers Steve Cunningham: Steve Cunningham is a former professor of computer science 
at CSU Stanislaus, Noyce Visiting Professor at Grinnnell College, and National Science Foundation Program 
Officer. Steve served ACM SIGGRAPH as Chair of the Education Committee, Director for Publications, and 
President, and served Eurographics as chair of the Education Board. Steve was Gemperle Distinguished 
Professor at Stanislaus, is a Eurographics Fellow, and received the SIGGRAPH Distinguished Service Award. 
Steve is the author of the textbook Computer Graphics: Programming in OpenGL for Visual Communication 
and co­author with Mike Bailey of the forthcoming text Computer Graphics Shaders: Theory and Practice. 
Michael McGrath: Mike McGrath was a professor of engineering at the Colorado School of Mines from 1985 
to 2007. He was a Program Director, National Science Foundation from 1992-1994 for the Graphics Center 
and National Supercomputing Centers programs. He has been involved in teaching and consulting in computer 
graphics since 1975. He has over 50 publications and presentations on education or research in computer 
graphics, CAD, scientific visualization and design. He is a member of the NSF Panel on Visualization 
in Scientific Computing. Mike served on the ACM SIGGRAPH Executive Committee as ACM SIGGRAPH Director 
for Education from 1998-2003. Lawrence J. Rosenblum: Lawrence J. Rosenblum is Program Director for Graphics 
and Visualization in the Computer and Information Science and Engineering Directorate at the National 
Science Foundation. He has previously served as Program Officer for Graphics and Visualization at the 
Office of Naval Research. His research group at the Naval Research Laboratory produced advances in mobile 
augmented reality (AR), scientific and uncertainty visualization, VR displays, applications of VR/AR 
systems, and understanding human performance in graphics systems. Rosenblum has published over 80 scientific 
articles and served on the editorial boards of several journals. He is a Director and former Chairman 
of the IEEE Visualization and Graphics Technical Committee. Granting Opportunities From 2006 to 2007 
the overall amount of grant dollars rose over ten percent from $39B to $42.9B, with six of the top fields 
posting double digit gains despite a tenuous economic outlook. Grant funding for 2008 is expected to 
have an upward trend with an estimated nine percent growth. Health related issues are the top priority 
of private and community foundations. Contributing to the growth in the health care arena are grants 
given by the Bill and Melinda Gates Foundation. With the help of the Gates Foundation, Health care issues 
surpassed Education for the first time in terms of number of grants dollars allocated. Independent Foundations 
spent the most by awarding $30.9B in grants in 2007, which was a 12.7 percent increase in giving from 
2006. Corporation giving was at $4.4B, and Community giving at $4.1B. Facts taken from the Foundation 
Center, Foundation Growth and Giving Estimate, 2008 Edition Grant Facts: During 2006 140, 484 grants 
were awarded  Of those grants awarded, they were awarded to 56,015 unique organizations  Largest share 
of funding went to educational institutions  The largest share of Independent foundations monies were 
spent for Health, International Affairs, Sciences, and Social Sciences.  Corporate foundations provide 
the biggest share of their giving to Education and Public Affairs/Society Benefit.  International giving 
increased 48.4 percent  Foundations awarded a record 386 grants of $5M or more  Facts taken from the 
Foundation Center, Foundation Giving Trends, 2008 Edition Subject Dollar Amount % of Grants # of Grants 
% Arts and Culture $2,329,708 12.1 20,095 14.3 Education $4,306,090 22.5 28,521 20.3 Environment and 
Animals $1,145,100 6.0 8,633 6.1 Health $4,394,462 23.0 18,260 13.0 Human Services $2,645,895 13.8 36,047 
25.7 International Affairs, Development, and Peace $1,019,739 5.3 3,763 2.7 Public Affairs* $2,042,490 
10.7 16,807 12.0 Science And Technology $550,591 2.9 2,422 1.7 Social Sciences $259,092 1.4 1,328 0.9 
Religion $412,955 2.2 4,486 3.2 Other $16,912 0.1 122 0.1 Note: Based on a sample of 1,263 larger foundations. 
Dollar figures in thousands *Includes Civil Rights and Social Action, Community Improvement and Development, 
Philanthropy and Voluntarism, and Public Affairs. Facts taken from the Foundation Center, Foundation 
Giving Trends Preview, 2007 Edition         L a r r y R o s e n b l u m  G e n e r a l p o 
i n t s a b o u t N S F a n d p r o p o s a l s  N S F i n a N u t s h e l l  I n d e p e n d e n t 
f e d e r a l a g e n c y u n d e r t h e E x e c u t i v e B r a n c h  S u p p o r t s b a s i c 
r e s e a r c h &#38; e d u c a t i o n  U s e s g r a n t m e c h a n i s m  D i s c i p l i n e 
- b a s e d s t r u c t u r e  C r o s s - d i s c i p l i n a r y m e c h a n i s m s  ~ 5 0 % P 
r o g r a m M a n a g e r s a r e R o t a t o r s / I P A s  O v e r s i g h t b y t h e N a t i o 
n a l S c i e n c e B o a r d   S e v e n D e a d l y S i n s o f P r o p o s a l W r i t i n g 1 
. F a i l u r e t o f o c u s o n t h e p r o b l e m s a n d p a y o f f s 2 . N o p e r s u a s i 
v e s t r u c t u r e : p o o r l y o r g a n i z e d 3 . N o c l e a r d i f f e r e n t i a t i o 
n : c o m p e t i t i v e a n a l y s i s 4 . F a i l u r e t o o f f e r a c o m p e l l i n g v a 
l u e p r o p o s i t i o n : p o t e n t i a l i m p a c t  5 . K e y p o i n t s a r e b u r i e 
d : n o h i g h l i g h t s , n o i m p a c t 6 . D i f f i c u l t t o r e a d : f u l l o f j a r 
g o n , t o o l o n g , t o o t e c h n i c a l 7 . C r e d i b i l i t y k i l l e r s : m i s s p 
e l l i n g s , g r a m m a t i c a l e r r o r s , w r o n g t e c h n i c a l t e r m s , i n c o n 
s i s t e n t f o r m a t , e t c .   I n g r e d i e n t s f o r a G o o d P r o p o s a l Educate 
the reviewers and Program Director  What problem(s) does your work address?  Why is this problem important? 
  What will you do to contribute to a solution?  What unique ideas/approaches do you have? Put in 
context.  Why are you the best person to do this work?  How will you evaluate your results?  H 
o w w i l l w e k n o w i f y o u w e r e s u c c e s s f u l o r i f y o u f a i l e d ?  How will 
you assure that the work has an impact?  K e y O n - l i n e D o c u m e n t s  F Y 2 0 0 8 N S F 
B u d g e t R e q u e s t  http://www.nsf.gov/about/budget/fy2008  F Y 2 0 0 7 N S F B u d g e t 
  http://www.nsf.gov/about/budget/fy2007  G r a n t P r o p o s a l G u i d e ( N S F 0 4 - 2 3 ) 
  http://www.nsf.gov/publications/pub_summ.jsp? ods_key=GPG  S c i e n c e a n d E n g i n e e r i 
n g I n d i c a t o r s  http://www.nsf.gov/sbe/srs/seind04/start.htm  General Information   http://www.nsf.gov/ 
                    Some Funding Resources Barb Helfer, with minor additions from 
Steve Cunningham Overall Search Pattern Google search for Funding Agency for University Education A 
number of individual university and government links will be shown, including listings for many countries 
 General Resources ACLS http://www.acls.org Agency Solicitations, procurement and program announcements 
http://www.ofm.wa.gov/contracts/procurement/postings.asp Annenberg Foundation http://www.whannenberg.org/ 
Appalachian Resource Center http://www.arc.gov/index.do?nodeId=1232 Carnegie Foundation http://www.carnegie.org/ 
CFDA http://12.46.245.173/cfda/cfda.html Charles Culpeper Foundation http://www.culpeper.org/ Charles 
Dana Foundation http://www.dana.org/grants/ Commerce Business Daily http://www.cbd-net.com/index.php/doc/home 
Community of Science (funding opportunities) http://fundingopps.cos.com/ Council of Foundations (gateway 
to philanthrophy on Web) http://www.cof.org/council/content.cfm?ItemNumber=586&#38;navItemNumber=2477 
DARPA http://www.darpa.mil/ipto/solicit/solicit.asp Defense Threat Reduction Agency (DTRA) Solicitations 
http://www.dtra.mil/be/business_opp/procurement/acq_procopp.cfm Department of Defense Small Business 
Innovation Research http://www.dodsbir.net/ Federal Aviation Administration http://www.faa.gov/education_research/ 
Federal Business Opportunities https://www.fbo.gov/index?s=main&#38;mode=list&#38;tab=list Federal Information 
Exchange http://www.info.gov/ Federal Register http://www.gpoaccess.gov/fr/index.html Federal Web Locator 
http://lawdbase.law.villanova.edu/fedweb/ FedWorld Network http://www.fedworld.gov/ Foundation Center 
http://foundationcenter.org/ Fundsnet Services http://www.fundsnetservices.com/main.htm Grants.gov http://www.grants.gov/search/searchHome.do 
Grants.gov Procurement Technical Assistance Program (PTAP) http://www07.grants.gov/search/search.do?&#38;mode=VIEW&#38;flag2006=false&#38;oppId=41186 
GrantsNet http://sciencecareers.sciencemag.org/funding Global Fund for Women http://www.globalfundforwomen.org/cms/ 
Higher Education Research Hub http://www.higher-ed.org/funding.htm National Aeronautical and Space Administration 
(NASA) http://education.nasa.gov/edprograms/fellowgrants/index.html National Endowment for the Arts (NEA) 
http://www.nea.gov/ National Endowment for the Humanities (NEH) http://www.neh.gov/grants/ National Institutes 
of Health (NIH) http://www.nih.gov/science/ National Institutes of Health Office of Extramural Research 
(OER) http://grants.nih.gov/grants/oer.htm National Science Foundation (Computer Science and Engineering) 
http://www.nsf.gov/dir/index.jsp?org=CISE National Science Foundation (Undergraduate Education) http://www.nsf.gov/div/index.jsp?div=DUE 
National Science Foundation (Education in Science and Engineering) http://www.nsf.gov/funding/research_edu_community.jsp 
USA.gov Index of U.S. Government Departments and Agencies http://www.usa.gov/Agencies/Federal/All_Agencies/index.shtml 
USA.gov Nonprofit Gateway http://www.usa.gov/Business/Nonprofit.shtml Small Business Administration http://sba.gov/ 
Sponsored Programs Information Network (SPIN) http://www.infoed.org/new_spin/spin.asp Society of Research 
Administrator's GrantsWeb http://web.fie.com/cws/sra/resource.htm#us The University of Iowa Division 
of Sponsored Programs http://research.uiowa.edu/dsp/main/?get=internat_funding_sources USAID - Education 
&#38; Universities http://www.usaid.gov/university/ US Department of Commerce http://www.commerce.gov/Grants/index.htm 
US Department of Defense: Defense Technical Information Center (DTIC) http://www.dtic.mil/dtic/ Office 
of Naval Research (ONR) http://www.onr.navy.mil/ US Army Research Institute (ARI) http://www.hqda.army.mil/ari/ 
US Department of Education http://www.ed.gov/about/offices/list/ocfo/grants/grants.html US Department 
of Energy http://www.science.doe.gov/grants/ Electronic Help Tools American Library Association (ALA) 
http://www.ala.org/ala/acrl/acrlpubs/crlnews/backissues1999/julyaugust4/grantresources.cfm Circle of 
Philanthropy http://philanthropy.com/ FEDIX Alert http://k12s.phast.umass.edu/stem/fedix.html FedBizOpps 
(FBO Daily) http://www.fbodaily.com/ Electronic Reports Foundation Center - Foundation Growth and Giving 
Estimates http://foundationcenter.org/gainknowledge/research/pdf/fgge08.pdf Foundation Center - Highlights 
of Foundation Giving Trends http://foundationcenter.org/gainknowledge/research/pdf/fgt08highlights.pdf 
Some comments (with minor changes) I have written on reviews Steve Cunningham The comments below come 
from my personal reviews on projects since I left NSF, covering intellectual merit and broader impacts, 
with a few changes to remove specific project or other references and to trim down longer comments. These 
are included to show some of the kinds of things that a reviewer may pull out of a proposal so that you 
can try to avoid them. Intellectual Merit The concepts discussed here are of potential interest, but 
do not yet seem well developed for computer science. These need to be better developed before they can 
be considered to be "well grounded in research and practice" and used as a basis for curricula. The project 
has a number of buzzwords strung together, but they do not seem to be well connected. It would surprise 
the reviewer if this [project work] hadn't been done someplace before, but there is no indication that 
this project has looked for that. Indeed, "literature search" is yet to be done. Although literature 
is cited in the proposal body, there is no bibliography with the proposal. The project plan is very open 
and is based on the assumption that XXX, but it would have been very helpful if any example of such an 
approach had been cited. The project faculty are active, but there is no apparent track record for their 
work with the kind of transformative ideas that would be needed by a project such as this. The reviewer 
does not share the optimistic outlook of the proposers. ... The question is whether there is enough push 
in the process to let it succeed. There are many steps involving many individuals and groups, but the 
narrative does not give any timelines. It's difficult to see how this much process could give any vibrant, 
strong products. How could they expect to do all this in two years? There are many good ideas in this 
project, and the reviewer is sure that it will serve the institution well. However, the reviewer questions 
whether this work is appropriate as a XXX project. Part of the difficulty the reviewer had with this 
project comes from the fact that it was difficult to see just what the eventual outcome of this project 
would be. There are some general statements about results, but little that was specific. The evaluation 
plan does not contain measurable goals or have methods to collect evidence about the project. Broader 
Impact The project feels weak in broader impact. It has very limited, passive dissemination and no active 
outreach. Its impacts are local and it's difficult to see how it would have an interest for anyone else. 
The project seems to be entirely local, with the project team coming from the departmental curriculum 
committee and the only outside input coming from the departmental advisory board. There is no history 
of working on transformational projects and the cited literature is quite thin. The reviewer was surprised 
to see no XXX publications or activities mentioned. In general, this project seems to be entirely focused 
on the local environment. The dissemination plan is completely passive, which is surprising given the 
presence of the external board, and there is no mention of plans to extend any results to other institutions. 
Dissemination looks vague, based on a range of unspecified journal publications. There are no workshops 
or other active processes planned to help other institutions develop similar programs. Student diversity 
seems to be expected because of being an urban campus and because the faculty are diverse, but no figures 
are given about the diversity of the students. It is not at all clear how this project would attract 
more women or underrepresented groups into computing degrees. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401253</section_id>
		<sort_key>1210</sort_key>
		<section_seq_no>29</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[SIGGRAPH Core: Tile-based methods for interactive applications]]></section_title>
		<section_page_from>29</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098907</person_id>
				<author_profile_id><![CDATA[81300379401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ares]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lagae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098908</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098909</person_id>
				<author_profile_id><![CDATA[81100244740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chi-Wing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098910</person_id>
				<author_profile_id><![CDATA[81100237726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ostromoukhov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098911</person_id>
				<author_profile_id><![CDATA[81100092246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deussen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401254</article_id>
		<sort_key>1220</sort_key>
		<display_label>Article No.</display_label>
		<pages>267</pages>
		<display_no>93</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Tile-based methods for interactive applications]]></title>
		<page_from>1</page_from>
		<page_to>267</page_to>
		<doi_number>10.1145/1401132.1401254</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401254</url>
		<abstract>
			<par><![CDATA[<p>Over the last years, several techniques have been demonstrated that rely on tile-based methods. A lot of interactive applications could potentially benefit from these techniques. However, the state-of-the-art is scattered over several publications, and survey works are not available. In this class we give a detailed overview of tile-based methods in computer graphics. The class consist of four parts, which are briefly covered in the following paragraphs.</p> <p>Tile-Based Methods using Wang and Corner Tiles The first part of the class introduces tile-based methods in computer graphics based on Wang tiles and corner tiles. This part serves as a general introduction for the class, but also covers methods and applications based on Wang tiles and corner tiles. We introduce Wang tiles and corner tiles, and present several tiling algorithms. We discuss in detail tile-based texture mapping using graphics hardware, tile-based generation of Poisson disk distributions, and object distribution for procedural texturing. We briefly cover other applications such as sampling, non-photorealistic rendering, and geometric object distribution. The lecturer for the first part is Ares Lagae, who recently finished his PhD about tile-based methods in computer graphics [Lagae, 2007].</p> <p>Periodic Tilings for Computer Graphics Applications The second part of the class introduces the mathematical and algorithmic aspects of decorative tilings such as those used by M. C. Escher. It focuses on the theory of isohedral tilings, tilings that cover the plane systematically with congruent copies of a single shape. The isohedral tilings are flexible enough to support a wide variety of applications in art and design, while admitting a compact and efficient implementation. We show how to store, manipulate and render isohedral tilings, and survey some recent applications. The lecturer for the second part is Craig Kaplan, an expert on the use of computer graphics in ornamental design Kaplan [2002].</p> <p>Tile-Based Methods for Surface Modeling The third part of the class covers tilebased methods for surface modeling. Tiling is a practical and cost-effective method for high-quality surface modeling and rendering. Rather than intensive data acquisition and synthesis, the generalized Wang tile set presented in this part of the talk allows us to seamlessly and non-periodically tile texture data on parameterized surfaces of arbitrary topology. Once we synthesize textures on tiles, we can reuse the same tile set on different surfaces and we can also instantaneously change the surface appearance by just switching the reference tile set. Further than color textures, we also extend surface tiling to include bump maps, geometry details, the BTF's, as well as Poisson disk tiling. The lecturer for the third part is Chi-Wing Fu, who wrote several papers on this topic [Fu and Leung, 2005].</p> <p>Non-Periodic Tilings for Computer Graphics Applications The fourth part of the class covers an important class of non-periodic tilings and their benefits for computer graphics applications. First, the theory of Penrose tilings is presented. We show how the inherent self-similarity of Penrose tiling can be exploited in order to get efficient implementation of uniform distributions with blue-noise properties. Then, we present polyomino-based uniform distributions, and show their advantages. Finally, we explore other non-periodic tiling systems, potentially usable for computer graphics applications: dodecagonal tiling, Ammann tiling, etc. The lecturer for the fourth part is Victor Ostromoukhov who is an expert in this topic [Ostromoukhov et al., 2004; Ostromoukhov, 2007].</p> <p>Tile-Based Methods for Non-Photorealistic Rendering and Landscape Modeling The fifth part of the class covers applications of tile-based methods in the fields of non-photorealistic rendering and landscape modeling [Cohen et al., 2003]. Using hierarchical tile sets one is able to create point sets with infinite density still showing Poisson disk characteristics [Kopf et al., 2006]. We will demonstrate this using a set of tiles that is recursively subdivided. This is possible because the set shows self similarity. The resulting points can be used to create stipple drawings and also distributions of plants that also show Poisson disk behavior. This will be demonstrated by an application that enables real-time modeling and rendering of complex landscapes. The lecturer for the fifth part is Oliver Deussen, who has considerable experience with tile-based design.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Geometrical problems and computations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Line and curve generation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098912</person_id>
				<author_profile_id><![CDATA[81300379401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ares]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lagae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Katholieke Universiteit Leuven]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098913</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098914</person_id>
				<author_profile_id><![CDATA[81100244740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chi-Wing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nanyang Technological University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098915</person_id>
				<author_profile_id><![CDATA[81100237726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ostromoukhov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Montreal]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098916</person_id>
				<author_profile_id><![CDATA[81100092246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deussen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#228;t Konstanz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882314</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Agarwal, S., Rammamoorthi, R., Belongie, S., and Jensen, H. W. Strcutured importance sampling of environment maps. <i>ACM Transactions on Graphics</i>, 22(3):605--612, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ball, W. W. R. <i>Mathematical recreations and essays</i>. MacMillan and Co., 1926.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Berger, R. The undecidability of the domino problem. <i>Memoirs American Mathematical Society</i>, 66:1--72, 1966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258882</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bonet, J. S. D. Multiresolution sampling procedure for analysis and synthesis of texture images. In <i>Proceedings of ACM SIGGRAPH 1997</i>, pages 361--368. 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>48159</ref_obj_id>
				<ref_obj_pid>48155</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Burchill, L. Graphics goodies #2 - a simple, versatile procedural texture. <i>Computer Graphics</i>, 22(1):29--30, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Catmull, E. E. <i>A Subdivision Algorithm for Computer Display of Curved Surfaces</i>. Ph.D. thesis, Department of Computer Science, University of Utah, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cipra, B. Packing challenge mastered at last. <i>Science</i>, 281, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cohen, J. and Debevec, P. LightGen, HDRShop plugin. http://gl.ict.usc.edu/HDRShop/lightgen/lightgen.html, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882265</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cohen, M. F., Shade, J., Hiller, S., and Deussen, O. Wang tiles for image and texture generation. <i>ACM Transactions on Graphics</i>, pages 287--294, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L. Stochastic sampling in computer graphics. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 86</i>), 5(1):51--72, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073264</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L. and DeRose, T. Wavelet noise. <i>ACM Transactions on Graphics</i>, 24(3):803--811, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>359869</ref_obj_id>
				<ref_obj_pid>359863</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Crow, F. C. The aliasing problem in computer-generated shaded images. <i>Communications of the ACM</i>, 20(11):799--805, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245814</ref_obj_id>
				<ref_obj_pid>245761</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Culik, II, K. An aperiodic set of 13 Wang tiles. <i>Discrete Mathematics</i>, 160(1--3):245--251, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Culik, II, K. and Kari, J. An aperiodic set of Wang cubes. <i>Journal of Universal Computer Science</i>, 1(10), 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280898</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Deussen, O., Hanrahan, P., Lintermann, B., M&#283;ch, R., Pharr, M., and Prusinkiewicz, P. Realistic modeling and rendering of plant ecosystems. In <i>Proceedings of ACM SIGGRAPH 1998</i>, pages 275--286. 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Deussen, O., Hiller, S., van Overveld, C., and Strothotte, T. Floating points: A method for computing stipple drawings. <i>Computer Graphics Forum</i>, 19(3):40--51, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Dipp&#233;, M. A. Z. and Wold, E. H. Antialiasing through stochastic sampling. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 85)</i>, 19(3):69--78, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141915</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Dunbar, D. and Humphreys, G. A spatial data structure for fast Poisson-disk sample generation. <i>ACM Transactions on Graphics</i>, 25(3):503--508, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>863241</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Dutr&#233;, P., Bala, K., and Bekaert, P. <i>Advanced Global Illumination</i>. A. K. Peters, Ltd., Natick, MA, USA, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>572337</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ebert, D. S., Musgrave, F. K., Peachey, D., Perlin, K., and Worley, S. <i>Texturing and Modeling: A Procedural Approach</i>. Morgan Kaufmann Publishers, Inc., 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Efros, A. A. and Freeman, W. T. Image quilting for texture synthesis and transfer. In <i>Proceedings of ACM SIGGRAPH 2001</i>, pages 341--346. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Efros, A. A. and Leung, T. K. Texture synthesis by non-parametric sampling. In <i>International Conference on Computer Vision</i>, pages 1033--1038. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Escher, M. C. and Locher, J. C. <i>The World of M. C. Escher</i>. Abrams, New York, NY, USA, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383668</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Fu, C.-W. and Leung, M.-K. Texture tiling on arbitrary topological surfaces using Wang tiles. In <i>Rendering Techniques 2005</i>, pages 99--104. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Gielis, J. A generic geometric transformation that unifies a wide range of natural and abstract shapes. <i>American Journal of Botany</i>, 90(3):333--338, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781647</ref_obj_id>
				<ref_obj_pid>781606</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Gielis, J., Beirinckx, B., and Bastiaens, E. Superquadrics with rational and irrational symmetry. In <i>Proceedings of the eighth ACM symposium on Solid modeling and applications</i>, pages 262--265. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>318952</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Glassner, A. <i>Andrew Glassner's notebook: recreational computer graphics</i>. Morgan Kaufmann Publishers, Inc., San Fransisco, CA, USA, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>558817</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Gooch, B. and Gooch, A. <i>Non-Photorealistic Rendering</i>. A. K. Peters, Ltd., Natick, MA, USA, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>19304</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Gr&#252;nbaum, B. and Shepard, G. C. <i>Tilings and patterns</i>. W. H. Freeman and Company, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383327</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Hausner, A. Simulating decorative mosaics. In <i>Proceedings of ACM SIGGRAPH 2001</i>, pages 573--580. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218446</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Heeger, D. J. and Bergen, J. R. Pyramid-based texture analysis/synthesis. In <i>Proceedings of ACM SIGGRAPH 1995</i>, pages 229--238. 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>718507</ref_obj_id>
				<ref_obj_pid>647260</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Hiller, S., Deussen, O., and Keller, A. Tiled blue noise samples. In <i>Vision, Modeling, and Visualization 2001</i>, pages 265--272. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Jones, T. R. Efficient generation of Poisson-disk sampling patterns. <i>Journal of Graphics Tools</i>, 11(2):27--36, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345022</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. and Salesin, D. H. Escherization. In <i>Proceedings of ACM SIGGRAPH 2000</i>, pages 499--510. 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Klassen, R. V. Filtered jitter. <i>Computer Graphics Forum</i>, 19(4):223--230, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Knuth, D. E. <i>The art of computer programming</i>, volume 1. Addison-Wesley, Reading, MA, USA, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882411</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Kollig, T. and Keller, A. Efficient illumination by high dynamic range images. In <i>Proceedings of the 14th Eurographics workshop on Rendering</i>, pages 45--50. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141916</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Kopf, J., Cohen-Or, D., Deussen, O., and Lischinski, D. Recursive Wang tiles for real-time blue noise. <i>ACM Transactions on Graphics</i>, 25(3):509--518, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073263</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Kwatra, V., Essa, I., Bobick, A., and Kwatra, N. Texture optimization for example-based synthesis. <i>ACM Transactions on Graphics</i>, 24(3):795--802, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882264</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Kwatra, V., Sch&#246;dl, A., Essa, I., Turk, G., and Bobick, A. Graphcut textures: image and video synthesis using graph cuts. <i>ACM Transactions on Graphics</i>, 22(3):277--286, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095888</ref_obj_id>
				<ref_obj_pid>1095878</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. A procedural object distribution function. <i>ACM Transactions on Graphics</i>, 24(4):1442--1461, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1183296</ref_obj_id>
				<ref_obj_pid>1183287</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. An alternative for Wang tiles: Colored edges versus colored corners. <i>ACM Transactions on Graphics</i>, 25(4):1442--1459, 2006a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. Long period hash functions for procedural texturing. In <i>Vision, Modeling, and Visualization 2006</i>, pages 225--228. Akademische Verlagsgesellschaft Aka GmbH, Berlin, 2006b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. Poisson sphere distributions. In <i>Vision, Modeling, and Visualization 2006</i>, pages 373--379. Akademische Verlagsgesellschaft Aka GmbH, Berlin, 2006c.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. The tile packing problem. Report CW 461, Department of Computer Science, K. U. Leuven, Leuven, Belgium, 2006d.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>62969</ref_obj_id>
				<ref_obj_pid>62959</ref_obj_pid>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[L'Ecuyer, P. Efficient and portable combined random number generators. <i>Communications of the ACM</i>, 31(6):742--749, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641518</ref_obj_id>
				<ref_obj_pid>641480</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Lefebvre, S. and Neyret, F. Pattern based procedural textures. In <i>Proceedings of the 2003 Symposium on Interactive 3D Graphics</i>, pages 203--212. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74360</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Lewis, J. P. Algorithms for solid noise synthesis. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 89</i>), 23(3):263--270, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>501787</ref_obj_id>
				<ref_obj_pid>501786</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Liang, L., Liu, C., Xu, Y.-Q., Guo, B., and Shum, H.-Y. Real-time texture synthesis by patch-based sampling. <i>ACM Transactions on Graphics</i>, 20(3):127--150, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015731</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Liu, Y., Lin, W.-C., and Hays, J. Near-regular texture analysis and manipulation. <i>ACM Transactions on Graphics</i>, 23(3):368--376, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Lloyd, S. P. Least squares quantization in PCM. <i>IEEE Transactions on Information Theory</i>, 28(2):129--137, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Lu, A. and Ebert, D. S. Example-based volume illustrations. In <i>Proceedings of IEEE Visualization</i>, pages 655--662. 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[MacMahon, M. P. A. <i>New mathematical pastimes</i>. Cambridge University Press, 1921.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>272995</ref_obj_id>
				<ref_obj_pid>272991</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Matsumoto, M. and Nishimura, T. Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator. <i>ACM Transactions on Modeling and Computer Simulation</i>, 8(1):3--30, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>155306</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[McCool, M. and Fiume, E. Hierarchical Poisson disk sampling distributions. In <i>Proceedings of Graphics Interface '92</i>, pages 94--105. 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37410</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Mitchell, D. P. Generating antialiased images at low sampling densities. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 87</i>), 21(4):65--72, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122736</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Mitchell, D. P. Spectrally optimal sampling for distribution ray tracing. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 91</i>), 25(4):157--164, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311561</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Neyret, F. and Cani, M.-P. Pattern-based texturing revisited. In <i>Proceedings of ACM SIGGRAPH 1999</i>, pages 235--242. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1252500</ref_obj_id>
				<ref_obj_pid>1251973</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Ng, T.-Y., Wen, C., Tan, T.-S., Zhang, X., and Kim, Y. J. Generating an &#969;-tile set for texture synthesis. In <i>Proceedings of Computer Graphics International 2005</i>, pages 177--184. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015750</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Ostromoukhov, V., Donohue, C., and Jodoin, P.-M. Fast hierarchical importance sampling with blue noise properties. <i>ACM Transactions on Graphics</i>, 23(3):488--495, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383292</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Parish, Y. I. H. and M&#252;ller, P. Procedural modeling of cities. In <i>Proceedings of ACM SIGGRAPH 2001</i>, pages 301--308. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Peachy, D. R. Solid texturing of complex surfaces. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 85</i>), 19(3):279--286, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Penrose, R. The r&#244;le of aesthetics in pure and applied mathematical research. <i>Bulletin of the Institute of Mathematics and its Applications</i>, 10:266--271, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. An image synthesizer. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 85</i>), 19(3):287--296, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566636</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. Improving noise. <i>ACM Transactions on Graphics</i>, pages 681--682, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74359</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. and Hoffert, E. M. Hypertexture. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 89</i>), 23(3):253--262, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975275</ref_obj_id>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Pharr, M. and Humphreys, G. <i>Physically Based Rendering</i>. Morgan Kaufmann Publishers, Inc., San Fransisco, CA, USA, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Saladin, H. <i>L'Alhambra de Grenade</i>. Morance, Paris, France, 1926.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581924</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Secord, A., Heidrich, W., and Streit, L. Fast primitive distribution for illustration. In <i>Proceedings of the 13th Eurographics workshop on Rendering</i>, pages 215--226. 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Shade, J., Cohen, M. F., and Mitchell, D. P. Tiling layered depth images. Technical report, University of Washington, Department of Computer Science and Engineering, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Stam, J. Aperiodic texture mapping. Technical Report ERCIM-01/97-R046, European Research Consortium for Informatics and Mathematics (ECRIM), 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Steinhaus, H. <i>Mathematical Snapshots</i>. Dover Publications, Inc., Mineaola, NY, USA, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>33404</ref_obj_id>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Tufte, E. R. <i>The Visual Display of Quantitative Information</i>. Graphics Press, Cheshire, CT, USA, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Turk, G. Generating textures on arbitrary surfaces using reaction-diffusion. <i>Computer Graphics (Proceedings of ACM SIGGRAPH 91</i>), 25(4):289--298, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>27674</ref_obj_id>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Ulichney, R. <i>Digital Halftoning</i>. The MIT Press, Cambridge, MA, USA, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Wang, H. Proving theorems by pattern recognition - II. <i>Bell Systems Technical Journal</i>, 40:1--42, 1961.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Wang, H. Games, logic and computers. <i>Scientific American</i>, 213(5):98--106, 1965.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1058138</ref_obj_id>
				<ref_obj_pid>1058129</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Wei, L.-Y. Tile-based texture mapping on graphics hardware. In <i>Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</i>, pages 55--63. 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345009</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Wei, L.-Y. and Levoy, M. Fast texture synthesis using tree-structured vector quantization. In <i>Proceedings of ACM SIGGRAPH 2000</i>, pages 479--488. 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Wichmann, B. A. and Hill, I. D. An efficient and portable pseudo-random number generator. <i>Applied Statistics</i>, 31:188--190, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237267</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Worley, S. A cellular texture basis function. In <i>Proceedings of ACM SIGGRAPH 1996</i>, pages 291--294. 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Yellot, Jr., J. I. Spectral analysis of spatial sampling by photoreceptors: Topological disorder prevents aliasing. <i>Vision Research</i>, 22:1205--1210, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Yellot, Jr., J. I. Spectral consequences of photoreceptor sampling in the rhesus retina. <i>Science</i>, 221:382--385, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>105491</ref_obj_id>
				<ref_obj_pid>105488</ref_obj_pid>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[E. M. Arkin, L. P. Chew, D. P. Huttenlocher, K. Kedem, and J. S. B. Mitchell. An efficiently computable metric for comparing polygonal shapes. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 13:209--216, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[William W. Chow. Automatic generation of interlocking shapes. <i>Computer Graphics and Image Processing</i>, 9:333--353, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Paul Church. Snakes in the plane. Master's thesis, School of Computer Science, University of Waterloo, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882265</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Michael F. Cohen, Jonathan Shade, Stefan Hiller, and Oliver Deussen. Wang tiles for image and texture generation. <i>ACM Trans. Graph</i>., 22(3):287--294, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[John H. Conway, Heidi Burgiel, and Chaim Goodman-Strauss. <i>The Symmetries of Things</i>. A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[H. S. M. Coxeter. Coloured symmetry. In H. S. M. Coxeter <i>et al.</i>, editor, <i>M.C. Escher: Art and Science</i>, pages 15--33. Elsevier Science Publishers B. V., 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Hallard T. Croft, Kenneth J. Falconer, and Richard K. Guy. <i>Unsolved Problems in Geometry</i>. Springer-Verlag, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245814</ref_obj_id>
				<ref_obj_pid>245761</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Karel Culik. An aperiodic set of 13 wang tiles. <i>Discrete Math.</i>, 160(1--3):245--251, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Olaf Delgado Friedrichs. Data structures and algorithms for tilings i. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Andreas W. M. Dress. The 37 combinatorial types of regular "Heaven and Hell" patterns in the euclidean plane. In H. S. M. Coxeter <i>et al.</i>, editor, <i>M.C. Escher: Art and Science</i>, pages 35--45. Elsevier Science Publishers B. V., 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[David W. Farmer. <i>Groups and Symmetry: A Guide to Discovering Mathematics</i>. American Mathematical Society, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383668</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Chi-Wing Fu and Man-Kang Leung. Texture tiling on arbitrary topological surfaces. In <i>Proceedings of Eurographics Symposium on Rendering 2005 (EGSR 2005)</i>, pages 99--104, June 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618551</ref_obj_id>
				<ref_obj_pid>616053</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Andrew Glassner. Andrew Glassner's notebook: Penrose tiling. <i>IEEE Computer Graphics &amp; Applications</i>, 18(4), jul--aug 1998. ISSN 0272-1716.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Solomon W. Golomb. <i>Polyominoes: Puzzles, Patterns, Problems and Packings</i>. Princeton University Press, second edition, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Branko Gr&#252;nbaum and G. C. Shephard. Spherical tilings with transitivity properties. In Chandler Davis, Branko Gr&#252;nbaum, and F. A. Sherk, editors, <i>The Geometric Vein: The Coxeter Festschrift</i>, pages 65--94. Springer-Verlag, New York, 1982.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Branko Gr&#252;nbaum and G. C. Shephard. <i>Tilings and Patterns</i>. W. H. Freeman, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[H. Heesch. Aufbau der ebene aus kongruenten bereichen. <i>Nachrichten von der Gesellschaft der Wissenschaften zu G&#246;ttingen</i>, pages 115--117, 1935. John Berglund provides an online English translation at http://www.angelfire.com/mn3/anisohedral/heesch35.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[H. Heesch and O. Kienzle. <i>Flachenschluss</i>. Springer-Verlag, 1963.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Daniel H. Huson. The generation and classification of tile-<i>k</i>-transitive tilings of the euclidean plane, the sphere, and the hyperbolic plane. <i>Geometriae Dedicata</i>, 47:269--296, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>936717</ref_obj_id>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Craig S. Kaplan. <i>Computer Graphics and Geometric Ornamental Design</i>. PhD thesis, Department of Computer Science &amp; Engineering, University of Washington, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345022</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Craig S. Kaplan and David H. Salesin. Escherization. In <i>Proceedings of the 27th annual conference on Computer graphics and interactive techniques (SIGGRAPH 2000)</i>, pages 499--510. ACM Press/Addison-Wesley Publishing Co., 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1006089</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Craig S. Kaplan and David H. Salesin. Dihedral Escherization. In <i>GI '04: Proceedings of the 2004 conference on Graphics interface</i>, pages 255--262. Canadian Human-Computer Communications Society, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141916</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[Johannes Kopf, Daniel Cohen-Or, Oliver Deussen, and Dani Lischinski. Recursive wang tiles for real-time blue noise. <i>ACM Trans. Graph.</i>, 25(3):509--518, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141916</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Johannes Kopf, Daniel Cohen-Or, Oliver Deussen, and Dani Lischinski. Recursive wang tiles for realtime blue noise. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2006)</i>, 25(3):509--518, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276475</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Victor Ostromoukhov. Sampling with polyominoes. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</i>, page 78. ACM, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015750</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[Victor Ostromoukhov, Charles Donohue, and Pierre-Marc Jodoin. Fast hierarchical importance sampling with blue noise properties. In <i>SIGGRAPH '04: ACM SIGGRAPH 2004 Papers</i>, pages 488--495. ACM, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Doris Schattschneider. <i>M.C. Escher: Visions of Symmetry</i>. W. H. Freeman, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Marjorie Senechal. <i>Quasicrystals and Geometry</i>. Cambridge University Press, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[A. V. Shubnikov and V. A. Koptsik. <i>Symmetry in Science and Art</i>. Plenum Press, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Jos Stam. Aperiodic texture mapping. Technical Report 01/97-R046, The Europeran Research Consotium for Informatics and Mathematics, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Dorothy K. Washburn and Donald W. Crowe. <i>Symmetries of Culture</i>. University of Washington Press, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[Hermann Weyl. <i>Symmetry</i>. Princeton Science Library, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364371</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[Jane Yen and Carlo S&#233;quin. Escher sphere construction kit. In <i>Proceedings of the 2001 symposium on Interactive 3D graphics</i>, pages 95--98. ACM Press, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[Ball, W. W. R. <i>Mathematical recreations and essays</i>. MacMillan and Co., 1926.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[Berger, R. The undecidability of the domino problem. <i>Memoirs American Mathematical Society</i>, 66:1--72, 1966.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028553</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[Chenney, S. Flow tiles. In <i>SCA '04: Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, pages 233--242. 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882265</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[Cohen, M. F., Shade, J., Hiller, S., and Deussen, O. Wang tiles for image and texture generation. <i>ACM Transactions on Graphics</i>, pages 287--294, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245814</ref_obj_id>
				<ref_obj_pid>245761</ref_obj_pid>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[Culik, II, K. An aperiodic set of 13 Wang tiles. <i>Discrete Mathematics</i>, 160(1--3):245--251, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[Culik, II, K. and Kari, J. An aperiodic set of Wang cubes. <i>Journal of Universal Computer Science</i>, 1(10), 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[Decaudin, P. and Neyret, F. Packing square tiles into one texture. In <i>Eurographics '04 (short papers)</i>, pages 49--52. 2004a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383546</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[Decaudin, P. and Neyret, F. Rendering forest scenes in real-time. In <i>Rendering Techniques '04 (Eurographics Symposium on Rendering)</i>, pages 93--102. 2004b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807383</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[Dungan, W., Jr., Stenger, A., and Sutty, G. Texture tile considerations for raster graphics. In <i>SIGGRAPH '78: Proceedings of the 5th annual conference on Computer graphics and interactive techniques</i>, pages 130--134. 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[Escher, M. C. and Locher, J. C. <i>The World of M. C. Escher</i>. Abrams, New York, NY, USA, 1971.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383668</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[Fu, C.-W. and Leung, M.-K. Texture tiling on arbitrary topological surfaces using Wang tiles. In <i>Rendering Techniques 2005</i>, pages 99--104. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618531</ref_obj_id>
				<ref_obj_pid>616052</ref_obj_pid>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[Glassner, A. Aperiodic tiling. <i>IEEE Computer Graphics &amp; Applications</i>, 18(3):83--90, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>318952</ref_obj_id>
				<ref_seq_no>129</ref_seq_no>
				<ref_text><![CDATA[Glassner, A. <i>Andrew Glassner's notebook: recreational computer graphics</i>. Morgan Kaufmann Publishers, Inc., San Fransisco, CA, USA, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>19304</ref_obj_id>
				<ref_seq_no>130</ref_seq_no>
				<ref_text><![CDATA[Gr&#252;nbaum, B. and Shepard, G. C. <i>Tilings and patterns</i>. W. H. Freeman and Company, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383327</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>131</ref_seq_no>
				<ref_text><![CDATA[Hausner, A. Simulating decorative mosaics. In <i>Proceedings of ACM SIGGRAPH 2001</i>, pages 573--580. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>718507</ref_obj_id>
				<ref_obj_pid>647260</ref_obj_pid>
				<ref_seq_no>132</ref_seq_no>
				<ref_text><![CDATA[Hiller, S., Deussen, O., and Keller, A. Tiled blue noise samples. In <i>Vision, Modeling, and Visualization 2001</i>, pages 265--272. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>133</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. Voronoi diagrams and ornamental design. In <i>ISAMA'99: The first annual symposium of the International Society for the Arts, Mathematics, and Architecture</i>, pages 277--283. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>134</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. Computer generated Islamic star patterns. In <i>Bridges 2000: Mathematical Connections in Art, Music and Science</i>, pages 105--112. 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>936717</ref_obj_id>
				<ref_seq_no>135</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. <i>Computer Graphics and Geometric Ornamental Design</i>. Ph.D. thesis, Department of Computer Science and Engineering, University of Washington, Seattle, USA, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089538</ref_obj_id>
				<ref_obj_pid>1089508</ref_obj_pid>
				<ref_seq_no>136</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. Islamic star patterns from polygons in contact. In <i>GI '05: Proceedings of the 2005 conference on Graphics interface</i>, pages 177--185. Canadian Human-Computer Communications Society, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>137</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. A meditation on Kepler's Aa. In <i>Bridges 2006: Mathematical Connections in Art, Music and Science</i>, pages 465--472. 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>138</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. The trouble with five. <i>Plus Magazine</i>, 2007. 15 pages, to appear. Invited article on five-fold tilings.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>139</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. and Hart, G. W. Symmetrohedra: polyhedra from symmetric placement of regular polygons. In <i>Bridges 2001: Mathematical Connections in Art, Music and Science</i>, pages 21--28. 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345022</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>140</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. and Salesin, D. H. Escherization. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, pages 499--510. ACM Press/Addison-Wesley Publishing Co., 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1006089</ref_obj_id>
				<ref_obj_pid>1006058</ref_obj_pid>
				<ref_seq_no>141</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. and Salesin, D. H. Dihedral Escherization. In <i>GI '04: Proceedings of the 2004 conference on Graphics interface</i>, pages 255--262. Canadian Human-Computer Communications Society, 2004a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>990003</ref_obj_id>
				<ref_obj_pid>990002</ref_obj_pid>
				<ref_seq_no>142</ref_seq_no>
				<ref_text><![CDATA[Kaplan, C. S. and Salesin, D. H. Islamic star patterns in absolute geometry. <i>ACM Trans. Graph.</i>, 23(2):97--119, 2004b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>245817</ref_obj_id>
				<ref_obj_pid>245761</ref_obj_pid>
				<ref_seq_no>143</ref_seq_no>
				<ref_text><![CDATA[Kari, J. A small aperiodic set of Wang tiles. <i>Discrete Mathematics</i>, 160(1--3):259--264, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141916</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>144</ref_seq_no>
				<ref_text><![CDATA[Kopf, J., Cohen-Or, D., Deussen, O., and Lischinski, D. Recursive Wang tiles for real-time blue noise. <i>ACM Transactions on Graphics</i>, 25(3):509--518, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>145</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. <i>Tile-Based Methods in Computer Graphics</i>. Ph.D. thesis, Department of Computer Science, K. U. Leuven, Leuven, Belgium, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095888</ref_obj_id>
				<ref_obj_pid>1095878</ref_obj_pid>
				<ref_seq_no>146</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. A procedural object distribution function. <i>ACM Transactions on Graphics</i>, 24(4):1442--1461, 2005a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>147</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. Template Poisson disk tiles. Report CW 413, Department of Computer Science, K. U. Leuven, Leuven, Belgium, 2005b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1183296</ref_obj_id>
				<ref_obj_pid>1183287</ref_obj_pid>
				<ref_seq_no>148</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. An alternative for Wang tiles: Colored edges versus colored corners. <i>ACM Transactions on Graphics</i>, 25(4):1442--1459, 2006a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>149</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. Generating well-distributed point sets with a self-similar hierarchical tile. Report CW 462, Department of Computer Science, K. U. Leuven, Leuven, Belgium, 2006b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>150</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. Long period hash functions for procedural texturing. In <i>Vision, Modeling, and Visualization 2006</i>, pages 225--228. Akademische Verlagsgesellschaft Aka GmbH, Berlin, 2006c.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>151</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. Poisson sphere distributions. In <i>Vision, Modeling, and Visualization 2006</i>, pages 373--379. Akademische Verlagsgesellschaft Aka GmbH, Berlin, 2006d.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>152</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. A comparison of methods for generating Poisson disk distributions. <i>Computer Graphics Forum</i>, 2007a. To appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>153</ref_seq_no>
				<ref_text><![CDATA[Lagae, A. and Dutr&#233;, P. The tile packing problem. <i>Geombinatorics</i>, 17(1), 2007b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>154</ref_seq_no>
				<ref_text><![CDATA[Lagae, A., Kari, J., and Dutr&#233;, P. Aperiodic sets of square tiles with colored corners. Report CW 460, Department of Computer Science, K. U. Leuven, Leuven, Belgium, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641518</ref_obj_id>
				<ref_obj_pid>641480</ref_obj_pid>
				<ref_seq_no>155</ref_seq_no>
				<ref_text><![CDATA[Lefebvre, S. and Neyret, F. Pattern based procedural textures. In <i>Proceedings of the 2003 Symposium on Interactive 3D Graphics</i>, pages 203--212. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>156</ref_seq_no>
				<ref_text><![CDATA[Lemmen, H. V. <i>Tiles 1000 Years of Architectural Decoration</i>. Harry N. Abrams, Inc., 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2231922</ref_obj_id>
				<ref_obj_pid>2231878</ref_obj_pid>
				<ref_seq_no>157</ref_seq_no>
				<ref_text><![CDATA[Leung, M.-K., Pang, W.-M., Fu, C.-W., Wong, T.-T., and Heng, P.-A. Tileable BTF. <i>IEEE Transactions on Visualization and Computer Graphics (TVCG)</i>, 13(5):953--965, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1446251</ref_obj_id>
				<ref_obj_pid>1446227</ref_obj_pid>
				<ref_seq_no>158</ref_seq_no>
				<ref_text><![CDATA[Li, H., Lo, K.-Y., Leung, M.-K., and Fu, C.-W. Dual Poisson-disk tiling: An efficient method for distributing features on arbitrary surfaces. <i>IEEE Transactions on Visualization and Computer Graphics (TVCG)</i>,???? Accepted for publication.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338555</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>159</ref_seq_no>
				<ref_text><![CDATA[Lo, K.-Y., Li, H., Fu, C.-W., and Wong, T.-T. Interactive reaction-diffusion on surface tiles. In <i>Proceedings of Pacific Graphics 2007</i>, pages 65--74. 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>160</ref_seq_no>
				<ref_text><![CDATA[Lu, A. and Ebert, D. S. Example-based volume illustrations. In <i>Proceedings of IEEE Visualization</i>, pages 655--662. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1243985</ref_obj_id>
				<ref_obj_pid>1243980</ref_obj_pid>
				<ref_seq_no>161</ref_seq_no>
				<ref_text><![CDATA[Lu, A., Ebert, D. S., Qiao, W., Kraus, M., and Mora, B. Volume illustration using Wang cubes. 26(2), 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>162</ref_seq_no>
				<ref_text><![CDATA[Lukkarila, V. The square tiling problem is NP-complete for deterministic tile sets. Technical Report TUCS Technical Report No 754, Turku Centre for Computer Science, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>163</ref_seq_no>
				<ref_text><![CDATA[MacMahon, M. P. A. <i>New mathematical pastimes</i>. Cambridge University Press, 1921.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311561</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>164</ref_seq_no>
				<ref_text><![CDATA[Neyret, F. and Cani, M.-P. Pattern-based texturing revisited. In <i>Proceedings of ACM SIGGRAPH 1999</i>, pages 235--242. 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1252500</ref_obj_id>
				<ref_obj_pid>1251973</ref_obj_pid>
				<ref_seq_no>165</ref_seq_no>
				<ref_text><![CDATA[Ng, T.-Y., Wen, C., Tan, T.-S., Zhang, X., and Kim, Y. J. Generating an &#969;-tile set for texture synthesis. In <i>Proceedings of Computer Graphics International 2005</i>, pages 177--184. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276475</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>166</ref_seq_no>
				<ref_text><![CDATA[Ostromoukhov, V. Sampling with polyominoes. <i>ACM Transactions on Graphics</i>, 26(3), 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015750</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>167</ref_seq_no>
				<ref_text><![CDATA[Ostromoukhov, V., Donohue, C., and Jodoin, P.-M. Fast hierarchical importance sampling with blue noise properties. <i>ACM Transactions on Graphics</i>, 23(3):488--495, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>168</ref_seq_no>
				<ref_text><![CDATA[Penrose, R. The r&#244;le of aesthetics in pure and applied mathematical research. <i>Bulletin of the Institute of Mathematics and its Applications</i>, 10:266--271, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>169</ref_seq_no>
				<ref_text><![CDATA[Robinson, R. M. Seven polygons which admit only nonperiodic tilings of the plane (abstract). <i>Notices of the American Mathematical Society</i>, 14:835, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>170</ref_seq_no>
				<ref_text><![CDATA[Saladin, H. <i>L'Alhambra de Grenade</i>. Morance, Paris, France, 1926.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>171</ref_seq_no>
				<ref_text><![CDATA[Shade, J., Cohen, M. F., and Mitchell, D. P. Tiling layered depth images. Technical report, University of Washington, Department of Computer Science and Engineering, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186439</ref_obj_id>
				<ref_obj_pid>1186415</ref_obj_pid>
				<ref_seq_no>172</ref_seq_no>
				<ref_text><![CDATA[Sibley, P., Montgomery, P., and Marai, G. E. Wang cubes for video synthesis and geometry placement. In <i>ACM SIGGRAPH 2004 Poster Compendium</i>. 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>173</ref_seq_no>
				<ref_text><![CDATA[Stam, J. Aperiodic texture mapping. Technical Report ERCIM-01/97-R046, European Research Consortium for Informatics and Mathematics (ECRIM), 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>174</ref_seq_no>
				<ref_text><![CDATA[Wang, H. Proving theorems by pattern recognition - II. <i>Bell Systems Technical Journal</i>, 40:1--42, 1961.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>175</ref_seq_no>
				<ref_text><![CDATA[Wang, H. Games, logic and computers. <i>Scientific American</i>, 213(5):98--106, 1965.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>176</ref_seq_no>
				<ref_text><![CDATA[Wang, H. Notes on a class of tiling problems. <i>Fundamenta Mathematicae</i>, 82:295--305, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1058138</ref_obj_id>
				<ref_obj_pid>1058129</ref_obj_pid>
				<ref_seq_no>177</ref_seq_no>
				<ref_text><![CDATA[Wei, L.-Y. Tile-based texture mapping on graphics hardware. In <i>Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</i>, pages 55--63. 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>178</ref_seq_no>
				<ref_text><![CDATA[Xu, J. and Kaplan, C. S. Vortex maze construction. <i>Journal of Mathematics and the Arts</i>, 1(1):7--20, 2007. A shorter version appeared in the proceedings of Bridges 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>1401255</section_id>
		<sort_key>1230</sort_key>
		<section_seq_no>30</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Impact on Society: Transportation visualization]]></section_title>
		<section_page_from>30</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P1098917</person_id>
				<author_profile_id><![CDATA[81350590594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theresa-Marie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rhyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098918</person_id>
				<author_profile_id><![CDATA[81421599565]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
			<ch_ed>
				<person_id>P1098919</person_id>
				<author_profile_id><![CDATA[81332505590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Editor]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1401256</article_id>
		<sort_key>1240</sort_key>
		<display_label>Article No.</display_label>
		<pages>12</pages>
		<display_no>94</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Transportation visualization]]></title>
		<page_from>1</page_from>
		<page_to>12</page_to>
		<doi_number>10.1145/1401132.1401256</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401256</url>
		<abstract>
			<par><![CDATA[<p>This class will highlight how transportation planners, engineers and members of the Transportation Research Board's Committee on Visualization in Transportation are using computer graphics techniques and interactive visual displays in their system planning, project design, construction and public involvement activities. Practical examples include the depiction of how three-dimensional models of alternate roundabout treatments in roadway designs are currently being used in conjunction with microsimulation models of driver-vehicle interactions to evaluate alternative crossing solutions for visually impaired pedestrians at roundabouts and channelized turn lanes. We would like to share some of our observations with the SIGGRAPH community to gain additional insight and foster future cross disciplinary interaction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.6.1</cat_node>
				<descriptor>Systems theory</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010346.10010347</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation theory->Systems theory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098920</person_id>
				<author_profile_id><![CDATA[81350590594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theresa-Marie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rhyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Carolina State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098921</person_id>
				<author_profile_id><![CDATA[81421599565]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AEC Visualization]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1098922</person_id>
				<author_profile_id><![CDATA[81332505590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Carolina State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ACM SIGGRAPH 2008 Class on Transportation Visualization Class Organizer &#38; Instructor: Theresa-Marie 
Rhyne Director of the Center for Visualization &#38; Analytics and the Renaissance Computing Institute's 
Engagement Facility at North Carolina State University tmrhyne@ncsu.edu OR tmrhyne@renci.org  Instructors: 
Michael Manore, P.E. Chair of the Transportation Research Board's Committee on Visualization in Transportation 
&#38; Independent Consultant - AEC Visualization Michael.manore@gmail.com Ronald G. Hughes, PhD. Program 
Director for Visual Analytics, Modeling &#38; Simulation, Institute for Transportation Research and Education 
North Carolina State University rghughes@ncsu.edu Abstract: This class will highlight how transportation 
planners, engineers and members of the Transportation Research Board's Committee on Visualization in 
Transportation are using computer graphics techniques and interactive visual displays in their system 
planning, project design, construction and public involvement activities. Practical examples include 
the depiction of how three-dimensional models of alternate roundabout treatments in roadway designs are 
currently being used in conjunction with microsimulation models of driver-vehicle interactions to evaluate 
alternative crossing solutions for visually impaired pedestrians at roundabouts and channelized turn 
lanes. We would like to share some of our observations with the SIGGRAPH community to gain additional 
insight and foster future cross disciplinary interaction. Introduction In this short class, we provide 
a brief overview of how computer graphics techniques and visualization methods are currently being applied 
to examine transportation planning and engineering design issues and solutions. Each instructor of this 
class is a member of the Transportation Research Board (TRB)'s Committee on Visualization in Transportation. 
We hope to cross boundaries and integrate the viewpoints of transportation engineering and computer graphics 
as well as illustrate parallels to the more recognized areas of scientific visualization and information 
visualization. In its September/October 2007 issue of TR News, the Transportation Research Board focused 
on "Visualization in Transportation". Each instructor prepared an article for this special TR News issue 
that highlighted the use of computer graphics and visualization for transportation education, research, 
planning and construction. With the permission of the TR News publication and the Transportation Research 
Board, we provide these discussions as our notes for our SIGGRAPH 2008 class. We also include the slides 
from the Defining Transportation Visualization topic shown in the class outline below. The TRB s Committee 
on Visualization in Transportation s web site is located at: (http://www.trbvis.org/MAIN/TRBVIS_HOME.html). 
 *Photo shown from a transportation visualization working session at the Renaissance Computing Institute's 
Engagement Facility at NC State University. Transportation Visualization Class Outline: Introductory 
Remarks (5 minutes) (Hughes, Manore, and Rhyne) Defining Transportation Visualization (25 minutes + 5 
minutes of questions) (Theresa-Marie Rhyne) Visualizing the various elements of transportation systems: 
how they look , how they work and different requirements for planners, engineers, and public stakeholders 
(25 minutes + 5 minutes of questions) (Ron Hughes) Applying Computer Graphics Techniques in Transportation 
Design and Construction including How to Educate Transportation Engineers about Visualization (25 minutes 
+ 5 minutes of questions) (Michael Manore) Concluding Remarks - Where to Find Out More (10 minutes) (Hughes, 
Manore, and Rhyne) *Photo shown from a transportation visualization working session at the Renaissance 
Computing Institute's Engagement Facility at NC State University. Prerequisites: An interest in learning 
more about transportation visualization from transportation engineers and the Transportation Research 
Board community. Instructors Short Biographies: Michael Manore is a practicing engineer and chair of 
the Transportation Research Board's Committee on Visualization in Transportation. He has pioneered the 
application of visualization techniques to transportation in his work with Bentley Systems, Inc., the 
Minnesota Department of Transportation and other engineering efforts. Ron Hughes has a Ph.D. in experimental 
psychology, and is responsible for the development, management, and execution of an extensive transportation 
visualization research program area for the Institute for Transportation Research and Education at NC 
State University. His transportation related research in this area draws upon years of prior experience 
with visual simulation and modeling in the military/defense environment and in the practical application 
of simulation in the aerospace area. Theresa-Marie Rhyne is a long time contributor to the ACM SIGGRAPH 
and IEEE Visualization communities, having organized and taught previous courses in visualization and 
internetworked 3D graphics at both annual conferences, and is currently the Director of the Center for 
Visualization &#38; Analytics and the Renaissance Computing Institute's Engagement Facility at NC State 
University. One Way to think of Computer Generated Visualization: Define it as Computationally Intense 
Visual Thinking. ACM SIGGRAPH 2008 Class on Transportation Visualization So, maybe in a Roundabout way: 
Visualization of Roundabouts created by Thomas Fischer of the New York State Department of Transportation 
in a collaboration with Ron Hughes of NCSU ITRE. Shown on the RENCI@NCSU Visualization Display Wall. 
Photo: Theresa-Marie Rhyne ACM SIGGRAPH 2008 Class on Transportation Visualization   GeoVisualization 
Example Working with faculty in NC State University s College of Design on a Visualization of the Blue 
Ridge Parkway. Original visualization by Michael Holmes, Dr. John Fels and James Tomlinson of NC State 
University s College of Design. Enhanced 3D Visualization by Theresa-Marie Rhyne, NC State University 
 Learning Technology Service (circa 2002/2003). ACM SIGGRAPH 2008 Class on Transportation Visualization 
 What about visualization in the planning &#38; engineering communities: Geographic Information Systems: 
 Computer Aided Design: Immersive Simulation: Other computer generated methods: In the Computer Science 
arenas: these methods would be categorized under the broad umbrella of Computer Graphics &#38; Interactive 
Techniques methods that are presented at the Association for Computing Machinery s Special Interest 
Group on Graphics (ACM SIGGRAPH) s annual conference and exhibition. ACM SIGGRAPH 2008 Class on Transportation 
Visualization  Example: NCSU Virtual Campus Using GoogleEarth: On August 2, 2007, RENCI@NCSU hosted 
a demonstration by the NC State Univ. Distance Education &#38; Learning Technology Applications (DELTA) 
Unit &#38; students from the College of Design of their NC State Univ. Virtual Campus application built 
in Google Earth. Photo: Theresa-Marie Rhyne ACM SIGGRAPH 2008 Class on Transportation Visualization 
 There are three Visualization Subfields: Scientific Visualization: defined by an NSF Report of 1987, 
first IEEE Visualization 1990 Conference focused on Scientific Visualization the visual display of spatial 
data associated with scientific processes such as the bonding of molecules in computational chemistry. 
 Information Visualization: evolved in early 1990 s, first IEEE Information Visualization Symposium held 
in 1995 with IEEE Visualization Conference developing visual metaphors for non-inherently spatial data 
such as the exploration of text-based document databases. Visual Analytics: defined by a 2005 Dept. 
of Homeland Security report, first IEEE Visual Analytics (VAST) Symposium held in 2006 with IEEE Visualization 
Conference. The science of analytical reasoning, facilitated by interactive visual interfaces. ACM SIGGRAPH 
2008 Class on Transportation Visualization  Let s consider Transportation Examples: Scientific Visualization 
Example: Visualization of Speed Profile Upstream of Freeway Bottleneck by Nagui M. Rouphail, ITRE, 2007. 
Information Visualization Example: from online GIS database of fatal crashes involving trucks in North 
Carolina, 2001-2005, ITRE. 1 2 3 4 5 6 7 8 9 10 1 2 3 4 0 10 20 30 40 50 60 70 Speed (mph) Segment Number 
Time Interval 60-70 50-60 40-50 30-40 20-30 10-20 0-10 ACM SIGGRAPH 2008 Class on Transportation Visualization 
 Let s consider Transportation Examples: Visual Analytics Example: display for collaboration and sharing 
of visual information by transportation researchers and users. Imagery is from a 3-D model of alternative 
roundabout treatments under consideration by National Cooperative Highway Research Program (NCHRP) Project 
3-78, Crossing Solutions for Visually Impaired Pedestrians at Roundabouts and Channelized Turn Lanes. 
Imagery developed in collaboration with Thomas Fischer, New York State Department of Transportation (NY 
DOT). Photo: Theresa-Marie Rhyne ACM SIGGRAPH 2008 Class on Transportation Visualization For Further 
Reading: Visualization and the Larger World of Computer Graphics What s Happening Out There? By Theresa-Marie 
Rhyne In TR News, September October 2007 issue See: http://research.csc.ncsu.edu/cva/TRN_RHYNE.pdf ACM 
SIGGRAPH 2008 Class on Transportation Visualization   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401257</article_id>
		<sort_key>1250</sort_key>
		<display_label>Article No.</display_label>
		<pages>5</pages>
		<display_no>95</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Visualization education and training]]></title>
		<page_from>1</page_from>
		<page_to>5</page_to>
		<doi_number>10.1145/1401132.1401257</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401257</url>
		<abstract>
			<par><![CDATA[<p>Learning the new tools---and new tools for learning</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Education</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098923</person_id>
				<author_profile_id><![CDATA[81421599565]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Manore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Infrastructure Visualization Center, Bentley Systems, Inc., Stillwater, Minnesota, and TRB Visualization in Transportation Committee]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
<i>The Engineer of 2020: Visions of Engineering in the New Century</i>. National Academy of Engineering, Washington, D.C., 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
<i>Educating the Engineer of 2020: Adapting Engineering Education to the New Century</i>, National Academy of Engineering, Washington, D.C., 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Messner, J. I., D. R. Riley, and M. J. Horman. An Interactive Visualization Environment for Construction Engineering Education. Presented at Construction Research Congress, American Society of Civil Engineers, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Coller, B. <i>Implementing a Video Game to Teach Principles of Mechanical Engineering</i>. Presented at American Society for Engineering Education Annual Conference, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Roberts, D. F., U. G. Foehr, and V. Rideout. Generation M: Media in the Lives of 8-18 Year-Olds. Technical Report, Kaiser Family Foundation, March 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Haque, M. E. Web-Based Visualization Techniques for Structural Design Education. Presented at the American Society of Engineering Education Annual Conference and Exposition, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hixon, C. L., III. <i>NCHRP Synthesis 361: Visualization for Project Development</i>. Transportation Research Board of the National Academies, Washington, D.C., 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Manore, M. A. <i>Enabling a More Visual and Immersive Engineering Environment for the Highway Transportation Industry</i>. Presented at IMAGE 2005 Conference, Scottsdale, Arizona, July 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Manore, M. A. It's About Decisions: Improving Transportation Project Development with Visualization Technologies. <i>TR News</i>, No. 248, January-February 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Schwartz, E. M. <i>An Exploration of Collaborative 3-D Visualization Processing in Engineering Education and Supporting Tools</i>. Thesis. University of Misouri-Rolla, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[
<i>Visions 2020: Transforming Education and Training Through Advanced Technologies</i>. Office of Public Affairs, U.S. Department of Commerce Technology Administration, September 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[
<i>The Visual-Spatial Learner: An Introduction</i>. The Gifted Development Center. www.gifteddevelopment.com/Visual_Spatial_Learner/vsl.htm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Wen, H., and K. Gramoll. Online 3-D Collaboration System for Engineering Education. Presented at American Society of Engineering Education Annual Conference, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualization Education and Training Learning the New Tools and New Tools for Learning MICHAEL A. MANORE 
The author is Director, Infrastructure Visualization Center, Bentley Systems, Inc., Stillwater, Minnesota, 
and Chair, TRB Visualization in Transportation Committee. T he transportation industry is advancing 
on all fronts to apply visual technologies, reflecting increased awareness, under­standing, and interest. 
Every month, trade and professional publications introduce new and innovative uses of visualization tools, 
as researchers and practitioners explore new ways to create and deliver infrastructure. The transportation 
industry is tasked with plan­ning, designing, building, and operating infrastruc­ture to accommodate 
evolving transportation needs. Is visualization another of many tools or is it a catalyst for a new way 
of thinking? Throughout history, every profession has adopted the latest tools to improve the quality 
and efficiency of the work performed. But often the adoption of a new tool typically in conjunction with 
professional and PHOTO: PENN STATE technological advances or some other major event has engaged practitioners 
in rethinking how they deliver their product or service. Transportation has arrived at such a transition. 
The presence of visualization has expanded so rapidly in the past 5 years that organizations have yet 
to fathom the implications for program delivery, and professionals have yet to sort out the implications 
for their practice and for the education of their successors.  Higher Levels Visualization is more than 
automating traditional tasks such as word processing or drafting, and it is more than the sharing of 
overwhelming amounts of infor­mation faster. Visualization enables communication, learning, problem solving, 
collaboration, and decision making at a higher level of thought, and it will chal­lenge current practices 
for delivering transportation programs. Visualization training and education, therefore, require more 
than teaching how to create and animate three­dimensional (3­D) geometry. The goals should include . 
Applying progressive visual methods to com­prehend and communicate the magnitude of trans­portation data 
and needs, to enable more targeted capital investment strategies; . Enabling organizations to incorporate 
project delivery practices that are visually and spatially enhanced; . Complementing traditional learning 
methods with innovative visual learning environments to  Expand problem­solving skills,  Enhance comprehension, 
and  Extend engineering communication to include visual with written and oral skills.    TR NEWS 
252 SEPTEMBER OCTOBER 2007 The transportation community leverages a wealth of data and information intended 
to define and make sense of society s evolving needs for transportation infrastructure. An even greater 
amount of data and information is leveraged to understand the state of the infrastructure and its performance. 
Transportation has become an extremely complex industry, and the abil­ity of professionals to fund, plan, 
design, and deliver functionally appropriate infrastructure will require tools that enhance the ability 
to learn, think, and com­municate. Advances Under Way Most engineering consulting firms today would 
recog­nize visualization as a necessity for winning big projects or as a line item to charge to a client. 
Transportation agencies may consider visualization a luxury that demands additional resources and is 
typically reserved for public involvement on large projects. But related advances, evolving and under 
way, show that the industry is rethinking ways to improve the delivery of products and services through 
visually enhanced tools. These advances include . Machine control and digital staking, both of which 
require accurate 3­D subgrade models; . Interactive 3­D and 4­D models linked to proj­ect scheduling 
software; . Immersive driving simulators to assess human factors in design and in work zones; . Immersive 
display systems for stakeholder involvement; . Terrestrial, mobile, and airborne lidar survey systems; 
 . 3­D geographic information systems and tech­nologies similar to Google Earth, applied in envi­ronmental 
impact statements and land use planning; . Microsimulation visualization methods for complex traffic 
modeling and forecasting; and . Temporal visualization of freight movement data, working with radio 
frequency identification technologies.  Resulting Concerns New advances, however, introduce new interests 
and new concerns. Some of the leading concerns expressed by the transportation industry about visualization 
include the following: . Defining the breadth of visualization, model­ing, and simulation; . Understanding 
which technology to apply at what point and for what purpose; . The cost of upgrading information technology 
infrastructure to manage and share data;  IMAGE: HNTB AND INTELISUM . Developing the expertise to handle, 
capture, and create data; . Training professionals to plan for and use visu­alization; . Understanding 
the organizational and profes­sional ethics related to visualization; . Projecting expected returns 
on investment compared with the costs of not using visualization; . Developing standards for the content, 
accu­racy, and quality of contractor and consultant data submittals; . Writing effective contract language 
for visual­ization services; . Understanding implications for organizational work flows and opportunities 
to improve business practices; . Integrating and relating the data to the kinds of data and information 
already in use; . Rethinking position descriptions for hiring professionals and specialists; . Promoting 
visual learning environments for engineering students to develop visual communica­tion skills; and . 
Rethinking the capture of, display of, and inter­action with transportation data to enable more effec­tive 
executive­level decision making.  Addressing Challenges Many organizations are addressing some of these 
chal­lenges by initiating small groups to master the basics. These organizations include the departments 
of trans­portation of Washington State, Minnesota, Utah, Alabama, and New York State; and such companies 
as Parsons Brinckerhoff, Freese and Nichols, and URS Corporation. In response to the new public involvement 
and planning­level requirements for visualization in the Safe, Accountable, Flexible, Efficient Transportation 
Equity Act: A Legacy for Users (SAFETEA­LU), the Federal Highway Administration (FHWA) has orga­nized 
a task force to identify and learn about all visu­alization­related efforts, resources, and opportunities 
and to lead the organization in addressing many of the interests and concerns. With Parsons Brinckerhoff, 
Visualization with lidar to plan and explain innovative accelerated construction for a bridge replacement 
on I­215 East in Salt Lake City, Utah. (For details, contact Rukhsana Lindsey, Utah DOT Director of Research, 
801­965­4196.) TR NEWS 252 SEPTEMBER OCTOBER 2007 TR NEWS 252 SEPTEMBER OCTOBER 2007 The Engineer of 
2020: Visions of Engineering in the New Century, produced by the National Academy of Engineering, engaged 
a diverse group of stakeholders to gather facts, make forecasts, and develop scenarios of world conditions 
for the 2020 engineer. The companion volume, Educating the Engineer of 2020: Adapting Engineering Education 
to the New Century, offers recommendations on how to enrich and broaden engineering education in a constantly 
changing global economy. (For ordering information, see www.nap.edu.) FHWA s Federal Lands Division 
has created a website to inform their staff and the transportation community about certain aspects of 
design visualization. The Fed­eral Transit Administration has initiated a research project, Evaluating 
the Effectiveness of Widely Avail­able 3­D Visualization Tools in Support of Public Participation, under 
the Public Transportation Partic­ipation Program.1 The Transportation Research Board recently estab­lished 
the Visualization in Transportation Committee to perform outreach and define areas for research. The 
committee convened the 5th International Visualiza­tion in Transportation Symposium and Workshop in October 
2006 and made the proceedings available on the Internet.2 In the absence of a focused resource for training 
and education, progress at transportation organiza­tions can be credited to . One or two visionaries 
who balance vision with strategic action; . Executive support and willingness to shoulder some trial 
and error; . Technology provider training and consulting; . Trade and research publications; . Conferences; 
and . Monetary returns, realized or anticipated.   The resources are sufficient for any transportation 
organization to get started in understanding the tech­nology and in leveraging the benefits. 21st Century 
Engineers In 2004 the National Academy of Engineering (NAE) published two timely books: The Engineer 
of 2020 (1) and Educating the Engineer of 2020 (2). The NAE efforts covered all fields of engineering, 
and discussed the demands that society and the profession will place on future engineers. Some of the 
major points include the following: . The world s population will reach 8 billion in 2020, with most 
of the growth in underdeveloped countries. The engineering profession will need to provide solutions 
to an increasingly diverse popula­tion. . The numbers of foreign­born engineering stu­dents in the United 
States may decline, creating a need to increase and retain U.S. students. . Globalized and virtual work 
teams will collab­orate on electronic designs.  1 www.fta.dot.gov/planning/programs/planning_ environment_5925.html. 
2 Proceedings may be found at www.teachamerica.com/viz/ viz2006.html. . More work will be done in multidisciplinary 
teams, requiring excellence in communication. The incorporation of social elements into the engineering 
process such as context­sensitive solutions will introduce complexity. . Greater social interaction 
will be required between engineers and their customers. . Engineers will engage in public policy, because 
of the implications that advances in technology and engineering practice will have for society.  The 
most prominent influence identified by NAE is technology, which not only defines new subdisciplines of 
engineering but also responds to demands from society, influencing how engineers develop the exper­tise 
to accommodate those demands. Society already is influencing transportation visualization technologies. 
For example, the prevalence of computer graphics in movies, commercials, video games, educational pro­grams, 
and learning software has prompted stake­holders to have little patience for a project team that shows 
up at a public meeting with 2­D computer­aided drawings.  Integrated Visualization With the complexity 
of today s transportation proj­ects, and the influence of innovative methods and practices such as machine 
control, context­sensitive solutions, and design build, more and more engi­neering firms are teaming 
together, and project col­leagues are often distributed globally. In these environments, clear and comprehensible 
communi­cation is vital. Sending and finding volumes of project informa­tion quickly is not enough the 
information must be comprehended almost as fast and must improve the project team s ability to interact. 
Effectively inte­grated visualization can address this need directly.  Visual Learning Environments 
The 2003 edition of the American Society of Civil Engineers (ASCE s) report card, America s Aging Infrastructure, 
awarded the nation s infrastructure a grade of D+, recently downgraded to D. The report card is not intended 
to cast blame but to summarize the state of affairs and its implications. At the same time, the United 
States has a shortage of engineer­ing professionals to address this issue efficiently and creatively. 
Influences on the enrollment of engineering freshmen include salary potential, career advance­ment opportunities, 
and the complexity, expense, and duration of the program. Retention is an even greater concern. According 
to NAE, if universities could retain their engineering freshmen to gradua­tion, the number of engineers 
would increase by almost 40 percent. The Engineer of 2020 starts out: Engineering is a profoundly creative 
process. A most elegant description is that engineering is about design under constraint. The engineer 
designs devices, components, subsystems, and systems and, to create a successful design, in the sense 
that it leads to an improvement in our qual­ity of life, must work within constraints provided by technical, 
economic, business, political, social, and ethical issues. (1) Visual learning environments that foster 
the devel­opment of this creative process may provide incentives for today s engineering freshmen. Most 
of today s engineering freshmen have grown up with PlayStation, Xbox, Microsoft Flight Simulator, SimCity, 
Toy Story, Lord of the Rings, cell phones, and instant messaging. In short, they are wired differently 
from the generation of engineers now practicing accordingly, some of the drivers and needs that sustain 
interest in an engineering career are different for today s freshmen. One approach attracting attention 
is the creation of visual learning environments to teach everything from basic courses in statics and 
dynamics to advanced top­ics such as constructability reviews and critical path project scheduling. Some 
engineering educators and universities already are employing visual learning environments for their students. 
 Virtual Construction The Computer­Integrated Construction(CIC) Research Program at Pennsylvania State 
University has gained considerable success in teachingan inherently complex task of engineering how to 
perform design reviews and optimize scheduling for construction. CIC Director John Messner developed 
the program and has noted improvement in students abilities not only to learn the subject matter, but 
to communicate individually and in teams (3). These skills have direct applications to proj­ect team 
dynamics in the real world. Traditionally, writing and speaking have been the communication skills emphasized 
for engineering stu­dents, but changes in technology and the evolving demands of the workplace merit 
the development of a student s visual communication skills beyond Pow­erPoint and 2­D plans. Visual communication 
skills can complement and enhance the quality and effec­tiveness of writing and speaking. Construction 
projects bring all these skills together. The best planning, the best design, the best engineering analysis 
mean little unless the project team can figure out how to stage, schedule, and build the project effec­tively. 
Communication, collective understanding, and timing are key for turning a design into a usable piece 
of infrastructure, and visualization has a role in this. Video Game Realities The annual conference 
of the American Society of Engineering Education is experiencing an increase in papers on visualization 
applications for the classroom. A 2007 paper, Implementing a Video Game to Teach Principles of Mechanical 
Engineering, not only reported on the use of visual technologies but noted an increased depth of learning 
among the students (4). The author, Brianno Coller of Northern Illinois Uni­versity, experimented with 
a new way of teaching a course in numerical methods to undergraduate mechanical engineering students. 
Coller cites findings from a March 2005 study of media in the lives of children ages 8 to 18: . 83 percent 
of 8­to 18­year­olds have at least one video game console at home; . 31 percent have three or more; 
and . All children in the study, regardless of race, gender, or economic status, spend an average of 
68 minutes per day playing video games (5).  Coller discusses the relationships between video games, 
motivation, problem solving, and improved learning. He organized two groups of students who were taught 
the same subject material one group with traditional methods only, and the other group with traditional 
methods complemented by a video game, NIU­Torcs, codeveloped by Coller s team. The results showed minimal 
differences between the two groups in recalling the major topics and tech­niques in numerical methods. 
The students who learned the subject using the video game technology, however, were significantly more 
able to demonstrate TR NEWS 252 SEPTEMBER OCTOBER 2007 Screen for the NIU­Torcs video game, developed 
at Northern Illinois University to teach principles of engineering.  Showing Students the Concepts A 
s an engineering educator, I have seen a constant evolution of visualization technology in the classroom, 
notes Steven Barrett, Associate Professor of Electrical and Computer Engineering at the University of 
Wyoming. Educators still use chalk talk lec­tures, but a PC equipped with a video projection system has 
become standard classroom equipment. Instead of telling students about engineering concepts, we now can 
show them the concepts. This is a great first step to understanding the often complex details. Barrett 
served as Program Chair for the Computers in Educa­tion Division (CoED) of the American Society for Engineering 
Edu­cation (ASEE) annual conference in Honolulu, Hawaii, in June 2007. ASEE works for the advancement 
of education in engi­neering and in allied branches of science and technology, includ­ing teaching and 
learning, counseling, research, extension services, and public relations. In the CoED programs at the 
2007 ASEE conference, we saw a plethora of new ideas on how to use tablet PCs with visualization software 
to capture and engage students attention in the class­room, Barrett reports. I also have witnessed an 
explosion of the use of modeling and visualization tools such as the Mathworks MATLAB which allow the 
educator and researcher the capability to model and visualize complex engineering systems. Involved in 
engineering education for the past two decades, Bar­rett previously was an active­duty faculty member 
at the U.S. Air Force Academy, Colorado Springs, Colorado, and was named the 2004 Wyoming Professor of 
the Year by the Carnegie Foundation for the Advancement of Teaching. . How the various numerical methods 
worked; . The appropriate uses and limitations of each method; and . How the methods depended on one 
another. The motivational effects of engaging visual tech­nologies, such as a video game, therefore, 
may deepen the level of learning and understanding in engineering students beyond mere recall, and may 
enhance their ability to apply their knowledge more effectively and creatively in the real world. Coller 
s work, together with Messner s, strongly suggests that more engaging, visual learning environments may 
help to produce the caliber of thinkers, problem­solvers, and communicators needed to address soci­ety 
s infrastructure needs. Influencing Changes On August 1, the I­35W Bridge in Minneapolis col­lapsed, 
making the national headline news for several weeks. The ASCE report card, which had highlighted the 
fragility of the nation s aging infrastructure, also attracted media attention. As a result, considerable 
TR NEWS 252 SEPTEMBER OCTOBER 2007 activity is under way at all levels to rethink the U.S. approach to 
addressing transportation infrastructure. Combined with professional and technological advances or a 
major event, the use of new tools can engage practitioners to rethink how they deliver a product or service. 
Visualization, in all its forms, can complement and influence the changes pending in the transportation 
profession. Visualization has far­reaching potential for . Communicating infrastructure needs to leaders 
who must prioritize budgets, . Enhancing the ability of transportation orga­nizations to deliver timely 
and ever more complex programs within those budgets, and . Educating the engineers who will make it 
all happen.  References 1. The Engineer of 2020: Visions of Engineering in the New Century. National 
Academy of Engineering, Washington, D.C., 2004. 2. Educating the Engineer of 2020: Adapting Engineering 
Education to the New Century, National Academy of Engineering, Wash­ington, D.C., 2004. 3. Messner, 
J. I., D. R. Riley, and M. J. Horman. An Interactive Visualization Environment for Construction Engineering 
Edu­cation. Presented at Construction Research Congress, Ameri­can Society of Civil Engineers, 2005. 
 4. Coller, B. Implementing a Video Game to Teach Principles of Mechanical Engineering. Presented at 
American Society for Engineering Education Annual Conference, 2007. 5. Roberts, D. F., U. G. Foehr, 
and V. Rideout. Generation M: Media in the Lives of 8 18 Year­Olds. Technical Report, Kaiser Family Foundation, 
March 2005.  Additional Resources Haque, M. E. Web­Based Visualization Techniques for Structural Design 
Education. Presented at the American Society of Engi­neering Education Annual Conference and Exposition, 
2001. Hixon, C. L., III. NCHRP Synthesis 361: Visualization for Project Development. Transportation Research 
Board of the National Academies, Washington, D.C., 2006. Manore, M. A. Enabling a More Visual and Immersive 
Engineering Environment for the Highway Transportation Industry. Presented at IMAGE 2005 Conference, 
Scottsdale, Arizona, July 2005. Manore, M. A. It s About Decisions: Improving Transportation Project 
Development with Visualization Technologies. TR News, No. 248, January February 2007. Schwartz, E. M. 
An Exploration of Collaborative 3­D Visualization Processing in Engineering Education and Supporting 
Tools. The­sis. University of Misouri Rolla, 2006. Visions 2020: Transforming Education and Training 
Through Advanced Technologies. Office of Public Affairs, U.S. Depart­ment of Commerce Technology Administration, 
September 2002. The Visual­Spatial Learner: An Introduction. The Gifted Develop­ment Center. www.gifteddevelopment.com/Visual_Spatial_ 
Learner/vsl.htm. Wen, H., and K. Gramoll. Online 3­D Collaboration System for Engineering Education. 
Presented at American Society of Engi­neering Education Annual Conference, 2007.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401258</article_id>
		<sort_key>1260</sort_key>
		<display_label>Article No.</display_label>
		<pages>5</pages>
		<display_no>96</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Research agenda for visualization in transportation]]></title>
		<page_from>1</page_from>
		<page_to>5</page_to>
		<doi_number>10.1145/1401132.1401258</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401258</url>
		<abstract>
			<par><![CDATA[<p>Incorporating new legislative directives for planning</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405</concept_id>
				<concept_desc>CCS->Applied computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098924</person_id>
				<author_profile_id><![CDATA[81332505590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Carolina State University, Raleigh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hughes, R. Research Agenda for the Application of Visualization to Transportation Systems. <i>In Transportation Research Record: Journal of the Transportation Research Board, No. 1937</i>, Transportation Research Board of the National Academies, Washington, D.C., 2005, pp. 145--151.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hixon, <i>C. NCHRP Synthesis 361: Visualization for Project Development</i>. Transportation Research Board of the National Academies, Washington, D.C., 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>326485</ref_obj_id>
				<ref_obj_pid>326460</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hibbard, B. Top Ten Visualization Problems. In <i>Proceedings of ACM SIGGRAPH</i>, Vol. 33, No. 2, ACM Press, 1999, pp. 21--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1018051</ref_obj_id>
				<ref_obj_pid>1018014</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Johnson, C. R. Top Scientific Visualization Research Problems. In <i>IEEE Computer Graphics and Visualization: Visualization Viewpoints</i>, July-August 2004, pp. 2--6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Hughes, R. Visualization in Transportation: Current Practice and Future Directions. <i>In Transportation Research Record: Journal of the Transportation Research Board, No. 1899</i>, Transportation Research Board of the National Academies, Washington, D.C., 2004, pp. 167--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The author is Program Director, Institute for Transportation Research and Education, North Carolina 
State University, Raleigh. Research Agenda for Visualization in Transportation Incorporating New Legislative 
Directives for Planning R. G. HUGHES T he capabilities of computational systems have grown rapidly, fueled 
by continuous development in the microprocessor and computer graphics industries. These advances have 
enabled levels of computer image gen­eration that were unimaginable by those involved in the defense 
applications of visual simulation in the 1980s. Although early monochrome, mainframe­based sys­tems lacked 
the resolution and scene content of today s microprocessor­based systems, they generated imagery in real 
time, usually for training simulator applica­tions a true computational achievement. Today the computergraphics 
industry has achieved an abundance of visual fidelity, vivid color, and realistic scene content, and 
yet at least in the area of transportation visualiza­tion only seems to be discovering real­time image 
generation, as distinct from animation. A key issue for transportation visualizations is the  PHOTO: 
URS CORPORATION value of real­time images. Does the ability to move freely within an environment have 
more value than experiencing the constrained path of an animation? Is it more important to permit unconstrained, 
real­time movement through a database or to be able to make changes to that database extemporaneously 
or on the fly, to enable a stakeholder and a designer to investi­gate alternative designs collaboratively 
in real time? Heritage and Distinctions TR NEWS 252 SEPTEMBER OCTOBER 2007 Wire­frame overlay showing 
visuals for ROC 52 (U.S. Highway 52) Computer­aided design (CAD) has a history that is equally long.1 
Early computer image­generation data­ reconstruction design­build project, near Rochester, Minnesota. 
base systems were, in large part, extensions of two­ 1 http://mbinfo.mbdesign.net/CAD­History.htm.  
dimensional (2­D) design capabilities inherent in CAD systems. Current visualization capabilities owe 
much to CAD but are distinguished more by image process­ing than by constructing objects in a database. 
The task of visualization often is less about the polygonal structure of the objects and more about the 
ability to render the faces or surfaces defined by the underlying polygonal network of lines. The realism 
associated with modern visualization is more closely linked to what we see than to how the underlying 
model was created. Color and texture are taken for granted today, but as recently as 30 years ago, every 
engineering design drawing was generated manually with pen and ink. Real time was the time required to 
produce an image TR NEWS 252 SEPTEMBER OCTOBER 2007 manually that is, to draw it. Today, real time refers 
to the computer s ability to redraw an image 30 to 60 times in a second. Defining the Agenda In 2005, 
a paper in the Transportation Research Record: Journal of the Transportation Research Board contained 
a preliminary research agenda for the application of visualization in transportation (1). The agenda 
repre­sented the collective input of members of the TRB Task Force on Visualization in Transportation. 
The research needs reflected members areas of expertise, which ranged from transportation to military 
and defense to aerospace. The agenda subsequently appeared in NCHRP Synthesis 361, Visualization for 
Project Development (2). The 2005 inventory of research needs addressed 17 issues grouped into four categories: 
1. Foundations for applied research, 2. Management­oriented and institutional issues, 3. Integration 
of modeling and simulation, and 4. Social­psychological and cognitive elements.  These research needs 
were reviewed again at the 2006 TRB Visualization in Transportation Symposium and Workshop.2 Surveying 
Practitioners Afterthe symposium, attendees completed a structured, online survey focusing on perceptions 
of the research needs from the agenda. Attendees ranked establishing foundations for applied research 
as the highest priority. Second wasresearch addressing the integration of mod­eling and simulation; third 
was the need for research addressing management issues; last was research addressing the cognitive elements 
of visualization. In establishing a foundation for research, high importance was attached to defining 
the real and per­ceived value of visualization. Also of high importance was research that would aid organizations 
with both the technical and organizational tasks related to start­up and for research products that would 
provide guid­ance to practitioners. In the integration of modeling and simulation, high importance was 
given to research addressing the visu­alization of system operation instead of system appear­ance. This 
reflects general advances in database modeling and the need to enable users to visualize how the system 
works for example, to visualize traf­fic generated from an underlying model. The need to establish a 
research­based foundation 2 www.teachamerica.com/VIZ/23d_Hughes/index.htm. Interactive proceedings are 
online at www.teachamerica.com/viz/viz2006.html. for the application of visualization in transportation 
has parallels in the field of scientific visualization. Using a phrase from an earlier article by Hibbard 
(3), one editorialist recommended that researchers should engage foundational problem[s] in the field 
and continued: As da Vinci understood the need for practitioners to study their own practices, whether 
the art of sci­ence or the science of art, so too did he compre­hend the need to theorize those practices 
 to understand them and hence to strengthen them: He who loves practice without theory is like the sailor 
who boards ship without a rudder and compass and never knows where he may cast. (4) Entrance of Planning 
The 2006 Symposium and Workshop differed from previous meetings by attracting almost equal partici­pation 
from the planning side of the transportation community as from the engineering and design side. Planning 
participants represented a range of organiza­tions for example, the Federal Highway Adminis­tration (FHWA), 
the Federal Transit Administration, metropolitan planning organizations (MPOs), and consulting firms. 
Past symposia had focused almost exclusively on the engineering and design compo­nents of project development. 
The 2006 symposium also formally recognized the relationship between the notion of visualization within 
the transportation community and developments in information visualization and scientific visualization, 
as well as with the evolving area of visual analytics.3 This expansion of interest was timely, because 
the new transportation system legislation, the Safe, Accountable, Flexible, Efficient Transportation 
Equity Act: A Legacy for Users (SAFETEA­LU), requires the use of visualization techniques in MPO participation 
plans. The legislation also calls for accessible public meetings and for information and by implication, 
visualizations to be available via the Internet. Visu­alization would be an integral tool in addressing 
sce­nario planning, community impact analyses, links between planning and the National Environmental 
Policy Act (NEPA) process, green infrastructure plan­ning, transportation and land use integration, con­text­sensitive 
solutions, and many other approaches described in the legislation. Effects on the Agenda How then does 
this expanded notion of the role of visu­alization affect the research agenda? Does the 3 See the presentation 
by M.­T. Rhyne, www.teachamerica.com/ VIZ/02g_Rhyne/index.htm. SAFETEA­LU focus on visualization in planning 
sug­gest new and different research needs, or canthe agenda accommodate the increasedscopeofvisualizationappli­cations? 
What needs to be changed or added? Although the agenda was developed without sig­nificant consideration 
of planning applications, much is still relevant. The following needs still apply: . Guidance for practitioners 
in some cases, using engineering and design tools, and in other cases, tools unique to the planning domain; 
 . Data on cost and effectiveness going beyond the cost of equipment and personnel to include data on 
the labor intensiveness of various applications and data requirements for each visualization appli­cation; 
and . Measures that reflect the multidimensional nature of effectiveness.   Planning Requirements 
Research on the planning applications of visualization will focus more on the human and environmental 
con­text of a project. FHWA s focus on context­sensitive solutions is appropriate the human context must 
be sensitive to stakeholder needs. Visualization can link stakeholder needs, which often may be ambiguous, 
to project design alternatives before the alternatives take form. How does one visualize for stakeholders 
such concepts as urban sprawl, walkable communities, con­nectivity, the economic impacts of blight, or 
the effects of noise? The visualization of physical structures cannot vio­late the rules of constructability; 
similarly, visualization of the complex underlying relationships that mediate the concerns addressed 
in the NEPA process cannot violate basic environmental principles. How some­thing looks, whether addressed 
as an engineering or a planning need, is no longer sufficient. The stakeholder wants to see how it works 
and to be convinced that the underlying engineering and science responsible for the visualization are 
correct. Planning often must deal with a level of vagueness and ambiguity not present in the consideration 
of alternative engineering designs. Yet the processes and rules of planning are nonetheless rigorous. 
The effec­tive use of visualization in a planning decision support system will require the development 
of ways to convey difficult concepts visually, along with the variables that affect them. Geographic 
information systems (GIS) are not the complete answer, nor is the evolution of 3­D within the GIS field. 
This raises another research need: how to influ­ence the requirements process that results in new visualization 
tools and methodologies for use by planners or project engineers. Within TRB, clearly NCHRP Synthesis 
361: Visualization for Project Development includes the 2005 research agenda developed by the TRB Task 
Force on Visualization in Transportation, as well as several case studies and a glossary of terms. To 
order, contact the TRB Bookstore at www. TRB.org/bookstore/, or view the book online at http://onlinepubs. 
TRB.org/onlinepubs/ nchrp/nchrp_syn_361.pdf. TR NEWS 252 SEPTEMBER OCTOBER 2007 The Virtual L.A. project 
of the Urban Simulation Team, University of California, Los Angeles (www.ust.ucla.edu/ ustweb/projects.html), 
incorporates design, planning, and public information, and includes transportation projects, such as 
a proposed metro station for Chinatown featuring traditional Chinese and international design motifs 
reflecting the neighborhood s history.  written, well­substantiated, statements of research need should 
be developed. Environmental Justice The planning focus of visualization introduces the concerns of environmental 
justice. According to FHWA, environmental justice supports the following goals: . To avoid, minimize, 
or mitigate disproportion­ately high and adverse human health and environ­mental effects, including social 
and economic effects, on minority populations and low­income popula­tions; and . To ensure the full 
and fair participation by all potentially affected communities in the transporta­tion decision­making 
process.4  Is it possible that socioeconomic status can have a bearing on the way in which visualization 
tools are used in the planning process? Everyone does not see the same thing when presented with the 
same image. Not all stakeholders have the same ability to process complex spatial information.5 The images 
presented in the course of public involvement are viewed, processed, and understood within a context 
of indi­vidual cognitive abilities and experiences. Despite every effort to convey the value and benefit 
of a proj­ect clearly, the stakeholder still may hear something completely different. Visualization, 
carefully used, can benefit commu­nication with a diverse stakeholder population in 4 www.fhwa.dot.gov/environment/ej2000.htm. 
5 For an overview of the problem in the field of geoscience, go to www.ldeo.columbia.edu/edu/DLESE/maptutorial/ 
introduction.html. which socioeconomic factors play a major role. Envi­ronmental justice introduces a 
broader range of con­siderations than those typically dealt with in current applications of visualization 
for project design. Environmental justice is more clearly aligned with planning than with engineering. 
Yet many of the fac­tors involved such as demographics and socioeco­nomic levels are traditionally represented 
by GIS mapping techniques.6 The common ground is grow­ing between GIS and more traditional visualization. 
The role of visualization in environmental justice is an area with a great need for research­based guidance 
for transportation planners and developers. Visualizing Research If a body of visualization research 
could be visual­ized the issues, the products, and the methodolo­gies what would that image convey? According 
to the survey results from attendees at the 2006 Visual­ization Symposium and Workshop and to input from 
members of the TRB Visualization Committee, the image would depict needs for research at many levels 
within engineering and planning applications: . The collection and synthesis of case studies and lessons 
learned from the systematic application of visualization methods and technologies to high­profile transportation 
system projects, producing practical guidance for practitioners; . The multidimensional measurement 
of effec­tiveness in terms of (a) project and program develop­ment; (b) public involvement and communication; 
and (c) organizational goals, particularly addressing costs, both in terms of workforce including training 
and time as well as equipment; this research should be integrated into the visualization support for 
high­profile projects and should capture labor and equip­ment costs; . The behavioral, psychological, 
and marketing factors that mediate the application of visualization methods and their observed outcomes; 
and . The identification of functional requirements for future capabilities and tools and the communi­cation 
of those requirements to developers of new system capabilities.  Products of basic and applied research 
in these areas should provide practitioners, project and pro­ 6 See J. Aguilar and J. Haracz, Environmental 
Justice: Visualization and Analyses with GIS to Facilitate Informed Decisions, http://gis2.esri.com/library/userconf/proc01/ 
professional/papers/ pap523/p523.htm; also J. Mennis, Using GIS, Spatial Statistics, and Visualization 
to Investigate Environmental Justice, http://astro.temple.edu/ ~jmennis/pubs/mennis_asaej99.doc . TR 
NEWS 252 SEPTEMBER OCTOBER 2007 gram engineers, and senior transportation system offi­cials with insight 
into . The goals to be achieved on a project; . The value of visualization in achieving the goals; . 
The visualization methods and techniques indicated by the goals; . The project costs associated with 
the methods and techniques; . The influences on the effectiveness of the appli­cations; and . The measures 
for evaluating the effectiveness of the visualization.  Because of the SAFETEA­LU focus on incorporat­ing 
visualization into the transportation system plan­ning process, attention should be on planning instead 
of on the already demonstrated engineering and design applications. Aid to Understanding The core of 
the research agenda for visualization in transportation remains relevant, with major additions or increased 
focus in the area of planning applications to come. Visualization research must recognize the dynamic, 
interactive, and collaborative nature of the planning process. Visualizations already have enabled stakeholders 
usually in a public involvement set­ting to review design alternatives, but applied research into planning 
applications will need to focus on applications that can clarify user requirements in a predesign setting 
and that can facilitate the translation of those requirements into scalable dimensions and effective 
designs. GIS plays a major role at this stage but often is more relevant to the planner than to the stakeholder. 
GIS capabilities therefore must be incorporated into new and evolving drawing tools that can rapidly 
gen­erate and modify the visual approximations of a result. The resolution and detail of the image are 
not para­mount, but the extent to which the sketch embodies the stakeholder­defined needs. Several presentations 
at the 2006 Visualization Symposium and Workshop indicated that such efforts already are under way. Planning 
tools must enable planners to use visu­alization extemporaneously and collaboratively as an aid in developing 
stakeholder awareness of the relationship between stakeholder­defined needs and the attributes of alternative 
design solutions, includ­ing operational trade­offs and system costs. The notion of environmental justice 
suggests that these tools blending visualization and GIS must take into account socioeconomic differences 
in stakeholder communities. IMAGES: HNTB Not everyone can read maps, interpret data pre­sented in charts 
and graphs, and comprehend with equal facility complex 3­D and 4­D presentations. The typical stakeholder 
may not understand the underly­ing structure of the rules that provide the basis for the models and simulations. 
Transparency should not mean simply being able to access these underlying rules, but the ability to grasp 
readily the nature of the complex relationships they define. Visualization first and foremost must serve 
as an aid to understanding, making relationships intuitively apparent, but not to increase complexity 
or confu­sion. Research therefore should focus on the informa­tion value of visualization, not on the 
technology. A visualization is a means to an end and not an end in itself. References 1. Hughes, R. Research 
Agenda for the Application of Visual­ization to Transportation Systems. In Transportation Research Record: 
Journal of the Transportation Research Board, No. 1937, Transportation Research Board of the National 
Academies, Washington, D.C., 2005, pp. 145 151. 2. Hixon, C. NCHRP Synthesis 361: Visualization for 
Project Development. Transportation Research Board of the National Academies, Washington, D.C., 2006. 
 3. Hibbard, B. Top Ten Visualization Problems. In Proceedings of ACM SIGGRAPH, Vol. 33, No. 2, ACM Press, 
1999, pp. 21 22. 4. Johnson, C. R. Top Scientific Visualization Research Prob­lems. In IEEE Computer 
Graphics and Visualization: Visual­ization Viewpoints, July August 2004, pp. 2 6.   Additional Resource 
Hughes, R. Visualization in Transportation: Current Practice and Future Directions. In Transportation 
Research Record: Jour­nal of the Transportation Research Board, No. 1899, Trans­portation Research Board 
of the National Academies, Washington, D.C., 2004, pp. 167 174. Visualizations showing two of several 
lighting options for the U.S. 90 bridge near Biloxi, Mississippi ribbon floodlights and ribbon jar lights 
with lit piers. TR NEWS 252 SEPTEMBER OCTOBER 2007  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>1401259</article_id>
		<sort_key>1270</sort_key>
		<display_label>Article No.</display_label>
		<pages>4</pages>
		<display_no>97</display_no>
		<article_publication_date>08-11-2008</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Visualization and the larger world of computer graphics]]></title>
		<page_from>1</page_from>
		<page_to>4</page_to>
		<doi_number>10.1145/1401132.1401259</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1401259</url>
		<abstract>
			<par><![CDATA[<p>What's happening out there?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1098925</person_id>
				<author_profile_id><![CDATA[81350590594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theresa-Marie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rhyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Carolina State University, Raleigh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[McCormick, B., T. DeFanti, and M. Brown (eds.) Visualization in Scientific Computing. ACM SIGGRAPH <i>Computer Graphics</i>, Vol. 21, No.6, November 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Eads, B., N. Rouphail, A. May, and F. Hall. Freeway Facility Methodology for the HCM 2000. In <i>Transportation Research Record: Journal of the Transportation Research Board, No. 1710</i>, Transportation Research Board of the National Academies, Washington, D.C., 2000, pp. 171--180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>108883</ref_obj_id>
				<ref_obj_pid>108844</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Robertson, G. G., J. D. Mackinlay, and S. K. Card. Cone Trees: Animated 3-D Visualizations of Hierarchical Information. In <i>Proc., ACM Conference on Human Factors in Computer Systems</i>, ACM Press, 1991, pp. 189--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Gershon, N., and S. Eick. Foreword. In <i>Proc., IEEE Symposium on Information Visualization</i>, IEEE CS Press, 1995, pp. vii-viii.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Thomas, J. J., and K. A. Cook (eds.). <i>Illuminating the Path: The Research and Development Agenda for Visual Analytics</i>. IEEE CS Press, 2005. http://nvac.pnl.govagenda.stm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Johnson, C. R., R. Moorehead, T. Munzner, H. Pfister, P. Rheingans, and T. S. Yoo (eds.). <i>NIH-NSF Visualization Research Challenges Report</i>. IEEE Press, 2006. http://tab.computer.org/vgtc/vrc/index.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1158849</ref_obj_id>
				<ref_obj_pid>1158811</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Rhyne, T.-M., A. MacEachren, and J. Dykes. Guest Editors' Introduction: Exploring Geovisualization. <i>IEEE Computer Graphics and Applications</i>, Vol. 26, No.4, July-August 2006, pp. 20--21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>621620</ref_obj_id>
				<ref_obj_pid>619058</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Rhyne, T.-M. Computer Games' Influence on Scientific and Information Visualization. <i>IEEE Computer</i>, Vol. 33, No.12, December 2000, pp 154--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>326485</ref_obj_id>
				<ref_obj_pid>326460</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hibbard, B. Top Ten Visualization Problems. <i>Computer Graphics</i>, May 1999, pp. 21--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1018051</ref_obj_id>
				<ref_obj_pid>1018014</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Johnson, C. Top Scientific Visualization Research Problems, <i>IEEE Computer Graphics and Applications</i>, Vol. 24, No. 4, July-August 2004, pp. 13--17.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1032327</ref_obj_id>
				<ref_obj_pid>1032274</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hibbard, B. The Top Five Problems That Motivated My Work. <i>IEEE Computer Graphics and Applications</i>, Vol. 24, No. 6, November-December 2004, pp. 9--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1042207</ref_obj_id>
				<ref_obj_pid>1042190</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Eick, S. G. Information Visualization at 10. <i>IEEE Computer Graphics and Applications</i>, Vol. 25, No. 1, January-February 2005, pp. 12--14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1079847</ref_obj_id>
				<ref_obj_pid>1079828</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Chen, C. Top 10 Unsolved Information Visualization Problems. <i>IEEE Computer Graphics and Applications</i>, Vol. 25, No. 4, July-August, 2005, pp. 12--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1034493</ref_obj_id>
				<ref_obj_pid>1032664</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Rhyne, T.-M., W. L. Hibbard, C. Johnson, C. Chen, and S. Eick. Can We Determine the Top Unresolved Problems of Visualization? <i>In IEEE Visualization 2004 Conference Proceedings</i>, IEEE Press, 2004, pp. 563--566.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1137267</ref_obj_id>
				<ref_obj_pid>1137231</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[North, C. Toward Measuring Visualization Insight. <i>IEEE Computer Graphics and Applications</i>, Vol. 26, No. 3, May-June 2006, pp. 6--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>776503</ref_obj_id>
				<ref_obj_pid>776500</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Rhyne, T.-M. Does the Difference Between Information and Scientific Visualization Really Matter? <i>IEEE Computer Graphics and Applications</i>, Vol. 23, No. 3, May-June 2003, pp. 6--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187629</ref_obj_id>
				<ref_obj_pid>1187619</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[van Wijk, J. J. Bridging the Gaps. <i>IEEE Computer Graphics and Applications</i>, Vol. 26, No. 6, November-December 2006, pp. 6--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualization and the Larger World ofComputer Graphics What s Happening Out There? THERESA­MARIE RHYNE 
The author is Director, Center for Visualization and Analytics, Department of Computer Science; and Director, 
Renaissance Computing Institute s Engagement Facility, North Carolina State University, Raleigh. TR 
NEWS 252 SEPTEMBER OCTOBER 2007 V isualization based on computer graphics and interactive techniques 
was formally defined 20 years ago in a landmark report sponsored by the National Science Foun­dation, 
Visualization in Scientific Computing (1). Visualizations frequently involve large displays and stereoscopic 
environments to immerse the viewer in an examination or exploration process. The Internet has facilitated 
collaborations among explorers at dis­tributed and remote sites. Visualization requires computationally 
intense visual thinking. The premier arena for presenting visu­alization research is the Institute of 
Electrical and Elec­tronics Engineers (IEEE) Visualization Conference Week, held annually in October 
since 1990. Subfields of Visualization Ongoing research and publication in visualization now includes 
two defined subfields scientific visualization and information visualization. A third subfield, visual 
analytics, is emerging. Scientific Visualization Scientific visualization produces visual displays of 
spa­tial data associated with scientific processes, such as the bonding of molecules in computational 
chemistry. As noted in the landmark report of 1987, visualization is a method of computing. It transforms 
the symbolic into the geometric, enabling researchers to observe the sim­ulations and computations (1). 
The methods of scientific visualization are well suited to transportation. They can enhance traffic data 
to microsimulate scenarios that support decision mak­ing for example, for evacuations, diversions, and 
rerouting schemes. The figure on this page shows how scientific visualization techniques are applied 
to exam­ine spatial and temporal speed profiles for a freeway bottleneck (2). Information Visualization 
With the evolution of visualization and of technolo­gies for human and computer interaction in the early 
1990s, the subfield of information visualization took shape. The focus was on developing visual metaphors 
for noninherently spatial data. A goal was to facilitate the exploration of text­based document databases. 
In 1991, researchers at Xerox published findings on the information visualizer system, which began to 
artic­ulate a difference between scientific and information visualization (3). The first IEEE Information 
Visualization Sympo­sium was held in conjunction with the IEEE Visual­ization Conference in 1995 (4). 
The symposium con­tinues to occupy the first part of the conference week, with the main Visualization 
Conference sessions later in the week. For the field of transportation, information visualization methods 
are well suited to address com­munity planning scenarios that combine diverse data sets from geographic 
information systems (GIS), visual impact assessments, and transportation analyses. Visual Analytics 
The subfield of visual analytics, defined in 2004 and 2005, is emerging to supply the need for visual 
inter­faces to explore analytical data in response to terror­ist attacks and natural disasters. The landmark 
report, Visual analytics display for collaboration and sharing of visual information by transportation 
researchers and users. Imagery is from a 3­D model of alternative roundabout treatments under consideration 
by National Cooperative Highway Research Program (NCHRP) Project 3­78, Crossing Solutions for Visually 
Impaired Pedestrians at Roundabouts and Channelized Turn Lanes. The imagery was developed in collaboration 
with Thomas Fischer, New York State Department of Transportation (DOT). Several static views from driver 
and pedestrian vantages are available for online review at www.itre.ncsu.edu/ NCHRP378/. Illuminating 
the Path: The Research and Development Agenda for Visual Analytics, published in 2005, defined visual 
analytics as the science of analytical reasoning facilitated by interactive visual interfaces (5). The 
first IEEE Symposium on Visual Analytics Sci­ence and Technology was held during the 2006 IEEE Visualization 
Conference. Still an emerging arena of research, visual analytics methods are being devel­oped to assist 
in emergency responses and rapid evac­uations of transportation arteries in densely populated communities. 
Visual analytics methods also are applicable to transportation planning and design. In 2005 and 2006, 
with the maturing of scientific and information visualization and the emergence of visual analytics, 
the National Institutes of Health and the National Science Foundation sponsored a reex­amination of research 
issues in visualization. The 2006 report, NIH­NSF Visualization Research Chal­lenges, presents the findings 
(6). Examining Geovisualization All three subfields of visualization provide insights and methods applicable 
to transportation planning, implementation, and evaluation. Cartographic and geographic information techniques 
also span all three research subfields. Cartography has produced extensive theory and writings on the 
spatialization of data and informa­tion. GIS consists of efficient repositories of layered spatial data. 
Geovisualization the merger of GIS and visualization technologies embraces specific domains of application, 
particularly in transportation (7). Other Graphics Tools The transportation community has created many 
examples of visualizations with high­end computer graphics tools and methods. Computer­aided design (CAD) 
has become an essential tool for establishing an accurately registered roadway, including the bridges 
and other infrastructure, for transportation engineering projects. GIS also is used for planning, as 
well as for design and construction. Animation techniques allow the production of high­end movies that 
depict how a proposed trans­portation project will be integrated into the commu­nity infrastructure. 
The image on the next page employs photorealistic computer graphics to display a proposed roadway project. 
 Online Resources Online resources and communities have provided new tools for the visual examination 
of data. Google Geovisualization of topographic data from the U.S. Geological Survey s website. (Image 
created by Theresa­Marie Rhyne and Thomas Fowler at the United States Environmental Protection Agency 
s Scientific Visualization Center in the late 1990s.) TR NEWS 252 SEPTEMBER OCTOBER 2007 Transportation 
visualization for the US­15 501 and Erwin Road Europa Drive Intersection Improvement Project, Chapel 
Hill, North Carolina. (Image created by Chris Parker, courtesy of North Carolina DOT. For more information, 
www.ncdot.org/projects/ Superstreet/.) Terrain gallery geovisualizations defined as elements in a 3­D 
virtual world. (The image and 3­D virtual world were created by Theresa­ 22 Marie Rhyne in 2002.) TR 
NEWS 252 SEPTEMBER OCTOBER 2007 Earth and Virtual Earth, for example, are powerful tools for geovisualization.1 
The website images can serve as a base for overlays of visual elements from online databases, to produce 
what are called mashup visualizations.2 The mashup visualizations can be shared and enhanced readily 
via online com­munities associated with Google Earth and Virtual Earth or via social networking websites 
like My Space.3 In addition to text­based social networking sites, three­dimensional (3­D) virtual worlds 
allow for building personal avatars images that serve as per­sonal signatures or identifications on the 
web and for arranging possessions in a defined cyberspace. Second Life is one of the more popular examples 
of an online 3­D virtual community.4 Specialized examples of 3­D virtual spaces have been developed for 
public participation across the web. Virtual London is an application currently under development by 
the University College of London s Centre for Advanced Spatial Analysis, with funding from the Greater 
London Authority and London Con­nects.5 This large­scale 3­D GIS and CAD model of the city uses Google 
Earth, as well as a variety of pho­torealistic imaging and photogrammetric methods. 1 Google Earth, http://earth.google.com; 
Virtual Earth, www.microsoft.com/virtualearth. 2 For an example of a mashup visualization of census data 
in Google Earth, see www.juiceanalytics.com/writing/ 2006/03/census­data­in­google­earth/. 3 www.myspace.com/. 
4 http://secondlife.com.   Serious Games One of the more widely known traffic simulation games enjoyed 
by the general public is SimCity s Rush Hour Expansion Pack.6 The module allows players to control vehicles 
on the streets of their own designed city and to fix transportation problems in their own virtual world. 
The popularity of these computer games has led to a repurposing of the technology to educate and train 
students at all levels. Simulations aimed at examining management and leadership challenges in the public 
sector are known as serious games. 7 Combined with visualizations of specific and tar­geted transportation 
issues, serious games provide an opportunity for an interactive examination of trans­portation scenarios 
(8). These scenarios can assist with key evaluation and decision­making efforts as well as with public 
participation activities. Top Problems in Visualization At the IEEE Visualization 2004 conference, a 
panel of leaders in the field examined future directions in a session, Can We Determine the Top Unresolved 
Problems of Visualization? (9 13) Each panelist pre­pared a list of the top 10 unresolved problems; as 
a panelist, I identified the following: 1. Effectively and accurately simulating Mother Nature and human 
behavior. Attempts to use visual­ization and numerical computational methods to cre­ate virtual scenarios 
are always challenged by real­world solutions or events never before consid­ered or modeled. 2. Usability 
of the visualization system. Can visu­alization tools gain use beyond the computer graph­ics experts 
who developed them? How well do these tools fit into the ongoing work in departments of transportation? 
 3. Evaluating the effectiveness of visualization tools. How can visualization methods be assessed for 
their helpfulness in resolving problems, and how can the methods be modified accordingly? 4. Addressing 
perceptual and cognitive issues. Can the effects of visual displays on viewers be under­stood and applied 
to improve the designs?  5. Supporting multidisciplinary collaborations. Experts from many disciplines 
provide the content for visualizations and contribute to resolving concerns about a visualization s details. 
How can ongoing col­laborations be facilitated? 6. Evolving graphics hardware and platform devel­opment. 
The hardware for producing computer 5 http://casa.ucl.ac.uk. 6 http://simcity.ea.com/about/rushhour/overview.php. 
7 www.seriousgames.org. The computer game SimCity offers a module for Finally, visualization faces many 
key problems. Readers are encouraged to examine the other papers in this issue and to seek other viewpoints 
from the visualization field. References 1. McCormick, B., T. DeFanti, and M. Brown (eds.) Visualiza­tion 
in Scientific Computing. ACM SIGGRAPH Computer Graphics, Vol. 21, No. 6, November 1987. 2. Eads, B., 
N. Rouphail, A. May, and F. Hall. Freeway Facility Methodology for the HCM 2000. In Transportation Research 
Record: Journal of the Transportation Research Board, No. 1710, Transportation Research Board of the 
National Acad­emies, Washington, D.C., 2000, pp. 171 180. 3. Robertson, G. G., J. D. Mackinlay, and 
S. K. Card. Cone  Trees: Animated 3­D Visualizations of Hierarchical Infor­ engineering traffic flows 
and addressing the problems that develop. graphics and visualization is constantly changing. The time 
frame for resolving application problems may be longer than the computer platform s life cycle how can 
users keep up with these changes? 7. Driving forces in the computer games and enter­tainment industries. 
In the arena of computer graph­ics, visualization is secondary in both economic and consumer demand to 
the requirements of the com­puter games and entertainment industries. What can be done to ensure the 
development of effective visu­alization tools? 8. Supporting expert domain knowledge. How can common 
practices and existing expertise be incorpo­rated into visual displays? 9. Facilitating insight. Can 
visualization methods be readily available to support new discoveries and inquiries? 10. Accepting the 
unexpected. Events occur that are beyond control, and situations can evolve to supersede anticipated 
and planned outcomes can this be accommodated? (14)   Expanding Views Visualization involves computationally 
intense visual thinking. Visualization consists of three subfields: . Scientific visualization the visual 
display of spatial data; . Information visualization the visual display of nonspatial data; and . Visual 
analytics analytical reasoning facili­tated by visual interfaces.  Online resources, such as Google 
Earth and Vir­tual Earth, serve as starting points for mashup visual­izations. In addition, serious games 
are repurposing computer game simulations and technologies to edu­ mation. In Proc., ACM Conference on 
Human Factors in Com­puter Systems, ACM Press, 1991, pp. 189 194. 4. Gershon, N., and S. Eick. Foreword. 
In Proc., IEEE Sympo­sium on Information Visualization, IEEE CS Press, 1995, pp. vii viii. 5. Thomas, 
J. J., and K. A. Cook (eds.). Illuminating the Path: The Research and Development Agenda for Visual Analytics. 
IEEE CS Press, 2005. http://nvac.pnl.govagenda.stm. 6. Johnson, C. R., R. Moorehead, T. Munzner, H. Pfister, 
P. Rheingans, and T. S. Yoo (eds.). NIH­NSF Visualization Research Challenges Report. IEEE Press, 2006. 
http://tab.computer.org/ vgtc/vrc/index.html. 7. Rhyne, T.­M., A. MacEachren, and J. Dykes. Guest Editors 
Introduction: Exploring Geovisualization. IEEE Computer Graphics and Applications, Vol. 26, No. 4, July 
August 2006, pp. 20 21. 8. Rhyne, T.­M. Computer Games Influence on Scientific and Information Visualization. 
IEEE Computer, Vol. 33, No.12, December 2000, pp 154 156. 9. Hibbard, B. Top Ten Visualization Problems. 
Computer Graphics, May 1999, pp. 21 22. 10. Johnson, C. Top Scientific Visualization Research Problems, 
IEEE Computer Graphics and Applications, Vol. 24, No. 4, July August 2004, pp. 13 17. 11. Hibbard, B. 
The Top Five Problems That Motivated My Work. IEEE Computer Graphics and Applications, Vol. 24, No. 6, 
November December 2004, pp. 9 13. 12. Eick, S. G. Information Visualization at 10. IEEE Computer Graphics 
and Applications, Vol. 25, No. 1, January February 2005, pp. 12 14. 13. Chen, C. Top 10 Unsolved Information 
Visualization Prob­lems. IEEE Computer Graphics and Applications, Vol. 25, No. 4, July August, 2005, 
pp. 12 16. 14. Rhyne, T.­M., W. L. Hibbard, C. Johnson, C. Chen , and S. Eick. Can We Determine the 
Top Unresolved Problems of Visualization? In IEEE Visualization 2004 Conference Pro­ceedings, IEEE Press, 
2004, pp. 563 566.  Additional Resources North, C. Toward Measuring Visualization Insight. IEEE Com­puter 
Graphics and Applications, Vol. 26, No. 3, May June 2006, pp. 6 9. Rhyne, T.­M. Does the Difference Between 
Information and Sci­entific Visualization Really Matter? IEEE Computer Graph­ics and Applications, Vol. 
23, No. 3, May June 2003, pp. 6 8. van Wijk, J. J. Bridging the Gaps. IEEE Computer Graphics and Applications, 
Vol. 26, No. 6, November December 2006, TR NEWS 252 SEPTEMBER OCTOBER 2007 cate and train professionals. 
pp. 6 9.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
</content>
</proceeding>
