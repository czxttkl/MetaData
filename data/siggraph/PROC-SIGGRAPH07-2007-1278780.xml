<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/05/2007</start_date>
		<end_date>08/09/2007</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[San Diego]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1278780</proc_id>
	<acronym>SIGGRAPH '07</acronym>
	<proc_desc>ACM SIGGRAPH 2007 sketches</proc_desc>
	<conference_number>2007</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2007</copyright_year>
	<publication_date>08-05-2007</publication_date>
	<pages>94</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Short illustrated talks on computer graphics and interactive techniques in art, cinema, advertising, design, science, and engineering. Sketch presenters summarize speculative breakthroughs, work in progress, and recent achievements. Following their presentations, they answer questions and discuss future implications of their work.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>PP40025189</person_id>
			<author_profile_id><![CDATA[81100235480]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Marc]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Alexa]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Technische Universit&#228;t Berlin]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>P10635</person_id>
			<author_profile_id><![CDATA[81100576882]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>2</seq_no>
			<first_name><![CDATA[Adam]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Finkelstein]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Princeton University]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2007</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<section>
		<section_id>1278781</section_id>
		<sort_key>1</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[The viz biz]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>1278782</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Light threads]]></title>
		<subtitle><![CDATA[illustrating movement dynamics in city models]]></subtitle>
		<page_from>1</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278782</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278782</url>
		<abstract>
			<par><![CDATA[<p>Visual representations of traffic flow and density in 3D city models provide substantial decision support in urban planning. While a large repertoire of efficient techniques exists for visualizing the static components of such environments (e.g., digital terrain models, building models, and vegetation), less is known about illustrating their dynamics nature.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14181388</person_id>
				<author_profile_id><![CDATA[81100522687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nienhaus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Potsdam / mental images]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P33441</person_id>
				<author_profile_id><![CDATA[81100057328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gooch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Victoria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047942</person_id>
				<author_profile_id><![CDATA[81100019985]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J&#252;rgen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[D&#246;llner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Potsdam]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D&#246;llner, J., Buchholz, H., Nienhaus, M., and Kirsch, F. 2005. Illustrative Visualization of 3D City Models, <i>Proceedings of VDA 2005</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>200550</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Norman, D. A., 1993. <i>Things That Make Us Smart -- Defending Human Attributes in the Age of the Machine</i>, Basic Books, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278783</article_id>
		<sort_key>2</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Mapping, illuminating, and interacting with science]]></title>
		<page_from>2</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278783</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278783</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14130270</person_id>
				<author_profile_id><![CDATA[81100364969]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Boyack]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sandia National Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P765462</person_id>
				<author_profile_id><![CDATA[81309501639]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Klavans]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SciTech Strategies, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051228</person_id>
				<author_profile_id><![CDATA[81100245710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[W.]]></first_name>
				<middle_name><![CDATA[Bradford]]></middle_name>
				<last_name><![CDATA[Paley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Image Design Incorporated]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39048750</person_id>
				<author_profile_id><![CDATA[81100564290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Katy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[B&#246;rner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Indiana University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B&#246;rner, K., C. Chen, K. W. Boyack (2003). <i>Visualizing Knowledge Domains</i>. Annual Review of Information Science &amp; Technology. B. Cronin. Medford, NJ, Information Today, Inc./ASIST. 37: 179--255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Paley, W. B. (2002). <i>Illuminated Diagrams: Using Light and Print to Comparative Advantage</i>. Accessed on 4-10-2007 at http://tinylink.com/?AtOWI0gYv5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shiffrin, R. M. and K. B&#246;rner, Eds. (2004). <i>Mapping Knowledge Domains</i>, PNAS, 101 (Suppl. 1).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278784</article_id>
		<sort_key>3</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[The regular 4-dimensional 57-cell]]></title>
		<page_from>3</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278784</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278784</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P41115</person_id>
				<author_profile_id><![CDATA[81100058395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlo]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[S&#233;quin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[U.C. Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891721</person_id>
				<author_profile_id><![CDATA[81335491338]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hamlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[U.C. Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Coxeter, H. S. M. 1982. Ten toroids and fifty-seven hemi-dodecahedra. Geometriae Dedicata, 13, pp 87--99.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gr&#252;nbaum, B. 1977. Regularity of Graphs, Complexes and Designs. Colloque Intern. C.N.R.S., Vol. 260, pp 191--197.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[S&#233;quin, C. H. and Lanier, J. 2007. Hyperseeing the Regular Hendecachoron. Proc ISAMA 2007, Texas A&M, pp 159--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lanier, J. 2007. Jaron's World: Shapes in Other Dimensions. Discover Magazine, April 2007, pp 28--29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278785</article_id>
		<sort_key>4</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Multi-user interaction on the DNA workbench]]></title>
		<page_from>4</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278785</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278785</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052808</person_id>
				<author_profile_id><![CDATA[81335496510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Price]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35046928</person_id>
				<author_profile_id><![CDATA[81320488330]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Vita]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berezina-Blackburn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278786</article_id>
		<sort_key>5</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[From DNA to 3D organic art forms]]></title>
		<page_from>5</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278786</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278786</url>
		<abstract>
			<par><![CDATA[<p>We present a novel "biological" approach to define and evolve 3D art forms. The work combines a re-implementation of the <i>Form-Grow</i> system of Todd and Latham [Todd and Latham 1992] with an external source to define the shapes: DNA sequences.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35046411</person_id>
				<author_profile_id><![CDATA[81100545095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Latham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goldsmiths, University of London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P890888</person_id>
				<author_profile_id><![CDATA[81333490921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Miki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goldsmiths, University of London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35046485</person_id>
				<author_profile_id><![CDATA[81100524954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Todd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goldsmiths, University of London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822579</person_id>
				<author_profile_id><![CDATA[81100279987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[Fol]]></middle_name>
				<last_name><![CDATA[Leymarie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Goldsmiths, University of London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891735</person_id>
				<author_profile_id><![CDATA[81332508345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lawrence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kelley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Imperial College, London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891693</person_id>
				<author_profile_id><![CDATA[81321492972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jefferys]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Imperial College, London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kumar, S., and Bentley, P. J. 2003. An introduction to computational development. In <i>On Growth, Form and Computers</i>. Elsevier Academic Press, 1--43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>561831</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Todd, S., and Latham, W. 1992. <i>Evolutionary Art and Computers</i>. Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278787</section_id>
		<sort_key>6</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Taking shape]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>1278788</article_id>
		<sort_key>6</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Surface network construction from non-parallel cross-sections]]></title>
		<page_from>6</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278788</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278788</url>
		<abstract>
			<par><![CDATA[<p>Surface construction from incomplete data is a topic of wide interest in geometry processing. A common class of incomplete inputs that arises in several application domains, including bio-medicine and geology, is a stack of curves representing the planar cross-sections of a complete surface. In medical imaging, for example, these curves are typically hand-drawn by physicians to delineate contours of anatomical structures on 2D cross-sections of a 3D image volume generated by MRI, CT or ultrasound. Given a stack of planar cross-sections, a complete 3D surface is desired that connects the curves on each cross-section plane. In particular, such surface needs to be both <i>topologically</i> correct (i.e., closed and interpolating input curves) and <i>geometrically</i> smooth.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39088057</person_id>
				<author_profile_id><![CDATA[81335493704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050392</person_id>
				<author_profile_id><![CDATA[81100098726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891708</person_id>
				<author_profile_id><![CDATA[81335494246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Low]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14117613</person_id>
				<author_profile_id><![CDATA[81100323675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chandrajit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bajaj]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Texas at Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>644129</ref_obj_id>
				<ref_obj_pid>644108</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barequet, G., Goodrich, M. T., Levi-Steiner, A., and Steiner, D. 2003. Straight-skeleton based contour interpolation. In <i>SODA '03: Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms</i>, 119--127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ju, T., Warren, J. D., Carson, J., Eichele, G., Thaller, C., Chiu, W., Bello, M., and Kakadiaris, I. A. 2005. Building 3d surface networks from 2d curve networks with application to anatomical modeling. <i>The Visual Computer 21</i>, 8--10, 764--773.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278789</article_id>
		<sort_key>7</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Realtime constructive solid geometry]]></title>
		<page_from>7</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278789</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278789</url>
		<abstract>
			<par><![CDATA[<p>Constructive solid geometry (CSG) is a modeling technique where objects are combined using boolean operations to build up complex shapes. Fig. 1 shows the results of union, intersection and subtraction between the Stanford bunny and a toroidal knot. While many CAD systems support CSG, it remains conspicuously absent from interactive environments. Part of the problem is that current techniques are slow, unstable and complicated. This algorithm addresses these issues by improving on Naylor et.al.'s tree merging algorithm [Naylor et al. 1990].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050398</person_id>
				<author_profile_id><![CDATA[81335494284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mikola]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lysenko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Michigan Technological University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>97892</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Naylor, B., Amanatides, J., and Thibault, W. 1990. Merging bsp trees yields polyhedral set operations. In <i>SIGGRAPH '90: Proceedings of the 17th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 115--124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>98570</ref_obj_id>
				<ref_obj_pid>98524</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Seidel, R. 1990. Linear programming and convex hulls made easy. In <i>SCG '90: Proceedings of the sixth annual symposium on Computational geometry</i>, ACM Press, New York, NY, USA, 211--215.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278790</article_id>
		<sort_key>8</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Volume-controlled surface fairing]]></title>
		<page_from>8</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278790</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278790</url>
		<abstract>
			<par><![CDATA[<p>Surface fairing is a central geometry processing tool, routinely used for denoising and smoothing applications---see [Botsch et al. 2006], Chap. 7 for an overview of the extensive literature on the subject. A notorious shortcoming of basic fairing schemes is volume shrinkage. Existing remedies often suffer from sensitivity to choice of parameters [Taubin 1995], are subject to unnatural global effects [Hermosillo et al. 1999], or resort to higher-order, computationally intensive surface evolutions [Bobenko and Schr&#246;der 2005]. Instead, we propose a simple, efficient and unconditionally stable smoothing scheme that implements a <i>locally</i> volume-controlled flow, with near preservation of volume. Our approach directly modifies the implicit fairing scheme [Desbrun et al. 1999], but it can be used to build volume control into any other surface flow.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891719</person_id>
				<author_profile_id><![CDATA[81315488832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[I.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eckstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053957</person_id>
				<author_profile_id><![CDATA[81100393143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Y.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39058243</person_id>
				<author_profile_id><![CDATA[81384616778]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[C.-C.]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Kuo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35043115</person_id>
				<author_profile_id><![CDATA[81100041821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Desbrun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Caltech]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1281937</ref_obj_id>
				<ref_obj_pid>1281920</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bobenko, A. I., and Schr&#246;der, P. 2005. Discrete Willmore Flow. In <i>Third Eurographics Symposium on Geometry Processing</i>, 101--110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1185839</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Botsch, M., Pauly, M., Rossl, C., Bischoff, S., and Kobbelt, L. 2006. Geometric modeling based on triangle meshes. <i>International Conference on Computer Graphics and Interactive Techniques</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311576</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Desbrun, M., Meyer, M., Schr&#246;der, P., and Barr, A. 1999. Implicit Fairing of Irregular Meshes using Diffusion and Curvature Flow. <i>ACM SIGGRAPH</i>, 317--324.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Hermosillo, G., Faugeras, O., and Gomes, J. 1999. Unfolding the Cerebral Cortex Using Level Set Methods. <i>Int. Conf. on Scale-Space Theories in Comp. Vision 8</i>, 12--28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218473</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Taubin, G. 1995. A Signal Processing Approach to Fair Surface Design. In <i>Proceedings of ACM SIGGRAPH</i>, 351--358.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278791</article_id>
		<sort_key>9</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Scalable freeform deformation]]></title>
		<page_from>9</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278791</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278791</url>
		<abstract>
			<par><![CDATA[<p>Freeform deformation techniques are powerful and flexible tools for interactive 3D shape editing. However, while interactivity is the key constraint for the usability of such tools, it cannot be maintained when the complexity of either the 3D model or the applied deformation exceeds a given workstation-dependent threshold. In this work, we solve this scalability problem by introducing a streaming system based on a sampling-reconstruction approach. First a fast <i>out-of-core adaptive simplification</i> algorithm is performed in a pre-processing step, for quick generation, of a simplified version of the model. The resulting model can then be submitted to arbitrary FFD tools, as its reduced size ensures interactive response. Second, a post-processing step performs a <i>feature-preserving deformation reconstruction</i> that applies to the original model the deformation undergone by its simplified version. Both bracketing steps share a <i>streaming</i> and <i>point-based</i> basis, making them fully scalable and compatible both with point-clouds and non-manifold meshes. Our system also offers a generic out-of-core multi-scale layer to FFD tools, since the two bracketing steps remain available for partial up-sampling during the interactive session. Arbitrarily large 3D models can thus be interactively edited with most FFD tools, opening the use of advanced deformation metaphors to models ranging from million to billion samples. Our system also enables offers to work on models that fit in memory but exceed the capabilities of a given FFD tool.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35033095</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bordeaux]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022986</person_id>
				<author_profile_id><![CDATA[81100036540]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Olga]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sorkine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109976</person_id>
				<author_profile_id><![CDATA[81100297901]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schlick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bordeaux]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Boubekeur, T., Heidrich, W., Granier, X., and Schlick, C. 2006. Volume-surface trees. <i>Computer Graphics Forum v25</i>, n3, p399--406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Heidrich, W. 2005. Computing the barycentric coordinates of a projected point. <i>Journal of Graphics Tools v10</i>, n3, p9--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141992</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Isenburg, M., Liu, Y., Shewchuk, J., and Snoeyink, J. 2006. Streaming computation of delaunay triangulations. <i>ACM Trans. Graph. v25</i>, n3, p1049--1056.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278792</article_id>
		<sort_key>10</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Nearly rigid deformation by linear optimization]]></title>
		<page_from>10</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278792</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278792</url>
		<abstract>
			<par><![CDATA[<p>Fast near-rigid 3D deformation of a shape is achieved by approximating a rigid transformation of each point by a moving least-squares (MLS) optimization with a linear closed-form solution. We can deform tens of thousands points in real time while preserving the rigidity of the original shape as far as possible. This technique inherits the properties of earlier MLS-based deformation techniques, such as independence of geometric representation and smoothness. The user can control the deformations by using control points or line segments and by the adjustment of weight functions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P899070</person_id>
				<author_profile_id><![CDATA[81336490211]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[SoHyeon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jeong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141920</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Schaefer, S., McPhail, T., and Warren, J. 2006. Image Deformation using Moving Least Squares. <i>ACM Transaction on Graphics</i>, 25, 3, 533--540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278793</section_id>
		<sort_key>11</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Is this for real?]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>1278794</article_id>
		<sort_key>11</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Seamless tangible interaction through selective stylization]]></title>
		<page_from>11</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278794</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278794</url>
		<abstract>
			<par><![CDATA[<p>We present a new style of tangible interaction, which provides an improved user experience by seamlessly combining real objects and graphical models. In the tangible user interaction zone of our system, physical objects and virtual models are displayed in the same technical illustration style. Regions outside of the interaction zone, and also the user's hands, are shown unaltered in order to maintain an unmodified visual feedback in these areas. Our example application is a tangible urban planning environment, in which the placement of both real and virtual building models affects the flow of wind and the casting of shadows.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP35049173</person_id>
				<author_profile_id><![CDATA[81341490168]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Victoria, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891707</person_id>
				<author_profile_id><![CDATA[81335490391]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Flohr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39064021</person_id>
				<author_profile_id><![CDATA[81100575262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stra&#223;er]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1079773</ref_obj_id>
				<ref_obj_pid>1078037</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fischer, J., Bartz, D., and Strasser, W. 2005. Stylized Augmented Reality for Improved Immersion. In <i>Proc. of IEEE Virtual Reality</i>, 195--202.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., and Ullmer, B. 1997. Tangible Bits: Towards Seamless Interfaces Between People, Bits and Atoms. In <i>Proc. of the SIGCHI Conference on Human Factors in Computing Systems</i>, 234--241.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278795</article_id>
		<sort_key>12</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Illumination sensitive dynamic virtual sets]]></title>
		<page_from>12</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278795</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278795</url>
		<abstract>
			<par><![CDATA[<p>We describe a novel motion capture method that is sensitive to incident illumination. We will demonstrate the corresponding system at Emerging Technologies at Siggraph 2007. In this sketch, we focus on the key technical and implementation contributions involved in building a real-time special effects system where a prop is turned into a virtual sword with appropriate orientation and incident illumination.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891694</person_id>
				<author_profile_id><![CDATA[81335489591]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[De Decker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40030372</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P779713</person_id>
				<author_profile_id><![CDATA[81311484645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P890264</person_id>
				<author_profile_id><![CDATA[81335494767]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dylan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40030391</person_id>
				<author_profile_id><![CDATA[81340493437]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Summet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40038192</person_id>
				<author_profile_id><![CDATA[81335500473]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822704</person_id>
				<author_profile_id><![CDATA[81319504113]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Westhues]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P221214</person_id>
				<author_profile_id><![CDATA[81100246613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dietz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P812296</person_id>
				<author_profile_id><![CDATA[81318497879]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barnwell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40027105</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40037593</person_id>
				<author_profile_id><![CDATA[81100093388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bekaert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022814</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Robertson, B. 2006. Big moves. In <i>Computer Graphics World 29, 11 (Nov)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278796</article_id>
		<sort_key>13</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[The "Castelet"]]></title>
		<subtitle><![CDATA[a dynamically reconfigurable stage for performing arts]]></subtitle>
		<page_from>13</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278796</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278796</url>
		<abstract>
			<par><![CDATA[<p>The hero jumped down the wall, escalated the hill, slid down the rock, climbed the stairs and finally left the vilain behind...In this classical scene, the hero has to move on different types of terrain. Recreating such a varied spectrum of terrains in a theater scene still remains a challenge.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891724</person_id>
				<author_profile_id><![CDATA[81335492557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jean-Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jobin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robotics Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P779155</person_id>
				<author_profile_id><![CDATA[81311480911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Comtois]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Vision and Systems Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891704</person_id>
				<author_profile_id><![CDATA[81100269352]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Cl&#233;ment]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gosselin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robotics Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891750</person_id>
				<author_profile_id><![CDATA[81335490157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faguy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LANTISS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14082131</person_id>
				<author_profile_id><![CDATA[81100209007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laurendeau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Vision and Systems Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383314</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Iwata, H., Yano, H., Nakaizumi, F., Kawamura, 2001, Project FEELEX: Adding Haptic Surface to Graphics, In <i>Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH</i>, New York, Emerging Technologies, pp. 469--475]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Clark, P. E., Rilee, M. L., Curtis, S. A., Cheung, C. Y., Truszkowski, W., Marr, G., Rudisill, M., 2004, "PAM: Biologically Inspired Engineering and Exploration Mission Concept, Components, and Requirements for Asteroid Population Survey, <i>Proc. Intl Aeronautical Congress</i>, Vancouver, IAC Proc, #IAC-04-IAA.3.8.1.08]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278797</article_id>
		<sort_key>14</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Implementing wave particles for real-time water waves with object interaction]]></title>
		<page_from>14</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278797</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278797</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35034261</person_id>
				<author_profile_id><![CDATA[81319505055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cem]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yuksel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044760</person_id>
				<author_profile_id><![CDATA[81100479080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[House]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39087974</person_id>
				<author_profile_id><![CDATA[81100431503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keyser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278798</article_id>
		<sort_key>15</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Teleshadow]]></title>
		<subtitle><![CDATA[feeling presence in private spaces]]></subtitle>
		<page_from>15</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278798</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278798</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891759</person_id>
				<author_profile_id><![CDATA[81421595008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shunpei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yasuda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311019500</person_id>
				<author_profile_id><![CDATA[81542800456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311019600</person_id>
				<author_profile_id><![CDATA[81538262756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mariko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koizumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034175</person_id>
				<author_profile_id><![CDATA[81319498464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okude]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Simpson, B, Z. 2002. ShadowGarden. In Proceedings of SIGGRAPH 2002 ArtGallery.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>108932</ref_obj_id>
				<ref_obj_pid>108844</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[John, C, T., and Scott, L, Minneman. 1991. Video Whiteboard: Video Shadows to Support Remote Collaboration. In Proceedings of the SIGCHI]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1031685</ref_obj_id>
				<ref_obj_pid>1031607</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yoshiyuki, M., and Chikara, I. 2004. Shadow Communication: System for Embodied Interaction with Remote Partners. In Proceedings of CSCW 2004]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278799</section_id>
		<sort_key>16</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Let's get physical]]></section_title>
		<section_page_from>16</section_page_from>
	<article_rec>
		<article_id>1278800</article_id>
		<sort_key>16</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Contact trees]]></title>
		<subtitle><![CDATA[adaptive contact sampling for robust dynamics]]></subtitle>
		<page_from>16</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278800</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278800</url>
		<abstract>
			<par><![CDATA[<p>Algorithms for rigid body dynamics with contact are well known, but challenging to implement due to the interplay between large time steps, general purpose collision detection packages and pragmatic approximations of the underlying inequality constrained contact problems. While research on rigid body simulation has focused heavily both on contact resolution and collision detection, contact generation has essentially been ignored. Most contact resolution algorithms presume that an ideal set of contacts, fully characterizing system constraints, are available, while collision detection methods generally presume that their task is finished once a set of intersecting primitives has been identified. Bridging the gap between these domains, by generating representative contact samples, contact point locations and their associated normals, is crucial for the accuracy, robustness and speed of simulation. This is highlighted by two observations:</p> <p>&#8226; A contact resolution method, no matter how robust and accurate, is restricted by the quality of constraints it has been provided with.</p> <p>&#8226; Narrow phase collision detection algorithms, although in some senses highly optimized, often perform <i>too much</i> and the <i>wrong kind</i> of work because they do not consider the form of input desired by the physical solver.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35049466</person_id>
				<author_profile_id><![CDATA[81100224608]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Danny]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Kaufman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia and Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052663</person_id>
				<author_profile_id><![CDATA[81335498230]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shinjiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P67322</person_id>
				<author_profile_id><![CDATA[81100642559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Pai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia and Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1053681</ref_obj_id>
				<ref_obj_pid>1053554</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baerentzen, J. A. 2005. Signed distance computation using the angle weighted pseudonormal. <i>IEEE Transactions on Visualization and Computer Graphics 11</i>, 3, 243--253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>274989</ref_obj_id>
				<ref_obj_pid>274976</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barzel, R., Hughes, J. F., and Wood, D. N. 1996. Plausible motion simulation for computer graphics animation. In <i>Computer Animation and Simulation 96</i>, 183--197.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344899</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Frisken, S. F., Perry, R. N., Rockwood, A. P., and Jones, T. R. 2000. Adaptively sampled distance fields: a general representation of shape for computer graphics. In <i>Proceedings of ACM SIGGRAPH 00</i>, 249--254.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278801</article_id>
		<sort_key>17</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Modal locomotion]]></title>
		<subtitle><![CDATA[controlling passive elastic dynamics]]></subtitle>
		<page_from>17</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278801</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278801</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P338124</person_id>
				<author_profile_id><![CDATA[81100269547]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Kry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#233; Ren&#233; Descartes Paris and Universit&#233; Grenoble / INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051830</person_id>
				<author_profile_id><![CDATA[81100400947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lionel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reveret]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#233; Grenoble / INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048119</person_id>
				<author_profile_id><![CDATA[81100210102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Francois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faure]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#233; Grenoble / INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P189239</person_id>
				<author_profile_id><![CDATA[81407594555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marie-Paule]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#233; Grenoble / INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alexander, R. M. 1996. <i>Optima for Animals</i>. Princeton University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[McGeer, T., and Alexander, R. 1990. Passive bipedal running. <i>Proceedings of the Royal Society of London, Series B, Biological Sciences 240</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74355</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pentland, A., and Williams, J. 1989. Good vibrations: model dynamics for graphics and animation. In <i>SIGGRAPH '89</i>, ACM Press, 215--222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278802</article_id>
		<sort_key>18</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Real-time voxelization of triangle meshes on the GPU]]></title>
		<page_from>18</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278802</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278802</url>
		<abstract>
			<par><![CDATA[<p>We present two algorithms to generate volumetric representations of triangle meshes on the GPU. Because all the data is generated in video memory, the algorithms are particularly useful for GPU-based physical simulation. We demonstrate their application to fluid simulation with a real-time smoke simulation where dynamic obstacles (skinned meshes) drive the fluid motion. The first algorithm provides an inside-outside representation of the triangle mesh, while the second one provides interpolated attributes (such as velocity) for those voxels intersected by the boundary of the mesh. Both of these algorithms are efficient on current graphics hardware allowing their use in real-time applications such as video games.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40038064</person_id>
				<author_profile_id><![CDATA[81335494354]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ignacio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Llamas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1111424</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eisemann, E., and Decoret, X. 2006. Fast scene voxelization and applications. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</i>, 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fang, S., and Cheng, H. 2000. Hardware accelerated voxelization. <i>Computers and Graphics 24</i>, 3, 433--442.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278803</article_id>
		<sort_key>19</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Dynamic execution tracing of physical simulations]]></title>
		<page_from>19</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278803</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278803</url>
		<abstract>
			<par><![CDATA[<p>Software systems that simulate physical phenomena such as renderers, fluid simulation engines, or cloth dynamics engines, provide special difficulties during debugging. In particular, they often process vast amounts of data, making it necessary to trace not only individual values through an execution, but rather gather higher-level information about how whole classes of data are processed. Furthermore, the data flow paths and code paths through a simulation system can be extremely complex, as well as different for every variable in the system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP311034400</person_id>
				<author_profile_id><![CDATA[81544827156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kiczales, G., Lamping, J., Mendhekar, A., Maeda, C., Lopes, C., Loingtier, J., and Irwin, J. 1997. Aspect-oriented programming. In <i>Proceedings of European Conference on Object-Oriented Programming</i>, vol. 1241, 220--242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1199544</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sun Microsystems. 2005. <i>Solaris Dynamic Tracing Guide</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278804</article_id>
		<sort_key>20</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Blobtacular]]></title>
		<subtitle><![CDATA[surfacing particle system in "Pirates of the Caribbean 3"]]></subtitle>
		<page_from>20</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278804</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278804</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39059536</person_id>
				<author_profile_id><![CDATA[81324492244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891739</person_id>
				<author_profile_id><![CDATA[81421598300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clive]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053725</person_id>
				<author_profile_id><![CDATA[81335500074]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nafees]]></first_name>
				<middle_name><![CDATA[Bin]]></middle_name>
				<last_name><![CDATA[Zafar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566585</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Museth, K., Breen, D., Whitaker, R., and Barr, A. 2002. Level set surface editing operators. <i>ACM Trans. on Graphics (SIGGRAPH) 21</i>, 3 (July), 330--338.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1132035</ref_obj_id>
				<ref_obj_pid>1132033</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nielsen, M. B., and Museth, K. 2006. Dynamic Tubular Grid: An efficient data structure and algorithms for high resolution level sets. <i>Journal of Scientific Computing, 26</i>, 3, 261--299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zhukov, L., Museth, K., Breen, D., Whitaker, R., and Barr, A. 2003. Level set segmentation and modeling of dt-mri human brain data. <i>Journal of Electronic Imagine 12</i> 1, 125--133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278805</section_id>
		<sort_key>21</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[I've got you covered]]></section_title>
		<section_page_from>21</section_page_from>
	<article_rec>
		<article_id>1278806</article_id>
		<sort_key>21</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Stencil routed A-Buffer]]></title>
		<page_from>21</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278806</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278806</url>
		<abstract>
			<par><![CDATA[<p>We have developed a stencil routing algorithm for implementing a GPU accelerated A-Buffer, by using a multisample texture to store a vector of fragments per pixel. First, all the fragments are captured per pixel in rasterization order. Second, a fullscreen shader pass sorts the fragments using a bitonic sort. At this point, the sorted fragments can be blended arbitrarily to implement various types of algorithms such as order independent transparency or layered depth image generation. Since we handle only 8 fragments per pass, we developed a method for detecting overflow, so we can do additional passes to capture more fragments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891733</person_id>
				<author_profile_id><![CDATA[81335495414]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Myers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047881</person_id>
				<author_profile_id><![CDATA[81319488052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Louis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bavoil]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>808585</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Carpenter, L. 1984. The a -buffer, an antialiased hidden surface method. In <i>SIGGRAPH '84: Proceedings of the 11th annual conference on Computer graphics and interactive techniques</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Everitt, C. 2001. Interactive order-independent transparency. Tech. rep., NVIDIA Corporation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>844181</ref_obj_id>
				<ref_obj_pid>844174</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Purcell, T. J., Donner, C., Cammarano, M., Jensen, H. W., and Hanrahan, P. 2003. Photon mapping on programmable graphics hardware.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278807</article_id>
		<sort_key>22</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Fast Poisson disk sampling in arbitrary dimensions]]></title>
		<page_from>22</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278807</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278807</url>
		<abstract>
			<par><![CDATA[<p>In many applications in graphics, particularly rendering, generating samples from a blue noise distribution is important. However, existing efficient techniques do not easily generalize beyond two dimensions. Here I demonstrate a simple modification to dart throwing which permits generation of Poisson disk samples in <i>O</i>(<i>N</i>) time, easily implemented in arbitrary dimension.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Poisson disk]]></kw>
			<kw><![CDATA[blue noise]]></kw>
			<kw><![CDATA[sampling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP39034414</person_id>
				<author_profile_id><![CDATA[81100248660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bridson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L. 1986. Stochastic sampling in computer graphics. <i>ACM Trans. Graph. 5</i>, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141915</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dunbar, D., and Humphreys, G. 2006. A spatial data structure for fast poisson-disk sample generation. <i>ACM Trans. Graph. 25</i>, 3, 503--508.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278808</article_id>
		<sort_key>23</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Pixmotor]]></title>
		<subtitle><![CDATA[a pixel motion integrator]]></subtitle>
		<page_from>23</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278808</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278808</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052076</person_id>
				<author_profile_id><![CDATA[81100390217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neulander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278809</article_id>
		<sort_key>24</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A system for efficient rendering of human skin]]></title>
		<page_from>24</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278809</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278809</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35047867</person_id>
				<author_profile_id><![CDATA[81335489650]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[d'Eon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14056692</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048175</person_id>
				<author_profile_id><![CDATA[81335490222]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Enderton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>965470</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Borshukov, G., and Lewis, J. 2003. Realistic human face rendering for "the matrix reloaded". In <i>ACM SIGGRAPH 2003 Conference Abstracts and Applications (Technical Sketch)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882433</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dachsbacher, C., and Stamminger, M. 2003. Translucent shadow maps. In <i>Eurographics Symposium on Rendering: 14th Eurographics Workshop on Rendering</i>, 197--201.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073308</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Donner, C., and Jensen, H. W. 2005. Light diffusion in multi-layered translucent materials. In <i>Proceedings of SIGGRAPH 2005</i>, 1032--1039.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278810</article_id>
		<sort_key>25</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Approximation of subdivision surfaces for interactive applications]]></title>
		<page_from>25</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278810</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278810</url>
		<abstract>
			<par><![CDATA[<p>Subdivision surfaces are undoubtedly the most flexible smooth geometric representation. By only manipulating a carefully designed low-resolution mesh, an high-resolution smooth version is automatically generated using a set of local recursive rules applied on each coarse polygon. However, while being intensively used in CAD and SFX industries, they have not yet gained a significant interest for interactive and real-time applications. In fact, their recursive definition imposes a non-trivial CPU overhead, difficult to hide in interactive applications. We propose a new efficient approximation of subdivision surfaces which offers a very close appearance compared to the true subdivision surface while being at least one order of magnitude faster than true subdivision rendering. Our technique uses enriched polygons, equipped with edge vertices, and replaces them on-the-fly with low degree polynomials for interpolating positions and normals. By systematically projecting the vertices of input mesh at their limit position on the subdivision surface, the visual quality of the approximation is good enough for imposing only a single subdivision step on the CPU, allowing real-time performances even for million polygons output. Additionally, the parametric nature of the approximation allows an efficient adaptive sampling for both adaptive rendering and displacement mapping.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35033095</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bordeaux]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109976</person_id>
				<author_profile_id><![CDATA[81100297901]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schlick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bordeaux]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Boubekeur, T., and Schlick, C. 2007. <i>GPU Gems 3</i>. NVidia, ch. Generic Adaptive Mesh Refinement.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073304</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shiue, L.-J., Jones, I., and Peters, J. 2005. A realtime gpu subdivision kernel. <i>ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364387</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Vlachos, A., Peters, J., Boyd, C., and Mitchell, J. 2001. Curved PN triangles. <i>ACM I3D</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278811</section_id>
		<sort_key>26</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[The Oasis]]></section_title>
		<section_page_from>26</section_page_from>
	<article_rec>
		<article_id>1278812</article_id>
		<sort_key>26</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The Birth of Sandman]]></title>
		<page_from>26</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278812</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278812</url>
		<abstract>
			<par><![CDATA[<p>The <i>Birth of Sandman</i> sequence in <i>Spider-Man 3</i> involved creating a human-sized character who emerges from a pile of sand in full view under a well-lit environment. The scale of the shot ranges from macro close up views of single sand grains to distant views of the entire sand pile as Sandman emerges. The sequence begins with a fully CG 2,672-frame shot that slowly revolves around sandman as he forms. The Imageworks Sand FX team had an enormous challenge of transferring character animation into a physically plausible animation of hundreds of millions of sand grains forming into a human form.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35047635</person_id>
				<author_profile_id><![CDATA[81335487633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ammann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891710</person_id>
				<author_profile_id><![CDATA[81335488006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bloom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048554</person_id>
				<author_profile_id><![CDATA[81335489198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891726</person_id>
				<author_profile_id><![CDATA[81335489605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Courte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048267</person_id>
				<author_profile_id><![CDATA[81544124856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lucio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Flores]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891758</person_id>
				<author_profile_id><![CDATA[81335491334]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891742</person_id>
				<author_profile_id><![CDATA[81335492342]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Nikos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kalaitzidis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052922</person_id>
				<author_profile_id><![CDATA[81335498514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Terrance]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tornberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891734</person_id>
				<author_profile_id><![CDATA[81335498605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Laurence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Treweek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891695</person_id>
				<author_profile_id><![CDATA[81335499601]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Bob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Winter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891701</person_id>
				<author_profile_id><![CDATA[81335499966]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073379</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bell, N., Yu, Y., and Mucha, P. J. 2005. Particle-based simulation of granular materials. In <i>ACM SIGGRAPH / Eurographics Symposium on Computer Animation</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946949</ref_obj_id>
				<ref_obj_pid>946250</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Onoue, K., and Nishita, T. 2003. Virtual sandbox. In <i>11th Pacific Conference on Computer Graphics and Applications</i>, 252--259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278813</article_id>
		<sort_key>27</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Rendering tons of sand]]></title>
		<page_from>27</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278813</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278813</url>
		<abstract>
			<par><![CDATA[<p>On <i>Spider-Man 3</i>, one of the villains is the Sandman, a character composed of sand. Sony Pictures Imageworks needed to render Sandman both as a solid form and as flowing sand grains, which in some scenes consisted of hundreds of millions of grains, and in some scenes the grains were close enough to the camera to see fine detail. This sketch describes our tools and workflow for rendering so many grains with RenderMan and controlling the level of detail for a consistent look.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891700</person_id>
				<author_profile_id><![CDATA[81540283256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891710</person_id>
				<author_profile_id><![CDATA[81335488006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bloom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048554</person_id>
				<author_profile_id><![CDATA[81335489198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891734</person_id>
				<author_profile_id><![CDATA[81335498605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Laurence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Treweek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278814</article_id>
		<sort_key>28</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[How to build a sixty foot man of moving sand]]></title>
		<page_from>28</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278814</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278814</url>
		<abstract>
			<par><![CDATA[<p>In the final battle sequence of <i>Spider-Man 3</i>, the Sandman character forms himself at a construction site. Pulling foreign materials such as dirt and construction debris into himself, he forms into a monstrous, 60 foot tall version of himself which we referred to simply as "Big." Big presented a number of challenges separate from the other incarnations of the character in the film.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891723</person_id>
				<author_profile_id><![CDATA[81335495829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jamie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pilgrim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052836</person_id>
				<author_profile_id><![CDATA[81335498514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Terrance]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tornberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278815</article_id>
		<sort_key>29</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Levelsets in production]]></title>
		<subtitle><![CDATA[<i>Spider-man 3</i>]]></subtitle>
		<page_from>29</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278815</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278815</url>
		<abstract>
			<par><![CDATA[<p>Levelsets are a flexible implicit surface representation that have many applications in production. A levelset represents a surface as a signed distance function (SDF) on a regularly sampled grid. For <i>Spider-Man 3</i>, we created a toolset called Sandstorm which included a rich set of levelset-based modeling and animation tools. This included methods for converting geometry into SDF fields and vice versa, methods for transferring surface attributes into these fields, tools for looking up values in an SDF, and many other flexible surface editing operators.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891700</person_id>
				<author_profile_id><![CDATA[81540283256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048554</person_id>
				<author_profile_id><![CDATA[81335489198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891710</person_id>
				<author_profile_id><![CDATA[81335488006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bloom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891706</person_id>
				<author_profile_id><![CDATA[81335490069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Ferreira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891758</person_id>
				<author_profile_id><![CDATA[81335491334]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891705</person_id>
				<author_profile_id><![CDATA[81335494543]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Cory]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMahon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278816</article_id>
		<sort_key>30</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Artistic direction of foliage]]></title>
		<page_from>30</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278816</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278816</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35047804</person_id>
				<author_profile_id><![CDATA[81100269613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Amaury]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aubel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35046625</person_id>
				<author_profile_id><![CDATA[81452612103]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278817</section_id>
		<sort_key>31</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[It's all about the environment]]></section_title>
		<section_page_from>31</section_page_from>
	<article_rec>
		<article_id>1278818</article_id>
		<sort_key>31</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D scene modeling using pose-free reconstruction]]></title>
		<page_from>31</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278818</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278818</url>
		<abstract>
			<par><![CDATA[<p>We present and illustrate the benefits of a fundamental change to the traditional 3D modeling pipeline. In sharp contrast with any previous method, including hardware trackers and self-calibration, we completely eliminate camera position and camera orientation parameters from the modeling process. This significantly simplifies the procedure for creating a 3D model of a real-world environment by omitting all concerns about providing camera pose information or assuming it can be accurately recovered from image features, landmarks, or hardware.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P58480</person_id>
				<author_profile_id><![CDATA[81100218141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Aliaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40038162</person_id>
				<author_profile_id><![CDATA[81335500290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36037159</person_id>
				<author_profile_id><![CDATA[81330488857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mireille]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boutin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278819</article_id>
		<sort_key>32</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Imaging and 3D tomographic reconstruction of time-varying, inhomogeneous refractive index fields]]></title>
		<page_from>32</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278819</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278819</url>
		<abstract>
			<par><![CDATA[<p>Time varying, inhomogeneous refractive index fields are every-where, from hot air rising above a fire and mirages above hot roads in the summer, to gases flowing from pressurized containers, and mixtures of different liquids. We present a novel method for imaging and volumetric reconstruction of time-varying refractive index fields, based on the Background Oriented Schlieren (BOS) technique developed in the fluid imaging community [Meier 2002].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891696</person_id>
				<author_profile_id><![CDATA[81385592276]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bradley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Atcheson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35041113</person_id>
				<author_profile_id><![CDATA[81331495034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ivo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ihrke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034122</person_id>
				<author_profile_id><![CDATA[81319488885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Derek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bradley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39052214</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083680</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Braunschweig University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P107233</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Meier, G. 2002. Computerized Background-Oriented Schlieren. <i>Experiments in Fluids 33</i>, 181--187.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Venkatakrishnan, L., and Meier, E. 2004. Density measurements using the background oriented schlieren technique. <i>Experiments in Fluids 37</i>, 237--247.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278820</article_id>
		<sort_key>33</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Modeling repetitive motions in real-world 3D scenes]]></title>
		<page_from>33</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278820</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278820</url>
		<abstract>
			<par><![CDATA[<p>Obtaining models of dynamic 3D objects is an important part of content generation for computer graphics. If the states or poses of the dynamic object repeat often during a sequence (but not necessarily periodically), we call such a <i>repetitive motion</i>. There are many objects, such as toys, machines, and humans, undergoing repetitive motions. Our key observation is that for repetitive motions we can use one fixed camera to perform robust motion analysis and a second capture-device to provide 3D information of each motion state. After the motion sequence, we group temporally disjoint observations of the same motion state and produce a smooth space-time reconstruction of the scene. Effectively, the dynamic scene modeling problem is converted to a series of static scene reconstructions, which are much easier to tackle. The second device can be either a passive camera or an active-light projector, resulting in two different modeling techniques. Based on this observation, we present an efficient passive multi-viewpoint acquisition and a robust structured-light acquisition of repetitive motions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39063335</person_id>
				<author_profile_id><![CDATA[81328490999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P58480</person_id>
				<author_profile_id><![CDATA[81100218141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Aliaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278821</article_id>
		<sort_key>34</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[1001 acquisition viewpoints]]></title>
		<subtitle><![CDATA[efficient and versatile view-dependent modeling of real-world scenes]]></subtitle>
		<page_from>34</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278821</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278821</url>
		<abstract>
			<par><![CDATA[<p>Constructing high-fidelity 3D models of real-world scenes is an important bottleneck for many computer graphics applications. Manual modeling using CAD or animation software requires artistic talent and a huge time investment. Automated modeling based on acquiring color and depth data is a promising alternative. The typical automated modeling approach densely samples the color and geometry of the scene from several acquisition viewpoints, and then merges the datasets to obtain the scene model. However, dense depth sampling of complex scenes remains a challenging process that can take tens of minutes per acquisition viewpoint (e.g. sequential scanning in laser range finding, robust off-line correspondence computation in depth from stereo, or sequential light pattern projection in depth from structured light). This limits acquisition to a sparse set of viewpoints, and, even with careful view planning, complex scenes remain inadequately sampled.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822853</person_id>
				<author_profile_id><![CDATA[81319498125]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mihai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mudure]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P292184</person_id>
				<author_profile_id><![CDATA[81100375260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Voicu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popescu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278822</article_id>
		<sort_key>35</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Interactive procedural street modeling]]></title>
		<page_from>35</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278822</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278822</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents a solution to efficiently model the street networks of large urban areas. Parish and M&#252;ller [2001] were the first to note that the street network is the key to create a large urban model. While this algorithm created a high quality solution, the method does not allow to incorporate user-control. To address this limitation we provide a rather different alternative to street modeling that allows to integrate a wide variety of user input. The key idea is to use tensor fields to guide the generation of street graphs. A user can interactively edit a street graph by either modifying the underlying tensor field or by changing the graph directly. This allows for efficient modeling, because we can combine high-level and low-level modeling operations, constraints, and procedural methods. The major contributions are as follows: (1) We are the first to introduce a procedural approach to model urban street networks that combines interactive user-guided editing operations and procedural methods. (2) We are introducing a new methodology to graph modeling in general. The idea of tensor-guided graph modeling together with the tight integration of interactive editing and procedural modeling has not been explored previously in related modeling problems, such as modeling of bark, cracks, fracture, or trees.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891717</person_id>
				<author_profile_id><![CDATA[81335490127]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Esch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044447</person_id>
				<author_profile_id><![CDATA[81100473306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wonka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arizona State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39076330</person_id>
				<author_profile_id><![CDATA[81332517324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETH Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088705</person_id>
				<author_profile_id><![CDATA[81100115231]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383292</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Parish, Y. I. H., and M&#252;ller, P. 2001. Procedural modeling of cities. In <i>Proceedings of ACM SIGGRAPH 2001</i>, ACM Press, E. Fiume, Ed., 301--308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278823</section_id>
		<sort_key>36</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Vogue!]]></section_title>
		<section_page_from>36</section_page_from>
	<article_rec>
		<article_id>1278824</article_id>
		<sort_key>36</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[High fashion in equations]]></title>
		<page_from>36</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278824</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278824</url>
		<abstract>
			<par><![CDATA[<p>Tailored out of exquisite materials and artful designed patterns, high fashion garments constitute the most sophisticated kind of clothes. The unique manufactured pieces, only affordable for a small circle of clientele, are not only envelopes for the human body, but artworks, visualizing cultural aspects, tendencies and trends. Historical haute couture garments are characterized by an additional aspect: Time specific garment details, which allow their affiliation to certain &#233;poques, become visible.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P841470</person_id>
				<author_profile_id><![CDATA[81322500130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christiane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luible]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Geneva]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14015481</person_id>
				<author_profile_id><![CDATA[81100007341]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Volino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Geneva]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35045671</person_id>
				<author_profile_id><![CDATA[81332531807]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nadia]]></first_name>
				<middle_name><![CDATA[Magnenat]]></middle_name>
				<last_name><![CDATA[Thalmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Geneva]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Steele V., 2000, Fifty Years of Fashion: New Look to Now, Yale University Press, ISBN 978-0300087383]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Volino P., Magnenat-Thalmann N., 2005, Accurate Garment Prototyping and Simulation, Computer-Aided Design and Applications, CAD Solutions, 2(5), pp 645--654.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142007</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Volino P., Magnenat-Thalmann N., 2006, Resolving Surface Collisions through Intersection Contour Minimization, ACM Transactions on Graphics, ACM Press, 25 (3), pp. 1154--1159]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278825</article_id>
		<sort_key>37</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Data driven cloth animation]]></title>
		<page_from>37</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278825</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278825</url>
		<abstract>
			<par><![CDATA[<p>We present a new method for cloth animation based on data driven synthesis. In contrast to approaches that focus on physical simulation, we animate cloth by manipulating short sequences of existing cloth animation. While our source of data is cloth animation captured using video cameras ([White et al. 2007]), the method is equally applicable to simulation data. The approach has benefits in both cases: current cloth capture is limited because small tweaks to the data require filming an entirely new sequence. Likewise, simulation suffers from long computation times and complications such as tangling. In this sketch we create new animations by fitting cloth animation to human motion capture data, i.e., we drive the cloth with a skeleton.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P774289</person_id>
				<author_profile_id><![CDATA[81310488692]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[White]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley and University of Illinois, Urbana Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40037668</person_id>
				<author_profile_id><![CDATA[81315488302]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Keenan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crane]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois, Urbana Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40030467</person_id>
				<author_profile_id><![CDATA[81100502370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Forsyth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois, Urbana Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015736</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sumner, R., and Popovi&#263;, J. 2004. Deformation transfer for triangle meshes. In <i>SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276420</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[White, R., Crane, K., and Forsyth, D. 2007. Capturing and animating occluded cloth. In <i>SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278826</article_id>
		<sort_key>38</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Driving motion control by motion capture using CG]]></title>
		<page_from>38</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278826</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278826</url>
		<abstract>
			<par><![CDATA[<p>In creating the Pepsi "Dance Tron" commercial (accepted to the 2007 SIGGRAPH Computer Animation Festival), Method Studios combined motion capture with motion control in a novel manner. Following a pre-shoot motion capture session with breakdancers, we used a skeleton in Autodesk's Maya for pre-visualization of the intricate dance moves to select the desired motions. Repeatable action rigging systems, built to hold ten dancers and mounted on a gimbal for the live shoot, were created to reproduce as much movement as possible, and these rigs were driven by data created in CG based upon the motion capture. In this manner, we were able to smoothly combine CG and practical elements, a method that we believe to be best for generating a believable appearance for such imaginative creations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891691</person_id>
				<author_profile_id><![CDATA[81335490652]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frisch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios, Santa Monica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891722</person_id>
				<author_profile_id><![CDATA[81335493605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[LeBloch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios, Santa Monica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891716</person_id>
				<author_profile_id><![CDATA[81335488154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Method Studios, Santa Monica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278827</section_id>
		<sort_key>39</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Bend and stretch]]></section_title>
		<section_page_from>39</section_page_from>
	<article_rec>
		<article_id>1278828</article_id>
		<sort_key>39</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Optimization-based interactive motion synthesis for virtual characters]]></title>
		<page_from>39</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278828</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278828</url>
		<abstract>
			<par><![CDATA[<p>Modeling the reactions of human characters to a dynamic environment is crucial for achieving perceptual immersion in applications such as video games, training simulations and movies. Virtual characters in these applications need to <i>realistically</i> react to environmental events and <i>precisely</i> follow high-level user commands. Most existing physics engines for computer animation facilitate synthesis of passive motion, but remain unsuccessful in generating motion that requires active control, such as character animation. We present an optimization-based approach to synthesizing active motion for articulated characters, emphasizing both physical realism and user controllability. At each time step, we optimize the motion based on a set of goals specified by higher-level decision makers, subject to the Lagrangian dynamics and the physical limitations of the character. Our framework represents each decision maker as a controller.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35049398</person_id>
				<author_profile_id><![CDATA[81335492422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sumit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891773</person_id>
				<author_profile_id><![CDATA[81335499985]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuting]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ye]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050158</person_id>
				<author_profile_id><![CDATA[81452598193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[C.]]></first_name>
				<middle_name><![CDATA[Karen]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278829</article_id>
		<sort_key>40</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Simulating coordinated movement with tendons]]></title>
		<page_from>40</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278829</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278829</url>
		<abstract>
			<par><![CDATA[<p>Despite significant advances in the research of human movement [Kry and Pai 2006], simulating the dynamics of the human body, such as compliance coupling between joints, remains challenging. Simulating biomechanics is difficult, in part, due to the complex inter-dependence between tendons and bones. Modeling this interdependence is necessary, however, since much of the coordinated motion in human biomechanics is due to the specific ways in which tendons are arranged with respect to bones. Accurate tendon modeling is also important for visual effects, since tendon motion has a noticeable effect on skin deformation. We address these issues with a novel simulation framework that efficiently and accurately models the coupled dynamics of tendons and bones.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052663</person_id>
				<author_profile_id><![CDATA[81335498230]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shinjiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P67322</person_id>
				<author_profile_id><![CDATA[81100642559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Pai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>357678</ref_obj_id>
				<ref_obj_pid>357670</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Delp, S. L., and Loan, J. P. 2000. A computational framework for simulating and analyzing human and animal movement. <i>Computing in Science &amp; Engineering 2, 5</i>, 46--55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141969</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kry, P. G., and Pai, D. K. 2006. Interaction capture and synthesis. In <i>Proc. ACM SIGGRAPH 2006 Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>988844</ref_obj_id>
				<ref_obj_pid>988834</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lenoir, J., Grisoni, L., Meseure, P., Remion, Y., and Chaillou, C. 2004. Smooth constraints for spline variational modeling. In <i>GRAPHITE '04</i>, ACM Press, 58--64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846285</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Teran, J., Blemker, S., Ng-Thow-Hing, V., and Fedkiw, R. 2003. Finite volume methods for the simulation of skeletal muscle. In <i>Symposium on Computer Animation 2003</i>, Eurographics Association, 68--74.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278830</article_id>
		<sort_key>41</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Musculo-skeletal shape skinning]]></title>
		<page_from>41</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278830</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278830</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051889</person_id>
				<author_profile_id><![CDATA[81335494902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049784</person_id>
				<author_profile_id><![CDATA[81322495702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278831</article_id>
		<sort_key>42</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Fantastic Four]]></title>
		<subtitle><![CDATA[stretching the limits]]></subtitle>
		<page_from>42</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278831</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278831</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051933</person_id>
				<author_profile_id><![CDATA[81335494902]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hydraulx]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047251</person_id>
				<author_profile_id><![CDATA[81335488539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Butler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hydraulx]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891752</person_id>
				<author_profile_id><![CDATA[81335491040]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rudy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grossman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hydraulx]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891753</person_id>
				<author_profile_id><![CDATA[81335496109]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Russell]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pearsall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hydraulx]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278832</section_id>
		<sort_key>43</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Nice gestures]]></section_title>
		<section_page_from>43</section_page_from>
	<article_rec>
		<article_id>1278833</article_id>
		<sort_key>43</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Wearable haptic display to present virtual mass sensation]]></title>
		<page_from>43</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278833</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278833</url>
		<abstract>
			<par><![CDATA[<p>In recent times, there have been a number of computer user interface devices that have some haptic feedback functions, such as the DUALSHOCK controller [Sony Computer Entertainment, Inc. 1997] or the Wii Remote [Nintendo Co., Ltd. 2006]. However, their haptic feedback is limited to only the vibration function since there is no other method to provide haptic feedback that can be implemented with a small and inexpensive device. On the other hand, there is an increasing demand for realistic haptic feedback; thus, a simple and inexpensive method for a highly realistic haptic display is required. To meet this requirement, we propose a wearable haptic display to present the mass of a virtual object as shown in Figure 1. We focused on the mass of a virtual object, which contributes to the weight and the inertia mass in haptic interaction. If the virtual mass is presented by a haptic device, the user can perceive a more realistic sensation of the virtual object by grasping than the vibration feedback.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P882929</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891417</person_id>
				<author_profile_id><![CDATA[81335490472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Souichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukamachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P536461</person_id>
				<author_profile_id><![CDATA[81100131163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P207192</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P272463</person_id>
				<author_profile_id><![CDATA[81100411569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1264778</ref_obj_id>
				<ref_obj_pid>1264358</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Minamizawa, K., Kajimoto, H., Kawakami, N. and Tachi, S. 2007. Wearable Haptic Display to Present Gravity Sensation -- Preliminary Observations and Device Design, In <i>Proceedings of World Haptics 2007</i>, Tsukuba, Tokyo, 133--138]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278834</article_id>
		<sort_key>44</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Subtle gaze direction]]></title>
		<page_from>44</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278834</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278834</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P791878</person_id>
				<author_profile_id><![CDATA[81100645165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Reynold]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington Univ. St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083713</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[St. Louis Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P699013</person_id>
				<author_profile_id><![CDATA[81100528357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nisha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sudarsanam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington Univ. St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39048487</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington Univ. St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dodge, R. 1900. Visual perception during eye movement. <i>Psychological Review 7</i>, 454--465.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278835</article_id>
		<sort_key>45</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[DirectCam]]></title>
		<subtitle><![CDATA[a gestural system for animatic creation]]></subtitle>
		<page_from>45</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278835</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278835</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891743</person_id>
				<author_profile_id><![CDATA[81335494212]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Noah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lockwood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891746</person_id>
				<author_profile_id><![CDATA[81335497206]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patricio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Simari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047655</person_id>
				<author_profile_id><![CDATA[81100290369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083980</person_id>
				<author_profile_id><![CDATA[81335497253]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Karan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278836</article_id>
		<sort_key>46</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Motion belts]]></title>
		<page_from>46</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278836</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278836</url>
		<abstract>
			<par><![CDATA[<p>Large scale motion databases have became essential for both computer animation and human motion analysis. We propose a novel visualization style of motion clip's outlines that unfolds a motion clip into a 2D stripe of keyframes. We call it a <i>motion belt</i>. Because of the form factor of the stripes, motion belts are suitable to display the outlines of the multiple motions simultaneously. This enables users to compare the multiple clips instantaneously skimming through the motion database. The existing outline visualization methods for motions extract keyframes and place them in a 3D space [Assa et al. 2005]. However, for the long clips without large locomotion, boxing for instance, the visualization results in a large cloud of keyframes that is hard to recognize the posture of each keyframe and the order of the actions in the clip.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP36044146</person_id>
				<author_profile_id><![CDATA[81100610717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yasuda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891755</person_id>
				<author_profile_id><![CDATA[81416593672]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ryota]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaihara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051966</person_id>
				<author_profile_id><![CDATA[81100652296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Suguru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088226</person_id>
				<author_profile_id><![CDATA[81100441141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073246</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Assa, J., Caspi, Y., and Cohen-Or, D. 2005. Action synopsis: pose selection and illustration. <i>ACM Trans. Graph. 24</i>, 3, 667--676.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>718491</ref_obj_id>
				<ref_obj_pid>647260</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Vazquez, P. P., Feixas, M., Sbert, M., and Heidrich, W. 2001. Viewpoint selection using viewpoint entropy. In <i>Proceedings of the Vision, Modeling, and Visualization (VMV) Conference</i>, 273--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278837</article_id>
		<sort_key>47</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Visualizing collision effects between penetrating and non-penetrating objects]]></title>
		<page_from>47</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278837</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278837</url>
		<abstract>
			<par><![CDATA[<p>We present a novel method for visualizing collisions effects between penetrating or non-penetrating objects. The interaction of 3D objects has been traditionally divided into two stages: the detection of a collision, and the response or effects after the collision. In this work, we concern ourselves with the latter. The rendering of collision effects is useful for virtual sculpting, virtual reality, surgery simulation and games. The interaction between two colliding objects has been used for virtual sculpting, where one object (virtual "tool"), generates indentations in the other object (virtual "clay"). This has been implemented in real-time with the aid of a distance field representation of the virtual tool. One of the assumptions of virtual sculpting, reminiscent of real sculpting materials (e.g., clay), is that objects are non-penetrating. In this paper, we generalize the notion of collision effects by allowing penetrating "tools", where the object is cut as the tool penetrates the object. The collision effect is such that the object is still deformed, but a break appears where the tool is. In this work, we use an implicit representation of the deformed object (virtual "clay"), as opposed to traditional mesh-based approaches. The ability to deform objects implicitly was exploited by our previous work [Correa et al. 2006a; Correa et al. 2006b], but they were concerned with simple primitive deformations such as twists or peels.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P648943</person_id>
				<author_profile_id><![CDATA[81100085579]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Correa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084011</person_id>
				<author_profile_id><![CDATA[81100336679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Deborah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Silver]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1280109</ref_obj_id>
				<ref_obj_pid>1280094</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Correa, C., and Silver, D. 2007. Programmable shaders for deformation rendering. In <i>Eurographics/SIGGRAPH Workshop on Graphics Hardware</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187827</ref_obj_id>
				<ref_obj_pid>1187627</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Correa, C., Silver, D., and Chen, M. 2006. Feature aligned volume manipulation for illustration and visualization. <i>IEEE Transactions on Visualization and Computer Graphics (Proceedings Visualization) 12, 5</i>, sss-eee.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Correa, C. D., Silver, D., and Chen, M. 2006. Discontinuous displacement mapping for volume graphics. In <i>Volume Graphics 2006</i>, Eurographics Association, 9--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1042395</ref_obj_id>
				<ref_obj_pid>1042201</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Gain, J., and Marais, P. 2005. Warp sculpting. <i>IEEE Transactions on Visualization and Computer Graphics 11</i>, 2, 217--227.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278838</section_id>
		<sort_key>48</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Oh, rats!]]></section_title>
		<section_page_from>48</section_page_from>
	<article_rec>
		<article_id>1278839</article_id>
		<sort_key>48</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Chop it up!]]></title>
		<subtitle><![CDATA[animation--driven modeling, simulation, and shading in the kitchen]]></subtitle>
		<page_from>48</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278839</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278839</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35048564</person_id>
				<author_profile_id><![CDATA[81100290369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios and University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048314</person_id>
				<author_profile_id><![CDATA[81442595983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Froemling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DeRose, T., Kass, M., and Truong, T. 1998. Subdivision Surfaces in Character Animation. In <i>Proceedings of SIGGRAPH 1998</i>, ACM Press, New York, 85--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015723</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Owada, S., Nielsen, F., Okabe, M., and Igarashi, T. 2004. Volumetric Illustration: Designing 3D Models with Internal Textures. <i>ACM Trans. Graph. 23</i>, 3, 322--328.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278840</article_id>
		<sort_key>49</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Virtual tailoring for <i>Ratatouille</i>]]></title>
		<subtitle><![CDATA[clothing the fattest man in the world]]></subtitle>
		<page_from>49</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278840</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278840</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891703</person_id>
				<author_profile_id><![CDATA[81335499313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Waggoner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39087462</person_id>
				<author_profile_id><![CDATA[81100334025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baraff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278841</article_id>
		<sort_key>50</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Articulating the appeal]]></title>
		<page_from>50</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278841</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278841</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891760</person_id>
				<author_profile_id><![CDATA[81335493085]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sonoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Konishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891740</person_id>
				<author_profile_id><![CDATA[81335498825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Venturini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278842</article_id>
		<sort_key>51</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[500 million and counting]]></title>
		<subtitle><![CDATA[hair rendering on <i>Ratatouille</i>]]></subtitle>
		<page_from>51</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278842</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278842</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051730</person_id>
				<author_profile_id><![CDATA[81335496511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ryu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276476</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cook, R., Halstead, J., Planck, M., and Ryu, D. Stochastic simplification of aggregate detail. In <i>Proceedings of SIGGRAPH 2007 (to appear)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278843</article_id>
		<sort_key>52</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Fast, soft reflections using radiance caches]]></title>
		<page_from>52</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278843</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278843</url>
		<abstract>
			<par><![CDATA[<p>In Pixar's Ratatouille a lot of scenes take place inside the kitchen where reflective surfaces like counter tops, stoves, pots and pans abound. Furthermore, these surfaces were often burnished or covered with dents, scratches or other displacements, which meant that the reflections were soft and fuzzy. Physically accurate reflections are most often achieved by tracing reflected rays into the scene. When the ray encounters another object, computationally expensive lighting and shading calculations must be performed to determine the contribution of the reflecting point. Paradoxically, surfaces that have soft or fuzzy reflections are more expensive since they have a larger reflection cone angle requiring more rays to adequately sample the reflected scene. We present a technique that utilizes radiance caches to significantly speed up the reflection calculations and discuss some of the accuracy trade-offs inherent to this approach.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052148</person_id>
				<author_profile_id><![CDATA[81335497331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Apurva]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891729</person_id>
				<author_profile_id><![CDATA[81335496705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ritter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050921</person_id>
				<author_profile_id><![CDATA[81406599978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[King]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049611</person_id>
				<author_profile_id><![CDATA[81335490776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gronsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CHR87 -- Robert Cook, Loren Carpenter, Edwin Catmull, The Reyes image rendering architecture, Proceedings of ACM Siggraph 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CHR04 -- Baking 3D Textures: Point Clouds and Brick Maps, PhotoRealistic RenderMan Application Note #39, Pixar 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CHR07 -- Per Christensen, Point Clouds and Brick Maps for Movie Production. Chapter in Point-Based Graphics, Markus Gross and Hanspeter Pfister editors, Morgan Kaufmann Publishers, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278844</article_id>
		<sort_key>53</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Effective toon-style rendering control using scalar fields]]></title>
		<page_from>53</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278844</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278844</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891692</person_id>
				<author_profile_id><![CDATA[81335491152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harvill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278845</section_id>
		<sort_key>54</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Traveling light]]></section_title>
		<section_page_from>54</section_page_from>
	<article_rec>
		<article_id>1278846</article_id>
		<sort_key>54</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[GPU-based light wavefront simulation for real-time refractive object rendering]]></title>
		<page_from>54</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278846</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278846</url>
		<abstract>
			<par><![CDATA[<p>In our paper on Eikonal Rendering [1], presented in the SIGGRAPH 2007 main paper program, we propose a novel algorithm to render a variety of sophisticated lighting effects in and around refractive objects in real-time on a single PC. Our method enables us to realistically display objects with spatially-varying refractive indices, inhomogeneous attenuation characteristics, as well as well as spatially-varying reflectance and anisotropic scattering properties. We can reproduce arbitrarily curved light paths, volume and surface caustics, anisotropic scattering as well as total reflection by means of the same efficient theoretical framework. In our approach, scenes are represented volumetrically. One core component of our method is a fast GPU particle tracer to compute viewing ray trajectories. It uses ordinary differential equations derived from the eikonal equation. The second important component is a light simulator, which utilizes a similar ODE-based framework to adaptively trace light wavefronts through the scene in a few seconds. While the conference paper [1] focuses on the development of the theory and the validation of our results, this sketch describes in detail the new concepts and data structures that we developed to implement view rendering and light wavefront tracing on the GPU (see figure 1 for a stage overview).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P829327</person_id>
				<author_profile_id><![CDATA[81320497031]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gernot]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ziegler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39068261</person_id>
				<author_profile_id><![CDATA[81331505042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Theobalt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35041113</person_id>
				<author_profile_id><![CDATA[81331495034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ivo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ihrke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044695</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University Braunschweig, Braunschweig, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P870290</person_id>
				<author_profile_id><![CDATA[81330499282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Art]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tevs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P107233</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276451</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[I. Ihrke, G. Ziegler, A. Tevs, C. Theobalt, M. Magnor and H-P. Seidel: Eikonal Rendering: Efficient Light Transport in Refractive Objects. <i>Proc. ACM SIGGRAPH 2007 (to be published)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[G. Ziegler, A. Tevs, C. Theobalt and H-P. Seidel. GPU Point List Generation through Histogram Pyramids. <i>Technical Reports of the MPI for Informatics</i>, June 2006, MPI-I-2006-4-002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278847</article_id>
		<sort_key>55</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Ray tracing dynamic scenes using selective restructuring]]></title>
		<page_from>55</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278847</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278847</url>
		<abstract>
			<par><![CDATA[<p>Ray tracing has been widely researched due to its ability to generate realistic images. However, the performance of current ray tracing algorithms is considerably slower than GPU-based rasterization algorithms, especially on dynamic scenes. In this paper we address the problem of efficient computation of bounding volume hierarchies (BVHs) of dynamic scenes for faster ray tracing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40034766</person_id>
				<author_profile_id><![CDATA[81100019061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sung-Eui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lawrence Livermore National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891756</person_id>
				<author_profile_id><![CDATA[81335489690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Curtis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P67316</person_id>
				<author_profile_id><![CDATA[81100618474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manocha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lauterbach, C., Yoon, S., Tuft, D., and Manocha, D. 2006. RT-DEFORM: Interactive ray tracing of dynamic scenes using bvhs. <i>IEEE Symposium on Interactive Ray Tracing</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1206075</ref_obj_id>
				<ref_obj_pid>1189762</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wald, I., Boulos, S., and Shirley, P. 2006. Ray Tracing Deformable Scenes using Dynamic Bounding Volume Hierarchies. <i>ACM Transactions on Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278848</article_id>
		<sort_key>56</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Radiance caching for participating media]]></title>
		<page_from>56</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278848</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278848</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P589983</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P575224</person_id>
				<author_profile_id><![CDATA[81100639133]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Donner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37026031</person_id>
				<author_profile_id><![CDATA[81100289561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zwicker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P109434</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., and Christensen, P. H. 1998. Efficient simulation of light transport in scenes with participating media using photon maps. In <i>Proceedings of SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ward, G. J., Rubinstein, F. M., and Clear, R. D. 1988. A ray tracing solution for diffuse interreflection. In <i>Proceedings of SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278849</article_id>
		<sort_key>57</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Interactive light transport editing for flexible global illumination]]></title>
		<page_from>57</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278849</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278849</url>
		<abstract>
			<par><![CDATA[<p>The limited flexibility imposed by the physics of light transport has been a major hurdle in the use of global illumination in production rendering. It has been a common practice in computer cinematography to write custom shaders [Christensen 2003] or manually edit rendered images [Thacker 2006] to achieve lighting effects desired by directors. This process is lengthy, cumbersome and too technical for artists. To remove this hurdle, we propose a novel interface that allows users to modify light transport in a scene. Users can adjust effects of indirect lighting cast by one object onto another or paint indirect illumination on surfaces. As opposed to previous approaches such as [Schoeneman et al. 1993] our work departs from solving optimization problems in favor of more user control.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891728</person_id>
				<author_profile_id><![CDATA[81335495702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Juraj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Obert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35027206</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P668956</person_id>
				<author_profile_id><![CDATA[81100219854]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[S&#253;kora]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14020468</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. 2003. Global illumination and all that. In <i>ACM SIGGRAPH Course 9 (RenderMan, Theory and Practice)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141998</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ha&#353;an, M., Pellacini, F., and Bala, K. 2006. Direct-to-indirect transfer for cinematic relighting. <i>ACM Transaction on Graphics 25</i>, 3, 1089--1097.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166135</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schoeneman, C., Dorsey, J., Smits, B., Arvo, J., and Greenberg, D. 1993. Painting with light. In <i>SIGGRAPH'93 Proceedings</i>, 143--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Thacker, J. 2006. How can I take my renders to the next level of realism? <i>3D Worlds</i> (June).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278850</article_id>
		<sort_key>58</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Global illumination using precomputed light paths for interactive light condition manipulation]]></title>
		<page_from>58</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278850</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278850</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we propose an interactive rendering method for taking into account global illumination using light path precomputation and a hierarchical data structure for fast final gathering. Our method can render static scenes interactively allowing the user to move the viewpoint and the positions of local light sources. Moreover, our method can change the materials (including the textures, diffuse and low-frequency glossy BRDFs) of objects at run-time. In recent years, many methods have been proposed to achieve interactive rendering by precomputing light transfer, for example, [Ha&#353;an et al. 2006]. These methods, however, do not allow the user to modify the materials of objects interactively.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891771</person_id>
				<author_profile_id><![CDATA[81384596311]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yonghao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35041108</person_id>
				<author_profile_id><![CDATA[81331494952]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wakayama Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P306681</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36036593</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383551</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. H., and Batali, D. 2004. An irradiance atlas for global illumination in complex production scenes. In <i>Rendering Techniques</i>, 133--142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141998</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ha&#353;an, M., Pellacini, F., and Bala, K. 2006. Direct-to-indirect transfer for cinematic relighting. In <i>SIGGRAPH '06</i>, ACM Press, New York, NY, USA, 1089--1097.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278851</section_id>
		<sort_key>59</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Making faces]]></section_title>
		<section_page_from>59</section_page_from>
	<article_rec>
		<article_id>1278852</article_id>
		<sort_key>59</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Anatomically accurate modeling and rendering of the human eye]]></title>
		<page_from>59</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278852</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278852</url>
		<abstract>
			<par><![CDATA[<p>Recovering anatomical features of organic materials is a challenging issue. The human eye, as an important part of the non verbal communication, needs to be accurately modeled and rendered to increase the realism of virtual characters. The recent improvements of the graphics hardware offer the opportunity of rendering complex organic materials, following correct anatomical properties. We propose a novel method that allows to recover the iris structure and scattering features from a single eye photograph. In this aim, we developed a method to unrefract iris photographs. We model the iris using the Subsurface Texture Mapping representation [Fran&#231;ois et al.] which allows to describe the relieves of the human iris. Finally, we introduce a refraction function for accurate real-time rendering of the eye, accounting for the refraction of the light at the corneal interface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822605</person_id>
				<author_profile_id><![CDATA[81319491424]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guillaume]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fran&#231;ois]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA -- France Telecom R&D]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35027861</person_id>
				<author_profile_id><![CDATA[81100339836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gautron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[France Telecom R&D]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048191</person_id>
				<author_profile_id><![CDATA[81309499650]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gaspard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Breton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[France Telecom R&D]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P156109</person_id>
				<author_profile_id><![CDATA[81100332780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kadi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouatouch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IRISA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1180064</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fran&#231;ois, G., Pattanaik, S., Bouatouch, K., and Breton, G. Subsurface texture mapping. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Sketches</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lam, M. W., and Baranoski, G. V. 2006. A predictive light transport model for the human iris. In <i>Computer Graphics Forum 25 (3)</i>, 359--368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pavlin, C. J., and Foster, F. 1994. <i>Ultrasound Biomicroscopy of the Eye</i>. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278853</article_id>
		<sort_key>60</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Sketching facial expressions]]></title>
		<page_from>60</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278853</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278853</url>
		<abstract>
			<par><![CDATA[<p>We present an innovative sketch-based interface for driving facial expressions. Unlike existing solutions [Chang and Jenkins 2006] our approach relies on recognition and constructs a semantically relevant representation of a sketched face. This representation is parameterized and used to drive a facial model. The main appeal of our method is that the interface is completely decoupled from the underlying facial model that is used. Therefore one single interface is capable of driving a variety of different models both 2D and 3D. The connection between our tool and the face model is defined by a library of template strokes that can be generated with ease.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P668165</person_id>
				<author_profile_id><![CDATA[81100485627]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gabriele]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nataneli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P225799</person_id>
				<author_profile_id><![CDATA[81100370834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Petros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arnheim, R. 1974. <i>Art and Visual Perception: A Psychology of the Creative Eye</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chang, E., and Jenkins, O. C. 2006. Sketching articulation and pose for facial animation. 19--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>652809</ref_obj_id>
				<ref_obj_pid>645439</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Saund, E., Mahoney, J., Fleet, D., Larner, D., and Lank, E., 2002. Perceptual organization as a foundation for intelligent sketch editing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278854</article_id>
		<sort_key>61</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A system for high-resolution face scanning based on polarized spherical illumination]]></title>
		<page_from>61</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278854</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278854</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P501647</person_id>
				<author_profile_id><![CDATA[81100447998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wan-Chun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Centers of Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117244</person_id>
				<author_profile_id><![CDATA[81100179328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hawkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Centers of Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891698</person_id>
				<author_profile_id><![CDATA[81319489365]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Charles-Felix]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chabert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Centers of Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P189952</person_id>
				<author_profile_id><![CDATA[81100467115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bolas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Centers of Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40037970</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Centers of Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P221188</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California Centers of Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Chabert, C.-F., Piers, P., Bolas, M., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination, Submitted to EGSR 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073226</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nehab, D., Rusinkiewicz, S., Davis, J., and Ramamoorthi, R. 2005. Efficiently combining positions and normals for precise 3d geometry. <i>ACM Transactions on Graphics 24</i>, 3 (Aug.), 536--543.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073258</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wenger, A., Gardner, A., Tchou, C., Unger, J., Hawkins, T., and Debevec, P. 2005. Performance relighting and reflectance transformation with time-multiplexed illumination. <i>ACM Transactions on Graphics 24</i>, 3 (Aug.), 756--764.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278855</article_id>
		<sort_key>62</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Light shower]]></title>
		<subtitle><![CDATA[a poor man's light stage built with an off-the-shelf umbrella and projector]]></subtitle>
		<page_from>62</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278855</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278855</url>
		<abstract>
			<par><![CDATA[<p>Compositing an actor or real-world object into a virtual background is a powerful and widely used tool in movic and TV production. To create a natural composite, it is necessary to maintain photometric consistency between the foreground object and the background environment. Various light stage systems have been developed to achieve this goal. Debevec et al. [Debevec et al. 2002] illuminated the actor by an array of inward-pointing RGB light-emitting diodes, and Mitsumine et al. [Mitsumine et al. 2005] surrounded the actor with back-projection screens and projected virtual images onto these screens. One problem with both of these strategies is that the systems involved are very expensive and time-consuming to build. We propose an inexpensive light stage system, <i>Light Shower</i>, which consists of an off-the-shelf projector and a white umbrella. The projector projects an image onto the white umbrella, which creates environment light for a human face or a real object inside the umbrella (see Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653915</person_id>
				<author_profile_id><![CDATA[81100546396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891732</person_id>
				<author_profile_id><![CDATA[81335498203]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takayama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050184</person_id>
				<author_profile_id><![CDATA[81100057408]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ijiri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P277957</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[JST PRESTO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566614</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Wenger, A., Tchou, C., Gardner, A., Waese, J., and Hawkins, T. 2002. A lighting reproduction approach to live-action compositing. In <i>Proceedings of SIGGRAPH 2002</i>, ACM Press, New York, NY, USA, 547--556.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187257</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mitsumine, H., Fukaya, T., Komiyama, S., and Yamanouchi, Y. 2005. Immersive virtual studio. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Sketches</i>, ACM Press, New York, NY, USA, 121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278856</article_id>
		<sort_key>63</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Fast and reusable facial rigging and animation]]></title>
		<page_from>63</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278856</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278856</url>
		<abstract>
			<par><![CDATA[<p>Reproducing the subtleties of a face through animation requires developing a sophisticated character rig. But, creating by hand the inner structure and controls of each character is a very labor-intensive and time-consuming task. We developed an application that is 90--99% faster than traditional manual rigging. The application automatically transfers the rig and animations from the source to a target model. Unlike prior work related to morphing and re-targeting [1] that focus on transferring animations, we transfer the complete facial setup in addition to animations. Our method is general, so artists can define their own rig and then quickly apply it to different models, even with disparate proportions and appearance (human, cartoon or fantastic). This gives artists complete freedom to manipulate the characters: they can create new animations and not be limited by pre-generated ones.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891768</person_id>
				<author_profile_id><![CDATA[81335495531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ver&#243;nica]]></first_name>
				<middle_name><![CDATA[Costa]]></middle_name>
				<last_name><![CDATA[Orvalho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universitat Polit&#232;cnica de Catalunya and Face in Motion]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P584018</person_id>
				<author_profile_id><![CDATA[81100254222]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Antonio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Susin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universitat Polit&#232;cnica de Catalunya]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383290</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Noh, J. I, Neumann, U. 2001. Expression Cloning. In <i>SIGGRAPH'01</i> (2001), ACM PREss, pp. 277--288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278857</section_id>
		<sort_key>64</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Drat, more rats!]]></section_title>
		<section_page_from>64</section_page_from>
	<article_rec>
		<article_id>1278858</article_id>
		<sort_key>64</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Acting with contact in <i>Ratatouille</i>]]></title>
		<subtitle><![CDATA[cartoon collision and response]]></subtitle>
		<page_from>64</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278858</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278858</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35047206</person_id>
				<author_profile_id><![CDATA[81100193621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cameron]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891751</person_id>
				<author_profile_id><![CDATA[81335496596]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Russ]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891690</person_id>
				<author_profile_id><![CDATA[81541621956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Woodbury]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278859</article_id>
		<sort_key>65</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Rivers of rodents]]></title>
		<subtitle><![CDATA[an animation-centric crowds pipeline for <i>Ratatouille</i>]]></subtitle>
		<page_from>65</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278859</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278859</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051827</person_id>
				<author_profile_id><![CDATA[81335496511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ryu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891747</person_id>
				<author_profile_id><![CDATA[81365593228]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanyuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278860</article_id>
		<sort_key>66</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Extracting and parametrizing temporally coherent surfaces from particles]]></title>
		<page_from>66</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278860</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278860</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37040557</person_id>
				<author_profile_id><![CDATA[81335497440]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052106</person_id>
				<author_profile_id><![CDATA[81335497331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Apurva]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1278862</ref_obj_id>
				<ref_obj_pid>1278780</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Froemling, E., Goktekin, T., and Peachey, D. Simulating whitewater rapids in <i>ratatouille. Submitted to Siggraph 2007 Technical Sketches</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Goktekin, T., Reisch, J., Peachey, D., and Shah, A. Rolling the dough, cracking the egg and pouring the sauce. <i>Submitted to Siggraph 2007 Technical Sketches</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015816</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shen, C., O'Brien, J. F., and Shewchuk, J. R. 2004. Interpolating and approximating implicit surfaces from polygon soup. <i>ACM Transactions on Graphics 23</i>, 3 (Aug.), 896--904.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073298</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zhu, Y., and Bridson, R. 2005. Animating sand as a fluid. <i>ACM Transactions on Graphics 24</i>, 3 (Aug.), 965--972.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278861</article_id>
		<sort_key>67</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[An effects recipe for rolling a dough, cracking an egg and pouring a sauce]]></title>
		<page_from>67</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278861</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278861</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P686357</person_id>
				<author_profile_id><![CDATA[81100433928]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tolga]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Goktekin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051588</person_id>
				<author_profile_id><![CDATA[81335496472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reisch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083848</person_id>
				<author_profile_id><![CDATA[81100022262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Darwyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peachey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052070</person_id>
				<author_profile_id><![CDATA[81335497331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Apurva]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1278860</ref_obj_id>
				<ref_obj_pid>1278780</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shen, C., and Shah, A. 2007. Extracting and parametrizing temporally coherent surfaces from particles. <i>Submitted to Siggraph 2007 Technical Sketches</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278862</article_id>
		<sort_key>68</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Simulating whitewater rapids in <i>Ratatouille</i>]]></title>
		<page_from>68</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278862</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278862</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35048315</person_id>
				<author_profile_id><![CDATA[81442595983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Froemling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048630</person_id>
				<author_profile_id><![CDATA[81100433928]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tolga]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goktekin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083845</person_id>
				<author_profile_id><![CDATA[81100022262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Darwyn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peachey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1278860</ref_obj_id>
				<ref_obj_pid>1278780</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shen, C. 2007. Extracting temporally coherent surfaces from particles. <i>sketch submitted to SIGGRAPH 2007</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278863</article_id>
		<sort_key>69</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Rat-sized water effects in <i>Ratatouille</i>]]></title>
		<page_from>69</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278863</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278863</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891715</person_id>
				<author_profile_id><![CDATA[81335488473]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bruins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052772</person_id>
				<author_profile_id><![CDATA[81335496472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reisch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278864</section_id>
		<sort_key>70</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Let there be light]]></section_title>
		<section_page_from>70</section_page_from>
	<article_rec>
		<article_id>1278865</article_id>
		<sort_key>70</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Geometric modeling using focal surfaces]]></title>
		<page_from>70</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278865</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278865</url>
		<abstract>
			<par><![CDATA[<p>The differential geometry of smooth three-dimensional surfaces can be interpreted from one of two perspectives: in terms of oriented frames located on the surface, or in terms of a pair of associated focal surfaces. These focal surfaces are swept by the loci of the principal curvatures' radii. The normal of each focal surface indicates a principal direction at the corresponding point on the original surface [Pottmann and Wallner 2001]. In this article, we utilize piecewise linear focal surfaces, which we call focal meshes, to model smooth surfaces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P544093</person_id>
				<author_profile_id><![CDATA[81100472930]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jingyi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Delaware]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891770</person_id>
				<author_profile_id><![CDATA[81335499884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xiaotian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SUNY Stony Brook]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045921</person_id>
				<author_profile_id><![CDATA[81455605505]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Xianfeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SUNY Stony Brook]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14058906</person_id>
				<author_profile_id><![CDATA[81100137780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Leonard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMillan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UNC Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891762</person_id>
				<author_profile_id><![CDATA[81100259454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gortler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1185728</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Grimm, C., and Zorin, D. 2006. Surface modeling and parameterization with manifolds: Siggraph 2006 course notes. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Courses</i>, ACM Press, New York, NY, USA, 1--81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581168</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pottmann, H., and Wallner, J. 2001. <i>Computational Line Geometry</i>. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>263849</ref_obj_id>
				<ref_obj_pid>263834</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[van Overveld, C. W. A. M., and Wyvill, B. 1997. Phong normal interpolation revisited. <i>ACM Trans. Graph. 16</i>, 4, 397--419.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278866</article_id>
		<sort_key>71</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Real-time shading with filtered importance sampling]]></title>
		<page_from>71</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278866</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278866</url>
		<abstract>
			<par><![CDATA[<p>Correct perception of materials requires complex, natural illumination [Fleming et al. 2003]. Thus for material or lighting design applications, realistic, interactive rendering of objects with arbitrary materials under natural illumination is essential. We present a simple and efficient technique for real-time, image-based lighting of objects with spatially-varying, glossy materials. The key to our algorithm is combining BRDF-proportional <i>importance sampling</i> with <i>environment map filtering</i> to attain computationally efficient rendering amenable to the GPU.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P767323</person_id>
				<author_profile_id><![CDATA[81309494438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Colbert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35027206</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#345;iv&#225;nek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Czech Technical University in Prague]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fleming, R. W., Dror, R. O., and Adelson, E. H. 2003. Real-world illumination and the perception of surface reflectance properties. <i>Journal of Vision</i> 3 (July), 347--368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732274</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kautz, J., V&#225;zquez, P.-P., Heidrich, W., and Seidel, H.-P. 2000. A unified approach to prefiltered environment maps. In <i>11th Eurographics Workshop on Rendering</i>, Eurographics Association, 185--196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383317</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ramamoorthi, R., and Hanrahan, P. 2001. An efficient representation for irradiance environment maps. In <i>Proc. of SIGGRAPH 2001</i>, ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278867</article_id>
		<sort_key>72</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[High dynamic range image hallucination]]></title>
		<page_from>72</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278867</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278867</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P855988</person_id>
				<author_profile_id><![CDATA[81408602744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lvdi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia and Tsinghua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39054725</person_id>
				<author_profile_id><![CDATA[81452594229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Li-Yi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40038346</person_id>
				<author_profile_id><![CDATA[81335500198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023518</person_id>
				<author_profile_id><![CDATA[81100085615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Baining]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P110231</person_id>
				<author_profile_id><![CDATA[81365591566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Heung-Yeung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Wang, L., Wei, L.-Y., Zhou, K., Guo, B., and Shum, H.-Y. 2007. High dynamic range image hallucination. In <i>Rendering Techniques '07 (Proc. of the Eurographics Symposium on Rendering)</i>, to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278868</article_id>
		<sort_key>73</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Mesostructure from specularity using coded illumination]]></title>
		<page_from>73</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278868</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278868</url>
		<abstract>
			<par><![CDATA[<p>In this paper we propose a technique for efficient acquisition of fine-scale surface details, or <i>mesostructure</i>. Inspired by Chen et al. [2006], we wish to recover mesostructure normal maps using a single top-view camera and a point light source based only on spec-larities. Many interesting materials such as plastic and metal can be analyzed this way. Moreover, certain translucent materials such as human skin and fruit are hard to analyze using traditional photometric stereo, due to excessive subsurface scattering. Specularities are not influenced by this, enabling us to capture detailed normal maps for such cases.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P873373</person_id>
				<author_profile_id><![CDATA[81330491117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yannick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Francken]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051661</person_id>
				<author_profile_id><![CDATA[81335494509]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mertens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891725</person_id>
				<author_profile_id><![CDATA[81335490845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gielis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023592</person_id>
				<author_profile_id><![CDATA[81100093388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bekaert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1153665</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chen, T., Goesele, M., and Seidel, H.-P. 2006. Mesostructure from specularity. In <i>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006)</i>, IEEE, New York, NY, USA, vol. 2, 1825--1832.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278869</article_id>
		<sort_key>74</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Fast image-based separation of diffuse and specular reflections]]></title>
		<page_from>74</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278869</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278869</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822468</person_id>
				<author_profile_id><![CDATA[81319495333]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lamond]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40037970</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P221188</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141977</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nayar, S., Krishnan, G., Grossberg, M. D., and Raskar, R. 2006. Fast Separation of Direct and Global Components of a Scene using High Frequency Illumination. <i>ACM Trans.</i> on Graphics (Jul).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Peers, P., Hawkins, T., and Debevec, P. 2006. A Reflective Light Stage. Tech. rep., USC, Dec. ICT-TR-04.2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278870</section_id>
		<sort_key>75</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Looking good]]></section_title>
		<section_page_from>75</section_page_from>
	<article_rec>
		<article_id>1278871</article_id>
		<sort_key>75</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Anime perspective]]></title>
		<page_from>75</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278871</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278871</url>
		<abstract>
			<par><![CDATA[<p>In traditional hand-drawn animation, the perspective view is not geometrically correct, unlike 3DCG. However, this perspective, which we may call <i>anime perspective</i>, is more natural for human eyes, especially for children. In this article, we present two anime perspective projection methods for seamlessly merging 3D models and traditional 2D animation. One is a view dependent deformer, <i>Anime-Pers deformer</i>, which offers functions, for example, of exaggerated scaling of foreground objects and of in-room view effect commonly found in hand-drawn animations. The other is <i>Anime-Lens shader</i>, which is based on a ray casting technique, is used for generating anime perspective images in a distant view.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891772</person_id>
				<author_profile_id><![CDATA[81335492772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katsura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39026528</person_id>
				<author_profile_id><![CDATA[81100085512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anjyo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258859</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Wood, D. N., Finkelstein, A., Hughes, J. F., Thayer, C. E., and Salesin, D. H. Multiperspective panoramas for cel animation. In <i>Proceedings of SIGGRAPH '97</i>, 243--250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278872</article_id>
		<sort_key>76</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Data-driven efficient production of cartoon character animation]]></title>
		<page_from>76</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278872</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278872</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we outline two new 3DCG technologies, <i>MoCaToon</i> and <i>AniFace</i>, which have been developed to improve the efficiency of the movie production process in the labor-intensive world of Japanese Anime, where much of the work is still done by hand. <i>MoCaToon</i> is a technology which enables motion capture systems to be used in the anime production process, and <i>AniFace</i> enables anime characters' lips to be synchronized with pre-scored voices. To verify the feasibility of these technologies, we reproduced the popular Japanese TV anime series, <i>The Galaxy Railways</i>, with 3D character animation utilizing both <i>MoCaToon</i> and <i>AniFace</i>. As a result of this experiment, it seems that the two technologies can be effectively used by animators to customize 3DCG characters according to their specific requirements, especially in terms of character style and motion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37023348</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P263739</person_id>
				<author_profile_id><![CDATA[81100252511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shigeru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuriyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050678</person_id>
				<author_profile_id><![CDATA[81100501237]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shinichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR SLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891763</person_id>
				<author_profile_id><![CDATA[81335498169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tadamichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891737</person_id>
				<author_profile_id><![CDATA[81335498241]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Masaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Trilogy Future Studio Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P648127</person_id>
				<author_profile_id><![CDATA[81100506136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Tatsuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yotsukura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR SLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40037941</person_id>
				<author_profile_id><![CDATA[81100584410]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Satoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR SLC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278873</article_id>
		<sort_key>77</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Sketching curves with immediate feedback]]></title>
		<page_from>77</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278873</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278873</url>
		<abstract>
			<par><![CDATA[<p>Computer-based drawing is important for applications ranging from sketching for illustration and design to signature capture for identity verification. While digital input devices typically provide a sequence of digitized points, applications often require input in the form of a parametric curve. Unfortunately, current methods for fitting curves to points can be complex, often require significant preprocessing of the digitized points, and can fail, especially when the path of the input points is complicated and self-intersecting. In addition, current methods typically require a full sequence of digitized points (e.g., all of the points recorded by a digital pen between pen-down and pen-up events) prior to determining any portion of the curve. This forces applications to draw an approximation of the curve until the full sequence is available (e.g., Adobe Illustrator draws the digitized points, while Microsoft's PowerPoint draws a polyline connecting the digitized points) and may result in a noticeable delay between the pen-up event and curve generation and/or a noticeable shape change when the generated curve replaces the approximation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P260194</person_id>
				<author_profile_id><![CDATA[81100502804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sarah]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Frisken]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tufts University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1044931</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{1} Ahn, <i>Least Squares Orthogonal Distance Fitting of Curves and Surfaces in Space</i>, Lecture Notes in Comp. Science, Springer-Verlag, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>709836</ref_obj_id>
				<ref_obj_pid>646873</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{2} Faugeras and Gomes, "Dynamic Shapes of Arbitrary Dimension: The Vector Distance Functions", Proc. IMA Conf. on Math. of Surfaces, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{3} Frisken, Perry, and Jones, "Detail-directed Hierarchical Distance Fields", U.S. Patent 6,396,492.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{4} Perry and Frisken, "Method and Apparatus for Rendering Cell-Based Distance Fields Using Texture Mapping", U.S. Patent 6,917,369.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>90941</ref_obj_id>
				<ref_obj_pid>90767</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{5} P. Schneider, "An Algorithm for Automatically Fitting Digitized Curves", in <i>Graphics Gems</i>, ed. Glassner, Academic Press, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278874</article_id>
		<sort_key>78</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Semiregular patterns on surfaces]]></title>
		<page_from>78</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278874</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278874</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14064111</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141993</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dong, S., Bremer, P.-T., Garland, M., Pascucci, V., and Hart, J. C. 2006. Spectral surface quadrangulation. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2006)</i> 25, 3, 1057--1066.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gr&#252;nbaum, B., and Shephard, G. C. 1987. <i>Tilings and Patterns</i>. W. H. Freeman.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1026043</ref_obj_id>
				<ref_obj_pid>1025128</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kaplan, M., Praun, E., and Cohen, E. 2004. Pattern oriented remeshing for Celtic decoration. In <i>Proceedings of the 12th Pacific Conference on Computer Graphics and Applications (PG 2004)</i>, 199--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882275</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Khodakovsky, A., Litke, N., and Schr&#246;der, P. 2003. Globally smooth parameterizations with low distortion. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2003)</i> 22, 3 (July), 350--357.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278875</article_id>
		<sort_key>79</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Combining computer vision and physics simulations using GPGPU]]></title>
		<page_from>79</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278875</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278875</url>
		<abstract>
			<par><![CDATA[<p>We present a system that uses the immense processing capabilities of graphics processors (GPUs) to enable a computer vision algorithm, such as stereo depth extraction, to drive a physics simulation in an interactive environment. This combination of processing has the potential to dramatically alter the way that people interact with computers through novel user interfaces and in interactive gaming.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37029096</person_id>
				<author_profile_id><![CDATA[81100566224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hensley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AMD Inc]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083421</person_id>
				<author_profile_id><![CDATA[81100365724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Isidoro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AMD Inc]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311052100</person_id>
				<author_profile_id><![CDATA[81540622156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Arcot]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Preetham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AMD Inc]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1249375</ref_obj_id>
				<ref_obj_pid>1249243</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Wang, L., Liao, M., Gong, M., Yang, R., and Nist&#233;r, D. 2006. High-quality real-time stereo using adaptive cost aggregation and dynamic programming. In <i>3DPVT</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278876</section_id>
		<sort_key>80</sort_key>
		<section_seq_no>17</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Spor(T)]]></section_title>
		<section_page_from>80</section_page_from>
	<article_rec>
		<article_id>1278877</article_id>
		<sort_key>80</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Fast object distribution]]></title>
		<page_from>80</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278877</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278877</url>
		<abstract>
			<par><![CDATA[<p>On Spore, we have a need to procedurally distribute game objects, subject to a number of constraints. As a starting point, the objects should positioned in a way that seems visually random, but be well spaced, as in a poisson disc distribution. There are other desirable characteristics, having multiple object types that don't overlap; naturally varying object scale, orientation and colour; control over the density of objects from some from of map, be it procedural, game-generated, or pre-authored. The problem domain is similar to that of [Ostromoukhov et al. 2004] and related papers, although our performance constraints are tighter.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39084256</person_id>
				<author_profile_id><![CDATA[81100159804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Willmott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>365104</ref_obj_id>
				<ref_obj_pid>355588</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Halton, J. H. 1964. Algorithm 247: Radical-inverse quasirandom point sequence. <i>Commun. ACM 7</i>, 12, 701--702.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015750</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ostromoukhov, V., Donohue, C., and Jodoin, P.-M. 2004. Fast hierarchical importance sampling with blue noise properties. In <i>SIGGRAPH'04: ACM SIGGRAPH 2004 Papers</i>, ACM Press, New York, NY, USA, 488--495.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278878</article_id>
		<sort_key>81</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Player-driven procedural texturing]]></title>
		<page_from>81</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278878</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278878</url>
		<abstract>
			<par><![CDATA[<p>In the video game Spore we make heavy use of player-driven procedural texturing. Our aim is to amplify the user's creativity by allowing them to control a powerful procedural texturing system. We attempt to strike a balance between giving the player a full painting interface (desirable for highly skilled artists, but tedious for others), and limiting them to simple compositions of preauthored textures. We solve this problem differently for our two model kinds: creatures, and buildings/vehicles.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35048817</person_id>
				<author_profile_id><![CDATA[81339496463]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[(grue)]]></middle_name>
				<last_name><![CDATA[DeBry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891718</person_id>
				<author_profile_id><![CDATA[81335490754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goffin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049011</person_id>
				<author_profile_id><![CDATA[81335491736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hecker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052636</person_id>
				<author_profile_id><![CDATA[81335496355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ocean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quigley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891757</person_id>
				<author_profile_id><![CDATA[81335497377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shalin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shodhan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088619</person_id>
				<author_profile_id><![CDATA[81100159804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Willmott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278879</article_id>
		<sort_key>82</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Creating spherical worlds]]></title>
		<page_from>82</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278879</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278879</url>
		<abstract>
			<par><![CDATA[<p>Our goal was to deliver procedurally created spherical planets for the game Spore, a considerable departure from our usual flat-world terrain approach. This presented three major problems: how to parameterize the planet's surface, how to create the surface itself, and how to texture the resulting geometry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891731</person_id>
				<author_profile_id><![CDATA[81335489037]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kate]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Compton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891720</person_id>
				<author_profile_id><![CDATA[81335490796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grieve]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891711</person_id>
				<author_profile_id><![CDATA[81335490746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052719</person_id>
				<author_profile_id><![CDATA[81335496355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ocean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quigley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891702</person_id>
				<author_profile_id><![CDATA[81335498297]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stratton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891713</person_id>
				<author_profile_id><![CDATA[81335498346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Todd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088649</person_id>
				<author_profile_id><![CDATA[81100159804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Willmott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278880</article_id>
		<sort_key>83</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Rigblocks]]></title>
		<subtitle><![CDATA[player-deformable objects]]></subtitle>
		<page_from>83</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278880</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278880</url>
		<abstract>
			<par><![CDATA[<p>We have built a system that allows the player to create various game models themselves, by assembling and deforming parts. This gives them compelling input into what their game world and avatar looks like. A key part of this system is what we term a <i>Rigblock</i>. A Rigblock is geometrical building block, representing a particular component of the model. For example, for a creature model the block may be a hand or mouth, for a vehicle, a wheel or jet engine. Physical analogues include Lego and mechano sets and their various parts.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891736</person_id>
				<author_profile_id><![CDATA[81335489396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lydia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891754</person_id>
				<author_profile_id><![CDATA[81335491890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ingram]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052627</person_id>
				<author_profile_id><![CDATA[81335496355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ocean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Quigley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891697</person_id>
				<author_profile_id><![CDATA[81540484456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sharp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088655</person_id>
				<author_profile_id><![CDATA[81100159804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Willmott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278881</article_id>
		<sort_key>84</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Improving real-time motion]]></title>
		<page_from>84</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278881</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278881</url>
		<abstract>
			<par><![CDATA[<p>Achieving believable motion in real-time is one of the biggest challenges faced by the video game industry. The interactive nature of the medium, combined with the need for hyper responsivity leads to consistently compromised motion. This is particularly common in titles with large numbers of characters that are required to carry out specific tasks while interacting with others.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35046814</person_id>
				<author_profile_id><![CDATA[81367594703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Armstrong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278882</section_id>
		<sort_key>85</sort_key>
		<section_seq_no>18</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Image is everything]]></section_title>
		<section_page_from>85</section_page_from>
	<article_rec>
		<article_id>1278883</article_id>
		<sort_key>85</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Digital restoration of moldy aged films]]></title>
		<page_from>85</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278883</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278883</url>
		<abstract>
			<par><![CDATA[<p>Film is regarded as an important art form, often reflecting the culture from which it is stemmed. Films record our history, represent contemporary culture and have great artistic value. Thus, they are precious cultural assets. Unfortunately, because of aging, improper storage conditions and other reasons, old films are threaten with defects caused by decaying, dust, dirt, scratch and mold. Consequently, digital film restoration, repairing defects in films, has been recognized as an important issue by archives, content owners and film companies. This paper proposes a learning-based defect detection method and a flow-based defect repairing algorithm for greatly reducing manual efforts in film restoration. The main contributions include a novel example-based approach for defect detection and a restoration algorithm which can repair seriously damaged films.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891699</person_id>
				<author_profile_id><![CDATA[81452602075]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chieh-Ju]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049598</person_id>
				<author_profile_id><![CDATA[81335490937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shuen-Huei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digimax Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P308194</person_id>
				<author_profile_id><![CDATA[81350582710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yung-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311172300</person_id>
				<author_profile_id><![CDATA[81538568156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jiann-Rong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digimax Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023797</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P201264</person_id>
				<author_profile_id><![CDATA[81100319756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ouhyoung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1153494</ref_obj_id>
				<ref_obj_pid>1153170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shiratori, T., Matsushita, Y., Tang, X., and Kang, S. B. 2006. Video completion by motion field transfer. In <i>Proceedings of CVPR 2006</i>, 411--418.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>966458</ref_obj_id>
				<ref_obj_pid>966432</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Viola, P., and Jones, M. J. 2004. Robust real-time face detection. <i>International Journal of Computer Vision 57</i>, 2, 137--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278884</article_id>
		<sort_key>86</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A texture synthesis approach to elastica inpainting]]></title>
		<page_from>86</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278884</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278884</url>
		<abstract>
			<par><![CDATA[<p>We present a new, fully automatic technique for wire and scratch removal (inpainting) that works well in both textured and non-textured areas of an image. [Chan et al. 2002] introduced a technique for inpainting using an Euler's elastica energy-based variational model that works well for repairing smooth areas of the image while maintaining edge detail. The technique is very slow (due to a stiff, 4th order PDE) and difficult to control. [Efros and Leung 1999] used texture synthesis techniques for inpainting and hole filling. This works well for areas of an image that contain repeating patterns.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891730</person_id>
				<author_profile_id><![CDATA[81414610103]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kangyu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCLA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052809</person_id>
				<author_profile_id><![CDATA[81100308346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39087628</person_id>
				<author_profile_id><![CDATA[81100178322]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCLA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chan, T., Kang, S., and Shen, J. 2002. Euler's elastica and curvature based inpaintings. <i>J. Appl. Math.</i> in press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Efros, A. A., and Leung, T. K. 1999. Texture synthesis by non-parametric sampling. In <i>IEEE International Conference on Computer Vision</i>, 1033--1038.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278885</article_id>
		<sort_key>87</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Image morphing for space-time interpolation]]></title>
		<page_from>87</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278885</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278885</url>
		<abstract>
			<par><![CDATA[<p>The human brain automatically attempts to interpret the physical visual inputs from our eyes in terms of plausible motion of the viewpoint and/or of the observed object or scene [Ellis 1938; Graham 1965; Giese and Poggio 2003]. In the physical world, the rules that define plausible motion are set by temporal coherence, parallax, and perspective projection. Our brain, however, refuses to feel constrained by the unrelenting laws of physics in what it deems plausible motion. Image metamorphosis experiments, in which unnatural, impossible in-between images are interpolated, demonstrate that under certain circumstances, we willingly accept chimeric images as plausible transition stages between images of actual, known objects [Beier and Neely 1992; Seitz and Dyer 1996]. Or think of cartoon animations which for the longest time were hand-drawn pieces of art that didn't need to succumb to physical correctness. The goal of our work is to exploit this freedom of perception for space-time interpolation, i.e., to generate transitions between still images that our brain accepts as plausible motion in a moving 3D world.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP36037793</person_id>
				<author_profile_id><![CDATA[81330498865]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CG Lab, TU Braunschweig, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044695</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CG Lab, TU Braunschweig, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Beier, T., and Neely, S. 1992. Feature-based image metamorphosis. In <i>Proceedings of SIGGRAPH'92</i>, Chicago, ACM, 35--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ellis, W., Ed. 1938. <i>A Source Book of Gestalt Psychology</i>. Kegan Paul, Trench, Trubner &amp; Co. Ltd.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Giese, M., and Poggio, T. 2003. Neural mechanisms for the recognition of biological movements. <i>Nature Reviews -- Neuroscience 4</i> (Mar.), 179--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Graham, C. 1965. <i>Vision and Visual Perception</i>. New York: Wiley, ch. Perception of movement.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Seitz, S. M., and Dyer, C. R. 1996. View morphing. In <i>Proceedings of SIGGRAPH'96</i>, New Orleans, ACM, 21--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278886</article_id>
		<sort_key>88</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Stabilizing video while keeping resolution and capturing intention]]></title>
		<page_from>88</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278886</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278886</url>
		<abstract>
			<par><![CDATA[<p>Annoying shaky motion is one of the significant problems in home videos, since hand shake is an unavoidable effect when capturing by using a hand-held camcorder. Video stabilization is an important technique to solve this problem. However, the stabilized videos resulted by current methods usually have decreased resolution and are still not so stable. In this sketch, we propose a novel, robust, and practical method of video stabilization while considering users' capturing intention. Our method can produce full-frame stabilized videos, and not only the high frequency shaky motions but also the low frequency unexpected movements are removed. To guess the user's capturing intention, we first consider the regions of interest (ROI) in the video to estimate which regions or objects the user wants to capture, and then use a polyline to estimate a new stable camcorder motion path while avoiding the user's interested regions being cut out. Then, we fill the dynamic and static missing areas caused by frame alignment from other frames to keep the same resolution and quality as the original video. Furthermore, we smooth the discontinuous regions by using a 3D Poisson-based method. After the above automatic operations, a full-frame stabilized video can be achieved and the important regions can also be preserved.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37023797</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891727</person_id>
				<author_profile_id><![CDATA[81335493884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jong-Shan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891769</person_id>
				<author_profile_id><![CDATA[81335492103]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wei-Ting]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1137553</ref_obj_id>
				<ref_obj_pid>1137249</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Matsushita, Y., Ofek, E., Ge, W., Tang, X., and Shum, H.-Y. 2006. Full-frame video stabilization with motion inpainting. <i>IEEE TPAMI 28</i>, 7, 1150--1163.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1180824</ref_obj_id>
				<ref_obj_pid>1180639</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zhai, Y., and Shah, M. 2006. Visual attention detection in video sequences using spatiotemporal cues. In <i>Prof. ACM MM 06'</i>, 815--824.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278887</article_id>
		<sort_key>89</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Improving real-time motion]]></title>
		<page_from>89</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278887</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278887</url>
		<abstract>
			<par><![CDATA[<p>Achieving believable motion in real-time is one of the biggest challenges faced by the video game industry. The interactive nature of the medium, combined with the need for hyper responsivity leads to consistently compromised motion. This is particularly common in titles with large numbers of characters that are required to carry out specific tasks while interacting with others.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35046799</person_id>
				<author_profile_id><![CDATA[81367594703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Armstrong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1278888</section_id>
		<sort_key>90</sort_key>
		<section_seq_no>19</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Go with the flow]]></section_title>
		<section_page_from>90</section_page_from>
	<article_rec>
		<article_id>1278889</article_id>
		<sort_key>90</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[End of the world waterfall setup for "Pirates of the Caribbean 3"]]></title>
		<page_from>90</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278889</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278889</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051926</person_id>
				<author_profile_id><![CDATA[81320494322]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891766</person_id>
				<author_profile_id><![CDATA[81335490012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dufor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35054446</person_id>
				<author_profile_id><![CDATA[81320496743]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zalzala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891748</person_id>
				<author_profile_id><![CDATA[81543472156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lambert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049506</person_id>
				<author_profile_id><![CDATA[81100036373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kapler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Peterson, S. 2003. Building a waterfall 1,000 particles at a time for "Shrek 4d". <i>ACM Trans. Graph.</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278890</article_id>
		<sort_key>91</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Wave displacement effects for <i>Surf's Up</i>]]></title>
		<page_from>91</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278890</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278890</url>
		<abstract>
			<par><![CDATA[<p>For <i>Surf's Up</i>, our challenge was to create realistic waves for an all-CG surfing mockumentary. The surfing waves were designed to be character animated, and it was the job of the effects animation team to ensure that the rest of the water effects combined seamlessly with the surfing waves. These included ocean surface displacement effects as well as particle effects for spray, splashes, drips, and white water. This sketch focuses specifically on the various surface displacement effects that were used in the film. The wave displacement effects can be generally categorized into ambient ocean procedural displacements, surfing wave-specific procedural displacements, and character-driven interactive water displacements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35048199</person_id>
				<author_profile_id><![CDATA[81539810356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Deborah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carlson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>545288</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hinsinger, D., Neyret, F., and Cani, M.-P. 2002. Interactive Animation of Ocean Waves. In <i>ACM SIGGRAPH Symposium on Computer Animation</i>, ACM Press, 161--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tessendorf, J. 2004. Interactive Water Surfaces. In <i>Game Programming Gems 4</i>, ed. A. Kirmse, Charles River Media, 265--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278891</article_id>
		<sort_key>92</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[<i>Surf's Up</i> beach break]]></title>
		<page_from>92</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278891</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278891</url>
		<abstract>
			<par><![CDATA[<p>When joining the <i>Surf's Up</i> team, we were asked do develop a solution for shore break -- or <b>beach break</b> -- for the 2 beaches featured in the movie. It soon became clear that the beach break would be visible in many shots (about 70). For that reason we envisioned a highly procedural setup, ideally allowing a TD to generate beach break for a shot in less than an hour. And this without overloading the Imageworks renderfarm in the process.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891767</person_id>
				<author_profile_id><![CDATA[81335493126]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kluyskens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Tomczak 2005. Oceanography Lecture Notes. http://gyre.umeoce.maine.edu/physicalocean/Tomczak/IntroOc/lec ture09.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278892</article_id>
		<sort_key>93</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Making waves for Surf's Up]]></title>
		<page_from>93</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278892</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278892</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051852</person_id>
				<author_profile_id><![CDATA[81442607798]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047199</person_id>
				<author_profile_id><![CDATA[81335488469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bredow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049676</person_id>
				<author_profile_id><![CDATA[81335492933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kramer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891738</person_id>
				<author_profile_id><![CDATA[81335491153]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hausman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891749</person_id>
				<author_profile_id><![CDATA[81474690369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinners]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048336</person_id>
				<author_profile_id><![CDATA[81539810356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Deborah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carlson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047565</person_id>
				<author_profile_id><![CDATA[81474686458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Clark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278893</article_id>
		<sort_key>94</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Simulation, simulation, simulation]]></title>
		<subtitle><![CDATA[integration of multiple simulation techniques to create realistic water motion in order to maintain highest level of flexibility]]></subtitle>
		<page_from>94</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278893</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278893</url>
		<abstract>
			<par><![CDATA[<p>Over the last couple of years, Rhythm and Hues Studios developed various fluid simulations techniques, which can be combined in a flexible process to maximize the ability to manipulate and art direct each step. This workflow was successfully developed and applied on recent feature work such as <i>"Superman Returns"</i> and <i>"Happy Feet"</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050008</person_id>
				<author_profile_id><![CDATA[81328489165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurtz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052881</person_id>
				<author_profile_id><![CDATA[81100445884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jerry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tessendorf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Tessendorf, <i>"Simulating Ocean Surfaces"</i>, Course Notes, Siggraph 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Tessendorf, <i>"Interactive Water Surfaces"</i>, Game Programming Gems 4, Charles River Media, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1278894</article_id>
		<sort_key>95</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[300's liquid battlefield]]></title>
		<subtitle><![CDATA[fluid simulation Spartan style]]></subtitle>
		<page_from>95</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1278780.1278894</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1278894</url>
		<abstract>
			<par><![CDATA[<p>This sketch will give insight into the methods Scanline used for creating the liquid battlefield scenes of Zach Snyder's 300. From massive parallel network simulation to rendering, we'll explore the innovative techniques that made it possible to bring these scenes to life with a relatively small team and a short production schedule.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35054065</person_id>
				<author_profile_id><![CDATA[81100604837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trojansky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Flowline, ScanlineVFX]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891765</person_id>
				<author_profile_id><![CDATA[81335490470]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ganshorn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Flowline, ScanlineVFX]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891745</person_id>
				<author_profile_id><![CDATA[81335496146]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pilarski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Flowline, ScanlineVFX]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
</content>
</proceeding>
