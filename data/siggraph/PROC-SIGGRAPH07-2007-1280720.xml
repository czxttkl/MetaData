<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/05/2007</start_date>
		<end_date>08/09/2007</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[San Diego]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1280720</proc_id>
	<acronym>SIGGRAPH '07</acronym>
	<proc_desc>ACM SIGGRAPH 2007 posters</proc_desc>
	<conference_number>2007</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-1828-0</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2007</copyright_year>
	<publication_date>08-05-2007</publication_date>
	<pages>197</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Graphic displays of incremental, preliminary, partial, and innovative insights that are important but not fully developed. Posters are displayed throughout the conference week, and presenters discuss their work in scheduled sessions.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>PP40025189</person_id>
			<author_profile_id><![CDATA[81100235480]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Marc]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Alexa]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Technische Universitat Berlin]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>P10635</person_id>
			<author_profile_id><![CDATA[81100576882]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>2</seq_no>
			<first_name><![CDATA[Adam]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Finkelstein]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Princeton University]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2007</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<section>
		<section_id>1280721</section_id>
		<sort_key>1</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Behavior]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>1280722</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Designing and implementing knowledge bases for narrative animations system]]></title>
		<page_from>1</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280722</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280722</url>
		<abstract>
			<par><![CDATA[<p>The current technology such as semantic web and ontology allows everyone to build rapidly vast knowledge bases for a specific domain. A development of story animation knowledge bases is presented. The knowledge bases are used to produce movie animation given a story. Databases are expressed in the form of ontology by extracting textual entities and parsing each sentence of the story. Rules are obtained by analyzing the process of manually produced movie animation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052733</person_id>
				<author_profile_id><![CDATA[81100643163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramamonjisoa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Iwate Prefectural University, Iwate, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383316</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Coyne, B and Sproat, R. <i>Wordseye: an automatic text-to-scene conversion system</i>. In proceedings of SIGGRAPH, 2001, 487--496.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Takahashi, N., Ramamonjisoa D., Ogata T. <i>A Tool for Supporting an Animated Movie Making Based on Writing Story in XML</i>. In Proceedings of Applied Computing, 2007, 405--409.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sowa J. <i>Knowledge Representation</i>, Brooks/Cole Publisher, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975621</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brachman, J. R. and Levesque, H. J. <i>Knowledge Representation and Reasoning</i>, Elsevier Publisher, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280723</article_id>
		<sort_key>2</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Synaesthetics]]></title>
		<page_from>2</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280723</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280723</url>
		<abstract>
			<par><![CDATA[<p>Synasthetics is a 3D random shape generator software which intends to investigate the possible connections of the textual and the visual and to use these connections to generate visuals which are user - specific and even personal.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893121</person_id>
				<author_profile_id><![CDATA[81421599374]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emrah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kavlak]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sabanci University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893109</person_id>
				<author_profile_id><![CDATA[81335498158]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Damla]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sabanci University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280724</article_id>
		<sort_key>3</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Toward real-time object manipulation in dynamic environments]]></title>
		<page_from>3</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280724</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280724</url>
		<abstract>
			<par><![CDATA[<p>In most interactive applications of virtual characters such as computer games and VR training, existing motion planners are not capable of computing grasping motions in real time, especially in realistic dynamic environments. This work proposes a learning-based motion planner that achieves improved performance by integrating the ability to reuse learned reaching and grasping skills on new tasks. Extensive tests show that our approach, the <b>Attractor-Guided Planner</b> (AGP), greatly improves the planning time and solution quality when comparing to traditional sampling-based motion planners.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893242</person_id>
				<author_profile_id><![CDATA[81335492308]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiaoxi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Merced]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050580</person_id>
				<author_profile_id><![CDATA[81100215070]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kallmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Merced]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kallmann, M. 2005. Scalable solutions for Interactive Virtual Humans that can Manipulate Objects. In AIIDE 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kuffner, J. J., and Lavalle, S. M. 2000. RRT-Connect: An Efficient Approach to Single-Query Path Planning. In ICRA 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280725</article_id>
		<sort_key>4</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Emergent geometry]]></title>
		<subtitle><![CDATA[procedural modeling through behavior]]></subtitle>
		<page_from>4</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280725</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280725</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280726</article_id>
		<sort_key>5</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Robot gaming and learning using augmented reality]]></title>
		<page_from>5</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280726</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280726</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822865</person_id>
				<author_profile_id><![CDATA[81319494845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mykhaylo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kostandov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893160</person_id>
				<author_profile_id><![CDATA[81335497111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schwertfeger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P211470</person_id>
				<author_profile_id><![CDATA[81100598466]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Odest]]></first_name>
				<middle_name><![CDATA[Chadwicke]]></middle_name>
				<last_name><![CDATA[Jenkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034184</person_id>
				<author_profile_id><![CDATA[81319494596]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Radu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jianu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047069</person_id>
				<author_profile_id><![CDATA[81414606955]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048802</person_id>
				<author_profile_id><![CDATA[81553061456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hartmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050407</person_id>
				<author_profile_id><![CDATA[81335494161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loper]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893090</person_id>
				<author_profile_id><![CDATA[81335498948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Aggeliki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsoli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893191</person_id>
				<author_profile_id><![CDATA[81335499010]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Marek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vondrak]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18012696</person_id>
				<author_profile_id><![CDATA[81319505859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Wenjin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15027670</person_id>
				<author_profile_id><![CDATA[81100261870]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council of Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Collett, T. H., MacDonald, B. A., and Gerkey, B. P. 2005. Player 2.0: Toward a practical robot programming framework. In <i>Proc. ACRA'05</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1069138</ref_obj_id>
				<ref_obj_pid>1068508</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fiala, M. 2005. Artag, a fiducial marker system using digital techniques. In <i>Proc. CVPR'05</i>, vol. 1, 590--596.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1228758</ref_obj_id>
				<ref_obj_pid>1228716</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Young, J., Xin, M., and Sharlin, E. 2007. Robot expressionism through cartooning. <i>ACM SIGCHI/SIGART Human-Robot Interaction</i>, 309--316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280727</section_id>
		<sort_key>6</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[GPU / Hardware]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>1280728</article_id>
		<sort_key>6</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Implementing wave particles for real-time water waves with object interaction]]></title>
		<page_from>6</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280728</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280728</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35034261</person_id>
				<author_profile_id><![CDATA[81319505055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cem]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yuksel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044760</person_id>
				<author_profile_id><![CDATA[81100479080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[House]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39087974</person_id>
				<author_profile_id><![CDATA[81100431503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keyser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280729</article_id>
		<sort_key>7</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Saving the z-cull optimization]]></title>
		<page_from>7</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280729</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280729</url>
		<abstract>
			<par><![CDATA[<p>Modern graphics hardware employs <i>z-culling</i>, or <i>early-z culling</i>, as a conservative z-test before the actual per-fragment z-test [Kilgariff and Fernando 2005]. Z-culling is used to avoid executing expensive fragment shaders for invisible fragments, and minimize reading, testing, and updating z-buffer entries. However, these optimizations need to be disabled for shaders that modify the z value of a fragment (<i>depth replace</i>). Such shaders are rapidly gaining in importance, however, especially in the context of per-pixel displacement mapping such as relief mapping [Policarpo et al. 2005]. We propose a slight modification to graphics APIs and hardware drivers that would allow to retain many of the z-cull optimizations for a large class of depth-modifying shaders, including displacement shaders. The basic idea is for shaders to specify a non-negative depth offset that is added to the z value of the fragment. While the exact z value is not known before the shader is executed, the lower bound is sufficient to perform all z-culling tests. Updates to the z-cull buffer are limited, but not worse than with texkill shaders.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051506</person_id>
				<author_profile_id><![CDATA[81100514766]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mantler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VRVis Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39087859</person_id>
				<author_profile_id><![CDATA[81100644757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadwiger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VRVis Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1283917</ref_obj_id>
				<ref_obj_pid>1283900</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hasselgren, J., and Akenine-M&#246;ller, T. 2006. Efficient Depth Buffer Compression. In <i>Proceedings of Graphics Hardware 2006</i>, 103--110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kilgariff, E., and Fernando, R. 2005. The GeForce 6 Series GPU Architecture. In <i>GPU Gems 2</i>, Addison-Wesley Professional, M. Pharr, Ed., 471--491.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053453</ref_obj_id>
				<ref_obj_pid>1053427</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Policarpo, F., Oliveira, M. M., and Comba, J. L. D. 2005. Real-time relief mapping on arbitrary polygonal surfaces. In <i>Proceedings of 2005 Symposium on Interactive 3D Graphics and Games</i>, 155--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280730</article_id>
		<sort_key>8</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A GPU interpolating reconstruction from unorganized points]]></title>
		<page_from>8</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280730</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280730</url>
		<abstract>
			<par><![CDATA[<p>We present a GPU method for surface reconstruction from unorganized point clouds without additional information, based on the work of [Gopi et al. 2000]. The main objective of this work is the generation of a GPU interpolating reconstruction method by using local Delaunay triangulations. Existing algorithms accelerated by graphic hardware are approximating approaches, usually based on either marching cubes or marching tetrahedras. Our work most valuable innovation is the GPU adaptation itself; we developed GPU suitable methods to replace those steps which could not to be implemented in graphics hardware. The two most noticeable ones are the O(log(k)) method which identifies Delaunay neighbors (replacing the O(k) backtracking) and the sorting of a great number of small lists in O(log2(k)). Additionally, a divide and conquer approach was adopted to suit video memory constrains; passes are then independent, so multi-GPU parallelization can be performed without changes. Our tests show an 11 times average gain over the CPU version.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893101</person_id>
				<author_profile_id><![CDATA[81335488387]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buchart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CEIT and Tecnun (University of Navarra)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P701521</person_id>
				<author_profile_id><![CDATA[81100006378]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Borro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CEIT and Tecnun (University of Navarra)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P701497</person_id>
				<author_profile_id><![CDATA[81100389147]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Aiert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Amundarain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CEIT and Tecnun (University of Navarra)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gopi, M., Krishnan, S., and Silva, C. T. 2000. Surface reconstruction based on lower dimensional localized delaunay triangulation. <i>Eurographics 19</i>, 3, 467--478.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280731</article_id>
		<sort_key>9</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[GPU accelerated SPH particle simulation and rendering]]></title>
		<page_from>9</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280731</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280731</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P527321</person_id>
				<author_profile_id><![CDATA[81319505564]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yanci]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P735220</person_id>
				<author_profile_id><![CDATA[81100111111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Solenthaler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39057298</person_id>
				<author_profile_id><![CDATA[81100087869]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Renato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pajarola]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Monaghan, J. 1992. Smoothed particle hydrodynamics. <i>Annu. Rev. Astron. Astrophys. 30</i>, 543--574.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1236002</ref_obj_id>
				<ref_obj_pid>1235887</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zhang, Y., and Pajarola, R. 2007. Deferred blending: Image composition for single-pass point rendering. <i>Computers &amp; Graphics 31</i>, 2, 175--189.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280732</article_id>
		<sort_key>10</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[<i>Anywhere</i> pixel compositor]]></title>
		<page_from>10</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280732</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280732</url>
		<abstract>
			<par><![CDATA[<p>Even with the recent rapid advancement in hardware, the demand from high-end graphics applications (including video games) seems to always outpace the capability that a single GPU can offer. Graphics hardware vendors are now offering dual or even quad GPU configurations (such as NVIDIA's SLI and ATI's CrossFire technology). As we migrate from a single GPU to multiple GPUs or eventually GPU clusters, how to effectively assemble the final image from these distributed rendering nodes becomes an important issue. Here we propose to develop a flexible pixel compositor to solve this problem. Our compositor is capable of performing an arbitrary mapping of pixels from any input frame to any output frame, and executing typical composition operations at the same time. Figure 1(a) shows a schematic of our design. The pixels are transmitted digitally. The core mapping and arithmetic operations are carried out by a programmable FPGA chip. It is connected to a large memory bank that stores both the mapping information and temporary frames (if necessary). A single compositor unit has four input links and four outputs. Multiple units can be arranged in a network, shown in Figure 1(b), to achieve the scalability for large clusters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P250287</person_id>
				<author_profile_id><![CDATA[81409593896]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ruigang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P20148</person_id>
				<author_profile_id><![CDATA[81100264426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anselmo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lastra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cavin, X., Mion, C., and Filbois, A. 2005. Cots cluster-based sort-last rendering: Performance evaluation and pipelined implementation. In <i>Proceedings of IEEE Visualization</i>, 15--23.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383273</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stoll, G., Eldridge, M., Patterson, D., Webb, A., Berman, S., Levy, R., Caywood, C., Taveira, M., Hunt, S., and Hanrahan, P. 2001. Lightning-2: a high-performance display subsystem for pc clusters. In <i>Proceedings of SIGGRAPH 2001</i>, 141--148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280733</article_id>
		<sort_key>11</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Hardware accelerated broad phase collision detection]]></title>
		<page_from>11</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280733</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280733</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893198</person_id>
				<author_profile_id><![CDATA[81335499475]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Muiris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Woulfe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Trinity College Dublin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P339499</person_id>
				<author_profile_id><![CDATA[81100626480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dingliana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Trinity College Dublin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034222</person_id>
				<author_profile_id><![CDATA[81319497258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manzke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Trinity College Dublin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280734</article_id>
		<sort_key>12</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Metatextures]]></title>
		<subtitle><![CDATA[a brief introduction]]></subtitle>
		<page_from>12</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280734</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280734</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893105</person_id>
				<author_profile_id><![CDATA[81335495516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oates]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. (2004). <u>Implementing Improved Perlin Noise</u>. GPU Gems, 73--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280735</section_id>
		<sort_key>13</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Geometry]]></section_title>
		<section_page_from>13</section_page_from>
	<article_rec>
		<article_id>1280736</article_id>
		<sort_key>13</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Feature-based subdivision surface fitting]]></title>
		<page_from>13</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280736</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280736</url>
		<abstract>
			<par><![CDATA[<p>Obtaining optimal and succinct representations for 3D models, usually defined as <i>redundant</i> dense polygonal meshes, is particularly of interest for many applications: animation, compression, recognition or understanding. Subdivision surfaces combine a lot of properties particularly relevant for this task: this model is very <i>compact</i>, can represent an <i>arbitrary topology</i>, authorizes a <i>local control</i> while being intrinsically <i>multi-resolution</i>. For these reasons, subdivision surfaces are increasingly popular in computer graphics and have been integrated to the MPEG4 standard. In this context, approximating a dense verbose polygonal mesh with this model becomes even more relevant.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35023835</person_id>
				<author_profile_id><![CDATA[81100081967]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Guillaume]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lavou&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INSA-Lyon]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P676558</person_id>
				<author_profile_id><![CDATA[81100487642]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Florent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dupont]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#233; Lyon]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015817</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cohen-Steiner, D., Alliez, P., and Desbrun, M. 2004. Variational shape approximation. In <i>ACM Siggraph</i>, 905--914.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DeRose, T., Kass, M., and Truong, T. 1998. Subdivision surfaces in character animation. In <i>ACM Siggraph</i>, 85--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>718484</ref_obj_id>
				<ref_obj_pid>647260</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kanai, T. 2001. Meshtoss:converting subdivision surfaces from dense meshes. In <i>Vision Modeling and Visualization</i>, 325--332.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lavou&#233;, G., Dupont, F., and Baskurt, A. 2007. A framework for quad/triangle subdivision surface fitting: Application to mechanical objects. <i>Computer Graphics Forum 26</i>, 1, 1--14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1120644</ref_obj_id>
				<ref_obj_pid>1120639</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Marinov, M., and Kobbelt, L. 2005. Optimization methods for scattered data approximation with subdivision surfaces. <i>Journal of Graphical Models 67</i>, 5, 452--473.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280737</article_id>
		<sort_key>14</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Part analogies of 3D objects]]></title>
		<page_from>14</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280737</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280737</url>
		<abstract>
			<par><![CDATA[<p>Geometric manipulation of 3D objects can benefit from analogies among similar shapes and parts. However, analogies that are easily found by humans are difficult to detect and maintain in algorithms that manipulate 3D meshes. One of the reasons for this is that such analogies are perceived between 3D volumetric parts of the objects whereas models defined by mesh surfaces are boundary representations of the object. The shape-diameter function (SDF) maps volumetric information of the shape onto the boundary mesh and provides a link between the two. This function is related to the medial axis transform, measuring the local diameter of the object at points on its boundary. The SDF remains largely oblivious to pose changes of the same object and maintains similar values in analogue parts of different objects. By using the SDF attribute of an object we are able to find similarities between object parts, even when the topology or structure of the objects are different.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052143</person_id>
				<author_profile_id><![CDATA[81335497317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lior]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shapira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel-Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35023786</person_id>
				<author_profile_id><![CDATA[81100081895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ariel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shamir]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interdisciplinary Center, Herzliya]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40030280</person_id>
				<author_profile_id><![CDATA[81100264399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen-Or]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel-Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280738</article_id>
		<sort_key>15</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Geometry sequence based progressive mesh compression]]></title>
		<page_from>15</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280738</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280738</url>
		<abstract>
			<par><![CDATA[<p>Nowadays computer graphics applications are widely applied in various fields. Due to the limitations of storage, transmission or display constraints, the compression, especially progressive compression of 3D meshes, is becoming a more and more urgent research topic. A lot of schemes have been proposed for compressing progressive meshes. However, there is still no common framework or international standard of compressing mesh, which restricts the application of meshes. In comparison, 2D video compression is a well-studied topic over the past decades and many international standards like MPEG 2, MPEG 4, have already been widely accepted by the industry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893156</person_id>
				<author_profile_id><![CDATA[81335499790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jiheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Beijing University Of Technology, Beijing, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P27497</person_id>
				<author_profile_id><![CDATA[81100408791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Baocai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Beijing University Of Technology, Beijing, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35029579</person_id>
				<author_profile_id><![CDATA[81100448623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yanfeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Beijing University Of Technology, Beijing, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P561573</person_id>
				<author_profile_id><![CDATA[81100394445]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dehui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Beijing University Of Technology, Beijing, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566589</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gu, X., Gortler, S. J., and Hoppe, H. 2002. Geometry images. In <i>ACM SIGGRAPH 2002</i>, 355--361.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280739</article_id>
		<sort_key>16</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Eureka]]></title>
		<subtitle><![CDATA[Euler spiral splines]]></subtitle>
		<page_from>16</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280739</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280739</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050211</person_id>
				<author_profile_id><![CDATA[81100417436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Raph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P41116</person_id>
				<author_profile_id><![CDATA[81100058395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carlo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[S&#233;quin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280740</article_id>
		<sort_key>17</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Leather texture generation considering sulci flow]]></title>
		<page_from>17</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280740</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280740</url>
		<abstract>
			<par><![CDATA[<p>This paper introduces a new method for generating leather texture by means of cell simulation considering sulci flow. The cell simulation is based on an analogy of cracks on drying or aging surface. The method uses a mass-spring model for generating sulci flow on a surface of leather. A user can specify a sulcus shape by applying a radial basis function (RBF) for its profile.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893169</person_id>
				<author_profile_id><![CDATA[81335497014]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kaisei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakurai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051949</person_id>
				<author_profile_id><![CDATA[81100338478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311141700</person_id>
				<author_profile_id><![CDATA[81100533332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP400055300</person_id>
				<author_profile_id><![CDATA[81319497380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsufuji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280741</article_id>
		<sort_key>18</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Surface-based deformation for disconnected mesh models]]></title>
		<page_from>18</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280741</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280741</url>
		<abstract>
			<par><![CDATA[<p>Surface-based deformation [Sorkine 2005] plays an important role to reuse existing mesh models. This technique encodes geometric shapes using linear partial differential equations and deforms mesh models in an interactive manner. However, surface-based deformation cannot consistently deform mesh models that include (1) Multiple disconnected components, (2) T-vertices, and (3) Non-manifold edges. These conditions commonly appear in mesh models. This paper shows how to deform such models consistently.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893180</person_id>
				<author_profile_id><![CDATA[81335495700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ogawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050668</person_id>
				<author_profile_id><![CDATA[81100610717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masuda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[O. Sorkine 2005. Laplacian Mesh Proceeding. <i>Eurographics STAR State-of-the-Art Report</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1221829</ref_obj_id>
				<ref_obj_pid>1221585</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H. Masuda, Y. Yoshioka, Y. Furukawa, 2006, Interactive mesh deformation using equality-constrained least squares, <i>Computers and Graphics</i>, Vol.30, No.6, 936--946.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280742</article_id>
		<sort_key>19</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Interactive modeling for augmented reality]]></title>
		<page_from>19</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280742</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280742</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35048317</person_id>
				<author_profile_id><![CDATA[81335490460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Russell]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Freeman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37028378</person_id>
				<author_profile_id><![CDATA[81100501447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anthony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Taylor, C., and Malik, J. 1996. Modelling and Rendering Architecture from Photographs. In proceedings of SIGGRAPH 96, 11--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946792</ref_obj_id>
				<ref_obj_pid>946248</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lepetit, V., Vacchetti, L., Thalmann, D., and Fua, P. 2003. Fully Automated and Stable Registration for Augmented Reality Applications. In ISMAR, 93--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280743</article_id>
		<sort_key>20</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Shape-preserving gray-scale skeletonization on 3D density maps]]></title>
		<page_from>20</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280743</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280743</url>
		<abstract>
			<par><![CDATA[<p>The medial surface that lies within a solid object, better known as the <i>skeleton</i>, is a local shape descriptor which is capable of capturing the shape and topological properties of a complex object. Due to its simple, yet informational nature, the skeleton shape descriptor has been widely used in graphics and computer vision for matching purposes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P860657</person_id>
				<author_profile_id><![CDATA[81331487943]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sasakthi]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Abeysinghe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35039630</person_id>
				<author_profile_id><![CDATA[81100098726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Couprie, M., Bezerra, F. N., and Bertrand, G. 2001. Topological operators for grayscale image processing. <i>Journal of Electronic Imaging 10</i>, 4, 1003--1015.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2164651</ref_obj_id>
				<ref_obj_pid>2164628</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ju, T., Baker, M., and Chiu, W. 2006. Computing a family of skeletons of volumetric models for shape description. In <i>Proceedings of Geometric Modeling and Processing</i>, 235--247.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280744</article_id>
		<sort_key>21</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[4-D/n-D computer aided design]]></title>
		<page_from>21</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280744</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280744</url>
		<abstract>
			<par><![CDATA[<p>In industrial design field, designers draw sketches in 2-D, while their final products must be defined in 3-D. (Very few designers directly make 3-D models.) How come they prefer 2-D to 3-D? They are lacking ability of imaging 3-D models? No. Definitely not.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP18006437</person_id>
				<author_profile_id><![CDATA[81452606695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893248</person_id>
				<author_profile_id><![CDATA[81335495604]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ogata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893177</person_id>
				<author_profile_id><![CDATA[81335492852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kazuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280745</article_id>
		<sort_key>22</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Fast and accurate estimation of principal curvatures and directions for morphable models]]></title>
		<page_from>22</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280745</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280745</url>
		<abstract>
			<par><![CDATA[<p>Recent systems for 3D recognition and rendering have made use of principal curvatures and directions to express local surface shape variation independent of position or parameterization. These descriptors of the directions in which a surface bends and the degree to which this bending occurs are the basis for algorithms in such widely-varying applications as non-photorealistic rendering and surface smoothing or remeshing. Despite these promising applications, no single method for computing curvature on polyhedral meshes has been developed for use in all situations; some examples of past algorithms include [Goldfeather and Interrante 2004; Meyer et al. 2003]. In general, algorithms that compute principal curvatures and directions forego accuracy for speed or vice versa.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40038214</person_id>
				<author_profile_id><![CDATA[81335497710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Solomon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blanz, V., and Vetter, T. 1999. A morphable model for the synthesis of 3d faces. In <i>Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques</i>, ACM, 187--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>966134</ref_obj_id>
				<ref_obj_pid>966131</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Goldfeather, J., and Interrante, V. 2004. A novel cubic-order algorithm for approximating principal direction vectors. <i>ACM Transactions on Graphics 23</i>, 45--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Meyer, M., Desbrun, M., Schr&#246;der, P., and Barr, A. H. 2003. Discrete differential-geometry operators for triangulated 2-manifolds. In <i>Visualization and Mathematics III</i>, H.-C. Hege and K. Polthier, Eds. Springer-Verlag, Heidelberg, 35--57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280746</article_id>
		<sort_key>23</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Robust fitting of super-helices to parametric curves]]></title>
		<page_from>23</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280746</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280746</url>
		<abstract>
			<par><![CDATA[<p>We present a robust technique for Super-Helix (SH) modeling based on a variational and iterative fitting of SH curves to arbitrary regular parametric curves.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893195</person_id>
				<author_profile_id><![CDATA[81335488247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mattias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bergbom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39059536</person_id>
				<author_profile_id><![CDATA[81324492244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Museth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052903</person_id>
				<author_profile_id><![CDATA[81100308346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1142012</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bertails, F., Audoly, B., Cani, M.-P., Querleux, B., Leroy, F., and L&#233;v&#234;que, J.-L. 2006. Super-helices for predicting the dynamics of natural hair. In <i>ACM Transactions on Graphics (Proceedings of the SIGGRAPH conference)</i>. accepted to Siggraph'06.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280747</article_id>
		<sort_key>24</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[A fast mesh deformation tool for blender]]></title>
		<page_from>24</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280747</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280747</url>
		<abstract>
			<par><![CDATA[<p>Standard mesh deformation techniques can sometimes provide unsatisfactory results when attempting to automatically deform natural objects, e.g humans. Many such recently developed techniques attempt to preserve some quantity (e.g. volume) via optimisation, e.g. [Huang et al. 2006]</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893150</person_id>
				<author_profile_id><![CDATA[81335498764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Verrill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050014</person_id>
				<author_profile_id><![CDATA[81100575511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lasenby]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1142003</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Huang, J., Shi, X., Liu, X., Zhou, K., Wei, L., Teng, S., Bao, H., Guo, B., and Shum., H.-Y. 2006. Subspace gradient domain mesh deformation. <i>Proceedings of ACM SIGGRAPH 2006</i>, 1126--1134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wareham, R. 2006. <i>Computer Graphics Using Conformal Geometric Algebra</i>. PhD thesis, University of Cambridge.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280748</section_id>
		<sort_key>25</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Modeling]]></section_title>
		<section_page_from>25</section_page_from>
	<article_rec>
		<article_id>1280749</article_id>
		<sort_key>25</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Origamizing 3D surface by symmetry constraints]]></title>
		<page_from>25</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280749</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280749</url>
		<abstract>
			<par><![CDATA[<p>We present a new method for "origamizing" or designing the origami crease pattern to construct a given arbitrary polyhedral surface through user interaction. The author [2006a; 2006b] previously proposed a method for manually designing 3D origami surface based on aligning facets and "tucking molecules." However, crease pattern generated by such method is supposed to be super-complex and hard to be folded due to the general tucking molecules without symmetry.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052544</person_id>
				<author_profile_id><![CDATA[81320494905]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tachi, T., 2006. 3D Origami Design based on Tucking Molecule. Fourth International Conference on Origami in Science, Mathematics, and Education (4OSME).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1180134</ref_obj_id>
				<ref_obj_pid>1180098</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tachi, T., 2006. 3D Origami Teapot. SIGGRAPH 2006 Teapot Exhibit.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280750</article_id>
		<sort_key>26</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Facial muscle adaptation for expression customization]]></title>
		<page_from>26</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280750</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280750</url>
		<abstract>
			<par><![CDATA[<p>There are two major approaches to creating 3DCG facial expressions: The first is based on facial muscle simulation and the second is the blend-shape approach. The blend shape approach is more familiar to creators than the facial muscle approach when they synthesize the facial expressions of 3DCG characters. However, the facial muscle model has the advantage of being physics-based. It can, therefore, produce realistic facial expressions and create facial expressions using fewer parameters than the blend shape approach, thereby reducing processing time and computational requirements. We introduce a method which can be used to synthesize individual facial expressions based on the facial muscle model [Waters 1987].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893245</person_id>
				<author_profile_id><![CDATA[81421594348]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishibashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822626</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822404</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109166</person_id>
				<author_profile_id><![CDATA[81100294834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Demetri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terzopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023348</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>37405</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Waters, A muscle model for animating three-dimensional facial expression, <i>Computer Graphics</i>, <b>22</b>(4), 1987, 17--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Y. Lee, D. Terzopoulos, K. Waters, Realistic modeling for facial animation, <i>Proc. SIGGRAPH 95</i>, 1995, 55--62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280751</article_id>
		<sort_key>27</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Baked crepe texture generation]]></title>
		<page_from>27</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280751</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280751</url>
		<abstract>
			<par><![CDATA[<p>We have built a simple model for crepe texture without fluid simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893169</person_id>
				<author_profile_id><![CDATA[81335497014]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kaisei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakurai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823199</person_id>
				<author_profile_id><![CDATA[81319501381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ENSAM P&I Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893125</person_id>
				<author_profile_id><![CDATA[81335490843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fabien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goslin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ENSAM P&I Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051949</person_id>
				<author_profile_id><![CDATA[81100338478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280752</article_id>
		<sort_key>28</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Variable rate speech animation synthesis]]></title>
		<page_from>28</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280752</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280752</url>
		<abstract>
			<par><![CDATA[<p>Speech animation has traditionally been achieved by two main approaches: image-based methods [Ezzat et al. 2002] and key-framing methods [Cohen and Massaro 1993]. Both approaches require large databases that include many images or 3D shapes with facial expressions and speech lip shapes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893091</person_id>
				<author_profile_id><![CDATA[81335499894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822626</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047594</person_id>
				<author_profile_id><![CDATA[81543539756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Adachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109166</person_id>
				<author_profile_id><![CDATA[81100294834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Demetri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terzopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023348</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566594</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Ezzat, G. Geiger, and T. Poggio, "Trainable Videorealistic Speech Animation," <i>Proc. SIGGRAPH 2002</i>, 388--398.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Cohen, D. Massaro "Modeling Coarticulation in Synthetic Visual Speech, N. Magnenat-Thalmann, D. Thalmann (eds), <i>Models and Techniques in Computer Animation</i>," Springer-Verlag, Tokyo, 1993, 139--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Y. Lee, D. Terzopoulos, K. Waters, "Realistic modeling for facial animation," <i>Proc. SIGGRAPH 1995</i>, 55--62.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280753</article_id>
		<sort_key>29</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[SketchBook]]></title>
		<subtitle><![CDATA[silhouette based 3D sketching interface]]></subtitle>
		<page_from>29</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280753</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280753</url>
		<abstract>
			<par><![CDATA[<p>SketchBook is a simple and quick sketchbook-like 3D modeling interface with which designers can modify a 3D object by drawing a new silhouette line over the object's silhouette. SketchBook provides two methods of modification that can be done in any viewpoint.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893151</person_id>
				<author_profile_id><![CDATA[81335493431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeehyung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073324</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nealen, A., Sorkine, O., Alexa, M., and Cohen-Or, D. 2005. A sketch-based interface for detail-preserving mesh editing. In <i>Proceedings of SIGGRAPH 2005</i>, 1142--1147.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280754</article_id>
		<sort_key>30</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Principal components analysis of 3-D scanned human heads]]></title>
		<page_from>30</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280754</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280754</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P885440</person_id>
				<author_profile_id><![CDATA[81332536815]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pengcheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council of Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39078975</person_id>
				<author_profile_id><![CDATA[81100207141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council of Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P187538</person_id>
				<author_profile_id><![CDATA[81100171733]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rioux]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council of Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882311</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Allen, B., Curless, B., and Popovic, Z. 2003. The space of human body shapes: reconstruction and parameterization from range scans. In <i>Proceedings of SIGGRAPH 2003</i>, ACM Press / ACM SIGGRAPH, Computer Graphics Proceedings, Annual Conference Series, ACM, 587--594.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280755</article_id>
		<sort_key>31</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[A real-time deformation model using patient-specific medical data]]></title>
		<page_from>31</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280755</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280755</url>
		<abstract>
			<par><![CDATA[<p>Recently, minimally invasive surgeries have become remarkably common. While such operations reduce patients' burden; they force surgeons to perform difficult surgeries due to lack of effective training methods and equipment. In fact, these difficult operations cause medical accidents, creating social issues to be solved in Japan. To help alleviate this problem, we have been developing a practical endoscopic surgery simulator to be used as an effective training method. In contrast to the functionalities of commercialized common surgical simulators, our system allows medical professionals to use patient-specific data in "rehearsal" operations, potentially preventing possible complications in advance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052110</person_id>
				<author_profile_id><![CDATA[81335495281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Manabu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagasaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Precision Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893182</person_id>
				<author_profile_id><![CDATA[81335498067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takanami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Precision Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893218</person_id>
				<author_profile_id><![CDATA[81335491599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hongo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Precision Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053964</person_id>
				<author_profile_id><![CDATA[81544268056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Precision Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893225</person_id>
				<author_profile_id><![CDATA[81335492749]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Precision Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052337</person_id>
				<author_profile_id><![CDATA[81546419056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Masato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ogata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Precision Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280756</article_id>
		<sort_key>32</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Tile-based ambiguous modeling]]></title>
		<page_from>32</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280756</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280756</url>
		<abstract>
			<par><![CDATA[<p>Two-dimensional (2D) computer graphics, while suitable for depicting 2D structures, are often used in computer games to present three-dimensional (3D) structures. Figure 1 shows an example of a game displayed in a 2D overhead format. The overhead view is an unrealistic expression without 3D geometry data, yet it is possible to perceive a rough 3D structure. In this study, a tile-based scheme for a more intuitive expression of 3D structures using 2D overhead graphics is presented. The scheme converts a conceptual structure designed in 2D overhead view into a 3D expression.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893189</person_id>
				<author_profile_id><![CDATA[81365592162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35033449</person_id>
				<author_profile_id><![CDATA[81311482229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujiki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37040580</person_id>
				<author_profile_id><![CDATA[81331505544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Reiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuruno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18002822</person_id>
				<author_profile_id><![CDATA[81331505568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kiyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomimatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1180054</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fujiki, J., Ushiama, T., and Tomimatsu, K. 2006. A view-oriented interface for block-based modeling. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Sketches</i>, ACM Press, New York, NY, USA, 164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311612</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rademacher, P. 1999. View-dependent geometry. In <i>SIGGRAPH '99: Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 439--446.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280757</article_id>
		<sort_key>33</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[A procedural workflow for set building and rendering]]></title>
		<page_from>33</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280757</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280757</url>
		<abstract>
			<par><![CDATA[<p>For an upcoming short, "Wings of the Avalerion," written by Joseph Guerrieri, an airplane set needed to be created that could be used in a number of configurations and from a number of angles. To easily accommodate the various shots, the decision was made to layout the airplane procedurally using Side Effects Software's Houdini and to rely on Pixar's RenderMan for flexible shading.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051676</person_id>
				<author_profile_id><![CDATA[81320493798]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Redmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Drexel University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383551</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christensen, P., and Batali, D. 2004. An Irradiance Atlas for Global Illumination in Complex Production Scenes. <i>Proceedings of the Eurographics Symposium on Rendering</i>, ACM, 133--141.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280758</article_id>
		<sort_key>34</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Facial type, expression, and viseme generation]]></title>
		<page_from>34</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280758</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280758</url>
		<abstract>
			<par><![CDATA[<p>The process of generating facial models and various poses of these models is a necessary part of most present-day movies, and usually required for any interactive game which features humans as a primary character. The generation of this face data can be approached in ways varying from pure computation to pure data acquisition. Computational models are flexible but can lack realism and intuitive or simple controls, while data-driven models produce realistic faces, but necessitate the often slow and cumbersome capture of new scan data for every desired set of face attributes. Our method is a hybrid approach, which combines a relatively small set of real world facial data with a computational algorithm that learns the underlying variations in this geometric information automatically. Given a sparse data set that spans variation in viseme, face type, and expression, we are able to generate new faces that exhibit combinations of these attributes, and were never part of the original data set. We rely on user-assisted categorization of our sparse data set to associate each piece of face data with a small set of attribute contributions, and then use this categorization data as a guide for binding abstract variation to concrete parameters. This process takes the complex, subtle, and often subjective qualities associated with visemes, expressions, and face types, and correlates them to known geometric features, in order to facilitate the creation of entirely new face poses.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893149</person_id>
				<author_profile_id><![CDATA[81421595472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Skorupski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893153</person_id>
				<author_profile_id><![CDATA[81543547856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jerry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893161</person_id>
				<author_profile_id><![CDATA[81335494706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Josh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McCoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083172</person_id>
				<author_profile_id><![CDATA[81100603625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Allen, B., Curless, B., and Popovi&#263;, Z. 2004. Exploring the space of human body shapes: data-driven synthesis under anthropometric control. In <i>Proc. Digital Human Modeling for Design and Engineering Conference</i>. SAE International.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blanz, V. and T. A. Vetter. 1999. Morphable Model for the Synthesis of 3D Faces. In <i>Proc. of ACM SIGGRAPH 1999</i>. 187--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015759</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zhang, L., et al. 2004. Spacetime faces: high resolution capture for modeling and animation. In <i>Proc. of ACM SIGGRAPH 2004</i>. 548--558.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280759</article_id>
		<sort_key>35</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Sketch-to-collage]]></title>
		<page_from>35</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280759</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280759</url>
		<abstract>
			<par><![CDATA[<p>Using query-by-sketch we propose an application to efficiently create collages from image collections. Using rough color strokes that represent the target collage, images are automatically retrieved from a database and segmented to synthesize a new image. The database is indexed using simple geometrical and color features for each region, and histograms that represent these features for each image. The image collection is queried by means of rough strokes on a simple paint tool. The individual segments retrieved are added to the collage using Poisson image editing and alpha matting.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P741013</person_id>
				<author_profile_id><![CDATA[81100426518]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gavilan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35032784</person_id>
				<author_profile_id><![CDATA[81100652296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Suguru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39042929</person_id>
				<author_profile_id><![CDATA[81100441141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gavilan, D., Takahashi, H., Saito, S., and Nakajima, M. 2006. Mobile image retrieval using morphological color segmentation. In <i>3rd Int'l Conference on Mobile and Ubiquitous Computing</i>, IPSJ, Ed., 51--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141934</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jia, J., Sun, J., Tang, C.-K., and Shum, H.-Y. 2006. Drag-and-drop pasting. <i>ACM Trans. Graph. 25</i>, 3, 631--637.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Johnson, M., Brostow, G., Shotton, J., Arandjelovic, O., Kwatra, V., and Cipolla, R. 2006. Semantic photo synthesis. <i>Computer Graphics Forum (EG'06) 25</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882269</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P&#233;rez, P., Gangnet, M., and Blake, A. 2003. Poisson image editing. <i>ACM Trans. Graph. 22</i>, 3, 313--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280760</article_id>
		<sort_key>36</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Simulation of autumn leaves]]></title>
		<page_from>36</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280760</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280760</url>
		<abstract>
			<par><![CDATA[<p>Visual simulation of autumn leaves and their weathering sequences requires both the convincing geometry shapes and the reasonable materials for the leaves. This problem can be solved from two major aspects, the geometric modeling and the material synthesis. With realistic rendering of autumn leaves, our system can generate convincing results as is shown in Fig. 1</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P897190</person_id>
				<author_profile_id><![CDATA[81451595724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xiaoyu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Software, Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP79027759</person_id>
				<author_profile_id><![CDATA[81323495580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Macau]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77045037</person_id>
				<author_profile_id><![CDATA[81408597193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yanyun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research Asia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39089683</person_id>
				<author_profile_id><![CDATA[81100657893]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Enhua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Macau & Chinese Academy of Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073251</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Runions, A., Fuhrer, M., Lane, B., Federl, P., Rolland-Lagan, A.-G., and Prusinkiewicz, P. 2005. Modeling and visualization of leaf venation patterns. <i>ACM Trans. Graph. 24</i>, 3, 702--711.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141951</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wang, J., Tong, X., Lin, S., Pan, M., Wang, C., Bao, H., Guo, B., and Shum, H.-Y. 2006. Appearance manifolds for modeling time-variant appearance of materials. <i>ACM Trans. Graph. 25</i>, 3, 754--761.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280761</article_id>
		<sort_key>37</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Surface network construction from non-parallel cross-sections]]></title>
		<page_from>37</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280761</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280761</url>
		<abstract>
			<par><![CDATA[<p>Surface construction from incomplete data is a topic of wide interest in geometry processing. A common class of incomplete inputs that arises in several application domains, including bio-medicine and geology, is a stack of curves representing the planar cross-sections of a complete surface. In medical imaging, for example, these curves are typically hand-drawn by physicians to delineate contours of anatomical structures on 2D cross-sections of a 3D image volume generated by MRI, CT or ultrasound. Given a stack of planar cross-sections, a complete 3D surface is desired that connects the curves on each cross-section plane. In particular, such surface needs to be both <i>topologically</i> correct (i.e., closed and interpolating input curves) and <i>geometrically</i> smooth.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39088057</person_id>
				<author_profile_id><![CDATA[81335493704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050392</person_id>
				<author_profile_id><![CDATA[81100098726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891708</person_id>
				<author_profile_id><![CDATA[81335494246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Low]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14117613</person_id>
				<author_profile_id><![CDATA[81100323675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chandrajit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bajaj]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Texas at Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>644129</ref_obj_id>
				<ref_obj_pid>644108</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barequet, G., Goodrich, M. T., Levi-Steiner, A., and Steiner, D. 2003. Straight-skeleton based contour interpolation. In <i>SODA '03: Proceedings of the fourteenth annual ACMSIAM symposium on Discrete algorithms</i>, 119--127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ju, T., Warren, J. D., Carson, J., Eichele, G., Thaller, C., Chiu, W., Bello, M., and Kakadiaris, I. A. 2005. Building 3d surface networks from 2d curve networks with application to anatomical modeling. <i>The Visual Computer 21</i>, 8--10, 764--773.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280762</article_id>
		<sort_key>38</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[A free-hand drawing with simultaneous projection of predicted guideline]]></title>
		<page_from>38</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280762</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280762</url>
		<abstract>
			<par><![CDATA[<p>Figure 1 demonstrates a novel computer aided drawing system proposed by the authors. This system, which is called Hyperdraw, assists designers' drawing of sketches immediately and intuitively without disturbing their characteristic tastes of drawing and delicate feeling of drawing on a canvas. Hyperdarw shows its user suggested curves/lines by predicting his/her drawing action in real-time. Drawing of straight line, circle, parabola, and ellipse that are commonly used on early stage of designing is assisted by Hyperdraw's projecting of dynamically predicted guideline on the canvas. The user can follow the guideline or can ignore it, which makes users of Hyperdraw be able to draw beautiful straight line, circle, parabola, ellipse, with mixing their touches, quite easily.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P875201</person_id>
				<author_profile_id><![CDATA[81331504563]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hirokatsu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[So]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35041181</person_id>
				<author_profile_id><![CDATA[81442593153]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ichiroh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University and Japan Science and Technology Agency]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37030898</person_id>
				<author_profile_id><![CDATA[81318498832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280763</article_id>
		<sort_key>39</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Modeling repetitive motions in real-world 3D scenes]]></title>
		<page_from>39</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280763</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280763</url>
		<abstract>
			<par><![CDATA[<p>Obtaining models of dynamic 3D objects is an important part of content generation for computer graphics. If the states or poses of the dynamic object repeat often during a sequence (but not necessarily periodically), we call such a <i>repetitive motion</i>. There are many objects, such as toys, machines, and humans, undergoing repetitive motions. Our key observation is that for repetitive motions we can use one fixed camera to perform robust motion analysis and a second capture-device to provide 3D information of each motion state. After the motion sequence, we group temporally disjoint observations of the same motion state and produce a smooth space-time reconstruction of the scene. Effectively, the dynamic scene modeling problem is converted to a series of static scene reconstructions, which are much easier to tackle. The second device can be either a passive camera or an active-light projector, resulting in two different modeling techniques. Based on this observation, we present an efficient passive multi-viewpoint acquisition and a robust structured-light acquisition of repetitive motions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39063335</person_id>
				<author_profile_id><![CDATA[81328490999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P58480</person_id>
				<author_profile_id><![CDATA[81100218141]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Aliaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280764</article_id>
		<sort_key>40</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[The D-BRDF model as a basis for BRDF acquisition]]></title>
		<page_from>40</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280764</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280764</url>
		<abstract>
			<par><![CDATA[<p>Real world materials exhibit characteristic surface reflectance, such as glossy or specular highlights, and anisotropy that need to be modeled accurately for realistic rendering applications. The surface reflectance of a material is formalized by the notion of the Bidirectional Reflectance Distribution Function (BRDF). The acquisition of real world BRDF data, particularly with image-based techniques, has been a very active area of research over the last few years. Independent of the acquisition process, the acquired data is generally not used directly due to its large size, the noise present in the measurement process, and missing data for certain incident and exitant directions. Instead, the data is usually either fitted to an analytical model or projected into a suitable basis as a post-process. Recently, Ghosh et al. [2007] have proposed an alternative approach where the BRDF data is <i>optically</i> projected into a suitable basis function directly <i>during</i> the capture process. This speeds up acquisition time to one or two <i>minutes</i> compared to a few hours required by traditional approaches. They develop a set of basis functions for this purpose that are similar to the spherical harmonics basis and are orthonormal over the zone of directions that can be simultaneously covered with their optical setup.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35034008</person_id>
				<author_profile_id><![CDATA[81318494009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39052214</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344814</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ashikhmin, M., Premo&#347;e, S., and Shirley, P. 2000. A microfacet-based BRDF generator. In <i>SIGGRAPH '00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques</i>, 65--74.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ashikhmin, M., 2006. Distribution-based BRDFs. http://jesper.kalliope.org/blog/library/dbrdfs.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ghosh, A., Achutha, S., Heidrich, W., and O'Toole, M. 2007. BRDF acquisition with basis illumination. Tech. Rep. TR-2007-10, The University of British Columbia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280765</section_id>
		<sort_key>41</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Art]]></section_title>
		<section_page_from>41</section_page_from>
	<article_rec>
		<article_id>1280766</article_id>
		<sort_key>41</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[hanahana]]></title>
		<subtitle><![CDATA[an interactive image system using odor sensors]]></subtitle>
		<page_from>41</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280766</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280766</url>
		<abstract>
			<par><![CDATA[<p>In our everyday life, we always make use of five senses to obtain information from our surroundings. To integrate computers into the real world seamlessly, interaction technologies using not only visual and auditory information but also other sensory information are necessary. In this project, we focus on the expression possibilities of scent information. In the field of Virtual Reality and Computer Human Interaction, there have been several researches in olfactory display[Barfield and Danas 1996][Yanagida et al. 2003]; however, ambient scent information has rarely been used as input for interation. In this project, we propose an interactive system utilizing the scent data as input and we correlated the input with visual output to display the existence and variation of ambient scents.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050502</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Science and Technology Agency]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P203183</person_id>
				<author_profile_id><![CDATA[81100047313]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Motoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chikamori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[plaplax ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P164149</person_id>
				<author_profile_id><![CDATA[81332510306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kyoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[plaplax ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barfield, W., and Danas, E. 1996. Comments on the use of olfactory displays for virtual environments. In <i>Presence: Teleoperators and Virtual Environments. Vol. 5, no. 1</i>, 109--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>965481</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yanagida, Y., Kawato, S., Noma, H., Tomono, A., and Tetsutani, N. 2003. A Nose-tracked, Personal Olfactory Display. In <i>SIGGRAPH 2003 Sketches and Applications</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280767</article_id>
		<sort_key>42</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Towards the living canvas]]></title>
		<page_from>42</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280767</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280767</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052086</person_id>
				<author_profile_id><![CDATA[81100426691]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naef]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Glasgow School of Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187769</ref_obj_id>
				<ref_obj_pid>1187626</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rhee, S.-M., Ziegler, R., Park, J., Naef, M., Gross, M., Kim, M.-H. 2007. Low-Cost Telepresence for Collaborative Virtual Environments. <i>IEEE Transactions on Visualization and Computer Graphics</i>, Vol. 13, No. 1, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280768</article_id>
		<sort_key>43</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Designing a new car body shape by PCA of existing car database]]></title>
		<page_from>43</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280768</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280768</url>
		<abstract>
			<par><![CDATA[<p>We have developed a tool to design car body shapes. It enables any user even with no experience and skill to design car body shapes interactively and easily. We report how we synthesize car body shapes by following process. Firstly, we create a database of existing cars and analyze all shapes in the database using Principal Component Analysis (PCA). Secondly we extract attributes that indicate the geometrical features of those car body shapes. The attributes are then mapped onto the Eigen space obtained as a result of analysis. We construct a GUI in which the distribution of the attributes on the Eigen space is visualized. By manipulating the weight of each attribute on our GUI, we can synthesize car body shapes which nobody has ever seen.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893230</person_id>
				<author_profile_id><![CDATA[81335491759]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tatsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayakawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823106</person_id>
				<author_profile_id><![CDATA[81319500637]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sekine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822404</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023348</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[V. Blanz, T. Vetter, A Morphable Model for the Synthesis of 3D Faces, ACM SIGGRAPH1999, 187--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280769</article_id>
		<sort_key>44</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Dewy]]></title>
		<subtitle><![CDATA[a condensation display]]></subtitle>
		<page_from>44</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280769</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280769</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents Dewy, a display surface of 'pixelized' condensation, creating images, patterns and text out of the process of the physical state change of water. Dewy works like a spatially controlled fogged window, one that can communicate back to you, conveying information through a slow and subtle means. In developing Dewy, we have been motivated by an aversion to the 'visual pollution' of many existing systems of public media display and seek to create a alternative method of display which references natural processes and can blend seamlessly in its materiality into varying environments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052590</person_id>
				<author_profile_id><![CDATA[81543811756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Amanda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parkes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083822</person_id>
				<author_profile_id><![CDATA[81332519345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dietmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Offenhuber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Virillio, P. "We may be entering an electronic gothic era" in Architectural Design -- Architects in Cyberspace II, Vol. 68 No. 11 / 12 (Nov. / Dec. 1998), pp. 61--65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Haacke, H. Condensation Cube 1963. http://www.artnet.com/Magazine/features/cone/cone8-6-9.asp]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280770</article_id>
		<sort_key>45</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Magnetosphere]]></title>
		<page_from>45</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280770</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280770</url>
		<abstract>
			<par><![CDATA[<p>'Magnetosphere' is an interactive work of art that provides novel tactile sensations using a specially made textured display where the hardness of the screen itself changes (Fig. 1). By providing a tactile display where the screen is specially made by using electromagnets to control miniature steel balls into soft expressions, this technology constitutes a new development in the fields of display technology and artistic expression. The aim of this development is to experiment with the depiction of video images together with actual tactile sensations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35049961</person_id>
				<author_profile_id><![CDATA[81100274861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052006</person_id>
				<author_profile_id><![CDATA[81319500609]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sasada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Electronics Collage]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893194</person_id>
				<author_profile_id><![CDATA[81335500062]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yasada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Electronics Collage]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893249</person_id>
				<author_profile_id><![CDATA[81335498306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzumura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Japan Electronics Collage]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Kushiyama, T. Kitazawa, M. Tamura, S. Sasada, {Thermoesthesia}SIGGRAPH 2006 ArtGallery, Sketch]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280771</article_id>
		<sort_key>46</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[3-D computer arts in clinical radiology]]></title>
		<page_from>46</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280771</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280771</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050613</person_id>
				<author_profile_id><![CDATA[81335494516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[McGhee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Dundee, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893131</person_id>
				<author_profile_id><![CDATA[81335492020]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Graeme]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NHS Tayside, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893208</person_id>
				<author_profile_id><![CDATA[81335489375]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rosemary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chesson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Robert Gordon University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chesson, R. A., Mckenzie, G. A. &amp; Mathers, S. A. (2002) What Do Patients Know About Ultrasound, CT and MRI? <i>Clinical Radiology</i>, 57, 477.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Broadbent, E., Petrie, K., Ellis, C., Ying, J., and Gamble, G. (2004) <i>A picture of health - myocardial infarction patients' drawings of their hearts and subsequent disability: A longitudinal study</i>. Journal of Psychosomatic Research 2004;57:583--58]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280772</article_id>
		<sort_key>47</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA["<i>Nausea</i> Transformer"]]></title>
		<page_from>47</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280772</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280772</url>
		<abstract>
			<par><![CDATA[<p>The word "noise" comes from the Latin word nausea meaning "seasickness", or from a derivative (perhaps Latin noxia) of Latin noceo = "I do harm", referring originally to nuisance noise. Generally all non-musical sounds are considered to be noise. Noise is a complex concept and source material to deal with; it is an invisible architectural element with an undefined aesthetics. It deeply affects people and yet people feel very powerless to interact with or control it. The fundamental idea is to turn noise into a reprocessed living, evolving and tangible experience, by interacting spatially and temporally with the environment and its observers. Our purpose is to raise people's awareness to sound, in all its forms: speech, non-speech sound (sound pollution sources) or natural sound, and treat it like data with a corporeal dimension. We aspire to convey an embodiment to an often neglected "hidden dimension", by adding it to a phenomenology and a poetics of visual space. Building up on our research in interactive membranes [1] [2], we introduce <i>"Nausea Transformer":</i> a sound reprocessed machine that unexpectedly can create pleasant behaviours by recycling noise into pleasant sound, therefore promoting new interactive experiences to a nearby audience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P689204</person_id>
				<author_profile_id><![CDATA[81100166418]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cesar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Branco]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Augmented Architectures, UK/Portugal]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048922</person_id>
				<author_profile_id><![CDATA[81544822256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dias]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISCTE, Portugal]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P690943</person_id>
				<author_profile_id><![CDATA[81100540981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nancy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Diniz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCL, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Diniz, N, Branco, C, Dias M, and Turner, A: 2007, Morphosis: An Interactive membrane, in Gero, JS (ed) CAAD Futures '07, 11--13 July 2007, Sydney, Australia. {forthcoming}.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Diniz, N and Branco, C: 2006, An Interactive membrane: Envisioning Physically Mutable Materials for Architecture, in Gero, JS (ed) Design Computing and Cognition '06, 10--12 July 2006, Heindhoven, Holland.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280773</article_id>
		<sort_key>48</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[nite_aura]]></title>
		<subtitle><![CDATA[an audio-visual interactive immersive installation]]></subtitle>
		<page_from>48</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280773</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280773</url>
		<abstract>
			<par><![CDATA[<p>Immersion and engagement are topics of great interest in the HCI community. Researchers in Gaming, virtual reality, and augmented reality are interested in what it is that draws and holds a user's attention. Most of these works are focused on active engagement and high stimulus. <i>nite_aura</i> is an audio-visual, interactive installation exploring physical, auditory and visual motion within an immersive environment. In this project, we tried to create an alternative immersive environment focusing on sensual and physical interaction with audio and visual aspects. The immersive environment created is an environment of introspection rather than achievement. The work investigates the effect of the texture of space, light and sound in providing a comforting, relaxed immersion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P854101</person_id>
				<author_profile_id><![CDATA[81324493649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jinsil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P892401</person_id>
				<author_profile_id><![CDATA[81335489569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Corness]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280774</article_id>
		<sort_key>49</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Proprioceptive sense in an art installation]]></title>
		<subtitle><![CDATA[AmputationBox]]></subtitle>
		<page_from>49</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280774</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280774</url>
		<abstract>
			<par><![CDATA[<p><i>AmputationBox</i> is an interactive art installation that blurs boundaries between virtual, networked and physical worlds. <i>AmputationBox</i> is also simply a box with a hole on the top. Participants can put their hands into the hole. Every object poked into the hole is visually amputated and redisplayed as virtual organism: creeping or flying flesh. The amputated flesh parts are displayed simultaneously in two different locations: the physical installation and the online website (www.amputationbox.com).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893113</person_id>
				<author_profile_id><![CDATA[81539869756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[Jhave]]></middle_name>
				<last_name><![CDATA[Johnston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P854101</person_id>
				<author_profile_id><![CDATA[81324493649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jinsil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35036980</person_id>
				<author_profile_id><![CDATA[81100453293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Diane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gromala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://www.cs.ucl.ac.uk/staff/P.Bentley/wc3paper.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280775</article_id>
		<sort_key>50</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[MyTree]]></title>
		<subtitle><![CDATA[a entertainment system of lifelog visualization]]></subtitle>
		<page_from>50</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280775</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280775</url>
		<abstract>
			<par><![CDATA[<p>A life log is a record of an individual's actions recorded using small Information Instruments. Various research has been done about the interface that checks the information in the lifelog[1]. Our research allows the visualization of life log information relating to a person's use of a public transportation system (in this case, the Tokyo subway). Furthermore, the system is designed to be interactive and entertaining.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893219</person_id>
				<author_profile_id><![CDATA[81335497396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seol]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893243</person_id>
				<author_profile_id><![CDATA[81335494501]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yamaguchi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masahiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893133</person_id>
				<author_profile_id><![CDATA[81335493250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hashimoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kotaro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P498943</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893146</person_id>
				<author_profile_id><![CDATA[81335498372]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Iwai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toshio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40025385</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1178914</ref_obj_id>
				<ref_obj_pid>1178823</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Hashimoto, Y. Suzuki, T. Tanikawa, T. Iwai, M. Hirose: "Sharelog:Digital Public Art Interaction by using Suica", In Proc. of ACM SIGCHI International Conference on Advances in Computer Entertainment Technology 2006 (ACE 2006), pp. 62, 2006.6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280776</section_id>
		<sort_key>51</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Physics-based animation]]></section_title>
		<section_page_from>51</section_page_from>
	<article_rec>
		<article_id>1280777</article_id>
		<sort_key>51</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Optimization-based interactive motion synthesis for virtual characters]]></title>
		<page_from>51</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280777</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280777</url>
		<abstract>
			<par><![CDATA[<p>Modeling the reactions of human characters to a dynamic environment is crucial for achieving perceptual immersion in applications such as video games, training simulations and movies. Virtual characters in these applications need to <i>realistically</i> react to environmental events and <i>precisely</i> follow high-level user commands. Most existing physics engines for computer animation facilitate synthesis of passive motion, but remain unsuccessful in generating motion that requires active control, such as character animation. We present an optimization-based approach to synthesizing active motion for articulated characters, emphasizing both physical realism and user controllability. At each time step, we optimize the motion based on a set of goals specified by higher-level decision makers, subject to the Lagrangian dynamics and the physical limitations of the character. Our framework represents each decision maker as a controller.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35049398</person_id>
				<author_profile_id><![CDATA[81335492422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sumit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891773</person_id>
				<author_profile_id><![CDATA[81335499985]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuting]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ye]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050158</person_id>
				<author_profile_id><![CDATA[81452598193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[C.]]></first_name>
				<middle_name><![CDATA[Karen]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280778</article_id>
		<sort_key>52</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Real-time particle-based simulation on GPUs]]></title>
		<page_from>52</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280778</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280778</url>
		<abstract>
			<par><![CDATA[<p>As physical laws govern the motion of objects around us, a physically-based simulation plays an important role in computer graphics. For instance, the motion of a fluid, which is difficult to generate by hand, can be produced by solving the governing equations. Acceleration of a simulation is one of the most important research themes because the speed and stability of a simulation are essential for real-time applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P823014</person_id>
				<author_profile_id><![CDATA[81319492976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084103</person_id>
				<author_profile_id><![CDATA[81541830956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038924</person_id>
				<author_profile_id><![CDATA[81329489943]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Seiichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koshizuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39087986</person_id>
				<author_profile_id><![CDATA[81100605028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>846298</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, M., Charypar, D., and Gross, M. 2003. Particle-based fluid simulation for interactive applications. In <i>Proc. of SIGGRAPH Symposium on Computer Animation</i>, 154--159.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280779</article_id>
		<sort_key>53</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Elastic objects for computer graphic field using MPS method]]></title>
		<page_from>53</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280779</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280779</url>
		<abstract>
			<par><![CDATA[<p>Physics simulations play an important role on computer graphics in recent years. We newly adopt MPS(Moving Particle Semiimplicit), which can calculates elastic objects based on continuum mechanics. Interaction is easily evaluated by particle collisions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050899</person_id>
				<author_profile_id><![CDATA[81547917456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kondo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084103</person_id>
				<author_profile_id><![CDATA[81541830956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823014</person_id>
				<author_profile_id><![CDATA[81319492976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35038924</person_id>
				<author_profile_id><![CDATA[81329489943]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Seiichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koshizuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kondo, M., Koshizuka, S., and Suzuki, Y. 2005. Application of symplectic scheme to three-dimensional elastic analysis using mps method. <i>Transactions of the Japan Society of Mechanical Engineers A 72</i>, 65--71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028542</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Muller, M., Keiser, R., Nealen, A., Pauly, M., Gross, M., and Alexa, M. 2004. Point based animation of elastic, plastic and melting objects. <i>ACM SIGGRAPH Symposium on Computer Animation</i>, 141--151.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Vignjevic, R., Campbell, J., and Libersky, L. 2000. A treatment of zero-energy modes in the smoothed particle hydrodynamics method. <i>Computational methods in applied mechanics and engineering 184</i>, 67--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280780</article_id>
		<sort_key>54</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Visual simulation of smoke using overlapping grids]]></title>
		<page_from>54</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280780</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280780</url>
		<abstract>
			<par><![CDATA[<p>Many methods for the visual simulation of natural phenomena, such as smoke and fire, have been proposed. These methods use a technique known as computational fluid dynamics. We propose a new method using overlapping grids for the visual simulation of fluids. Previously, the motion of the fluids was computed by using a single computational grid. When an object interacts with a fluid, the resolution of a grid must be sufficiently high to accurately represent the shape of the object that is voxelized by sampling the shape at the grid points. To address this problem, an adaptive method using a tetrahedral mesh has been proposed to represent the shape of the object accurately [Klinger et al. 2006]. However, this requires restructuring of the mesh when the objects move. The computational algorithm also becomes complex due to the uneven structure of the mesh. Our method addresses these problems by using multiple grids, and provides a practical and efficient way of handling objects interacting with fluids.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P306681</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36030472</person_id>
				<author_profile_id><![CDATA[81100602092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tsuyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893234</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishtia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141961</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Klingner, B. M., Feldman, B. E., Chentanez, N., and O'Brien, J. F. 2006, Fluid animation with dynamic meshes, <i>ACM Trans. on Graphics, 25</i>, 3, 283--284.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280781</article_id>
		<sort_key>55</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Poseable dynamics for knee-length beards]]></title>
		<page_from>55</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280781</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280781</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893111</person_id>
				<author_profile_id><![CDATA[81544594656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048675</person_id>
				<author_profile_id><![CDATA[81100421152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sunil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hadap]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280782</article_id>
		<sort_key>56</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Environment-based physical motion for secondary characters]]></title>
		<page_from>56</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280782</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280782</url>
		<abstract>
			<par><![CDATA[<p>Motion pictures of all sorts make heavy use of "extra" actors to bring fidelity and liveliness to a scene. These secondary characters are usually engaged in low-energy, background actions and are often seen close-up and in-detail. This fabric of background human activity comes with two main challenges. First, secondary characters must be able to react to events in their environment. Second, characters should not exhibit motions that are too repetitive. In this paper, we propose an approach to generating physically realistic and non-repetitive motion for small groups of autonomous secondary characters. To equip a character with a wide range of motor skills we introduce the concepts of <i>behavior</i> and <i>transition</i>. A behavior models the ability of a character to remain within range of a set of well-defined configurations, while a transition allows the character to change its behavior. Characters choose their behavior as a response to the environment and in accordance to a simple physiological model.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP77033998</person_id>
				<author_profile_id><![CDATA[81407591949]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Allen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049398</person_id>
				<author_profile_id><![CDATA[81335492422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sumit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P225799</person_id>
				<author_profile_id><![CDATA[81100370834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Petros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050158</person_id>
				<author_profile_id><![CDATA[81452598193]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[C.]]></first_name>
				<middle_name><![CDATA[Karen]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280783</article_id>
		<sort_key>57</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Fluid flow on interacting deformable surfaces]]></title>
		<page_from>57</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280783</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280783</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893201</person_id>
				<author_profile_id><![CDATA[81335495479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893207</person_id>
				<author_profile_id><![CDATA[81100388468]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Metoyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084393</person_id>
				<author_profile_id><![CDATA[81100115231]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oregon State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141959</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Irving, G., Guendelman, E., Losasso, F., and Fedkiw, R. 2006. Efficient simulation of large bodies of water by coupling two and three dimensional techniques. <i>ACM Transactions on Graphics (SIGGRAPH 2006) 25</i>, 3, 805--811.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1071162</ref_obj_id>
				<ref_obj_pid>1071157</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shi, L., and Yu, Y. 2004. Inviscid and incompressible fluid simulation on triangle meshes: Research articles. <i>Comput. Animat. Virtual Worlds 15</i>, 3--4, 173--181.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882338</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Stam, J. 2003. Flows on surfaces of arbitrary topology. <i>ACM Trans. Graph. 22</i>, 3, 724--731.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1183290</ref_obj_id>
				<ref_obj_pid>1183287</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zhang, E., Mischaikow, K., and Turk, G. 2006. Vector field design on surfaces. <i>ACM Trans. Graph. 25</i>, 4, 1294--1326.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280784</article_id>
		<sort_key>58</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Creating realistic CG honey]]></title>
		<page_from>58</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280784</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280784</url>
		<abstract>
			<par><![CDATA[<p>During the pre-production phase of Bee Movie we faced the challenge of creating realistic looking CG honey. The development approach was divided into two main parts: creating the overall look, and simulating the dynamics. In this sketch I will discuss how we met these challenges by incorporating various elements into a raytrace render, and by applying viscoelasticity in the fluid solver used for the particle simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893094</person_id>
				<author_profile_id><![CDATA[81335496895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Allen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ruilova]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dreamworks Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015746</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bargteil A. W., Goktekin T. G., O'Brien J. F.: A method for animating viscoelastic fluids. In <i>Proceedings of ACM SIGGRAPH 2004</i> (Aug. 2004), pp. 463--468.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280785</article_id>
		<sort_key>59</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Simulation of a pliable jellyfish in fluids]]></title>
		<page_from>59</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280785</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280785</url>
		<abstract>
			<par><![CDATA[<p>Physical simulation of molluscous is hard task. Recently motion of molluscous has been represented in various fields such as movies and computer games, however physical simulation techniques is often ignored due to huge computational cost, therefore simple motion - completely regular, cyclic or symmetrical motion - is frequently used. In this paper, we propose the method for asymmetrical motion of molluscous, especially jellyfish, by an easy mechanism with few computational complexity.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893165</person_id>
				<author_profile_id><![CDATA[81421595326]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junsei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083481</person_id>
				<author_profile_id><![CDATA[81100605028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Koshizuka, and Y. Oka. 1996. Moving-particle semi-implicit method for fragmentation of incompressible fluid. <i>Nucl. Sci. Eng. 123</i>, 3, 421--434.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280786</section_id>
		<sort_key>60</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Images]]></section_title>
		<section_page_from>60</section_page_from>
	<article_rec>
		<article_id>1280787</article_id>
		<sort_key>60</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Mapping normal vectors and colors onto kaleidogram]]></title>
		<page_from>60</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280787</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280787</url>
		<abstract>
			<par><![CDATA[<p>We propose a new method for enriching the appearances of gratings called Kaleidogram&#174; without recording diffraction fringes. Kaleidogram [KALEIDO] is an embossed hologram that consists of multiple gratings of which grating elements elongate to the orientation of an individual piece of the original design. Each piece causes iridescence, reacting to the lighting and viewing condition as shown in Figure 1. This characteristic makes Kaleidogram useful for brand identifications and security purposes. While Kaleidogram is bright and low-priced, the expression is limited to two-dimensional motifs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050775</person_id>
				<author_profile_id><![CDATA[81100533332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893217</person_id>
				<author_profile_id><![CDATA[81542638256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shin-ichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893228</person_id>
				<author_profile_id><![CDATA[81335495660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kaleido. http://www.dnp.co.jp/bf/hologram/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187124</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kawai, N. 2005. Bump Mapping onto Real Objects. <i>ACM SIGGRAPH 2005 Sketches</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280788</article_id>
		<sort_key>61</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Pseudo expansion of field-of-view for immersive projection displays]]></title>
		<page_from>61</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280788</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280788</url>
		<abstract>
			<par><![CDATA[<p>We developed a pseudo expansion of field-of-view of image contents for immersive projection displays like a CAVE or tileddisplay. Despite the wide spread of the large and immersive displays, lack of their applications is a serious problem. Our approach can reconstruct invisible peripheral images from past image frames by using an adaptive depth model. This approach can achieve both real-time processing and reduction of perceptible distortion and discontinuity in the expanded images. It also contributes to using our accessible image contents, like interactive video games, for the immersive displays, and enhancing enjoyment of them.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893179</person_id>
				<author_profile_id><![CDATA[81335491699]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Honda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36024479</person_id>
				<author_profile_id><![CDATA[81100135451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36044011</person_id>
				<author_profile_id><![CDATA[81100567066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chiba, N., Kano, H., Higashihara, M., and Osumi, M. 1998. Feature-based image mosaicing. in. <i>Proc. IAPR Workshop on Machine Vision Applications</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1180007</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sato, T., Iketani, A., Ikeda, S., Kanbara, M., Nakajima, N., and Yokoya, N. 2006. Video mosaicing for curved documents by structure from motion. <i>Proc. SIGGRAPH2006 Sketches</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280789</article_id>
		<sort_key>62</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Histogram-based HDR video]]></title>
		<page_from>62</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280789</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280789</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P174540</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto de Matematica Pura e Aplicada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882270</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kang, S. B., Uyttendaele, M., Winder, S., and Szeliski, R. 2003. High dynamic range video. <i>ACM TOG 22</i>, 3, 319--325.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Robertson, M., Borman, S., and Stevenson, R. 1999. Dynamic range improvement through multiple exposures. In <i>Proceedings of ICIP</i>, vol. 3, 159--163.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280790</article_id>
		<sort_key>63</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Video super-resolution using texton substitution]]></title>
		<page_from>63</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280790</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280790</url>
		<abstract>
			<par><![CDATA[<p>A video camera produces an image sequence with a specific frame rate (e.g. 30 frames per second). This imposes limits on the spatial resolution due to limited bandwidth. Image interpolation, such as bi-cubic interpolation, can increase the resolution, but yields a blurring of edges and image details. To create plausible high-frequency details in the blurred image, super-resolution technique has been a long studied area[Baker and Kanade 2002; Capel and Zisserman 2003]. However, it is difficult to apply these methods to a video sequence, since [Capel and Zisserman 2003] requires multiple images, and [Baker and Kanade 2002] requires a high computational cost. In order to resolve these problems, we proposed a method called "texton substitution"[Kamimura et al. 2006]. In this paper, we improve the method of texton substitution by using temporal connection.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822748</person_id>
				<author_profile_id><![CDATA[81319494204]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P587730</person_id>
				<author_profile_id><![CDATA[81100577741]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norimichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsumura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823057</person_id>
				<author_profile_id><![CDATA[81319497762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Toshiya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P424968</person_id>
				<author_profile_id><![CDATA[81319497766]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35041555</person_id>
				<author_profile_id><![CDATA[81331499447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hideto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Motomura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Matsushita Electric Industrial Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>628815</ref_obj_id>
				<ref_obj_pid>628330</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baker, S., and Kanade, T. 2002. Limits on super-resolution and how to break them. <i>IEEE Tran. on PAMI 24</i>, 9, 1167--1183.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Capel, D., and Zisserman, A. 2003. Computer vision applied to super resolution. <i>IEEE Signal Processing Magazine</i>, 75--86.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179709</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kamimura, K., Tsumura, N., Nakaguchi, T., Miyake, Y., Motomura, H., and Kanamori, K. 2006. Substituting crossed textons for super resolution of video. <i>SIGGRAPH poster</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280791</article_id>
		<sort_key>64</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[High dynamic range images capturing with multi-band camera]]></title>
		<page_from>64</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280791</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280791</url>
		<abstract>
			<par><![CDATA[<p>The "Natural Vision" [1] project aims at an innovative video and still image communication technology with high fidelity color reproduction capability, based on spectral information. We have researched and developed multi-band cameras and multi-primary displays [2], to break through the limitations of conventional RGB systems. Our goal is to reproduce super-natural images with true color, gloss and texture using visual telecommunication systems.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050916</person_id>
				<author_profile_id><![CDATA[81320491476]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kishimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Data Community Produce]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35029602</person_id>
				<author_profile_id><![CDATA[81100449795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893172</person_id>
				<author_profile_id><![CDATA[81335492977]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kanno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Data Community Produce]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yamaguchi. M, 2006 "High-fidelity video and still-image communication based on spectral information: Natural Vision system and its applications" Proc. of SPIE-IS&T Electronic Imaging, SPIE Vol. 6062, 60620G,]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186169</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kishimoto. J, Yamaguchi. M, Haneishi. H, and Ohyama. N, 2004 "IRODORI -- A Color-rich Palette Based on Natural Vision Technology," ACM SIGGRAPH 2004 Emerging Technology]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., and Malik, J. 1997. Recovering high dynamic range radiance maps from photographs. In Proceedings of SIGGRAPH 1997, Annual Conference Series, ACM, 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280792</article_id>
		<sort_key>65</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Model-based color manipulation]]></title>
		<page_from>65</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280792</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280792</url>
		<abstract>
			<par><![CDATA[<p>The manipulation of the colors in an image and color spaces in general can be highly complex and non-intuitive. Converting the thoughts or vision of the designer to specific color modifications on an image is hard, while envisioning the effect of a specific color manipulation is difficult as well. Furthermore, recoloring must take into account spatial image considerations along with the color constraints. We introduce a method that models possible color manipulations of an image by a structured search space. We utilized the design galleries metaphor to present the designer with an interface for navigating this search space toward a desired target color variation. The pixels' colors are modeled by a spatially-constrained Gaussian mixture model of the chroma channels, and the search space is defined by applying affine transformations to the parameters of this model. This approach provides both inspiration and intuitive navigation in the complex space of image color manipulations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052143</person_id>
				<author_profile_id><![CDATA[81335497317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lior]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shapira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel-Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40030280</person_id>
				<author_profile_id><![CDATA[81100264399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen-Or]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel-Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893187</person_id>
				<author_profile_id><![CDATA[81335490423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lior]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gavish]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel-Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35023786</person_id>
				<author_profile_id><![CDATA[81100081895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ariel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shamir]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interdisciplinary Center, Herzliya]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258887</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Marks, J., Andalman, B., Beardsley, P. A., Freeman, W., Gibson, S., Hodgins, J., Kang, T., Mirtich, B., Pfister, H., Ruml, W., Ryall, K., Seims, J., and Shieber, S. 1997. Design galleries: a general approach to setting parameters for computer graphics and animation. <i>Computer Graphics 31</i>, Annual Conference Series, 389--400.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280793</article_id>
		<sort_key>66</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Multi-frame video representation using feature preserving directional blur]]></title>
		<page_from>66</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280793</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280793</url>
		<abstract>
			<par><![CDATA[<p>The long-standing goal of the video-based graphics community is to be able to view video content at one view. This sketch shows a new video representation method for visualizing video content quickly and effectively. Traditionally, a video thumbnail or movie thumbnail is used for a video browsing interface. It is insufficient to represent the video content because it shows only one sample frame of the video or it is annoying for viewing because of the difficulty of distinguishing the beginning and the end frame of the movie content. On the other hand, a multi-frame representation is used for editing and quick viewing of the video sequence [1]. However, it is hard to view when each frame overlaps and a special frame alignment is necessary to compose a summarized frame. We propose a "video flash" representation method, which uses transparent multi-frames with directional blur effects dependent on a layout. A directional blur effect is used to diminish noticeable artifacts on overlapping frames and visualize frame sequences at the same time. Furthermore, there is no need to consider each frame alignment in detail. This method is also seamlessly transferable from conventional movie thumbnails and applicable for new kinds of video browsing interfaces with a quick preview mode.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35054432</person_id>
				<author_profile_id><![CDATA[81100323314]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasunobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamauchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1047940</ref_obj_id>
				<ref_obj_pid>1047936</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Teodosio, L. et al., Salient Stills, ACM Transaction on Multimedia Computing, Communications, and Applications, Volume 1, Issue 1, pp. 16--36 (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383325</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brostow, J. G. et al., Image-based motion blur for stop motion animation. Proc. of SIGGRAPH'01 pp. 561--566 (2001).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280794</article_id>
		<sort_key>67</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Color correction of high dynamic range images at HDR-level]]></title>
		<page_from>67</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280794</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280794</url>
		<abstract>
			<par><![CDATA[<p>In recent research in representing realistic appearance of materials, HDR(High Dynamic Range) images constructed from LDR(Low Dynamic Range) images taken with different exposure time using a digital camera are used to measure radiance of materials like BRDF. However, in order to reproduce the original color of the materials, they require color correction of the HDR images. Before constructing an HDR image, the color of each LDR image is corrected by using the ICC profile or other method. Then they make a color corrected HDR image from the color corrected LDR images [Goesele et al. 2001]. We call this method the LDR-level color correction. However, each pixel value of LDR images represents not only color but also intensity. Since the pixel values of the LDR images are modified for color correction before making an HDR image, the intensity of each pixel is also changed. In this paper, we propose a color correction method at HDR-level, where the correction is made after the HDR image is constructed. This gives a much better reproduction of the original color of a given material in our experiment.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[HDRI]]></kw>
			<kw><![CDATA[color correction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893143</person_id>
				<author_profile_id><![CDATA[81335499822]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hyun]]></first_name>
				<middle_name><![CDATA[Jin]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893170</person_id>
				<author_profile_id><![CDATA[81335492745]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kang]]></first_name>
				<middle_name><![CDATA[Yeon]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049468</person_id>
				<author_profile_id><![CDATA[81335492623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hoe]]></first_name>
				<middle_name><![CDATA[Min]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051181</person_id>
				<author_profile_id><![CDATA[81335495725]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kang]]></first_name>
				<middle_name><![CDATA[Su]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893216</person_id>
				<author_profile_id><![CDATA[81452602365]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Seung]]></first_name>
				<middle_name><![CDATA[Joo]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050861</person_id>
				<author_profile_id><![CDATA[81335493214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kwang]]></first_name>
				<middle_name><![CDATA[Hee]]></middle_name>
				<last_name><![CDATA[Ko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051005</person_id>
				<author_profile_id><![CDATA[81319495441]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kwan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Goesele, M., Heidrich, W., and Seidel, H.-P. 2001. Color calibrated high dynamic range imaging with icc profiles. In <i>Proceedings of the 9th IS&T Color Imaging Conference</i>, 15--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280795</article_id>
		<sort_key>68</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Filling time by plugging the holes]]></title>
		<page_from>68</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280795</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280795</url>
		<abstract>
			<par><![CDATA[<p>We explore a new technique for video frame rate up-conversion (FRUC). A noniterative multilayer motion estimation algorithm is investigated, based on spatio-temporal smoothness constraints. Our algorithm's performance is at par with complicated iterative algorithms based on motion segmentation. For regions in the interpolated frame which cannot be motion compensated, we use an exemplar based video inpainting algorithm. To our knowledge, this is the first use of inpainting to fill in holes for FRUC.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893238</person_id>
				<author_profile_id><![CDATA[81335496290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vikas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramachandra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[U.C. San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893174</person_id>
				<author_profile_id><![CDATA[81539818156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Karl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[U.C. San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088223</person_id>
				<author_profile_id><![CDATA[81501641242]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Truong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nguyen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[U.C. San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2319414</ref_obj_id>
				<ref_obj_pid>2318952</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Altunbasak Y. and Tekalp, A. M. 1997 Closed-form connectivity-preserving solutions for motion compensation using 2-D meshes. In <i>IEEE Transactions on Image Processing</i> Vol.6, Iss.9, Pages:1255--1269.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kumar S., Biswas, M., Belongie, S. J. and Nguyen, T. Q. 2005. Spatio-temporal texture synthesis and image inpainting for video applications. In <i>IEEE International Conference on Image processing</i> Vol.2, No. 11--14, Pages: II- 85--89.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schutten R. J., Pelagotti A and De Haan, G. 1998. Layered motion estimation. In <i>Philips Journal of Research</i> Vol 51, No. 2. Pages:253--267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280796</article_id>
		<sort_key>69</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Video enhancement using reference photographs]]></title>
		<page_from>69</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280796</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280796</url>
		<abstract>
			<par><![CDATA[<p>Handheld digital video cameras have become increasingly popular and cheaper in recent years. Even still cameras offer additional functionality for shooting videos. Unfortunately, the small form factor of these devices limits the light sensitivity, and often the the lens and sensor do not allow for satisfactory image quality. Matters become worse as each frame can be exposed for only a fraction of a second. Even for such short exposures, motion blur is still noticeable and may destroy visual details. In addition, the internal bandwidth of the storage unit inside the camera also puts a limit on resolution. Image quality may also suffer from the ability of the person that operates the camera. The dynamic range of current cameras is not high enough to correct over- and under-exposure afterward, and excessive shake increases motion blur. On the other hand, when taking photographs with a still camera, we are much less subject to these issues. We wish to process a video into a more aesthetically pleasing version, by borrowing information from high quality reference photographs of the same scene. Since the process of taking a photograph is not time-critical, we can afford a longer exposure for reducing noise, and record more information to increase resolution. Also, photographs are less prone to motion blur if a tripod is used.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893107</person_id>
				<author_profile_id><![CDATA[81365594532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cosmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ancuti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048662</person_id>
				<author_profile_id><![CDATA[81335491115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051661</person_id>
				<author_profile_id><![CDATA[81335494509]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mertens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023592</person_id>
				<author_profile_id><![CDATA[81100093388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bekaert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>618889</ref_obj_id>
				<ref_obj_pid>616075</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Freeman, W. T., Jones, T. R., and Pasztor, E. C. 2002. Example-based super-resolution. <i>IEEE Computer Graphics and Applications 22</i>, 2 (March/April), 56--65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, A., Jacobs, C. E., Oliver, N., Curless, B., and Salesin, D. H. 2001. Image analogies. In <i>SIGGRAPH 2001, Computer Graphics Proceedings</i>, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882264</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kwatra, V., Schdl, A., Essa, I., Turk, G., and Bobick, A. 2003. Graphcut textures: Image and video synthesis using graph cuts. <i>ACM Transactions on Graphics, SIGGRAPH 2003 22</i>, 3 (July), 277--286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>996342</ref_obj_id>
				<ref_obj_pid>993451</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lowe, D. 2004. Distinctive image features from scale-invariant keypoints. In <i>International Journal of Computer Vision</i>, vol. 20, 91--110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280797</article_id>
		<sort_key>70</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[GPU-oriented light field compression for real-time streaming]]></title>
		<page_from>70</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280797</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280797</url>
		<abstract>
			<par><![CDATA[<p>Emerging applications such as free-viewpoint video and 3D-TV can enhance our viewing experience by rendering arbitrary viewpoint images using transmitted light field data. Compression is one of the key technologies for such systems due to the huge amount of data, typically captured with hundreds of cameras or thousands of lenslets.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893236</person_id>
				<author_profile_id><![CDATA[81335487659]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034819</person_id>
				<author_profile_id><![CDATA[81320495523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P422611</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Taguchi, Y., and Naemura, T. 2006. View-dependent coding of light fields based on free-viewpoint image synthesis. In <i>Proc. IEEE Int. Conf. Image Processing (ICIP 2006)</i>, 509--512.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186172</ref_obj_id>
				<ref_obj_pid>1186155</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yamamoto, T., Kojima, M., and Naemura, T., 2004. LIFLET: Light field live with thousands of lenslets. ACM SIGGRAPH 2004 Emerging Technologies #0130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280798</article_id>
		<sort_key>71</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Vectorization of gridded urban land use data]]></title>
		<page_from>71</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280798</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280798</url>
		<abstract>
			<par><![CDATA[<p>In the digital entertainment industry, cities are one of the largest artifacts modeled by artists. One alternative to modeling an entire city by hand is to use an urban simulation. Often, those simulations use a gridded terrain representation. Translating gridded simulation results into a more continuous, realistic representation can often be difficult. Our vectorization process transforms gridded urban land use data into a representation that mimics what might be seen in GIS or online mapping tools.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893106</person_id>
				<author_profile_id><![CDATA[81335497410]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sexton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Carolina State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39040047</person_id>
				<author_profile_id><![CDATA[81100375996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Carolina State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383292</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Parish, Y. I. H., and M&#252;ller, P. 2001. Procedural modeling of cities. In <i>Proc. SIGGRAPH 2001</i>, 301--308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358023</ref_obj_id>
				<ref_obj_pid>357994</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zhang, T. Y., and Suen, C. Y. 1984. A fast parallel algorithm for thinning digital patterns. <i>Commun. ACM 27</i>, 3, 236--239.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280799</article_id>
		<sort_key>72</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Decomposing non-rigid cell motion via kinematic skeletonization]]></title>
		<page_from>72</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280799</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280799</url>
		<abstract>
			<par><![CDATA[<p>Considering that most cells have an intricate spatial organization to their components and biological processes, it is beneficial to express sub-cellular positions and displacements in coordinates relative to the enclosing cell. This spatial context is often more meaningful than that of the microscope field of view in which a cell is observed. However, defining such a cell coordinate system can be difficult for cells which are moving rapidly and changing shape, requiring that one define a moving frame of reference to which measurements can be transformed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893108</person_id>
				<author_profile_id><![CDATA[81332535801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyrus]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Wilson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893162</person_id>
				<author_profile_id><![CDATA[81335498432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Theriot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shi, J., and Tomasi, C. 1994. Good features to track. <i>Proceedings CVPR '94</i>, 593--600.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wilson, C. A., and Theriot, J. A. 2006. A correlation-based approach to calculate rotation and translation of moving cells. <i>IEEE Trans Image Process 15</i>, 7, 1939--51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280800</article_id>
		<sort_key>73</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Light source estimation using segmented HDR images]]></title>
		<page_from>73</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280800</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280800</url>
		<abstract>
			<par><![CDATA[<p>To achieve seamless integration of the real scene with virtual objects, we need to obtain accurate illumination condition of the real scene. Using HDRI(High Dynamic Range Image) we can acquire and control real world illumination information. There are various intensity levels of light sources in a HDRI; some regions are very bright and others are not. Bright regions contain much more lighting information than the other regions. Thus in this paper we propose a light source estimation method by using the ratio of intensity of radiation from segmented HDR images which are divided by characteristics of histogram value. This paper describes three mains steps performed in the research. First, we segment an image efficiently; second, we analyze the distribution of intensity of radiation. Finally, we estimate light sources from segmented HDR images by using the ratio of intensity of radiation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Fuzzy-C mean]]></kw>
			<kw><![CDATA[HDRI]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893148</person_id>
				<author_profile_id><![CDATA[81335500061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jae]]></first_name>
				<middle_name><![CDATA[Doug]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893155</person_id>
				<author_profile_id><![CDATA[81421593527]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ji]]></first_name>
				<middle_name><![CDATA[Ho]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050624</person_id>
				<author_profile_id><![CDATA[81335492623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hoe]]></first_name>
				<middle_name><![CDATA[Min]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052304</person_id>
				<author_profile_id><![CDATA[81335495725]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kang]]></first_name>
				<middle_name><![CDATA[Su]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893216</person_id>
				<author_profile_id><![CDATA[81452602365]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Seung]]></first_name>
				<middle_name><![CDATA[Joo]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050006</person_id>
				<author_profile_id><![CDATA[81319495441]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kwan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187029</ref_obj_id>
				<ref_obj_pid>1186954</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 2005. A median cut algorithm for light probe sampling. In <i>Poster of SIGGRAPH 2005</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>202684</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Klir, G., and Yuan, B. 2002. <i>Fuzzy Sets and Fuzzy Logic Thory and Applications</i>. Prentice Hall of India.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280801</article_id>
		<sort_key>74</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[A catadioptric projector system with application to pseudo HDR display]]></title>
		<page_from>74</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280801</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280801</url>
		<abstract>
			<par><![CDATA[<p>It has been shown in computer vision that combining reflective components (e.g. mirrors) with refractive components (e.g. lenses) can greatly increase the flexibility and functionality of an optic imaging system, such as increased field of view and reconstruction from a single image. Cameras with both lenses and mirrors are typically referred to as <i>catadioptric</i> systems. A projector, as the dual of a camera, can also benefit from catadioptric optics. For example, it is possible to bend the light rays to optically compensate for distortions caused by projection [Swaminathan et al. 2004]. However, different mirror shapes have to be used for different setups.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893209</person_id>
				<author_profile_id><![CDATA[81435604863]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baumann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084402</person_id>
				<author_profile_id><![CDATA[81547678156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P250287</person_id>
				<author_profile_id><![CDATA[81409593896]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ruigang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015797</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Seetzen, H., Heidrich, W., Stuerzlinger, W., Ward, G., Whitehead, L., Trentacoste, M., Ghosh, A., and Vorozcovs, A. 2004. High dynamic range display systems. In <i>SIGGRAPH '04: ACM SIGGRAPH 2004 Papers</i>, ACM Press, New York, NY, USA, 760--768.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Swaminathan, R., Nayar, S., and Grossberg, M. 2004. Designing Mirrors for Catadioptric Systems that Minimize Image Errors. In <i>Fifth Workshop on Omnidirectional Vision</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280802</article_id>
		<sort_key>75</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[From photographs to procedural facade models]]></title>
		<page_from>75</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280802</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280802</url>
		<abstract>
			<par><![CDATA[<p>The network-based visualization of large photo-realistic 3D landscape and city models has received significant attention over the last five years in the computer graphics and network community. To make such 3D geoweb services appealing to a large audience, 3D city models must reach a sufficient level of realism and faithfulness. The first problematic was the transmission and visualization of such complex models that required the use of view-dependent, compact and progressive representations. The combination of a multiresolution footprint based representation called <i>PBTree</i> [Royan et al. 2006] associated with a procedural based modeling of facades [Mueller et al. 2006] has proved its great efficiency for 3D geoweb applications. Secondly, the procedural modeling of buildings from real world acquisition is not so obvious. Therefore we will present next a solution to determine the structure of facades, in order to model real buildings using a procedural approach. Only the image based acquisition will be considering next, due to its low production cost and storage requirements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052947</person_id>
				<author_profile_id><![CDATA[81335496737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ricard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[France Telecom R&D]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P828925</person_id>
				<author_profile_id><![CDATA[81319500119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Royan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[France Telecom R&D]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P828965</person_id>
				<author_profile_id><![CDATA[81320487852]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[O.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aubault]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[France Telecom R&D]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141931</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mueller, P., Wonka, P., Haegler, S., Ulmer, A., and Gool., L. V. 2006. Procedural modeling of buildings. <i>ACM Transactions on Graphics, SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1249312</ref_obj_id>
				<ref_obj_pid>1249243</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Royan, J., Balter, R., and Bouville, C. 2006. Hierarchical representation of virtual cities for progressive transmission over networks. In <i>3DPVT</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280803</section_id>
		<sort_key>76</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Animation]]></section_title>
		<section_page_from>76</section_page_from>
	<article_rec>
		<article_id>1280804</article_id>
		<sort_key>76</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The basic and general idea of motion]]></title>
		<page_from>76</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280804</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280804</url>
		<abstract>
			<par><![CDATA[<p>People recognize a complex phenomenon divided into units, and animation is divided into three main elements: "How did, what and when" too. These Elements can be subdivided more, but the subdividing levels have to be in the user' s hands. This proposal provides "ReTeMo" to be composed by three tree structures as "Region, Tempo and Movement." ReTeMo does need complex algorithms for its execution, however it has excellent analyticity and reusability, and it is a user-friendly data structure.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893176</person_id>
				<author_profile_id><![CDATA[81335497275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimohira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Coduchi Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[TVML (TV program making language) {Hayashi.1998}]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280805</article_id>
		<sort_key>77</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A method of melting snow animation using mathematical morphology]]></title>
		<page_from>77</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280805</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280805</url>
		<abstract>
			<par><![CDATA[<p>Recently, computer graphics has been often used to express natural phenomena. In this study, we propose a new technique for representing the appearance of snow melting by a heat source in virtual space. The conventional methods focus on heat calculations from the source. However, these approaches do not consider that water refreezes after melting. In order to express the snow melting phenomenon with a high degree of realism, we introduce the Mathematical Morphology method to reduce the processing cost, and to consider that water infiltrates into the snow and freezes again. In addition, some experimental results show that images generated by this method are more realistic than conventional ones. Our method achieves the effect that the snow melts on the side facing the heat source, and that the hollow part of the snow is again filled by refreezing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P759015</person_id>
				<author_profile_id><![CDATA[81309499607]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teshima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Shizuoka Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P759012</person_id>
				<author_profile_id><![CDATA[81309510014]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima International University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051102</person_id>
				<author_profile_id><![CDATA[81335495395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Koji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893178</person_id>
				<author_profile_id><![CDATA[81335492956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kenichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280806</article_id>
		<sort_key>78</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Hair motion reconstruction using motion capture system]]></title>
		<page_from>78</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280806</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280806</url>
		<abstract>
			<par><![CDATA[<p>Processes such as modeling, rendering and simulating hair are essential for creating virtual humans in various CG applications. CG hair motion simulations are commonly constructed using a physics-based model and have been extensively researched. While physics-based approaches have achieved effective and sophisticated hair motion, data-driven approaches for hair motion have been scarcely researched. This paper therefore outlines our hair animation approach which uses Motion Capture System to capture 3D hair motion. Our approach enables users to easily simulate complex hair motions such as the collision between hair strands and the human body and the movement of hair strands as they are blown in a variety of directions by wind.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893227</person_id>
				<author_profile_id><![CDATA[81100353688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takahito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P794445</person_id>
				<author_profile_id><![CDATA[81315489101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kazama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P794422</person_id>
				<author_profile_id><![CDATA[81315491993]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugisaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023348</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073316</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[L. Ren,"A Data-Driven approach to Quantifying Natural Human Motion", in proceedings of SIGGRAPH 2005 pp 1090--1097.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280807</article_id>
		<sort_key>79</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Magnetic motion capture system measuring movements of hands and a body simultaneously]]></title>
		<page_from>79</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280807</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280807</url>
		<abstract>
			<par><![CDATA[<p>The motion capture (MoCap) has become a premiere technique for measuring the human movements. The MoCap has been used to create computer animation for the cinema and video games, and to measure a dance performance. As for measurement of the hand the various systems exist, but they cannot measure the movement accurately, because the hand moves complicatedly. Hence, we have developed the highly accurate 'Hand MoCap system' by using the electromagnetic tracker (LIBERTY#8482; 16 system, Polhemus) that used small and light receivers [Mitobe et al. 2006].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP18006728</person_id>
				<author_profile_id><![CDATA[81319494392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaiga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Warabi-za]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034258</person_id>
				<author_profile_id><![CDATA[81319504710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Asia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822742</person_id>
				<author_profile_id><![CDATA[81319497817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kazutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823019</person_id>
				<author_profile_id><![CDATA[81331499988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034226</person_id>
				<author_profile_id><![CDATA[81100502669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hideo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822879</person_id>
				<author_profile_id><![CDATA[81319504709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Noboru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179740</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mitobe, K., et al. 2006. Development of a Motion Capture System for a Hand Using a Magnetic Three Dimensional Position Sensor, <i>ACM SIGGRAPH2006, Research Posters</i>, 115]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280808</article_id>
		<sort_key>80</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Directable crowd animation]]></title>
		<page_from>80</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280808</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280808</url>
		<abstract>
			<par><![CDATA[<p>We present an interactive system for creating directable crowd animation, focusing on making the CG crowd shots suited especially for a TV program. Our system cannot make such "advanced" crowd animations as those for a blockbuster movie, which were made, for example, using AI techniques. However, our system allows a user to quickly make the crowd scenes under a limited time and budget. Our "handy" crowd system is currently available as a Maya plug-in our workplace, offering several features, which we believe would be useful enough to quickly make various crowd shots.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893240</person_id>
				<author_profile_id><![CDATA[81335491862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wataru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishihara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NHK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39082957</person_id>
				<author_profile_id><![CDATA[81100085512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anjyo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[OLM Digital]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Raynolds, C. 1987. Flocks, Herds, and Schools: A Distributed Behavioral Model. In <i>Proceedings of SIGGRAPH 87</i>, 15--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Unuma, M., Anjyo, K., and Takeuch, R. 1995. Modeling Crowd: A descriptive method of interaction among crowd and their environment in a virtual scity. IEEJ 115-C 2, 212--221. (in Japanese)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280809</article_id>
		<sort_key>81</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Evaluation of similarity of motion in dancing using information of correlation relationship in motion characteristics]]></title>
		<page_from>81</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280809</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280809</url>
		<abstract>
			<par><![CDATA[<p>In the field of analysis of motion in dancing, application of factor analysis to motion capture data was suggested as the method for extraction of primitive motions in a dance [Miura et al. 2006]. In this study, we try to evaluate the similarity of plural dances, by using the data of characteristics in motion obtained by the above factor-analysis method.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P823019</person_id>
				<author_profile_id><![CDATA[81331499988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822742</person_id>
				<author_profile_id><![CDATA[81319497817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18006728</person_id>
				<author_profile_id><![CDATA[81319494392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaiga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Warabi-za]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034258</person_id>
				<author_profile_id><![CDATA[81319504710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Asia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823058</person_id>
				<author_profile_id><![CDATA[81319502599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Toshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taniguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822879</person_id>
				<author_profile_id><![CDATA[81319504709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Noboru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179741</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Miura, T., Mitobe, K., Kaiga, T., Yukawa, T., Taniguchi, T. and Yoshimura, N. 2006. Extraction of Characteristic Motion in Dancing by Factor Analysis, <i>ACM SIGGRAPH2006, Research Posters</i>, 103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shigeizumi, Y. and Kudo, E. 1981. The History of Folkdance "AKITA ONDO", <i>Memoirs of the Faculty of Education, Akita University, Educational Science</i>, 31, 114--126 (in Japanese).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280810</article_id>
		<sort_key>82</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Perceptually valid facial expression blending using expression units]]></title>
		<page_from>82</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280810</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280810</url>
		<abstract>
			<par><![CDATA[<p>The human face is a rich source of information regarding underlying emotional states. Facial expressions are crucial in showing the emotions as well as increasing the quality of communication and speech comprehension. The detailed study of facial actions involved in the expression of the six universal emotions [1] has helped the computer graphics community develop realistic facial animations. Yet the visual mechanisms by which these facial expressions are altered or combined to convey more subtle information remains less well understood by behavioural psychologists and animators. This lack of a strong theoretical basis for combining facial actions has resulted in the use of ad-hoc methods for blending facial expression in animations [2--3]. They mainly consider the facial movements for transient or combined expressions a simple mathematical function of the main expressions involved. The methods that have emerged are therefore computationally tractable, but the question of their "perceptual" and "psychological" validity has not yet been answered. Examples of such methods are "Sum of two expressions with or without limits," "Weighted averaging," and "MAX operator".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P794413</person_id>
				<author_profile_id><![CDATA[81315487846]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ali]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carleton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052428</person_id>
				<author_profile_id><![CDATA[81100256725]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Avi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parush]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carleton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893093</person_id>
				<author_profile_id><![CDATA[81335494519]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alicia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMullan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carleton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ekman, P., Emotions Revealed, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Paradiso, A., "An Algebra of Facial Expressions," Proceedings of ACM SIGGRAPH, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>259313</ref_obj_id>
				<ref_obj_pid>259081</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Perlin, K., "Layered Compositing of Facial Expression," Sketches of ACM SIGGRAPH, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Russell, J. A., "A Circumplex Model of Affect," Journal of Personality and Social Psychology, 39, 1161--1178, 1980.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280811</article_id>
		<sort_key>83</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Global illumination using precomputed light paths for interactive light condition manipulation]]></title>
		<page_from>83</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280811</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280811</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we propose an interactive rendering method for taking into account global illumination using light path precomputation and a hierarchical data structure for fast final gathering. Our method can render static scenes interactively allowing the user to move the viewpoint and the positions of local light sources. More-over, our method can change the materials (including the textures, diffuse and low-frequency glossy BRDFs) of objects at run-time. In recent years, many methods have been proposed to achieve interactive rendering by precomputing light transfer, for example, [Hasan et al. 2006]. These methods, however, do not allow the user to modify the materials of objects interactively.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891771</person_id>
				<author_profile_id><![CDATA[81384596311]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yonghao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35041108</person_id>
				<author_profile_id><![CDATA[81331494952]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wakayama Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P306681</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36036593</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383551</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. H., and Batali, D. 2004. An irradiance atlas for global illumination in complex production scenes. In <i>Rendering Techniques</i>, 133--142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141998</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hasan, M., Pellacini, F., and Bala, K. 2006. Direct-to-indirect transfer for cinematic relighting. In <i>SIGGRAPH '06</i>, ACM Press, New York, NY, USA, 1089--1097.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280812</article_id>
		<sort_key>84</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Interface techniques for 3D control of spatial keyframing]]></title>
		<page_from>84</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280812</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280812</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893144</person_id>
				<author_profile_id><![CDATA[81335495027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Igor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mordatch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047655</person_id>
				<author_profile_id><![CDATA[81100290369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083980</person_id>
				<author_profile_id><![CDATA[81335497253]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Karan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P239617</person_id>
				<author_profile_id><![CDATA[81100503904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ravin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Balakrishnan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1029644</ref_obj_id>
				<ref_obj_pid>1029632</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Grossman, T., Wigdor, D., and Balakrishnan, R. 2004. Multi-Finger Gestural Interaction with 3D Volumetric Displays. In <i>Proceedings of the UIST 2004</i>, 61--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073383</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Moscovich, T., and Hughes, J. F. 2005. Spatial Keyframing for Performance-Driven Animation. In <i>Proceedings of SCA 2005</i>, 107--115.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280813</article_id>
		<sort_key>85</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Reactive episode control system]]></title>
		<page_from>85</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280813</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280813</url>
		<abstract>
			<par><![CDATA[<p>Constructing a narrative ecosystem is one of the important technologies for many computer games. This paper presents a reactive episode control system that allows for dynamic creation of a long-term story in response to the active interaction between multiple characters and the user's actions. This system makes possible a story world where the user's actions not only create reactions among the characters that are currently present, but also influence the long-term story events.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893157</person_id>
				<author_profile_id><![CDATA[81335493236]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Komura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39051683</person_id>
				<author_profile_id><![CDATA[81100633427]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893124</person_id>
				<author_profile_id><![CDATA[81335495110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39033599</person_id>
				<author_profile_id><![CDATA[81451595006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280814</article_id>
		<sort_key>86</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Speech driven head motion synthesis based on a trajectory model]]></title>
		<page_from>86</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280814</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280814</url>
		<abstract>
			<par><![CDATA[<p>Making human-like characters more natural and life-like requires more inventive approaches than current standard techniques such as synthesis using text features or triggers. In this poster we present a novel approach to automatically synthesise <i>head motion</i> based on speech features. Previous work has focused on frame wise modelling of motion [Busso et al. 2007] or has treated the speach data and motion data streams separately [Brand 1999], although the trajectories of the head motion and speech features are highly correlated and dynamically change over several frames. To model longer units of motion and speech and to reproduce their trajectories during synthesis, we utilise a promising time series stochastic model called "Trajectory Hidden Markov Models" [Zen et al. 2007]. Its parameter generation algorithm can produce motion trajectories from sequences of units of motion and speech. These two kinds of data are simultaneously modelled by using a multi-stream type of the trajectory HMMs. The models can be viewed as a Kalman-smoother-like approach, and thereby are capable of producing smooth trajectories.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893132</person_id>
				<author_profile_id><![CDATA[81335491640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gregor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hofer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052273</person_id>
				<author_profile_id><![CDATA[81541360956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimodaira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053684</person_id>
				<author_profile_id><![CDATA[81100338274]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamagishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311537</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brand, M. 1999. Voice puppetry. In <i>Proc. of SIGGRAPH '99</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2210057</ref_obj_id>
				<ref_obj_pid>2209795</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Busso, C., Deng, Z., Grimm, M., Neumann, U., and Narayanan, S. 2007. Rigid Head Motion in Expressive Speech Animation: Analysis and Synthesis. <i>IEEE Transactions on ASLP</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zen, H., Tokuda, K., and Kitamura, T. 2007. Reformulating the HMM as a trajectory model by imposing explicit relationships between static and dynamic feature vector sequences. <i>Computer Speech and Language 21</i>, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280815</article_id>
		<sort_key>87</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Behavior-graph for crowd simulation]]></title>
		<page_from>87</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280815</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280815</url>
		<abstract>
			<par><![CDATA[<p>Computer generated crowds can be found in films, commercials, computer games and other virtual environment applications. Usually, crowd simulation techniques, such as [Treuille et al. 2006; Lerner et al. 2007] focus on controlling the direction and speed of the simulated agents, aiming at generating realistic crowds at the trajectory level. In principle, if the simulated paths reflect paths taken by real people, then the agents would appear to behave correctly. However, the definition of behavior does not end at the trajectory level. It can be defined as the aggregate actions of a person to internal or external stimuli. Talking to nearby people, or glancing at something of interest are common actions performed by a person in a crowd. Actions such as these, which are often missing from simulated crowds, are vital for generating seemingly natural behaviors. Ideally, actions should be generated by the crowd simulation algorithm itself. However, in order to enrich existing simulations, one can examine its output, deduce the stimuli of each simulated agent in the scene and assign plausible actions that are compatible with their trajectory. In this sketch we describe a technique for adding behavior information to simulated crowds as can be seen in the accompanying video.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP36044365</person_id>
				<author_profile_id><![CDATA[81335493753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lerner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893119</person_id>
				<author_profile_id><![CDATA[81335490310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eitan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fitusi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P305893</person_id>
				<author_profile_id><![CDATA[81100561258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yiorgos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chrysanthou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cyprus]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40035529</person_id>
				<author_profile_id><![CDATA[81100264399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen-Or]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tel Aviv University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lerner, A., Chrysanthou, Y., and Lischinski, D. 2007. Crowds by example. to appear in EG2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142008</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Treuille, A., Cooper, S., and Popovic, Z. 2006. Continuum crowds. <i>ACM Trans. Graph. 25</i>, 3, 1160--1168.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280816</article_id>
		<sort_key>88</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Imposing constraints on fragmented body motion for synthesis]]></title>
		<page_from>88</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280816</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280816</url>
		<abstract>
			<par><![CDATA[<p>The generation of realistic motion of human figures for computer games and animation has been under active research in recent years. Since the nature of human motion is complex, motion capture is commonly used to obtain high-quality motion. Although capturing motions directly from human can acquire life-like motion, these motions cannot be easily reused. In order to maximize usage of captured motions, existing approaches attempt to reuse by combining motions of specific parts of the body [Ikemoto and Forsyth 2004] or by splicing actions with locomotion [Heck et al. 2006] to create new motions. While these approaches explore conditions for combining specific body parts to generate convincing full body motions, we lack a generalized representation for animators to specify the conditions under which motions from which body parts can be synthesized to produce convincing full body motions. With a generalized representation, it is possible for animators to make use of a motion library of body parts to synthesize a wide variety of motions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893241</person_id>
				<author_profile_id><![CDATA[81335495434]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[Wai-Lam]]></middle_name>
				<last_name><![CDATA[Ng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Hong Kong Polytechnic University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047571</person_id>
				<author_profile_id><![CDATA[81451592487]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Clifford]]></first_name>
				<middle_name><![CDATA[Sze-Tsan]]></middle_name>
				<last_name><![CDATA[Choy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Hong Kong Polytechnic University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Heck, R., Kovar, L., and Gleicher, M. 2006. Splicing Upper-Body Actions with Locomotion. <i>Computer Graphics Forum</i>, 25, 3, 459--466]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028537</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ikemoto L. and Forsyth D. A. 2004. Enriching a Motion Collection by Transplanting Limbs. <i>In Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 99--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kovar, L., Gleicher, M., and Pighin, F. 2002. Motion Graphs. <i>ACM Transactions on Graphics</i>, 21, 3, 473--482.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280817</article_id>
		<sort_key>89</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[DirectCam]]></title>
		<subtitle><![CDATA[a gestural system for animatic creation]]></subtitle>
		<page_from>89</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280817</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280817</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P891743</person_id>
				<author_profile_id><![CDATA[81335494212]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Noah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lockwood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891746</person_id>
				<author_profile_id><![CDATA[81335497206]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patricio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Simari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047655</person_id>
				<author_profile_id><![CDATA[81100290369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083980</person_id>
				<author_profile_id><![CDATA[81335497253]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Karan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280818</article_id>
		<sort_key>90</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Extraction of a feature of a word expressing a human motion from motion capture data]]></title>
		<page_from>90</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280818</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280818</url>
		<abstract>
			<par><![CDATA[<p>In this study, we propose a method to quantify a relation between a feature of a human motion and a word used to express its name using the subspace method. Motion name is divided into some words, and it is examined which component of a feature vector is related to each word. The proposed method is applicable to a motion database [Yukawa et al. 2000]. Using this method, 1) a search key can be automatically generated from the relationship between the features of motion data and their name when inserting human motion data into the database, and 2) we can search required human motion data from the database using a motion word as a query in the same way as is used in a search engine of a web system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35034258</person_id>
				<author_profile_id><![CDATA[81319504710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Asia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18006728</person_id>
				<author_profile_id><![CDATA[81319494392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaiga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Warabi-za]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823019</person_id>
				<author_profile_id><![CDATA[81331499988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822742</person_id>
				<author_profile_id><![CDATA[81319497817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034226</person_id>
				<author_profile_id><![CDATA[81100502669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hideo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822879</person_id>
				<author_profile_id><![CDATA[81319504709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Noboru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yukawa, T., Kaiga, T., Nagase, K. and Tamamoto, H. 2000. Human Motion Description System by Using BUYO-FU. <i>IPSJ Journal</i>, 41, 10, 2873--2880.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280819</article_id>
		<sort_key>91</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Nearly rigid deformation by linear optimization]]></title>
		<page_from>91</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280819</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280819</url>
		<abstract>
			<par><![CDATA[<p>Fast near-rigid 3D deformation of a shape is achieved by approximating a rigid transformation of each point by a moving least-squares (MLS) optimization with a linear closed-form solution. We can deform tens of thousands points in real time while preserving the rigidity of the original shape as far as possible. This technique inherits the properties of earlier MLS-based deformation techniques, such as independence of geometric representation and smoothness. The user can control the deformations by using control points or line segments and by the adjustment of weight functions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141920</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Schaefer, S., McPhail, T., and Warren, J. 2006. Image Deformation using Moving Least Squares. <i>ACM Transaction on Graphics</i>, 25, 3, 533--540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280820</article_id>
		<sort_key>92</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Insect project]]></title>
		<subtitle><![CDATA[simulation of the crowd behaviour of insect / Chinese calligraphy (single / dual screen)]]></subtitle>
		<page_from>92</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280820</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280820</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280821</article_id>
		<sort_key>93</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Keyframe animation using an artist's doll]]></title>
		<page_from>93</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280821</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280821</url>
		<abstract>
			<par><![CDATA[<p>Keyframe animation of an articulated human figure is a difficult task due to the large number of degrees of freedom inherent in the model's 3D joints, the complex range of motion and the vast amount of possible interpolations between keyframes. Existing software solutions, such as Maya and Poser, try to solve these problems by providing extensive user control over positioning and orientation of joints, etc. However, due to the sheer number of adjustable parameters and settings in these interfaces, skills of a master animator are required to create satisfactory results.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893203</person_id>
				<author_profile_id><![CDATA[81335491263]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Prabath]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gunawardane]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893118</person_id>
				<author_profile_id><![CDATA[81335488898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eddy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chandra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893231</person_id>
				<author_profile_id><![CDATA[81335490240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tien-Chieng]]></first_name>
				<middle_name><![CDATA[Jack]]></middle_name>
				<last_name><![CDATA[Feng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39087694</person_id>
				<author_profile_id><![CDATA[81100603625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>199424</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Esposito, C., Paley, W. B., Ong, J. C.: Of mice and monkeys: a specialized input device for virtual body animation. In: Proc. of the Symposium on Interactive 3D Graphics, 1995, pp. 109--114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kovar, L., Gleicher, M., Pighin, F. 2002: Motion graphs. In Proceedings of SIGGRAPH 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280822</article_id>
		<sort_key>94</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Human motion perception]]></title>
		<subtitle><![CDATA[does actor size matter in motion capture?]]></subtitle>
		<page_from>94</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280822</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280822</url>
		<abstract>
			<par><![CDATA[<p>The rapid growth of the entertainment industry, for both cinema and video games applications, has increased the demand for an improved simulation and representation of reality. Of particular interest is the capture and simulation of human characters' motion. <i>Motion capture (Mocap)</i> is one of the techniques utilized in the entertainment industry for character animation. The kinematic patterns of an actor are acquired using a system of cameras which track the position of retroreflective markers placed over anatomical landmarks on the subject. An anatomical model of the actor is derived from the marker positions and finally the acquired motion is applied to the graphically designed character.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893202</person_id>
				<author_profile_id><![CDATA[81335488781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pierfrancesco]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Celada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Newcastle University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893220</person_id>
				<author_profile_id><![CDATA[81335493503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sian.]]></first_name>
				<middle_name><![CDATA[E. M.]]></middle_name>
				<last_name><![CDATA[Lawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Newcastle University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bernstein, N. 1967. Oxford, England: Pergamon Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pollick, F. E et al. <i>Vision research</i>, &#60;b&#62;42&#60;/b&#62;, 2345--2355]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Runeson, S., Frikholm, G. 1983. <i>J Exp Psy G</i>, &#60;b&#62;112&#60;/b&#62; (4) 585--615.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Stevenage, S. V., Nixon, M. S., Vince, K. 1999 <i>App Cog Psy</i>, &#60;b&#62;13&#60;/b&#62;, 513--526.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280823</article_id>
		<sort_key>95</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Smooth key framing using the image plane]]></title>
		<page_from>95</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280823</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280823</url>
		<abstract>
			<par><![CDATA[<p>Key framing is an integral part of the animation making process. The user places the camera in a sequence of "key" positions, and the computer produces a set of intermediate camera locations that interpolates between these key frames. Traditional methods treat the camera as just another 3D object in the scene, with intermediate frames produced by interpolating the position and focal point of the camera in space. Unlike a 3D object, however, the camera's role is to <i>project</i> the 3D scene into 2D. The user indirectly controls the projection -- how objects in the scene are placed in the 2D image -- by adjusting the camera parameters for each key frame. Automatic 3D camera interpolation adds yet another layer of indirection. The net effect is that the user must solve a complicated inverse problem in order to move objects across the 2D scene in the desired manner. Figure 1 shows an example where the user wanted the table to move down and across the scene while moving the view to the top of the table. Traditional interpolation (top row) rotates the table out of the view. Correcting this using traditional approaches requires the addition of a substantial number of key frames which in turn reduce the smoothness of the motion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822946</person_id>
				<author_profile_id><![CDATA[81331504821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ross]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Sowell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P48640</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35047059</person_id>
				<author_profile_id><![CDATA[81309511366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Leon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barrett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. 1988. Where am i? what am i looking at? In <i>IEEE Computer Graphics and Applications</i>. Vol. 22. 179--188.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134088</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gleicher, M. and Witkin, A. 1992. Through-the-lens camera control. In <i>Siggraph</i>, E. E. Catmull, Ed. Vol. 26. 331--340. ISBN 0-201-51585-7. Held in Chicago, Illinois.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1029648</ref_obj_id>
				<ref_obj_pid>1029632</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Singh, K., Grimm, C., and Sudarsanan, N. 2004. The ibar: a perspective-based camera widget. In <i>UIST</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280824</article_id>
		<sort_key>96</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[A compact representation for articulated human motion]]></title>
		<page_from>96</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280824</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280824</url>
		<abstract>
			<par><![CDATA[<p>While motion capture data provides a rich resource for representing articulated human motion, the huge size of such data makes reusing and editing difficult. Although there has been significant research into compression of mesh animation [Alexa and M&#252;ller 2000], little work on finding more compact representations for mocap data exists.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893102</person_id>
				<author_profile_id><![CDATA[81100399701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chaohua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034199</person_id>
				<author_profile_id><![CDATA[81100575511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lasenby]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alexa, M., and M&#252;ller, W. 2000. Representing animations by principal components. <i>Comput. Graph. Forum 19</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141971</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Arikan, O. 2006. Compression of motion capture databases. <i>ACM Transactions on Graphics</i>, 890--897.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Capin, T. K., Petajan, E., and Ostermann, J. 2000. Efficient modeling of virtual humans in mpeg-4. <i>Proceedings of ICME</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179742</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lee, C.-H., and Lasenby, J. 2006. 3d human motion compression. <i>International Conference on Computer Graphics and Interactive Techniques archive ACM SIGGRAPH 2006 Research posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280825</article_id>
		<sort_key>97</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Real-time generation of CG and sound of liquid with bubble]]></title>
		<page_from>97</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280825</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280825</url>
		<abstract>
			<par><![CDATA[<p>Recently, we can render real-time CG of liquid which shows various phenomena by global lighting and physical simulation according to the improvement of the computing power. However, the effect sound for real-time CG of liquid should be made corresponding to all the behavior of the liquid beforehand. Therefore, it is difficult for high reproducibility to be made for all sounds by hand. In liquid, the sound is generated according to the change of the radius of the moving bubble. Therefore, making the effect sound of liquid should consider the movement of bubbles.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P799925</person_id>
				<author_profile_id><![CDATA[81317494648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masataka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311021400</person_id>
				<author_profile_id><![CDATA[81538868156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P590035</person_id>
				<author_profile_id><![CDATA[81100057947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yasumuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P421244</person_id>
				<author_profile_id><![CDATA[81100289747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoshitsugu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P421414</person_id>
				<author_profile_id><![CDATA[81100358111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kunihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chihara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280826</article_id>
		<sort_key>98</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Smart motion synthesis]]></title>
		<page_from>98</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280826</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280826</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37040471</person_id>
				<author_profile_id><![CDATA[81335495744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oshita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{HKG06} Rachel Heck, Lucas Kovar, Michael Gleicher. Splicing Upper-Body Actions with Locomotion. Computer Graphics Forum (EUROGRAPHICS 2006), Volume 25, Issue 3, pp. 459--466, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028567</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{MKMA04} S. Menardais, R. Kulpa, F. Multon, B. Arnaldi. Synchronization for dynamic blending of motions. ACM SIGGRAPH/Eurographics 2004, pp. 325--335, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280827</section_id>
		<sort_key>99</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Production]]></section_title>
		<section_page_from>99</section_page_from>
	<article_rec>
		<article_id>1280828</article_id>
		<sort_key>99</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[From movie to comic, informed by the screenplay]]></title>
		<page_from>99</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280828</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280828</url>
		<abstract>
			<par><![CDATA[<p>Semi-automated solutions can transform the images of a film into a comic strip. Whereas the conversion of movies [Hwang et al. 2006] and 3D games [Shamir et al. 2006] to comics has been addressed before, we propose novel ways of user interaction and--most prominently--leverage much information from a movie's screenplay. Thanks to standard screenplays' highly structured "studio format," we can employ textual analysis at several stages:</p> <p>&#8226; The line counts of the scenes in the screenplay are used to estimate every scene's temporal placement in the film.</p> <p>&#8226; The scenes' dialogue is extracted from the screenplay and turned into speech balloons placed sequentially.</p> <p>&#8226; Verbal directions such as (whispers) are recognized and mirrored by corresponding type and speech balloon styles.</p> <p>&#8226; If a character's dialogue is interrupted by actions, its different parts are used to form a double-burger style speech balloon.</p> <p>&#8226; Offscreen speech is put into a speech balloon emanating from a panel's side; a voice-over is placed in a rectangle.</p> <p>&#8226; Verbs in action lines such as The VASE crashes onto the floor. are recognized and turned into noise balloons.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893147</person_id>
				<author_profile_id><![CDATA[81335496341]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jacqueline]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Preu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hochschule Bremen]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051569</person_id>
				<author_profile_id><![CDATA[81408599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loviscach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hochschule Bremen]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hwang, W.-I., Lee, P.-J., Chun, B.-K., Ryu, D.-S., and Cho, H.-G. 2006. Cinema comics: Cartoon generation from video stream. In <i>Proc. of GRAPP 2006</i>, 299--304.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1137275</ref_obj_id>
				<ref_obj_pid>1137231</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shamir, A., Rubinstein, M., and Levinboim, T. 2006. Parametric comics creation from 3D interaction. <i>IEEE Computer Graphics and Applications 26</i>, 3, 53--61.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142018</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Winnem&#246;ller, H., Olsen, S. C., and Gooch, B. 2006. Real-time video abstraction. <i>ACM TOG 25</i>, 3, 1221--1226.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280829</article_id>
		<sort_key>100</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Interactive city generation methods]]></title>
		<page_from>100</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280829</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280829</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822589</person_id>
				<author_profile_id><![CDATA[81319494191]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kelly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Technology Blanchardstown, Ireland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822630</person_id>
				<author_profile_id><![CDATA[81319497537]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hugh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McCabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Technology Blanchardstown, Ireland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lechner, T., Watson, B., Ren, P., Wilensky, U., Tisue, S., and Felsen, M. 2004. Procedural modeling of land use in cities. Tech. rep., Northwestern University, August.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383292</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Parish, Y. I. H., and M&#252;ller, P. 2001. Procedural modeling of cities. In <i>SIGGRAPH 2001, Computer Graphics Proceedings</i>, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., 301--308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>585747</ref_obj_id>
				<ref_obj_pid>585740</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sun, J., Yu, X., Baciu, G., and Green, M. 2002. Template-based generation of road networks for virtual city modeling. In <i>VRST-02</i>, ACM Press, H. Sun and Q. Peng, Eds., 33--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280830</article_id>
		<sort_key>101</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Developing software for translation of comic strips]]></title>
		<page_from>101</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280830</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280830</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P681688</person_id>
				<author_profile_id><![CDATA[81100330777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kyoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P548636</person_id>
				<author_profile_id><![CDATA[81100084997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Qinglian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP38026984</person_id>
				<author_profile_id><![CDATA[81319500804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Norio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823103</person_id>
				<author_profile_id><![CDATA[81319493163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yuko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893193</person_id>
				<author_profile_id><![CDATA[81335497110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Masahito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179710</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Qinglian Guo, Kyuko Kato, Norio Sato, Yuko Hoshino, An algorithm for extracting text strings from comic strips, SIGGRAPH2006, Research poster, Boston, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280831</article_id>
		<sort_key>102</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[10.000 samurais in the War of Sekigahara]]></title>
		<page_from>102</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280831</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280831</url>
		<abstract>
			<par><![CDATA[<p>For this two-minute 4K movie production, the continous battle flow is simulated with a custom crowd animation system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893173</person_id>
				<author_profile_id><![CDATA[81335495557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kaoru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ogata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Digital Hollywood Media Science Laboratory (Tokyo, Japan)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893183</person_id>
				<author_profile_id><![CDATA[81335498017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Digital Hollywood Media Science Laboratory (Tokyo, Japan)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893199</person_id>
				<author_profile_id><![CDATA[81335490544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Noboru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Digital Hollywood Media Science Laboratory (Tokyo, Japan)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893138</person_id>
				<author_profile_id><![CDATA[81335487760]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hisashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arakaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Digital Hollywood Media Science Laboratory (Tokyo, Japan)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893196</person_id>
				<author_profile_id><![CDATA[81335489237]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mihai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cioroba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Digital Hollywood Media Science Laboratory (Tokyo, Japan)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893235</person_id>
				<author_profile_id><![CDATA[81335498214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate School of Digital Hollywood Media Science Laboratory (Tokyo, Japan)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tomoyuki Sugiyama, Keiji Mitsubuchi, "Research of CG Production Environment for the Next Generation Super-highdefinition Image", IIEE of Japan, Vol.35, No5, pp422--427]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1129013</ref_obj_id>
				<ref_obj_pid>1129006</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dean Wright, Bill Westenhofer, Jim Benrney, Scott Farrar, "The Visual Effects of The Chronicles of Narnia: The Lion, the Witch and the Wardrobe", ACM Computers in Entertainment, Vol.4, Number 2, April 2006, Article 3A]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280832</article_id>
		<sort_key>103</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Shrek the Third]]></title>
		<subtitle><![CDATA[casting the crowded world]]></subtitle>
		<page_from>103</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280832</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280832</url>
		<abstract>
			<par><![CDATA[<p>The sketch focuses on the advances made in the process of casting crowd members on Shrek the Third, how and why it is different from procedural and rule-based casting on previous Shrek movies.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893237</person_id>
				<author_profile_id><![CDATA[81335496447]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vanitha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rangaraju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893167</person_id>
				<author_profile_id><![CDATA[81335495779]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Onstine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280833</article_id>
		<sort_key>104</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Tiled crack networks]]></title>
		<page_from>104</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280833</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280833</url>
		<abstract>
			<par><![CDATA[<p>This sketch presents a simple, easy-to-implement way to create realistic cracks and fracture fragments on a polygonal mesh.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052623</person_id>
				<author_profile_id><![CDATA[81100537885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raghavachary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280834</article_id>
		<sort_key>105</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Preference galleries for material design]]></title>
		<page_from>105</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280834</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280834</url>
		<abstract>
			<par><![CDATA[<p>Properly modeling the appearance of a material is very important for realistic image synthesis. The appearance of a material is formalized by the notion of the Bidirectional Reflectance Distribution Function (BRDF). In computer graphics, BRDFs are most often specified using various analytical models. Analytical models that are of interest to realistic image synthesis are the ones that observe the physical laws of reciprocity and energy conservation while typically also exhibiting shadowing, masking and Fresnel reflectance phenomenon. Realistic models are hence fairly complex with many parameters that need to be adjusted by the designer for the proper material appearance. Unfortunately these parameters can interact in non-intuitive ways, and small adjustments to certain settings may result in non-uniform changes in the appearance. This can make the material design process hard for an artist or a non-expert user. To alleviate this problem, Ngan et al. [2006] recently presented an interface for navigation in a perceptually uniform BRDF space based on a metric derived from user studies. However, this is still somewhat constraining as the user has to develop an understanding of the various aspects of material appearance such as varying degrees of diffuseness, glossiness, specularity, Fresnel effects and/or anisotropy in order to navigate such an interface. An artist or a user often knows the look that he or she desires for a particular application without necessarily being interested in understanding the various subtleties of reflection! This is what we seek to address in this work with a 'preference gallery' approach to material design.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893123</person_id>
				<author_profile_id><![CDATA[81335488303]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brochu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049692</person_id>
				<author_profile_id><![CDATA[81318494009]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048876</person_id>
				<author_profile_id><![CDATA[81335489736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nando]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[de Freitas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1102369</ref_obj_id>
				<ref_obj_pid>1102351</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chu, W., and Ghahramani, Z. 2005. Preference learning with Gaussian processes. In <i>Proc. ICML*2005</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383945</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ngan, A., Durand, F., and Matusik, W. 2006. Image driven navigation of analytical BRDF models. In <i>Proceedings of the Eurographics Symposium on Rendering</i>, 399--407.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280835</section_id>
		<sort_key>106</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Vision]]></section_title>
		<section_page_from>106</section_page_from>
	<article_rec>
		<article_id>1280836</article_id>
		<sort_key>106</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D point of regard and subject motion from a portable video-based monocular eye tracker]]></title>
		<page_from>106</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280836</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280836</url>
		<abstract>
			<par><![CDATA[<p>The ability to determine the motion of an observer and his/her <i>point of regard (POR)</i> in the world can be very beneficial to the study of human visual behavior and cognition. State-of-the-art eye-tracking systems output an observer's POR in 2D image or screen coordinates. Here we present a novel technique for determining POR in 3D world coordinates using a monocular video-based eye tracker. Currently, there are various techniques to determine an observer's gaze within an image sequence but many fewer techniques for determining his/her POR in three dimensions. Existing eye-tracking methods for determining a subject's 3D POR are limited to virtualreality applications and/or can only be used with binocular eye trackers. Techniques that provide 3D information in the real world are limited to gaze direction without precise report on POR.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P776157</person_id>
				<author_profile_id><![CDATA[81311483776]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Susan]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Kolakowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology, Rochester, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43116763</person_id>
				<author_profile_id><![CDATA[81100138613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Pelz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology, Rochester, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bouguet, J., 2007. http://www.vision.caltech.edu/bouguetj/calib_doc/. Camera Calibration Toolbox for Matlab&#174;, Accessed: April 3, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>373536</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hartley, R., and Zisserman, A. 2004. <i>Multiple View Geometry</i>, 2nd ed. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1117348</ref_obj_id>
				<ref_obj_pid>1117309</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kolakowski, S. M., and Pelz, J. B. 2006. Compensating for eye tracker camera movement. In <i>Eye Tracking Research and Applications (ETRA) '2006</i>, 79--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kovesi, P., 2007. http://www.csse.uwa.edu.au/~pk/research/matlabfns/. The University of Western Australia, Accessed: February 22, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1068543</ref_obj_id>
				<ref_obj_pid>1068500</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Repko, J., and Pollefeys, M. 2000. 3D models from extended uncalibrated video sequences: Addressing key-frame selection and projective drift. <i>3D Digital Imaging and Modeling (3DIM '05)</i>, 00, 150--157.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280837</article_id>
		<sort_key>107</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Subjective assessment of luminance compensation for projected images]]></title>
		<page_from>107</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280837</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280837</url>
		<abstract>
			<par><![CDATA[<p>This research focuses on the subjective assessment of luminance compensation for projected images on a non-uniform reflective screen. Recently, image projection on a large screen that includes an interior or exterior wall has been attracting a great deal of attention. However, this type of projection requires proper luminance compensation [Grundhofer and Bimber 2006]. It is difficult to compensate the luminance perfectly because of the hardware limitations of the projector. To obtain better results for the luminance compensation, with a large variety of non-uniform reflective screens and projected images, we examine how viewers feel about compensated images and how far the viewers can accept those images. The results of the subjective assessment show that the viewer-based limitation of the luminance compensation is not necessarily the same as the hardware limitations of the projector and this limitation cannot be found with only objective measurements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35053126</person_id>
				<author_profile_id><![CDATA[81100567066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Utsunomiya University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893190</person_id>
				<author_profile_id><![CDATA[81335497310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Makiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sembon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Utsunomiya University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36024479</person_id>
				<author_profile_id><![CDATA[81100135451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822827</person_id>
				<author_profile_id><![CDATA[81319494559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kasuga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Utsunomiya University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179686</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Grundhofer, A., and Bimber, O. 2006. Real-time adaptive radiometric compensation. <i>SIGGRAPH 2006, Research Posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179705</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hashimoto, N., Sato, M., Takahashi, Y., Kasuga, M., and Sato, M. 2006. Luminance correction with raw digital images. <i>SIGGRAPH 2006, Research Posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280838</article_id>
		<sort_key>108</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Camera motion estimation in ocean scene]]></title>
		<page_from>108</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280838</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280838</url>
		<abstract>
			<par><![CDATA[<p>CG and Photo-realistic image composition in ocean scenes is frequently used in movies and TV advertisement. For complete image composition, we need camera motion data. In the case of an ocean scene, camera tracking is a very difficult task because it's impossible to use calibration tool in outdoor environment. Auto-calibration algorithm proposed by Pollefeys is also not appropriate because natural features like KLT can't be extracted from ocean background. We propose a simple, practical method for solving camera motion tracking using previous knowledge about background structure. We applied our method to the production of a commercial movie, 'Hanbando' and the result was satisfactory.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893164</person_id>
				<author_profile_id><![CDATA[81335499880]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jung-Jae]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETRI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050673</person_id>
				<author_profile_id><![CDATA[81452596034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jae-Hean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETRI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893141</person_id>
				<author_profile_id><![CDATA[81335492828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hye-Mi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETRI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893100</person_id>
				<author_profile_id><![CDATA[81542663356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Byoung-Tae]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETRI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280839</article_id>
		<sort_key>109</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[360 degrees-viewable display of 3D solid images]]></title>
		<page_from>109</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280839</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280839</url>
		<abstract>
			<par><![CDATA[<p>Three-dimensional displays are drawing attention as next-generation devices. All-around displays have been proposed by [Maeda et al. 2003][Endo et al. 2004]. We have ever presented 360-degrees viewable 3D display called "Transpost" [Otsuka et al. 2006]. The previous prototype was capable of displaying QVGA resolution images of 10 by 15cm. One of the issues was to realize larger-sized and higher resolution 3D displays. In this paper, we present a prototype which can display XGA resolution images. Furthermore, we propose a novel simple structured design using LCD to achieve clearer images. We believe that our work provide new possibilities of 3D expression.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1110750</ref_obj_id>
				<ref_obj_pid>1110642</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Otsuka, R., Hoshino, T., AND Horry, Y. "A Novel Approach to the Display and Transmission of 360 Degrees-Viewable 3D Solid Images", <i>IEEE</i> TVCG 2(2)March/April 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946848</ref_obj_id>
				<ref_obj_pid>946248</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Maeda, H., Hirose, K., Yamashita, J., Hirota, K., AND Hirose, M., "All-Around Display for Video Avatar in Real World", In <i>Proceedings of ISMAR '03 The Second IEEE and ACM International Symposium on Mixed and Augmented Reality</i>, 288--289, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Endo, T. SeeLINDER, <i>CREST Symposium</i>, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280840</article_id>
		<sort_key>110</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Wave animation synthesis directly from a real video]]></title>
		<page_from>110</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280840</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280840</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35053086</person_id>
				<author_profile_id><![CDATA[81335496988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hidetomo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakaino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Energy and Environment Systems Labs.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Tessendorf. 1999. Simulating ocean water. SIGGRAPH, in Course Notes.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073273</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Y. Y. Chuang et al. 2005. Animating pictures with stochastic motion textures. Proc. SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[K. Horikawa. 1978. Coastal engineering - an introduction to ocean engineering. Univ. of Tokyo Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280841</article_id>
		<sort_key>111</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Filling-in stochastic texture]]></title>
		<page_from>111</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280841</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280841</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051929</person_id>
				<author_profile_id><![CDATA[81335496988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hidetomo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakaino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Energy and Environment Systems Labs.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Bertalmio et al. 2001. Navier-stokes, fluid dynamics, and image and video inpainting. IEEE CVPR, 1, pp. 355--362.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882264</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[V. Kwatra et al. 2003. Graphcut Textures: Image and video synthesis using graph cuts. ACM Proc. SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073263</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[V. Kwatra et al. 2005. Texture optimization for example-based synthesis. ACM Proc. SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320602</ref_obj_id>
				<ref_obj_pid>2319036</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Criminisi et al. 2004. Region filling and object removal by exemplar-based image inpainting. IEEE Trans. Image Process]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311548</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Stam. 1999. Stable fluids. ACM Proc. SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073273</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Yung-Yu et al. 2005. Animating pictures with stochastic motion textures. ACM Proc. SIGGRAPH, 24, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280842</article_id>
		<sort_key>112</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Multiresolution and local search methods for optimizing visual tracking processes on GPU]]></title>
		<page_from>112</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280842</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280842</url>
		<abstract>
			<par><![CDATA[<p>Some visual applications, such as <i>virtual blackboards</i> or gesture recognition for HCI applications, rely on precise visual tracking algorithms. In order to provide this accuracy, some refinement methods are needed as a complement to a coarser approach. In this case, Local Search (LS) methods are typically used, as their purpose is the exploration of a search space to improve a previous estimation. In this category fall many search algorithms such as Tabu Search, Simulated Annealing, Scatter Search, etc.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822920</person_id>
				<author_profile_id><![CDATA[81100001899]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ra&#250;l]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cabido]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad Rey Juan Carlos (Madrid, SPAIN)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P798910</person_id>
				<author_profile_id><![CDATA[81317489034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Antonio]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Montemayor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad Rey Juan Carlos (Madrid, SPAIN)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822713</person_id>
				<author_profile_id><![CDATA[81309504058]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Juan]]></first_name>
				<middle_name><![CDATA[Jos&#233;]]></middle_name>
				<last_name><![CDATA[Pantrigo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad Rey Juan Carlos (Madrid, SPAIN)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P745200</person_id>
				<author_profile_id><![CDATA[81100557742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bryson]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Payne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Georgia College & State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179685</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Montemayor, A. S., Pantrigo, J. J., Cabido, R., Payne, B. R., Cabido, &#193;. S., and Fern&#225;ndez, F. 2006. Improving GPU particle filter with shader model 3.0 for visual tracking. In <i>Proc. of ACM SIGGRAPH 2006-Research Posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280843</article_id>
		<sort_key>113</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Planar tracking using the GPU for augmented reality and games]]></title>
		<page_from>113</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280843</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280843</url>
		<abstract>
			<par><![CDATA[<p>Planar tracking on video streams is a subject of interest in the augmented reality, robotics, human computer interaction for games or computer vision fields. Traditional approaches to tracking are based on finding correspondences in successive images. This can be achieved by computing optical flow or by matching a sparse collection of features. The tracking method presented in this paper belongs to the first group of methods. It is based on minimizing the sum-of-squared differences (SSD) between a previously stored image of the target (the template) and the current image of it.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893188</person_id>
				<author_profile_id><![CDATA[81413601851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Luis]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Leiva]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad Rey Juan Carlos (MADRID, SPAIN)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893097</person_id>
				<author_profile_id><![CDATA[81335497117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Antonio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sanz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad Rey Juan Carlos (MADRID, SPAIN)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P469761</person_id>
				<author_profile_id><![CDATA[81100203495]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jos&#233;]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Buenaposada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad Rey Juan Carlos (MADRID, SPAIN)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>964604</ref_obj_id>
				<ref_obj_pid>964568</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baker, S., and Matthews, I. 2004. Lucas-kanade 20 years on: A unifiying framework. <i>International Journal of Computer Vision (IJCV) 56</i>, 3, 221--255.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280844</section_id>
		<sort_key>114</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization]]></section_title>
		<section_page_from>114</section_page_from>
	<article_rec>
		<article_id>1280845</article_id>
		<sort_key>114</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Identifying & visualizing surface detail on Michelangelo's David]]></title>
		<page_from>114</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280845</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280845</url>
		<abstract>
			<par><![CDATA[<p>We present the results of new experiments in which we have identified, characterized, and produced visualizations of selected fine surface detail on Michelangelo's David statue. Starting with available raw scan data [Levoy et al. 2000], we have applied a number of techniques, both developed and refined by us, including the calculation of <i>curvature maps, 2.5D spatial noise filtering, texture projection merging</i> [Rugis 2006], and <i>image processing assisted physical measurement</i>.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP18009684</person_id>
				<author_profile_id><![CDATA[81319500464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rugis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Auckland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bracci, S., Falletti, F., Matteini, M., and Scopigno, R., Eds. 2004. <i>Exploring David: Diagnostic Tests and State of Conservation</i>. Giunti, Florence.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Klette, R., and Rosenfeld, A. 2004. <i>Digital Geometry</i>. Morgan Kaufmann, San Francisco.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344849</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., Pulli, K., Curless, B., and Rusinkiewicz, S. 2000. The digital michelangelo project. In <i>Proceedings of SIGGRAPH 2000</i>, ACM Press/ACM SIGGRAPH, K. Akeley, Ed., 131--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179817</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rugis, J. 2006. Projecting surface curvature maps. In <i>Full Conference DVD: SIGGRAPH 2006</i>, ACM Press/ACM SIGGRAPH, Research Posters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280846</article_id>
		<sort_key>115</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Photorealistic facial reconstruction of Ramses II for virtual sets]]></title>
		<page_from>115</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280846</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280846</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a facial reconstruction and photorealistic visualization technique of the ancient Egyptian Pharaoh Ramses II. G. Attardi et al. [1] have reconstructed a three dimensional computer graphics facial model from computerized tomography data. K. Kahler et al. [2] have successfully visualized a facial expression model made from facial expression muscles. We obtained x-ray photographs capturing the cranial bones of Ramses II from Professor Faure, who headed the x-ray photography of Ramses II. Our facial reconstruction process is based on the x-ray photographs. In addition, we have made photorealistic visualization of Ramses II using subsurface scattering technique to capture the complex reflection model of the skin. Furthermore, we used the rendering results in Virtual Sets of a television program.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822793</person_id>
				<author_profile_id><![CDATA[81319490414]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Danjou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Joshibi University of Art and Design, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823104</person_id>
				<author_profile_id><![CDATA[81319497382]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masunaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Joshibi University of Art and Design, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822444</person_id>
				<author_profile_id><![CDATA[81319502488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ayako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tateishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Joshibi University of Art and Design, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39054720</person_id>
				<author_profile_id><![CDATA[81545492156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hiroko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uchiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Joshibi University of Art and Design, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893137</person_id>
				<author_profile_id><![CDATA[81335492927]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hiraku]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kasahara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Broadcasting System Television, INC. Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822957</person_id>
				<author_profile_id><![CDATA[81319504836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Sakuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cyber University, Fukuoka, Japan and Waseda University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048898</person_id>
				<author_profile_id><![CDATA[81335491553]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kazuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[St. Marianna University School of Medicine, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823051</person_id>
				<author_profile_id><![CDATA[81319497695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Tomohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagaoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[St. Marianna University School of Medicine, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39041135</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Masa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>312106</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Attardi, G., Betro, M., Forte, M., Gori, R., Guidazzoli, A., Imboden, S., and Mallegni, F.: "3D Facial Re-construction and Visualization of Ancient Egyptian Mummies using spiral CT Data," SIGGRAPH99 Abstracts and Applications; CD-ROM, 1999,]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882307</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kahler, K., Haber, J. and Seidel, H.: "Reanimating the Dead: Reconstruction of Expressive Faces from Skull Data," ACM Transactions on Graphics, 22, 3, 2003, pp.554--561]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Seta, S. and Yoshino, M., <u>Hakkotsu-Shitai no Kantei</u>. (Identification of Human Skeletal Remains), Reibunsha, 1990, pp.341--344 (in Japanese)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wilkinson, C., <u>Forensic Facial Reconstruction</u>, Cambridge University Press, 2004, pp125--141]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166139</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, P. and Krueger, W.: "Reflection from layered surfaces due to subsurface scattering," Proceedings of SIGGRAPH '93, 1993, pp.165--174]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141987</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Weyrich, T., Matusik, W., Pfister, H., Bickel, B., Donner, C., Tu, C, McAndless, Lee, J. J., Ngan A., Jensen, H. W., and Gross. M.: "Analysis of human faces using a measurement-based skin reflectance model," Proceedings of SIGGRAPH '06, 2006, pp.1013--1024]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280847</article_id>
		<sort_key>116</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[BRDF display]]></title>
		<page_from>116</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280847</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280847</url>
		<abstract>
			<par><![CDATA[<p>We propose a BRDF display that reproduces the textures of a light field. Conventional displays are now high-definition (HD). HDdisplays are ideal in terms of their excellent resolution. This, of course begs, the questions of whether resolution is the most important factor when judging the quality of a display? If we think of displays as optical reproducers of real objects or environments, it is important to reproduce the light transport of real objects or environments. If we think of light transport, resolution is only one of the parameters in representing light transport. A light field has four components: two parameters give positions and two parameters give directions of light transport.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35041214</person_id>
				<author_profile_id><![CDATA[81331496069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo and Systems Development Laboratory, Hitachi, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P422611</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344929</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Isaksen, A., Et Al. 2000. Dynamically reparameterized light fields. In <i>ACM SIGGRAPH 2000</i>, 297--306.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179793</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Koike, T., Et Al. 2006. Interactive Autostereoscopic Display with 60 Ray Directions. In <i>ACM SIGGRAPH 2006</i>, Poster no. 147.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280848</article_id>
		<sort_key>117</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[3D computer graphics and diabetes disease understanding]]></title>
		<page_from>117</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280848</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280848</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893120</person_id>
				<author_profile_id><![CDATA[81335490493]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emma]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Fyfe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Dundee, Scotland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Boulos, M. N. K. 2005. British Internet-Derived Patient Information on Diabetes Mellitus: is it Readable? <i>Diabetes Technology and Therapeutics</i> 7, 528--535.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Coulter, A. 1998. Information in Practice: Evaluation of Readability and Accuracy of Information Leaflets in General Practice for Patients with Asthma, <i>British Medical Journal</i> 317, 264--265.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Estey, A., Musseau, A., and Keehn, L. 1991. Comprehension Levels of Patients Reading Health Information, <i>Patient Education and Counselling</i> 18, 165--169.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Greenfield, S., Kaplan, S., and Ware, J. E. J. 1985. Expanding Patient Involvement in Care: Effects on Patient Outcomes, <i>Annals of Internal Medicine</i> 102, 520--528.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258908</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[McArthur, V. E., DiLorenzo, C. L., Jessup, M., Lynch, P., Herbert, C., and Hurd, J. 1997. Medical visualization-why we use CG and does it REALLY make a difference in creating meaningful images (panel). In <i>Proceedings of the 24th Annual Conference on Computer Graphics and interactive Techniques</i> International Conference on Computer Graphics and Interactive Techniques. ACM Press/Addison-Wesley Publishing Co., New York, NY, 459--461.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Stewart, M. 1995. Effective Physician-Patient Communication and Health Outcomes: A Review, <i>Canadian Medical Association Journal</i> 152, 1423--1433.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280849</article_id>
		<sort_key>118</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[GRADE-IV]]></title>
		<subtitle><![CDATA[visualizing graphics library operations in an executing program]]></subtitle>
		<page_from>118</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280849</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280849</url>
		<abstract>
			<par><![CDATA[<p>It is difficult for inexperienced programmers to debug three-dimensional (3D) computer graphics programs. One reason is that the relationship between the visual information rendered on the screen and the numerical information in the source code is difficult to understand. This sketch introduces a system to help programmers understand and debug computer graphics programs by visualizing those hidden relationships. The system allows the user to observe the spatial relationship between the camera and the scene from a third person view. The system also tells the user in which statement of the code each vertex position is specified. In addition, the system visualizes the sequence of spatial transformations applied to each vertex. To support these services, the system continuously observes the access to graphics library commands in the executing program and reconstructs the visual representation associated with the corresponding statements in the code. We implemented a prototype system, called GradeIV, using Java Debug Interface Library and Java Binding for the OpenGL to show the feasibility of the idea.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893135</person_id>
				<author_profile_id><![CDATA[81335487648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hidehiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P277957</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073213</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Duca, N., Niski, K., Bilodeau, J., Bolitho, M., Cchen, Y., and Cohen, J. 2005. A relational debugging engine for the graphics pipeline. ACM Trans. Graph. 24, 3, 453--463]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hawk Software. GLTrace. http://www.hawksoft.com/gltrace/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508533</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mohr, A., and Gleicher, M. 2002. HijackGL: reconstructing from streams for stylized rendering. In NPAR '02]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280850</article_id>
		<sort_key>119</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Tree model simplification for fast interactive rendering]]></title>
		<page_from>119</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280850</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280850</url>
		<abstract>
			<par><![CDATA[<p>Although billboards are often used in representing trees and grass of natural scenes, they are only suitable for distant objects, which are far away from the camera viewpoint. Human visual perception can point out billboards under close examination of such objects. In addition, a moving camera viewpoint around the objects further takes out the realism as some billboards are often programmed to rotate along the camera's viewing direction. In this on-going research, we consider rendering techniques built upon a 3D tree model and focus on geometrical simplification algorithms, which are used to save the rendering effort while keeping the visual quality close to that of a full tree model. Some recent research efforts [Lee et al. 2007] have tried to address this problem under the context of viewdependent rendering. Here, we generalize the framework so that a simplified tree model can be observed from different angles through user interaction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893154</person_id>
				<author_profile_id><![CDATA[81335493370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jessy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39058242</person_id>
				<author_profile_id><![CDATA[81384616778]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[C.-C.]]></first_name>
				<middle_name><![CDATA[Jay]]></middle_name>
				<last_name><![CDATA[Kuo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lee, J., and Kuo, C.-C. J. 2007. Fast tree model rendering with enhanced visibility estimation. In <i>Manuscript submitted for review</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, J., Peng, J., and Kuo, C.-C. J. 2007. View-dependent visibility estimation for tree models. In <i>Proc. IEEE Intl. Conf. Multimedia and Expo</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280851</article_id>
		<sort_key>120</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[A plane-based model of four dimensional snowflakes]]></title>
		<page_from>120</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280851</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280851</url>
		<abstract>
			<par><![CDATA[<p>Four-dimensional (4D) space is not intuitive to human mind. Commonly used examples of 4D space, such as the deep sea and the universe, are difficult for many people to feel or image. Research on the visualization of high dimensional polygons [1] helps people become aware of high dimensional spaces and feel their beauty, complexity and variations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P548636</person_id>
				<author_profile_id><![CDATA[81100084997]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qinglian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology, Ishikawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P681688</person_id>
				<author_profile_id><![CDATA[81100330777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kyoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology, Ishikawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP38026984</person_id>
				<author_profile_id><![CDATA[81319500804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Norio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology, Ishikawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893224</person_id>
				<author_profile_id><![CDATA[81335492695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sueko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanazawa Institute of Technology, Ishikawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Miyazaki, Shape and space- in n-dimensional world, Asakurasyuten, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H.S.M. Coxeter, Regular Polytopes, Dover Publ. Inc. 1973.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280852</article_id>
		<sort_key>121</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Bridging the resolution gap]]></title>
		<subtitle><![CDATA[superimposition of multiple multi-channel volumes]]></subtitle>
		<page_from>121</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280852</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280852</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893104</person_id>
				<author_profile_id><![CDATA[81542648356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chih]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCSD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P385073</person_id>
				<author_profile_id><![CDATA[81100047808]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J&#252;rgen]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Schulze]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCSD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053198</person_id>
				<author_profile_id><![CDATA[81320496219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ruth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[West]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCSD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P800472</person_id>
				<author_profile_id><![CDATA[81100103504]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Maryann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCSD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37040688</person_id>
				<author_profile_id><![CDATA[81100289561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zwicker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCSD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>897857</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cullip, T., and Neumann, U. 1993. <i>Accelerating Volume Reconstruction With 3D Texture Hardware</i>. Technical Report TR93-027, University of North Carolina, Chapel Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1034533</ref_obj_id>
				<ref_obj_pid>1032664</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Schulze, J., and Rice, A. 2004. <i>Real-Time Volume Rendering of Four-Channel Data Sets</i>. In poster proceedings of IEEE Visualization 2004, Austin, TX.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280853</article_id>
		<sort_key>122</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Dyeing theory based liquid diffusion model on woven cloth]]></title>
		<page_from>122</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280853</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280853</url>
		<abstract>
			<par><![CDATA[<p>This paper describes a method for simulating and visualizing dyeing based on weave patterns and the physical parameters of the threads and the dye. We apply Fick's second law with a variable diffusion coefficient. Visual simulation of dyeing is important for representing garments in computer graphics images and it is also important as a drawing simulation. The liquid penetration into cloth is a complicated phenomenon from both a scientific and a physical standpoint. The pattern and color of dyed cloth is a function of the physical properties of the dye and the fabric. It is a different phenomenon from liquid diffusion on a paper. Some characteristic features of liquid diffusion on a cloth that are influenced by weave patterns, such as thin spots and mottles. In an actual dyeing, wet cloth is used for preventing color heterogeneity, so this paper presents the dyeing system on wet cloth. The results obtained using our model demonstrate that it is capable of modeling many of the characteristics of dyeing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39080741</person_id>
				<author_profile_id><![CDATA[81343501259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Y.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088480</person_id>
				<author_profile_id><![CDATA[81100132412]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053013</person_id>
				<author_profile_id><![CDATA[81331505544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuruno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893168</person_id>
				<author_profile_id><![CDATA[81331505568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomimatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adabala, N. 2003. A procedural thread texture model. 33--40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S.-H., B. August 1997. Diffusion/adsorption behaviour of reactive dyes in cellulose. <i>Dyes and Pigments 34</i>, 321--340(20).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[van den, B. R. 1994. Human exposure to soil contamination: a qualitative and quantitative analysis towards proposals for human toxicological intervention values (partly revised edition). <i>RIVM Rapport 725201011</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280854</article_id>
		<sort_key>123</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[State-of-the-art rendering techniques in real-time architectural visualization]]></title>
		<page_from>123</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280854</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280854</url>
		<abstract>
			<par><![CDATA[<p>Today, high dynamic range (HDR) imaging, image-based lighting (IBL) and special effects techniques are common features found in many non-real-time rendering systems ("offline renderers"). Although slowly being adopted by game developers, few real-time rendering tools provide a seamless integration and persistent use of these techniques throughout their systems.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893206</person_id>
				<author_profile_id><![CDATA[81335493057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuchar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of T&#252;bingen]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893232</person_id>
				<author_profile_id><![CDATA[81335496966]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Timo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schairer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of T&#252;bingen]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1015750</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ostromoukhov, V., Donohue, C., and Jodoin, P.-M. 2004. Fast hierarchical importance sampling with blue noise properties. <i>ACM Trans. Graph.</i> &#60;b&#62;23&#60;/b&#62;, 3, 488--495.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566575</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Stark, M., Shirley, P., and Ferwerda, J. 2002. Photographic tone reproduction for digital images. <i>ACM Trans. Graph. 21</i>, &#60;b&#62;3&#60;/b&#62;, 267--276.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Scheuermann, T. 2004. Advanced depth of field.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280855</article_id>
		<sort_key>124</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Visualizing many-particle astronomical simulations]]></title>
		<page_from>124</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280855</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280855</url>
		<abstract>
			<par><![CDATA[<p>Astronomers often work on many-particle systems. For example, the Earth had been formed from millions of planetesimals, and a galaxy contains hundreds of billions of stars. We have developed a GUI visualization tool specialized for astronomical many-particle simulations. Our purpose is to provide a visualization tool which is simple enough for the usage of astronomers who are not specialists of computer graphics (CG), while powerful enough to render highquality simulation CG movies for scientific shows. We named the tool as "Zindaiji" from a place-name near the main campus of the National Astronomical Observatory of Japan. One of our movies made by Zindaiji, "Formation of A Spiral Galaxy" is accepted for CAF (caf 0761) and will be shown in Electronic Theater. (Figure 1(a)) We report the way how we visualized this million-particle simulation and other astronomical simulations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052712</person_id>
				<author_profile_id><![CDATA[81335498305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Astronomical Observatory of Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053898</person_id>
				<author_profile_id><![CDATA[81320495794]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Riken (The Institute of Physical and Chemical Research)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280856</article_id>
		<sort_key>125</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Daily Life in the Middle Ages - Parma in the Cathedral Age]]></title>
		<page_from>125</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280856</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280856</url>
		<abstract>
			<par><![CDATA[<p>Aim of the exhibit "Daily Life in the Middle Ages. Parma in the Cathedral age", which took place in Parma, from October 2006 8<sup>th</sup> to January 14<sup>th</sup> 2007, was to communicate the experience of the daily life of common people in Parma in the Middle Ages starting from historical sources. Technological and historical knowledge interactions have been performed by different disciplines experts including communication ones in order to create from philological models narrative spaces for the public.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P20729</person_id>
				<author_profile_id><![CDATA[81100058896]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Antonella]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guidazzoli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CINECA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18010190</person_id>
				<author_profile_id><![CDATA[81335496000]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Francesca]]></first_name>
				<middle_name><![CDATA[Delli]]></middle_name>
				<last_name><![CDATA[Ponti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CINECA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048919</person_id>
				<author_profile_id><![CDATA[81320489208]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tiziano]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Diamanti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CINECA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893185</person_id>
				<author_profile_id><![CDATA[81335497115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Leonardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sangiorgi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Studio Azzurro]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893126</person_id>
				<author_profile_id><![CDATA[81335489073]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cirifino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Studio Azzurro]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280857</article_id>
		<sort_key>126</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[All about the data]]></title>
		<subtitle><![CDATA[effective visual pipeline management]]></subtitle>
		<page_from>126</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280857</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280857</url>
		<abstract>
			<par><![CDATA[<p>All medium to large CGI productions require a pipeline of skilled people, hardware, software, standard procedures, and automation in order to ensure timely delivery of a final product. A manager or liaison is needed to coordinate all of the contributors and users of the pipeline, and making the pipeline scalable enough to handle increasing demands for more and larger data sets. Most importantly, the pipeline must be built around the data if the production is to realize greatest efficiency gains.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893233</person_id>
				<author_profile_id><![CDATA[81335491792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haynes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Manager Virtual Fleet DaimlerChrysler]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280858</article_id>
		<sort_key>127</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Graphlets]]></title>
		<subtitle><![CDATA[a method for visualizing dynamic data]]></subtitle>
		<page_from>127</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280858</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280858</url>
		<abstract>
			<par><![CDATA[<p>Dynamic contrast-enhanced MRI is often performed to characterize tissue by observing the passage of an agent injected into the bloodstream. MR images are acquired at regular intervals during a time period that begins prior to injection of the contrast agent, and extends through the agent's passage through the vasculature under study. The MRI signal observed during this time period can be transformed into a curve of contrast concentration over time [Ostergaard]. Parametric maps can be computed from the time-series and overlaid on an anatomical image for review by radiologists. Typical hemodynamic parameters include regional cerebral blood flow (rCBF), mean transit time (MTT), and regional Cerebral Blood Volume (rCBV). The shortcoming of this approach is that the parametric maps, although convenient, do not allow the clinician the opportunity to observe the full richness of the dynamic data.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35048423</person_id>
				<author_profile_id><![CDATA[81100129289]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GE Healthcare]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893158</person_id>
				<author_profile_id><![CDATA[81335491429]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoford]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GE Healthcare]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ostergaard, L., Weisskoff, R. M., Chesler, D. A., Gyldensted, C., Rosen, B. R. High resolution measurement of Cerebral blood flow using intravascular tracer bolus passages, part I: mathematical approach and statistical analysis. <i>Magn Reson Med</i> 1996; 36:715--725]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280859</article_id>
		<sort_key>128</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Flattened anatomy for interactive segmentation & measurement]]></title>
		<page_from>128</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280859</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280859</url>
		<abstract>
			<par><![CDATA[<p>Anatomical surfaces can be extracted from a volume of medical imagery through segmentation, which is the process of labeling image voxels according to the tissue type represented. Many anatomical surfaces can be described as 2-D manifolds embedded within 3-D space. The manifolds could be linear, such as a plane, or non-linear, such as a curved sheet. In some applications, it would be desirable for the user to be able to interact with the manifold by drawing upon it in some way. The purpose of this drawing could be to perform quantitative measurements such as to measure distances, surface areas, or volumes. The purpose could also be to analyze local properties of the 3-D surface at specific locations, where these properties include thickness and curvature.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35048313</person_id>
				<author_profile_id><![CDATA[81100129289]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GE Healthcare]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893140</person_id>
				<author_profile_id><![CDATA[81335496187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hollis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Potter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hospital for Special Surgery]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893186</person_id>
				<author_profile_id><![CDATA[81335490628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Li]]></first_name>
				<middle_name><![CDATA[Foong]]></middle_name>
				<last_name><![CDATA[Foo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hospital for Special Surgery]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2384128</ref_obj_id>
				<ref_obj_pid>2384110</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bartroli, A. V., Wegenkittl, R., Konig, A., Groller, E., Sorantin, E. Virtual Colon Flattening. <i>VisSym 2001</i>, p. 127--136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Halir, R., Flussr, J. Numerically Stable Direct Least Squares Fitting Of Ellipses. Proc. 6&#60;sup&#62;th&#60;/sup&#62; <i>WCSG</i>, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280860</article_id>
		<sort_key>129</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Interactively mitigating visual confusion by using a visual mixing board]]></title>
		<page_from>129</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280860</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280860</url>
		<abstract>
			<par><![CDATA[<p>This paper presents the idea of a visual mixing board, a new user interface that provides control over the appearance of rendered scientific data in a CAVE [2]. The mixing board controls visibility of the objects contained in the scene. For each type of data in the rendered dataset, the user can control how visually apparent the objects that represent that type of data are. As well as proposing the idea of a visual mixing board, this paper reports that this interface has promise for implementation, as found through a preliminary Wizard of Oz [1] evaluation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P829214</person_id>
				<author_profile_id><![CDATA[81320495187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chipalo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Street]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893114</person_id>
				<author_profile_id><![CDATA[81100589961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[LaidLaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[N. Dahlb&#228;ck, et al, 3&#60;sup&#62;rd&#60;/sup&#62; Conf. on Appl. Nat. Lang. Processing]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C. Crus-Neira, et al, SIGGRAPH 1993]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Daniel F. Keefe, et al, SIGGRAPH 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Johnson, A. E., et al IEEE Trans. on PMMI.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2141293</ref_obj_id>
				<ref_obj_pid>2141202</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lee at al ISBN:978-3-540-28226-6 pp. 839--848]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280861</section_id>
		<sort_key>130</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Algorithms]]></section_title>
		<section_page_from>130</section_page_from>
	<article_rec>
		<article_id>1280862</article_id>
		<sort_key>130</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Similarity metrics for bounding volumes]]></title>
		<page_from>130</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280862</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280862</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052532</person_id>
				<author_profile_id><![CDATA[81542599856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mayur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Patel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spin Productions]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aggarwal, C., et al. 2001. On the Surprising Behavior of Distance Metrics in High Dimensional Spaces. <i>Proc. 8&#60;sup&#62;th&#60;/sup&#62; International Conference on Database Theory</i>. 420--434.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Henrikson, Jeff. 1999. Completeness and Total Boundedness of the Hausdorff Metric. <i>MIT Undergraduate Journal of Mathematics</i>, 1. 69--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1023567</ref_obj_id>
				<ref_obj_pid>1023562</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Levin, V. I. 2004. Ordering of Intervals and Optimization Problems with Interval Parameters. <i>Cybernetics and Systems Analysis, 40</i>, 3. 316--324.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280863</article_id>
		<sort_key>131</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Vision-space]]></title>
		<page_from>131</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280863</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280863</url>
		<abstract>
			<par><![CDATA[<p>Vision-Space is a new form of illusionary space. It complies with 'perceptual structure' or the structure of vision as opposed to 'optical structure' (eg. camera optics) or perspective. It derives from a direct and unique understanding of the phenomenon of vision.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893159</person_id>
				<author_profile_id><![CDATA[81335492528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jupe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vision-Space imaging]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893221</person_id>
				<author_profile_id><![CDATA[81335495647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parish]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vision-Space imaging]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893112</person_id>
				<author_profile_id><![CDATA[81335491810]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoskins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[QuikQuak]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893110</person_id>
				<author_profile_id><![CDATA[81335492459]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Julian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Time Slice Films]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893096</person_id>
				<author_profile_id><![CDATA[81335487932]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Andy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baker&#248;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swansea Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280864</article_id>
		<sort_key>132</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Realtime constructive solid geometry]]></title>
		<page_from>132</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280864</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280864</url>
		<abstract>
			<par><![CDATA[<p>Constructive solid geometry (CSG) is a modeling technique where objects are combined using boolean operations to build up complex shapes. Fig. 1 shows the results of union, intersection and subtraction between the Stanford bunny and a toroidal knot. While many CAD systems support CSG, it remains conspicuously absent from interactive environments. Part of the problem is that current techniques are slow, unstable and complicated. This algorithm addresses these issues by improving on Naylor et.al.'s tree merging algorithm [Naylor et al. 1990].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050467</person_id>
				<author_profile_id><![CDATA[81335494284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mikola]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lysenko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Michigan Technological University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>97892</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Naylor, B., Amanatides, J., and Thibault, W. 1990. Merging bsp trees yields polyhedral set operations. In <i>SIGGRAPH '90:</i> Proceedings of the 17th annual conference on Computer graphics and interactive techniques, ACM Press, New York, NY, USA, 115--124.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>98570</ref_obj_id>
				<ref_obj_pid>98524</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Seidel, R. 1990. Linear programming and convex hulls made easy. In <i>SCG '90: Proceedings of the sixth annual symposium on Computational geometry</i>, ACM Press, New York, NY, USA, 211--215.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280865</article_id>
		<sort_key>133</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Generating freehand style drawings with SVG]]></title>
		<page_from>133</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280865</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280865</url>
		<abstract>
			<par><![CDATA[<p>Vector graphics are often known for their clean edges, flat colors, and even sterility. In this project we attempt to add nuance-rich and personal looks to them using freehand style drawings generated as SVG (Scalable Vector Graphics) by Python programming language. The algorithms are developed based on the findings of studying the hand drawing process in art creation. Methods of defining line stroke segments as drawing characteristics using formal grammar are explored, and stochastic turtle graphic interpretation is implemented.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893205</person_id>
				<author_profile_id><![CDATA[81335499637]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rebecca]]></first_name>
				<middle_name><![CDATA[Ruige]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Missouri State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893214</person_id>
				<author_profile_id><![CDATA[81335500271]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sean]]></first_name>
				<middle_name><![CDATA[Hongsheng]]></middle_name>
				<last_name><![CDATA[Zhai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LogicaCMG, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hansmeyer, Michael, 2007, <i>Algorithms in Architecture</i>, http://www.mh-portfolio.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[W3C, 2003, <i>Scalable Vector Graphics (SVG) 1.1 Specification</i>, http://www.w3.org/TR/SVG/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280866</article_id>
		<sort_key>134</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Global illumination with replica-exchange Monte-Carlo method]]></title>
		<page_from>134</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280866</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280866</url>
		<abstract>
			<par><![CDATA[<p>Global illumination (GI) is essential for photorealistic image synthesis, and is formalized by the light transport problem. Among hundreds of algorithms proposed to solve this problem, algorithms based on ray tracing and numerical integration are most promising and they are categorized as "consistent" or "unbiased" algorithms. The consistent algorithms (e.g., photon mapping) use (ir)radiance cashing and interpolation to connect paths from light sources to a camera, and results always include statistical biases. On the other hand, in the unbiased algorithms, although artifacts appear only as results of estimation variance, they are inherently time-consuming. For example, Metropolis Light Transport (MLT) algorithm, which is one of the unbiased algorithms, solves the light transport problem efficiently, however, it has slow mixing (start-up bias) problem.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP18006757</person_id>
				<author_profile_id><![CDATA[81319494785]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shinya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kitaoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P306500</person_id>
				<author_profile_id><![CDATA[81100273942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshifumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kitamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14122116</person_id>
				<author_profile_id><![CDATA[81100339162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Fumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kishino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Csaba, K., L&#225;szl&#243;, S.-K., Antal, G., and Csonka, F. 2002. A simple and robust mutation strategy for metropolis light transport algorithm. <i>Computer Graphics Forum 21</i>, 3, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Iba, Y. 2001. Extended ensemble monte carlo. <i>International Journal of Modern Physics C, 12</i>, 623--656.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280867</article_id>
		<sort_key>135</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Reassembling ancient monuments via constrained registration]]></title>
		<page_from>135</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280867</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280867</url>
		<abstract>
			<par><![CDATA[<p>As demonstrated in our work [Huang et al. 2006] fracture surfaces of broken solids contain rich geometric information that are sufficient for reassembly based on geometric matching. However, the task of reassembling an ancient monument from its building blocks differs significantly from that of reassembling a broken solid. These building blocks are fragments in a much looser sense as opposing faces of neighboring stones do not contain the rich geometric features of fracture surfaces. The contribution of the present work is an algorithm for automatic reassembly of ancient monuments guided by high-level adjacent features such as edges, clamp holes, or ornaments. The data we use to test our new method comprises the remaining stones of the Octagon (Fig. 1, top, right) monument in Ephesos, Turkey, which fell apart a thousand years ago and whose reconstruction is currently undertaken.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35049129</person_id>
				<author_profile_id><![CDATA[81335490132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fl&#246;ry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083360</person_id>
				<author_profile_id><![CDATA[81335491479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hofer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893098</person_id>
				<author_profile_id><![CDATA[81335498521]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thuswaldner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049092</person_id>
				<author_profile_id><![CDATA[81313484038]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Qi-Xing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1141925</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Huang, Q.-X., Fl&#246;ry, S., Gelfand, N., Hofer, M., and Pottmann, H. 2006. Reassembling fractured objects by geometric matching. <i>ACM Trans. Graphics 25</i>, 3, 569--578.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280868</article_id>
		<sort_key>136</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[3000+ variations of the Voronoi diagram]]></title>
		<page_from>136</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280868</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280868</url>
		<abstract>
			<par><![CDATA[<p>The Voronoi diagram, which describes a way to partition space, is very useful in a variety of fields ranging from astronomy to computer graphics to zoology. This sketch discusses a general idea which makes it possible to easily construct many alternatives to the classical Voronoi polygon. These alternatives could give rise to novel applications in the areas of fracture synthesis, mosaic generation and surface texture warping.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051683</person_id>
				<author_profile_id><![CDATA[81100537885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raghavachary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088159</person_id>
				<author_profile_id><![CDATA[81335494721]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matthews]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Weisstein, Eric W., 2003. "Triangle Center." http://mathworld.wolfram.com/TriangleCenter.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bisse, Edward. http://www.mathpuzzle.com/EdwardBrisse.txt]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kimberling, Clark. Encyclopedia of triangle centers. http://faculty.evansville.edu/ck6/encyclopedia]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280869</article_id>
		<sort_key>137</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[GDPS]]></title>
		<subtitle><![CDATA[geometric animation based dynamic photo system]]></subtitle>
		<page_from>137</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280869</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280869</url>
		<abstract>
			<par><![CDATA[<p>GDPS: Geometric Animation Based Dynamic Photo System is a movie generating system to connect 2 digital photo images with the geometric animation. The geometric animation is produced from 2 photo images. GDPS will carve out the next stage of movie generating system using digital photo images, for the emerging digital photo market.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893244</person_id>
				<author_profile_id><![CDATA[81335497107]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University Inakage Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39041135</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University Inakage Lab.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893181</person_id>
				<author_profile_id><![CDATA[81335491293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Olympus Co., ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Seitz, Steven M., and Dyer, Charles R. 1996. View Morphing. <i>In Proceedings of SIGGRAPH 1996</i>, ACM Press / ACM SIGGRAPH, New York, 21--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hertzmann, Aaron., Jachobs, Charles E., Oliver, Nuria., Curless, Brian., and Salesin, David H. Image Analogies. <i>In Proceedings of ACM SIGGRAPH 2001</i>, ACM Press / ACM SIGGRAPH, New York, 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280870</section_id>
		<sort_key>138</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Interaction]]></section_title>
		<section_page_from>138</section_page_from>
	<article_rec>
		<article_id>1280871</article_id>
		<sort_key>138</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The sound of touch]]></title>
		<page_from>138</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280871</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280871</url>
		<abstract>
			<par><![CDATA[<p>All people have experienced hearing sounds produced when they touch and manipulate different materials. We know what it will sound like to bang our fist against a wooden door, or to crumple a piece of newspaper. We can imagine what a coffee mug will sound like if it is dropped onto a concrete floor. But our wealth of experience handling physical materials does not typically produce much intuition for operating a new electronic instrument, given the inherently arbitrary mapping from gesture to sound.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35038436</person_id>
				<author_profile_id><![CDATA[81328489350]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Merrill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P507458</person_id>
				<author_profile_id><![CDATA[81100381813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hayes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raffle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aimi, R. Hybrid Percussion: <i>Extending Physical Instruments Using Sampled Acoustics</i>. Ph.D. Dissertation, Massachusetts Institute of Technology, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Oppenheim, A. V. and Schafer, R. W. <i>Digital Signal Processing</i>. Prentice-Hall, Englewood Cliffs, NJ, 1975.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1056816</ref_obj_id>
				<ref_obj_pid>1056808</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ryokai, K., Marti, S., Ishii, H. Designing the World as Your Palette. In <i>Proceedings of Conference on Human Factors in Computing Systems</i> (CHI '05), Portland, OR, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280872</article_id>
		<sort_key>139</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Freqtric game]]></title>
		<subtitle><![CDATA[video game which uses skin contact as controller input]]></subtitle>
		<page_from>139</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280872</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280872</url>
		<abstract>
			<par><![CDATA[<p>Touch is communication on the most basic level. Hand-shakes, holding a child by his or her arms, high or low five, these comminications have a special intimacy that is missing from virtual environment and conveys a special emotion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P823030</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034227</person_id>
				<author_profile_id><![CDATA[81319502887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Taketoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ushiama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18002822</person_id>
				<author_profile_id><![CDATA[81331505568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kiyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomimatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179146</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baba, T., and Tomimatsu, K. 2006. Freqtric drums. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Emerging technologies</i>, ACM Press, New York, NY, USA, 12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280873</article_id>
		<sort_key>140</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[An electrical muscle stimulation haptic feedback for mixed reality tennis game]]></title>
		<page_from>140</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280873</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280873</url>
		<abstract>
			<par><![CDATA[<p>We have developed a novel haptic interface for mixed reality applications. Specifically, we have constructed an electrical muscle stimulation system which is wirelessly controlled by a computer and generates electrical pulses with controlled amplitude, timing, and frequencies. The characteristics of the pulses are similar to the commercial electrical muscle stimulators used in medical applications. We apply these pulses through two electrode pads attached to the user's forearms. Hence, the user can feel muscle contractions when pulses are activated.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39083233</person_id>
				<author_profile_id><![CDATA[81100397017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Farzam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farbiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[A*STAR Institute for Infocomm Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893253</person_id>
				<author_profile_id><![CDATA[81335499863]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhou]]></first_name>
				<middle_name><![CDATA[Hao]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[A*STAR Institute for Infocomm Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083683</person_id>
				<author_profile_id><![CDATA[81100126224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Corey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Manders]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[A*STAR Institute for Infocomm Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37040478</person_id>
				<author_profile_id><![CDATA[81545687856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Waqas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ahmad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[A*STAR Institute for Infocomm Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[MadCatz. Madcatz bioforce article at newsfactor, available at. <i>http://www.newsfactor.com/perl/story/12528.html</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280874</article_id>
		<sort_key>141</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Persistence and propagation of shadow direction in mobile and multi-device graphics]]></title>
		<page_from>141</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280874</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280874</url>
		<abstract>
			<par><![CDATA[<p>Multi-Device Virtual Environments (MDVEs) - in which several collocated graphical computing devices integrate together to form a single virtual world - are a novel area for applications in entertainment, education and simulation. MDVEs composed of stationary and mobile devices, though, pose new challenges in the area of lighting design. Traditional computational lighting design techniques are static and do not take into account physical movement of the device. However, static light design does not work well in mobile devices; for example, the shadows in the environment remain static even if the user rotates the device. This paper addresses this challenge by attaching sensing devices to mobile computers to detect their physical movements. The MDVE then uses this feedback to adjust the virtual light source so that it remains consistent with how light sources work in real world. This lighting model may then be propagated via IrDA among the devices in the MDVE. By improving lighting in MDVEs, this research seeks to bridge the gap between the real world and a virtual world and to help enhance the believability of MDVEs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P720982</person_id>
				<author_profile_id><![CDATA[81100417233]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Man]]></first_name>
				<middle_name><![CDATA[Lok]]></middle_name>
				<last_name><![CDATA[Yau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Irvine, Irvine, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35042369</person_id>
				<author_profile_id><![CDATA[81331499794]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moore]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Irvine, Irvine, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893252</person_id>
				<author_profile_id><![CDATA[81335492486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Zack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Irvine, Irvine, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893192</person_id>
				<author_profile_id><![CDATA[81335496515]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Irvine, Irvine, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15032752</person_id>
				<author_profile_id><![CDATA[81100457381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomlinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Irvine, Irvine, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1056857</ref_obj_id>
				<ref_obj_pid>1056808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tomlinson, B., Yau, M. L., O'Connell, J., Williams, K. and Yamaoka, S., 2005. The Virtual Raft Project: A Mobile Interface for Interacting with Communities of Autonomous Characters, In <i>Conference Abstracts and Applications, ACM Conference On Human Factors In Computing Systems (CHI 2005)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280875</article_id>
		<sort_key>142</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Haptic telexistence]]></title>
		<page_from>142</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280875</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280875</url>
		<abstract>
			<par><![CDATA[<p>Nowadays, we can interact with humans or objects even if they are located in remote places or in virtual environments. In these interactions, we can watch, listen, touch, and move objects. However, the properties of an object are not present in conventional systems. When we communicate or perform a task, a lack of haptic sensation reduces the realism and interactivity. Therefore, there is increasing requirement for haptic technology presently.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P882913</person_id>
				<author_profile_id><![CDATA[81331503619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katsunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P882929</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P207192</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P272463</person_id>
				<author_profile_id><![CDATA[81100411569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280876</article_id>
		<sort_key>143</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[HECTOR - scripting-based VR system design]]></title>
		<page_from>143</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280876</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280876</url>
		<abstract>
			<par><![CDATA[<p>Modern VR systems embrace the idea of scripting interfaces for rapid prototyping of applications. HECTOR goes one step further: the entire core of the VR system is written in PYTHON, easily gluing interchangeable high-performance C++ libraries, a module-based system infrastructure and a scripting-based application layer. Thus, the time consuming and error prone compile-debug cycle for application and system development becomes obsolete -- both, the infrastructure and the application layer, can be extended or modified even at run-time.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[distributed graphics]]></kw>
			<kw><![CDATA[scripting]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35053448</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia and Bauhaus-Universit&#228;t Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893197</person_id>
				<author_profile_id><![CDATA[81335491004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Moritz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[G&#246;llner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ceetron GmbH and Bauhaus-Universit&#228;t Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893222</person_id>
				<author_profile_id><![CDATA[81100558043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-Universit&#228;t Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893127</person_id>
				<author_profile_id><![CDATA[81335499290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Felix]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weiszig]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-Universit&#228;t Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893215</person_id>
				<author_profile_id><![CDATA[81335489542]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derkau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-Universit&#228;t Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052312</person_id>
				<author_profile_id><![CDATA[81313481790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Springer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-Universit&#228;t Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35049437</person_id>
				<author_profile_id><![CDATA[81100162399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Bernd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fr&#246;hlich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-Universit&#228;t Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>835847</ref_obj_id>
				<ref_obj_pid>580521</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bierbaum, A., Just, C., Hartling, P., Meinert, K., Baker, A., and Cruz-Neira, C. 2001. VRJuggler: A Virtual Platform for Virtual Reality Application Development. In <i>Proceedings of IEEE Virtual Reality 2001 Conference</i>, IEEE Computer Society, 89--97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>296488</ref_obj_id>
				<ref_obj_pid>296477</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blach, R., Landauer, J., R&#246;sch, A., and Simon, A. 1998. A Highly Flexible Virtual Reality System. <i>Future Generation Computer Systems 14</i>, 3--4, 167--178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rohlf, J., and Helman, J. IRIS Performer: A High Performance Multiprocessing Toolkit for 3D Graphics. In <i>Proceedings of ACM SIGGRAPH 94</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835699</ref_obj_id>
				<ref_obj_pid>554230</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Tramberend, H. 1999. Avocado: A Distributed Virtual Reality Framework. In <i>Proceedings of IEEE Virtual Reality '99 Conference</i>, IEEE Computer Society, 14--21.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280877</article_id>
		<sort_key>144</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Tangible user interface for supporting disaster education]]></title>
		<page_from>144</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280877</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280877</url>
		<abstract>
			<par><![CDATA[<p>We introduce an interactive disaster simulation system for disaster education. This system incorporates a tabletop tangible user interface [Ishii and Ullmer 1997] and a digital pen input to provide a wide range of users with a collaborative and easy-to-use learning environment. Users can manipulate the map data to learn more about their towns. Likewise, through natural interaction with the physical tools on the table, users can explore disaster evacuation scenarios based on simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050808</person_id>
				<author_profile_id><![CDATA[81100603878]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazue]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P779676</person_id>
				<author_profile_id><![CDATA[81311485048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tatsuhito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakizaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P507413</person_id>
				<author_profile_id><![CDATA[81100342152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Atsunobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P507553</person_id>
				<author_profile_id><![CDATA[81100281714]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mitsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034135</person_id>
				<author_profile_id><![CDATA[81319494324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kase]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Comware Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anoto. http://www.anoto.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179163</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Haller, M., Leithinger, D., Leitner, J., Seifried, T., Brandl, P., Zauner, J., and Billinghurst, M. The shared design space. ACM SIGGRAPH 2006, E-Tech.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., and Ullmer, B. 1997. Tangible bits: Towards seamless interfaces between people bits and atoms. In <i>Proceedings of CHI</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Katada, T. 2000. Development of practical scenario simulator for dissemination of disaster information. <i>Journal of Civil Engineering Information Processing System</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280878</article_id>
		<sort_key>145</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[inter-glow]]></title>
		<page_from>145</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280878</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280878</url>
		<abstract>
			<par><![CDATA[<p>Light means information for human beings. Since early times, human beings use light to receive information. In contrast, light do not have meaning for computers. Recently visible-light communication (VLC) [1] has received a lot of attention. VLC is unnoticeable to humans and makes it possible to have a broadband communication in everyday living space by flickering LEDs at a speed undetectable to the human eye.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P875246</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[the University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P875170</person_id>
				<author_profile_id><![CDATA[81331494695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[the University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P498943</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[the University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40025385</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[the University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Y. Tanaka et al.: Wireless optical transmission with the white colored LED for the wireless home links, Proc. on PIMRC, pp.1325 -- 1329, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280879</article_id>
		<sort_key>146</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[NeuroFloat]]></title>
		<subtitle><![CDATA[real-time state-sensitive brain spaces]]></subtitle>
		<page_from>146</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280879</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280879</url>
		<abstract>
			<par><![CDATA[<p>We present a novel interactive system that uses electroencephalographic (EEG) signals obtained from the user as a primary input for navigation through an immersive real-time 3-D visualization of various regions of the human brain. This "NeuroFloat" system is delivered via a head-mounted display (HMD). Our primary goal in developing NeuroFloat was to create a system where the brain-computer interface (BCI) was intrinsically related to the virtual-reality (VR) environment that it interfaced with. Accordingly, at this early stage in the development of NeuroFloat, we chose to develop multiple VR environments that collectively represent a tour through the early stages of the user's own visual system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893223</person_id>
				<author_profile_id><![CDATA[81100476455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Barnes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053520</person_id>
				<author_profile_id><![CDATA[81335497696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Meehae]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Song]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP311076300</person_id>
				<author_profile_id><![CDATA[81543889056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kyung]]></first_name>
				<middle_name><![CDATA[Jae]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048615</person_id>
				<author_profile_id><![CDATA[81100453293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Diane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gromala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083977</person_id>
				<author_profile_id><![CDATA[81100396860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Shaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Wingate, R. and Kwint, M. 2006. Imagining the brain cell: the neuron in visual culture. <i>Nature Neuroscience Reviews, 7</i>, 745--752.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280880</article_id>
		<sort_key>147</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Spellbinder]]></title>
		<subtitle><![CDATA[a medium for social interaction in branded city space]]></subtitle>
		<page_from>147</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280880</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280880</url>
		<abstract>
			<par><![CDATA[<p>Spellbinder is a new interactive digital medium based on camera phones and image matching. Using Spellbinder, digital content can be embedded in the real world by taking a photograph of an object or place. The digital content can be released by another user by taking another photograph of the same location. Spellbinder does not require special markers or barcodes to be placed in the world. Unlike tracking technologies such as global positioning systems, the focus is on what specifically is being looked at rather than where the user is. The Branded Tribes research project is exploring uses of this technology for social interaction in city spaces using an approach of research by design. Graphics of real world brandscapes are used as placeholders for virtual computer graphics. We report innovative outcomes from the first phase of this research.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computer vision]]></kw>
			<kw><![CDATA[interaction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>J.5</cat_node>
				<descriptor>Architecture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010472</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P191344</person_id>
				<author_profile_id><![CDATA[81100659955]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35053522</person_id>
				<author_profile_id><![CDATA[81335497699]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stewart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048833</person_id>
				<author_profile_id><![CDATA[81547538256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Edinburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Azuma, R. T. 1997. A survey of augmented reality. <i>Presence: Teleoperators and Virtual Environements 6</i>, 4 (Aug.), 355--385.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1143522</ref_obj_id>
				<ref_obj_pid>1143518</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Benford, S., Crabtree, A., Flintham, M., Drozd, A., Anastasi, R., Paxton, M., Tandavanitj, N., Adams, M., and Row-Farr, J. 2006. Can you see me now? <i>ACM Trans. Computer Human Interaction 13</i>, 1, 100--133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280881</article_id>
		<sort_key>148</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Tiny dancing robots]]></title>
		<subtitle><![CDATA[Display-based computing for multi-robot control systems]]></subtitle>
		<page_from>148</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280881</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280881</url>
		<abstract>
			<par><![CDATA[<p>Computer graphic (CG) images exist only in a display device, and cannot physically interact with the real world, whereas robots have a physical existence that can be controlled by a computer. In this study, we developed robots that behave along with fiducial graphics. This technology can be used to synchronize physical actions based on the graphics shown in the display devices. Figure 1 shows tiny robots controlled by this technique.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35027934</person_id>
				<author_profile_id><![CDATA[81100344143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052756</person_id>
				<author_profile_id><![CDATA[81332531857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P891423</person_id>
				<author_profile_id><![CDATA[81335487716]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aruga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P799928</person_id>
				<author_profile_id><![CDATA[81317498050]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naohisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052283</person_id>
				<author_profile_id><![CDATA[81316490330]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Noriyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimizu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40027105</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Jarvis, Video plane robot swarms. Journal of Robotics and Computer-Integrated Manufacturing, 11(4):249--258, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Sugimoto, M. Kojima, A. Nakamura, G. Kagotani, H. Nii and M. Inami, Augmented Coliseum, SIGGRAPH 2005 Full Conference DVD-ROM Disk1 Emerging Technologies, 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280882</article_id>
		<sort_key>149</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[TimeWarp]]></title>
		<subtitle><![CDATA[an explorative outdoor mixed reality game]]></subtitle>
		<page_from>149</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280882</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280882</url>
		<abstract>
			<par><![CDATA[<p>As mobile technology has advanced a number of electronic city guide products have become available. However, most of these products still maintain a conventional form of tour that is information centered and forces the user to take a certain route through a city. In contrast, TimeWarp is an outdoor edutainment game that provides an immersive experience of the history of a city using Mixed Reality (MR) and mobile devices. The mixed reality experience is extended to a web application that provides travel journals for the mixed reality time travelers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P736818</person_id>
				<author_profile_id><![CDATA[81100254434]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Iris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herbst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer FIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893211</person_id>
				<author_profile_id><![CDATA[81335490679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sabiha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghellah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Net Services]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P855945</person_id>
				<author_profile_id><![CDATA[81325487617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anne-Kathrin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Braun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer FIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280883</article_id>
		<sort_key>150</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Meshless visual and haptic interaction from a real-time depth image]]></title>
		<page_from>150</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280883</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280883</url>
		<abstract>
			<par><![CDATA[<p>Recently, the technological advance of 3D depth sensors such as the Z-Cam[1] has made it possible to acquire a dynamic real object in real time(See Fig 1.(a)). The raw depth image is time-consuming to generate connectivity or pre-computed hierarchical data structure from the heavy data set in order to perform graphic and haptic interaction. In this paper, therefore, we propose mesh-less graphic and haptic rendering algorithm based on the point-based techniques by utilizing graphics hardware for providing efficient visualization and haptic interaction from a depth image acquired in real time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822453</person_id>
				<author_profile_id><![CDATA[81319495386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Beom-Chan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893117</person_id>
				<author_profile_id><![CDATA[81335492570]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Duck-Bong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893145</person_id>
				<author_profile_id><![CDATA[81335488740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[In-Yeop]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051257</person_id>
				<author_profile_id><![CDATA[81335495808]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hyeshin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050948</person_id>
				<author_profile_id><![CDATA[81546867856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kwang]]></first_name>
				<middle_name><![CDATA[Hee]]></middle_name>
				<last_name><![CDATA[Ko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088240</person_id>
				<author_profile_id><![CDATA[81100087869]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Renato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pajarola]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050053</person_id>
				<author_profile_id><![CDATA[81319495441]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kwan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088324</person_id>
				<author_profile_id><![CDATA[81100170372]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Jeha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ryu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://www.3dvsystems.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187021</ref_obj_id>
				<ref_obj_pid>1186954</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. P. Kim, B. C. Lee, and J Ryu, "Haptic Rendering for Hybrid Environments", ACM SIGGRAPH 2005, research poster, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280884</article_id>
		<sort_key>151</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Shape and material property modeling with haptic interaction]]></title>
		<page_from>151</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280884</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280884</url>
		<abstract>
			<par><![CDATA[<p>Rapid development of data acquisition technology like 3D scanner makes it possible to obtain geometry information of highly complex objects at high speed and with good accuracy. For the realistic haptic interaction with these objects, we propose an interactive efficient haptic reshaping and material property modeling. The proposed modeling supports not only a physically-based reshaping of very complex objects such as a real human face captured from 3D scanner but also efficient modeling of material properties such as roughness, stiffness, and friction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822453</person_id>
				<author_profile_id><![CDATA[81319495386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Beom-Chan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050762</person_id>
				<author_profile_id><![CDATA[81319494990]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jong-Phil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051310</person_id>
				<author_profile_id><![CDATA[81335495808]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hyeshin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083922</person_id>
				<author_profile_id><![CDATA[81100170372]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jeha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ryu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179691</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. C. Lee, J. P. Kim, and J Ryu, "K-Touch#8482; Haptic API for various datasets", ACM SIGGRAPH 2005, research poster, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Real Human Face Data, Geometricinformatics, Inc., http://www.geometricinformatics.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280885</article_id>
		<sort_key>152</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Magic lens augmented reality]]></title>
		<subtitle><![CDATA[table-top and augmentorium]]></subtitle>
		<page_from>152</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280885</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280885</url>
		<abstract>
			<par><![CDATA[<p><i>Augmented Reality</i> (AR) brings virtual objects into the real world instead of making people go into the computer world. With AR, virtual objects appear to coexist with the real in the user's real environment [Bier et al. 1993].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP15027670</person_id>
				<author_profile_id><![CDATA[81100261870]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39088327</person_id>
				<author_profile_id><![CDATA[81100200820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gerhard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://www.artag.net/ (artag webpage).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166126</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bier, E., Stone, M., Pier, K., Buxton, W., and DeRose, T. 1993. Toolglass and magiclenses: The see-through interface. In <i>Proc. Siggraph93, Computer Graphics Annual Conference Series</i>, 73--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1069138</ref_obj_id>
				<ref_obj_pid>1068508</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fiala, M. 2005. Artag, a fiducial marker system using digital techniques. In <i>CVPR'05</i>, vol. 1, 590--596.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280886</article_id>
		<sort_key>153</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[The audiator]]></title>
		<subtitle><![CDATA[a device-independent active marker for spatially aware displays]]></subtitle>
		<page_from>153</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280886</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280886</url>
		<abstract>
			<par><![CDATA[<p>The LightSense system [Olwal 2006] promotes printed public information sources, such as maps, into interactive surfaces enriched with digital information, which is accessed through a spatially aware display [Fitzmaurice 1993]. Using outside-in tracking, LED lights on cell phones are located using an instrumented environment with either a combination of a computer/camera or microcontroller/photosensors. The location is continuously sent to the phone over Bluetooth, such that context-sensitive information can be shown as the device is moved by the user.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35051267</person_id>
				<author_profile_id><![CDATA[81309497296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olwal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[School of Computer Science and Communication]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>159566</ref_obj_id>
				<ref_obj_pid>159544</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fitzmaurice, G. W. 1993. Situated information spaces and spatially aware palmtop computers. <i>Comm. ACM</i>, 36, 7, 39--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1052387</ref_obj_id>
				<ref_obj_pid>1052380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Henrysson, A. and Ollila, M. 2004. UMAR: Ubiquitous Mobile Augmented Reality. <i>MUM</i>, 41--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Olwal, A. 2006. LightSense: Enabling Spatially Aware Handheld Interaction Devices. <i>ISMAR</i>, 119--122.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280887</article_id>
		<sort_key>154</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[MuseSpace]]></title>
		<subtitle><![CDATA[a touchable 3D museum with maximum usage of haptics]]></subtitle>
		<page_from>154</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280887</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280887</url>
		<abstract>
			<par><![CDATA[<p>We present a networked haptic virtual museum; MuseSpace. The main objective of our MuseSpace is to build a virtual musical instrument museum for both education and entertainment. In our MuseSpace, various touchable musical instruments and artifacts are displayed and people can touch them freely by using haptic devices, while in real museums, people are usually not allowed to touch them by keeping those instruments and artifacts in glass showcases in order to prevent possible damages from touches. In addition, we enhance the accessibility, e.g. navigation control, instrument handling, with the help of haptic devices. One of the significant advantages of MuseSpace is that it provides visitors with the chances to experience instruments more interactively by supporting diverse interface media: audio, video, and touching.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893247</person_id>
				<author_profile_id><![CDATA[81363600663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yonghee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[You]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Incheon, Incheon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052647</person_id>
				<author_profile_id><![CDATA[81335498167]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mee]]></first_name>
				<middle_name><![CDATA[Young]]></middle_name>
				<last_name><![CDATA[Sung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Incheon, Incheon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35041173</person_id>
				<author_profile_id><![CDATA[81331495554]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kyungkoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Incheon, Incheon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893212</person_id>
				<author_profile_id><![CDATA[81335493635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sang-Rak]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Incheon, Incheon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280888</section_id>
		<sort_key>155</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Human-Computer Interaction (HCI)]]></section_title>
		<section_page_from>155</section_page_from>
	<article_rec>
		<article_id>1280889</article_id>
		<sort_key>155</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Perceptual interaction of optical BCI applications]]></title>
		<page_from>155</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280889</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280889</url>
		<abstract>
			<par><![CDATA[<p>A non-invasive brain-computer Interface (BCI) offers a direct interaction between the human brain and an external device without any surgical risk. Currently, electroencephalography (EEG) has been the most studied non-invasive interface because of its portability and low set-up cost. However, susceptibility to electromagnetic noise has been a barrier to promoting practical non-invasive BCIs in some cases.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35054107</person_id>
				<author_profile_id><![CDATA[81319502900]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Utsugi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd., Kawasaki, Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893092</person_id>
				<author_profile_id><![CDATA[81335495811]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Obata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd., Saitama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051996</person_id>
				<author_profile_id><![CDATA[81335497044]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd., Saitama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893229</person_id>
				<author_profile_id><![CDATA[81335492586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takusige]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katsura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd., Saitama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893175</person_id>
				<author_profile_id><![CDATA[81335496561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kazuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sagara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd., Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050460</person_id>
				<author_profile_id><![CDATA[81545354456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd., Saitama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893134</person_id>
				<author_profile_id><![CDATA[81335493081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koizumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd., Kanagawa, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280890</article_id>
		<sort_key>156</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Clay tone]]></title>
		<subtitle><![CDATA[a music system using clay for user interaction]]></subtitle>
		<page_from>156</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280890</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280890</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893122</person_id>
				<author_profile_id><![CDATA[81335499085]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, imgl]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893251</person_id>
				<author_profile_id><![CDATA[81335491156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanzawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, imgl]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39041135</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, imgl]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>633457</ref_obj_id>
				<ref_obj_pid>633292</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Weinberg, G., Orth, M., and Russo, P. The Embroidered Musical Ball: A Squeezable Instrument for Expressive Performance. <i>Proceedings of CHI</i>. The Hague: ACM Press, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187321</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Iwai. T and Nishibori. Y: "TENORI-ON," In Proc. <i>SIGGRAPH'05</i>, Emerging Technologies, Los Angeles, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280891</article_id>
		<sort_key>157</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Applying 'Second Life' to a CAVE#8482;-like system for the elaboration of interaction methods with programmable interfaces]]></title>
		<page_from>157</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280891</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280891</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P811385</person_id>
				<author_profile_id><![CDATA[81318494589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P773681</person_id>
				<author_profile_id><![CDATA[81310483773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lindinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P864674</person_id>
				<author_profile_id><![CDATA[81310494188]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Horst]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[H&#246;rtner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893128</person_id>
				<author_profile_id><![CDATA[81537731856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Florian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893116</person_id>
				<author_profile_id><![CDATA[81387595662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Doris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zachhuber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ars Electronica Futurelab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Berger, F., ., Lindinger, C., Ziegler, W. 2004. VRizer -- Using Arbitrary OpenGL Software in the CAVE or other Virtual -- Environments. In <i>IEEE Virtual Reality 2004 Workshop - Proceedings</i>. Pape, D., Roussos, M., Anstey, J., Ed., <i>Virtual Reality for Public Consumption</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Berger, F., H&#246;rtner, H., Lindinger, C., Maresch, P., Pl&#246;sch, R., Pomberger, G., Praxmarer, R., and Ziegler, W. 2005. ARSBOX and Palmist -- Technologies for Digital Mock-up Development in Immersive Virtual Environments. In <i>Proceedings of IASTED International Conference on Internet and Multimedia Systems and Applications -- EuroIMSA 2005</i>, ACTA Press / IASTED, Grindelwald, Switzerland, February 21--23, 2005, 521--526]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280892</article_id>
		<sort_key>158</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Tangible search system using RFID technology]]></title>
		<page_from>158</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280892</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280892</url>
		<abstract>
			<par><![CDATA[<p>Many types of information input methods into computers have been invented and used from keyboard and mouse to touch-panel display. However, each operation method is totally different and it often takes much time to learn how to operate it. For example, when customers use an information terminal at a retail store, they may have to browse more than a few pages or type several appropriate words with a keyboard or something to get to the information they want. It is very laborious and complicated especially for inexperienced users. Therefore, more intuitive and simple user interface has been desired for a long time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35048948</person_id>
				<author_profile_id><![CDATA[81335491630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takuma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hosokawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT COMWARE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893200</person_id>
				<author_profile_id><![CDATA[81335497580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shioiri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT COMWARE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P507413</person_id>
				<author_profile_id><![CDATA[81100342152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Atsunobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT COMWARE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P507553</person_id>
				<author_profile_id><![CDATA[81100281714]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mitsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT COMWARE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034135</person_id>
				<author_profile_id><![CDATA[81319494324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kase]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT COMWARE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258715</ref_obj_id>
				<ref_obj_pid>258549</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishii, H. and Ullmer, B. Tangible Bits: Towards Seamless Interfaces between People, Bits and Atoms. <i>Proceedings of CHI'97</i>, ACM Press, (1997), 234--241.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280893</article_id>
		<sort_key>159</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Development of a high precision hand motion capture system and an auto calibration method for a hand skeleton model]]></title>
		<page_from>159</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280893</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280893</url>
		<abstract>
			<par><![CDATA[<p>Motor cortex that controls the movements of the human body is divided functionally into each control region. In the motor cortex, the area for the hand is almost the same as the total area for arm, torso and lower body. This physiological fact indicates that human hand movements require very complicated control. As a result, our hand can perform high precision movements as an actuator. Motion capture (MoCap) technique that can digitize a position and a posture as a function of time is widely used in order to create animation and CG. It is very difficult to measure all hand movements because one hand has twenty-seven bones and nineteen joints. Therefore, it has been impossible to record the finger movements of a sports player that are high in speed and in accuracy.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822742</person_id>
				<author_profile_id><![CDATA[81319497817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083965</person_id>
				<author_profile_id><![CDATA[81100565428]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18006728</person_id>
				<author_profile_id><![CDATA[81319494392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaiga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Warabi-za]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034258</person_id>
				<author_profile_id><![CDATA[81319504710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Asia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823019</person_id>
				<author_profile_id><![CDATA[81331499988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034226</person_id>
				<author_profile_id><![CDATA[81100502669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hideo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822879</person_id>
				<author_profile_id><![CDATA[81319504709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Noboru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179740</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mitobe, K., et al. 2006. Development of a Motion Capture System for a Hand Using a Magnetic Three Dimensional Position Sensor. In <i>Proceedings of ACM SIGGRAPH 2006</i>, ACM Press]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280894</article_id>
		<sort_key>160</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Gazecoppet]]></title>
		<subtitle><![CDATA[hierarchical gaze-communication in ambient space]]></subtitle>
		<page_from>160</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280894</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280894</url>
		<abstract>
			<par><![CDATA[<p>This research aims to naturally evoke human-robot communications in ambient space based on a hierarchical model of gazecommunication. The interactive ambient space is created with our remote gaze-tracking technology based on image analyses and our gaze-reactive robot system. A single remote camera detects the user's gaze in unrestricted situations by using eye-ball estimation. The robot's gaze reacts with both 1) "positive evocation" by direct <i>eye contact</i> with multimodal reactions and 2) "passive evocation" by indirect <i>co-gazing</i> (watching a common object or place) according to the user's conscious/unconscious gaze.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P430792</person_id>
				<author_profile_id><![CDATA[81100469647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yonezawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P761309</person_id>
				<author_profile_id><![CDATA[81309508949]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hirotake]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamazoe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39050895</person_id>
				<author_profile_id><![CDATA[81100614815]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Utsumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35035394</person_id>
				<author_profile_id><![CDATA[81322487560]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Intelligent Robotics and Communication Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Kendon. "Some functions of gaze-direction in social interaction." <i>Acta Psychologica</i>, Vol. 26, pp. 22--63, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H. Kojima. "Infanoid: A babybot that explores the social environment." <i>Socially Intelligent Agents, K. Dautenhahn and A. H. Bond and L. Canamero and B. Edmonds, Kluwer Academic Publishers</i>, pp. 157--164, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1124886</ref_obj_id>
				<ref_obj_pid>1124772</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Santella, M. Agrawala, D. DeCarlo, D. Salesin, and M. Cohen, "Gaze-Based Interaction for Semi-Automatic Photo Cropping." In CHI 2006, pp. 771--780, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1117350</ref_obj_id>
				<ref_obj_pid>1117309</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Li, J. Babcock, D. J. Parkhurst, "openEyes: A low-cost head-mounted eye-tracking solution." <i>Proc. ACM Eye Tracking Research and Applications Symposium</i>, 2006. SensoMetoric Instruments, "3D-VOG." http://www.smi.de/3d/index.htm]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280895</article_id>
		<sort_key>161</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Mamagoto]]></title>
		<subtitle><![CDATA["playing" with food]]></subtitle>
		<page_from>161</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280895</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280895</url>
		<abstract>
			<par><![CDATA[<p>"Mamagoto" is an interactive and context-aware dining system which encourage small children to "play" with food. Even greatest scientists had times when their parents told them not to play with food. For small children, especially newly-born infants, everything they see, touch, hear, smell, taste, are new experiences. What they encounter at dining tables are no exceptions. For those curious little adventurers, tremendous varieties of colors, textures, tastes, shapes of food and of dishes and silverwares are full of amusement. Even when they seem to be messing up everything on the table, they are fully using their senses for new exploration, and this process of "playing" is important for development of their senses. This project encourages them to expand their sensory experience while eating, using "Mamagoto".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893226</person_id>
				<author_profile_id><![CDATA[81335487854]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arakawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39041135</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Maynes-Aminzade, Dan. 2005. <i>Edible Bits: Seamless Interfaces between People, Data and Food</i>. CHI2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280896</article_id>
		<sort_key>162</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[AaahCam]]></title>
		<subtitle><![CDATA[a tool a small child can use to capture and play with images]]></subtitle>
		<page_from>162</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280896</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280896</url>
		<abstract>
			<par><![CDATA[<p>The variety of movie content has been expanding. Not only conventional TV companies but also other companies (e.g., IT companies) or even individuals can "broadcast" movie content using the Internet and a camera. An era in which everyone, even a small child, can broadcast will come in the near future. However, parents may have concerns about the relationship between digital content or the devices used to access them and their children. For example, letting a child watch too much TV may not be good for them. We designed and implemented "AaahCam", which is a tool that small children can use to capture images and play with them without changing their natural behavior.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893163</person_id>
				<author_profile_id><![CDATA[81326494026]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jun-ichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1109553</ref_obj_id>
				<ref_obj_pid>1109540</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jean-Baptiste, L., and Wendy, M., Tangicam: exploring observation tools for children. In <i>Proceedings of the 2005 conference on Interaction design and children</i>, pp. 95--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280897</article_id>
		<sort_key>163</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[A context visualiser]]></title>
		<subtitle><![CDATA[the generative website project]]></subtitle>
		<page_from>163</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280897</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280897</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a prototype generative system that visualises possible sequences of information, often called <i>scenarios, narratives, stories, or contexts</i>, out of existing information so that (1) information designers and audiences of information can explore and discover possible contexts that otherwise could be missed; and (2) the generated information artefacts can stimulate creative thinking. The generated sequences are expected to work as : (1) final products that a user (audience) can enjoy; and (2) draft materials that a user (information designer) can modify.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35046581</person_id>
				<author_profile_id><![CDATA[81100379462]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shigeki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Amitani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Technology, Sydney]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P79028</person_id>
				<author_profile_id><![CDATA[81100163076]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ernest]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Edmonds]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Technology, Sydney]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1154646</ref_obj_id>
				<ref_obj_pid>1153927</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Akaishi, M., Hori, K., and Satoh, K. 2006. Topic tracer: A visualization tool for quick reference of stories embedded in document set. In <i>International Conference on Information Visualisation 2006</i>, D. A. Keim, F. Mansmann, J. Schneidewind, H. Ziegler, C. Tominski, J. Abello, F. v. Ham, H. Schumann, M. Jern, and J. Franz, Eds., 101--106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Amitani, S., and Hori, K. 2003. Knowledge nebula crystallizer for knowledge lequidization &amp; crystallization - from a theory to a methodology of knowledge management. <i>Proceedings of Expertise In Design, Design Thinking Research Symposium 6</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280898</article_id>
		<sort_key>164</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[A ball type vibro-tactile space mouse using one web camera]]></title>
		<page_from>164</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280898</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280898</url>
		<abstract>
			<par><![CDATA[<p>A mouse is a useful input device which helps a user to do some tasks using computers with ease. Recently, many 3-dimensional input devices were developed using various kinds of sensors, e.g., gyro-sensors, acceleration sensors, magnetic sensors, ultrasonic sensors, and image sensors, etc. We developed a ball type space mouse which use a web camera to sense the spatial position of the device and can give some vibro-tactile stimulation on the user's fingertip. By using a web camera, we tracked the ball(circle), covered with a certain color, as a marker. The diameter of the ball type mouse is 4cm, and all the circuits for controlling the vibro-tactile device and wireless communication with the host computer and a battery to supply the power are integrated in the ball.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893115</person_id>
				<author_profile_id><![CDATA[81335499890]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dong-jin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kangwon National University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822628</person_id>
				<author_profile_id><![CDATA[81331507416]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ho-joong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kangwon National University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822706</person_id>
				<author_profile_id><![CDATA[81319488032]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jongwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Back]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kangwon National University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P277338</person_id>
				<author_profile_id><![CDATA[81100635118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tae-Jeong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kangwon National University, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Intel Technology Journal Q2 '98, Computer Vision Face Tracking For Use in a Perceptual Use Interface, Gary R. Bradski]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179677</ref_obj_id>
				<ref_obj_pid>1179622</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ho-joong Yong, Jongwon Back, and Tae-Jeong Jang, "A stereo vision based virtual reality game by using a vibrotactile device and two position sensors," <i>ACM SIGGRAPH 2006</i>, July 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280899</article_id>
		<sort_key>165</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[The effect of using large, high-resolution stereoscopic displays for flow visualization]]></title>
		<page_from>165</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280899</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280899</url>
		<abstract>
			<par><![CDATA[<p>We report on a pilot experiment to explore the effects of display resolution, size, and viewing distance on three-dimensional flow visualizations. Participants performed three common flow visualization tasks using streamtubes, pathlines, and particle flurries on a rear-projected stereo display. Results show that participants favor high-resolution displays, although the display preference depends on the properties of the dataset. When interacting with the flow datasets, participants tend to place them at locations where they could see the whole dataset without losing context.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39087595</person_id>
				<author_profile_id><![CDATA[81100119328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17000780</person_id>
				<author_profile_id><![CDATA[81100250503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Forsberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822865</person_id>
				<author_profile_id><![CDATA[81319494845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mykhaylo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kostandov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35054359</person_id>
				<author_profile_id><![CDATA[81540655956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Willis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University and MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P62493</person_id>
				<author_profile_id><![CDATA[81100589961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Laidlaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>985748</ref_obj_id>
				<ref_obj_pid>985692</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tan, D. S., Gergle, D., Scupelli, P. G., and Pausch, R., "Physically large displays improve path integration in 3D virtual navigation tasks", <i>SIGCHI</i>, 439--446, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>329554</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ware, C., Information visualization: perception for design, Morgan Kaufmann, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280900</article_id>
		<sort_key>166</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[WikiTUI]]></title>
		<page_from>166</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280900</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280900</url>
		<abstract>
			<par><![CDATA[<p>WikiTUI is a tangible multimedia annotation system that allows users to manipulate multimedia information in common physical books. WikiTUI also facilitates the exchange of information with other readers through wiki services. Using tangible interface techniques, we seek to bring the collaborative annotation and authoring capabilities that are supported by the growing space of online wikimedia into the real world, thereby extending the use of paper books into new realms.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893103</person_id>
				<author_profile_id><![CDATA[81331507194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chih-Sung]]></first_name>
				<middle_name><![CDATA[(Andy)]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GVU Center Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050599</person_id>
				<author_profile_id><![CDATA[81324491962]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ali]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mazalek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GVU Center Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>786327</ref_obj_id>
				<ref_obj_pid>786112</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Koike, H., Kobayashi, M. 1998. EnhancedDesk: Integrating Paper Documents and Digital Documents. In <i>Proc. APCHI 1998</i>, ACM Press, 57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>159630</ref_obj_id>
				<ref_obj_pid>159544</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wellner, P. 1993. Interacting with Paper on the DigitalDesk. In <i>Commun. ACM 36, 7</i>, ACM Press, 87--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280901</article_id>
		<sort_key>167</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Facial expression affective state recognition for air traffic control automation concept exploration]]></title>
		<page_from>167</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280901</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280901</url>
		<abstract>
			<par><![CDATA[<p>Current methods for evaluating workload and usability of air traffic control automation concepts are often heavily reliant on subjective data. Typically subjects (often off-duty air traffic controllers) assess new tools or technologies in controlled experimental sessions, and then report on their memories of the session experience in some structured format, such as interviews, surveys, or scalar responses to standardized questions. These subjective reports may be affected by post-session memory distortions, and concurrent validation of is often difficult.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052668</person_id>
				<author_profile_id><![CDATA[81100551194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reisman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA Ames Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35034158</person_id>
				<author_profile_id><![CDATA[81100612806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[el Kaliouby]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Reisman, R., and Brown, D, (2006) Design of Augmented Reality Tools for Air Traffic Control Towers, 6&#60;sup&#62;th&#60;/sup&#62; AIAA Aviation Technology, Integration and Operations Conference (ATIO), Wichita, Kansas (September, 2006)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[El Kaliouby, R., and Robinson, P., (2005) Real-time Inference of Complex Mental States from Facial Expressions and Head Gestures, in <u>Real-Time Vision</u> for Human-Computer Interaction, ed. Kisacanin, B., et al, pp. 181--200 (Springer-Verlag, 2005)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>1280902</section_id>
		<sort_key>168</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rendering]]></section_title>
		<section_page_from>168</section_page_from>
	<article_rec>
		<article_id>1280903</article_id>
		<sort_key>168</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Spatial image based lighting]]></title>
		<page_from>168</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280903</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280903</url>
		<abstract>
			<par><![CDATA[<p>When compositing computer generated imagery into photographed scenes, image based lighting (IBL) [Debevec 1998] is used so that synthetic and real objects are consistently illuminated. Traditional IBL takes light captured from a single point in space however light varies spatially in many scenes. Unger et al. [2003] captured a light field for lighting, but the acquisition procedures were not practical for typical scenes. I present a practical IBL method for rendering scenes with spatially variant light.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893152</person_id>
				<author_profile_id><![CDATA[81335496258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pronk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of New South Wales]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 189--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pronk, J. 2006. <i>Spatially Variant Real World Light for Computer Graphics</i>. Honours thesis, University of New South Wales.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882425</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Unger, J., Wenger, A., Hawkins, T., Gardner, A., and Debevec, P. 2003. Capturing and rendering with incident light fields. In <i>EGRW '03: Proceedings of the 14th Eurographics workshop on Rendering</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 141--149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280904</article_id>
		<sort_key>169</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Texturing on patterned cloth with wrinkles in a 2D illustration]]></title>
		<page_from>169</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280904</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280904</url>
		<abstract>
			<par><![CDATA[<p>This research present the method to generate wrinkles on patterned cloth on a 2-D illustration from a sample texture. Because our method just using simple affine transformation in 2-D space, users can be free from preparing or operating any 3-D models until they finishes their work. This is useful especially for putting screen tones on comics or manga drawings for any type of cloth representation such as clothes, furniture and so on. First, we prepare a sample texture. It is better the texture is generated to be able to wrap around because we pick nonexistence pixels from the opposite side. Then we draw wrinkle lines on the texture with a tablet pen or a mouse. The texture is warped taking into account the input properties. By using drawing input for control transformation, we can provide intuitive operation and inspiration especially for comics and manga artists.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37034525</person_id>
				<author_profile_id><![CDATA[81328489571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kyoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Murakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Prometech Software Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37036495</person_id>
				<author_profile_id><![CDATA[81331505544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Reiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuruno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280905</article_id>
		<sort_key>170</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Interactive shade control for cartoon animation]]></title>
		<page_from>170</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280905</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280905</url>
		<abstract>
			<par><![CDATA[<p>Shade in traditional cel animation is an essential element and plays a symbolic role in the artistic portrayal of character and scene. The shade on a character's face can be used to show emotion such as anger, sadness and hatred. Despite its usability, shade is relatively quickly or roughly drawn in animation for the following reasons. Firstly, cel animators require a large amount of time to draw shade. Secondly, because of the time requirements, producing complicated shade is expensive. On the other hand, 3D models enable users to more easily render or recreate shade than cel animation techniques. Consequently, 3D models are being gradually introduced into cartoon animation. Using Phong's 3D shading model, however, shade is rendered too realistically for cartoon animation, and is therefore not appropriate for traditional-style cel animation. To solve similar problems with highlighting, Anjyo and Hiramatsu proposed a procedural method for designing stylized highlights [Anjyo and Hiramatsu 2003]. This concept has been applied to shade in our system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893246</person_id>
				<author_profile_id><![CDATA[81335497519]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimotori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893136</person_id>
				<author_profile_id><![CDATA[81335495405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hidehito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P794422</person_id>
				<author_profile_id><![CDATA[81315491993]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugisaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822404</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023348</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>858654</ref_obj_id>
				<ref_obj_pid>858619</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Anjyo and K. Hiramatsu, "Stylized Highlights for Cartoon Rendering and Animation", IEEE Computer Graphics and Applications, vol.23, No.4, pp.54--61, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280906</article_id>
		<sort_key>171</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Faster ray packets - triangle intersection through vertex culling]]></title>
		<page_from>171</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280906</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280906</url>
		<abstract>
			<par><![CDATA[<p>To eliminate unnecessary ray packet-triangle intersection tests, we check for separation of two convex objects: a triangle and a frustum containing intersections of rays with a leaf node of an acceleration structure. We show a performance improvement that is proportional to the ratio of lengths of average leaf edge to triangle edge, which opens new possibilities for creation of better acceleration structures.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052811</person_id>
				<author_profile_id><![CDATA[81314494641]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reshetov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Boulos S., Wald I., and Shirley P. 2006. Geometric and Arithmetic Culling Methods for Entire Ray Packets, <i>Technical Report UUCS-06-10, SCI Institute, University of Utah</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kensler. J. and Shirley P. 2006. Optimizing Ray-Triangle Intersection via Automated Search. In <i>Proceedings of the IEEE Symposium on Interactive Ray Tracing, 2006</i>, 33--38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280907</article_id>
		<sort_key>172</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Computational lighting reproduction for facial live video with rigid facial motion]]></title>
		<page_from>172</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280907</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280907</url>
		<abstract>
			<par><![CDATA[<p>In the field of virtual studios in the television industry, the facial live video stream and background scenes must be composed in real time. One of the complex problems in this compositing is matching the lighting between the facial live video and the background scene. In this paper, we develop a practical lighting reproduction technique to reproduce the appearance of a face in the facial live video with rigid facial motion in real time under an arbitrary environmental lighting condition.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P829423</person_id>
				<author_profile_id><![CDATA[81320492477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Makino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P587730</person_id>
				<author_profile_id><![CDATA[81100577741]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Norimichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsumura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P828937</person_id>
				<author_profile_id><![CDATA[81320495472]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takase]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893210</person_id>
				<author_profile_id><![CDATA[81335491425]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ryusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Honma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P822743</person_id>
				<author_profile_id><![CDATA[81319498622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Keiichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35044697</person_id>
				<author_profile_id><![CDATA[81332519085]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Nobutoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kao Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823057</person_id>
				<author_profile_id><![CDATA[81319497762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Toshiya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P424968</person_id>
				<author_profile_id><![CDATA[81319497766]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882344</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tsumura, N., Ojima, N., Sato, K., Shiraishi, M., Shimizu, H., Nabeshima, H., Akazaki, S., Hori, K., and Miyake, Y. 2003. Image-based skin color and texture analysis/synthesis by extracting hemoglobin and melanin information in the skin. <i>ACM Transactions on Graphics 22</i>, 3, 770--779.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280908</article_id>
		<sort_key>173</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Interactive rendering of fenestration materials for architectural design]]></title>
		<page_from>173</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280908</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280908</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35053475</person_id>
				<author_profile_id><![CDATA[81450592888]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rensselaer Polytechnic Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050709</person_id>
				<author_profile_id><![CDATA[81405592422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rensselaer Polytechnic Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39087684</person_id>
				<author_profile_id><![CDATA[81100560718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Barbara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cutler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rensselaer Polytechnic Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Andersen, Innovative bi-directional videogoniophotometer for advanced fenestration systems, Swiss Federal Institute of Technology (EPFL), 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tim Heidmann, Real Shadows, Real Time, Iris Universe, Silicon Graphics, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808601</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cindy Goral, et. al., Modeling the Interaction of Light Between Diffuse Surfaces, SIGGRAPH 1984.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280909</article_id>
		<sort_key>174</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[DXRenderFarm]]></title>
		<subtitle><![CDATA[an Xgrid based render farm for Maya&#174;]]></subtitle>
		<page_from>174</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280909</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280909</url>
		<abstract>
			<par><![CDATA[<p>DXRenderFarm is distributed rendering software for Maya&#174; on Mac OS X, build upon Xgrid, Apple's grid computing technology. Since it was originally developed for computer graphics courses at DXARTS, University of Washington, it mainly targets the use in the open environment where a number of the students need to share the computational resources to render their CG works and effective system administration is required, and where all those computers must be available to any user at the same time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052116</person_id>
				<author_profile_id><![CDATA[81537101456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Autodesk. Autodesk -- Autodesk Maya. &lt; http://www.autodesk.com/maya &gt;]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Apache Tomcat. Apache Tomcat -- Apache Tomcat. &lt; http://tomcat.apache.org/ &gt;]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[PostgreSQL. PostgreSQL:The world's most advanced open source database. &lt; http://www.postgresql.org &gt;]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Xgrid. Apple -- Mac OS X server -- Xgrid. &lt; http://www.apple.com/acg/xgrid/ &gt;]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280910</article_id>
		<sort_key>175</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Real-time rendering of dynamic clouds]]></title>
		<page_from>175</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280910</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280910</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a fast cloud rendering method for dynamic scenes, where cloud shapes and lighting environments dynamically change. Although the Harris method [Harris and Lastra 2001] has been widely used for static cloud rendering, it can be fatally slow for real-time applications when, for example, light directions change. By introducing the 3D attenuation buffer and re-arranging the algorithm, we improved the rendering speeds of dynamic clouds by a factor of 10--100 times. The image quality is also improved due to a finer representation of light distribution.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893250</person_id>
				<author_profile_id><![CDATA[81335497432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Prometech Software, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36029177</person_id>
				<author_profile_id><![CDATA[81100495695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mikio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toho University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35052145</person_id>
				<author_profile_id><![CDATA[81335497324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shiraishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toho University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P823014</person_id>
				<author_profile_id><![CDATA[81319492976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Harris, M. J., and Lastra, A. 2001. Real-Time Cloud Rendering. Computer Graphics Forum, 20, 3, 76--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280911</article_id>
		<sort_key>176</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Simple cellular texturing for medieval castles]]></title>
		<page_from>176</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280911</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280911</url>
		<abstract>
			<par><![CDATA[<p>Procedurally generated 3d models and textures are becoming ever more important for content-hungry applications such as video games. Previously we reported on procedurally generating medieval castle models from landscape specifications [Colyar and Matthews 2005]. For texturing these castles a Worley cellular texture [Worley 1996] is natural, as each cell in the tesselation can represent a single stone. It has the advantage over 2d textures in that for stones going around complex corners, such as at the crenellations and machicolations surmounting a castle wall, the textures on all sides of the geometry match up [Legakis et al. 2001].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35049837</person_id>
				<author_profile_id><![CDATA[81543572156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hall]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Western Washington University, Bellingham, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893129</person_id>
				<author_profile_id><![CDATA[81335494688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Geoffrey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matthew]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Western Washington University, Bellingham, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1186437</ref_obj_id>
				<ref_obj_pid>1186415</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chan, B., and McCool, M. 2004. Worley cellular textures in sh. In <i>SIGGRAPH '04: ACM SIGGRAPH 2004 Posters</i>, ACM Press, New York, NY, USA, 18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1186963</ref_obj_id>
				<ref_obj_pid>1186954</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Colyar, K. S., and Matthews, G. B. 2005. Procedural modeling of medieval castles. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Posters</i>, ACM Press, New York, NY, USA, 8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383293</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Legakis, J., Dorsey, J., and Gortler, S. 2001. Featurebased cellular texturing for architectural models. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 309--316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237267</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Worley, S. 1996. A cellular texture basis function. In <i>SIGGRAPH '96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 291--294.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280912</article_id>
		<sort_key>177</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Single-pass shadow volumes for arbitrary meshes]]></title>
		<page_from>177</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280912</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280912</url>
		<abstract>
			<par><![CDATA[<p>We introduce a new method for rendering shadow volumes of arbitrary meshes as fast as those of specially prepared models, i.e., with a single pass over the shadow geometry. We use only unsigned 8- or 16-bit formats, which are supported on all major video-game console and workstation GPUs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35050743</person_id>
				<author_profile_id><![CDATA[81327490282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Morgan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGuire]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Williams College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>988877</ref_obj_id>
				<ref_obj_pid>988834</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aldridge and Woods, 2004. Robust, geometry-independent shadow volumes. <i>In</i> GRAPHITE '04, ACM Press, New York, NY, 250--253(Oct).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Everitt and Kilgard. 2002, Practical and robust stenciled shadow volumes for hardware-accelerated rendering. Tech. rep. NVIDIA, Austin TX, Mar.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Roettger, Irion, and Ertl, 2002. Shadow volumes revisited. <i>In</i> WSCG '02, V. Skala, Ed., vol. 2, 373--379.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280913</article_id>
		<sort_key>178</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Subtle gaze direction]]></title>
		<page_from>178</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280913</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280913</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P791878</person_id>
				<author_profile_id><![CDATA[81100645165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Reynold]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bailey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington Univ. St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083713</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[St. Louis Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P699013</person_id>
				<author_profile_id><![CDATA[81100528357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nisha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sudarsanam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington Univ. St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39048487</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington Univ. St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dodge, R. 1900. Visual perception during eye movement. <i>Psychological Review 7</i>, 454--465.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280914</article_id>
		<sort_key>179</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Interactive backprojected soft shadows with an occlusion camera shadow map]]></title>
		<page_from>179</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280914</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280914</url>
		<abstract>
			<par><![CDATA[<p>Soft shadows are crucial for computer generated imagery, as they increase the level of realism and improve spacial perception. Generating soft shadows is inherently difficult because it involves evaluating the visibility between an area light source and every point in the scene. One way to accurately compute this visibility is to backproject occluder geometry onto the light for every point in the scene. However, searching for relevant edges is costly so backprojection scales poorly with increased scene complexity. We present an interactive soft shadow algorithm that draws insight from the depth discontinuity occlusion camera model (DDOC) [Popescu and Aliaga 2006], a non-pinhole camera that bends light rays near discontinuity edges to sample normally hidden surfaces. By carefully choosing the size of DDOC distortion regions based on light and occluder geometry, we generate a map that stores the closest silhouette edges for each pixel. This enables us to find the most relevant edge to backproject using a single map lookup and additional edges with a small neighborhood search. Our method combines the accuracy of object-space backprojection methods with the efficiency of an image-based approach.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893204</person_id>
				<author_profile_id><![CDATA[81335495020]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Iowa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39053665</person_id>
				<author_profile_id><![CDATA[81100265704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wyman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Iowa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192207</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Drettakis, G., and Fiume, E. 1994. A fast shadow algorithm for area light sources using backprojection. In <i>Proceedings of SIGGRAPH</i>, 223--230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383922</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Guennebaud, G., Barthe, L., and Paulin, M. 2006. Realtime soft shadow mapping by backprojection. In <i>Eurographics Symposium on Rendering</i>, 227--234.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mo, Q., Popescu, V., and Wyman, C. 2007. The soft shadow occlusion camera. Tech. Rep. UICS-07-02, University of Iowa. (<i>Submitted to Pacific Graphics</i>).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111436</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Popescu, V., and Aliaga, D. 2006. The depth discontinuity occlusion camera. In <i>Proc. Symp. Interactive 3D Graphics and Games</i>, 139--143.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280915</article_id>
		<sort_key>180</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Better faster noise with the GPU]]></title>
		<page_from>180</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280915</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280915</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P94933</person_id>
				<author_profile_id><![CDATA[81100583797]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Geoff]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wyvill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Otago]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35048348</person_id>
				<author_profile_id><![CDATA[81335490578]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeppe]]></first_name>
				<middle_name><![CDATA[Revall]]></middle_name>
				<last_name><![CDATA[Frisvad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Denmark]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. 1985. An image synthesizer. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH '85)</i>, 19, 3, ACM, 287--296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566636</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. 2002. Improving noise. <i>ACM Transactions on Graphics</i>, 21, 3, 681--682.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74360</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lewis, J. P. 1989. Algorithms for solid noise synthesis. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH '89)</i>, 23, 3, ACM, 263--270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122751</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[van Wijk, J. J. 1991. Texture synthesis for data visualization. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH '91</i>), 25, 4, ACM, 309--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280916</article_id>
		<sort_key>181</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Image morphing for space-time interpolation]]></title>
		<page_from>181</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280916</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280916</url>
		<abstract>
			<par><![CDATA[<p>The human brain automatically attempts to interpret the physical visual inputs from our eyes in terms of plausible motion of the viewpoint and/or of the observed object or scene [Ellis 1938; Graham 1965; Giese and Poggio 2003]. In the physical world, the rules that define plausible motion are set by temporal coherence, parallax, and perspective projection. Our brain, however, refuses to feel constrained by the unrelenting laws of physics in what it deems plausible motion. Image metamorphosis experiments, in which unnatural, impossible in-between images are interpolated, demonstrate that under certain circumstances, we willingly accept chimeric images as plausible transition stages between images of actual, known objects [Beier and Neely 1992; Seitz and Dyer 1996]. Or think of cartoon animations which for the longest time were hand-drawn pieces of art that didn't need to succumb to physical correctness. The goal of our work is to exploit this freedom of perception for space-time interpolation, i.e., to generate transitions between still images that our brain accepts as plausible motion in a moving 3D world.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP36037793</person_id>
				<author_profile_id><![CDATA[81330498865]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Timo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044695</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Beier, T., and Neely, S. 1992. Feature-based image metamorphosis. In <i>Proceedings of SIGGRAPH'92</i>, Chicago, ACM, 35--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ellis, W., Ed. 1938. <i>A Source Book of Gestalt Psychology</i>. Kegan Paul, Trench, Trubner &amp; Co. Ltd.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Giese, M., and Poggio, T. 2003. Neural mechanisms for the recognition of biological movements. <i>Nature Reviews -- Neuroscience 4</i> (Mar.), 179--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Graham, C. 1965. <i>Vision and Visual Perception</i>. New York: Wiley, ch. Perception of movement.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Seitz, S. M., and Dyer, C. R. 1996. View morphing. In <i>Proceedings of SIGGRAPH'96</i>, New Orleans, ACM, 21--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280917</article_id>
		<sort_key>182</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Noise-free BSSRDF rendering on the cheap]]></title>
		<page_from>182</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280917</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280917</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893095</person_id>
				<author_profile_id><![CDATA[81421594914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anders]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Langlands]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35050568</person_id>
				<author_profile_id><![CDATA[81335494509]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mertens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566619</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., and Buehler, J. 2002. A rapid hierarchical rendering technique for translucent materials. In <i>SIGGRAPH 2002</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. In <i>SIGGRAPH 2001</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mertens, T., and Langlands, A., 2007. Noise reduction for rendering with diffusion BSSRDFs. Submitted.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280918</article_id>
		<sort_key>183</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Fast rendering of realistic faces with wavelength dependent normal maps]]></title>
		<page_from>183</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280918</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280918</url>
		<abstract>
			<par><![CDATA[<p>Due to complex nature of the facial reflectance, high end rendering techniques often require too much data or too much computation time to be suitable for production or real time rendering. In this work, we present a novel and efficient face rendering technique based on the use of wavelength dependent normal maps. Normal maps are typically used for the high frequency surface details they contain. Previous work showed that normal maps can be adjusted to include more complex effects such as ambient occlusion We extend this further and demonstrate that by using a separate normal map for each color channel it is possible to represent complex wavelength dependent material properties such as subsurface scattering. Our technique is easy to integrate into both real time and global illumination rendering frameworks. We demonstrate how to use these normal maps to produce photorealistic digital faces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822485</person_id>
				<author_profile_id><![CDATA[81319489365]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charles-F&#233;lix]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chabert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P501647</person_id>
				<author_profile_id><![CDATA[81100447998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wan-Chun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117244</person_id>
				<author_profile_id><![CDATA[81100179328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hawkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40037970</person_id>
				<author_profile_id><![CDATA[81100016992]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P221188</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>965470</ref_obj_id>
				<ref_obj_pid>965400</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Borshukov, G., and Lewis, J. P. 2003. Realistic human face rendering for "the matrix reloaded". <i>SIGGRAPH 2003</i>. Sketches and Applications Program.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566619</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., and Buhler, J. 2002. A rapid hierarchical rendering technique for translucent materials. <i>ACM Trans. Graph. 21</i>, 3, 576--581.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Wiess, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>EGSR 2007</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073226</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Nehab, D., Rusinkiewicz, S., Davis, J., and Ramamoorthi, R. 2005. Efficiently combining positions and normals for precise 3D geometry. <i>ACM Trans. Graph. 24</i>, 3, 536--543.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280919</article_id>
		<sort_key>184</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Interactive rendering of dynamic environment using PID control]]></title>
		<page_from>184</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280919</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280919</url>
		<abstract>
			<par><![CDATA[<p>To achieve unobtrusive interactivity in rich multimedia applications, techniques such as level-of-detail control and fixed frame rate schedulers are commonly used. However, the former technique does not guarantee a bounded frame rate while the latter may not be straightforward to implement. We present an approach to interactive rendering of complex and dynamic virtual environment with very high accuracy in frame rate control using the <i>Proportional, Integral</i> and <i>Derivative</i> technique.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35046978</person_id>
				<author_profile_id><![CDATA[81405596063]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chee-Kien]]></first_name>
				<middle_name><![CDATA[Gabriyel]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nanyang Technological University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35054206</person_id>
				<author_profile_id><![CDATA[81320496237]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jianliang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nanyang Technological University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>863276</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Luebke, M. Reddy, J. Cohen, A. Varshney, B. Watson, and R. Huebner, <i>Level of Detail for 3D Graphics</i>. Morgan Kaufman 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C. G. Wong, J. Wang, "Modeling Real-time Rendering", EUROGRAPHICS Conference 2006, Short Papers, ISSN 1017-4656, pages 89--93, Vienna, Austria.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280920</article_id>
		<sort_key>185</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[An optical system for single-image environment maps]]></title>
		<page_from>185</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280920</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280920</url>
		<abstract>
			<par><![CDATA[<p>We present an optical setup for capturing a full 360&#176; environment map in a single image snapshot. The setup, which can be used with any camera device, consists of a curved mirror swept around a negative lens, and is suitable for capturing environment maps and light probes. The setup achieves good sampling density and uniformity for all directions in the environment.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP36040048</person_id>
				<author_profile_id><![CDATA[81100120690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Unger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P819754</person_id>
				<author_profile_id><![CDATA[81320490266]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gustavson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>339364</ref_obj_id>
				<ref_obj_pid>339355</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baker, S., and Nayar, S. 1999. A Theory of Single-Viewpoint Catadioptric Image Formation. <i>International Journal on Computer Vision 35</i>, 2 (Nov), 175--196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, ACM Press, New York, NY, USA, 189--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1117983</ref_obj_id>
				<ref_obj_pid>1117939</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Swaminathan, R., Grossberg, M. D., and Nayar, S. K. 2006. Non-Single Viewpoint Catadioptric Cameras: Geometry and Analysis. <i>International Journal of Computer Vision 66</i>, 3 (Mar), 211--229.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280921</article_id>
		<sort_key>186</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[NPR in production]]></title>
		<subtitle><![CDATA[animating the Sung dynasty painting "Children at Play"]]></subtitle>
		<page_from>186</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280921</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280921</url>
		<abstract>
			<par><![CDATA[<p>Research on Non-Photorealistic Rendering (NPR) [Ma et al. 2002] has produced an array of techniques such as outline detection, simulation of brush strokes, cartoon shading, etc. However, the practical application of these techniques to reproduce a specific masterpiece is seldom discussed. During a recent project in cooperation with the National Palace Museum of Taiwan, we were given the task of rendering animated characters in the style of a well-known Chinese Sung dynasty painting, "Children at Play." This sketch describes a step-by-step method for achieving this effect using a variety of NPR techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35049556</person_id>
				<author_profile_id><![CDATA[81335490937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shuen-Huei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digimax]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35051478</person_id>
				<author_profile_id><![CDATA[81320492417]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digimax]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023797</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P308194</person_id>
				<author_profile_id><![CDATA[81350582710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yung-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cahill, J. 1960. <i>Chinese Painting</i>. Rizzoli.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ma, K.-L., Hertzmann, A., Interrante, V., and Lum, E. B. 2002. Recent advances in non-photorealistic rendering for art and visualization. In <i>SIGGRAPH 2002 Course Notes. Course 23</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280922</article_id>
		<sort_key>187</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Multi-scale shape manipulations in photographs]]></title>
		<page_from>187</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280922</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280922</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35052264</person_id>
				<author_profile_id><![CDATA[81335495800]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexandrina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Orzan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ARTIS Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P789067</person_id>
				<author_profile_id><![CDATA[81314487621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ARTIS Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P789059</person_id>
				<author_profile_id><![CDATA[81314487615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adrien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bousseau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ARTIS Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40038081</person_id>
				<author_profile_id><![CDATA[81319502458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jo&#235;lle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thollot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ARTIS Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>370561</ref_obj_id>
				<ref_obj_pid>370550</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Elder, J. H., and Goldberg, R. M. 2001. Image editing in the contour domain. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 23</i>, 3, 291--296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kang, H. W., Chui, C. K., and Chakraborty, U. K. 2005. Interactive sketch generation. <i>The Visual Computer (Proceedings of Pacific Graphics 2005)</i> 21, 8, 821--830.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280923</article_id>
		<sort_key>188</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Extended diffuse reflection model for reflectance estimation]]></title>
		<page_from>188</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280923</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280923</url>
		<abstract>
			<par><![CDATA[<p>Reflectance properties play an important roll in generating realistic images and a lot of research has been done to determine the reflectance of real objects from images[Sato et al. 1997]. In the process of the estimation, the Lambertian model is commonly used to estimate the diffuse reflection of real objects. However, this model could not represent the diffuse reflection properly[Oren and Nayar 1994]. In this paper, we propose a new diffuse reflection model which can represent the diffuse reflection of real objects. Our proposed model is very simple and can be used for estimating reflectance of real objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P557185</person_id>
				<author_profile_id><![CDATA[81100395213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893166</person_id>
				<author_profile_id><![CDATA[81335491576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Junya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hiramatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P479719</person_id>
				<author_profile_id><![CDATA[81100138836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mukunoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P207182</person_id>
				<author_profile_id><![CDATA[81100515686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192213</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Oren, M., and Nayar, S. K. 1994. Generalization of lambert's reflectance model. In <i>Proceedings of SIGGRAPH 94</i>, Computer Graphics Proceedings, Annual Conference Series, 239--246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258885</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sato, Y., Wheeler, M. D., and Ikeuchi, K. 1997. Object shape and reflectance modeling from observation. In <i>Proceedings of SIGGRAPH 97</i>, Computer Graphics Proceedings, Annual Conference Series, 379--388.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280924</article_id>
		<sort_key>189</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Real-time volume shading for deformable model]]></title>
		<page_from>189</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280924</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280924</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893213</person_id>
				<author_profile_id><![CDATA[81100255754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Satoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology, Nara, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P480344</person_id>
				<author_profile_id><![CDATA[81100552380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Megumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology, Nara, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P806161</person_id>
				<author_profile_id><![CDATA[81317498396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kotaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology, Nara, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[C. D. Correa, D. Silver, M. Chen, "Discontinuous Displacement Mapping for Volume Graphics", Proc. Volume Graphics, pp. 9--16, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187110</ref_obj_id>
				<ref_obj_pid>1186954</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Nakao, T. Kuroda, K. Minato, "Volume Interaction with Voxels by Manipulating 3D General Grids", ACM SIGGRAPH Poster, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280925</article_id>
		<sort_key>190</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[A low-cost test bed for light field capture experiments]]></title>
		<page_from>190</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280925</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280925</url>
		<abstract>
			<par><![CDATA[<p>Current light-field capture devices are complex, usually require modifying the internals of a conventional camera, and discourage new experimentation in a topic area full of new research questions. We adapt a novel 'home-made' camera devised by Wang and Heidrich [2004] for use in a variety of light-field experiments. Unlike commercially available cameras, the camera provides a large effective sensor size, large distance between the lens and the sensor, much larger effective pixel size, extremely high resolution (490 million pixels), low cost (around $500US), and a nominal capture speed of a few seconds. This easily-modified design encourages novices to experiment with light fields. We demonstrate how to implement the microlens based light-field camera of Ng et al. [2005], and the mask based light field camera of Veeraraghavan et al. [2007].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40038255</person_id>
				<author_profile_id><![CDATA[81300333501]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ashok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veeraraghavan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Maryland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P828539</person_id>
				<author_profile_id><![CDATA[81320492783]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ankit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mohan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Northwestern University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P130685</person_id>
				<author_profile_id><![CDATA[81100216430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tumblin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Northwestern University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022814</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40025705</person_id>
				<author_profile_id><![CDATA[81100634373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Amit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Agrawal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ng, R., Levoy, M., Br&#233;dif, M., Duval, G., Horowitz, M., and Hanrahan, P. 2005. Light field photography with a hand-held plenoptic camera. Tech. Rep. CSTR 2005-02, Stanford University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Veeraraghavan, A., Raskar, R., Agrawal, A., Mohan, A., and Tumblin, J. 2007. Coded aperture and optical heterodyning. In <i>SIGGRAPH</i>, to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wang, S., and Heidrich, W. 2004. The design of an inexpensive very high resolution scan camera system. In <i>Eurographics</i>, 441--450.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280926</article_id>
		<sort_key>191</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Fast approximate ambient occlusion]]></title>
		<page_from>191</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280926</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280926</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P893130</person_id>
				<author_profile_id><![CDATA[81335488452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gilles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cadet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Engineering School Supa&#233;ro, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P893099</person_id>
				<author_profile_id><![CDATA[81537911256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bernard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[L&#233;cussan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[HPC-SA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christensen P. H., Fong J., Laur D. M. and Batali D.: Ray Tracing for the Movie 'Cars'. <i>Proceedings of the IEEE Symposium on Interactive Ray Tracing 2006</i>, pages 1--6. IEEE, September 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1053434</ref_obj_id>
				<ref_obj_pid>1053427</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kontkanen J., Laine S.: Ambient occlusion fields. In <i>Proc. I3D</i> (2005), ACM Press, pp. 41--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Landis H.: Production-ready global illumination. <i>In SIGGRAPH 2002</i> course note #16, pages 87--102, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Malmer M., Malmer F., Assarsson U., Holzschuch N.: <i>Fast Precomputed Ambient Occlusion for Proximity Shadows</i>. Tech. Rep. RR-5779, INRIA, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Pharr M.: Ambient occlusion. In <i>GPU Gems</i> (2004), Fernando R., (Ed.), Addison Wesley, pp. 667--692.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Zhukov S., Iones A., Kronin G.: An ambient light illumination model. In <i>Rendering Techniques '98 (Proc. EGWR)</i> (1998), pp. 45--55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280927</article_id>
		<sort_key>192</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Skin image rendering by multi-resolution texture synthesis]]></title>
		<page_from>192</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280927</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280927</url>
		<abstract>
			<par><![CDATA[<p>Skin image rendering is important for several research areas, such as computer graphics, medical simulation, and cosmetics development. Elimination of conspicuous patterns, such as scar and mole, from skin image and attachment of the patterns to skin image are required in these areas. However, such a skin image retouch is not simple. When we crop an image piece including a conspicuous pattern from a person's skin image and put it on another person's skin image, it is difficult to fit its skin color and texture to the color and texture of the surrounding area. Human skin surface is not uniform but non-uniformly textured with some tissues such as blood spots, wrinkles, and fingerprint. Therefore the skin image rendering includes precise color reproduction and synthesis of the complicated tissue patterns.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35047949</person_id>
				<author_profile_id><![CDATA[81100266545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Motonori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Doi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka Electro-Communication University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36044489</person_id>
				<author_profile_id><![CDATA[81100197625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shoji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tominaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chiba University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Efros, A. A., and Leung, T. K. 1999. Texture synthesis by non-parametric sampling. In <i>IEEE International Conference on Computer Vision</i>, 1033--1038.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280928</article_id>
		<sort_key>193</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Image-space particle emission]]></title>
		<page_from>193</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280928</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280928</url>
		<abstract>
			<par><![CDATA[<p>Particle systems are visualized by rendering some fixed number of particles per frame, where particle information is sampled from emitters represented by a location, and a range of potential initial particle parameters, such as origin, size, etc. For some complicated effects, especially for such effects where there are many potential emitters, where the location of the emitter is not well-defined, or when particle origins or other parameters can not be easily defined by an analytic boundary, the problem of generating new particle samples becomes difficult. One concrete example of such a scenario is the modelling of rain or hail splattering from visible objects as particles, where every point on a surface that can receive rain is a potential emitter. Another example comes in the form of a large-scale detailed forest fire simulation, where each leaf (or portion of leaf) that is burning is an emitter of flame particles, and there are potentially thousands of leaves visible at any given time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P822649</person_id>
				<author_profile_id><![CDATA[81331496748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaakko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Konttinen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14020468</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39073474</person_id>
				<author_profile_id><![CDATA[81410595526]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1111428</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dachsbacher, C., and Stamminger, M. 2006. Splatting indirect illumination. In <i>SI3D '06: Proceedings of the 2006 symposium on Interactive 3D graphics and games</i>, ACM Press, New York, NY, USA, 93--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1058146</ref_obj_id>
				<ref_obj_pid>1058129</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kipfer, P., Segal, M., and Westermann, R. 2004. Uber-flow: a gpu-based particle engine. In <i>HWWS '04: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</i>, ACM Press, New York, NY, USA, 115--122.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1280929</article_id>
		<sort_key>194</sort_key>
		<display_label></display_label>
		<article_publication_date>08-05-2007</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[4D view synthesis]]></title>
		<subtitle><![CDATA[navigating through time and space]]></subtitle>
		<page_from>194</page_from>
		<page_to>es</page_to>
		<doi_number>10.1145/1280720.1280929</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1280929</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39088486</person_id>
				<author_profile_id><![CDATA[81335498182]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mingxuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Inst. of Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P870598</person_id>
				<author_profile_id><![CDATA[81330497661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Grant]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schindler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Inst. of Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40027887</person_id>
				<author_profile_id><![CDATA[81385598072]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sing]]></first_name>
				<middle_name><![CDATA[Bing]]></middle_name>
				<last_name><![CDATA[Kang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39064437</person_id>
				<author_profile_id><![CDATA[81100404071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dellaert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Inst. of Tech.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237191</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., Taylor, C. J., and Malik, J. 1996. Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. In <i>Proceedings of SIGGRAPH 2000</i>, ACM Press / ACM SIGGRAPH, vol. 30, ACM, 11--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Schindler, G., Dellaert, F., and Kang, S. B. 2007. Inferring temporal order of images from 3D structure. In <i>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</i>, IEEE.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141964</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Snavely, N., Seitz, S., and Szeliski, R. 2006. Photo tourism: Exploring photo collections in 3D. In <i>Proceedings of SIGGRAPH 2000</i>, ACM Press / ACM SIGGRAPH, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2007</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
</content>
</proceeding>
