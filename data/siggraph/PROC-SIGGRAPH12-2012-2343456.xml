<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/05/2012</start_date>
		<end_date>08/09/2012</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2343456</proc_id>
	<acronym>SIGGRAPH '12</acronym>
	<proc_desc>ACM SIGGRAPH 2012 Emerging Technologies</proc_desc>
	<conference_number>2012</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-1680-4</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2012</copyright_year>
	<publication_date>08-05-2012</publication_date>
	<pages>26</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Interact with the latest systems before they become hot topics in mainstream media and blogs. Emerging Technologies presents innovative technologies and applications in several fields, from displays and input devices to collaborative environments and robotics, and technologies that apply to film and game production.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>2343457</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D capturing using multi-camera rigs, real-time depth estimation and depth-based content creation for multi-view and light-field auto-stereoscopic displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343457</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343457</url>
		<abstract>
			<par><![CDATA[<p>The wide variety of commercially available and emerging 3D displays - such as stereoscopic, multi-view and light-field displays - makes content creation challenging as each displays technology requires a different number of views available of the scene. As consequence, the content creation pipelines differ considerably and involve different camera setups such as beam-splitter rigs with small baselines and high quality cameras used for stereo 3D productions or camera arrays for auto-stereoscopic displays which usually use small lower quality cameras in a side-by-side arrangement. Converting content shot for a specific display technology into a different format usually impairs the image quality and is very labor-intensive.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738766</person_id>
				<author_profile_id><![CDATA[81430603911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[Tamas]]></middle_name>
				<last_name><![CDATA[Kovacs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Holografika]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738767</person_id>
				<author_profile_id><![CDATA[81479662420]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frederik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zilly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer HHI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Capturing using Multi-Camera Rigs, Real-time Depth Estimation and Depth-based Content Creation for 
Multi-view and Light-field Auto-Stereoscopic Displays Peter Tamas Kovacs1, Frederik Zilly2 Overview The 
wide variety of commercially available and emerging 3D displays - such as stereoscopic, multi-view and 
light-field displays - makes content creation challenging as each displays technology requires a different 
number of views available of the scene. As consequence, the content creation pipelines differ considerably 
and involve different camera setups such as beam­splitter rigs with small baselines and high quality 
cameras used for stereo 3D productions or camera arrays for auto-stereoscopic displays which usually 
use small lower quality cameras in a side-by-side arrangement. Converting content shot for a specific 
display technology into a different format usually impairs the image quality and is very labor-intensive. 
Against this background a generic method for capturing and rendering live 3D footage for stereoscopic, 
multi-view and light­field displays is presented. The system consists of a wide­baseline multi-camera 
rig, a camera assistance system, a real­time depth estimator, a real-time view generation and rendering 
engine, and multiple displays, one multi-view auto-stereoscopic and a light-field display. The system 
features several innovative components: the professional-grade multi-camera assistance and calibration 
system; a real-time depth estimator producing convincing depth maps; a real-time and generic depth-image 
based rendering (DIBR) engine that is suitable for generating imagery for a range of 3D displays; and 
the largest auto­stereoscopic light-field display to date. The Experience for SIGGRAPH Attendees The 
exhibited system will allow attendants (watching one of the 3D displays) to see other attendants (standing 
in front of the multi-camera rig) in 3D, without glasses, with the possibility to walk around the perceived 
3D image, experience smooth motion parallax and large depth-range. Visitors are interested in the technical 
details will also have the opportunity to see the camera calibration and assistance system in operation, 
as well as the output of the real-time depth estimation system (as gray-scale depth maps), and also other 
3D content including interactive 3D applications on the light-field display. Core Technical Innovations 
The first 4-camera rig built from professional HD cameras (Sony HDC-P1) is presented. In order to support 
a wide range of 3D displays while being backwards compatible with already established stereo displays, 
two of the captured views are shot using a beam-splitter which allows showing them directly on a stereoscopic 
3D display without any further processing. The additional satellite cameras placed outside the mirror 
box provide the information that is needed to create a generic depth­based 3D representation format and 
content for other wide baseline applications. The first professional-grade multi-camera calibration and 
assistance system is demonstrated as part of the system. The precise calibration of a multi-camera system 
is a demanding task as the system has many degrees of freedom. However, to keep the system easy to use 
and robust, a dedicated assistance system has been developed as an extension of the stereoscopic analyzer 
(STAN) towards a quadrifocal setup. For the multi-camera setup we bring all four cameras in a position 
such that each pair 1 Holografika 2 Fraunhofer HHI of two cameras is rectified. The system has shown 
its maturity and suitability for productions during two field trials. We present the first real-time 
multi-view depth estimation system based on line-recursive matching that generates depth maps for the 
visualization components. We have extended the existing depth estimator Hybrid Recursive Matcher (HRM) 
towards parallelization. Although the HRM is able to generate depth maps in real-time for smaller resolutions, 
its recursive structure prevents it to take advantage of multiple CPU cores. We broke the recursive structure 
of the HRM and limited the recursion on a line-wise level. Thus, each line can be processed in a different 
thread, resulting in a significant speed-up when executed on multi-core CPUs. The estimation is applied 
independently for the images with subsequent filtering of left/right consistent disparities. Temporal 
stability avoiding flickering artifacts is achieved by incorporating temporal disparity candidates in 
the estimation process. The first real-time Multi-View plus Depth (MVD) based view generation &#38; rendering 
system targeted for wide-baseline light­field displays is presented. The view generator renders interpolated 
views (between original cameras) as well as heavily extrapolated (outside the original cameras) novel 
views. The interpolation process detects and keeps gap area information from the content using depth 
layers. Extrapolation is hierarchical, using each image from the closest to the furthest. Holes are filled 
using information coming from the other images, where available. Inpainting techniques are used where 
no information is available, during which texture and structure is rendered, propagating contour gradients 
with prioritized matching costs. The largest glasses-free light-field 3D display to date (140 screen 
diagonal) will be shown. The display presents natural 3D light-field to a larger audience on a cinema-sized 
screen size previously not possible with auto-stereoscopic displays. The display itself consists of a 
complex hardware and software system, being the first front-projected light-field 3D display, controlling 
63 Mega-Pixels in total. It consists of an array of optical engines, projecting light rays onto a reflective 
holographic screen, in front of which viewers can see 3D content with an exceptionally wide Field Of 
View and depth range. The Future of this Work Today s glasses-based stereoscopic 3D display systems 
can be seen as stepping stones towards more advanced 3D display technologies. The generic Multi-View 
plus Depth (MVD) representation used inside the system can serve as the future 3DTV format, which is 
generic enough to drive a multitude of 3D displays, independent of the underlying technology. The presented 
approach is also in line with MPEG s efforts towards future 3DTV formats.  Acknowledgments The demonstrated 
system is based on work partially supported by the MUSCADE European FP7 project (EU-FP7-247010). Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>MUSCADE European FP7 project</funding_agency>
			<grant_numbers>
				<grant_number>EU-FP7-247010</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2343458</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A colloidal display]]></title>
		<subtitle><![CDATA[membrane screen that combines transparency, BRDF and 3D volume]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343458</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343458</url>
		<abstract>
			<par><![CDATA[<p>It is a common knowledge that the surface of soap bubble is a micro membrane. It allows light to pass through and displays the color on its structure. We developed an ultra thin and flexible BRDF screen using the mixture of two colloidal liquids. There have been several researches on dynamic BRDF display[1] in the past. However, our work is different in several points. Our membrane screen can be controlled using ultrasonic vibrations. Membrane can change its transparency and surface states depending on the scales of ultrasonic waves. Based on these facts, we developed several applications of the membranes such as 3D volume screen.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738768</person_id>
				<author_profile_id><![CDATA[81466641621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738769</person_id>
				<author_profile_id><![CDATA[81504682972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738770</person_id>
				<author_profile_id><![CDATA[81482649155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Keisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toyoshima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Hullin et al., Dynamic Display of BRDFs. In: Proceedings of Eurographics 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Colloidal Display:membrane screen that combines transparency, BRDF and 3D volume Yoichi Ochiai* Alexis 
Oyama** Keisuke Toyoshima*** *the University of Tokyo **Carnegie Mellon University ***University of Tsukuba 
 Figure 1: (top-left)controllable transparency, (top-center) sardine s back texture (top-right) wood 
texture on membrane screen (bottom-left) plane based 3D screen (bottom-center) metal texture on screen 
(bottom-right) piercing the object into the screenIt s very dif.cult to take the picture as we see this 
display. The re.ection of projector light is dif.cult to capture in cameras. 1. Introduction It is a 
common knowledge that the surface of soap bubble is a micro membrane. It allows light to passthrough 
and displays the color on its structure. We de­veloped an ultra thin and .exible BRDF screen using themixture 
of two colloidal liquids. There have been severalresearches on dynamic BRDF display[1] in the past. However, 
our work is different in several points. Our membrane screen can be controlled using ultrasonic vi­brations. 
Membrane can change its transparency and surface states depending on the scales of ultrasonic waves. 
Based on these facts, we developed several ap­plications of the membranes such as 3D volume screen. The 
combination of the ultrasonic waves and ultra thin membranes makes more realistic, distinctive, and vivid 
imageries on screen. This system contributes to open up a new path for display engineering with sharp 
imageries, transparency, BRDF and .exibility. 2. Design We developed the .rst prototype by using soap 
and milk. These are colloidal liquids and their moleculeshave different sizes and colors. With ultrasonic 
paramet­ric speakers, we could control their movements (liquidsand particles) on the membranes. If they 
move with in­tensity, the re.ections change while the membrane works as a projector screen. Since we 
can control thedynamics of the wavelengths, the state of the surface can be easily changed.    0-4000Hz 
Ultrasonic modulated by 50kHz Light membrane surface -40° to 40° display transparency transparency With 
the parametric speakers, this system can make thescreens drastically thinner since it does not need anyadditional 
systems or materials on the screen. The thin­ness of the screen is approximately 1 micrometer. Sincewe 
could control the state of the surface, we were able to have several interactions such as piercing a 
.nger through it or enlarging it by using its elasticity and .exi­bility. 3. Application First we developed 
a screen for displaying realisticmaterial. The display s state changes in correspondenceto the images 
the projector shows. Secondly, we developed the plane based 3D screen with three membranes using a single 
projector. (chang­ing frequency 25Hz) The transparency of each mem­brane is controllable by frequency 
of the sound from theparametric speakers. We set the projector and linked it with the transparency of 
each membrane. In addition, we developed polyhedrons made fromthese membranes and displayed several images 
on it. This system shows that this is useful to .exibly display 3D objects. 4. Future Work We introduced 
the .rst prototype of a new kind of dis­play by using colloidal liquids and a method of control­ling 
them by using ultrasonic waves. Due to its thinnessand transparency, the method could be applied to a 
vari­ety of cases in using display technologies. The 3Ds screen and texture screen are exemplary applications 
of this system. Currently our membrane can maintain its surface (screen) for 5 minutes. However, there 
are several solu­tions for this problem and there are rooms for new po­tential colloidal materials for 
the use. REFERENCES [1] M. Hullin et al., Dynamic Display of BRDFs. In: Proceed­ings of Eurographics 
2011. Figure2: ultrasonic &#38; membrane Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 *email: ochyai@me.com 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343459</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Augmented reflection of reality]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343459</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343459</url>
		<abstract>
			<par><![CDATA[<p>To support augmented reality (AR), various display systems have been proposed to mix the real and virtual worlds together. The existing display technologies can be mainly categorized into two classes: video-based mixing and optical combination [Azuma et al. 2001; Bimber and Raskar 2005]. Both technologies have their own advantages and disadvantages. This work falls into the second category. Traditional AR applications focus on interacting with the augmented physical environment surrounding a user. Therefore, although half-silvered mirrors are sometimes used, they mainly serve as see-through displays (i.e., the real world is behind the mirrors).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738771</person_id>
				<author_profile_id><![CDATA[81504683284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wing]]></first_name>
				<middle_name><![CDATA[Ho Andy]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[City University of Hong Kong]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[andy@onthewings.net]]></email_address>
			</au>
			<au>
				<person_id>P3738772</person_id>
				<author_profile_id><![CDATA[81547063756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hongbo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[City University of Hong Kong]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fuplus@gmail.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>618862</ref_obj_id>
				<ref_obj_pid>616073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Azuma, R. and Baillot, Y. and Behringer, R. and Feiner, S. and Julier, S. and MacIntyre, B. 2001. Recent advances in augmented reality. <i>Computer Graphics and Applications</i> 21, 6, 34--47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bimber, O. and Raskar, R. 2005. <i>Spatial Augmented Reality</i>. AK Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2154298</ref_obj_id>
				<ref_obj_pid>2154273</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fujinami, K. and Kawsar, F. and Nakajima, T. 2005. Awaremirror: A personalized display using a mirror. <i>Pervasive Computing</i>, 137--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Infiniti Interactive Mirrors, http://www.interactivemirror.net/]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Augmented Reflection of Reality http://augmented-mirror.onthewings.net/ Wing Ho Andy LI Hongbo FU City 
University of Hong Kong City University of Hong Kong andy@onthewings.net fuplus@gmail.com Figure 1: 
With our system for augmented reflection of reality, the user is able to interact with virtual objects 
(e.g., playing virtual drums) in a mixed world through optical combination between optical reflection 
of the user and rear-projected synthetic data. (Left) Installation design. (Middle) The augmented reflection 
from the user s perspective. (Right) Drum playing in action (from the third-person point of view). To 
support augmented reality (AR), various display systems have been proposed to mix the real and virtual 
worlds together. The existing display technologies can be mainly categorizedinto two classes: video-based 
mixing and optical combination [Azuma et al. 2001; Bimber and Raskar 2005]. Both technologies have their 
own advantages and disadvantages. This work falls into the second category. Traditional AR applications 
focus on interacting with the augmented physical environment surrounding a user. Therefore, although 
half-silvered mirrors are sometimes used, they mainly serve as see-through displays (i.e., the real world 
is behind the mirrors). The reflective nature of half-silvered mirrors has been limitedly explored in 
several interactive applications [Fujinami et al. 2005; Infiniti]. By displaying rear-projected content 
onto theback side of a half-silvered mirror, those applications let users have unusual sensation of seeing 
their reflection and the projected content simultaneously. However, those systems largely support touch-based 
interactivity only and the interaction with digital content is thus limited to the surface of the mirror. 
Inspired by the above technologies, this work introduces the concept of augmented reflection of reality 
for user-centered interactivity. A half-silvered mirror is used to provide a mixed world through optical 
combination between the reflection of the user (standing in front of the mirror) and rear-projected synthetic 
data (Figure 1). In other words, compared to traditional AR scenarios, the real and virtual worlds here 
are swapped with respect to the mirror. With a live and direct view of the user himself/herself and the 
surrounding environment,our system allows the user to intuitively control virtual objects (e.g., playing 
virtual drums) via such augmented reflection. To properly align virtual objects with the user in the 
reflection, the Kinect system of Microsoft is employed. The Kinect systemenables users to control and 
interact with the digital world through a natural user interface using gestures. It has been extensively 
used to achieve user-centered interactivity but typically based on video-based mixing, which significantly 
limits the richness of interaction experience given the low resolution of the Kinect video camera (640 
x 480 pixels). Instead, since the real world is already naturally reflected in the mirror, our work employs 
the Kinect only as a tool for capturing the position and movement of the user instead of the real-world 
scene. We demonstrate the usefulness of our system using an application, we call Air Drum, where the 
user standing in front of the mirror controls and plays a set of virtual drums via the augmented reflection. 
The mirror enables the natural fusion of the virtual and real worlds, serving as a window through which 
we see the virtual world. In this application, the Kinect is mounted on the ceiling, since we care more 
about the positionof the drumsticks in the horizontal plane instead of the full-body motion of the user. 
Such setup allows the user to stand closely to the mirror and to carefully observe his/herperformance 
through the augmented reflection. Please see the accompanying video for demonstration. Our system can 
be applied to scenarios other than musical instrument practice or performance. One idea is to demonstrate 
usage of wearable equipment. Users can put on equipment in front of our system where visual guideline 
can be displayed.Additional check on the usage of equipment can also be done and user will be alerted 
if necessary. Another idea is to use oursystem as a special tool for psychology session, where virtual 
objects (e.g. faces with emotion) or scenes (e.g. diving into ocean) can be displayed to assist a process 
called mirror meditation. References Azuma, R. and Baillot, Y. and Behringer, R. and Feiner, S. and 
Julier, S. and MacIntyre, B. 2001. Recent advances in augmented reality. Computer Graphics and Applications 
21, 6, 34-47. Bimber, O. and Raskar, R. 2005. Spatial Augmented Reality. AK Peters. Fujinami, K. and 
Kawsar, F. and Nakajima, T. 2005. Awaremirror: A personalized display using a mirror. Pervasive Computing, 
137-150. Infiniti Interactive Mirrors, http://www.interactivemirror.net/ Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343460</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Botanicus Interacticus]]></title>
		<subtitle><![CDATA[interactive plants technology]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343460</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343460</url>
		<abstract>
			<par><![CDATA[<p><i>Botanicus Interacticus</i> is a technology for designing highly expressive interactive plants, both <i>living</i> and <i>artificial</i>. We are motivated by the rapid fusion of computing and our dwelling spaces, as well as the increasingly tactile and gestural nature of our interactions with digital devices. Today, however, this interaction happens either on the touch screens of tablet computers and smart phones, or in free air, captured by camera-based devices, such as the Kinect. What if, instead of this limited range of devices, a broad variety of objects in living, social and working spaces become aware and responsive to human presence, touch and gesture?</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738773</person_id>
				<author_profile_id><![CDATA[81100593278]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poupyrev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738774</person_id>
				<author_profile_id><![CDATA[81479664920]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schoessler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of the Arts Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738775</person_id>
				<author_profile_id><![CDATA[81504685003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Studio NAND, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738776</person_id>
				<author_profile_id><![CDATA[81421593159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Munehiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sommerer, C. and Mignonneau, L. <i>The interactive plant growing</i>, 1993: ARS Electronica.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lasserre, G. and A. Ancxt, <i>Akousmaflore: Sensitive and interactive musical plants</i>, 2007: www.scenocosme.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2207743</ref_obj_id>
				<ref_obj_pid>2207676</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sato, M., Poupyrev, I., <i>et al</i>. Touch&#233;: <i>Enhancing touch interaction on humans, screens, liquids, and everyday objects</i>. In CHI'2012. ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 BOTANICUS INTERACTICUS: Interactive Plants Technology Ivan Poupyrev 1, Philipp Schoessler 2, Jonas Loh 
3, Munehiko Sato 4 1 Disney Research Pittsburgh, USA 2 University of the Arts Berlin, Germany 3 Studio 
NAND, Germany 4 University of Tokyo, Japan 1. Introduction Botanicus Interacticus is a technology for 
designing highly ex­pressive interactive plants, both living and artificial. We are moti­vated by the 
rapid fusion of computing and our dwelling spaces, as well as the increasingly tactile and gestural nature 
of our interac­tions with digital devices. Today, however, this interaction hap­pens either on the touch 
screens of tablet computers and smart phones, or in free air, captured by camera-based devices, such 
as the Kinect. What if, instead of this limited range of devices, a broad variety of objects in living, 
social and working spaces be­come aware and responsive to human presence, touch and gesture? 2. Interactive 
Plants, Living and Artificial Botanicus Interacticus is an interactive plants technology. A num­ber of 
unique properties set it apart from previous projects [1, 2]. Simple and ad-hoc instrumentation. Enhancing 
plants with interac­tivity is simple, non-invasive and does not damage the plants all that is needed 
is a single wire placed anywhere in the plant soil.Furthermore, the technology does not require instrumenting 
either the environment, or the users, which allows creating truly ad-hoc plant-based user interfaces 
Rich set of gestures on plants. Plants instrumented with our tech­nology respond to a rich set of gestures, 
e.g. sliding fingers on a stem, discriminating touched leaves, determining level of the user proximity 
and the amount of touch. This large range of gestures makes many applications possible with Botanicus 
Interacticus. Precise recognition. We use machine-learning techniques for pre­cise and unambiguous recognition 
of gestures on plants. Therefore, plants can be used to issue commands, such as play a musical note, 
select a date on the calendar, or flip channels on TV. Unique interactive character. Structure and physical 
properties of each plant species exhibit unique interaction constraints and af­fordances. In other words, 
for each type of plant certain gestures are more natural then others, e.g. an orchid invites users to 
slide fingers along it s stem, while a gardenia suggests unstructured, playful interaction (Figure 1). 
These physical, tangible propertiesof plants map naturally into the control variables that we capture 
using our sensing technology. Thus, each kind of plant species has a unique and specific interactive 
character. Real and artificial plants. The sensing approach used in our tech­nology treats plants as 
an electrical circuit that can be modeled and replicated with standard electrical components. This allows 
us to design a broad variety of biologically inspired artificial plants that would behave nearly exactly 
same as their biological counterparts. From the point of view of our sensor there would be no difference 
between real and artificial plants: they would all represent the same plant specie: Botanicus Interacticus. 
A possibility to create both real and artificial interactive plants using the same technology allows 
to designing environments where artificial plants can beused where real plants are not be appropriate, 
without changes to sensing technology, infrastructure, interfaces or applications. 3. Sensing Technology. 
Botanicus Interacticus uses the recently developed Swept Fre­quency Capacitive Sensing technology [3]. 
Previous capacitive sensing techniques measure response to touch by exciting the tar­get objects with 
an electrical signal at a single frequency. We ex­cite plants at multiple frequencies, by sweeping them 
through a range between 0.1 and 3 Mhz. Because the path of the electrical signal inside the plant varies 
with frequency, we can estimate touch locations by observing the frequencies at which the signal was 
affected by user touch. Since the plant has a complex and dynamic electrical structure, we use machine-learning 
techniques to recognize gestures on plants reliably and with high precision. 4. Applications of Interactive 
Plants Technology Although there are many applications of interactive plants tech­nology, we are currently 
focusing on experiential, entertainment and aesthetic uses. We are particularly interested in technologies 
that would encourage children and adults to move and engage with surrounding physical environments and 
each other, rather than with personal digital devices such as smart phones. This can be achieved by enhancing 
living, working and social spaces to make them responsive, intelligent and adaptive. Plants represent 
a natural platform for such enhancement. Indeed, they already have a special place in our dwelling, serving 
as a decoration and living companions that we nurture and care for. Giving plants a voice, a possibility 
to respond and engage us would lead to new forms of entertainment, enhance our lifestyles and form a 
new environmental computational platform that can be used both for education and entertainment. 5. Siggraph 
Demonstration At the SIGGRAPH Emerging Technology we will demonstrate a garden composed of real and artificial 
interactive plants. Visitors will be interacting with them by touch and observing their visual and audio 
responses (Figure 2). Each plant would recognize a unique set of gestures and, similarly to popular virtual 
pet comput­er games, the more visitors engage with plants the more complex and rich visuals and audio 
would become. Engaging with more painful plants, such as cactuses, would provide far more rewarding experiences. 
The visitors will be encouraged to go through all levels of plants responses and the entire experience 
would take between 2 to 3 minutes for each visitor. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343461</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Chilly chair]]></title>
		<subtitle><![CDATA[facilitating an emotional feeling with artificial piloerection]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343461</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343461</url>
		<abstract>
			<par><![CDATA[<p>In audio-visual entertainment such as listening to music, game playing, and viewing movies, people frequently seek a richer experience. However the improvement in experience that can be obtained by improving audio and visual quality is reaching its limit.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738777</person_id>
				<author_profile_id><![CDATA[81350575122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shogo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukushima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Chofugaoka Chofu, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shogo@kaji-lab.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738778</person_id>
				<author_profile_id><![CDATA[81100131163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Chofugaoka Chofu, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kajimoto@hc.uec.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1550336</ref_obj_id>
				<ref_obj_pid>1549825</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lemmens, P. et al., A body-conforming tactile jacket to enrich movie viewing. In Proc. <i>World Haptics '09</i>, pages 7--12. 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1754121</ref_obj_id>
				<ref_obj_pid>1753846</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Israr, A. et al., Exploring surround haptics displays. In Proceedings of <i>CHI '10 (Ex-tended Abstracts)</i>, ACM, pp.4171--4176, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Antonio R. Damasio. et al., Subcortical and cortical brain activity during the feeling of self-generated emotions. Nature Neuroscience, 3, pp1049--1056, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2160133</ref_obj_id>
				<ref_obj_pid>2160125</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Fukushima, S. et al., Facilitating a Surprise Feeling by Artificial Control of Piloerection on the Forearm, To appear in 3rd Augmented Human international conference, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Chilly Chair: Facilitating an Emotional Feeling with Artificial Piloerection C:\Users\Shogo\Dropbox\.....\2011..\..........\siggraph2012\pictures\principle.jpg 
C:\Users\Shogo\Dropbox\.....\2011..\..........\..·.\20110919\DSC00952.JPG C:\Users\Shogo\Dropbox\.....\2011..\..........\siggraph2012\pictures\Chilly 
Chair2.jpg C:\Users\Shogo\Dropbox\.....\2011..\..........\siggraph2012\pictures\hayward1.jpg (a) (b) 
 (c) (d)  Shogo Fukushima The University of Electro-Communications JSPS Research Fellow 1-5-1 Chofugaoka 
Chofu, TOKYO, JAPAN shogo@kaji-lab.jp Hiroyuki Kajimoto The University of Electro-Communications Japan 
Science and Technology Agency 1-5-1 Chofugaoka Chofu, TOKYO, JAPAN kajimoto@hc.uec.ac.jp Figure 1. (a)Principle 
of a prototype device, (b)Forearm hair behavior with electrostatic force, (c) Schematic view of the Chilly 
Chair, (d) Surprised expression on a participant when both audio stimulation and piloerection were presented. 
 1. Introduction In audio-visual entertainment such as listening to music, game playing, and viewing 
movies, people frequently seek a richer experience. However the improvement in experience that can be 
obtained by improving audio and visual quality is reaching its limit. Adding haptic stimulation to entertainment 
is a promising step to enrich these experiences and has been attempted in a number of studies such as 
in a vibration headphone [Koss Corporation], vibration-embedded chair [Israr et al. 2010], and jacket-shaped 
tactile display [Lemmens et al. 2009]. These tactile interfaces improve the reality of entertainment 
by transferring various cues from the contents onto the body. In contrast, we present a novel approach 
that directly facilitates the emotional feeling evoked by the content, using haptic technologies. We 
speculated that the quantity of emotional feeling evoked by the content is one of the most critical factors 
in determining the quality of the entertainment. To enhance the emotional feeling with haptic technologies, 
we focus on piloerection, which is a type of involuntary emotional reaction. According to James Lange 
theory, emotional feeling is experienced as a result of physiological change in the body induced by the 
autonomic nervous system. Damasio et al. also predicted that emotional feeling is evoked by the insula 
cortex that represents particular bodily reactions such as the sensations of butterflies in the stomach 
and goose bumps [Damasio et al. 2000]. Our hypothesis is that not only is piloerection an emotional reaction 
, but it can also work as an emotional input that enhances the emotional feeling itself. 2. Principle 
and verification We developed a prototype device that raises forearm hairs using an electrostatic force 
(Figure 1(a)). An acrylic plate with a copper electrode and a crude rubber sheet attached was bent along 
the forearm. The electrode was connected to a high voltage source (HJPQ-30P1, Matsusada Precision Inc), 
and the forearm was connected to the ground. When a high voltage (0~20 kV) is applied to the electrode, 
the acrylic plate is polarized and the forearm hair is attracted to the acrylic plate by the electrostatic 
force (Figure 1(b)). We carried out a psychophysical experiment with the prototype device, to verify 
our hypothesis that a feeling of surprise (one of the emotions that relate to piloerection) was enhanced 
by piloerection of the participant s body hair synchronized with the subjective feeling of surprise. 
To surprise the participants, we used an audio warning alarm. Two conditions were prepared: one with 
only audio stimulation, and the other with both audio stimulation and piloerection. We compared the two 
conditions by means of a questionnaire, asking the participant to quantify the amount of surprise they 
felt, and by observing the skin conductance reaction (SCR), which is known to vary with the activation 
of the sympathetic nervous system. From the results, both the amount of subjective surprise and the SCR 
value increased significantly when piloerection was added to the audio emotional stimulation [Fukushima 
et al. 2012]. 3. Chilly Chair Based on these observations, we constructed the Chilly Chair , a chair 
type piloerection control system that raises the hairs on the back and forearms (Figure 1(c)). Contrary 
to our previous experiments, we first measured the SCR value and controlled the piloerection accordingly. 
Consequently we established that the Chilly Chair can be applied not only to audio-visual entertainment, 
but also to non-computational entertainment such as reading books and dreaming while asleep. Reference 
 LEMMENS, P. et al., A body-conforming tactile jacket to enrich movie viewing. In Proc. World Haptics 
09, pages 7-12. 2009. ISRAR, A. et al., Exploring surround haptics displays. In Proceedings of CHI 10 
(Ex-tended Abstracts), ACM, pp.4171-4176, 2010. ANTONIO R. DAMASIO. et al., Subcortical and cortical 
brain activity during the feeling of self-generated emotions. Nature Neuroscience, 3, pp1049-1056, 2000. 
FUKUSHIMA, S. et al., Facilitating a Surprise Feeling by Artificial Control of Piloerection on the Forearm, 
To appear in 3rd Augmented Human international conference, 2012. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343462</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[ClaytricSurface]]></title>
		<subtitle><![CDATA[an interactive surface with dynamic softness control capability]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343462</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343462</url>
		<abstract>
			<par><![CDATA[<p>For a long time, solid planar surfaces have been commonly used for computer displays. Recently, there have been many attempts to provide a flexible element with the display surface to extend possibilities of interaction by utilizing many flexible materials, such as elastomer, sand or clay [Piper et al. 2002]. However, in many traditional flexible display to date, this softness element has been considered as a static element and thus unchangeable. Thus, traditional flexible surface limits the possible interaction that is supported on each surface due to the interactive element being strongly dependent on the physical/chemical properties of display material.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738779</person_id>
				<author_profile_id><![CDATA[81490686533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matoba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications (UEC Tokyo)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[matoba@vogue.is.uec.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738780</person_id>
				<author_profile_id><![CDATA[81421592614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications (UEC Tokyo)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[den@atsuage.net]]></email_address>
			</au>
			<au>
				<person_id>P3738781</person_id>
				<author_profile_id><![CDATA[81502661661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nobuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications (UEC Tokyo)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[novriki@acm.org]]></email_address>
			</au>
			<au>
				<person_id>P3738782</person_id>
				<author_profile_id><![CDATA[81504682541]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hideki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications (UEC Tokyo)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[koike@acm.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>503439</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Piper, B., Ratti, C., and Ishii, H. 2002. Illuminating clay: a 3-d tangible interface for landscape analysis. In <i>Proc. of the SIGCHI conference on Human factors in computing systems</i>, ACM, New York, NY, USA, CHI '02, 355--362.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ClaytricSurface:An Interactive Surface with Dynamic Softness Control Capability  Yasushi Matoba* Toshiki 
SatoNobuhiroTakahashi HidekiKoike § The Universityof Electro-Communications (UECTokyo) Figure 1: Surface 
in a Soft State(left) and Hard State(right) Figure 2: System Hardware Figure 3: Pen-based Interaction 
1 INTRODUCTION For a long time, solid planar surfaces have been commonly used for computer displays. 
Recently, there have been manyattempts to provide a .exible element with the display surface to extend 
pos­sibilities of interaction by utilizing many .exible materials, such as elastomer, sand or clay [Piper 
et al. 2002]. However, in many traditional .exible display to date, this softness element has been considered 
as a static element and thus unchangeable. Thus, tra­ditional .exible surface limits the possible interaction 
that is sup­ported on each surface due to the interactive element being strongly dependent on the physical/chemical 
properties of display material. In order to address this problem, we proposed the idea of Dy­namic Flexibility 
for the display medium and developed a novel interactivesurface with the capability of dynamic .exibility 
control. The main features of this surface are as follows: First, the user can use the display as both 
a traditional rigid/planar display and also a .exible/non-planar display. Second, the surface of the 
display is able to present the gradual dynamic translation of its properties from soft to hard and vice 
versa. In addition, the user or the system can quickly switch the surface softness state at anytime. 
 2 DYNAMIC SOFTNESS CONTROL In order to develop the surface that has such dynamically chang­ing softness 
properties, we looked into the application of small particle materials together with a particle density 
control tech­nique. Polystyrene particles exhibit liquid-like behavior due to its lightweight and low 
friction characteristics. However, if you seal these particlesintoanairtightplasticbagandextracttheairfromthe 
bag, the particles trapped within are compressed and the bag will gradually harden. If the pressure approaches 
vacuum pressure lev­els, the surface container (the plastic bag) will become completely hard. Based on 
this technique, we developed an tabletop system named ClaytricSurface (Figure.2). Figure4 shows the relationship 
be­tween internal particle density and softness of the surface. At .rst, when the pressure of the table 
is at atmospheric pressure, the sur­face behaves asasoft surface. In this state, the surface provides 
soft touch sensation likesand or clothing, and enables the user to change the shape easily with simple 
hand manipulation or with tools. Once *e-mail:matoba@vogue.is.uec.ac.jp e-mail:den@atsuage.net e-mail:novriki@acm.org 
§e-mail:koike@acm.org the pressure starts to decrease by the vacuum pump, the surface gradually becomes 
harder. In the median state, the user can de­form the surface with his/her hands like clay(Figure.1(left)). 
Fur­thermore, once the internal pressure approaches the maximum level determinedbythedegreeofvacuum,thesurface 
becomes hardened and its current shape is .xed(Figure.1(right)). Figure 4: Particle Density Control Using 
Pressure Manipulation 3 APPLICATIONS Firstly, we have developed a novel 2.5-dimensional modeling ap­plication. 
The user can forma 2.5D modelby usingvarietyof hand movement such as gathering with both hands or pulling 
and tugging while experiencing the touch sensation of a dynamically­changing material. In this application, 
user stouch input is detected by the depth camera above the table and the surface softness can be controlledbya 
simple GUI slider orbutton. Additionally, once the model is in a .xed state, he/she can instantaneously 
paint a tex­ture with direct touch input.We alsodevelopeda handy modeling support device by using a vacuum 
forming technique that enables the user to easily create a pre-prepared 2.5D shape. Furthermore, we developed 
a entertainment application that allows user interac­tion with characters that he/she has made via modeling 
application through direct touch input. We have also developed another application augments pen-based 
interaction. In this application, the user s pen input is detected by electromagnetic .elds produced 
from under the table. The user can dynamically change the shape of the surface and tactile sensation, 
moreover, adjust the friction resistance between pen tip and the sur­face(Figure.3). References PIPER, 
B., RATTI, C., AND ISHII, H. 2002. Illuminating clay: a 3-d tangible interface for landscape analysis. 
In Proc. of the SIGCHI conference on Human factors in computing systems, ACM,NewYork,NY, USA, CHI 02, 
355 362. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 
 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343463</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Combiform]]></title>
		<subtitle><![CDATA[a combinable social gaming platform]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343463</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343463</url>
		<abstract>
			<par><![CDATA[<p>Combiform [see Fig. 1 <i>center</i>, video: 4-1] is a novel digital gaming console featuring four combinable handheld controllers. In particular, multiple players may freely combine and lock together their handheld game controllers, thereby creating a very flexible collective and transformable tangible interface. It can be understood as an emerging gaming system attempting to expand the co-located (the same confined physical space) social play experiences introduced by platforms such as <i>Nintendo Wii, Microsoft Kinect</i> and <i>Hasbro Bop-it</i>. Most co-located video games direct players' attention to the screen but not to the other players. This arrangement counter-acts the natural affordances required for social interaction [1]. Combiform is especially designed to drastically improve co-attentiveness during co-located play. The system and its 10 games emphasize tangible body-to-body interaction via a flexible combining and decombining mechanism. This new interactive technique affords an exceptionally strong re-enforcement for player to pay close attention to other players. In addition to significantly improving focus attention, players could now feel and direct each other's movements and gestures via these tangible links between controllers [video: 4-2].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738783</person_id>
				<author_profile_id><![CDATA[81502662470]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Edmond]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[edmond.yee@usc.edu]]></email_address>
			</au>
			<au>
				<person_id>P3738784</person_id>
				<author_profile_id><![CDATA[81502660838]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Josh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738785</person_id>
				<author_profile_id><![CDATA[81502733855]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738786</person_id>
				<author_profile_id><![CDATA[81504683585]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uehara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[de Kort, Y. et al., 2007. People, places and play: A research framework for digital game experience in a socio-spatial context. DiGRA 2007 Proceedings "Situated Play"]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1462519</ref_obj_id>
				<ref_obj_pid>1462505</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gajadhar, B. et al., 2008. Shared fun is doubled fun: player enjoyment as a function of social setting. In P. Markopoulos, B. de Ruyter, W. IJsselsteijn, &amp; D. Rowland, Fun and Games (pp. 106--117). New York: Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Vanden Abeele, V. et al., 2009. Gaming Naturally is more Fun Together: the Influence of Controller Type on Player Experience. ACE 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Video Links]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Promotional Video http://youtu.be/r92cDygrDiM?hd=1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Blow-it up http://youtu.be/aGZG5hGJuDw?hd=1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Switch http://youtu.be/L94odcsVWHI?hd=1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Match! http://youtu.be/U5QS-e2G50M?hd=1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[T.A.I http://youtu.be/I2BNBu7aMYI]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Pop Quiz http://youtu.be/tV_HWMqIenI]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Combiform: a Combinable Social Gaming Platform Edmond Yee*, Josh Joiner, Andrew Dang, Andy Uehara University 
of Southern California, Interactive Media Division *edmond.yee@usc.edu *M.F.A. Dissertation    1. 
Introduction Combiform [see Fig. 1 center, video: 4-1] is a novel digital gaming console featuring four 
combinable handheld controllers. In particular, multiple players may freely combine and lock together 
their handheld game controllers, thereby creating a very flexible collective and transformable tangible 
interface. It can be understood as an emerging gaming system attempting to expand the co-located (the 
same confined physical space) social play experiences introduced by platforms such as Nintendo Wii, Microsoft 
Kinect and Hasbro Bop-it. Most co-located video games direct players attention to the screen but not 
to the other players. This arrangement counter-acts the natural affordances required for social interaction 
[1]. Combiform is especially designed to drastically improve co-attentiveness during co-located play. 
The system and its 10 games emphasize tangible body-to-body interaction via a flexible combining and 
decombining mechanism. This new interactive technique affords an exceptionally strong re-enforcement 
for player to pay close attention to other players. In addition to significantly improving focus attention, 
players could now feel and direct each other s movements and gestures via these tangible links between 
controllers [video: 4-2]. 290550_311060105590658_238890762807593_1212766_2143050131_o IMGP7389 339848_311054075591261_238890762807593_1212719_1051214535_o 
   Figure 1. left: joint gesture control, center: Combiform controllers right: players bodies facing 
each other while combining  2. From Co-located to Tangible Co-attentive play co-located-coattentive-tangible-co-attentive 
Gajadhar. et. al. [2] found that co-located co-player settings significantly add to the fun, challenge, 
and perceived competence in gaming compared to both meditated (online play) and virtual co-play (single 
player mode). However, there are many factors that could influence players excitement level even if playing 
against human players in the same confined space. Interpersonal distance, body orientation, physical 
interaction, gesture, verbal communications, facial expression, etc. are all important factors in determining 
the level of social engagement [1].These could be summarized as a form of attention between players, 
in which we call co-attentive play (see Fig. 2). A recent study has shown that natural-mapping interfaces 
used in system such as Nintendo Wii and Microsoft Kinect improve player co-attentiveness during play 
[3]. Combiform attempts to improve co-attentiveness from a new perspective. Players are required to actively 
choose to attach and detach their interfaces in some games; thus, they must pay close attention to others 
while playing. Since players have to move closer and away from each other for combining, interpersonal 
distances are dynamically changing as illustrated in [video: 4-5]. Players are required to move their 
arm (or even jump) together after they have combined, achieving a natural-mapping synchronized interface 
that draws face-to-face and body-to-body attentions among players. It also amplifies players gestures 
which enhance social presence in gaming [3, video: 4-2]. One very unique aspect of Combiform not found 
in any other previous work is the tangibility it possesses. Combiform creates a physical link among players 
to directly feel each others gestures and movements. This opens up possibilities for directing other 
players movements during play. Communication during play can even be done via this tangible link in non-verbal 
form [4-3]. The combining mechanic also affords players to tap into other people s play space in competitive 
games. Attaching and detaching the controllers is a highly visible action for all players. This visibility 
of movements increases possibilities for immediate physical response from other players as seen in [video: 
4-5]. These emerging actions can be interpreted as tangible means of augmenting digital gaming through 
the introduction of the flexible attaching and detaching interface. 3. Hardware Design and the Affordances 
Combiform s configuration (see Fig. 1 center) is especially designed to encourage players to pay close 
attentions to others rather than to the screen. When all four controllers are combined, the players bodies 
face each other (see Fig. 1 right). The neodymium magnets provide about 10lb of perpendicular pull force. 
In addition to the attaching mechanism, all assets of the controller are physically enlarged and simplified 
to maximize direct visibility and transparency of in-game actions. Detailed information about hardware 
design could be found in the Supplementary Text Document. 4. Game Design: Enriching Co-attentive Play 
We would highly encourage readers to view the videos online along with the description of the games in 
the Supplementary Text Document. We are presenting 10 different game experiences that range from purely 
active social-fun (e.g. Blow-It Up) [4-2], to full-body twisting challenge (e.g. Switch) [4-3], to board-game-like 
social experiences (e.g. Match!) [4-4], to serious games for improving Mathematics skills (e.g. Pop Quiz) 
[4-6]. Note that all of these games can be mapped in a continuum of social presence that could lay a 
foundation for later studies. References [1] de Kort, Y. et al., 2007. People, places and play: A research 
framework for digital game experience in a socio-spatial context. DiGRA 2007 Proceedings "Situated Play" 
[2] Gajadhar, B. et al., 2008. Shared fun is doubled fun: player enjoyment as a function of social setting. 
In P. Markopoulos, B. de Ruyter, W. IJsselsteijn, &#38; D. Rowland, Fun and Games (pp. 106-117). New 
York: Springer. [3] Vanden Abeele, V. et al., 2009. Gaming Naturally is more Fun Together: the Influence 
of Controller Type on Player Experience. ACE 2009 [4] Video Links [4-1] Promotional Video http://youtu.be/r92cDygrDiM?hd=1 
[4-2] Blow-it up http://youtu.be/aGZG5hGJuDw?hd=1 [4-3] Switch http://youtu.be/L94odcsVWHI?hd=1 [4-4] 
Match! http://youtu.be/U5QS-e2G50M?hd=1 [4-5] T.A.I http://youtu.be/I2BNBu7aMYI [4-6] Pop Quiz http://youtu.be/tV_HWMqIenI 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343464</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[<b>Drum On</b>]]></title>
		<subtitle><![CDATA[interactive personal instrument learning system]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343464</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343464</url>
		<abstract>
			<par><![CDATA[<p>Drum On is a prototype system to enhance personal instrument practice that may conventionally create boredom and limitation. The MIDI signal generating from the electrical drum is handled by a computer and each animating image interacts with this signal. The images displayed on the drum by a projector are animated to directly give rhythm cues for the drum player. The animations subsequently react with proper hit timing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738787</person_id>
				<author_profile_id><![CDATA[81504683482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaehyuck]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bae]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jaehyuck.bae@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P3738788</person_id>
				<author_profile_id><![CDATA[81504684130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Byungjoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bjlee@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3738789</person_id>
				<author_profile_id><![CDATA[81504684716]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sungmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sungmins@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3738790</person_id>
				<author_profile_id><![CDATA[81421594806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yunsil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yunsil@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3738791</person_id>
				<author_profile_id><![CDATA[81421599972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hyunwoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[savoy@snu.ac.kr]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Drum On : Interactive Personal Instrument Learning System   Jaehyuck Bae*, Byungjoo Lee*, Sungmin 
Cho*, Yunsil Heo** and Hyunwoo Bang* *New Media Lab., School of Mechanical &#38; Aerospace Engineering, 
Seoul National University **Department of Crafts &#38; Design, School of Arts, Seoul National University 
   C:\Users\jaehyuck\Desktop\siggraph2012\..4.JPG C:\Users\jaehyuck\Desktop\siggraph2012\..3.JPG 
    Figure 1.left:before hitting, right:hitting moment    Figure 2.install and performing situation 
 1. Introduction Drum On is a prototype system to enhance personal instrument practice that may conventionally 
create boredom and limitation. The MIDI signal generating from the electrical drum is handled by a computer 
and each animating image interacts with this signal. The images displayed on the drum by a projector 
are animated to directly give rhythm cues for the drum player. The animations subsequently react with 
proper hit timing. 2. Setup To detect exact hit signal, we use an electrical drum. A short throw projector 
is positioned at a proper height to cover all of the drum kits projecting with the object mapping method 
on the drum. The animating effect of playing on the drum kit informs the hit timing to the users and 
the MIDI signal generated by playing is transmitted to the computer. Sequentially, the computer can detect 
correct hit timing from this signal and the player is given feedback through the interacting animation. 
To play smooth animation for eight drum kits, all the image effects are generated by the GPU process. 
3. Interaction The system has three modes of play and practice. Beginner mode: practice basic drum bit 
sequences categorized by three steps (easy, medium, hard). Practice mode: practice with background music. 
Free mode: play the drum freely with interactive images on the drum. With the first two modes for educational 
purposes, the user would become familiar with basic drum beat hitting for the drum kits when the moving 
animation is close to the red circular area at the center point. After finishing one play loop, the player 
can check his performance score. One problem is that the kick drum is located at the bottom where it 
cannot be projected by the projector. As a result, the animation for the kick drum projects on the player 
s knee, which plays the kick drum. The user still gets visual feedback directly from the animating images. 
Practice mode with background music is implemented by BMS (Be Music Source) to generate correct hitting 
timing with background music. It also makes the users feel as if they are performing one piece of music. 
In free mode, the player is able to play drums with various interactive visual effects on the drum for 
entertainment value. 4. Conclusion Drum On provides not only effective practice but also affordable playing 
enjoyment. Generally, most rhythmical instrument practices are based on paper note or independent display. 
This leads to diffused sight and interruption in the direct connection between instrument and note, which 
consequently results in incorrect playing posture as well as boredom. This system provides exact drum 
bit directly while the drummer's eyes are fixed on the drum kit. The player can practice efficiently 
while maintaining the right posture while playing. In addition, the gaming factor and interactive animation 
relieves the tediousness of personal drum practice. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343465</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Gocen]]></title>
		<subtitle><![CDATA[a handwritten notational interface for musical performance and learning music]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343465</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343465</url>
		<abstract>
			<par><![CDATA[<p>Not only in childhood but also adulthood, we need some trainings to read music scores, which sometimes make music hard to learn and enjoy. In this article, we shall propose the system that enables users to play their handwritten musical notations by our musical interface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738792</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan Univeristy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tetsuaki.baba@acm.org]]></email_address>
			</au>
			<au>
				<person_id>P3738793</person_id>
				<author_profile_id><![CDATA[81488673349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan Univeristy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738794</person_id>
				<author_profile_id><![CDATA[81504687300]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoshiike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan Univeristy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738795</person_id>
				<author_profile_id><![CDATA[81442606589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tatsuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan Univeristy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738796</person_id>
				<author_profile_id><![CDATA[81504688027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Rika]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shoji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan Univeristy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738797</person_id>
				<author_profile_id><![CDATA[81100274861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan Univeristy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738798</person_id>
				<author_profile_id><![CDATA[81504684281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Corporation Co., Ltd]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2047249</ref_obj_id>
				<ref_obj_pid>2047196</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yamamoto, Y., Uchiyama, H., and Kakehi, Y. 2011. on-note: playing printed music scores as a musical instrument. In <i>Proceedings of the 24th annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, UIST '11, 413--422.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Gocen: a Handwritten Notational Interface for Musical Performance and Learning Music Tetsuaki Baba 
*,Yuya Kikukawa ,ToshikiYoshiike ,Tatsuhiko Suzuki , Rika Shoji ,KumikoKushiyama , Makoto Aoki Graduate 
Schoolof Design,Tokyo Metropolitan Univeristy, With Corporation Co.,Ltd  Figure 1: A sketch of the 
system 1 Introduction Not only in childhoodbut also adulthood, we need some trainings to read music 
scores, which sometimes make music hard to learn and enjoy. In this article, we shall propose the system 
that enables users to play their handwritten musical notations by our musical interface. Since 1960s, 
Optical Music Recognition (OMR) has become ma­turedinthe .eldof printed score. In recent,Yamamoto[Yamamoto 
et al. 2011]proposed musical interactive system that directly uti­lizes printed music score asa instrument 
withkeypoints matching. However, few researches on handwritten notations have been done, as well as on 
interactive system for OMR.We combined notating with performing in order to make the music more intuitive 
for users and give aids to learn music to users. 2 System Our system mainly consists of a scan device, 
computer and sound module. A user can play simple music by tracing notes with the scandevice as shownin 
.gure1. The computer processes captured images by using openCV and our algorithm at 30fps, then outputs 
sounds according to the data from the notations. We do not need any special materials other than this 
system. A user can use nor­mal white paper and his/herown pen. Ourdeviceisbuilt witha USB camera, microcontroller, 
and vibration motor( see .gure2). The vibration motor is used for tactile feedback while the user is 
playing. 3 Interactions Our system is not only an OMR systembut also performance sys­tem. We developed 
several musical interactions for this interface. Note on/off A user can make a sound by getting the green 
bar onthe computer display(see .gure3)to pass througha simpli.ed *e-mail:tetsuaki.baba@acm.org Figure 
3: A captured and processed image. note, while pressing manual playbutton . Control velocity(volume) 
Our system can detect the size of a musical notation. It is interpreted as control note velocity. Figure 
3shows the relation between a recognized musical notation of pro­cessed image and its velocity. Pitch 
bend While a user is playing a note, he/she can change the pitch of the note by moving the device vertically 
like a vibrato. Change instrument Auser can change the instrument of sound by covering a text he/she 
prints to indicate the name of instrument, such as pf(piano), bs(bass), gt(guitar), dr(drums), etc., 
with the de­vice,by meansof Optical Character Recognition1. Record and play like an music sequencer Auser 
can record soundevents into timeline, while pressing recordingbutton and make a loop like a sequencer. 
Each recorded note will be set in the quantized timeline. Weare nowdeveloping other devices for more 
sophisticated ensem­ble performance with this system. Furthermore, we are trying to make a more consumer 
oriented device from our system for learn­ing music. References YAMAMOTO, Y., UCHIYAMA, H., AND KAKEHI, 
Y. 2011. on­note: playing printed music scores as a musical instrument. In Proceedings of the 24th annual 
ACM symposium on User inter­face software and technology,ACM,NewYork,NY,USA, UIST 11, 413 422. 1We adapted 
a Ocrad Library-The GNU OCR Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, 
California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343466</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Hand-rewriting]]></title>
		<subtitle><![CDATA[automatic rewriting like natural handwriting]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343466</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343466</url>
		<abstract>
			<par><![CDATA[<p>Approaches for combining handwriting acts and control by computer are gaining force. As two types of general classifications of these approaches, the "pen tablet" and the "digital pen" are often cited. As for the former approach, in addition to being able to display handwritten information, it can display drawing effects and additional information related to that information. Data output by this approach, however, can only be handled by digital-type displays. As for the latter approach, although written information acquired as paper input can be digitized, it is not possible to express additional information as output on paper (i.e., a paper display). In contrast to those two approaches, the system proposed by the authors---called "Handrewriting"---automatically performs "rewrite" processing on paper in correspondence with handwriting using pen and paper. More specifically, with the proposed system, when the user writes on a piece of paper with a pen, for example, the information written on the paper is automatically erased as required, and additional information is displayed on the paper in natural print-like colors.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738799</person_id>
				<author_profile_id><![CDATA[81100411762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738800</person_id>
				<author_profile_id><![CDATA[81504685158]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738801</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2048270</ref_obj_id>
				<ref_obj_pid>2048259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hashida, T., Kakehi, Y., and Naemura, T. 2011. Photochromic Sculpture: Volumetric Color-forming Pixels, <i>In Proceedings of SIGGRAPH2011 Emerging Technologies</i>]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hand-rewriting: Automatic Rewriting like Natural Handwriting Tomoko Hashida KoheiNishimuraTakeshiNaemura 
The University of Tokyo    The University of Tokyo The University of Tokyo   Fig.1: Decorating characters 
by erasing effect Fig.2:Automatic painting by color-forming effect Fig.3:Automatic correction by combination 
of both effect 1. Introduction Approaches for combining handwriting acts and control by computer are 
gaining force. As two types of general classifications of these approaches, the pen tablet and the digital 
pen are often cited. As for the former approach, inaddition to being able to display handwritten information, 
it candisplay drawing effects and additional information related to thatinformation. Data output by this 
approach, however, can only behandled by digital-type displays. As for the latter approach, although 
written information acquired as paper input can be digitized, it is not possible to express additional 
information as output on paper (i.e., a paper display). In contrast to those two approaches, the system 
proposed by the authors called Hand­rewriting automatically performs rewrite processing on paperin correspondence 
with handwriting using pen and paper. Morespecifically, with the proposed system, when the user writes 
on a piece of paper with a pen, for example, the information written on the paper is automatically erased 
as required, and additional information is displayed on the paper in natural print-like colors. 2. Hand-rewriting 
The Hand-rewriting system combines two technical innovations. The first one is a function that automatically 
erasesspecific areas of the paper, without the need to use an eraser,when the user writes letters or 
draws pictures on the paper. Local erasure on the paper in this manner can express letters and pictures 
written in thermochromic ink by means of thermal conversion by laser heating. As a thermochromic ink 
pen, a commercial pen ( FRIXION, Pilot Pen Co., Ltd.) with 24 colors(including black, blue, purple, and 
pink) is used. This pen canerase the color of written letters (i.e., make them colorless andtransparent) 
by means of frictional heat generated by rubbing witha special rubber. In detail, local colors are erased 
by irradiating ablack film coated on the underside of the paper with laser lightfrom a galvanometer scanner 
controlled by a captured image. When it hits the film, the light is converted to heat, and when thetemperature 
reaches about140°F, colors of local areas are erased. The galvanometer scanner can control a laser beam 
to a distanceof 0.068 mm (in the case of letter-size paper). The second innovation is a function that 
can repeatedly displayadditional related information on the paper in color when the userwrites letters 
or sketches pictures on the paper. The local colorson the paper are created by projecting an ultraviolet 
(UV) pattern from a UV projector onto paper coated with photochromic material (PM). As the PM, spiropyran 
(which produces colorunder UV light and returns gradually to colorless and transparentwhen the UV light 
is blocked) is used. In detail, four types of PM were used: blue, purple, pink, and yellow purchased 
fromKIROKUSOZAI Co., Ltd. Each PM requires a different time forproducing and loosing its respective color. 
For the projecting theUV pattern, a UV projector fitted with a digital micromirror (which can illuminate 
in units of pixels) and an invisible (i.e.,365-nm wavelength) light source is used [Hashida et al., 2011]. 
Applying the above-described control technologies, three types ofinteractive applications of the Hand-rewriting 
system have been developed. The first application is focused on controlling erasureof colors. For example, 
as shown in Fig. 1, when the user writes something by hand in the Roman alphabet, this applicationautomatically 
erases parts of the characters, and the effect is totransform the letters into ornamental writing. The 
second application is controlling generation of colors. As shown in Figure2, when the user sketches something 
by hand, this applicationautomatically colors the interior of the outline sketch, and the effect is to 
replicate the sketch itself in the manner of a stamp.The third application utilizes controlling generation 
and erasure ofcolors at the same time. When the user makes a mistake while writing something, this application 
automatically erases the incorrect part and displays a guide giving the correct entry incolor. Using 
these applications, the user could enjoy performingvarious creative activities on the paper by freely 
using both kindsof control (i.e., generation and erasure of colors). 3.Concluding remarks A system by 
which a computer performs automatic rewrite processing on paper by means of temperature control and ultraviolet-light 
control in correspondence with handwritingusing a pen and paper was developed. From now onwards, it isplanned 
to experimentally investigate the possibility of linking this system with other electronic pen devices. 
 References HASHIDA, T., KAKEHI, Y., AND NAEMURA, T. 2011. PhotochromicSculpture: Volumetric Color-forming 
Pixels, In Proceedings of SIGGRAPH2011 Emerging Technologies Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343467</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[HDRchitecture]]></title>
		<subtitle><![CDATA[real-time stereoscopic HDR imaging for extreme dynamic range]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343467</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343467</url>
		<abstract>
			<par><![CDATA[<p>The history of HDR (high dynamic range) imaging in digital photography goes back 19 years, as Robertson et al. state[2]:</p> <p>"The first report of digitally combining multiple pictures of the same scene to improve dynamic range appears to be Mann[1]."</p> <p>HDR combines multiple differently exposed pictures of the same subject matter to be able to see a much greater dynamic range than is possible in a single exposure.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738802</person_id>
				<author_profile_id><![CDATA[81504684378]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738803</person_id>
				<author_profile_id><![CDATA[81100147121]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Raymond]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738804</person_id>
				<author_profile_id><![CDATA[81541075556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738805</person_id>
				<author_profile_id><![CDATA[81490696642]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Valmiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rampersad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738806</person_id>
				<author_profile_id><![CDATA[81320490635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ryan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Janzen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Mann. Compositing multiple pictures of the same scene. In <i>Proceedings of the 46th Annual IS&T Conference</i>, volume 2, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Robertson, S. Borman, and R. Stevenson. Estimation-theoretic approach to dynamic range enhancement using multiple exposures. <i>Journal of Electronic Imaging</i>, 12:219, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 HDRchitecture: Real-time Stereoscopic HDR Imaging for Extreme Dynamic Range Steve Mann, Raymond Lo, 
Jason Huang, Valmiki Rampersad, and Ryan Janzen University of Toronto, Faculty of Engineering, Arts and 
Sciences, and Forestry  Figure 1: EyeTap welding helmet causes the eyes themselves, to, in e.ect, become 
both cameras and displays by providing exact POE (Point-of-Eye) cap­ture for a stereo headup display. 
1. BACKGROUND ON HDR The history of HDR (high dynamic range) imaging in digital photography goes back 
19 years, as Robertson et al. state[2]: The .rst report of digitally combining multiple pictures of the 
same scene to improve dynamic range appears to be Mann[1]. HDR combines multiple di.erently exposed 
pictures of the same subject matter to be able to see a much greater dy­namic range than is possible 
in a single exposure. 2. REALTIME HDR VIDEO In this demonstration, we present extreme HDR adapted for 
use in GlassEyesTM (EyeTap) electric arc welding helmets, as well as for a general-purpose seeing aid. 
Our WeldCam HDRchitecture (abbreviated HDRchitec­ture ) system uses one or more cameras, and optional 
active illumination systems, that can be used by welding schools and professionals to inspect welding 
in real-time. We present HDRchitecture as either a .xed camera system (e.g. for use on a tripod), or 
as a stereo EyeTap cybernetic welding hel­met (see Figure 1) that records and streams live video to observers, 
nearby or remote. By capturing a dynamic range of more than a million to one, we can see details that 
cannot be seen by the human eye or any currently existing commer­cially available cameras. We present 
a highly parallelizable and computationally ef­.cient HDR reconstruction and tonemapping algorithm for 
extreme dynamic range scenes. In comparison to existing HDR work, our system runs in real-time, and requires 
no user intervention or .ne-tuning of parameters. It renders im­ages with a high image quality up to 
1920x1080 resolution. HDRchitecture uses FPGAs, GPUs, or multi-core CPUs for real-time (30 to 120 frames/sec) 
stereoscopic HDR process­ing. Figure 2: (Left) State-of-the-art digital cameras can not capture extreme 
dynamic range of TIG welding. The electric arc is overexposed, yet the surroundings are underexposed. 
(Right) Single frame from the real-time HDR video glasses. Our system captures 120 frames/sec. Figure 
3: HDR technology developed for welding can be used for everyday life. AugmediatedTM Reality and HDR 
Cearable Computing, etc, http://eyetap.org/publications Our initial FPGA-based hardware con.guration 
.ts inside a shirt pocket, and can be built into ordinary eyeglasses if desired. There are two HDMI camera 
inputs, one for the left eye, and the other for the right eye, as well as HDMI outputs fed back to the 
left and right eyes, after processing of the video signals. The circuit board facilitates processing 
by way of a Xilinx FPGA, but our approach also works with Altera FPGAs. A goal of the demonstration is 
to show the development of HDR eyeglasses as a general-purpose seeing aid for everyday life (Fig 3). 
 3. REFERENCES [1] S. Mann. Compositing multiple pictures of the same scene. In Proceedings of the 46th 
Annual IS&#38;T Conference, volume 2, 1993. [2] M. Robertson, S. Borman, and R. Stevenson. Estimation-theoretic 
approach to dynamic range enhancement using multiple exposures. Journal of Electronic Imaging, 12:219, 
2003. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 
9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343468</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Interactive light field painting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343468</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343468</url>
		<abstract>
			<par><![CDATA[<p>Since the seminal SketchPad work of Sutherland [1964], direct interaction with a computer has been compelling: we can directly touch, move, and change what we see. Direct interaction is a major contribution to the success of smartphones and tablets; yet, the world is not flat. While existing technologies can display realistic multi-view stereoscopic 3D content reasonably well [Lueder 2012], interaction within the same 3D space often requires extensive additional hardware. We present a cheap and easy system that uses the same lenslet array for both multi-view autostereoscopic display and 3D light-pen position sensing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738807</person_id>
				<author_profile_id><![CDATA[81453653111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tompkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Boston]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738808</person_id>
				<author_profile_id><![CDATA[81504688738]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Samuel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Muff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Boston]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738809</person_id>
				<author_profile_id><![CDATA[81504687634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stanislav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jakuschevskij]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Boston]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738810</person_id>
				<author_profile_id><![CDATA[81504682391]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McCann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adobe]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738811</person_id>
				<author_profile_id><![CDATA[81100016395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kautz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Boston and UCL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738812</person_id>
				<author_profile_id><![CDATA[81100235480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Boston and TU Berlin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738813</person_id>
				<author_profile_id><![CDATA[81100458116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matusik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Boston and MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lueder, E. 2012. <i>3D Displays</i>. Wiley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>810742</ref_obj_id>
				<ref_obj_pid>800265</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. E. 1964. Sketchpad: A man-machine graphical communication system. In <i>Proceedings of the SHARE design automation workshop</i>, DAC '64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Light Field Painting James Tompkin1 Samuel Muff1 Stanislav Jakuschevskij1 Jim McCann2 Jan 
Kautz1,3 Marc Alexa1,4 Wojciech Matusik1,5 1Disney Research Boston 2Adobe 3UCL 4TU Berlin 5MIT Figure 
1: An example of a user interacting with our .rst system. Introduction Since the seminal SketchPad 
work of Sutherland [1964], direct inter­ action with a computer has been compelling: we can directly 
touch, move, and change what we see. Direct interaction is a major contri­bution to the success of smartphones 
and tablets; yet, the world is not .at. While existing technologies can display realistic multi-view 
stereoscopic 3D content reasonably well [Lueder 2012], interaction within the same 3D space often requires 
extensive additional hard­ware. We present a cheap and easy system that uses the same lenslet array for 
both multi-view autostereoscopic display and 3D light-pen position sensing. The display provides multi-user 
glasses-free autostereoscopic view­ing with motion parallax. A single near-infrared camera located behind 
the lenslet array is used to track a light pen held by the user. Full 3D position tracking is accomplished 
by analysing the pattern produced when light from the pen shines through the lenselet array. This light 
pen can be used to directly draw into a displayed light .eld, or as input for object manipulation or 
de.ning parametric lines. Our system has a number of advantages. First, it inexpensively provides both 
multi-view autostereoscopic display and 3D sensing with a 1:1 mapping. To the best of our knowledge, 
this has not been offered in previous interactive content creation systems. Second, as the same lenslet 
array provides both 3D display and 3D sensing, our system design is extremely simple, inexpensive, and 
easy to build and calibrate. We show a variety of interesting interaction styles with a prototype implementation: 
freehand drawing, polygonal and parametric line drawing, model manipulation and model editing. Demonstration 
Prototypes While our .rst camera/projector prototype begins to demonstrate what is possible with such 
a system, it has some problems. These problems are largely mechanical and concern what equipment is used 
and how the equipment is set up, and do not signi.cantly affect our algorithms for sensing and drawing. 
We are currently building a new prototype (V2, Figure 2) which will .x these problems, and this is what 
we wish to show at the Emerging Technologies exhibition. We outline a list of problems and how our new 
prototype .xes them: 1. Small display/interaction area: V2 will have a 4x larger display and sensing 
area. 2. Low display resolution: V2 will increase spatial resolution 4x by using a 4 projector array. 
 3. Less accurate far-distance sensing: V2 will provide more accurate sensing by using a 4MPix camera. 
 4. Slow sensing speed (30Hz): V2 will provide much faster sens­ing and more .uid interaction by using 
a high-speed camera (100Hz+) and optimized software. 5. Slow CPU rendering: V2 will provide much faster 
render­ing, with a GPU-based point renderer able to handle more complicated scenes. 6. Diffusor sparkle 
artefacts: V2 will provide a display with less distracting sparkle artefacts. 7. No haptics/no buttons 
on pen: V2 will provide haptic feedback as the user paints in 3D -intersecting existing content in free 
space will cause the light pen to rumble -and V2 will also provide function control buttons on the pen. 
 8. Limited painting tools: V2 will add curves, primitives, scene hierarchy, selection tools, a eraser, 
etc. -more and better painting tools.   Figure 2: Top: Scale sketch of our model as we expect to present 
it to attendees. Bottom: Photograph of our improved system in con­struction. A small camera (not visible) 
lies between the projectors. References LUEDER, E. 2012. 3D Displays. Wiley. SUTHERLAND, I. E. 1964. 
Sketchpad: A man-machine graphical communication system. In Proceedings of the SHARE design automation 
workshop, DAC 64. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, 
August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343469</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[JUKE cylinder]]></title>
		<subtitle><![CDATA[a device to metamorphose hands to a musical instrument]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343469</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343469</url>
		<abstract>
			<par><![CDATA[<p>If you knock an object, it sounds. If you play music, loudspeakers sound. The sound you usually listen to is generated by actions or objects supposed to generate sound. However, the development of parametric loudspeakers made it possible for people to feel that the sound comes from actions or objects which are not supposed to generate sound, because the parametric loudspeaker can localize the sound image on the reflected surface[1][2]. The parametric speakers work in an entirely different way from conventional loudspeakers. They generate ultrasound, and it travels out from a parametric loudspeaker in a narrowly focused column like a flashlight beam. When it hits something, it turns back into ordinary sound you can hear. There is a computer interface using this characteristic of the parametric speaker[3].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738814</person_id>
				<author_profile_id><![CDATA[81504687353]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masamichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ueta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ueta@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738815</person_id>
				<author_profile_id><![CDATA[81320490436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Osamu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshuyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Information and Media Processing Laboratories, NEC Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[houshu@bq.jp.nec.com]]></email_address>
			</au>
			<au>
				<person_id>P3738816</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narumi@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738817</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tani@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738818</person_id>
				<author_profile_id><![CDATA[81496668464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[F. J. Pompei. The use of airborne ultrasonics for generating audible sound beams. J. Audio Eng. Soc, Vol. 47, No. 9, pp.726--731, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[W. S. Gan, E. L. Tan, and S. M. Kuo. Audio projection. Signal Processing Magazine. IEEE, Vol. 28, No. 1, pp. 43--57, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Zerroug, A. Cassinelli and M. Ishikawa. Invoked computing: Spatial audio and video AR invoked through miming. Proceedings of Virtual Reality International Conference, pp. 31--32, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 JUKE Cylinder: a device to metamorphose hands to a musical instrument Masamichi Ueta , Osamu Hoshuyama 
, Takuji Narumi , Tomohiro Tanikawa , Michitaka Hirose Graduate School of Information and Media Graduate 
School of Graduate School of Graduate School of Information Science and Processing Laboratories, Information 
Science and Information Science and Information Science and Technology, NEC Corporation Technology, Technology, 
Technology, The University of Tokyo The University of Tokyo The University of Tokyo The University of 
Tokyo  Fig. 1 Hands metamorphosed Fig. 2 Whole view of JUKE to any musical instrument Cylinder 1. Introduction 
If you knock an object, it sounds. If you play music, loudspeakers sound. The sound you usually listen 
to is generated by actions or objects supposed to generate sound. However, the development of parametric 
loudspeakers made it possible for people to feel that the sound comes from actions or objects which are 
not supposed to generate sound, because the parametric loudspeaker can localize the sound image on the 
reflected surface[1][2]. The parametric speakers work in an entirely different way from conventional 
loudspeakers. They generate ultrasound, and it travels out from a parametric loudspeaker in a narrowly 
focused column like a flashlight beam. When it hits something, it turns back into ordinary sound you 
can hear. There is a computer interface using this characteristic of the parametric speaker[3]. 2. Concept 
The characteristic of the parametric loudspeaker provides people with extraordinary sound experiences. 
We came up the idea from this amazing characteristic. In the case of invoked computing[3], users can 
t control the sound by themselves. The concept of our study is to metamorphose hands to a musical instrument 
by localizing the sound image on the hands and make it possible for users to control the pitches of the 
sound (Fig 1). The only way to use one s hands as a musical instrument is hand clapping. However, our 
system can metamorphose the hands to various musical instruments like a guitar, a piano, or a flute, 
and users are able to control the pitches of the sound like those real musical instruments. With our 
system, people can feel that the sound is generated on their hands, and have an illusion that their hands 
metamorphose to a musical instrument.  3. Implementation We developed a cylindrical interactive device 
(Fig. 2), named JUKE Cylinder , that could localize the sound image on one s hands and play various tones. 
JUKE Cylinder consists of four parametric loudspeakers, two Kinects, eight dimmable LEDs, and four fog 
machines. All parametric loudspeakers, all email: {ueta, narumi, tani, hirose}@cyber.t.u-tokyo.ac.jp 
email: houshu@bq.jp.nec.com Fig. 3 Interaction and how to Fig. 4 Multi-user interaction output sound 
Kinects and four LEDs are put on the upper part, and the rest, four LEDs and four fog machines, are put 
on the lower part. 4. Interaction The interaction with JUKE Cylinder is to hold one s hands to the misted 
light and go up and down them. The parametric loudspeakers output sounds to their hands (Fig 3). Therefore, 
we should design this device to make users to hold their hands to the light. The fog machines are used 
to visualize the ray of LEDs and lead users to hold their hands to the light. This design guides users 
to interact with JUKE Cylinder naturally. The mist from their hands looks like the sound is spreading 
from them. This effect makes users to recognize that the sound is coming from their hands. When users 
hold their hands to the light, the Kinects get the depth data of them. By the depth data, the pitch of 
sound and the brightness of the LEDs are decided. Each parametric loudspeaker is assigned its output 
sound by software instrument like a guitar, a piano, or a flute. The parametric loudspeakers output sounds 
that have pitches and tones to one s hands, and the sounds spread from their hands. When you play a musical 
instrument with other people, it is important to act in harmony, called session . One to four users can 
play with JUKE Cylinder simultaneously (Fig. 4). When multiple users play with JUKE Cylinder, a beat 
sound is generated with the original sound from the parametric loudspeakers. By the beat sound, users 
feel cooperativeness with others. References [1] F. J. Pompei. The use of airborne ultrasonics for generating 
audible sound beams. J. Audio Eng. Soc, Vol. 47, No. 9, pp.726-731, 1999. [2] W. S. Gan, E. L. Tan, and 
S. M. Kuo. Audio projection. Signal Processing Magazine. IEEE, Vol. 28, No. 1, pp. 43-57, 2011. [3] A. 
Zerroug, A. Cassinelli and M. Ishikawa. Invoked computing: Spatial audio and video AR invoked through 
miming. Proceedings of Virtual Reality International Conference, pp. 31-32, 2011. Copyright is held by 
the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343470</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Magic Pot]]></title>
		<subtitle><![CDATA[interactive metamorphosis of the perceived shape]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343470</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343470</url>
		<abstract>
			<par><![CDATA[<p>"Magic Pot" is an interactive system which changes the perceived shape of a physical object by using haptic illusion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738819</person_id>
				<author_profile_id><![CDATA[81488658104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ban]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ban@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738820</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narumi@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738821</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tani@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738822</person_id>
				<author_profile_id><![CDATA[81496668464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1502413</ref_obj_id>
				<ref_obj_pid>1502409</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Lecuyer: Simulating Haptic Feedback Using Vision: a Survey of Research and Applications of Pseudo-Haptic Feedback, Presence: Teleoperators and Virtual Environments, MIT Press, Volume 8, Issuel, pp. 39-Si53, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141920</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S. Schaefer and T. McPhail and J. Warren: Image Deformation Using Moving Least Squares, ACM SIGGRAPH 2006 Papers (SIGGRAPH '06), ACM, New York, NY, USA, pp.533--540, 2006]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Magic Pot: Interactive Metamorphosis of the Perceived Shape Yuki Ban , Takuji Narumi , Tomohiro Tanikawa 
, Michitaka Hirose Graduate School of Information Science and Technology, the University of Tokyo  
 Fig.1 Video See-through System Fig.2 Fitting Hand based on Distortion Fig.3 Interactive Metamorphosis 
of Shape  1. Introduction1  email: {ban, narumi, tani, hirose}@cyber.t.u-tokyo.ac.jp "Magic Pot" is 
an interactive system which changes the perceived shape of a physical object by using haptic illusion. 
Haptics is one of the most important sensations in our life, and many researches have been conducted 
to realize a device to present virtual haptic sensations. However, because these devices are mainly focus 
on active haptics which aim to reproduce physical force feedback, they become physically too complicated 
when we try to reproduce complex haptic sensations to use them easily. On the other hand, there are increasing 
numbers of works which focus on alternative approaches such as passive haptics, which include pseudo-haptics. 
Pseudo-haptics is a kind of cross modal effect between our visual and haptic sense [1], which indicates 
an illusional perception in our haptic sensation evoked by vision. This illusion is evoked when we work 
under an inconsistent situation between the physical behavior of our body and the one observed in our 
vision. For example, it is revealed that the mismatch between a speed of physical computer mouse and 
the one of corresponding cursor in display evokes illusional force feedback on our hands. Pseudo-haptic 
approach is a potential solution for exploiting boundaries and capabilities of the human sensory system 
to simulate haptic information without physical force feedback. In our system, we compose a rendering 
algorithm of visual feedback to evoke pseudo-haptic effects, which affects our haptic perception about 
shape. This algorithm compose the visual feedback in which we can observe as if we were touching the 
virtual shape of the object, and evoke a pseudo-haptic illusion which make us to perceive as if the shape, 
especially the curvature and the size of the static object changed. By using this algorithm, we realize 
an interactive metamorphosis in the shape of "Magic Pot" using simple interface to shapen it. 2. Rendering 
algorithm for Visual Feedback  To make up an inconsistent situation between our vision and haptic sensation 
to evoke effective pseudo-haptic illusion, we composed a video see-through system (Fig. 1). Two cameras 
and a mirror are placed to capture the images around the physical object placed behind the display from 
the position corresponding to the user's eyes. These images are processed to change the shape of an object 
to the one of virtual objects based on the rendering algorithm. Using these composed images for each 
user's eye, we realize stereoscopic video see-through, in which the user can touch an object observed 
in display. Our rendering algorithm is composed of three processes. First, we detect the point on which 
a user is touching on the physical object. The area of the user's hand is extracted from captured images 
based on color to find finger tips of the user's pointing finger and thumb as first and second height 
of the area. Second process is a generation of a distortion map based on difference between the shape 
of the physical object and the one of virtual object. The area of the physical object is extracted from 
captured images as green colored area to detect its outline. Then we compute the distortion which fits 
the outline of the physical object to the virtual one. Finally, we translate and deform the shape of 
the user's hand and fit it to the virtual shape. To deform user's hand, we use the algorithm based on 
moving least squares [2], which can generate the natural deformation considering the rigidity of the 
object, based on the displacement of control points. We displace the user's hand according to the distortion 
map computed in previous step. Then we deform its shape based on the displacement of two control points, 
pointing finger and thumb (Fig. 2). These processes make up the inconsistency between the position and 
shape of user's hand and the one in the visual feedback, and make up pseudo-haptic effect, which makes 
the user to perceive virtual shape in visual feedback. We conducted some studies using this system, and 
revealed 85 percent of people felt the virtual curvature and the size of shape in visual feedback, although 
they were only touching on a statics cylinder. 3. Interactive System to "Change" Shape  With the rendering 
algorithm previously described, we implemented an interactive system which displays a variety of shapes 
of virtual magic pots (Fig. 3). In this experience, a user draws an outline of the magic pot on touch 
panel. The shape of virtual magic pot is generated based on this drawing as the rotating shape. Then 
we compare the shape of a physical cylinder placed behind the display and the one of magic pot in the 
display, and render the visual feedback to evoke pseudo-haptic sensations, which make users feel as if 
the curvature of the cylinder changed according to user input. In addition, the scale of magic pot also 
changes as user touches on it. The distortion map is regenerated according to the change in scale, and 
also makes up pseudo-haptic illusion on the perception about the scale of the object. References [1] 
A. Lecuyer: Simulating Haptic Feedback Using Vision: a Survey of Research and Applications of Pseudo-Haptic 
Feedback, Presence: Teleoperators and Virtual Environments, MIT Press , Volume 8, Issuel, pp. 39-Si53, 
2009. [2] S. Schaefer and T. McPhail and J. Warren: Image Deformation Using Moving Least Squares, ACM 
SIGGRAPH 2006 Papers (SIGGRAPH '06), ACM, New York, NY, USA, pp.533-540, 2006 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343471</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Mood Meter]]></title>
		<subtitle><![CDATA[large-scale and long-term smile monitoring system]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343471</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343471</url>
		<abstract>
			<par><![CDATA[<p>Have you ever wondered whether it's possible to quantitatively measure how friendly or welcoming a community is? Or imagined which parts of the community are happier than others? In this work, we introduce a new technology that begins to address these questions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738823</person_id>
				<author_profile_id><![CDATA[81490645252]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Javier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hernandez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[javierhr@mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3738824</person_id>
				<author_profile_id><![CDATA[81548025756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mohammed]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Hoque]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mehoque@mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3738825</person_id>
				<author_profile_id><![CDATA[81100496593]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rosalind]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Picard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[picard@media.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2370264</ref_obj_id>
				<ref_obj_pid>2370216</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hernandez, J., Hoque M. E., drevo W., Picard, R. W. 2012. Mood Meter: Counting Smiles in the Wild. In Proceedings of the 14th International Conference on Ubiquitous Computing.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mood Meter: Large-Scale and Long-Term Smile Monitoring System 5737898074_b9d1233dac_o (Medium) heatmap 
    Figure 1. (a) Interactive display shown to passers-by. (b)Schematics of one installation. (c) 
Real-time graphs showing the amount of smiles. Javier Hernandez * MIT Media Lab Cambridge, MA, USA javierhr@mit.edu 
Mohammed E. Hoque * MIT Media Lab Cambridge, MA, USA mehoque@mit.edu Rosalind W. Picard MIT Media Lab 
Cambridge, MA, USA picard@media.mit.edu   1. Introduction Have you ever wondered whether it s possible 
to quantitatively measure how friendly or welcoming a community is? Or imagined which parts of the community 
are happier than others? In this work, we introduce a new technology that begins to address these questions. 
 Mood Meter is a computer vision based system that automatically encourages, recognizes and counts smiles 
of a large environment or a community. For our project, we installed Mood Meter in a college campus during 
a 10-weeks long festival to count smiles of people at 4 different key locations. Our system collected 
aggregated anonymous information, and then displayed the information in real time in various intuitive 
and interactive formats on a public website, depicting the emotional footprint of the community as a 
function of smiles at any point in time. 2. Design Mood Meter contained two different components. 1) 
A local set up in each location with a laptop, projector, and camera (as shown in Figure 1.b); 2) A web 
front that collected all the information from all the locations to generate a public website (http://moodmeter.media.mit.edu/) 
with several interactive graphs that reflected the emotional footprint of the campus community (Figure 
1.c shows a heat map as a function of smiles). In each location, we had a camera set up looking over 
a large public space. The camera was connected with a laptop that would process the images to identify 
human faces, and smile intensity scores on each face (ranging from 0-100, 0 = no smile, 100 = very intense 
smile). Our computer vision system utilized OpenCV and Shore framework and extracted several geometric 
properties (e.g., distances between the corner lips and the nose) that are used to predict intensity 
of a smile. To make it more fun and engaging, we projected the live-feed captured from the camera using 
large projectors, as shown in Figure 1.a. Since some people may not enjoy looking at themselves in a 
large projector in public places, we intentionally drew blobs on each face. The interface overlaid a 
yellow neutral face if the person was not smiling ( smile intensity < 50%) and a green happy smiley otherwise. 
On the left side of the interface display, there is a smile-barometer that depicted the aggregated smile 
estimation for everyone present in the image. For example, if there are two people smiling with smile 
intensity values being 70% and 80%, the smile-barometer would have a value of (70+80)/2 = 75%. To encourage 
further participation, each person got a bow tie drawn around his/her neck whenever the aggregate smile 
intensity reached above 50%. We noticed that this feature often led members of a group to encourage other 
members to smile. In order to promote transparency, the lower part of the image contained the disclaimer, 
along with the project website address, that clearly stated, This is a live feed, no information is recorded. 
Moreover, there was physical signage next to the camera explaining the purpose of the installation along 
with privacy details. The website also contained information about the project and how it addressed privacy. 
 3. Interaction Over the ten weeks of installation, thousands of people passed by the installations 
and enjoyed interacting with the system in many different ways. Through our analysis, we were able to 
monitor and compare the emotional responses of people to several academic events such as holidays, exam 
periods, open houses, and graduation day. Analysis of the data revealed very periodic patterns, as well 
as a strong correlation with these events that supported traditional expectations of a university community 
(Hernandez, Hoque, Drevo &#38; Picard, 2012). 4. Conclusions We are excited about the other possible 
applications of the Mood Meter, especially in context of audience engagement, measuring stress in work 
environment, or even perhaps in a comedy club to see audience s live response. The Mood Meter was both 
a scientific experiment and an interactive installation, and succeeded in creating a novel emotional 
portrait of a public environment while respecting people s feelings and privacy. References HERNANDEZ, 
J., HOQUE M. E., DREVO W., PICARD, R. W. 2012. Mood Meter: Counting Smiles in the Wild. In Proceedings 
of the 14th International Conference on Ubiquitous Computing. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343472</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[PossessedHand]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343472</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343472</url>
		<abstract>
			<par><![CDATA[<p>Human have the dexterous fingers because of many muscle actuators in the forearm that are used for communications, playing and so on. If a device can control human hands, the device can be useful for HCI(Human-Computer Interface) application's output.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738826</person_id>
				<author_profile_id><![CDATA[81416597514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hoimei@acm.org]]></email_address>
			</au>
			<au>
				<person_id>P3738827</person_id>
				<author_profile_id><![CDATA[81100008564]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rekimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rekimoto@acm.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Handa, Y and Yagi, R and Hoshimiya, N 1998. Application of functional electrical stimulation to the paralyzed extremities. Neurologia medicochirurgica, vol. 38,No.11, p 784--788.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PossessedHand Emi Tamaki Graduate school of Arts and Sciences, The University of Tokyo. Tokyo, Japan 
hoimei@acm.org 1. Introduction Human have the dexterous fingers because of many muscle actuators in 
the forearm that are used for communications, playingand so on. If a device can control human hands, 
the device can beuseful for HCI(Human-Computer Interface) application s output.Many devices and systems 
that directly stimulate a user s fingers have been researched. However, users prefer to avoid wearingdevices 
on the palm and the back of the hand because he skin on the area is important sensor for communication, 
touching, playing musical instruments or in other performing arts. To solve theseproblems, we develop 
PosssessedHand that can control a user shand without covering the hand. PossessedHand can be used forcontrolling 
finger joint movements; it operates by applying an electrical stimulus to the muscles in the forearm 
with noninvasive electrode pads. 2. System Configuration PossessedHand consists of a pulse generator 
(micro-controller andswitching board) and two forearm belts, which have 28 electricpads (Figure 1). Each 
muscle is stimulated via the pads. Functional Electrical Stimulation (FES)[1] is adapted to the electric 
stimuli of PossessedHand. The applied stimulation pulsefrequency was set to 40Hz, and the pulse width 
was 0.2 ms, andthe pulse height (voltage) was in the range of 15-42 V. Byapplying the electrical stimuli 
to the forearm muscles,PossessedHand can control hand motions of 16 finger joints. 3. Auto-calibration 
System A controlling the muscles requires special knowledge for thesystem setting because positions of 
the stimulation, the stimulation level and timing differ greatly in individuals. We introduce an automatic 
calibration system that stimulates all ofpattern of the pad s positions and the stimulation levels and 
estimates relations between the finger movements and the pattern. On this calibration system, users can 
use PossessedHand withoutany special knowledge. Figure 2 shows the controlled finger jointswith this 
system.  3. Application Finally, we also introduce an application about playing a musicalinstrument, 
the Koto. The Koto is a traditional Japanese stringed instrument. Kotos are about 180 centimeters wide 
and made from paulownia wood. Koto has 13 strings that are strung over 13 movable bridges along the width 
of the instrument. Players canadjust the string pitches by moving these bridges before playing.A koto 
player uses all fingers to pluck the strings. The player putsthree different picks on the thumb, index 
finger, and middle finger, respectively. In playing koto, it is important when and whichfinger should 
be moved because each finger produces differentsound and has a different playing style.It is difficult 
for performers to use visual or sound guides for thesupport of the Koto performance, because the performers 
shouldread a Koto score and listen to their own performances duringtheir performances. On the score, 
string numbers, timings andfinger numbers are written. Almost beginners can t read the stringnumbers, 
timings and finger numbers at the same time. Then,PossessedHand directly tells the finger number and 
timing to theperformers Jun Rekimoto Interfaculty Initiative in Information Studies, The University 
of Tokyo. Tokyo, Japan rekimoto@acm.org In our experiment, we checked that beginners could get therhythm 
and make fewer mistakes by wearing PossessedHand.Four beginner subjects tried to play the koto by wearing 
PossessedHand. The subject played 24-bar. For 9 times, the subject alternated between normal playing, 
supported playingwith Metronome and PossessedHand. Figure 3 shows the errors ofthe performance. This 
result indicates PossessedHand decreasesthe errors and supports the musical performance.  Individual 
jointcontrol  linked joint control 12.5~25.0% Not Actuated joint Figure 2. Controlled joints with 
PossessedHand.  Figure 3. A result of the application for musical performance.  References HANDA, Y 
AND YAGI, R AND HOSHIMIYA, N 1998. Application offunctional electrical stimulation to the paralyzed extremities. 
Neurologia medicochirurgica, vol. 38,No.11, p 784-788. Copyright is held by the author / owner(s). SIGGRAPH 
2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343473</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[REVEL]]></title>
		<subtitle><![CDATA[tactile feedback technology for augmented reality]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343473</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343473</url>
		<abstract>
			<par><![CDATA[<p>Augmented Reality has recently emerged as one of the key application areas of interactive computer graphics and is rapidly expanding from research laboratories into everyday use. The fundamental premise of AR is to enable us to interact with virtual objects immediately and directly, seeing, feeling and manipulating them just as we do physical objects. Most AR applications, however, provide only visual augmentation of the real world and do not provide the means to let the user <i>feel</i> tactile, physical properties of virtual objects or to enhance the physical world with computer-generated tactile textures. The absence of tactile feedback does not allow us to take advantage of the powerful mechanisms of the human sense of touch and diminishes the quality of the experience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738828</person_id>
				<author_profile_id><![CDATA[81311486200]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Olivier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738829</person_id>
				<author_profile_id><![CDATA[81100593278]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poupyrev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738830</person_id>
				<author_profile_id><![CDATA[81504683791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mathieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Le Goc]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738831</person_id>
				<author_profile_id><![CDATA[81504687702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Laureline]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Galliot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738832</person_id>
				<author_profile_id><![CDATA[81504687586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glisson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research, Pittsburgh]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bau, O. and Poupyrev, I. 2012. REVEL: Tactile Feedback Technology for Augmented Reality. ACM Trans. Graphics (Proc. SIGGRAPH).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866074</ref_obj_id>
				<ref_obj_pid>1866029</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bau, O., Poupyrev, I., Israr, A. and Harrison, C. 2010. TeslaTouch: electrovibration for touch surfaces. ACM UIST, 283--292.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 REVEL: Tactile Feedback Technology for Augmented Reality Olivier Bau, Ivan Poupyrev, Mathieu Le Goc, 
Laureline Galliot, Matthew Glisson Disney Research, Pittsburgh 1. REVEL: Augmented Reality Tactile Display 
Augmented Reality has recently emerged as one of the key appli­cation areas of interactive computer graphics 
and is rapidly ex­panding from research laboratories into everyday use. The funda­mental premise of AR 
is to enable us to interact with virtual ob­jects immediately and directly, seeing, feeling and manipulatingthem 
just as we do physical objects. Most AR applications, how­ever, provide only visual augmentation of the 
real world and donot provide the means to let the user feel tactile, physical proper­ties of virtual 
objects or to enhance the physical world with com­puter-generated tactile textures. The absence of tactile 
feedbackdoes not allow us to take advantage of the powerful mechanismsof the human sense of touch and 
diminishes the quality of theexperience. REVEL [Bau and Poupyrev 2012] is a new augmented reality(AR) 
tactile technology. Using a device worn by the user, REVELchanges the tactile feeling of real objects 
by augmenting themwith virtual tactile textures. Unlike previous attempts to enhanceAR environments with 
haptics, we neither physically actuate ob­jects or use any force-or tactile-feedback devices, nor requireusers 
to wear tactile gloves or other apparatus on their hands.Instead, we employ the principle of reverse-electrovibration 
where we inject a weak electrical signal anywhere on the userbody creating an oscillating electrical 
field around the user s fin­gers. When sliding his or her fingers on a surface of the object, theuser 
perceives highly distinctive tactile textures augmenting thephysical object. By tracking the objects 
and location of the touch,we associate dynamic tactile sensations to the interaction context. REVEL is 
built upon our previous work on designing electrovi­bration-based tactile feedback for touch surfaces 
[Bau, et al. 2010].REVEL expands tactile interfaces based on electrovibration be­yond touch surfaces 
and brings them into the real world, and canbe used to enhance AR interaction with dynamic and unobtrusivetactile 
feedback. 2. REVEL System Design Figure 3 presents the design of an AR tactile display based on reverse 
electrovibration using REVEL. A tactile signal generator(Figure 2) worn by the user communicates with 
an AR display and a context-sensing system. This, for example, can be an overheadprojector and computer 
vision tracking system that recognizeswhen the user is touching a physical object augmented with virtu­al 
content. The context-sensing system triggers the signal genera­tor to inject a tactile AC signal into 
the user s body. Thus, whenthe user is sliding fingers on the surface of a physical object, he orshe 
would feel virtual tactile textures when necessary, e.g., whenthe user s fingers touch virtual content 
overlaid on the physicalobject. Although the physical object is completely passive, thereare two crucial 
requirements that must be met for REVEL systemto function. First, the surface of the object or the parts 
of it that are touched by the user must be conductive and covered with a very thin layer of insulator 
(Figure 1). Second, the conductivesurface of the object and tactile signal generator should share a common 
electrical ground (Figure 3 Z and Z ). The REVEL tactile technology allows designing new and excitingAR 
experiences that are either difficult or impossible to create with existing tactile AR technologies, 
including interactive sur­faces, video see-through AR (Figure 1) and tangible AR interfaces. Copyright 
is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343474</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Shader printer]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343474</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343474</url>
		<abstract>
			<par><![CDATA[<p>Recently, spatial augmented reality regained popularity in media arts and in the field of tangible, embedded interaction [Holman and Vertegaal 2008], by using 3D objects as information displays. The advantage of projected graphics is obvious: it allows for updatable appearance of 3D objects without refabrication. However, in many everyday objects such as fashion and shoes, projector technology is not feasible due to power and tracking requirements, and the need for a darkened environment. Users cannot take the objects outside the calibrated projector setup.</p> <p>We introduce a novel concept of a stateful projector display that uses bi-stable color changing inks. We augment non-planar and complex painted surfaces by projecting high resolution imagery that persist. Similar to e-ink, graphics last without requiring additional power and are rewritable.</p> <p>Our printer builds upon earlier Emerging Technologies demonstrations [Saakes et al. 2010] and [Hashida et al. 2011] that used mono-stable photosensitive materials and near UV projectors. Because our system uses bi-stable thermochromic materials, our "prints" persist for at least a month, and are least 10 times rewritable. That makes the "Shader Printer" concept is useful for various applications that are not catered by traditional displays or fabrication technologies. For instance, fashion items such as shoes or bags, wallpapers, and floors can be updated frequently. Design prototypes can be tested outside the laboratory environment in outdoors and in real-use scenarios. Another application could be very large size, high resolution information displays that are constantly updated.</p> <p>Our workflow starts by coating a target object with bi-stable, thermochromic inks. We scan the object to obtain its geometry. We apply graphics as 2D textures on the 3D model in modeling software. In the printing process, we align the 3D model with the object. For each point (pixel) on the object, we estimate the color with a scanning laser. We compare the obtained color with the intended color from the textured model, and a writing laser then makes the color selectively transparent by applying heat. Depending on the ink used, a second process (for instance freezing the object) reverses this process and makes the inks opaque to allow for updatable content.</p> <p>We have tested two bi-stable thermochromic inks. The first is harvested from frixion pens that come in a colored state. By heating above 70 degrees Celcius, the ink becomes white and stays that way until it is cooled below -10 degrees Celsius and reaches the second stable state. The other material switches to the colored state when the material is heated to 180 degrees Celcius and then cooled down quickly. The translucent state is achieved by heating the material up to 120 degrees before cooling it down slowly.</p> <p>The opportunities for projector displays using bi-stable inks are immense. The Shader Printer fills a niche of non-planar surfaces and supports applications of dynamically attaching images onto complex surfaces allowing for non-traditional displays. Finally, low costs enable easy adoption of the new technology.</p> <p>There are several limitations in our technology in the area of pixel density and color reproduction that obviously can not match the years of research in commercially available display or laser systems. However, we hope to have demonstrated the advantages of a Shader Printer system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738833</person_id>
				<author_profile_id><![CDATA[81309492813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saakes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[JST erato]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[daniel@saakes.net]]></email_address>
			</au>
			<au>
				<person_id>P3738834</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738835</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738836</person_id>
				<author_profile_id><![CDATA[81316489433]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koizumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738837</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2048270</ref_obj_id>
				<ref_obj_pid>2048259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hashida, T., Kakehi, Y., and Naemura, T. 2011. Photochromic sculpture: volumetric color-forming pixels. In <i>ACM SIGGRAPH 2011 Emerging Technologies</i>, ACM, New York, NY, USA, SIGGRAPH '11, 11:1--11:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1349037</ref_obj_id>
				<ref_obj_pid>1349026</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Holman, D., and Vertegaal, R. 2008. Organic user interfaces: designing computers in any way, shape, or form. <i>Commun. ACM 51</i> (June), 48--55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1836843</ref_obj_id>
				<ref_obj_pid>1836821</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Saakes, D., Chiu, K., Hutchison, T., Buczyk, B. M., Koizumi, N., Inami, M., and Raskar, R. 2010. Slow display. In <i>ACM SIGGRAPH 2010 Emerging Technologies</i>, ACM, New York, NY, USA, SIGGRAPH '10, 22:1--22:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shader Printer Daniel Saakes* Masahiko Inami Takeo Igarashi Naoya Koizumi Ramesh Raskar JST erato JST 
erato JST erato Keio University MIT Keio University Tokyo University  Figure 1: Our Shader Printer allows 
projected imagery to persist on objects, providing a low-power, .exible and reversible manipulation appearance. 
We paint the object surfaces with a color dithered thermochromic ink and print or erase color images 
with a laser projector. Abstract Recently, spatial augmented reality regained popularity in media arts 
and in the .eld of tangible, embedded interaction [Holman and Vertegaal 2008], by using 3D objects as 
information displays. The advantage of projected graphics is obvious: it allows for updatable appearance 
of 3D objects without refabrication. However, in many everyday objects such as fashion and shoes, projector 
technology is not feasible due to power and tracking requirements, and the need for a darkened environment. 
Users cannot take the objects outside the calibrated projector setup. We introduce a novel concept of 
a stateful projector display that uses bi-stable color changing inks. We augment non-planar and complex 
painted surfaces by projecting high resolution imagery that persist. Similar to e-ink, graphics last 
without requiring additional power and are rewritable. Our printer builds upon earlier Emerging Technologies 
demonstra­tions [Saakes et al. 2010] and [Hashida et al. 2011] that used mono­ stable photosensitive 
materials and near UV projectors. Because our system uses bi-stable thermochromic materials, our prints 
persist for at least a month, and are least 10 times rewritable. That makes the Shader Printer concept 
is useful for various applica­tions that are not catered by traditional displays or fabrication tech­nologies. 
For instance, fashion items such as shoes or bags, wall­papers, and .oors can be updated frequently. 
Design prototypes can be tested outside the laboratory environment in outdoors and in real-use scenarios. 
Another application could be very large size, high resolution information displays that are constantly 
updated. Our work.ow starts by coating a target object with bi-stable, ther­mochromic inks. We scan the 
object to obtain its geometry. We apply graphics as 2D textures on the 3D model in modeling soft­ware. 
In the printing process, we align the 3D model with the ob­ject. For each point (pixel) on the object, 
we estimate the color with a scanning laser. We compare the obtained color with the intended color from 
the textured model, and a writing laser then makes the color selectively transparent by applying heat. 
Depending on the ink used, a second process (for instance freezing the object) re­verses this process 
and makes the inks opaque to allow for updat­ *e-mail: daniel@saakes.net able content. We have tested 
two bi-stable thermochromic inks. The .rst is har­vested from frixion pens that come in a colored state. 
By heating above 70 degrees Celcius, the ink becomes white and stays that way until it is cooled below 
-10 degrees Celsius and reaches the second stable state. The other material switches to the colored state 
when the material is heated to 180 degrees Celcius and then cooled down quickly. The translucent state 
is achieved by heating the material up to 120 degrees before cooling it down slowly. The opportunities 
for projector displays using bi-stable inks are im­mense. The Shader Printer .lls a niche of non-planar 
surfaces and supports applications of dynamically attaching images onto com­plex surfaces allowing for 
non-traditional displays. Finally, low costs enable easy adoption of the new technology. There are several 
limitations in our technology in the area of pixel density and color reproduction that obviously can 
not match the years of research in commercially available display or laser sys­tems. However, we hope 
to have demonstrated the advantages of a Shader Printer system. References HASHIDA, T., KAKEHI, Y., 
AND NAEMURA, T. 2011. Pho­tochromic sculpture: volumetric color-forming pixels. In ACM SIGGRAPH 2011 
Emerging Technologies, ACM, New York, NY, USA, SIGGRAPH 11, 11:1 11:1. HOLMAN, D., AND VERTEGAAL, R. 
2008. Organic user inter­faces: designing computers in any way, shape, or form. Com­mun. ACM 51 (June), 
48 55. SAAKES, D., CHIU, K., HUTCHISON, T., BUCZYK, B. M., KOIZUMI, N., INAMI, M., AND RASKAR, R. 2010. 
Slow dis­play. In ACM SIGGRAPH 2010 Emerging Technologies, ACM, New York, NY, USA, SIGGRAPH 10, 22:1 
22:1. Copyright is held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 
2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343475</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[SplashDisplay]]></title>
		<subtitle><![CDATA[volumetric projection using projectile beads]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343475</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343475</url>
		<abstract>
			<par><![CDATA[<p>The prime feature of the SplashDisplay is that it uses projectile beads as a display medium that are launched from the table; this means that the tangible medium can be removed from the surface and allowed free movement in air. In current research [1] [2], displays have tended toward fixed mediums and therefore defining and visualizing explosive activity on such displays has been difficult. However, the SplashDisplay is made for this particular purpose, and given the nature of the display medium it pulls away from conventional standard surfaces. Given this tradeoff, it is possible for the SplashDisplay to launch projectile beads from millimeters to meters into the air freely making it possible to attain an image 'depth' much like the Z-axis in 3D. In the simulation of 'explosions', this system launches beads into the air much like the physical phenomenon, making the projected object 'feel' like it actually exploded (Figure 1). As the beads are white in color, it is possible for these beads to act as a display backing. When the system is still, then the beads play the role of a stationary screen; once the beads are in air, they can still be recognized as 'screen' material. If light is projected onto these airborne beads, they will illuminate as they fall, giving a 'fireworks' like effect in real time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738838</person_id>
				<author_profile_id><![CDATA[81490686533]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matoba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[matoba@vogue.is.uec.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738839</person_id>
				<author_profile_id><![CDATA[81442602487]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Taro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tokui@vogue.is.uec.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738840</person_id>
				<author_profile_id><![CDATA[81503683307]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sato@vogue.is.uec.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738841</person_id>
				<author_profile_id><![CDATA[81421592614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[den@vogue.is.uec.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738842</person_id>
				<author_profile_id><![CDATA[81100297951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hideki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[koike@vogue.is.uec.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2048268</ref_obj_id>
				<ref_obj_pid>2048259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[N. Lee, J. Kim, J. Lee, M. Shin, W. Lee, "MoleBot: Mole in a Table", SIGGRAPH 2011 Emerging Technologies.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1979754</ref_obj_id>
				<ref_obj_pid>1979742</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Blacshaw, A. DeVincenzi, D. Lakatos, D. Leithinger, H. Ishii, "Recompose: direct and gestural interaction with an actuated surface," CHI 2011, May 7--12, 2011, pp.1237--1242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SplashDisplay: Volumetric Projection using Projectile Beads Yasushi Matoba, Taro Tokui, Ryo Sato, Toshiki 
Sato, Hideki Koike University of Electro-Communications {matoba | tokui | sato | den | koike}@vogue.is.uec.ac.jp 
1. INTRODUCTION The prime feature of the SplashDisplay is that it uses projectile beads as a display 
medium that are launched from the table; this means that the tangible medium can be removed from the 
surface and allowed free movement in air. In current research [1] [2], displays have tended toward fixed 
mediums and therefore defining and visualizing explosive activity on such displays has been difficult. 
However, the SplashDisplay is made for this particular purpose, and given the nature of the display medium 
it pulls away from conventional standard surfaces. Given this trade-off, it is possible for the SplashDisplay 
to launch projectile beads from millimeters to meters into the air freely making it possible to attain 
an image depth much like the Z-axis in 3D. In the simulation of explosions , this system launches beads 
into the air much like the physical phenomenon, making the projected object feel like it actually exploded 
(Figure 1). As the beads are white in color, it is possible for these beads to act as a display backing. 
When the system is still, then the beads play the role of a stationary screen; once the beads are in 
air, they can still be recognized as screen material. If light is projected onto these airborne beads, 
they will illuminate as they fall, giving a fireworks like effect in real time.  Figure 1: Splashed 
and illuminated beads in the air 2. CONFIGURATION The SplashDisplay consists of a table that collects 
and holds beads, a projectile launching speaker, the speaker movement system, an IR LED and IR Camera 
system for input detection, and a visible light projector and two control computers (Figure 2). The container 
table is 900mm by 600mm and built on top of a net with a lattice distance of 1mm. Settled on top of this 
net is a volume of projectile beads filled to a 70mm level, each at approximately 5mm in diameter. Below 
the net is a single speaker installed to use air vibrations to launch the projectile beads into the air, 
this speaker mounted on an X-Y table that can move the speaker freely under the container. The X-Y table 
consists of a stepping motor to pull a belt to shift the speaker mount. The bead surface projector is 
set 600mm above the container and is connected to a computer that allows for projection of images onto 
the bead s surface. In order to identify user interaction, there is an IR LED array set along the 900mm 
side with an IR Camera set on the top of the system. The current system is divided into three software 
components. The Motor Control Program (MCP) controls the X-Y table s movements. The Image Processing 
Program (IPP) receives information from the IR cameras and detects an input object s position and thus 
can detect that object s movements. Within the Application Program (AP), the object information received 
from the Image Processing program and the speaker position information from the MCP is processed to activate 
the occurrence of an event. The event result is displayed through the connected projector and also sent 
through an audio cable to the mounted speaker below the net. The speaker (260mm in diameter) is covered 
by a 5mm thick board with a 50mm diameter hole opening at its center to prevent sound leakage. This speaker 
is set to a frequency, 10Hz, which is undetectable by the human ear and blasted into the bead volume 
through the net through the hole opening by reverberation of the sound waves. When this occurs, the wind 
caused by this air pressure accumulation released out of the hole is of single vector nature. By using 
this wind blast, it is possible to freely create explosions. This method is preferred for the effect 
as opposed to a spinning fan. However, to obtain the effect to a substantial level the required components 
and construct becomes large and heavy. Mounted on the top of the system, the USB Camera is set to receive 
scattered IR light. The IR light source is mounted directly opposite the user s position 50mm above the 
bead surface. 3. REFERENCES [1] N. Lee, J. Kim, J. Lee, M. Shin, W. Lee, MoleBot: Mole in a Table , SIGGRAPH 
2011 Emerging Technologies. [2] M. Blacshaw, A. DeVincenzi, D. Lakatos, D. Leithinger, H. Ishii, Recompose: 
direct and gestural interaction with an actuated surface, CHI 2011, May 7-12, 2011, pp.1237-1242.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343476</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Stuffed toys alive!]]></title>
		<subtitle><![CDATA[cuddly robots from fantasy world]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343476</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343476</url>
		<abstract>
			<par><![CDATA[<p>Stuffed toys live with all ages and hold them in some physical and mental aspects. With the soft feel and cute characters, stuffed toys play with them, sleep together and listen to their complaints. These roles of stuffed toys show that people imagine stuffed toys are interactive creature. Indeed, there are many stories and movies in which stuffed toys work as living characters. However, stuffed toys in real world are just dolls and they cannot move and react.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[haptic interaction]]></kw>
			<kw><![CDATA[human friendly robot]]></kw>
			<kw><![CDATA[stuffed toy]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738843</person_id>
				<author_profile_id><![CDATA[81504687256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamashita@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738844</person_id>
				<author_profile_id><![CDATA[81333489343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tatsuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ishikawa@haselab.net]]></email_address>
			</au>
			<au>
				<person_id>P3738845</person_id>
				<author_profile_id><![CDATA[81316489667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mitake@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738846</person_id>
				<author_profile_id><![CDATA[81485652285]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takase]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takase@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738847</person_id>
				<author_profile_id><![CDATA[81504687060]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fumihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fumihiro.k@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738848</person_id>
				<author_profile_id><![CDATA[81421595917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ikumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Susa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[susa@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738849</person_id>
				<author_profile_id><![CDATA[81481647072]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hase@hi.pi.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738850</person_id>
				<author_profile_id><![CDATA[81100567066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[msato@hi.pi.titech.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shibata, T., and Tanie, K. 2001. Physical and affective interaction between human and mental commit robot. In <i>Proc. of ICRA 2001</i>, vol. 3, 2572--2577 vol.3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179149</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stiehl, W. D., Breazeal, C., Han, K.-H., Lieberman, J., Lalla, L., Maymin, A., Salinas, J., Fuentes, D., Toscano, R., Tong, C. H., and Kishore, A. 2006. The huggable: a therapeutic robotic companion for relational, affective touch. In <i>ACM SIGGRAPH 2006 Emerging technologies</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stuffed Toys Alive! : Cuddly Robots from Fantasy World Yohei Yamashita * Tatsuya Ishikawa Hironori 
Mitake* Yutaka Takase* Fumihiro Kato* Ikumi Susa* Shoichi Hasegawa* Makoto Sato* {yamashita, mitake, 
takase, fumihiro.k, susa, msato, hase}@hi.pi.titech.ac.jp* ishikawa@haselab.net Tokyo Institute of Technology* 
University of Electro-Communications Keywords: human friendly robot, stuffed toy, haptic interaction 
1 Introduction Stuffed toys live with all ages and hold them in some physical and mental aspects. With 
the soft feel and cute characters, stuffed toys play with them, sleep together and listen to their complaints. 
These roles of stuffed toys show that people imagine stuffed toys are inter­active creature. Indeed, 
there are many stories and movies in which stuffed toys work as living characters. However, stuffed toys 
in real world are just dolls and they cannot move and react. Animatronics, pets and social robots with 
skin of stuffed toys [Shibata and Tanie 2001][Stiehl et al. 2006] realize soft motion of creatures in 
our imagination world. However, they don t have soft feelings. It is known that transitional object for 
infants begins from soft object. The soft feel will be a cause for familiarity of stuffed toys. Thus, 
we propose a mechanism for stuffed toy type robots ( cuddly robots ) which keep the soft feel of stuffed 
toys. 2 Innovation The soft feel both on mechanics and motion in the proposed robot is realized by three 
innovations; Driving mechanism retaining nature of stuffed toys The arms, legs and a head are soft to 
bone. They consist of strings, cotton wool and cloth, and have no hard materials. The sack of cloth is 
in.ated by the pressure from the internal cotton wool. The string is connected to the end of the sack 
and be sewed to the opening of the sack. Then the other side of the string is connected to a small winch 
placed in the center of the body where all hard mechanism is placed. The arms and legs have a good soft 
feel because they don t give sensation of any hard obstacles. In addition, they are robust to bending. 
When bending, the driving mechanism makes natural soft curves on arms and legs and gives soft impression. 
These shapes give natural impression as stuffed toys because they are same to the shapes of the curves 
which are generated when we animate them by taking their hands and feet. Motion control for the soft 
mechanism Proposed mechanism has repeat accuracy enough to create a map between the hand position and 
lengths of strings. Reaching motions to any target positions are available and various interactions can 
be realized. The smooth acceleration and motions conscious on the elasticity of the cotton wool gives 
impression of natural motion of stuffed toys. Force sensor with driving strings With force sensors embedded 
in the winches, the driving strings work as tension sensor. They sense forces on arms, legs and a head 
and realize reactions to user s interaction. In addition, the tension sensors are accurate enough to 
realize impedance control. Interac­tions such as handshakes are realized by controlling target position 
of the motion based on the sensed force. Figure 1: An example of touching interaction and appearance 
and inside of the cuddly robot. The cuddly robot is soft to the bone.  3 Exposition When the stuffed 
bear (cuddly robot) notices the user approaching to him, he stretches his hand to attract attention of 
the user. Then, the user takes his hand and he moves his hand to correspond to the motion of the user. 
If the user puts and leaves him, he will writhe and struggle to get the user s favor until the user takes 
him up in the arms. Finally, the user holds him and he feels a relief and moves his hands up and down. 
 4 Vision Recently, alternate reality and gami.cation, which embed story­telling and entertainments into 
our daily world, are getting public attention. We had proposed mixed reality installations such as Ko­bito 
or Haptic Ring which realize interaction with characters in the real world. However, getting a real body 
is ideal for haptic in­teraction for characters. Stuffed toys get into our daily lives. They have both 
bodies of real substances and their own story and fantasy world. Cuddly robot, which inherits these characteristics 
of stuffed toys, can be a new medium which entertains us in our daily lives and gives a sense of unity 
to our daily world and the fantasy world. Imagine the future, the stuffed toys from the fantasy world 
live with us and enchant our daily lives. References SHIBATA, T., AND TANIE, K. 2001. Physical and affective 
interac­tion between human and mental commit robot. In Proc. of ICRA 2001, vol. 3, 2572 2577 vol.3. 
STIEHL, W. D., BREAZEAL, C., HAN, K.-H., LIEBERMAN, J., LALLA, L., MAYMIN, A., SALINAS, J., FUENTES, 
D., TOSCANO, R., TONG, C. H., AND KISHORE, A. 2006. The huggable: a therapeutic robotic companion for 
relational, affec­tive touch. In ACM SIGGRAPH 2006 Emerging technologies. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343477</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Tavola]]></title>
		<subtitle><![CDATA[holographic user experience]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343477</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343477</url>
		<abstract>
			<par><![CDATA[<p>We demonstrate a new platform for holographic interactive 3D experience. New user experience includes holographic 3D visual and audio experience, natural free-space 3D interaction, and augmenting the interface of smaller devices (e.g. smartphones). The head tracking component is compact and non-intrusive to 3D glasses' appearance. Depth sensor based 3D hand and object tracking enables in-the-air dual hand 3D interactions. Hand tracking system and holographic visualization system are calibrated so user can directly interact with the virtual objects. And we created a set of easy and natural free hand interactions so hat the system can have wide range of applications and usable by the general public.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738851</person_id>
				<author_profile_id><![CDATA[81504684761]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yue]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Panasonic Silicon Valley Laboratory, Cupertino, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yue.fei@us.panasonic.com]]></email_address>
			</au>
			<au>
				<person_id>P3738852</person_id>
				<author_profile_id><![CDATA[81504688227]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kryze]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Panasonic Silicon Valley Laboratory, Cupertino, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738853</person_id>
				<author_profile_id><![CDATA[81504688191]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrea]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Melle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Panasonic Silicon Valley Laboratory, Cupertino, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1837099</ref_obj_id>
				<ref_obj_pid>1837026</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dittlo, N., Courtois M, and Orvain, E. 2010. 3D multitouch: when tactile tables meet immersive visualization technologies. In <i>ACM SIGGRAPH 2010 Talks</i>, Article 56, ACM]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2048266</ref_obj_id>
				<ref_obj_pid>2048259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bossavit et al. 2011. An immersive multitouch workspace. In <i>ACM SIGGRAPH 2011 Emerging Technologies</i>, Article 7, ACM]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tavola: Holographic User Experience Yue Fei David Kryze Andrea Melle Panasonic Silicon Valley Laboratory 
Cupertino, CA, USA yue.fei@us.panasonic.com 1. Introduction We demonstrate a new platform for holographic 
interactive 3D experience. New user experience includes holographic 3D visual and audio experience, natural 
free-space 3D interaction, and augmenting the interface of smaller devices (e.g. smartphones). The head 
tracking component is compact and non-intrusive to 3D glasses appearance. Depth sensor based 3D hand 
and object tracking enables in-the-air dual hand 3D interactions. Hand tracking system and holographic 
visualization system are calibrated so user can directly interact with the virtual objects. And we created 
a set of easy and natural free hand interactions so hat the system can have wide range of applications 
and usable by the general public.  2. System Overview This new system combines a large 80 horizontal 
stereoscopic projection surface, 6DOF head tracking unit and 3D hand tracking unit to create a holographic 
interactive 3D experience. The head tracking component is compact, and the method of instrument the glasses 
does not alter the appearance. Combined with stereoscopic projection surface, users can visualize holographic 
scenes and virtual objects above and below the surface. The 3D hand and object tracking component contains 
a depth sensor on top of the working area to recognize different hand / object shapes, and can track 
their positions in 3D. The system also contains 5.1 speakers embedded in the horizontal surface to create 
3D sound experience. The hand tracking system and the holographic visualization system are calibrated 
so user can directly interact with the virtual objects, and the display system can also augment physical 
objects placed on the surface. 3. Interaction The interactions are designed to be easy, intuitive and 
natural so that the system is not only a virtual workspace, but also can have wide range of applications 
and usable by the general public. Users can interact by 3D in-the-air gestures and direct 3D manipulations. 
In additional to standard multi-touch gestures such as pan, zoom rotate, this system includes a set of 
natural 3D interactions including binary gestures, continuous gestures, virtual joystick, direct 3D manipulation, 
and physics based user interfaces. Using head tracking information, the system also gives visual and 
audio feedbacks based on where the user is looking at. The system can augment personal device to present 
richer UI and interaction. Personal device can augment the table by giving user visual and haptic feedback. 
 4. User Experience The demo scenario allows users to take virtual tours of a city, experience 3D music 
hall, virtual shopping and dining; and multi­user collaborative scenario involving personal devices, 
such as manipulating personal device using the 3D interactive space and use personal device as virtual 
camera and make virtual bookmarks. 5. Conclusions We propose this new platform for holographic and interactive 
3D experience. With easy, intuitive and natural interactions the system is not only a virtual workspace, 
but also can have wide range of applications and usable by the general public. The potential applications 
include kiosks, virtual tourism, shopping, education, training, environment simulation, and data visualization. 
 References DITTLO, N., COURTOIS M, AND ORVAIN, E. 2010. 3D multitouch: when tactile tables meet immersive 
visualization technologies. In ACM SIGGRAPH 2010 Talks, Article 56, ACM BOSSAVIT et al. 2011. An immersive 
multitouch workspace. In ACM SIGGRAPH 2011 Emerging Technologies, Article 7, ACM Copyright is held by 
the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343478</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[TECHTILE toolkit]]></title>
		<subtitle><![CDATA[a prototyping tool for designing haptic media]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343478</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343478</url>
		<abstract>
			<par><![CDATA[<p>"TECHTILE" is a fundamental concept by combining "TECHnology" with "tacTILE" perception/expression. Our aim is to disseminate the haptic technologies as the third media in the field of art, design, and education and extends the conventional "multi-media" which consists of visual information and auditory information.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738854</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KMD, Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kouta@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P3738855</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SFC, Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738856</person_id>
				<author_profile_id><![CDATA[81100311640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SDM, Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nakatani@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P3738857</person_id>
				<author_profile_id><![CDATA[81503688044]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Soichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mihara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[YCAM InterLab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mhrs@ycam.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738858</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KMD, Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tachi@tachilab.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2331745</ref_obj_id>
				<ref_obj_pid>2331714</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Minamizawa, K., Kakehi, Y., Nakatani, M., Mihara, S., and Tachi, S. 2012. TECHTILE toolkit - A prototyping tool for design and education of haptic media. In Proceedings of ACM VRIC 2012, Laval, France.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TECHTILE toolkit - A prototyping tool for designing haptic media - Kouta Minamizawa Yasuaki Kakehi 
Masashi Nakatani Soichiro Mihara Susumu Tachi KMD, Keio University SFC, Keio University SDM, Keio University 
YCAM InterLab KMD, Keio University kouta@tachilab.org ykakehi@sfc.keio.ac.jp nakatani@tachilab.org mhrs@ycam.jp 
tachi@tachilab.org    Figure 1: TECHTILE toolkit Figure 2: Real-time haptic sharing Figure 3: Recorded 
haptic video Figure 4: TECHTILE workshops TECHTILE is a fundamental concept by combining TECHnology 
with tacTILE perception/expression. Our aim is to disseminate the haptic technologies as the third media 
in the field of art, design, and education and extends the conventional multi-media which consists of 
visual information and auditory information. There has been various haptic devices proposed so far, but 
most of them are still in emerging stage. To attract the interest of potential users of haptics such 
as designers, educators, and students, it is necessary to provide easy-to-make and easy-to-use haptic 
device. We then developed an introductory haptic device called "TECHTILE toolkit". In designing the toolkit, 
the most important point is that the toolkit can be used in favorite method for non-professional users. 
We then focused on the conventional method on auditory media. The sources of auditory sensation and tactile 
sensation are the same; vibration of an object generates a sequence of vibration of the air and perceived 
as sound, on the other hand, if the object were touched directly, it would be perceived as tactile sensation. 
When we create audio contents, we use microphone and speakers. The auditory sensation is recorded as 
a sequence of sound wave and we can easily edit using sound editors. If we want to share the audio contents 
on Internet, we can upload audio files (mp3, wmv, wav etc.) on YouTube or other content­sharing websites. 
Current version of the toolkit (Figure 1) composed of a haptic recorder (microphone), haptic reactors 
(small voice-coil actuators), and a signal amplifier that is optimized to present not only the zone of 
audibility (30-20000Hz) but also low frequency (less than 30Hz) vibrotactile sensation. Although this 
toolkit is intuitive to use and can be developed with low cost, it can deliver even higher-realistic 
haptic sensation than many other conventional haptic devices. Project website: http://techtile.org/ For 
example, if the user want to deliver the haptic sensation of pouring liquid in a cup , they just need 
to attach the haptic recorder on the bottom of a cup, and the haptic reactor on the bottom of another 
cup using scotch tape as shown in Figure 2. Then when they pour liquid in the cup with haptic recorder, 
the haptic sense from the liquid would be copied to another cup in real-time. It is also possible to 
record the haptic signal as an audio track of movie file through the USB port of the toolkit, and playback 
with video and sound as shown in Figure 3. It means that you can upload your original haptic contents 
on YouTube or Ustream to share the haptic content all over the world. We are currently holding a series 
of workshops in universities and science museums as shown in figure 4, so that we have confirmed that 
this device is suitable as an educational tool for learning possible applications of haptic design. The 
attendees, aged from 6 to 50 s, could easily understand how-to-use the toolkit in just 10 minutes. After 
that, they can create their original haptic artworks using their personal belongings such as papers, 
crayons, scissors, umbrellas, sandals and so on. In SIGGRAPH 2012 emerging technologies, we will demonstrate 
some examples of real-time haptic transmission, recorded haptic videos, and also we will hold rapid hands-on­workshops 
in our booth to show the essence of this toolkit. Acknowledgment This project is supported by JST-CREST 
Haptic Media project and Yamaguchi Center for Arts and Media. And this e-tech demonstration is supported 
by Prix Emerging Technologies award in Laval Virtual 2012. References MINAMIZAWA, K., KAKEHI, Y., NAKATANI, 
M., MIHARA, S., AND TACHI, S. 2012. TECHTILE toolkit -A prototyping tool for design and education of 
haptic media. In Proceedings of ACM VRIC 2012, Laval, France. Copyright is held by the author / owner(s). 
SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343479</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[TELESAR V]]></title>
		<subtitle><![CDATA[TELExistence surrogate anthropomorphic robot]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343479</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343479</url>
		<abstract>
			<par><![CDATA[<p>Telexistence is fundamentally a concept named for the general technology that enables a human being to have a real-time sensation of being at a place other than where he actually exist, and to interact with the remote environment [Tachi et al. 1990]. We have achieved human-like neck movements to visually interact with a remote object in 3 dimensional space through previous versions of TELESAR [Watanabe et al. 2007]. We introduce "TELESAR V" which maps a user's spinal neck, head and arm movements into a dexterous slave robot and allowing the operator to feel the robot's body as his own body through visual, auditory, kinesthetic and fingertip tactile [Sato et al. 2007] sensation. With TELESAR V, operator can perform teleoperations confidently with no prior practise.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738859</person_id>
				<author_profile_id><![CDATA[81435595273]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charith]]></first_name>
				<middle_name><![CDATA[Lasantha]]></middle_name>
				<last_name><![CDATA[Fernando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[charith@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P3738860</person_id>
				<author_profile_id><![CDATA[81335490502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Furukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738861</person_id>
				<author_profile_id><![CDATA[81504687773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tadatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kurogi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738862</person_id>
				<author_profile_id><![CDATA[81504686362]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kyo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirota]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738863</person_id>
				<author_profile_id><![CDATA[81421597185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738864</person_id>
				<author_profile_id><![CDATA[81331503619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Katsunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738865</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738866</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401029</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Minamizawa, K., Kamuro, S., Fukamachi, S., Kawakami, N., and Tachi, S. 2008. Ghostglove: haptic existence of the virtual world. In <i>ACM SIGGRAPH 2008 new tech demos</i>, ACM, New York, NY, USA, SIGGRAPH '08, 18:1--18:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278291</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sato, K., Minamizawa, K., Kawakami, N., and Tachi, S. 2007. Haptic telexistence. In <i>ACM SIGGRAPH 2007 emerging technologies</i>, ACM, New York, NY, USA, SIGGRAPH '07.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tachi, S., Arai, H., and Maeda, T. 1990. Tele-existence master-slave system for remote manipulation. In <i>Proceedings. IROS '90. IEEE International Workshop on</i>, 343--348 vol.1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278302</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Watanabe, K., Kawabuchi, I., Kawakami, N., Maeda, T., and Tachi, S. 2007. Torso: completion of egocentric telegnosis system. In <i>ACM SIGGRAPH 2007 emerging technologies</i>, ACM, New York, NY, USA, SIGGRAPH '07.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TELESAR V: TELExistence Surrogate Anthropomorphic Robot Charith Lasantha Fernando* , Masahiro Furukawa, 
Tadatoshi Kurogi, Kyo Hirota, Sho Kamuro, Katsunari Sato, Kouta Minamizawa, Susumu Tachi Graduate School 
of Media Design, Keio University, Japan Figure 1: (a) Glass ball pouring, (b) Playing Shogi (Japanese 
Chess), (c) Shuji (Japanese Calligraphy) 1 Introduction Telexistence is fundamentally a concept named 
for the general tech­nology that enables a human being to have a real-time sensation of being at a place 
other than where he actually exist, and to interact with the remote environment [Tachi et al. 1990]. 
We have achieved human-like neck movements to visually interact with a remote ob­ject in 3 dimensional 
space through previous versions of TELE-SAR [Watanabe et al. 2007]. We introduce TELESAR V which maps 
a user s spinal1, neck, head and arm movements into a dex­terous slave robot and allowing the operator 
to feel the robot s body as his own body through visual, auditory, kinesthetic and .ngertip tactile [Sato 
et al. 2007] sensation. With TELESAR V, operator can perform teleoperations con.dently with no prior 
practise. 2 System Overview As shown in Figure 2, TELESAR V consists of a Master(local) and Slave(remote) 
system. In slave side, a dexterous robot is developed with 54 DOF similar to an ordinary human s body 
structure, shape, size and joints. Apart from audio and video sensing using conven­tional methods, low 
frequency haptics are captured using a vision based tactile sensor and low to mid frequency haptics were 
captured using a omni-directional microphone based tactile sensor. In order to track fast and human-like 
smooth movements, an ungrounded motion capturing system is used in Master side. Robot s joint space is 
updated real-time with dynamic tracking data. Finger bending is captured upto 14 DOF on each hand using 
.ber-optic bend sensors. We also designed a HD (1280×800 pixels) Head Mounted Display to experience realistic 
wide angle .rst person view as seen from the Robot. Remote .ngertip proprioceptive sensation is reproduced 
using a combination of vertical and shearing stresses excreted by a motor driven belt mechanism [Minamizawa 
et al. 2008] while tactile sensation is reproduced using a voice coil based vibration actuator.  3 User 
Experience We will demonstrate a complete Telexistence operation involving head, body, arms and hands 
including transmission of haptic sensa­tion. Attendee can dive into the slave robot by wearing the HMD 
and gloves and look through robot s eyes. Due to the kinesthetic and tactile sensation, operator will 
feel that his head, body, arms * e-mail: charith@kmd.keio.ac.jp 1extension, .exion, lateral .exion, and 
axial rotation  MASTER (LOCAL) SLAVE (REMOTE) Figure 2: System Overview and .ngers are bound to the 
robot during picking up and pouring glass balls. As a result, they can perceive the slave robot as their 
own body. 4 Acknowledgement This project is supported by JST-CREST Haptic Media Project and collaborated 
with Kyokko Electric co.,Ltd. References MINAMIZAWA, K., KAMURO, S., FUKAMACHI, S., KAWAKAMI, N., AND 
TACHI, S. 2008. Ghostglove: haptic existence of the virtual world. In ACM SIGGRAPH 2008 new tech demos, 
ACM, New York, NY, USA, SIGGRAPH 08, 18:1 18:1. SATO, K., MINAMIZAWA, K., KAWAKAMI, N., AND TACHI, S. 
2007. Haptic telexistence. In ACM SIGGRAPH 2007 emerging technologies, ACM, New York, NY, USA, SIGGRAPH 
07. TACHI, S., ARAI, H., AND MAEDA, T. 1990. Tele-existence master-slave system for remote manipulation. 
In Proceedings. IROS 90. IEEE International Workshop on, 343 348 vol.1. WATANABE, K., KAWABUCHI, I., 
KAWAKAMI, N., MAEDA, T., AND TACHI, S. 2007. Torso: completion of egocentric teleg­nosis system. In ACM 
SIGGRAPH 2007 emerging technologies, ACM, New York, NY, USA, SIGGRAPH 07. Copyright is held by the author 
/ owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343480</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Tensor displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343480</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343480</url>
		<abstract>
			<par><![CDATA[<p>We introduce tensor displays: a family of light field displays comprising all stacked display architectures employing light-attenuating layers illuminated by uniform or directional backlighting (i.e., any low-resolution light field emitter). Tensor displays include the capability to time-multiplex content across frames on each of the layers to improve image quality. We show that the light field emitted by an <i>N</i>-layer, <i>M</i>-frame tensor display can be represented by an <i>N</i><sup>th</sup>-order, rank-<i>M</i> tensor. In a related technical paper submission, we use this representation to introduce a unified optimization framework, based on nonnegative tensor factorization (NTF), encompassing all tensor display architectures (see supplementary supporting document). In this emerging technologies demonstration, we show both static, printed tensor displays, and dynamic LCD-based systems, providing wide field-of-view, bright, high-resolution, glasses-free 3D display.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738867</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mhirsch@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P3738868</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738869</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738870</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1866164</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lanman, D., Hirsch, M., Kim, Y., and Raskar, R. 2010. Content-adaptive parallax barriers: Optimizing dual-layer 3D displays using low-rank light field factorization. <i>ACM Trans. Graph. 29</i>, 163:1--163:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024220</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lanman, D., Wetzstein, G., Hirsch, M., Heidrich, W., and Raskar, R. 2011. Polarization fields: Dynamic light field display using multi-layer LCDs. <i>ACM Trans. Graph. 3</i>, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964990</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wetzstein, G., Lanman, D., Heidrich, W., and Raskar, R. 2011. Layered 3D: Tomographic image synthesis for attenuation-based light field and high dynamic range displays. <i>ACM Trans. Graph. 30</i>, 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tensor Displays Matthew Hirsch* Douglas Lanman Gordon Wetzstein Ramesh Raskar MIT Media Lab MIT Media 
Lab MIT Media Lab MIT Media Lab  Figure 1: Wide .eld of view glasses-free 3D imagery on tensor displays. 
(Left) A new family of light .eld displays (tensor displays), comprising stacks of light-attenuating 
layers (e.g., LCDs). Rapid temporal modulation of the layers and directional backlighting allows large 
separation between viewers. (Center) Target light .eld view (rendered), and tensor display (photograph). 
Layers are shown to the right of each photograph. Rows depict perspectives seen to the left and right 
of the display. (Right) Directional backlight tensor display hardware. A rail and clip system accurately 
positions an LCD in front of a directional backlight. Abstract We introduce tensor displays: a family 
of light .eld displays comprising all stacked display architectures employing light­attenuating layers 
illuminated by uniform or directional backlight­ing (i.e., any low-resolution light .eld emitter). Tensor 
displays include the capability to time-multiplex content across frames on each of the layers to improve 
image quality. We show that the light .eld emitted by an N-layer, M-frame tensor display can be rep­resented 
by an Nth-order, rank-M tensor. In a related technical paper submission, we use this representation to 
introduce a uni­.ed optimization framework, based on nonnegative tensor factor­ization (NTF), encompassing 
all tensor display architectures (see supplementary supporting document). In this emerging technolo­gies 
demonstration, we show both static, printed tensor displays, and dynamic LCD-based systems, providing 
wide .eld-of-view, bright, high-resolution, glasses-free 3D display. 1 Vision Consumer stereoscopic displays 
have been enabled by the intro­duction of high-speed LCDs and inexpensive shutter glasses. Man­ufacturers 
are beginning to offer automultiscopic (glasses-free) 3D displays, primarily based on the century-old 
concepts of parallax barriers and integral imaging. Products based on these methods have met limited 
commercial success due to intrinsic limitations of the approaches, particularly narrow .elds of view 
and reduced spa­tial resolution. We are inspired to address the limitations of exist­ing automultiscopic 
displays by taking advantage of three emerging display technologies: multilayer panels, high-speed temporal 
mod­ulation, and directional backlighting, in combination with ef.cient high-speed GPU processing. 2 
Tensor Display Framework Our tensor display framework is the .rst to allow joint multilayer, multiframe 
light .eld decompositions, signi.cantly reducing arti­facts observed with prior multilayer-only and multiframe-only 
de­compositions [Lanman et al. 2010; Lanman et al. 2011; Wetzstein et al. 2011]; it is also the .rst 
optimization method for designs com­ bining multiple layers with directional backlighting. *e-mail: mhirsch@media.mit.edu 
 3 Prototypes We show two prototype tensor displays. Static scenes can be cre­ated cheaply using stacked 
layers of printed transparent sheets. By constructing a dynamic, recon.gurable prototype using modi.ed 
LCD panels and a custom integral imaging backlight, we improve display quality through time-multiplexing, 
and show video content. Our ef.cient, GPU-based NTF implementation enables interactive applications. 
Prototype tensor displays reveal practical architec­tures with greater depths of .eld, wider .elds of 
view, and thinner form factors, compared to prior automultiscopic displays. 4 Conclusion Automultiscopic 
displays have not found widespread consumer adoption. While compelling multiview content is .rst necessary, 
long-standing optical and algorithmic limitations must be conclu­sively resolved. Viable solutions must 
preserve the thin form fac­tors, low power consumption, and high resolution of modern dis­plays, using 
near-term, mass-market technology. Tensor displays provide the .rst framework combining the advantages 
of three key multiview display trends: multilayer panels, high refresh rates, and directional backlighting. 
This framework exempli.es emerg­ing computational displays, wherein the display architecture and encoding 
algorithm are jointly optimized to maximize optical and computational ef.ciency.  References LANMAN, 
D., HIRSCH, M., KIM, Y., AND RASKAR, R. 2010. Content-adaptive parallax barriers: Optimizing dual-layer 
3D displays using low-rank light .eld factorization. ACM Trans. Graph. 29, 163:1 163:10. LANMAN, D., 
WETZSTEIN, G., HIRSCH, M., HEIDRICH, W., AND RASKAR, R. 2011. Polarization .elds: Dynamic light .eld 
display using multi-layer LCDs. ACM Trans. Graph. 3, 1 9. WETZSTEIN, G., LANMAN, D., HEIDRICH, W., AND 
RASKAR, R. 2011. Layered 3D: Tomographic image synthesis for attenuation-based light .eld and high dynamic 
range displays. ACM Trans. Graph. 30, 1 11. Copyright is held by the author / owner(s). SIGGRAPH 2012, 
Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343481</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Turn]]></title>
		<subtitle><![CDATA[a virtual pottery by real spinning wheel]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343481</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343481</url>
		<abstract>
			<par><![CDATA[<p>Is it possible to transfer real-world sculpting expertise to a virtual space? Through digital pottery, the answer is yes. Digital pottery is a collection of systems that makes it convenient to make pottery in 3D space. In this study, we present a natural & tangible user interface system that fluently connects users' real-world sculpting experience to virtual pottery making. Introducing a real spinning wheel, we could extend the physical concept of making pottery into 3D space without the burden of bridging the gaps between real & virtual worlds. As a result, we could also retain its unique characteristics of pottery by preserving its essential mechanism of shape formation: Turn.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738871</person_id>
				<author_profile_id><![CDATA[81504684716]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sungmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sungmins@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3738872</person_id>
				<author_profile_id><![CDATA[81421594806]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yunsil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yunsil@snu.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P3738873</person_id>
				<author_profile_id><![CDATA[81421599972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hyunwoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[savoy@snu.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>261171</ref_obj_id>
				<ref_obj_pid>261135</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kameyama, K.: Virtual clay modeling system. In: <i>Proceedings of the ACM Symposium on Virtual Reality Software and Technology</i> pp, 227--234 (1997)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[W.E.Lorenson and H. E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. <i>Computer Graphics</i>, 21(4):163--169, July 1987]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 C:\Users\Vonham\Desktop\1.jpg http://everyware.kr/portfolio/contents/11_turn/imgs/best6.jpg Turn: A 
Virtual Pottery by Real Spinning Wheel Sungmin Cho*       Yunsil Heo*     &#38; Hyunwoo Bang* 
New Media Lab  Department of Crafts &#38; Design   New Media Lab Mechanical &#38;   School of Arts 
    Mechanical &#38; Aerospace Engineering  Seoul National University    Aerospace Engineering 
Seoul National University     Seoul National University  Figure 1(a)Virtual pottery making (b)Image 
seen by the depth camera and rendition of virtual clay (c)output samples 1. Introduction Is it possible 
to transfer real-world sculpting expertise to a virtual space? Through digital pottery, the answer is 
yes. Digital pottery is a collection of systems that makes it convenient to make pottery in 3D space. 
In this study, we present a natural &#38; tangible user interface system that fluently connects users' 
real-world sculpting experience to virtual pottery making. Introducing a real spinning wheel, we could 
extend the physical concept of making pottery into 3D space without the burden of bridging the gaps between 
real &#38; virtual worlds. As a result, we could also retain its unique characteristics of pottery by 
preserving its essential mechanism of shape formation: Turn. 2. Interaction A wooden rotating wheel is 
placed in front of an LED display panel. When you spin the wheel, an imaginary spinning wheel in the 
virtual world also revolves correspondingly. As you move your other hand over the wheel, you can see 
a digital rendition of clay that smears out from the tip of your fingers like toothpaste on the screen(Figure1-a). 
As we designed this virtual space, you can put, add, subtract &#38; modify clay at anywhere you want 
without considering real world constraints like gravity. This gives you another level of freedom to create 
unique shapes shown on the images above while preserving the tangibility of real world pottery. 3. Implementation 
The Kinect sensor from Microsoft installed on top of the display panel traces your hand in 3D space. 
The position of your hand is calibrated in real time based on the relative position from the top plate 
of the spinning wheel. A wireless mouse is installed upside down beneath the spinning plate sensing the 
speed of rotation with its optical sensor without giving any friction to the moving parts. Raw depth 
information obtained from the Kinect sensor is processed &#38; transformed to construct 3D points cloud 
above the imaginary spinning wheel in the virtual space. A graphical sculpting tool gizmo gives you the 
clue where your hand is at and what you re doing now. The imaginary cylinder above the spinning wheel 
constructs a control volume that limits the span of your gestural inputs where a volumetric rendering 
of processed points cloud is drawn as a sculpted pottery(Figure1-b). This volumetric data is computed 
as iso-surfaces by employing the marching cubes algorithms for 3D arrays. This algorithm allows the final 
3D model to be a free-from surface whose surface continuity is maintained. We used OpenGL with GLSL for 
the 3D visualizations. 4. Conclusion In this study, we propose a digital pottery system that successfully 
reflects real-world gestures to their virtual counterparts. By introducing a tangible user interface 
that extends real-world experiences, "Turn" offers a natural, intuitive and interactive virtual 3D modeling 
method. Acting out the sculpting experience on virtual space, users can easily create organic forms of 
virtual pottery that can hardly be made in real-world circumstances (Figure1-c). In addition, this spinning-wheel 
metaphor can be extended to various design interfaces including architectural modeling, product designs 
and fine art sculptures. Digital pottery is a good example for utilizing our technique but it is not 
the last stop of this interface. Turn opens a new way of modeling method feasible for making intuitive 
virtual 3D modeling environments. It will offer a revolutionary way to extend users expertise of real-world 
sculpting to its virtual counterpart. 5. Reference Kameyama, K.: Virtual clay modeling system. In: Proceedings 
of the ACM Symposium on Virtual Reality Software and Technology pp, 227-234 (1997) W.E.Lorenson and H.E.Cline. 
Marching cubes: A high resolution 3d surface construction algorithm. Computer Graphics, 21(4):163-169,July 
1987 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343482</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Ungrounded haptic rendering device for torque simulation in virtual tennis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2343456.2343482</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343482</url>
		<abstract>
			<par><![CDATA[<p>We present an ungrounded haptic device to render a variety of tennis ball impact accelerations and torques in an interactive virtual tennis system. Impact forces are common in real life, particularly in sports. Our daily experiences shows that our sensory system can discern the variations in these resulting strong vibrations. Common eccentric mass vibration motor-based haptics cannot create the magnitudes of the force and torque, or the variations in frequencies and waveforms, or the immediacy of such impacts[Yao and Hayward 2010]. Our device overcomes the above deficiencies and increases the richness of user experience in virtual reality.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738874</person_id>
				<author_profile_id><![CDATA[81474704294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fong]]></first_name>
				<middle_name><![CDATA[Wee]]></middle_name>
				<last_name><![CDATA[Teck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, A*STAR, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wtfong@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P3738875</person_id>
				<author_profile_id><![CDATA[81492640669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chin]]></first_name>
				<middle_name><![CDATA[Ching]]></middle_name>
				<last_name><![CDATA[Ling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, A*STAR, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738876</person_id>
				<author_profile_id><![CDATA[81100397017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Farzam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farbiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, A*STAR, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738877</person_id>
				<author_profile_id><![CDATA[81448594993]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Huang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhiyong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, A*STAR, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2073365</ref_obj_id>
				<ref_obj_pid>2073304</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fong, W., Cher, J., Farbiz, F., and Huang, Z. 2011. Variable frequency 60-g haptic renderer for virtual tennis simulation. In <i>SIGGRAPH Asia 2011 Posters</i>, 54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yao, H., and Hayward, V. 2010. Design and analysis of a recoil-type vibrotactile transducer. <i>The Journal of the Acoustical Society of America 128</i>, 619--627.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ungrounded haptic rendering device for torque simulation in virtual tennis Fong Wee Teck* Chin Ching 
Ling Farzam Farbiz Huang Zhiyong Institute for Infocomm Research, A*STAR, Singapore  (a) Racket prototype 
(b) A-shape vibration (c) Device generated torque (d) Device generated torque in virtual tennis element 
for ball impact for racket twisting Figure 1: The haptic device shown in (a) utilizes the A-shape vibration 
element and high-power push-pull solenoids shown in (b). The large moment of inertia of the racket rim 
makes it an effective counterweight to generate torque at the handle, as shown (c) and (d) 1 Introduction 
We present an ungrounded haptic device to render a variety of ten­nis ball impact accelerations and torques 
in an interactive virtual tennis system. Impact forces are common in real life, particularly in sports. 
Our daily experiences shows that our sensory system can discern the variations in these resulting strong 
vibrations. Com­mon eccentric mass vibration motor-based haptics cannot create the magnitudes of the 
force and torque, or the variations in frequencies and waveforms, or the immediacy of such impacts[Yao 
and Hay­ward 2010]. Our device overcomes the above de.ciencies and in­creases the richness of user experience 
in virtual reality. Novel features are the A-shape vibration element actuated using dual push-pull solenoids, 
and utilizing the inertia of the racket rim to act as an counterweight to generate torque at the handle. 
The rim has low mass, but acting like a ring, it has a large moment of inertia, which make it as an effective 
counterweight that is also light. Compared to our previous work [Fong et al. 2011], the ability to render 
high accelerations of different frequencies and waveforms is extended to rendering torque at the handle. 
 2 Haptic Device Design and Performance The A-shape vibration element, constructed using FR4 .berglass, 
can be considered as two of the haptic devices described in [Fong et al. 2011] forming the slanted sides, 
and joined by a horizon­tal beam coupling. Part of the handle of the racket, shown in Figure 1(b), houses 
the above vibration element, while the rim is mounted to the horizontal beam of A. Therefore, the racket 
is divided into two .exibly coupled moving masses. Each slanted side of A can render large accelerations 
of varying frequencies and waveforms[Fong et al. 2011]. Actuating the slanted beams bends the rim relative 
to the handle, which generates a torque laterally about the handle that simulates ball impacts. Additionally, 
actu­ating the beams out of phase adds a twisting effect that simulates off-center line impacts, where 
the racket twists in the player s grip. By using the inertia of the rim, and bending the A-shape vibrating 
element appropriately, it is possible to generate fairly strong torque up to 0.4Nm while swinging in 
the air. This is without any anchor to the ground or use of large mass. Device generated torques are 
shown in Figure 1, and those typical of ball drops from a height of *e-mail: wtfong@i2r.a-star.edu.sg 
two meters are shown in Figure 2. The device is actuated using a 70Hz signal, lasting 14ms. These measurements 
are measured by clamping the handles of the prototype and real racket, respectively, to the FT9039 multi-axis 
torque sensor from Industrial Automation. The data was sampled at 200Hz. We can observe that the the 
de­vice generated ball impact torque is two orders of magnitude lower, while that for racket twisting 
is within one order of magnitude. As with accelerations shown in [Fong et al. 2011], it takes less than 
10ms to use the recoil effect to build up the maximum torque, which compares well to hundreds of milliseconds 
that rotary motors take to spin up. Ball impact torque Racket twisting torque Figure 2: The torques 
measured for a typical tennis ball drop from a height of two meters onto a real racket. Though the torques 
generated are lower than real tennis, visual ob­servation and videos show that they are suf.cient to 
de.ect and twist the device while held in the hand, giving the sensation that it is impacted. We are 
conducting perceptual studies to determine how far our device is perceptually, from real impacts, particularly 
in the presence of strong visual and auditory simulations in virtual reality. References FONG, W., CHER, 
J., FARBIZ, F., AND HUANG, Z. 2011. Variable frequency 60-g haptic renderer for virtual tennis simulation. 
In SIGGRAPH Asia 2011 Posters, 54. YAO, H., AND HAYWARD, V. 2010. Design and analysis of a recoil-type 
vibrotactile transducer. The Journal of the Acoustical Society of America 128, 619 627. Copyright is 
held by the author / owner(s). SIGGRAPH 2012, Los Angeles, California, August 5 9, 2012. ISBN 978-1-4503-1435-0/12/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
</content>
</proceeding>
