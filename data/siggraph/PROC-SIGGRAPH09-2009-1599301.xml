<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/03/2009</start_date>
		<end_date>08/07/2009</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[New Orleans]]></city>
		<state>Louisiana</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>1599301</proc_id>
	<acronym>SIGGRAPH '09</acronym>
	<proc_desc>SIGGRAPH '09: Posters</proc_desc>
	<conference_number>2009</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2009</copyright_year>
	<publication_date>08-03-2009</publication_date>
	<pages>103</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>SIGGRAPH 2009 Posters are presented and discussed in scheduled sessions where poster authors meet with attendees. These sessions provide a low-key venue for participants to meet and converse with people who share their interests.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2009</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>1599302</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<display_no>1</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Animating character images in 3D space]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599302</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599302</url>
		<abstract>
			<par><![CDATA[<p>In this extended abstract, we present a system that allows the user to animate character images in 3D space by applying an existed 3D character model with motion data. The character model with skeleton rigged is used as a template model to fit the silhouette of the character image. After assigning some corresponding points between the character image and template model, the system then fits the model to the image and transfer the colors and patterns of the image to the model as the textures. Finally, the user can apply any motion data to animate the fitted 3D character model in 3D space.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621298</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621299</person_id>
				<author_profile_id><![CDATA[81442594574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shih-Chiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621300</person_id>
				<author_profile_id><![CDATA[81335490937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shuen-Huei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digimax]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621301</person_id>
				<author_profile_id><![CDATA[81100539710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073323</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Moscovich, T., and Hughes, J. F. 2005. As-rigid-as-possible shape manipulation. <i>ACM Trans. Gr. 24</i>, 3, 1134--1141. (SIGGRAPH 2005 Conf. Proc.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animating Character Images in 3D Space Bing-YuChen * Shih-ChiangDai Shuen-HueiGuan TomoyukiNishita§ 
NationalTaiwanUniversity NationalTaiwanUniversity Digimax TheUniversity ofTokyo Figure 1: From left 
to right: the originalinput image;the template character model after .tting to the character image;the 
character model texturedbythecharacterimage;themotiondataisapplied toanimatethecharactermodel. 1 Introduction 
In this extended abstract, we present a system that allows the user to animate character images in 3D 
space by applying an existed 3D character model with motion data. The character model with skeleton rigged 
is used as a template model to .t the silhouette of the character image. After assigning some corresponding 
points between the character image and template model, the system then .ts themodeltotheimageandtransfer 
thecolorsandpatternsofthe image to the model as the textures. Finally, the user can apply any motiondata 
toanimatethe.tted3D charactermodelin3D space. 2 Overview The input of oursystem isan imageand askeletonrigged3D 
char­acter model and a 2D character image. The system .rst samples somepointson thecontourofthecharacter 
imageas thevertices in 3D space, and the points form a close loop as a contour loop C. Then, thesystemroughlygeneratessilhouetteverticesfrom 
the3D charactermodel,which alsoformacloseloop asa silhouetteloop S. Then, our model .tting algorithm 
is applied to .t S to C while considering the original shape of the 3D character model. Finally, wecanapplyany 
motiondata tothe3D charactermodel oradd any visual effect in3D space. 3 Model Fitting Algorithm Given 
a contour loop C extracted from the input character image I and a silhouette loop S = {si|i = 1, ··· 
, n}, sn = s1 with n sil­houette vertices of the 3D character model T, we have to match S with C. The 
user .rst drag m = n vertices of the silhouette loop S'= {sp( j)|j = 1, ··· , m}. S, sp(m)= sp(1)= s1 
to their corre­ sponding points of C manually, where p( j)= i denotes an index mapping from the dragged 
vertices sp( j) to the silhouette vertices si, and s= s1. Then, the system .ts the remaining n-m sil­houette 
vertices to C automatically while satisfying the constraint: sisi+1 = sp( j)sp( j+1) /( p( j+ 1) -p( 
j)), p( j) =i= p( j+ 1). p(1) After .tting S to C,wehave todeformtheshapeof T to match that of I while 
satisfying the constraint(i.e., S'). We can .rst keep the * e-mail: robin@ntu.edu.tw e-mail:jeffrey@cmlab.csie.ntu.edu.tw 
e-mail: drake.guan@gmail.com §e-mail: nis@is.s.u-tokyo.ac.jp z coordinate .xed and only consider the 
3D model T as a2D mesh Txy. Then, as-rigid-as-possible shape manipulation[Igarashi et al. 2005]isused 
todeforme Txy with constrained S'. Through a sparse linear solver, we can .t the remaining vertices of 
Txy within a sec. After .tting the skin(only x-y coordinates) of T, we still have to .t the original 
skeleton of T corresponding to the .tted skin. Beforeskeleton .tting,we .rstprojectT andits skeleton 
joints onto x-y plane(i.e., Txy), andrecord each jointpositionby barycentric coordinate of the triangle 
which contains the joint. If there exists several triangles contain the same joint, we choose the one 
nearest tothejointintheoriginal3D spaceandbelongs tothatbone. After .tting the skin and skeleton, we 
have transformed T to .t I by adjusting its projected x-y coordinates in2D space(i.e., Txy). However, 
thequantityofthe third(z)coordinate(or thickness) must alsoberevised togenerateaconvincing.tted3D charactermodel. 
In our experience, we observed that the distance between the bone and the skin is highly correlated to 
the average distance between the silhouette vertices si . S and the bone bk . B they belong to. Hence, 
for each bk we record the average distance dk to its nearest silhouette loop Sbefore the model.tting. 
Ifthe vertexsi belongs to several bones, we compute the average with its bone weight .ik as dk .ikdik/(n-1), 
dik = ||si-bk||, where n is the number of = .n-1 i si .S and .ik = 0 if si and bk have no binding relationship. 
After skin .tting, we can then compute the new average distance d'by k usingthenewpositionof si. Then, 
the new z value of each vertex is scaledbytheratio: .k .vk(d'/dk), where .vk is thebinding weight kbetween 
the vertex v .T and thebone bk.  4 Result Our system is implemented in C++ with OpenGL, and the char­acter 
deformation is performed by following the standard linear blendskinning(LBS) method. The motiondata isdownloadedfrom 
http://mocap.cs.cmu.edu/. Fig. 1 shows the result. The computation time is interactive except inpainting 
and silhouette cut out.Theuserinteraction timeisabout10 min.foratrained user.  References IGARASHI, 
T., MOSCOVICH, T., AND HUGHES, J. F. 2005. As­rigid-as-possible shape manipulation. ACM Trans. Gr. 24, 
3, 1134 1141.(SIGGRAPH2005Conf.Proc.). Copyright is held by the author / owner(s). SIGGRAPH 2009, New 
Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599303</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<display_no>2</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Animating lip-sync speech faces by dominated animeme models]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599303</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599303</url>
		<abstract>
			<par><![CDATA[<p>Speech animation is traditionally considered as important but tedious work for most applications, because the muscles on the face are complex and dynamically interacting. In this paper, we introduce a framework for synthesizing a 3D lip-sync speech animation by a given speech sequence and its corresponding texts. We first identify the representative key-lip-shapes from a training video that are important for blend-shapes and guiding the artist to create corresponding 3D key-faces (lips). The training faces in the video are then cross-mapped to the crafted key-faces to construct the Dominated Animeme Models (DAM) for each kind of phoneme. Considering the coarticulation effects in animation control signals from the cross-mapped training faces, the DAM computes two functions: polynomial-fitted animeme shape functions and corresponding dominance weighting functions. Finally, given a novel speech sequence and its corresponding texts, a lip-sync speech animation can be synthesized in a short time with the DAM.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621302</person_id>
				<author_profile_id><![CDATA[81440599610]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fu-Chung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621303</person_id>
				<author_profile_id><![CDATA[81442609486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yu-Mei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621304</person_id>
				<author_profile_id><![CDATA[81421598073]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tse-Hsien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621305</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621306</person_id>
				<author_profile_id><![CDATA[81335490937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shuen-Huei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digimax]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cohen, M. M., and Massaro, D. W. 1993. Modeling coarticulation in synthetic visual speech. In <i>Computer Animation 1993 Conference Proceedings</i>, 139--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566594</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ezzat, T., Geiger, G., and Poggio, T. 2002. Trainable videorealistic speech animation. <i>ACM Transactions on Graphics 21</i>, 3, 388--398. (SIGGRAPH 2002 Conference Proceedings).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animating Lip-Sync Speech Faces by Dominated Animeme Models Fu-ChungHuang* Yu-MeiChen Tse-HsienWang 
Bing-YuChen Shuen-HueiGuan *University ofCaliforniaatBerkeley NationalTaiwanUniversity Digimax 'M''M'+'B' 
+'R' 'EH''EH' Figure1:Theresultof speakingthe6phonemesoftheword - member .Notethat althoughthe1st and3rdphonemesarethesame,thelips 
shapesaredifferentdue tothe coarticulation effect. 1 Introduction Speech animation is traditionally considered 
as important but te­dious work for most applications, because the muscles on the face are complex and 
dynamically interacting. In this paper, we intro­duceaframeworkforsynthesizing a3D lip-syncspeech animation 
by a given speech sequence and its corresponding texts. We .rst identifytherepresentativekey-lip-shapesfroma 
training videothat areimportantforblend-shapesandguiding theartist tocreatecor-responding3Dkey-faces(lips). 
The trainingfaces in thevideoare then cross-mapped to the crafted key-faces to construct the Dom­inatedAnimemeModels(DAM) 
for eachkind ofphoneme. Con­sideringthecoarticulationeffects inanimationcontrol signalsfrom the cross-mapped 
training faces, the DAM computes two func­tions: polynomial-.tted animeme shape functions and correspond­ing 
dominance weighting functions. Finally, given a novel speech sequence and its corresponding texts, a 
lip-sync speech animation canbesynthesizedinashort timewiththeDAM. 2 Overview Asshown inFigure2, theframework 
canbedivided into2 subsys­tems. The .rstone learns thephoneme-animemerelationshipwith facecross-mappingfunctionality,sincethe 
trainingfaceisdifferent from the targetface. BothsubsystemsuseSPHINX-II asphoneme alignment tool.TheproposedDAM 
takesa time-aligned script and animationcontrol signalsastheinput.Intheend of training,DAM learns theanimemeshapefunctionof 
theanimationcontrol signals for phonemes, and the dominance weighting functions for inter­ference. In 
the second subsystem, the DAM generates the anima­tion control signals from an arbitrary speech, which 
can be used to generate the output animation. The major contribution of our framework is how the DAM 
learns the animation control signals for each phoneme and the interference (the coarticulation effect) 
amongthem. aligned scripts texts lip-sync speech anima on Figure2:System .owchart.Thephoneme-animemerelationship 
is learnedinthe trainingphase,andthespeech animationisgenerated in thesynthesisphase.  3 Dominated Animeme 
Model To modelthe animation controlsignals across time, we use a mathe­matical approach todescribetheanimemesandinterferenceamong 
 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008 them. The animeme, approximated by an M-order polynomial, is the shapefunctionfor 
a sequence of animation control signalsfor a speci.cphoneme. The interferenceamong animemes,called coar­ticulation 
effects, are modeled through weighted Gaussian domi-nancefunction.Foraresulting animeme, theoutput signalsare 
the convolution of its animeme and dominance functions. For a sen­tence composing several animemes, the 
signals are simply the sum­mation of these convoluted signals. In mathematical expression, at i time 
tthe control signal wi is the summation of J animemes in a sentence,givenby: JM i imi w = . Dj(tj ). 
aj (tj)m, j=im=0 im where tj is the local-frame transformed time ti, and aj is the M­ order polynomial 
coef.cient for j-th animeme. The dominance i function Dj(t) isgivenby: j {} i (t ij -µ j ) Dj(tj)= exp, 
(dj ×s j )2 + e where µ j is the center time of the occurring animeme, dj is the duration, and s j is 
the animeme speci.c constant that controls the span of in.uence. 4 Result We compare our result, shown 
in Figure 3, with other methods by Cohen-Massaro [1993] and Multi-dimensional Morphable Model (MMM)[Ezzatetal.2002]along 
with theoriginal animationcon­trol signals. The resultbyCohen-Massarohaspoorly modeled span and low synthesized 
control signals that lead to overly smoothed animation.MMMproducesgood span timingbutstillsuffersfrom 
lowcontrol signals.Notice that ourmodel not onlysynthesizesbet­tersignal values,butalsoappropriatelypreserves 
thespanof each occurrence. In our accompanying video, our methodpreservesbet­ter coarticulation effects 
and characteristicsfrom each animeme. 1.2 1 0.8 0.6 0.4 0.2 0 -0.2 Figure3: Comparison withdifferentstrategies. 
L2 normsforDAM, Cohen-Massaro, andMMM are0.0524,0.0858, and0.0793. References COHEN, M. M., AND MASSARO, 
D. W. 1993. Modeling coartic­ulation in synthetic visual speech. In Computer Animation 1993 Conference 
Proceedings,139 156. EZZAT, T., GEIGER, G., AND POGGIO, T. 2002. Trainable vide­orealistic speech animation. 
ACM Transactions on Graphics 21, 3,388 398.(SIGGRAPH2002ConferenceProceedings).  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599304</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<display_no>3</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[CG animation for piano performance]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599304</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599304</url>
		<abstract>
			<par><![CDATA[<p>Technologies recreating piano performance in the form of CG animation are eagerly anticipated by people working in various fields, such as content production, music education, etc. Nonetheless, much of the past research has dealt with the mechanical finger movements in piano practice support systems and performance support GUIs, etc. and there has been little research recreating the reality of finger movements. We are promoting research into the analysis and CG expression of realistic and natural piano fingering. This paper describes the following aspects of this research program: (i) measurement of piano fingering using motion capture technology, (ii) generation of a CG animation of fingering using offline/realtime rendering, and (iii) automatic generation of fingering using optimized algorithms. And finally we will introduce examples in which the fingering data created in (i) is used in TV animation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Performing arts (e.g., dance, music)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621307</person_id>
				<author_profile_id><![CDATA[81442618671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nozomi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621308</person_id>
				<author_profile_id><![CDATA[81442618759]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyazono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621309</person_id>
				<author_profile_id><![CDATA[81442603398]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Omori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621310</person_id>
				<author_profile_id><![CDATA[81442605365]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621311</person_id>
				<author_profile_id><![CDATA[81442612208]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shinichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Furuya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621312</person_id>
				<author_profile_id><![CDATA[81100388990]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Haruhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Katayose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621313</person_id>
				<author_profile_id><![CDATA[81536426856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Hiroyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miwa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621314</person_id>
				<author_profile_id><![CDATA[81100522455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Noriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alia Al Kasimi, Eric Nichols and Christopher Raphael (2005). Automatic fingering system (AFS). ISMIR 2005 poster presentation, ISMIR 2005 Automatic Fingering System Kasimi Nichols Raphael.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Katayose, H.&amp;Okudaira, K. (2004). iFP A music interface using an expressive performance template, In EC2004, Lecture Notes in Computer Science 3166. 529--540, Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CG Animation for Piano Performance Nozomi Kugimoto Rui Miyazono Kosuke Omori Takeshi Fujimura Haruhiro 
Katayose Hiroyoshi Miwa* Noriko Nagata Kwansei Gakuin University 1 Introduction Technologies recreating 
piano performance in the form of CG ani­mation are eagerly anticipated by people working in various .elds, 
such as content production, music education, etc. Nonetheless, much of the past research has dealt with 
the mechanical .nger movements in piano practice support systems and performance sup­port GUIs, etc. 
and there has been little research recreating the real­ity of .nger movements. We are promoting research 
into the analy­sis and CG expression of realistic and natural piano .ngering. This paper describes the 
following aspects of this research program: (i) measurement of piano .ngering using motion capture technology, 
(ii) generation of a CG animation of .ngering using of.ine/real­time rendering, and (iii) automatic generation 
of .ngering using optimized algorithms. And .nally we will introduce examples in which the .ngering data 
created in (i) is used in TV animation. 2 Measurement of piano .ngering using mo­tion capture and of.ine 
rendering First, we obtain the .nger movements during a piano performance using motion capture technology. 
We attach optical markers to the joints of the .ngers and measure the 3-dimensional location coor­dinates. 
Next we apply the obtained motion capture data to the CG model of the hand with 16 joints and bones. 
The motion of the piano keyboard is created from performed music data. By linking the notes and numbers 
of the MIDI to the movements of the each key prior to the motion capture photography we are able to gener­ate 
keyboard movements that correspond to the MIDI data. In the of.ine rendering stage we produced the model 
of the hand and the model of the piano using CG software (Figure 1). 3 Real-time rendering and its application 
to music performance interface We implemented a real time rendering program using directX based on the 
obtained motion capture data. Furthermore, we achieved the synchronous processing of music performance 
interface iFP, which works by using motion capture data, interactively obtaining the tempo and the intensity 
(Figure 2). 4 Automatic Generation of Optimal Fingering from a Musical Score Aside from the researches 
mentioned above, we are developing a system that automatically generates an animation of a pianist play­ing 
piano from a musical score . This system is mainly composed of the function determining an optimal piano 
.ngering and the func­tion determining the trajectories of all control points in the bony frameworks 
of hands based on the .ngering. We designed ef.cient algorithms to realize these functions from the view 
of the discrete and the continuous optimization theory. As a result, our system generates plausible and 
reasonable CG animations for the musical scores of some famous piano works. *e-mail:miwa@kwansei.ac.jp 
e-mail:nagata@kwansei.ac.jp  5 Animation Production Using Motion Cap­ture Data 6 Figure 3: Examples 
of hand­applied animation based on the Figure 2: Realtime rendering of obtained motion capture data. 
images c Tomoko Ninomiya/Kodansha Ltd./Nodame Cantabile 2 Produc­ 7 Reference tion Committee. [1] Alia 
Al Kasimi, Eric Nichols and Christopher Raphael (2005). Automatic .ngering system (AFS). ISMIR 2005 poster 
presenta­tion, ISMIR 2005 Automatic Fingering System Kasimi Nichols Raphael.pdf. [2] Katayose, H. &#38; 
Okudaira, K. (2004). iFP A music interface us­ing an expressive performance template, In EC2004, Lecture 
Notes in Computer Science 3166. 529-540, Springer-Verlag. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599305</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<display_no>4</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Characteristic gait animation synthesis from single view silhouette]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599305</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599305</url>
		<abstract>
			<par><![CDATA[<p>Characteristics of human motion, such as walking, running or jumping vary from person to person. Differences in human motion enable people to identify oneself or a friend. However, it is challenging to generate animation where individual characters exhibit characteristic motion using computer graphics. Our goal is to construct a system that synthesizes characteristic gait animation automatically. As a result, when crowd animation is generated for instance, the motion with the variation can be made using our system. In our system, we first acquire a silhouette image as input data using a video camera. Second, we extract gait feature from single view silhouette. Finally we automatically synthesize 3D gait animation using the method blending a small number of motion data [KOVAR, L et al 2003].This blending weight is estimated using the gait feature automatically.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Video analysis</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010230</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Video summarization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621315</person_id>
				<author_profile_id><![CDATA[81447596181]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shinsuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621316</person_id>
				<author_profile_id><![CDATA[81100474115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shiraishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621317</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621318</person_id>
				<author_profile_id><![CDATA[81442608434]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mayu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okumura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621319</person_id>
				<author_profile_id><![CDATA[81100169567]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Makihara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621320</person_id>
				<author_profile_id><![CDATA[81100450431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yagi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2129575</ref_obj_id>
				<ref_obj_pid>2129560</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Y. Makihara, R. Sagawa, Y. Mukaigawa, T. Echigo, Y. Yagi, "Gait Recognition Using a View Transformation Model in the Frequency domain", Proc. of the 9&#60;sup&#62;th&#60;/sup&#62; European Conf. on Computer Vision (ECCV2006), Vol. 3, pp. 96--99, 2006]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846307</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[L, Kovar, M, Gleicher, "Flexible Automatic Motion Blending with Registration Curves", SIGGRAPH2003, pp. 214--224, 2003]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Characteristic Gait Animation Synthesis from Single View Silhouette Shinsuke Nakamurai, Masashi Shiraishi., 
Shigeo Morishima. Mayu Okumura., Yasushi Makihara., Yasushi Yagi. Waseda University  Osaka University 
 1 Introduction Characteristics of human motion, such as walking, running or jumping vary from person 
to person. Differences in human motion enable people to identify oneself or a friend. However, it is 
challenging to generate animation where individual characters exhibit characteristic motion using computer 
graphics. Our goal is to construct a system that synthesizes characteristic gait animation automatically. 
As a result, when crowd animation is generated for instance, the motion with the variation can be made 
using our system. In our system, we first acquire a silhouette image as input data using a video camera. 
Second, we extract gait feature from single view silhouette. Finally we automatically synthesize 3D gait 
animation using the method blending a small number of motion data [KOVAR, L et al 2003].This blending 
weight is estimated using the gait feature automatically. 2 Extraction Gait Feature We record a walker 
who goes straight from side view using a video camera. For the purpose of extracting gait silhouette 
from background easily, we set up the background of single color. The height of silhouette image acquired 
from video camera is normalized, because we don t consider the size of body. In this paper we extract 
the width of step, the area where arm sweep, and the degree of stoop as gait feature. from silhouette 
image.  3 Estimation Blending Weight First of all, we must prepare a small number of motions for synthesizing 
new motion beforehand. For our experiment, a participant was required to walk on a treadmill at a speed 
of 4[km/h], while we acquired 3D motion data using Mocap. The motion data is data of 6,...2,1=i kinds 
of walking styles, such as large or small width of step, large or small area where arm sweep, stoop, 
and recurved, which are cut out of two steps from the left leg landing. These motions are represented 
by, and called key motions. is number of frame. )(iifMifWe presented the method of estimating blending 
weighti. First, we extract the gait featuresijusing the 3DCG character animation made of the key motionsi. 
The gait feature of synthesized motionis represented (1)   (1) iiInput data is extracted using the 
silhouette image by recording a walker. Second, we estimate blending weighti that minimizing Euclidean 
distance between and is minimized using the active set method of convex quadratic programming problem. 
   Finally, we recalculate blending weightiby weighting to the gait feature which is the nearest input 
data in the gait features.  4 Models for gait animation synthesis As for key motionsi, the frameiand 
timing are different. Linear blending of each frame cannot be done usingi. We synchronize all the motions 
using the method of time warping [KOVAR, L et al 2003]. Synthesized motion  is by linear blending synchronized 
motions. (a) (b) (c) (d) Figure 1: Example of input data, silhouette, and synthesized gait animation 
The blending weightis the value estimated from the gait feature.  (3) The key motions i are the same 
all speeds, and frameis different according to the width of step.  iTherefore, the frame of synthesized 
motion should be determined by its step. The relation between the frame and the step of key motionsiis 
applied to the expression linear approximated, and the frame of the synthesized motion is calculated 
by its step. The Synthesized motion is expanded to the size of the calculated frame using the method 
of time warping. If we use this motion as long walking animation, we smooth this motion in order to keep 
continuousness.  5 Results We show synthesized gait animation using our system at Fig 1. For example, 
the gait feature extracted from the participant shown (a) is a large degree of stoop, and (c) is a large 
area where arm sweep. These synthesized gait animation can express the feature of walking enough. Very 
delicate feature like small difference walking (b) and walking (d) in Fig.1 can be expressed in our system. 
 6 Future Work In this paper we presented the method of constructing a system that synthesizes characteristic 
gait animation automatically from gait feature using single view silhouette. As future work, we need 
to increase the number of walking styles and a number of gait features for representing all walkers in 
the world. In this paper model of body is only one. However, each person s body is different size and 
proportion. Therefore, we should synthesize animation including not only information of motion, but also 
body. Finally, it is necessary to evaluate how accuracy original walker and synthesized animation look 
like. References Y. Makihara, R. Sagawa, Y. Mukaigawa, T. Echigo, Y. Yagi, Gait Recognition Using a 
View Transformation Model in the Frequency domain , Proc.of the 9th European Conf.on Computer Vision 
(ECCV2006), Vol.3, pp.96-99, 2006 L,Kovar, M, Gleicher, Flexible Automatic Motion Blending with Registration 
Curves , SIGGRAPH2003, pp.214-224, 2003 . e-mail: pencil@moegi.waseda.jp . e-mail: s-masashi331@moegi.waseda.jp 
. e-mail: shigeo@waseda.jp ., ., . e-mail: { okumura, makihara, yagi }@am.sanken.osaka-u.ac.jp   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599306</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<display_no>5</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Extraction of characteristic postures in a dance by statistical analysis of a database of motion data]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599306</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599306</url>
		<abstract>
			<par><![CDATA[<p>In the field of dance motion analysis, development of the technique for extraction of characteristic postures peculiar to each dance number is needed [Hachimura 2006]; extracted postures can be used as the indexes for the retrieval of motion data. In this study, the authors suggest a novel method for extraction of characteristic postures from the motion data of a dance number; the information of uniqueness of the dance number given by the statistical analysis of a database including motion data of plural dance numbers is used in the extraction process.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621321</person_id>
				<author_profile_id><![CDATA[81331499988]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621322</person_id>
				<author_profile_id><![CDATA[81319497817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazutaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitobe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621323</person_id>
				<author_profile_id><![CDATA[81319494392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takaaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaiga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Warabi-za]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621324</person_id>
				<author_profile_id><![CDATA[81319504710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Asia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621325</person_id>
				<author_profile_id><![CDATA[81319502599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Toshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taniguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621326</person_id>
				<author_profile_id><![CDATA[81100502669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hideo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Akita University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hachimura, K. 2006. Digital Archiving of Dancing, <i>Review of the National Center for Digitization</i>, 8, 51--56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>781241</ref_obj_id>
				<ref_obj_pid>781238</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Grassia, F. S. 1998. Practical Parameterization of Rotations Using the Exponential Map, <i>Journal of Graphics Tools</i>, 3, 3, 29--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kemanai Bon Odori Hozonkai, ed. 2007. <i>Kemanai no Bon Odori</i>, Kemanai Bon Odori Hozonkai, DAF 07N02 (DVD in Japanese).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Etchu Gokayama Kokiriko-uta Hozonkai, ed. 2005. <i>Kokiriko</i>, Etchu Gokayama Kokiriko-uta Hozonkai, DAF 05N01 (DVD in Japanese).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Extraction of Characteristic Postures in a Dance by Statistical Analysis of a Database of Motion Data 
Takeshi Miura Kazutaka Mitobe Takaaki Kaiga Takashi Yukawa Toshiyuki Taniguchi Hideo Tamamoto Akita 
University Akita University Warabi-za, North Asia University Akita University Akita University 1. Introduction 
 In the field of dance motion analysis, development of the technique for extraction of characteristic 
postures peculiar to each dance number is needed [Hachimura 2006]; extracted postures can be used as 
the indexes for the retrieval of motion data. In this study, the authors suggest a novel method for extraction 
of characteristic postures from the motion data of a dance number; the information of uniqueness of the 
dance number given by the statistical analysis of a database including motion data of plural dance numbers 
is used in the extraction process. 2. Extraction of Characteristic Postures  Figure 1 shows the model 
of a human body used in this study; it has the skeletal structure, and the posture of the body is described 
in the form of the exponential maps [Grassia 1998] of the joint angles. The components of the exponential 
map correspond to the respective joint motions as shown in Figure 1. The motion data of a dance number 
are described as the time-series data of the vector u(i) =[ u (i) u (i) L u (i)] T where i is the frame 
number. 1 2 42 The variance-covariance matrix for the time-series data of the mth dance number which 
belong to a database consisting of the motion data of M dance numbers is given as follows: m m m . ss 
L s . 11 12 142 s s L s 12 22 242 m ..mm m .. S= (1) . . M MOM . . m m m s s L s . 142 242 4242 . . . 
m N m 1 m uj =.uj (i) (2) mN i=1 m m mmmm s = 1 . N { u (i)-u }{ u (i)- u } (3) jk jjkk mN i=1 mN where 
is the number of the frames of the mth dance number. The values of the elements of (1) give the characteristics 
of the motion of the mth dance number such as the frequency of each joint motion and the information 
of joint coordination. The central tendency and dispersion of the characteristics throughout all the 
motion data in the database can be summarized by calculating the mean and variance of each element as 
follows: 1Mm s =. s (4) jk jk M m=1 1 M 2 m 2 s=.( s -s ) (5) jk jkjk M m=1 The standardized value of 
each element for the mth dance mm number s' jk =( s jk -s jk )/sjk gives the measure of distance from 
the average of the database, namely the uniqueness of the motion of this dance number. Therefore, the 
weighted sum of all the products of the deviations at the ith frame, in which the weight for m each product 
is set at | s' | as the following equation, gives the jk degree of the uniqueness appearing in the posture 
of this frame: m mmm mm m d(i) =..sgn( s )| s' |{ u (i)- u }{ u (i)- u } (6) jkjkj jk k jk ( j=k ) As 
a result, the frames each of which has a high value of (6) give e- m a il: m i u r a@ipc.akita-u.ac 
. j p , k a ig a@wa r a b i .o r . jp ,  y ukaw a @ nau. a c .jp  nf= [ nxf nyf nzf]T = [ uj uj+1 
uj+2 ]T n : unit vector (direction of rotation axis) f: rotation angle nxf: flexion/extension nyf: pronation/supination 
nzf: adduction/abduction number of joints:14 number of variables:42 (3 components ×14 joints) Figure 
1: Model of a human body. the characteristic postures peculiar to the mth dance number. 3. Results Motion 
data acquired by a motion capture system with magnetic sensors are used in this study; the database used 
consists of the motion data of 24 Japanese folk dance numbers. Figure 2 shows the examples of the characteristic 
postures extracted from two dance numbers in the database; each of them gives the maximum value of (6). 
Both the postures agree with those used in the jackets of the DVDs published for education of Japanese 
folk dances [Kemanai Bon Odori Hozonkai, ed. 2007] [Etchu Gokayama Kokiriko-uta Hozonkai, ed. 2005]; 
for these jackets, the postures each of which symbolizes the choreography of the respective dance number 
are selected. This fact shows the possibility that the present method is effective for automatic extraction 
of characteristic postures in a dance number.  (a) Kemanai Bon Odori, (b) Kokiriko, Sasara Odori. Jinku 
Odori. (Folk dance of Toyama (Folk dance of Akita Prefecture, Japan) Prefecture, Japan) Figure 2: Extracted 
characteristic postures. This study was supported by 'Strategic Information and Communications R&#38;D 
Promotion Programme (SCOPE)' of Ministry of Internal Affairs and Communications of Japan. Hachimura, 
K. 2006. Digital Archiving of Dancing, Review of the National Center for Digitization, 8, 51-56. Grassia, 
F. S. 1998. Practical Parameterization of Rotations Using the Exponential Map, Journal of Graphics Tools, 
3, 3, 29-48. Kemanai Bon Odori Hozonkai, ed. 2007. Kemanai no Bon Odori, Kemanai Bon Odori Hozonkai, 
DAF 07N02 (DVD in Japanese). Etchu Gokayama Kokiriko-uta Hozonkai, ed. 2005. Kokiriko, Etchu Gokayama 
Kokiriko-uta Hozonkai, DAF 05N01 (DVD in Japanese). Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599307</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<display_no>6</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Directable anime-like shadow based on water mapping filter]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599307</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599307</url>
		<abstract>
			<par><![CDATA[<p>Shadows in 2D Anime play a significant role for expressing symbolic visual effects such as the character's position and shape. However, animators frequently can't draw detailed shadows according to their intentions because of time constraints and a lack of skilled animators. For solving this problem, we have developed a system that can generate shadows automatically. Our system provides simple shadows and shadows on the water by applying Simplification Filter and Water Mapping Filter. Also, our system only requires inputs of the 2D character animation layers generally composed in the Anime industry. Consequently, our system enables animators to intuitively produce Anime-like shadow animation in a short time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621327</person_id>
				<author_profile_id><![CDATA[81335497519]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shimotori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621328</person_id>
				<author_profile_id><![CDATA[81421601671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shiori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621329</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1274884</ref_obj_id>
				<ref_obj_pid>1274871</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DeCoro, C., Cole, F., Finkelstein, A., and Rusinkiewicz, S. 2007. Stylized shadows. In <i>International Symposium on Non-Photorealistic Animation and Rendering (NPAR)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Directable Anime-like Shadow based on Water Mapping Filter Yohei Shimotori* Shiori Sugimoto Shigeo Morishima 
Waseda University Figure 1: Examples produced by our system. (a) is an input image, (b) is a simple shadow, 
(c) is a simple shadow related to character s shape, (d) and (e) are shadows with different wavelength 
and amplitude of Water Mapping Filter. 1 Introduction Shadows in 2D Anime play a signi.cant role for 
expressing sym­bolic visual effects such as the character s position and shape. How­ever, animators frequently 
can t draw detailed shadows according to their intentions because of time constraints and a lack of skilled 
animators. For solving this problem, we have developed a system that can generate shadows automatically. 
Our system provides sim­ple shadows and shadows on the water by applying Simpli.cation Filter and Water 
Mapping Filter. Also, our system only requires inputs of the 2D character animation layers generally 
composed in the Anime industry. Consequently, our system enables animators to intuitively produce Anime-like 
shadow animation in a short time. 2 Shadow Generation System In our system, animators set character 
animation layers with trans­parency as inputs. Then, our system renders shadows onto the ground by applying 
Shadow Map Method to the inputs. Also, shad­ows are .ne-tuned by adjusting parameters such as lights 
and the ground with simple mouse operations. As a result, our system en­ables animators to render shadows 
from the inputs. 3 Simpli.cation Filter For creating Anime-like shadow, our system provides Simpli.ca­tion 
Filter. This .lter enables the shadow shape to gradually convert accurate shape into ellipses. Referring 
to the concept of DeCoro et al. [2007], Simpli.cation Filter based on Gaussian Filter is applied to the 
shadow for simpli.cation by controlling Simple Parameter and Distance Parameter interactively. 4 Water 
Mapping Filter For creating the shadow on the water requiring substantial time to draw, our system provides 
Water Mapping Filter. This .lter en­ables the shadow shape to convert simple shape into the waving or 
striped shape. Bump Mapping technique is applied to the shadow *e-mail: y-shimotori@asagi.waseda.jp e-mail: 
shigeo@waseda.jp for distortion. First, the texture D, the basis for Bump Mapping, is created by the 
equation 1. D stores the wave direction of each pixel. { Du(i, j) = cos[p(ai + bj + vel · t)/.] (1) Dv(i, 
j) = sin[p(ai + bj + vel · t)/.] . is the wavelength, a and b are the parameter related to the wave direction, 
vel is the velocity of the wave, and t is time sequence. Second, our system creates Bump Map D0 by applying 
the follow­ing formula to D. A is the amplitude parameter. ()()( ) D ' u Du A00 A10 = (2) D ' v Dv A01 
A11 Compositors can easily create a variety of waves by tuning above mentioned parameters. Finally, our 
system creates random waves by mapping two sets of Bump Map D0 with shadow layer. 5 Result The demo 
.lm demonstrates the appearance of the edit and results of shadow animation created by our system. Figure 
1 shows shad­ows of various shapes created using our system. Figure 1a is the character layer as input. 
After setting the input into our system, we .ne-tune parameters of Simpli.cation Filter to create simple 
shad­ows (Figure 1b and c). Next, we create shadows on the water by .ne-tuning parameters of Water Mapping 
Filter (Figure 1d and e). 6 Conclusion Our system can generate several kinds of shadow by .ne-tuning 
parameters and Filters, as shown in Figure 1. In addition, anima­tors can set key-frames to create Anime-like 
shadow animation, as shown in demo .lm. Thus, Our system enables animators to intu­itively create and 
edit shadow animation in a short time. References DECORO, C., COLE, F., FINKELSTEIN, A., AND RUSINKIEWICZ, 
S. 2007. Stylized shadows. In Inter­national Symposium on Non-Photorealistic Animation and Rendering 
(NPAR). Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 
2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599308</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<display_no>7</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Expressive facial subspace construction from key face selection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599308</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599308</url>
		<abstract>
			<par><![CDATA[<p>MoCap-based facial expression synthesis techniques have been applied to provide CG character with expressive and accurate facial expressions [Deng et al. 2006: Lau et al. 2007]. The representative performance of these techniques depends on the variety of captured facial expressions. It is also difficult to guess what expressions are needed to synthesize expressive face before capture. Therefore, much MoCap data are required to construct a subspace employing dimensional compression techniques, and then the space enables us to synthesize expressions with linear-combination of basis vectors of the space. However, it is hard work to take much facial MoCap data to obtain expressive result.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621330</person_id>
				<author_profile_id><![CDATA[81442613683]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621331</person_id>
				<author_profile_id><![CDATA[81421595499]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takanori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621332</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621333</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621334</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1111419</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Deng, Z. Chiang, PY. Neumann, U. et al. 2006. Animating Blendshape Faces by Cross-Mappipng Motion Capture Data. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games</i>, 43--48]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272712</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lau, M. Chai, J. Xu, YQ. et al. 2007. Face Poser: Interactive Modeling of 3D Facial Expressions Using Model Priors. In <i>Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, 161--170]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Expressive Facial Subspace Construction from Key Face Selection Ryo Takamizawa Takanori Suzuki Hiroyuki 
Kubo Akinobu Maejima Shigeo Morishima Waseda University 1 Introduction MoCap-based facial expression 
synthesis techniques have been applied to provide CG character with expressive and accurate facial expressions 
[Deng et al. 2006: Lau et al. 2007]. The representative performance of these techniques depends on the 
variety of captured facial expressions. It is also difficult to guess what expressions are needed to 
synthesize expressive face before capture. Therefore, much MoCap data are required to construct a subspace 
employing dimensional compression techniques, and then the space enables us to synthesize expressions 
with linear-combination of basis vectors of the space. However, it is hard work to take much facial MoCap 
data to obtain expressive result. In this talk, we present a technique of choosing minimal face shapes 
automatically from motion capture data for constructing an expressive facial subspace. We call them key 
faces . First, we construct a basic facial subspace using PCA onto only neutral face shapes. All motion 
capture data are projected onto the space, and reconstructed as a face from mapped coordinate in the 
space. By measuring the reconstruction s error, we select the shape with maximum error as a new key face. 
By repeating this process, a set of key faces can be obtained. A facial subspace constructed by a group 
of a few key faces can be used to synthesize sensitive facial expressions.  2 Construction of Facial 
Subspace 2.1 Face MoCap We employ the VICON motion capture system to capture facial expressions. An actor, 
who has 226 markers put on his face, is directed to act 67 expressions. The captured data includes expressions 
of 6 basic emotions (surprise, happiness, disgust etc.), partial expressions (tightening eyelid, opening 
mouth etc.), speaking Japanese vowels etc. We then choose two neutral shapes from the data, and then 
construct a basic facial subspace with neutral expressions using PCA.  2.2 Adding Key Faces Because 
the basic space contains only of neutral expressions, it can t produce other sensitive expressions. Therefore, 
we have to add key faces. First, all motion capture data are projected onto the basic space. On a facial 
subspace, one shape is represented as a point on the space. We then reconstruct the facial shapes from 
points on the space using the eigen vector, which covers 99.9% of the variation. An error is determined 
by measuring a Euclidean distance between a marker of an original shape and that of a reconstructed one 
and calculating the average. A shape which has the largest error of all is selected as a new key face. 
Repeating this process enables us to superadd more key faces. As the shown in Fig.1 (blue line), the 
average error decreases as the number of the key faces increase (Fig. 1). 5 4.5 4 3.5 3 2.5 2 1.5 1 0.5 
0 average error(mm) Fig. 1 Reconstruction and Evaluation Error ryo.g.takamizawa@suou.waseda.jp shigeo@waseda.jp 
 Figure 2 Comparison of Synthesized face (a) Original, (b) using 4 key faces, (c) using 15 key faces, 
(d) using 100 key faces  3 Evaluation of Facial Subspace A decrease of the error means the facial subspace 
is getting more expressive. However, Fig. 1 (blue line) only shows that the space which has many key 
faces can be used to synthesize expressions which are projected on the space. Therefore, new motion capture 
data, which aren t used in subspace construction should be prepared, and is necessary to be more natural 
than the data used in the construction of the space: talking with some emotion, or changing emotions 
etc. We repeat the same process as the previous chapter: calculating errors for the new motion capture 
data. In this process, it isn t required to select new key faces or reconstruct a new space. Figure 1 
(red line) shows that the average error decreases as the number of the key faces increase. Fig. 2 (a),(b) 
shows that as the number of key faces increases, the synthesized face gets to resemble the original face. 
This means that the facial subspace which has many key faces can synthesize facial expressions naturally 
which aren t used for the space construction. However, if enough key faces are collected, the synthesized 
face remains expressive (Fig 2 (c),(d)) 4 Results and Discussions We discuss a method of automatic selection 
of key faces which can be used for expressive facial subspace. A subspace which has many key faces can 
be used to synthesize many kinds of facial expressions, but there is some number of used key faces that 
enable us to obtain expressive faces from the subspace. So we don t have too much MoCap data to construct 
a subspace. There is a need to decide baseline that a number of key faces is enough or not, and we plan 
to increase a number of examinees and motion capture data, and to prove the validity of this method. 
We assume that Mocap markers are located on the same position on a face during data capture, but actually, 
the markers don t remain still on a face. Therefore, adequate data can t be taken. In addition, our method 
is applied only to a particular face model, not general one. We should take these problems into consideration 
in our future work. Reference Deng, Z. Chiang , PY. Neumann, U. et al. 2006. Animating Blendshape Faces 
by Cross-Mappipng Motion Capture Data. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics 
and Games, 43-48 Lau, M. Chai, J. Xu, YQ. et al. 2007. Face Poser: Interactive Modeling of 3D Facial 
Expressions Using Model Priors. In Proceedings of the 2007 ACM SIGGRAPH/Eurographics symposium on Computer 
animation, 161-170 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599309</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<display_no>8</display_no>
		<article_publication_date>01-01-2009</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Model-based synthesis of visual speech movements from 3D video]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599309</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599309</url>
		<abstract>
			<par><![CDATA[<p>We describe a method for the synthesis of visual speech movements using a hybrid unit selection/model-based approach. Speech lip movements are captured using a 3D stereo face capture system and split up into phonetic units. A dynamic parameterisation of this data is constructed which maintains the relationship between lip shapes and velocities; within this parameterisation a model of how lips move is built and is used in the animation of visual speech movements from speech audio input. The mapping from audio parameters to lip movements is disambiguated by selecting only the most similar stored phonetic units to the target utterance during synthesis. By combining properties of model-based synthesis (e.g., HMMs, neural nets) with unit selection we improve the quality of our speech synthesis.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P2274117</person_id>
				<author_profile_id><![CDATA[81320489362]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Edge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Centre for Vision, Speech and Signal Processing, The University of Surrey, Surrey, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2274118</person_id>
				<author_profile_id><![CDATA[81100027411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hilton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Centre for Vision, Speech and Signal Processing, The University of Surrey, Surrey, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2274119</person_id>
				<author_profile_id><![CDATA[81100096263]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jackson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Centre for Vision, Speech and Signal Processing, The University of Surrey, Surrey, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Mori, "The uncanny valley," <i>Energy</i>, vol. 7, no. 4, pp. 33-35, 1970, translated by K. F. MacDorman and T. Minato.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C. G. Fisher, "Confusions among visually perceived consonants," <i>Journal of Speech and Hearing Research</i>, vol. 11, no. 4, pp. 796-804, 1968.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566594</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T. Ezzat, G. Geiger, and T. Poggio, "Trainable videorealistic speech animation," in <i>Proceedings of the 29th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '02)</i>, vol. 21, pp. 388-398, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[I. Albrecht, J. Haber, and H.-P. Seidel, "Speech synchronization for physics-based facial animation," in <i>Proceedings of the 10th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG '02)</i>, pp. 9-16, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[L. Reveret, G. Bailly, and P. Badin, "Mother: a new generation of talking heads providing a flexible articulatory control for video-realistic speech animation," in <i>Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP '00)</i>, pp. 755-758, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. M. Cohen and D. W. Massaro, "Modeling coarticulation in synthetic visual speech," in <i>Models and Techniques in Computer Animation</i>, Springer, Berlin, Germany, 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[A. L&#246;fqvist, "Speech as audible Gestures," in <i>Speech Production and Speech Modelling</i>, pp. 289-322, Springer, Berlin, Germany, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>847765</ref_obj_id>
				<ref_obj_pid>846222</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Cohen, D. Massaro, and R. Clark, "Training a talking head," in <i>Proceedings of the 4th IEEE International Conference on Multimodal Interfaces</i>, pp. 499-510, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. &#214;hman, "Numerical model of coarticulation," <i>Journal of the Acoustical Society of America</i>, vol. 41, pp. 310-320, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1176164</ref_obj_id>
				<ref_obj_pid>1175891</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Z. Deng, U. Neumann, J. P. Lewis, T.-Y. Kim, M. Bulut, and S. Narayanan, "Expressive facial animation synthesis by learning speech coarticulation and expression spaces," <i>IEEE Transactions on Visualization and Computer Graphics</i>, vol. 12, no. 6, pp. 1523-1534, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. Black, P. Taylor, and R. Caley, "The festival speech synthesis system," 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[T. Dutoit, V. Pagel, N. Pierret, E. Bataille, and O. van der Vrecken, "The MBROLA project: towards a set of high quality speech synthesizers free of use for non commercial purposes," in <i>Proceedings of the International Conference on Spoken Language Processing (ICSLP '96)</i>, vol. 3, pp. 1393-1396, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258880</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[C. Bregler, M. Covell, and M. Slaney, "Video Rewrite: driving visual speech with audio," in <i>Proceedings of the ACM SIGGRAPH Conference on Computer Graphics (SIGGRAPH '97)</i>, pp. 353-360, Los Angeles, Calif, USA, August 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218099</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Z. Deng and U. Neumann, "eFASE: expressive facial animation synthesis and editing with phoneme-isomap controls," in <i>Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA '06)</i>, pp. 251-260, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[S. Kshirsagar and N. Magnenat-Thalmann, "Visyllable based speech animation," in <i>Proceedings of the Annual Conference of the European Association for Computer Graphics (EUROGRAPHICS '03)</i>, vol. 22, pp. 631-639, September 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095881</ref_obj_id>
				<ref_obj_pid>1095878</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Y. Cao, W. C. Tien, P. Faloutsos, and F. Pighin, "Expressive speech-driven facial animation," <i>ACM Transactions on Graphics</i>, vol. 24, no. 4, pp. 1283-1302, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[L. Zhang and S. Renals, "Acoustic-articulatory modeling with the trajectory HMM," <i>IEEE Signal Processing Letters</i>, vol. 15, pp. 245-248, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311537</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. Brand, "Voice puppetry," in <i>Proceedings of the 26th International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '99)</i>, pp. 21-28, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. W. Massaro, J. Beskow, M. M. Cohen, C. L. Fry, and T. Rodriguez, "Picture my voice: audio to visual speech synthesis using artificial neural networks," in <i>Proceedings of the International Conference on Auditory-Visual Speech Processing (AVSP '99)</i>, pp. 133-138, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[B. Theobald and N. Wilkinson, "A probabilistic trajectory synthesis system for synthesising visual speech," in <i>Proceedings of the 9th International Conference on Spoken Language Processing (Interspeech '08)</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[T. Ezzat and T. Poggio, "Videorealistic talking faces: a morphing approach," in <i>Proceedings of the ESCA Workshop on Audio-Visual Speech Processing (AVSP '97)</i>, pp. 141-144, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. D. Edge, A. Hilton, and P. Jackson, "Parameterisation of 3D speech lip movements," in <i>Proceedings of the International Conference on Auditory-Visual Speech Processing (AVSP '08)</i>, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[P. Mueller, G. A. Kalberer, M. Proesmans, and L. Van Gool, "Realistic speech animation based on observed 3D face dynamics," <i>IEE Vision, Image &#38; Signal Processing</i>, vol. 152, pp. 491-500, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1949791</ref_obj_id>
				<ref_obj_pid>1949767</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[I. A. Ypsilos, A. Hilton, and S. Rowe, "Video-rate capture of dynamic face shape and appearance," in <i>Proceedings of the 6th IEEE International Conference on Automatic Face and Gesture Recognition (FGR '04)</i>, pp. 117-122, May 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015759</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[L. Zhang, N. Snavely, B. Curless, and S. M. Seitz, "Spacetime faces: high resolution capture for modeling and animation," in <i>Proceedings of the 31st International Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '04)</i>, pp. 548- 558, Los Angeles, Calif, USA, August 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[O. Govokhina, G. Bailly, G. Breton, and P. Bagshaw, "A new trainable trajectory formation system for facial animation," in <i>Proceedings of the ISCA Workshop on Experimental Linguistics</i>, pp. 25-32, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[http://www.3dmd.com/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>195970</ref_obj_id>
				<ref_obj_pid>195967</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Z. Zhang, "Iterative point matching for registration of free-form curves and surfaces," <i>International Journal of Computer Vision</i>, vol. 13, no. 2, pp. 119-152, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[W. Fisher, G. Doddington, and K. Goudie-Marshall, "The DARPA speech recognition research database: specifications and status," in <i>Proceedings of the DARPA Workshop on Speech Recognition</i>, pp. 93-99, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>302762</ref_obj_id>
				<ref_obj_pid>302528</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. Roweis, "EM algorithms for PCA and SPCA," in <i>Proceedings of the Neural Information Processing Systems Conference (NIPS '97)</i>, pp. 626-632, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[J. Kruskal and M. Wish, <i>Multidimensional Scaling</i>, Sage, Beverly Hills, Calif, USA, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[P. Mermelstein, "Distance measures for speech recognition, psychological and instrumental," in <i>Pattern Recognition and Artificial Intelligence</i>, pp. 374-388, Academic Press, New York, NY, USA, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[E. Vatikiotis-Bateson and D. J. Ostry, "Analysis and modeling of 3D jaw motion in speech and mastication," in <i>Proceedings of the IEEE International Conference on Systems, Man and Cybernetics</i>, vol. 2, pp. 442-447, Tokyo, Japan, October 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[D. J. Ostry, E. Vatikiotis-Bateson, and P. L. Gribble, "An examination of the degrees of freedom of human jaw motion in speech and mastication," <i>Journal of Speech, Language, and Hearing Research</i>, vol. 40, no. 6, pp. 1341-1351, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>791528</ref_obj_id>
				<ref_obj_pid>521641</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[E. Cosatto and H.-P. Graf, "Sample-based synthesis of photorealistic talking heads," in <i>Proceedings of the Computer Animation Conference</i>, pp. 103-110, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>648939</ref_obj_id>
				<ref_obj_pid>645312</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[T. F. Cootes, G. J. Edwards, and C. J. Taylor, "Active appearance models," in <i>Proceedings of the European Conference on Computer Vision (ECCV '98)</i>, pp. 484-498, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hindawi Publishing Corporation EURASIP Journal on Audio, Speech, and Music Processing Volume 2009, Article 
ID 597267, 12 pages doi:10.1155/2009/597267 ResearchArticle Model-BasedSynthesisofVisualSpeechMovements 
from 3DVideo JamesD.Edge,AdrianHilton,and PhilipJackson Centre forVision,SpeechandSignalProcessing,TheUniversityofSurrey,SurreyGU2 
7XH,UK Correspondence should be addressed to James D. Edge, j.edge@surrey.ac.uk Received 1 March 2009; 
Revised 30 July 2009; Accepted 23 September 2009 Recommended by G´erard Bailly We describe a method 
for the synthesis of visual speech movements using a hybrid unit selection/model-based approach. Speech 
lip movements are captured using a 3D stereo face capture system and split up into phonetic units. A 
dynamic parameterisation of this data is constructed which maintains the relationship between lip shapes 
and velocities; within this parameterisation a model of how lips move is built and is used in the animation 
of visual speech movements from speech audio input. The mapping from audio parameters to lip movements 
is disambiguated by selecting only the most similar stored phonetic units to the target utterance during 
synthesis. By combining properties of model-based synthesis (e.g., HMMs, neural nets) with unit selection 
we improve the quality of our speech synthesis. Copyright &#38;#169; 2009 James D. Edge et al. This is 
an open access article distributed under the Creative Commons Attribution License, which permits unrestricted 
use, distribution, and reproduction in any medium, provided the original work is properly cited. 1.Introduction 
Synthetic talking heads are becoming increasingly popular across a wide range of applications: from entertainment 
(e.g., Computer Games/TV/Films) through to natural user inter­faces and speech therapy. This application 
of computer ani­mation and speech technology is complicated by the expert nature of any potential viewer. 
Face-to-face interactions are the natural means of every-day communication and thus it is very di.cult 
to fool even a na¨ive subject that synthetic speech movements are real. This is particularly the case 
as the static realism of our models get closer to photorealistic. Whilst a viewer may accept a cartoon-like 
character readily, they are often more sceptical of realistic avatars. To explain this phenomena Mori 
[1] posited the uncanny valley , the idea that the closer a simulcra comes to human-realistic, the more 
slight discrepancies with observed reality disturb a viewer. Nevertheless, as the technology for capturing 
human likeness becomes more widely available, the application of lifelike synthetic characters to the 
above mentioned applications has become attractive to our narcissistic desires. Recent .lms, such as 
the The Curious Case of Benjamin Button , demonstrate what can be attained in terms of mapping­captured 
facial performance onto a synthetic character. However, the construction of purely synthetic performance 
is a far more challenging task and one which has yet to be fully accomplished. The problem of visual 
speech synthesis can be thought of as the translation of a sequence of abstract phonetic commands into 
continuous movements of the visible vocal articulators (e.g., lips, jaw, tongue). It is often considered 
that audible phonemes overspecify the task for animation, that is, an audio phoneme can discriminate 
based upon nonvisible actions (e.g., voicing in pat versus bat), and thus visible-phonemes/visemes (a 
term coined by Fisher [2]) are often used as basis units for synthesis. The simplest attempts at synthesis 
often take static viseme units and interpolate between them in some manner to produce animation [3 6]. 
It should be noted that visemes in this context are often considered to be instantaneous static targets, 
whereas phonemes refer to a sequence of audio or vocal tract parameters. It is a limitation of this kind 
of approach that the kinematics of articulatory movement are often not included explicitly. In particular 
the context speci.city of visemes must be modelled to correctly synthesise speech, that is, coarticulation. 
Viseme-interpolation techniques typically model coarticulation using a spline-based model (with reference 
to L¨ofqvist s earlier work on coarticulation [7]) to blend the speci.ed targets over time [6]. However, 
it is di.cult to derive the parameters for such models from real articulatory data and it is not even 
known what shape the basis functions should take as they cannot be directly observed. Given these limitations 
current systems typically build models from the kinematics of the vocal tract which can be directly observed. 
In [8] motion-captured markers (Optotrak) are recorded for natural speech for a single speaker; these 
are then used to train the parameters for an adapted version of the authors earlier coarticulation model 
[6]. In [5]tracked markersofisolatedFrenchvowelsand ¨ VCV syllables are used to train the parameters 
from Ohman s numerical model of coarticulation [9]. In [3]video of a speaker is used to train the distribution 
of visual parameters for each viseme, with synthesis performed by generating a trajectory that passes 
through the relevant distributions. In [10] viseme transition functions for diphones and triphones are 
trained using motion capture data, combinations of which can be used to synthesise novel utterances. 
One of the most common techniques in audio speech synthesis is the selection and concatenation of stored 
pho­netic units (e.g., Festival [11], MBROLA [12]). By combining short sequences of real speech, improvements 
in quality over parametric models of the vocal tract can be achieved. Analogously for visual synthesis 
short sections of captured speech movements can be blended together to produce animation. An example 
of this is Video-Rewrite [13]where short sections of video are blended together to produce what are termed 
video-realistic animations of speech. In [14, 15] motion-captured marker data is concatenated to similar 
e.ect, albeit without the advantage of photorealistic texture. Cao et al. [16] use similarity in the 
audio parameters between stored units and the target utterance as a selection criterion, along with terms 
which minimize the number of units and cost of joining selected units. By indexing into real data unit-selection 
methods bene.t from the intrinsic realism of the data itself. However, coarticulation is still manifest 
in how the units are blended together. It is not adequate to store a single unit for each phoneme; many 
examples must be stored across the various phonetic contexts and selected between during synthesis. In 
fact the best examples of concatenative synthesis select between speech units at di.erent scales (e.g., 
phonemes, syllables, words) to reduce the amount of blending and thus maximise the realism of the .nal 
animation (this is e.ectively being done in [16]). As the size of the underlying unit basis increases, 
the size of the required database exponentially increases; this leads to a trade-o. between database 
size and animation quality. The approaches described thus far do not use the audio of the target utterance 
to guide the generation of a synthetic speech trajectory. It is necessarily true that articulatory movements 
are embedded within the audio itself, albeit perhaps sparsely, and this should be taken advantage of 
during synthesis. The .nal group of visual synthesis techniques take advantage of the audio data to map 
into the space of visual speech movements. These audio-visual inversion models are typically based upon 
Hidden Markov Models (HMMs) [17, 18], neural networks [19], or other lookup models [20]. Brand [18]constructed 
an HMM-based animation system to map from audio parameters (LPC/Rasta-PLP) to marker data which can be 
used to animate a facial model. The HMM is initially trained to recognise the audio data, and for animation 
the output for each state is replaced by the corresponding distribution of visual parameters. Thus, a 
path through the hidden states of the HMM implies a trajectory through the articulatory space of a speaker. 
Zhang and Renals [17]use a trajectory formulation of HMM synthesis to synthesise Electro-Magnetic Articulography 
(EMA) trajectories from the MOCHA-TIMIT corpus. Trajectory HMMs incorporate temporal information in the 
model formulation which means that they generate continuous trajectories and not a discrete sequence 
of states. Problematically for all HMM synthesis a model trained on audio data and another trained on 
the accompanying visual data would produce two very di.erent network topologies. The approach of Brand 
makes the assumption that the two are at least similar, and this is unfortunately not the case. Constructing 
a global mapping in this way can produce a babbling level of synthesis but does not accurately preserve 
the motion evident in the original training data. This can be improved by using HMMs representing smaller 
phonetic groupings (e.g., triphones), and using a lattice of these smaller units to both recognise the 
audio and animate the facial model. This is similar to the way that HMM speech recognition systems work; 
although in recognition we are making a binary decision, that is, is this the correct triphone or not, 
whereas for animation we wish to recover a trajectory (sequence of states) that the vocal tract must 
pass through to produce the audio a more di.cult task. Also, because HMMs model speech according to the 
statistical mass of the training data, the .ne-scale structure of the individual trajectories can be 
lost in such a mapping. In ordertocapture speecharticulatorymovements several methods have been used; 
these include photogra­phy/video [3, 13, 21], marker-based motion capture [8, 10, 14, 15], and surface-capture 
techniques [22 25]. Video has the advantage of realism, but because the view is .xed, the parameters 
of such models do not fully capture the variability in human faces (e.g., in the absence of depth, lip 
protrusion is lost). Marker-based motion capture systems allow the capture of a small number of markers 
(usually less than 100) on the face and provide full 3D data. However, marker-based systems are limited 
by the locations in which markers can be placed; in particular the inner lip boundary cannot be tracked 
which is problematic for speech synthesis. Furthermore, systems such as Vicon and Optotrak require the 
placement of physical markers and sometimes wires on the face which do not aid the subject in speaking 
in a natural manner. Surface capture technologies, usually based upon stereophotogrammetry, produce sequences 
of dense scans of a subject s face. These are generally of a much higher resolution than possible with 
marker-based mocap (i.e., in the order of thousands of vertices), but frames are generally captured without 
matching geometry over time. This unregistered data requires a second stage of alignment before it can 
be used as an analytical tool. It can be seen that concatenative and model-based techniques have complementary 
features. In concatenative synthesis the .delity of the original data is maintained; yet there is no 
global model of how lips move and a decision must be made on how to select and blend units. Model­based 
synthesis provides a global structure to constrain the movement of the articulators and traverses through 
this structure according to the audio of the target utterance; however, by matching the input audio to 
the statistical mass of training data the detailed articulatory movements can be lost. In this paper 
we use a hybrid approach which attempts to take the advantages of both models and combine them into a 
single combined system. The most similar approach to that described can be found in [26]where an HMM 
model is used together with a concatenation approach for speech synthesis of both audio and visual parameters. 
However, Govokhina et al. use a HMM to select units for concatenation, whereas we select units to train 
a state-based model for synthesis (i.e., e.ectively the opposite order). The data used comes from a high-resolution 
surface capture system combined with marker capture to aid the registration of face movements over time. 
This paper is structured in the following manner: Section 2 describes our dynamic face capture and the 
makeup of our speech corpus; Section 3 describes the parameterisation of this data and the recovery of 
an underlying speech behaviour manifold; Section 4 describes our approach to the synthesis of speech 
lip movements; Section 5 describes the rendering/display of synthetic speech animation on a photorealistic 
model; .nally, Section 6 discusses a perceptual evaluation study into the quality of our synthesis approach. 
 2. Data Capture Many di.erent forms of data have been used as the basis of visual speech synthesis: 
from photographs of visemes [21], frontal video of a speaker [3, 13], marker-based motion­capture data 
[16], and surface scans of a subject during articulation [23]. The research described in this paper is 
based on data recorded using the 4D capture system developed by 3dMD [27] for high-resolution capture 
of facial movement; see Figure 1(a). This system works on the principal of stereophotogrammetry, where 
pairs of cameras are used to determine the location of points on a surface. The system consists of two 
stereo pairs (left/right) which use a projected infra-red pattern to aid stereo registration. Two further 
cameras capture colour texture information simultaneously with the surface geometry. All cameras have 
a resolution of 1.2 Megapixels and operate at 60 Hz, and the output 3D models have in the order of 20 
000 vertices (full face ear-to-ear capture). Each frame of data is reconstructed independently; this 
means that there is no initial temporal registration of the data. Audio data is also captured simultaneously 
with the 3D geometry and texture. To register the geometry over time markers are applied to the face 
of the subject. These take the form of blue painted dots on the skin and blue lipstick to track the contours 
of the lips; see Figure 1(b). Between the markers Table 1: Selected sentences from the corpus. Herb s 
birthday occurs frequently on Thanksgiving She took it with her wherever she went Alice s ability to 
work without supervision is noteworthy Boy you are stirrin early a sleepy voice said Employee layo.s 
coincided with companies reorganisation The armchair traveller preserves his illusions Don task me to 
carryanoilyrag like that Why buy oil when you always use mine The sound of Jennifer s bugle scared the 
antelope Don t look for group valuables in a bank vault Continental drift is a geological theory alignment 
is performed by calculating the geodesic distance (i.e., across the surface of the skin) from a vertex 
in the .rst frame to its surrounding markers; in subsequent frames the location on the surface with the 
same relative position to surrounding markers is taken as the matching point. In this manner a dense-registered 
surface reconstruction of the face can be captured for a subject. Due to the combination of the contour 
markers on the lips and the surface capture technology used we get a highly detailed model of the lips; 
in particular this is a great improvement over traditional motion-capture technology which is limited 
by the locations that markers can be attached to the face. We also get details of the movement of the 
skin surrounding the lips and in the cheeks which are commonly missed in synthesis systems. In the rest 
of this paper the data used is the registered 3D geometry; the texture images are only used to track 
the markers for registration. For the purposes of speech synthesis we isolate the data for the lower 
face (i.e., jaw, cheeks, lips) so that our system only drives the movement of the articulators. During 
data capture the subject is asked to keep their head still to prevent them leaving the capture volume 
which is relatively restrictive. However, no physical constraint is applied and it is found that the 
subject s head will drift slightly during recording (a maximum 2 minutes of continuous data capture is 
performed) which is removed using the Iterative Closest Point (ICP [28]) rigid alignment algorithm. The 
captured corpus consists of 8 minutes of registered 3D geometry and simultaneous audio captured of a 
male native British English speaker. Sentences were selected from the TIMIT corpus [29] to provide a 
good sampling across all phonemes, there are 103 sentences in all (see Table 1, e.g., sentences), and 
the sampling of phonemes can be seen in Table 2. This does not represent a high sampling of phonemes 
in terms of context, as this was seen as too great a data capture e.ort to be feasible with the current 
equipment and time required to process the data. However, when considered as a reduced set of visemes, 
as opposed to phonemes, we have a relatively large set of exemplar animations in a high quality to facilitate 
the synthesis technique described in the following sections. The audio data is manually transcribed to 
allow both the audio and geometry data to be cut into Phone segments.  (a) (b) Figure 1: Capture of 
facial movements: (a) the face capture system; (b) frames and tracked geometry from a sequence in the 
captured dataset. Table 2: Frequency of English phonemes in the captured data. p 72 b 79 m 99 ch 31 
jh34 s313z109sh41 zh20 f 69 v 58th28Consonants dh81 k133g 39 t241 d187 r 136w 68 n254 ng28hh29 l 170y 
62 aa 24 ae 85 ah 48 ao 49 aw 23 ay 57 ax 299 ea 26 Vowels eh 73 ey 65 ia 22 ih 198 iy 126 oh 62 ow 47 
oy 24 ua 23 uh 30  3. DataParameterisation The3Dregistereddatafromthe speechcorpusis parameterised 
in a manner which facilitates the struc­turing of a state-based model. The dataset consists of asequenceofframes, 
F, where the ith frame Fi = xyz0, xyz1, ..., xyzxyz is 3D {XXxyzXi, ..., X} and Xa vertex. n Principal 
Component Analysis (PCA) is applied directly to F to .lter out low variance modes. By applying PCA we 
get a set of basis vectors, XThe EM method for computing X. principal components [30] is used here due 
to the size of the data matrix, F, which holds 28, 833 frames ×12, 784 xyz coordinates. The .rst 100 
basis vectors are computed, with the .rst 30 holding over 99% of the recovered variance. The percentage 
of the total variance accounted for will be lower, but the scree-graph shows that the important features 
of F are compressed in only a few dominant components (i.e., ~95% in the .rst 10 components and ~99% 
in the .rst 30 components indicating a .attening of the scree-graph, see the blue line in Figure 2(a)). 
F can be projected onto the basis X X to produce the parameterisation FX.Soeachframe Fi can be projected 
onto XX .FX X, Fi ×Xi . Broadly, the 1st component of X X can be categorised as jaw opening, the 2nd 
is lip rounding/protrusion, and lower variance components are not as easily contextualised in terms of 
observed lip-shape qualities but generally describe protrusion, asymmetries, and the bulging of the cheeks. 
The .rst derivative for each frame can be estimated as ' FX =FX -FiX -1 (the parametric displacement 
of the lips in ii ' 1/60th of a second). Each pair {FiX , FX }describes a distinct i point in the physical 
space of lip movement. Another level of PCA could be applied directly upon this data; however as the 
.rst derivative is at a di.erent scale, the parameters need to be normalized such that FiX does not dominate 
over '' FX .Thusamatrix M ={(1/s2)(FX -µ), (1/s'2)(FX -µ')}' i ii is constructed where the FiX and FiX 
are scaled to have unit variance. The matrix M is now processed in a manner similar to Multidimensional 
Scaling (MDS) [31]; that is, a symmetric distance matrix . is formed where each element .ij is the Euclidean 
distance between Mi and Mj (the ith and jth elements of M), that is, .ij = (Mi -Mj)2.The matrix . is 
then decomposed using another iteration of PCA forming a basis YX; so for each of the initial frames 
Fi we have a corresponding projected coordinate FiY. The .rst 3 dimensions of YXaccount for over 93% 
of the recovered variance in .. The described parameterisation is used to reduce the dimensionality from 
38, 352 (number of vertices ×3) dimen­sions down to 10 dimensions, which account for ~99% Residual variance 
(%) 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0  (a) (b) Figure 2: Parameterisation of speech lip movements: 
(a) residual variances for the .rst 30 dimensions of XX(blue) and YX(red); (b) the speech manifold evident 
in the .rst 3 dimensions of YX: colour indicates density of the projection (blue least dense . red most 
dense), dashed line indicates the plane of symmetry between opening/closing of the lips, and vectors 
S and V indicate maximum change in lip shape and velocity, respectively. of the variance in . (as shown 
in the scree plot, see the red line in Figure 2(a)). The manifold evident in this reduced space also 
demonstrates several properties that are of interest for the visualisation of articulatory movements. 
The .rst 3 dimensions of the recovered speech manifold are shown in Figure 2(b). The major properties 
of this manifold are an ordering of frames according to change in both lip shape (the non linear vector 
S)and velocity (the nonlinear vector V). The manifold is also symmetric about a plane which divides lip-opening 
states from lip­closing states, and as a consequence of this speech trajectories are realised as elliptical 
paths on the manifold (i.e., open­close-open cycles). This structured representation is useful for the 
visualisation of speech movements, and a more detailed discussion of the properties of the recovered 
speech manifoldcan be foundin[22]. As this parameterisation maintains the relationship between lip shapes 
and their derivatives, it is ideal for structuring a state-based model of speech movements. For the purposes 
of speech synthesis we use the reduced space, Y, to cluster the data, where each individual cluster represents 
a state of motion in the system. Clustering is performed in this manner to avoid the dimensionality problem 
which would make clustering of the raw data computationally expensive and error prone. Furthermore, by 
clustering according to both position and velocity, we implicitly prestructure our state-based model 
of speech articulation discussed in the next section. Details of the state clustering and model construction 
are given in Section 4.  4.SynthesisofSpeechLipMovements Synthesis of speech lip movements in our system 
is charac­terised by a hybrid approach that combines unit selection with a model-based approach for traversing 
the space of the selected phonemes. This can be seen as a traversal of a sub­space on the manifold of 
lip motion described in the previous section. By cutting down the possible paths, according to the input 
audio, we reduce the ambiguity of the mapping from audio to visual speech movements and produce more 
realistic synthetic motions. The input to our system is a combination of both a phonetic transcription 
and the audio for the target utterance. Some systems attempt to avoid the necessity for a phonetic transcription 
by using a model thatise.ectively both recognising the phonetic content and synthesising the visual component 
simultaneously, or which forego any phonetic structure and attempt to directly map from audio parameters 
to the space of visual movements [18, 20]. In our experience, recognition and synthesis are very di.erent 
problems and improved results can be attained by separating the recognition and transcription component, 
which can be dealt with either using a specialised recognition module or manually depending upon the 
requirements of the target application. In overview, see Figure 3, our system proceeds through the following 
steps. (1) Input audio is decomposed into Mel Frequency Cepstral Coe.cients [32] (MFCCs), and a phonetic 
transcription of the content. (2) A unit selection algorithm is used to determine the closest stored 
unit to each segment in the target utterance. (3) Selected units are used to train a state-based model 
for each phone-phone transition. (4) An optimal path through the trained model, that is, across the 
learned manifold from Section 3,is determined using a Viterbi type algorithm. (5) The recovered sequence 
of states, which map onto a sequence of distributions of lip shapes/velocities, is used to generate a 
smooth output trajectory for animation.  Synthesis begins by taking the phonetic transcription and the 
audio for the target utterance (decomposed into 12th order MFCCs at the same frame rate as the geometry, 
 Figure 3: Schematic of the synthesis process: stored phoneme exemplars along with the input audio features 
are used to select optimal units to train a state-based manifold traversal model. 60 Hz) and selecting 
for each segment the most similar stored phone. A phone for our purposes consists of the sequence from 
the centre of the preceding phone to the centre of the following phone, similar to a triphone but only 
classi.ed according to the central phone (i.e., not according to context). The distance between a segment 
of the target utterance and a stored phone is calculated using Dynamic Time Warping (DTW). This algorithm 
calculates the minimum aligned distance between two time-series using using the following recursive equation: 
 2 di, j =xi -yj, .. .. . Di-1, j + di, j . (1) .. .. Di, j =min Di, j-1+ di, j. .. .. .. .. Di-1, j-1+2di, 
j Here di, j is the local Euclidean distance between a frame of the input data xi and a frame from a 
stored exemplar yj ,and Di, j is the global distance accumulated between the sequences x . [1, i]and 
y . [1, j]. The smallest global matching distance between the segment from the target utterance and an 
exemplar from the stored dataset indicates the best available unit. Note that because the algorithm .nds 
the best alignment between the two sequences, small inaccuracies in the input transcription will not 
reduce the quality of the .nal animation. This is in contrast to other concatenative synthesis systems 
(e.g., [13, 15]) where the accuracy of the transcription is key to producing good results. Our system 
aligns to the audio itself rather than to a, potentially inaccurate, transcription. Usually in unit selection 
synthesis models, the motions are blended directly to produce a continuous animation trajectory. This 
is problematic as the boundaries of the units may not align well, leading to jumps in the animation. 
However, if the units are selected to allow good transitions, then they may not be optimal for the target 
utterance. Furthermore, some phonemes have a stronger e.ect upon the output motion than others, and it 
would be advantageous to use the evidence available in the target audio to determine the .nal trajectory. 
In our system, we select the best units given the target audio, as described above, and use a model­based 
approach built from these units to determine a global trajectory for the target utterance. A state-based 
model is built to .t the input audio to the global structure of speech lip movements stored in our dataset. 
States are clusters forming a discretisation of the speech manifold described in Section 3. We use the 
bisecting K-means algorithm to cluster the parameterised data into states. The model we use consists 
of N = 200 states, each of which corresponds to a single distribution of lip shapes and velocities. The 
number of states is chosen as a trade­o. between dynamic .delity (i.e., a higher number of states gives 
a more accurate representation of speech movements), database size (i.e., the number of states must be 
much less than the number of samples in the dataset), and processing time (i.e., more states take longer 
to produce a global alignment). An N ×N binary transition matrix, T, is also constructed with each element 
Ti, j containing 0 to indicate connected states and 8 to indicate unconnected states. A connection in 
Ti, j means that a frame from the captured dataset classi.ed in state i is followed by a frame classi.ed 
in state j. Given that states are clustered on both position and velocity, the transition matrix is an 
implicit constraint upon the second derivative (acceleration) of speech lip movements. Note that this 
model is entirely built on the space of visual movements; that is, this is the opposite to models such 
as [18] where the state-based model is initially trained on the audio data. Each of our states will correspond 
to a range of possible audio parameters. In fact, the range of possible audio parameters that correspond 
to a single dynamic state can be widely distributed across the space of all speech audio. This is problematic 
for a probabilistic HMM approach that models these distributions using Gaussian Mixture Models (GMMs) 
and has an underlying assumption that they are relatively well clustered. Instead, we consider each example 
within a state to be independent rather than a part of a probabilistic distribution and use the best 
available evidence of being in a state to traverse the model and generate a synthetic trajectory. The 
choice of using a binary transition matrix (i.e., not probabilistic as in a HMM) also means that transitions 
which occur infrequently in the original data are equally as likely to be traversed during synthesis 
as those which are common. In this way we increase the importance of infrequent sequences, maximising 
the use of the captured data. The structure of the state model is constructed as a preprocessing step 
using the entire dataset. To generate a trajectory from the state-based model we use a dynamic programming 
approach similar to Viterbi, albeit to calculate a path using a minimum aligned dis­tance criteria and 
not maximum probability. The algorithm proceeds by calculating a state distance matrix Sd of size L × 
N (i.e., number of frames in the target utterance × number of states). Each element Sdi, j contains the 
minimum Euclidean cepstral distance between the ith frame of input data to all the contextually relevant 
frames in state j.Here a frame from state j is considered only if it is from one of the previously selected 
units which bracket frame i (i.e., the selected left-right phonetic context of the frame). Because of 
this the distance between a frame of audio data and a state will change according to its phonetic context 
in the target utterance. This optimises the mapping from audio to visual parameters according to the 
selected units. If we have asequenceof P phonemes, this is similar to training P -1 models, one for each 
phoneme-phoneme transition in the sequence, during synthesis (i.e., not as a preprocessing step). Each 
element of Sd , Sid , j, is a minimum distance value between a window surrounding the ith frame of audio 
data from the target utterance and each of the contextually relevant examples in state Sj. We use a window 
size of 5 frames to perform this distance calculation, multiplied by a n Gaussian windowing function, 
.(n) =(1/ v 2p)exp(-2/2),to emphasise the importance of the central frame. The distance function, dist, 
between an input window of audio data, u,at time i, and a state in the context of its left and right 
selected units, Slrj ,isde.nedin(2)where each vis a window of audio frames, centred at time k, from either 
the left or right selected .Slr units at this point in the sequence (i.e., where vj ). The x and y are 
individual frame samples from each of the windows, uand v,respectively, {} ui =.(-2)xi-2, ..., .(0)xi, 
..., .(2)xi+2, {} vk =.(-2)yk-2, ..., .(0)yk, ..., .(2)yk+2, (2) Sd ui, Slr vk .Slr ij =dist j =min(ui 
-vk)2, .j. To calculate the optimal trajectory across the speech manifold, we perform a simple recursive 
algorithm to accumulate distance according to the allowable transitions in T. The accumulated distance 
matrix, SD , is calculated according to the recursion in the following equation: {} SDi, j =minSDi-1,k 
+ Tk, j + Sdi, j, k .[1, N]. (3) This recursion is virtually identical to the Viterbi algo­rithm (when 
using log probabilities), the di.erence being that Viterbi is probabilistic whereas here we are simply 
accu­mulating distances and only use a binary transition matrix. Equation (3) is a simple distance accumulation 
operation with the transition matrix ensuring that transitions between states can only occur if that 
transition was seen in the original dataset. The minimum distance to a state at frame Lidenti.es the 
optimal alignment. By maintaining back-pointers the sequence of states can be traced back through SD 
. One problem with the proposed method is that by only selecting the best units for training the state-based 
model, there is a possibility that the model cannot transition between two neighbouring selected units. 
This could occur, for example, if the context for the selected units means that the boundaries are very 
far apart. Constraints on the size of database we can capture means that it is impossible to store exemplars 
for all phonemes in all contexts. Thus a back­o. solution for this problem is used. The point at which 
the model has failed to transition is simple to .nd, given that SD will contain 8forall columnspastthispoint.We 
can add examples from the dataset, in order of similarity to the target audio which will weaken the initial 
constraint on which parts of the speech manifold can be traversed. This is done by selecting the next 
most similar unit for the left and right context at this point in the sequence and adding the frames 
from these examples to each of the Slr context states. So the Slr are initially trained on the two most 
similar phones for the context, then four, then six, and so forth until the algorithm can pass through 
the segment. In practice, this is an infrequent problem and this solution does not add greatly to the 
complexity of the algorithm (given that we have already calculated a ranking of similarity between each 
input segment and all relevant stored examples). The output at this stage of synthesis is a sequence 
of states, where each state is characterised by a distribution of visual parameters. Given that for each 
state we have a distribution of positions and velocities for the lips, we use Brand s [18]approach forderiving 
acontinuoustrajectory. Each state has a mean position µi and velocity µ ' as well i as a full-rank covariance 
matrix Ci relating positions and velocities. For a sequence of states, S ={S1, ..., Si, ..., SL}, and 
frame parameters Z ={z1, ..., zi, ..., zL}T (where zi is a vector containing both the position and velocity 
at time i) this can be formulated as a maximum likelihood problem: () Z*=arg max logNZzi; CS(i). (4) 
Z i In (4) N(z; C) is the Gaussian probability of Zz according to the state covariance matrix C where 
Zz is mean centered. The optimal trajectory, Z*, of this formulation can be found by solving a block-banded 
system of linear equations. The output is a continuous trajectory of parameters, which yields a smooth 
animation of lower facial movement of the same form seen in our database (see Figure 6 for examples of 
the output 3D meshes from synthesis). Processing time for the sentences from our dataset, including both 
model building and synthesis, was in the range 30 50 seconds, depending upon the length of the target 
utterance. Figure 4 shows several examples of synthesised trajectories next to the real data for utterances 
in the dataset (the sentences were held out of the training set for synthesis). Section 5 discusses how 
this is turned into a photoreal animation of a speaker for display.  5.Animation Each frame of output 
from the synthesis procedure outlined in the previous section is a 3D surface scan of the same form tracked 
in the original data (i.e., geometry of the lower face). This means that we only have surface detail 
for the region of the face bounded by the tracked markers. Because markers cannot be placed in regions 
of shadow or where occlusions may occur, we do not have geometry for the region between the neckline 
and the jaw. Also, as the colour texture from the dynamic scanner contains markers, it is impractical 
to use for display. For these reasons we need to supplement the data originally captured to produce a 
photorealistic rendered animation. Note that the synthesis results from the previous Alices ability 
to work without supervision is noteworthy (a) Don t look for group valuables in a bank vault (b)  (c) 
Figure 0 4: 20 40 60 While wComparison 80 100 120 140 160 180 200 aiting for Chipper, she criss-crossed 
the square many times (d) of synthesised trajectories using our  Aluminium silverware can often be .imsy 
approach (blue) and real data (red) for the .rst dimension of the PCA model XX. section are used to animate 
the lower face, and the following model is used only to integrate this into a full face model. In the 
animation results, jaw rotation is modelled using a 3D morph-target model. Scans from a static surface 
scanner are used to model a 1D jaw rotation parameter; that is, in­between shapes are taken as an alpha-blend 
between two extrema (shown in Figure 5). Generally this is inadequate, in [33, 34] the 6 degrees-of-freedom 
of the jaw are examined in detail, but for our purposes where only speech movements of relatively low 
amplitude are being synthesised a single degree-of-freedom has been found to be adequate (i.e., the join 
between the synthesis results and the jaw model is not noticeable). It is important to note that the 
original captured data includes the actual motion of the jaw, and this 1D model is only intended to .ll 
in the region beneath the jawline to prevent a discontinuity in the rendered results. The jaw model is 
.tted to the synthesis results by performing a 1D line search to .nd the position at which the jawline 
of the synthetic lower face geometry .ts that of the jaw model. The function, f (a), which de.nes the 
goodness of .t of the jaw model given a particular interpolation parameter, a, is shown in the following 
equation: f (a) = si - a ·ti 0 +(1 -a) ·ti 1, a .[0, 1]. (5) i In this equation the si are the jawline 
vertices for a frame of the synthesised lower face geometry, and the ti 0 and t1 are i the matching vertices 
of the jaw model for the two extrema Figure 6: Rendered frames and generated 3D meshes (in red boxes) 
for the utterance Morphophonemic rules may be thought of as joining certain points in a system . (closed 
and open, resp.). Newton s method with derivatives calculated by .nite di.erences is used to .nd the 
minima of (5), which is adequate as there is only a single minima within the range a . [0, 1]. For the 
purposes of .tting the jaw model it is important that the jaw extrema are chosen such that they bracket 
the range of speech movements during normal speech. The results shown in this paper are produced by warping 
a single image using the synthetic mouth data and the .tted jaw model. This is done using a layered model 
where the image is progressively warped at each level to produce each output frame. The optimal projection 
of the jaw model into the image plane is calculated along with the nonrigid alignment with facial features 
in the photograph; using this information the image can be warped to .t the required jaw rotation. The 
synthetic mouth data is simply overlayed on top of the jaw animation using a second image warping oper­ation. 
This is similar to the work of [35], albeit our model is purely 3D. Because the image itself is not parameterised, 
as in EURASIP Journal on Audio, Speech, and Music Processing Table 3: The mean and variance of responses 
for the naturalness evaluation study; the three cases are real data playback (µreal , s2 real ), synthetic 
trajectories using the technique described in this paper (µsynth, s2 interp). synth), and synthetic trajectories 
using viseme interpolation (µinterp, s2 Subject µreal s2 real µsynth s2 synth µinterp s2 interp Subject 
1 3.45 1.11 2.95 1.37 2.63 1.95 Subject 2 4.00 0.85 3.22 1.13 2.14 0.69 Subject 3 3.55 0.74 2.90 1.51 
1.73 0.87 Subject 4 3.84 0.62 3.11 0.85 3.07 0.44 Subject 5 3.32 0.79 2.73 0.39 2.27 0.68 Subject 6 3.68 
1.17 3.55 0.92 2.64 0.81 Subject 7 4.36 0.43 3.90 0.65 3.13 0.59 Overall 3.74 0.89 3.19 1.09 2.52 1.05 
 active appearance models [36], we maintain the quality of the image itself after animation (i.e., we 
do not get the blurring associated with such models). Furthermore, because a true 3D model underlies 
the synthesis; the same technique could be potentially used on video sequences with extreme changes in 
head pose, which is generally problematic for purely 2D methods (such as [3, 13]). Frames from a synthetic 
sequence for the sentence Morphophonemic rules maybe thoughtof as joining certain points in a system 
are shown in Figure 6. The major problems in the animation of our model are the missing features, in 
particular the lack of any tongue model. Ideally we would also animate the articulation of the tongue; 
however, gathering dynamic data regarding tongue movement is complex. Our capture setup does not currently 
allow this, and image-based modelling of the tongue from photographs yields parameters poorly suited 
to animation. Were we to include head movements, eye blinks, and other nonarticulatory motions, this 
would inevitably lead to a great improvement in the naturalness of our output animations. Improvements 
could be achieved; yet the current system is focused upon creating natural lower facial for speech and 
would only be a part of a full facial animation system.  6.Evaluation A short evaluation study has been 
conducted to determine the quality of the rendered animations. Seven subjects (with no special prior 
knowledge of the experimental setup) were shown synthetic sentences in several categories: (1) real data 
played back using the animation system (see Section 5); (2) animations generated using the model described 
in this paper; (3) animations generated using a technique which interpolates viseme centres. The interpolation 
method we use selects context-viseme examples from the dataset to match the phonetic transcription of 
the target utterance. These centres are interpolated using C1 continuous Catmull-Rom splines to produce 
a continuous trajectory. The three di.erent cases are each rendered using the same technique to remove 
any in.uence of the method of display on naturalness. Each animation consisted of three repetitions of 
a single sentence with natural audio, and the subject was asked to mark the quality of the animation 
on a 5-point scale from 1 (completely unnatural) through to 5 (completely natural). In total 66 sentences 
were presented to participants, 22 sentences repeated for each of the cases. The sentences selected for 
evaluation were taken from a 2-minute segment of recorded TIMIT sentences not used in training the model. 
These sentences were selected randomly and contained no overlap with the training set. The intention 
was to evaluate the quality of generated synthetic trajectories, whilst not also implicitly evaluating 
the quality of the animation technique itself. The playback of real data provides a ceiling on the attainable 
quality; that is, it is likely not possible to be more-real-than-real. Furthermore, the viseme-interpolation 
method is the lowest quality technique which does not produce entirely random or babbling speech animations. 
In this way we attempt to .nd where between these two quality bookends our technique falls. The results 
of the study for individual participants and overall are summarised in Table 3. As expected overall and 
individually participants rated our method better than simple viseme interpolation. Gen­erally, our technique 
came out as a mid-way point between the real and interpolated sentences. Furthermore, in some cases our 
technique was rated equal in quality to the equivalent animation from the real data, although this was 
for a minority of the sentences. The most obvious di.erence between our technique and the real motions 
is overarticulation. Our trajectories tend to articulate all the syllables in a sentence, whereas real 
speech tends to .nd a smoother trajectory. Having said this, our method does not overarticulate to the 
degree seen in the viseme-interpolation case, and the state-based model ensures that there is a strong 
constraint on how the lips move. Several subjects commented that the smoothness of the animation was 
a major factor in determining the naturalness of an animation. Potentially moving to a syllabic unit 
basis (or a multiscale basis, e.g., phoneme/triphone/syllable combined) may yield this smoothness, yet 
with the drawback of a much larger data capture requirement. It is also worth noting that the results 
of our technique are quite variable, as is the case with most data-driven techniques. If an appropriate 
exemplar is not available in the database then the result can be a poor animation. It only takes a problem 
with a single syllable of a synthetic sentence to leave a large impact upon its perceived naturalness. 
Again this is most likely a problem of database size, notably audio speech synthesis databases are often 
far larger than the 8 minutes/103 sentences that we use as the basis for our system; however, the problem 
of capturing and processing a large corpus of visual speech movements needs to be solved to address this 
issue. 7.SummaryandDiscussion In this paper we describe a hybrid technique for the synthesis of visual 
speech lip movements from audio, using elements of both unit selection and a global state-based model 
of speech movements. The underlying data for our system is captured surface movements for the lips and 
jaw gathered using a dynamic face capture system. By using dense surface data we are able to model the 
highly complex deformations of the lips during speech to a greater degree of accuracy than traditional 
capture techniques such as motion-capture and image-based modelling. From this data a speech manifold 
is recovered using dimensionality reduction techniques; this manifold demonstrates a strong structure 
related to the cyclical nature of speech lip movements. Our state-based model is constructed according 
to the clustering of data on this manifold. At synthesis time phonetic units are selected from the stored 
corpus and used to cull possible paths on the speech manifold and reduce the ambiguity in the mapping 
of audio speech parameters to visual speech lip movements. A Viterbi-type algorithm is used to determine 
an optimal traversal of the state-based model and infer a trajectory across the manifold and therefore 
a continuous sequence of lip movements. We generate animations using a layered model which combines the 
synthetic lip movements with a 3D jaw rotation model. The animations deform an image-plane according 
to the 3D speech lip movements and therefore create photorealistic output animations. A short perceptual 
study has been conducted to determine the quality of our output animations in comparison with both real 
data and simple viseme-interpolation. The results of this study indicate that in some cases our technique 
can be mistaken for real data (i.e., the naturalness is ranked equal or higher then the equivalent real 
movements), but in general the quality lies somewhere in-between the two extremes. In terms of evaluation 
this is not speci.c enough to truly de.ne the quality of the technique, and further experimentation is 
required to compare with other existing techniques available in the literature. The resulting animations 
are certainly far from perfect; we can see clearly from Figure 4 where the generated trajectory diverges 
from the real signal. It is worth noting that techniques driven entirely or partially (as is the case 
here) from audio tend to lag behind the quality of target driven techniques. This may be due to several 
factors, ranging from issues related to the capture of large visual speech databases to problems with 
the ambiguity in mapping from audio to visual trajectories. Visual speech databases, particularly in 
3D, are far more di.cult to capture than audio corpora. This is in large part due to the camera equipment 
used to capture facial movement, which in our case leads to restricted head movement (i.e., due to the 
size of the capture volume) and the need to place markers on the skin to get temporal registration. Any 
capture of this form is not going to get truly natural speech due to the intrusive nature of the setup, 
which may be a factor in the quality of our synthetic lip movements. Furthermore, the physical size of 
3D databases and the time required to capture and reconstruct consistent data is a limiting factor in 
the size of our captured corpus. Eight minutes of data are small when compared to databases that are 
commonly used in speech analysis, and there is certainly an issue with sparsity when synthesising an 
utterance with our technique. With a data-driven approach missing data is a di.cult problem to tackle, 
except with the obvious method of capturing more data. It is our hope that with the development of 3D 
capture technology these issues will be reduced, which will increase the viability of using surface capture 
technology for speech analysis and synthesis. Lastly, ambiguity in the mapping from audio to visual movements 
is also signi.cant. We have found that it is generally true that clustering in the common audio parametric 
spaces (e.g., MFCC, PLP, etc.) does not lead to tight clusters in the visual domain, and vice versa when 
clustering in the visual domain. This is a fundamental problem and the motivation behind combining unit 
selection into the technique pre­sented in this paper. However, this may be an issue with how we parameterise 
speech audio itself. These parametric spaces seem to serve speech recognition well, where we are decomposing 
a signal into a discrete sequence of symbols but may be less appropriate for generating continuous speech 
movements. There is a great deal of information within the audio signal which is not relevant to animating 
visual speech movements, for example, the distinction of nasalised or voiced sounds. There may also be 
information missing, such as information regarding respiration, which is important in producing realistic 
speech animations. It is obvious that the representation of the audio signal is key in determining the 
quality of animation from techniques such as our own, and perhaps research is required into the joint 
representation of speech audio and visual movements to reduce the ambiguity of this mapping. Generating 
truly realistic speech animation is a very challenging task. The techniques described in this paper demonstrate 
the quality of animation that are attained when real lip movements can be used to infer the task space 
of speech production. Potentially capture techniques will advance such that more complex interactions 
between the lips and teeth can be captured (e.g., the f-tuck) which are not well modelled in the reported 
approach. However, this is only a part of the problem. To get truly natural characters we need to extend 
our models to full facial movement, to blinks, nods, and smiles. It is di.cult to drive the movement 
of the articulators using the information embedded in a speech audio signal, let alone the complex emotional 
behaviour of a character. Yet this is the outcome that a viewer is looking for. Naturalness is perceived 
globally with regards to the movement of the entire face, and indeed body; this hampers current models 
which treat speech animation as an isolated part of human behaviour. It is probably the case that the 
next breakthrough in generating truly naturalistic synthetic facial animation will come as a result of 
a holistic approach to the modelling of behaviour, as opposed to the piecemeal approaches commonly seen. 
Advances have currently been made as a result of data-driven modelling, as in this paper, and these approaches 
can yield convincing results. The drawback to such approaches lies in data capture; is it possible to 
capture truly comprehensive databases across speech and emotion? This is a huge problem that must be 
addressed if we are to reach the next level in purely synthetic character animation. References [1] 
M. Mori, The uncanny valley, Energy, vol. 7, no. 4, pp. 33 35, 1970, translated by K. F. MacDorman and 
T. Minato. [2] C. G. Fisher, Confusions among visually perceived conso­nants, Journal ofSpeech andHearingResearch, 
vol. 11, no. 4, pp. 796 804, 1968. [3] T. Ezzat, G. Geiger, and T. Poggio, Trainable videoreal­istic 
speech animation, in Proceedings of the 29th Annual Conference on Computer Graphics and Interactive Techniques 
(SIGGRAPH 02), vol. 21, pp. 388 398, July 2002. [4] I. Albrecht, J. Haber, and H.-P. Seidel, Speech synchroniza­tion 
for physics-based facial animation, in Proceedings of the 10thInternationalConference inCentralEuropeonComputer 
Graphics,VisualizationandComputerVision(WSCG 02),pp. 9 16, 2002. [5] L. Reveret, G. Bailly, and P. Badin, 
Mother: a new generation of talking heads providing a .exible articulatory control for video-realistic 
speech animation, in Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP 
00), pp. 755 758, 2000. [6] M.M.Cohen andD.W.Massaro, Modeling coarticulation in synthetic visual speech, 
in Models andTechniquesinComputer Animation, Springer, Berlin, Germany, 1993. [7] A. L¨ofqvist, Speech 
as audible Gestures, in SpeechProduction andSpeechModelling, pp. 289 322, Springer, Berlin, Germany, 
1990. [8] M. Cohen, D. Massaro, and R. Clark, Training a talking head, in Proceedings of the 4th IEEE 
International Conference on MultimodalInterfaces, pp. 499 510, 2002. ¨ Acoustical SocietyofAmerica, vol. 
41, pp. 310 320, 1967. [9] S. Ohman, Numerical model of coarticulation, Journalofthe [10] Z. Deng, U. 
Neumann, J. P. Lewis, T.-Y. Kim, M. Bulut, and S. Narayanan, Expressive facial animation synthesis by 
learning speech coarticulation and expression spaces, IEEE Transactions onVisualizationandComputerGraphics, 
vol. 12, no. 6, pp. 1523 1534, 2006. [11] A. Black, P. Taylor, and R. Caley, The festival speech synthesis 
system, 1999. [12] T. Dutoit,V.Pagel,N.Pierret,E.Bataille, andO.van der Vrecken, The MBROLA project: 
towards a set of high quality speech synthesizers free of use for non commercial purposes, in Proceedings 
of the International Conference on Spoken LanguageProcessing (ICSLP 96), vol. 3, pp. 1393 1396, 1996. 
[13] C. Bregler, M. Covell,and M. Slaney, VideoRewrite:driving visual speech with audio, in Proceedings 
of the ACM SIG­GRAPHConferenceonComputerGraphics(SIGGRAPH 97), pp. 353 360, Los Angeles, Calif, USA, 
August 1997. [14] Z. Deng and U. Neumann, eFASE: expressive facial animation synthesis and editing with 
phoneme-isomap controls, in Proceedings of theACMSIGGRAPH/EurographicsSymposium onComputerAnimation(SCA 
06), pp. 251 260, 2006. [15] S. Kshirsagar and N. Magnenat-Thalmann, Visyllable based speech animation, 
in Proceedings of the Annual Conference of theEuropeanAssociation forComputerGraphics(EURO-GRAPHICS 03), 
vol. 22, pp. 631 639, September 2003. [16] Y. Cao, W. C. Tien, P. Faloutsos, and F. Pighin, Expressive 
speech-driven facial animation, ACMTransactions onGraph­ics, vol. 24, no. 4, pp. 1283 1302, 2005. [17] 
L. Zhang and S. Renals, Acoustic-articulatory modeling with the trajectory HMM, IEEE Signal Processing 
Letters, vol. 15, pp. 245 248, 2008. [18] M. Brand, Voice puppetry, in Proceedings of the 26th InternationalConference 
onComputerGraphics andInteractive Techniques (SIGGRAPH 99), pp. 21 28, 1999. [19] D. W. Massaro, J. Beskow, 
M. M. Cohen, C. L. Fry, and T. Rodriguez, Picture my voice: audio to visual speech synthesis using arti.cial 
neural networks, in Proceedingsofthe InternationalConference onAuditory-VisualSpeechProcessing (AVSP 
99), pp. 133 138, 1999. [20] B. Theobald and N. Wilkinson, A probabilistic trajectory synthesis system 
for synthesising visual speech, in Proceedings of the 9th International Conference on Spoken Language 
Processing (Interspeech 08), 2008. [21] T. Ezzat and T. Poggio, Videorealistic talking faces: a morph­ing 
approach, in Proceedingsofthe ESCAWorkshop onAudio­VisualSpeechProcessing(AVSP 97), pp. 141 144, 1997. 
[22] J. D. Edge, A. Hilton, and P. Jackson, Parameterisation of 3D speech lip movements, in Proceedings 
oftheInternational Conference on Auditory-Visual Speech Processing (AVSP 08), 2008. [23] P. Mueller, 
G. A. Kalberer, M. Proesmans, and L. Van Gool, Realistic speech animation based on observed 3D face dynamics, 
IEEVision,Image&#38;SignalProcessing, vol. 152, pp. 491 500, 2005. [24] I. A. Ypsilos, A. Hilton, and 
S. Rowe, Video-rate capture of dynamic face shape and appearance, in Proceedingsofthe 6th IEEEInternationalConferenceonAutomaticFace 
andGesture Recognition(FGR 04), pp. 117 122, May 2004. [25] L. Zhang, N. Snavely, B. Curless, and S. 
M. Seitz, Spacetime faces: high resolution capture for modeling and animation, in Proceedingsofthe 31stInternationalConference 
onComputer GraphicsandInteractiveTechniques (SIGGRAPH 04), pp. 548 558, Los Angeles, Calif, USA, August 
2004. [26] O. Govokhina, G. Bailly, G. Breton, and P. Bagshaw, A new trainable trajectory formation system 
for facial animation, in Proceedingsofthe ISCAWorkshoponExperimentalLinguistics, pp. 25 32, 2006. [27] 
http://www.3dmd.com/. [28] Z. Zhang, Iterative point matching for registration of free­form curves and 
surfaces, InternationalJournal ofComputer Vision, vol. 13, no. 2, pp. 119 152, 1994. [29] W. Fisher, 
G. Doddington, and K. Goudie-Marshall, The DARPA speech recognition research database: speci.cations 
and status, in Proceedings of theDARPAWorkshop onSpeech Recognition, pp. 93 99, 1986. [30] S. Roweis, 
EM algorithms for PCA and SPCA, in Proceedings oftheNeuralInformationProcessingSystemsConference(NIPS 
97), pp. 626 632, 1997. [31] J. Kruskal and M. Wish, Multidimensional Scaling, Sage, Beverly Hills, Calif, 
USA, 1979. [32] P. Mermelstein, Distance measures for speech recognition, psychological and instrumental, 
in Pattern Recognition and Arti.cialIntelligence, pp. 374 388, Academic Press, New York, NY, USA, 1976. 
EURASIP Journal on Audio, Speech, and Music Processing [33] E. Vatikiotis-Bateson and D. J. Ostry, Analysis 
and modeling of 3D jaw motion in speech and mastication, in Proceedings of the IEEE International Conference 
on Systems, Man and Cybernetics, vol. 2, pp. 442 447, Tokyo, Japan, October 1999. [34] D. J. Ostry, E. 
Vatikiotis-Bateson, and P. L. Gribble, An examination of the degrees of freedom of human jaw motion in 
speech and mastication, Journal of Speech, Language, and HearingResearch, vol. 40, no. 6, pp. 1341 1351, 
1997. [35] E. Cosatto and H.-P. Graf, Sample-based synthesis of pho­torealistic talking heads, in Proceedings 
of the Computer AnimationConference, pp. 103 110, 1998. [36] T. F. Cootes, G. J. Edwards, and C. J. Taylor, 
Active appearance models, in Proceedings of the European Conference on Com-puterVision(ECCV 98), pp. 
484 498, 1998.  
			]]></ft_body>
		</fulltext>
	</article_rec>
	<article_rec>
		<article_id>1599310</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<display_no>9</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Muscle-based facial animation considering fat layer structure captured by MRI]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599310</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599310</url>
		<abstract>
			<par><![CDATA[<p>Muscle-based facial animation [Lee et al. 1995] is one of the best approaches to realize facial expressions of characters. However, this approach does not consider the personal variation in facial tissue model such as skin thickness. So personal character in emotional expression can not be reflected in this model.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621338</person_id>
				<author_profile_id><![CDATA[81442616677]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yarimizu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621339</person_id>
				<author_profile_id><![CDATA[81421594348]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishibashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621340</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621341</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621342</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>218407</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Y. Lee, D. Terzopoulos, K. Waters. 1995. Realistic Modeling for Facial Animation. In <i>Proceeding of SIGGRAPH ' 95</i>, 55--62]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073208</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Sifakis, I. Neverov R. Fedkiw. 2005. Automatic Determination of Facial Muscle Activations from Sparse Motion Capture Marker Data. In <i>SIGGRAPH2005 Conference Proceeding</i>, 417--425]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Muscle-based Facial Animation Considering Fat Layer Structure Captured by MRI Hiroto Yarimizu* Yasushi 
Ishibashi Hiroyuki Kubo Akinobu Maejima Shigeo Morishima. Waseda University 1 Introduction Muscle-based 
facial animation [Lee et al. 1995] is one of the best approaches to realize facial expressions of characters. 
However, this approach does not consider the personal variation in facial tissue model such as skin thickness. 
So personal character in emotional expression can not be reflected in this model. In this paper, we improve 
Lee s facial muscle model and propose muscle-based facial animation considering fat layer structure captured 
by magnetic resonance imaging (MRI). Facial structures are constructed by MRI images. We reassemble facial 
fat layer structure of Lee s facial muscle model to real person s internal structure. Facial fat layer 
structure is easily extracted from MRI images by focusing on brightness. Because fat layer structure 
of Lee s facial muscle model is defined as unique unit, facial structure obtained from MRI scans can 
not be applied to facial muscle model directly. Therefore we define personal parameter as a facial fat 
layer thickness in a facial muscle model. By our facial muscle model, it is possible to generate facial 
expression with a personal character. 2 MRI Scans MRI images can show internal structures of human 
body as luminance. First, we acquire real person s facial structure by MRI scans. Figure 1 shows internal 
structure around the lower jaw. It is hard to distinguish muscles and bones by means of luminance. However 
luminance of Facial fat layer is higher than bone, muscle, and other structure. Figure 1 shows that facial 
fat layer thickness has variation among individuals depending on body size and gender. Therefore, we 
make facial muscle structure imitate a real person s volume by adjusting fat layer size in a muscle model. 
 3 Facial Muscle Model Structure In Lee s facial muscle model, the skin structure is constructed uniformly. 
However, real facial skin-thickness has difference for each facial region. Especially fat layer of cheek 
part is thicker than other facial part. To introduce fat layer thickness of real person by MRI scans 
to facial muscle model, we separate a cheek part into three regions(Fig.2). The first part is the structure 
around mouth. The second part is structure around the cheek. The last part is zygomatic bone(cheek bone) 
structure. Figure 1: MRI Scans Result  *c0a7pp0i6yh.y@moegi.waseda.jp .shigeo@waseda.jp Figure.3: 
Simulation Results (a) Lee Model (b) Our Model  4 Fat Layer from MRI Images We extract fat layer in 
MRI images, and introduce fat layer into facial muscle model. In MRI images, luminance of Facial fat 
layer is higher than other structures. Using binary processing, we remove structures displayed lower 
luminance than fat layer from MRI images. Area of fat layer in MRI image is defined by measuring number 
of pixels in fat layer. Fat layer volume and cephalic volume are measured for each subject by the range 
from jaw to cheek in MRI images. Of course, Lee's facial muscle model considers the thickness of facial 
muscle tissue and control with volume preservation force. The thickness is uniform independent of face 
part. By dividing the facial part into three region, we can get regional fat layer structure to generate 
more delicate facial expression with personal character First the personal parameter is defined by thickness 
around cheek, second parameter is defined by thickness around zygomatic bone. Last parameter is defined 
by thickness around mouth. First personal parameter is defined by multiplying basic fat layer thickness 
of facial model by the ratio of the subject s fat layer volume to the average fat layer volume and the 
radio of the average cephalic volume to the subject s cephalic volume against cheek. Second parameter 
is defined in a similar way against zygomatic bone. Then, MRI image shows structure around mouth does 
not have a fat layer thickness. Therefore, personal parameter around mouth is defined by multiplying 
half basic facial fat thickness of facial muscle model by the ratio of average cephalic volume to personal 
cephalic volume. To apply three personal parameters to Lee s model, we try facial expression generation. 
 5 Results and Discussion We introduce a new facial muscle model considering the thickness of subject 
s facial fat layer as a personal parameters. And after simulation, we can get facial expression reflecting 
personality. After comparing the synthesized face by our model with Lee s model (Fig.3), our model can 
add more original future to synthesized face than existing model. Our facial model can realize to generate 
more natural facial expression. We are getting more variety of MRI data to improve personal parameters 
and construct bone structure and muscle structure to conform to human structure. Reference Y. Lee, D. 
Terzopoulos, K. Waters. 1995. Realistic Modeling for Facial Animation. In Proceeding of SIGGRAPH 95, 
55-62 E. Sifakis, I.Neverov R. Fedkiw. 2005. Automatic Determination of Facial Muscle Activations from 
Sparse Motion Capture Marker Data. In SIGGRAPH2005 Conference Proceeding, 417-425 Copyright is held by 
the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599311</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<display_no>10</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Pliant motion]]></title>
		<subtitle><![CDATA[integration of virtual trajectory control into LCP based physics engines]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599311</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599311</url>
		<abstract>
			<par><![CDATA[<p>Interactive applications such as Video Games require characters, which generate motions corresponding to user's interaction. Motion capture is an effective technique to reproduce realistic motion. However, to produce a motion which is appropriate to the operation of the user, a lot of motions must be prepared and one of the motions which is suitable for the user's operation must be selected and played. Because the user's operation changes the motion trajectory, unexpected contact to objects may happen. The amount of change on a trajectory depends on not only the trajectory of motion but also internal tensions of skeletal muscles - co-contraction level, when a person put one's hand down on a table or collides with an object. [Hogan 1984] proposed that reaching motion of human is supposed to be generated by spring damper characteristics of muscles dragging to the virtual trajectory. Human controls not only trajectories of motions but also spring-damper characteristics of muscles by changing co-contraction levels. Realistic character motions contacting to objects can be generated easily with virtual trajectory tracking control which is integrated to physics engines for character motions.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[iterative LCP solver]]></kw>
			<kw><![CDATA[physics simulation]]></kw>
			<kw><![CDATA[virtual creature]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621343</person_id>
				<author_profile_id><![CDATA[81421598717]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokizaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621344</person_id>
				<author_profile_id><![CDATA[81442593261]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tazaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621345</person_id>
				<author_profile_id><![CDATA[81316489667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621346</person_id>
				<author_profile_id><![CDATA[81100270860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hogan, N. 1984. An organizing principle for a class of voluntary movements. <i>Journal of Neuroscience 4</i>, 2745--2754.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pliant Motion : Integration of Virtual Trajectory Control into LCP Based Physics Engines Takashi Tokizaki 
* Yuuichi Tazaki Hironori Mitake Shoichi Hasegawa * University of Electro-Communications * Dept. of Math. 
and Comp. Sci, Tokyo Institute of Technology P &#38; I lab. Tokyo Institute of Technology Keywords: 
Iterative LCP Solver, Physics simulation, virtual crea­ture 1 Introduction Interactive applications 
such as Video Games require characters, which generate motions corresponding to user s interaction. Mo­tion 
capture is an effective technique to reproduce realistic motion. However, to produce a motion which is 
appropriate to the opera­tion of the user, a lot of motions must be prepared and one of the motions which 
is suitable for the user s operation must be selected and played. Because the user s operation changes 
the motion tra­jectory, unexpected contact to objects may happen. The amount of change on a trajectory 
depends on not only the trajectory of mo­tion but also internal tensions of skeletal muscles -co-contraction 
level, when a person put one s hand down on a table or collides with an object. [Hogan 1984] proposed 
that reaching motion of hu­man is supposed to be generated by spring damper characteristics of muscles 
dragging to the virtual trajectory. Human controls not only trajectories of motions but also spring-damper 
characteristics of muscles by changing co-contraction levels. Realistic character motions contacting 
to objects can be generated easily with virtual trajectory tracking control which is integrated to physics 
engines for character motions. However, it is not easy to make a virtual trajectory by hand. Though a 
real trajectory tracks the virtual trajectory, the real trajectory parts from virtual trajectory by effects 
of inertia and Coriolis force. Thus making a virtual trajectory which generates speci.ed real trajectory 
is not easy. Thus, we propose a realtime method to generate a vir­tual trajectory tracking any real trajectory 
for any articulated bodies with dynamics simulator based on iterative LCP solvers. 2 Computation of 
virtual trajectory Proposed method computes joint torques to track desired (real) tra­jectory using 
physics engines (1st step), then executes one step of simulation to update posture and velcity of the 
articulated body with PD control targeting to the desired trajectory (2nd step). Because the computed 
torque from 1st step is almost enough to track de­sierd trajectory, the feedback coef.cients for the 
PD control are not matter. Therefore, the generated trajectories have no difference between small and 
large feedback coef.cients. Though proposed method do not compute virtual trajectory explicitly, virtual 
trajec­tory can be found from the torques computed in 1st step by suppos­ing that the torques are generated 
from the PD control targeting to the virtual trajectory. 3 Result We create examples of constrainted 
motion with proposed and pre­vious method. We generate motion of pushing three boxes on a *e-mail:{tokizaki, 
hase}@hi.mce.uec.ac.jp e-mail:tazaki@cyb.mei.titech.ac.jp e-mail:mitake@hi.pi.titech.ac.jp desk by tracking 
pre-de.ned trajectory with (a) simple PD control, (b) proposed method with stiff (large) feedback coef.ecent 
and (c) proposed method with soft (small) feedback coef.cent. .gure 1 shows examples of the generated 
motion. In (a), the hand of the  Figure 1: Comparison of generated motion with (a) simple PD con­trol, 
(b) proposed method with stiff feedback coef.cient and (c) pro­posed method with soft feedback coef.cient 
character penetrates into the box. Though, large contact force acts on the hand, the tracking force breaks 
down the contact force. In (b) and (c), the contact force prevent the penetration through the box. In 
(b), The hand gives enough force to move the box and both boxes move, while (c), the hand gives not enough 
force and the hand stops when the small box contact to large one. 4 Conclusion It is dif.cult to generate 
a trajectory which trace surfaces of desks or walls. We think human beings generate such motions by comply­ing 
with environmental constraints using virtual trajectory track­ing. Proposed method mimics this motion 
generation method of human and easily generates realistic motion trajectories. Proposed method generates 
motion trajectory in realtime with low computa­tioal amount using LCP solvers in physics engines. References 
 HOGAN, N. 1984. An organizing principle for a class of voluntary movements. Jour­nal of Neuroscience 
4, 2745 2754. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599312</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<display_no>11</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Polygonal-functional hybrids for computer animation and games]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599312</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599312</url>
		<abstract>
			<par><![CDATA[<p>The modern world of computer graphics is mostly dominated by polygonal models. Due to their scalability and ease of rendering such models have various applications in a wide range of fields. Unfortunately some shape modelling and animation problems can hardly be overcome using polygonal models only. For example, dramatic changes of the shape (involving change of topology) or metamorphosis between different shapes can not be performed easily. The Function Representation (FRep) [Pasko et al. 1995] allows us to overcome some of the problems and simplify the process of the major model modification. Our system is based on a hybrid modelling concept, where polygonal and FRep models are combined together and can be evaluated in near-real or real time. It allows us to:</p> <p>&#8226; produce animations involving dramatic changes of the shape (e.g. metamorphosis, viscoelastic behaviour, character modifications etc) in short times (Fig. 1)</p> <p>&#8226; interactively create complex shapes with changing topology (Fig. 2) and specified level of detail (LOD)</p> <p>&#8226; integrate existing animated polygonal models and FRep models within a single model</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621347</person_id>
				<author_profile_id><![CDATA[81442614407]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kravtsov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621348</person_id>
				<author_profile_id><![CDATA[81100594560]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[O.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fryazinov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621349</person_id>
				<author_profile_id><![CDATA[81100169564]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[V.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Adzhiev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621350</person_id>
				<author_profile_id><![CDATA[81100137963]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pasko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621351</person_id>
				<author_profile_id><![CDATA[81100459280]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Comninos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kravtsov, D., Fryazinov, O., Adzhiev, V., Pasko, A., and Comninos, P. 2008. Embedded implicit stand-ins for animated meshes: a case of hybrid modelling. Tech. rep., NCCA, Bournemouth University, UK.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pasko, A., Adzhiev, V., Sourin, A., and Savchenko, V. 1995. Function representation in geometric modeling: Concepts, implementation and applications. <i>The Visual Computer</i>, 11, 429--446.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Polygonal-Functional Hybrids for Computer Animation and Games D. Kravtsov*, O. Fryazinov, V. Adzhiev, 
A. Pasko and P. Comninos NCCA, Bournemouth University, UK 1 Introduction The modern world of computer 
graphics is mostly dominated by polygonal models. Due to their scalability and ease of rendering such 
models have various applications in a wide range of .elds. Unfortunately some shape modelling and animation 
problems can hardly be overcome using polygonal models only. For example, dramatic changes of the shape 
(involving change of topology) or metamorphosis between different shapes can not be performed eas­ily. 
The Function Representation (FRep) [Pasko et al. 1995] allows us to overcome some of the problems and 
simplify the process of the major model modi.cation. Our system is based on a hybrid modelling concept, 
where polygonal and FRep models are com­bined together and can be evaluated in near-real or real time. 
It allows us to: produce animations involving dramatic changes of the shape (e.g. metamorphosis, viscoelastic 
behaviour, character modi­.cations etc) in short times (Fig. 1)  interactively create complex shapes 
with changing topology (Fig. 2) and speci.ed level of detail (LOD)  integrate existing animated polygonal 
models and FRep mod­els within a single model  2 Hybrid Model Outline One of the promising classes 
of the available FRep primitives is convolution surfaces. These surfaces are speci.ed by an underlying 
skeleton and a radius. They can be easily controlled during mod­elling process by the end-user. The ease 
of modelling makes them both useful for artists working on complex shapes and animations as well as for 
gamers producing user-generated content in games environment (as in SporeTM ). The skeletal representation 
also makes these surfaces accessible to animators who can work in a fa­miliar environment. We choose 
convolution surfaces as the main modelling primitive and combine them with other FRep operations. Unfortunately 
FRep models are computationally expensive in general. That is one of the reasons they are still rarely 
used. Usu­ally one needs to extract the surface from the volume and render the resulting mesh. Most of 
the existing mesh extraction approaches are designed for static models. Our method employs time coher­ence, 
using information from previous frames to reduce computa­tion complexity. We have also introduced a number 
of approxima­tions to accelerate the model evaluation. We have .rst implemented our approach in MayaTM 
demonstrating that it is possible for artists to work with FRep models in the familiar environment [Kravtsov 
et al. ]. Parameters of created models can then be exported to an external application for near or real-time 
playback (depending on the com­plexity of the model). If the complexity of the model is too high for 
the real-time evaluation in an interactive environment, the model evaluation can be skipped for a .xed 
numbers of frames (e.g. the model is re-evaluated only at each fourth frame). In this case inter­mediate 
results of the mesh extraction can be rendered with alpha­blending. One of the important factors allowing 
us to control the LOD is resolution independence of the model. We only need to choose * dkravtsov@bournemouth.ac.uk 
 Figure 1: Viscoelastic behaviour and hyrbid characters Figure 2: Iterations of character growth controlled 
by the user the discrete step for the mesh extraction. This step depends on the distance from the viewer 
to the object (the smaller the step, the higher the quality of the resulting mesh). When no further changes 
of the mesh are required (at the .nal stage of the metamorphosis or change of topology), the extracted 
mesh can be rigged with the skeleton used to create the mesh. Skinning of the resulting mesh would happen 
in the same way as for an ordinary polygonal mesh, which simpli.es the integration of the technique into 
existing ap­plications. Extracted meshes can be rendered using sphere or cube mapping, procedural shaders 
or alpha-blended reprojected textures (where blending coef.cients are based on the normal direction) 
as seen in (Fig. 1). Material systems available in modern packages and game engines are thus applicable 
to FRep models as well. It is important to note that the model evaluation is easily paral­lelized. Performance 
grows almost linearly with the increase of the number of available processors. Our partial implementation 
of the prototype on the GPU using CUDA SDK allows for the real-time evaluation of complex models. We 
think that further integration of hybrid models into modelling software and game engines can introduce 
new opportunities for artists and greatly enhance user experience. References KRAVTSOV, D., FRYAZINOV, 
O., ADZHIEV, V., PASKO, A., AND COMNINOS, P. 2008. Embedded implicit stand-ins for ani­mated meshes: 
a case of hybrid modelling. Tech. rep., NCCA, Bournemouth University, UK. PASKO, A., ADZHIEV, V., SOURIN, 
A., AND SAVCHENKO, V. 1995. Function representation in geometric modeling: Con­cepts, implementation 
and applications. The Visual Computer, 11, 429 446. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599313</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<display_no>12</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Retrieval of motion capture data based on short-term feature extraction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599313</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599313</url>
		<abstract>
			<par><![CDATA[<p>This poster presents a motion retrieval algorithm, which searches the motions in the same category as a query's (known as logically similar motions) in a motion capture database. The challenge is that logically similar motions may not be numerically similar due to the motion variations [M&#252;ller et al. 2005]. In this poster, we propose a novel short-term feature that extracts both symbolized representation and continuous features from joint velocities in a motion clip, which is employed to effectively retrieve logically similar motions to the query. Although symbolized representation of human motion has been studied [M&#252;ller et al. 2005], our approach is different in that we consider temporal correlation instead of M&#252;ller's spatial relationship. Moreover, not only symbolized representation (dynamic pattern) but also continuous features (average speed) are extracted in our short-term feature. Furthermore, our method is more friendly to novices as it requires no prior knowledge to determine features. Our experiments demonstrate that our algorithm greatly improves the performance compared to two conventional methods.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.3.3</cat_node>
				<descriptor>Retrieval models</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003338</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Retrieval models and ranking</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621352</person_id>
				<author_profile_id><![CDATA[81416604499]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jianfeng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KDDI R&D Laboratories Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621353</person_id>
				<author_profile_id><![CDATA[81319494297]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Haruhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KDDI R&D Laboratories Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621354</person_id>
				<author_profile_id><![CDATA[81100353575]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoneyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KDDI R&D Laboratories Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CMU MoCap Database. http://mocap.cs.cmu.edu/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073247</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, M., R&#246;der, T., and Clausen, M. 2005. Efficient content-based retrieval of motion capture data. <i>ACM Transactions on Graphics 24</i>, 3 (July), 677--685.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Onuma, K., Faloutsos, C., and Hodgins, J. K. 2008. FMDistance: A fast and effective distance function for motion capture data. In <i>EUROGRAPHICS 2008, Short Papers</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Retrieval of Motion Capture Data Based on Short-Term Feature Extraction Jianfeng Xu* Haruhisa Kato* 
Akio Yoneyama* KDDI R&#38;D Laboratories Inc. 1 Introduction This poster presents a motion retrieval 
algorithm, which searches the motions in the same category as a query s (known as logically similar motions) 
in a motion capture database. The challenge is that logically similar motions may not be numerically 
similar due to the motion variations [M¨uller et al. 2005]. In this poster, we propose a novel short-term 
feature that extracts both symbolized representa­tion and continuous features from joint velocities in 
a motion clip, which is employed to effectively retrieve logically similar motions to the query. Although 
symbolized representation of human motion has been studied [M¨uller et al. 2005], our approach is different 
in that we consider temporal correlation instead of M¨uller s spatial re­lationship. Moreover, not only 
symbolized representation (dynamic pattern) but also continuous features (average speed) are extracted 
in our short-term feature. Furthermore, our method is more friendly to novices as it requires no prior 
knowledge to determine features. Our experiments demonstrate that our algorithm greatly improves the 
performance compared to two conventional methods. 2 Motion Retrieval The proposed algorithm works in 
three stages. (1) By a psycholog­ical fact that joint positions and velocities are essential to perceive 
a motion, we transform Euler angles to joint velocities. To become invariant to coordinate transformation, 
the absolute values of rela­tive velocities to the root joint are calculated, called joint speeds. (2) 
It is observed that the dynamic pattern structure of joint speeds in a motion clip is rather stable in 
a category of motions. Therefore, we divide a human motion into many overlapped motion clips with a .xed 
length as basic units in the short term, see Fig. 1. The joint speeds are classi.ed into one of the six 
pre-de.ned templates (called dynamic pattern) by a decision tree. The average value of the joint speeds 
forms the second part in the feature. (3) De.ne motion dissimilarity. Firstly, we de.ne the distances 
of dynamic patterns and average speeds respectively. For dynamic pattern, if two dynamic patterns are 
the same, their distance is 0; if parts of them are shared such as UP and PEAK, their distance is 0.5; 
otherwise, their distance is 1.0. For two average speeds v¯i and v¯j , d(¯vi,v¯j ) =1.0 - min(¯vi,v¯j 
)/ max(¯vi,v¯j ), resulting d(¯vi,v¯j ). [0, 1]. Therefore, we simply sum the two distances as the distance 
of two short-term features for a joint. Then, we de.ne the distance of two motion clips as the weighted 
sum of all joints. Finally, motion dissimilarity is calculated by dynamic time warping to align two motions 
temporally.  3 Simulation Results We have evaluated our algorithm using four categories from [CMU 
]. The retrieved result is ranked by their motion dissimilarities to the query. For comparison, we have 
implemented a prevailing method as a baseline, where the only difference from the proposal is that frame 
distance is employed instead of motion clip distance. A re­cent method FMDistance is also compared [Onuma 
et al. 2008]. Figure 2 shows the results of average P (NR ), which is the preci­sion after NR (number 
of relevant motions) motions are retrieved. * e-mail:{ji-xu,hkato,yoneyama}@kddilabs.jp  Figure 1: Short-term 
feature extraction. (left) motion clipping. (right) six pre-de.ned dynamic patterns. Figure 2: Comparison 
of average P (NR). The proposal achieves much higher performance than the other two (90.9% vs. 69.1% 
vs. 76.9% for entire testset). Since the amount of data is reduced in feature extraction, a speed-up 
of nearly 100 times has been achieved compared to the baseline in our experiments. Another merit is that 
our method requires no prior knowledge from the user, which is desired by novices, while [M¨uller et 
al. 2005] demands considerable experience to choose their features. 4 Conclusion This poster presents 
a motion retrieval algorithm with promising performance. Our main contributions are as follows: Short-Term 
Features: It is our basic idea that short-term analysis is essential for logical motion similarity. We 
pro­pose a novel feature extracted from both the dynamic pat­tern and magnitude of joint velocities in 
the short term, which are robust to motion variations. Our experiments demonstrate that short-term features 
are more effective than a single frame based method such as the baseline.  Distance Metric: A distance 
metric is carefully de.ned for our short-term features, which is a crucial ingredient of mo­tion retrieval 
technique. A trade-off is achieved to balance the dynamic pattern and average speed in our short-term 
features. Such a metric is also necessary in other techniques such as motion graphs and motion clustering. 
 Friendly to novices: It is easy to apply our short-term fea­ture in many applications including motion 
retrieval since it requires no prior knowledge from the user.  References CMU MOCAP DATABASE. http://mocap.cs.cmu.edu/. 
M¨ODER, T., AND CLAUSEN, M. Ef.cient ULLER, M., R ¨2005. content-based retrieval of motion capture data. 
ACM Transac­tions on Graphics 24, 3 (July), 677 685. ONUMA, K., FALOUTSOS, C., AND HODGINS, J. K. 2008. 
FMDistance: A fast and effective distance function for motion capture data. In EUROGRAPHICS 2008, Short 
Papers. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 
2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599314</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<display_no>13</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Story engine for interactive characters]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599314</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599314</url>
		<abstract>
			<par><![CDATA[<p>In recent years, many entertainment systems have relied on the progress of interaction technology to create characters that act autonomously. To show these lifelike characters, it is important for them to perform various actions, such as daily actions, reflex actions that require reacting to input from a user, perceiving actions where the character perceives an object and reacts to it, and actions based on personalities or feelings. This results in the problem of complex action planning. A character has to carry out the actions listed, keep schedules and maintain a personality, and react flexibly to user interaction, while still maintaining story flow. In this paper, we propose the Story Engine, which can execute various actions in multiple characters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Plan execution, formation, and generation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010199</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621355</person_id>
				<author_profile_id><![CDATA[81435601481]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katsutoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hamana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621356</person_id>
				<author_profile_id><![CDATA[81100501102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621357</person_id>
				<author_profile_id><![CDATA[81100633427]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621358</person_id>
				<author_profile_id><![CDATA[81451595006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Story Engine for Interactive Characters C:\meeting&#38;semi\2007\CG........\...\capture_small.png Katsutoki 
HAMANA, Hiroshi MORI, Atsushi NAKANO, Junichi HOSHINO System and Information Engineering, University 
of Tsukuba info@entcomp.esys.tsukuba.ac.jp Introduction In recent years, many entertainment systems 
have relied on the progress of interaction technology to create characters that act autonomously. To 
show these lifelike characters, it is important for them to perform various actions, such as daily actions, 
reflex actions that require reacting to input from a user, perceiving actions where the character perceives 
an object and reacts to it, and actions based on personalities or feelings. This results in the problem 
of complex action planning. A character has to carry out the actions listed, keep schedules and maintain 
a personality, and react flexibly to user interaction, while still maintaining story flow. In this paper, 
we propose the Story Engine, which can execute various actions in multiple characters. Story Engine ..PerceptionsVisualAuditoryHapticInternal 
statesMemoriesFeelingsPersonalities.Direction of eyes, Gestures, Appearances, etc.Narrative World ModelEpisode 
Selection MechanismRankType1Rejection2Order3Play::Social motivesEvent execution mechanismEvent selection 
mechanismEnd judgments mechanism of episodes and eventsEvent local time update mechanismAttentionStageActorEpisodedaily 
routine episodesjob dependent episodescharacter specific episodesEpisodeTree DatabaseLily Figure 2: 
Story Engine system configuration To construct lifelike animated characters, it is important to have 
them perform various actions, such as daily tasks, reflexes, acts based on their perceptions, and actions 
based on personalities or feelings. The Story Engine performs these various actions in complex narrative 
situations. Each character uses the system (Figure 2). The inputs to Story Engine are sensory information 
from the external narrative world, the character s current actions, and its internal status. Story Engine 
outputs various actions based on the input information and the stored episode group. Episodes in which 
multiple characters interact are shared by the characters. This system has three main features: . Structuring 
Actions: Conditional branching becomes very complex when the control system treats individual actions 
that are elements of units, such as standing, sitting, or waving. Thus, we define each action as one 
structure (episode tree), where multiple elements and the start or end conditions are combined hierarchically. 
 . Prioritizing Actions: A character s intentions are important in entertainment systems. Therefore, 
our system prioritizes actions based on internal states, such as the character s personality, significant 
motivations behind the actions, and the character s feelings at that time. . Interpolating Actions: 
If a character s action sequence changes without notice, the action will not seem natural. Therefore, 
it is necessary to create transitional actions to link a sequence of actions smoothly together. Pre-processing 
before each episode tree node assists in transitioning between actions and returning to an action performed 
in the past. Post-processing cleans out previous actions at the end of an episode and when current actions 
are interrupted. Story Engine interpolates the actions by inserting special processes automatically. 
 And, this system consists of four mechanisms of event selection, event execution, updating event time, 
and end judgments for episodes/events. . Event selection mechanism: This mechanism extracts events satisfying 
AND/OR conditions of a higher rank class from the episode tree under execution and selects an event 
having the highest priority in the episode. The event selection process is as follows. 1 Extracting 
candidates for the next event considering a recently finished event. 2 Adding candidates to task lists 
if conditions in an event are satisfied. 3 Sorting tasks in order of the priority. 4 Selecting the 
event having the highest priority as the next event.  . Event execution mechanism: This mechanism performs 
all actions included in the selected event. An event has a local timeline, which begins when the event 
starts. The characters perform the appropriate action corresponding to a moment in local time. . Event 
Local Time Update mechanism: This mechanism updates local time in events. . End judgments mechanism 
of episodes and events: This mechanism judges the end of episode trees and events.  Figure 1: An interactive 
application called Spilant World system using Story Engine By repeating and processing these mechanisms, 
each character selects and executes events. However, when multiple characters share the same talk like 
the narrative as an exception, these characters share the same event and perform it synchronously. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599315</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<display_no>14</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[GreenLite Dartmouth]]></title>
		<subtitle><![CDATA[unplug or the polar bear gets it]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599315</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599315</url>
		<abstract>
			<par><![CDATA[<p>With glaciers melting, sea levels rising and natural disasters---such as hurricanes and cyclones---intensifying, climate change is a growing concern. While innovations in renewable energy are critical, research shows that changing energy use behavior has become increasingly important in the fight against global warming. GreenLite Dartmouth focuses on changing behavior by making energy conservation a priority for students by creating both an intellectual and emotional connection between daily actions and their adverse effects on the environment. We combine computer graphics, art, engineering, sociology, environmental science, systems-thinking and behavioral psychology to turn real-time energy use data into a meaningful interactive display. GreenLite employs innovative methods for displaying complex data using interactivity, storytelling, animation, competition and goal-setting. Appealing animated information-display and "mood" algorithms put data into context to make it meaningful. We incorporate a system of digital energy meters, a custom database, computational analysis, 2D and 3D animations, interactive design and a game-engine to spur behavior change and, hopefully, reverse the course of climate change.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621359</person_id>
				<author_profile_id><![CDATA[81442593606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tice]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621366</person_id>
				<author_profile_id><![CDATA[81442619155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tregubov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621367</person_id>
				<author_profile_id><![CDATA[81442617630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kate]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schnippering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621368</person_id>
				<author_profile_id><![CDATA[81442608667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoon-Ki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621369</person_id>
				<author_profile_id><![CDATA[81442618477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ray]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[diCiaccio]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621370</person_id>
				<author_profile_id><![CDATA[81442600628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Friedman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621371</person_id>
				<author_profile_id><![CDATA[81442610530]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Jennifer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621372</person_id>
				<author_profile_id><![CDATA[81442614602]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Justin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621373</person_id>
				<author_profile_id><![CDATA[81442616277]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Giulia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Siccardo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621360</person_id>
				<author_profile_id><![CDATA[81442619345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Glago]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621361</person_id>
				<author_profile_id><![CDATA[81442619315]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Stephanie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trudeau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621362</person_id>
				<author_profile_id><![CDATA[81442615856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gobaud]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621363</person_id>
				<author_profile_id><![CDATA[81442596914]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>13</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garcia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621364</person_id>
				<author_profile_id><![CDATA[81442604703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>14</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slagel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621365</person_id>
				<author_profile_id><![CDATA[81100576482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>15</seq_no>
				<first_name><![CDATA[Lorie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loeb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dartmouth College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GreenLite Dartmouth: Unplug or the Polar Bear Gets It Evan Tice · Tim Tregubov · Kate Schnippering · 
Yoon-Ki Park · Ray diCiaccio · Max Friedman · Jennifer Huang · Justin Slick · Giulia Siccardo · Jessica 
Glago · Stephanie Trudeau · Daniel Gobaud · Daniel Garcia · Craig Slagel · Lorie Loeb Dartmouth College 
· June 22, 2009 With glaciers melting, sea levels rising and natural disasters such as hurricanes and 
cyclones intensifying, climate change is a growing concern. While innovations in renewable energy are 
critical, research shows that chang­ing energy use behavior has become increasingly impor­tant in the 
.ght against global warming. GreenLite Dart­mouth focuses on changing behavior by making energy conservation 
a priority for students by creating both an in­tellectual and emotional connection between daily actions 
and their adverse e.ects on the environment. We combine computer graphics, art, engineering, sociology, 
environ­mental science, systems-thinking and behavioral psychol­ogy to turn real-time energy use data 
into a meaningful interactive display. GreenLite employs innovative meth­ods for displaying complex data 
using interactivity, story­telling, animation, competition and goal-setting. Appeal­ing animated information-display 
and mood algorithms put data into context to make it meaningful. We incorpo­rate a system of digital 
energy meters, a custom database, computational analysis, 2D and 3D animations, interac­tive design and 
a game-engine to spur behavior change and, hopefully, reverse the course of climate change. GreenLite 
Dartmouth visualizes complex, real-time en­ergy data using interactive animations. When electricity use 
is low, for example, the animated polar bear is happy and playful: As use goes up, the bear becomes distressed 
and his well-being is endangered. We use a custom mood algorithm to determine which animation to display 
based on historical data and predic­tions of what energy use should be on a particular time and day. 
We created three-dimensional animations using motion capture and Autodesk Maya. At runtime, the an­imations 
run in the powerful Unity3D browser plugin; we also have a .ash version of the animations. We display 
content on low-energy displays in dormitory halls, a web­site, and a desktop widget. We chose the polar 
as the .rst character for the animation because it is an animal whose existence is signi.cantly impacted 
by climate change. In our animated world, the actions of the bear re.ect the amount of energy being used; 
to keep the bear safe and happy, dorm power use must improve. We supplement the animation with text and 
images about a user s goals, progress towards these goals, and information about cur­rent energy use. 
With just a quick glance, students can monitor the polar bear s state and instantly be informed about 
their energy impact without having to ponder any di.cult graphs or trying to decipher scienti.c units. 
As smart-grid technology brings digital meters into businesses, institutions and homes, there is a growing 
in­terest in displaying energy-use data via the web or mo- Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 bile devices. 
Data is generally displayed using charts and graphs that focus on the amount of energy used and mon­etary 
cost. Over 40 studies have shown that this type of display can reduce energy use by 5-10%. Charts and 
graphs show important information, but they are not ap­pealing to a wide demographic. Our work tries 
to appeal to a wider group, tell a bigger story and produce a larger reduction in energy use. We translate 
data from ticks on a meter to numbers in a database and then into interactive animation. We use appealing 
animated characters in set­tings a.ected by climate change to turn real-time energy use into a meaningful 
story about current energy use and how it compares to prior behavior, where energy comes from, how our 
energy use behavior compares to others and the impact of our behavior on the environment. We believe 
this emotional connection will produce a signi.­cantly larger reduction in energy use and will appeal 
to a wider group of people including children and teens who could become the driving force in energy 
conservation. Initial tests of a prototype system in four dorms, resulted in a reduction of electric 
use (plug load and lighting only) of 14% and 22% on the two .oors. We will conduct addi­tional user studies 
in April and May, 2009. GreenLite Dartmouth combines computer science, in­formation visualization, digital 
art, engineering, behav­ioral psychology and environmental science to create a system that: Pulls data 
from digital meters,  Stores the data in a custom database,  Analyzes the data using a mood algorithm 
which looks at historical data and predicts what current energy use should be at this time and on this 
day,  Uses the mood score to call up animations in which the health and happiness of a polar bear is 
depen­dent on energy use, thereby creating an emotional connection between students and the data and 
an understanding of the impacts of their actions,  Analyzes the data with contextual mapping,  Displays 
the animations and graphs on the web, kiosks, widgets and social network applications,  Visualizes short 
term and long term data,  Normalizes data, and compares it using matrices,  Provides data on multiple 
types of energy use such as electricity, water, heat and printing.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599316</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<display_no>15</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[H-link 3D]]></title>
		<subtitle><![CDATA[hyper-learning interface and navigational toolkit in 3D virtual worlds experimental interface design for cobalt, a Croquet metaverse]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599316</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599316</url>
		<abstract>
			<par><![CDATA[<p>With today's increased interest in advanced digital networking and social online communities, many higher educational institutes have been exploring online three-dimensional virtual environments as a new medium for distance learning. These technologies provide multiple features for online interaction and collaboration, a visual cyberspace for text and voice communication, the ability to manipulate multimedia in real time with a group of individuals, and the opportunity to link users to Web sites.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor>Distance learning</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.3.1</cat_node>
				<descriptor>Collaborative learning</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010489.10010492</concept_id>
				<concept_desc>CCS->Applied computing->Education->Collaborative learning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489.10010495</concept_id>
				<concept_desc>CCS->Applied computing->Education->E-learning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489.10010494</concept_id>
				<concept_desc>CCS->Applied computing->Education->Distance learning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621374</person_id>
				<author_profile_id><![CDATA[81442619775]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saba]]></first_name>
				<middle_name><![CDATA[Hashem]]></middle_name>
				<last_name><![CDATA[Kawas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Arts and Design|Animation and New Media]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SABA HASHEM KAWAS Graduate Program: Arts and Design.Animation and New Media Advisor: Patrick Fitzgerald 
 H-Link 3D Hyper-Learning Interface and Navigational toolkit in 3D Virtual Worlds Experimental Interface 
Design for Cobalt, A Croquet Metaverse With today s increased interest in advanced digital networking 
and social online communities, many higher educational institutes have been exploring online three­dimensional 
virtual environments as a new medium for distance learning. These technologies provide multiple features 
for online interaction and collaboration, a visual cyberspace for text and voice communication, the ability 
to manipulate multimedia in real time with a group of individuals, and the opportunity to link users 
to Web sites. Cobalt is an emerging open source, multi-platform virtual environment established for those 
who are interested in creating sharable virtual world technologies supporting the needs of education 
and research. It is built through a community-based effort. Cobalt is a highly collaborative multi-user 
cross-platform, and it provides a flexible framework in which developers can prototype and deploy user 
interface concepts. There is, however, no current user interface or content, and it currently lacks hyperlinking 
metaphors. The objective of this project is to conceptualize a user interface design and 3D navigational 
toolkit for the Cobalt metaverse that can support effective distance learning. The theoretical framework 
is influenced by the constructivist theories of Piaget (Explore Mode), the inferential role theories 
of Pinker (Explain Mode), and conversational learning theories of Pask (Interact Mode). The hyperlearning 
project proposal is based on two core concept pillars: The Media Explanation Generator essentially generates 
artificial intelligent 3D audiovisual explanations from a dynamic database. These explanations are manifested 
by the role of natural language generation, negotiated understanding, and the use of shared knowledge 
and context. Contextualized Hyperlinking is a means of creating visually organized links in order to 
help users select and filter information in an efficient manner. The system is influenced by Bruner s 
categorization theories and by Wurman s LATCH organization system, which includes location, alphabet, 
time, category, or hierarchy. In addition, the Contextualized Hyperlinking system is represented by visually 
customizable hierarchical arrangements of icons and media to relate categories and content. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599317</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<display_no>16</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Himawari]]></title>
		<subtitle><![CDATA[a plant robot]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599317</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599317</url>
		<abstract>
			<par><![CDATA[<p>Himawari is a sunflower robot composed of mechanical parts and electronic parts, such as servo motors, LEDs and shape-memory alloy actuators, that reacts, slowly and fluidly, facing and communicating to humans in front of it.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621375</person_id>
				<author_profile_id><![CDATA[81442619290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakayasu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ADCDU Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621376</person_id>
				<author_profile_id><![CDATA[81331505568]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kiyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomimatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Faculty of Design Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Himawari: A Plant Robot Akira Nakayasu Graduate School of Design, ADCDU Kyushu University info@ander.jp 
 Kiyoshi Tomimatsu Faculty of Design Kyushu University tomimatsu@design.kyushu-u.ac.jp 1 Introduction 
 Himawari is a sunflower robot composed of mechanical parts and electronic parts, such as servo motors, 
LEDs and shape-memory alloy actuators, that reacts, slowly and fluidly, facing and communicating to humans 
in front of it.  Figure 1: A Snapshot of Installation 2 Concept Plants, which unlike animals have 
no central nervous system, seem to spend their whole life quietly, without thinking or reacting. However, 
the more one observes the plant s biology (the photosynthesis process, blooming, cross-pollination by 
insects, etc.) the thought that some intentions exist in the plant springs to mind. Himawari means sunflower 
in Japanese. Sunflowers exhibit heliotropism which is the tendency to move the heads of their flowers 
so they face the sun, during sunrise. That movement of the flower stem and the blossoming towards the 
sun, seems to communicate a message. Figure 2: The Head of Himawari 3 Himawari Himawari is an originally 
created robotic plant, influenced by the sunflower. Using an infrared camera in the plant as eyes, Himawari 
moves, following human movement. The stalk of the plant is driven by servo motors. There are two kinds 
of LEDs in the head of the plant. Infrared LEDs which are used to reflect IR light from the human motion 
into the camera, and white LEDs that shine inside the flower. Originally developed shape-memory alloy 
actuators are used to move the petals of the flower and the tentacles inside the head. 80 shape-memory 
alloy actuators and 48 white LEDs wriggle and twinkle according to human movements. This wriggling and 
twinkling reaction introduces the expression of Himawari. The plant robot Himawari, composed of both 
mechanical and electronic parts, quietly reacts to human presence. It s slow, weak and slender movements 
interact with human motion, trying to communicate, the same way a sunflower communicates with the morning 
sun.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599318</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<display_no>17</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[IArtist]]></title>
		<subtitle><![CDATA[a self learning computer artist]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599318</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599318</url>
		<abstract>
			<par><![CDATA[<p>Designing a program that is not a tool of artistic creation, but a creator itself have been a real challenge for both digital artists and researchers. The most famous program of artistic creation is AARON [Cohen], which is in continual development since 1973 by its creator, Harold Cohen. Unfortunately, AARON cannot learn new styles or imagery by its own, each new capability must be hand-coded by Harold Cohen. Roxame [Berger], another artistic creation program created in 2001 by Pierre Berger, is based on artificial intelligence, and have its own style, emerging from both the artistic preferences of the user, and a stochastic process. This style can evolve and is refined at each work.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computer artist]]></kw>
			<kw><![CDATA[self learning]]></kw>
			<kw><![CDATA[style analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003241.10003243</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Decision support systems->Expert systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621377</person_id>
				<author_profile_id><![CDATA[81442613385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raynal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institut Gaspard Monge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621378</person_id>
				<author_profile_id><![CDATA[81442604743]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xavier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gouchet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[French Algorists]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621379</person_id>
				<author_profile_id><![CDATA[81442615071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Venceslas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institut Gaspard Monge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621380</person_id>
				<author_profile_id><![CDATA[81460640794]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Vincent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nozick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institut Gaspard Monge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Berger, P. Roxame. www.roxame.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cohen, H. Aaron. www.kurzweilcyberart.com/aaron/history.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 IArtist : A self learning computer artist *  BenjaminRaynal XavierGouchet VenceslasBiri VincentNozick 
InstitutGaspardMonge FrenchAlgorists InstitutGaspardMonge InstitutGaspardMonge Figure 1: Image before 
application, Image with style keyword monet and with style keyword picasso Keywords: Style analysis, 
self learning, computer artist 1 Introduction Designing a program that is not a tool of artistic creation, 
but a creator itself have been a real challenge for both digital artists and researchers. The most famous 
program of artistic creation is AARON[Cohen],which is incontinualdevelopmentsince1973 by its creator, 
Harold Cohen. Unfortunately, AARON cannot learn new styles or imagery by its own, each new capability 
must be hand-coded by Harold Cohen. Roxame [Berger ], another artis­tic creation program created in 2001 
by Pierre Berger, is based on arti.cial intelligence, and have its own style, emerging from both the 
artistic preferences of the user, and a stochastic process. This style can evolve and is re.ned at each 
work. These programs can create paintings, but their styles need to be hand-coded by the author, or to 
be approved by the user. We pro­poseanewmethodthatperforms iterativelyrandomprocessonthe image and submits 
the result to a judging function. If this function detects an amelioration according to the target style, 
the program continues toperformanotherprocessrandomlychosen.Else,itre­movesthelastprocessand try anotherone. 
 2 Program structure We divide the creation process into three parts splitted as follows: de.ne a painting 
subject and a style, represent the style from its Principal Component Analysis and then start the creation 
process. These threepartsaredetailed in thefollowing sections. The subject and the style The program 
requires two inputs : the subject of the painting and the reference style. These inputs can be an image, 
a set of images, oraset of words.In the lattercase,aset of imagescanbeautomat­icallyobtainedfrompublicdatabase 
likeFlikrorGoogleImage. Style analysis Foreach imageof thestyle imageset,webuildavectorcontaining valuations 
of image analysis criteria. We use an extensive list of different criteria such as: * e-mail: raynal@univ-mlv.fr 
 color histograms and gradient color histograms, from differ­ent color models(RGB,YCrCb,HSV...)  connected 
components size and shape.  signi.cantDiscreteCosinusTransform coef.cients.  quanti.cationof linesand 
circles,fromHoughTransforms.  normalized cross correlation between the input image and its .ipped image(symmetryproperties). 
 The program will perform a Principal Component Analysis of the vector set and then de.ne the Reference 
Style Subspace (RSS) among the criteria that represents the best the style. The minimal zone containing 
the projection of these style vectors represents the target zone to reach to determine whether an image 
corresponds to that style or not. Creation process The creation process transforms a subject image to 
reach the tar­get style. Our program will iteratively perform a random elemen­taryprocessonthesubjectimage 
likecoloroperations,smoothing, morphological operations,edgesharpening, imagefusion,pixeliza­tion, etc. 
For each transformation, a valuation vector is build from the candidate image and is projected on the 
RSS. The transformed image is validated if the last projection is closer to the target zone compared 
to thepreviousresult.Iftheprojected vectorreaches the target zone, thecreationprocess isstopped and thecurrent 
image is returned as the .nal result.  3 Results and discussion Figure 1 depicts a sample result of 
our method. The left image is transformed to .t to Monet style on the center image an to Picasso styleontherightimage.These 
twostyleshavebeenboth set up from a set of 30 images. We can notice that this method is subject tolocal 
minimaconvergence.Thisproblemiseasilysolved withan undohistory stack .Wealsoset up ourprogram toaccept 
up to two non-successful transformations. References BERGER, P. Roxame. www.roxame.com. COHEN, H. Aaron. 
www.kurzweilcyberart.com/aaron/ history.html. Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599319</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<display_no>18</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Thermotaxis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599319</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599319</url>
		<abstract>
			<par><![CDATA[<p>To change the spatial structures of our living spaces, we usually take an architectural approach. However once such structures have been constructed, re-configuring them incurs substantial costs. Reserving the flexibility to change spatial design is essential for the effective use of spaces. Our research aims to make spatial design flexible. New characteristics are added to an existing space using information technology, so the relationship between the space and people within changes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621381</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621382</person_id>
				<author_profile_id><![CDATA[81413597224]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akagawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of the Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621383</person_id>
				<author_profile_id><![CDATA[81413594858]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Young]]></first_name>
				<middle_name><![CDATA[Ah]]></middle_name>
				<last_name><![CDATA[Seong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621384</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Thermotaxis Takuji Narumi , Tomohiro Akagawa , Young Ah Seong , Michitaka Hirose Graduate School of 
Engineering, the University of Tokyo Graduate School of Film and New Media, Tokyo University of the Arts 
Graduate School of Interdisciplinary Information Studies, the University of Tokyo Graduate School of 
Information Science and Technology, the University of Tokyo   1. Introduction1 To change the spatial 
structures of our living spaces, we usually take an architectural approach. However once such structures 
have been constructed, re-configuring them incurs substantial costs. Reserving the flexibility to change 
spatial design is essential for the effective use of spaces. Our research aims to make spatial design 
flexible. New characteristics are added to an existing space using information technology, so the relationship 
between the space and people within changes. In this paper we propose a method of characterizing the 
space by presenting thermal information. Although thermal sensing is slower in response time and lower 
in resolution than visual and audio sensing, it is suitable for informing users of characteristics gradually, 
without giving a clear border. Moreover, a thermal spot has power to encourage people to gather together. 
By presenting thermal information, we can create a novel type of communication field. 2. Thermotaxis: 
Characterizing the Space by Presenting Thermal Information   Figure 1: Thermotaxis Thermotaxis (Fig.1) 
is a system that creates thermal sensory spots in an open space. The term "Thermotaxis" signifies a movement 
of a living organism in response to heat stimulation. The work displays temperatures in several grades. 
In this system, we use earmuff like wearable devices that provide thermal sensation depending on the 
location of people. By feeling the temperature, people distinguish different thermal areas, though there 
is no visual distinction between them. The system is designed to be controlled by a computer as an operating 
unit via wireless communication, and all electronic modules that control temperature are installed in 
the earmuff. This system consists of several earmuff-like wearable devices and a control unit that controls 
the wearable devices and recognizes their locations. Figure 2 shows the configuration of devices. An 
IEEE 1394 camera with an infrared filter for earmuff tracking is attached to the ceiling above the floor. 
Infrared LEDs are attached to the top of the earmuff device for camera tracking. To build a wearable 
device that displays warm and cool temperatures, we use an Arduino Nano. It controls two Peltier devices 
in each side of the earmuff. It also controls infrared LEDs. There are five heating levels on the Peltier 
device control. Two of them are heating, two of them are cooling and one in the middle is without heating 
or cooling level. The difference in temperature created by these Peltier devices is about 15 degrees 
to 40 degrees. The control unit and the earmuff device communicate via a Zigbee network. If the position 
is detected, the unit determines the heating level according to the thermal field and sends the temperature 
level to the earmuff device. Because the thermal fields are defined by software, we can change the map 
of fields dynamically.  Figure 2: Earmuff Device and System Configuration 3. Visitor Experience A 
thermal spot has power to encourage people to gather together. Like open fires in winter and water places 
in summer, thermal locations have been work as attractive location since early times. We observed visitors 
behavior at the experimental exhibition on December 4th to 8th. At that time, the air temperature is 
about 10 degrees Celcius, so it's cold outside. Most visitors have wandered from place to place to explore 
the difference of the presented thermal information. Over 80% of visitors have experienced the warm area 
and intend to stay there for a long while in the cold conditions. The visibility of other visitors also 
effects how visitors transition through the areas. In the open space, people can see each other. If one 
stays in a particular place, other people may be wonder why he or she is there. So, people influence 
each other by their positions. The result of this effect is that if there is at least one person in a 
warm or slightly warm area, people tend to become close to each other. In other words, people stay by 
the thermal spot.  email: {narumi, hirose}@cyber.t.u-tokyo.ac.jp email: toakmoak@gsfnm.jp email: yabird@hc.ic.i.u-tokyo.ac.jp 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599320</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<display_no>19</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Adaptive coded aperture projection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599320</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599320</url>
		<abstract>
			<par><![CDATA[<p>With adaptive coded aperture projection, we present solutions for taking projectors to the next level. By placing a programmable liquid crystal array at a projectors aperture plane we show how the depth of field (DOF) of a projection can be greatly enhanced. This allows focussed imagery to be shown on complex screens with varying distances to the projectors focal plane, such as projection domes as in planetariums or cylindrical canvases as in IMAX theaters. We demonstrate that adaptive apertures outperform previous methods of projector defocus compensation for objective lenses with static apertures. In addition, our adaptive apertures can perform the type of temporal contrast enhancement employed by common auto-iris projection lenses, and also produce high-quality depixelated images. The latter is beneficial for close-view displays with limited resolution, such as rear-projected TV sets.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621385</person_id>
				<author_profile_id><![CDATA[81421597701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grosse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621386</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621387</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621388</person_id>
				<author_profile_id><![CDATA[81100631719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Anselm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grundh&#246;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adaptive Coded Aperture Projection Max Grosse* Gordon Wetzstein Anselm Grundh¨ofer Bauhaus-University 
Weimar University of British Columbia Bauhaus-University Weimar Oliver Bimber§ Bauhaus-University Weimar 
 Figure 1: Placing a transparent liquid crystal array at the aperture plane of a projector lens allows 
encoding the aperture s mask pattern dynamically depending on the perceivable frequencies of the displayed 
images. Such adaptive coded apertures signi.cantly improve depth­of-.eld through inverse .ltering when 
compared to static circular aperture stops or broadband masks. They can also be applied for high quality 
projector de-pixelation and for increasing the temporal contrast of video sequences in a similar way 
as auto-iris projection lenses. The inlays illustrate the computed intensity codes and the applied binary 
masks. All images displayed with an adaptive coded aperture are perception optimized and have been computed 
for the given viewing conditions (50 cm distance to screen, displayed at a diagonal of 20 cm / 10 cm 
for top row / bottom row). Possible artifacts can only be perceived when observing the images at closer 
distances or larger sizes. The bottom row illustrates a comparison of adaptive apertures with various 
static (circular and broadband) ones. While the .ve images on the bottom-left are adjusted to a similar 
brightness to enable a better comparison of focus, the lower right image is captured using the same exposure 
as the image left to it. A small circular aperture has been applied that achieves the same depth-of-.eld 
as with the binarized adaptive coded aperture at the cost of a signi.cant loss of brightness. With adaptive 
coded aperture projection, we present solutions for taking projectors to the next level. By placing a 
programmable liquid crystal array at a projectors aperture plane we show how the depth of .eld (DOF) 
of a projection can be greatly enhanced. This allows focussed imagery to be shown on complex screens 
with varying distances to the projectors focal plane, such as projection domes as in planetariums or 
cylindrical canvases as in IMAX the­aters. We demonstrate that adaptive apertures outperform previ­ous 
methods of projector defocus compensation for objective lenses with static apertures. In addition, our 
adaptive apertures can per­form the type of temporal contrast enhancement employed by com­mon auto-iris 
projection lenses, and also produce high-quality de­pixelated images. The latter is bene.cial for close-view 
displays with limited resolution, such as rear-projected TV sets. Several approaches have been proposed 
to increase the DOF of con­ventional projectors using image deconvolution with known point­spread functions 
(PSF). All of these approaches share two limita­tions: Firstly, they are far from being able to reach 
real-time perfor­mance not even if the time necessary for measuring the local blur functions is not 
considered. This prevents them from displaying dy­namic content. Secondly, the amount of defocus that 
can be com­pensated through deconvolution is clearly limited when the PSF is Gaussian. Ringing artifacts 
will dominate if the blur becomes too large. In fact, only little defocus can be compensated ef.ciently 
with such techniques. *e-mail:max.grosse@uni-weimar.de e-mail: wetzste1@cs.ubc.ca e-mail:grundhoe@uni-weimar.de 
§e-mail:bimber@uni-weimar.de We show that if adaptive coded apertures are applied instead of sim­ple 
static ones (circular or coded), more image details can be recov­ered from optical defocus while a high 
light throughput is main­tained. Furthermore, our implementation uses the graphics hard­ware for computation 
and thus achieves interactive frame-rates of currently 8-16 fps at XGA resolution. Our approach computes 
and displays a dynamic aperture pattern, based on the analysis of the projected image content and on 
limita­tions of human visual perception. This analysis allows us to deter­mine and .lter out spatial 
frequencies of the input image that cannot be perceived by a human observer under the given viewing condi­tions. 
An optimal aperture can then be computed by maximizing its light transmission while preserving the perceivable 
frequencies, rather than being restricted to support a constant frequency band. We show that our adaptive 
apertures produce better results than pre­vious methods with the same or even an increased amount of 
light transmission. Adaptive apertures are also useful for planar screens that do not re­quire a large 
DOF: Defocussing the projector optically to make the pixel structure vanish, and applying deconvolution 
to recover the image details leads to better image quality. This is known as projec­tor de-pixelation. 
Our technique enhances projector de-pixelation signi.cantly. For video frames with different brightness, 
our adap­tive aperture can be scaled with respect to the mean image bright­ness to increase the temporal 
contrast of a video sequence as con­ventional auto-iris projection lenses but with a larger DOF. In 
combination with re.ective spatial light modulators, adaptive coded apertures can potentially lead to 
a new generation of auto­iris projector lenses. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599321</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<display_no>20</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Mobile tagging and mixed realities]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599321</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599321</url>
		<abstract>
			<par><![CDATA[<p>The objective of this paper/presentation is to describe the potentialities of Mobile Tagging as a tool for increasing and spreading the effects of Mixed Realities. In this sense, we will start introducing the main concepts and some examples of Mixed Realities followed by the concepts and examples of Mobile Tagging, showing that they are connected and benefit each other.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621389</person_id>
				<author_profile_id><![CDATA[81442603774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Martha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carrer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sao Paulo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621390</person_id>
				<author_profile_id><![CDATA[81442597969]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cruz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gabriel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sao Paulo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
<i>Milgram, P.&amp;Kishino, F. (1994). A Taxonomy of Mixed Reality Visual Displays. IEICE Transactions on Information Systems. Vol. E77-D, No. 12. Dec. 1994</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://www.youtube.com/watch?v=HtYjHNeR_iM]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[http://www.youtube.com/watch?v=dsb76pva4s4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[http://www.youtube.com/watch?v=P9KPJlA5yds]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[http://www.arcane-technologies.com/?page_id=19]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mobile Tagging and Mixed Realities Martha Carrer Cruz Gabriel (*) University of Sao Paulo 1 Introduction 
The objective of this paper/presentation is to describe the potentialities of Mobile Tagging as a tool 
for increasing and spreading the effects of Mixed Realities. In this sense, we will start introducing 
the main concepts and some examples of Mixed Realities followed by the concepts and examples of Mobile 
Tagging, showing that they are connected and benefit each other. Mixed Reality (or MR) refers to the 
fusion of the physical and virtual worlds to produce new environments and visualizations where physical 
and digital objects co-exist and interact in real time. On the other hand, mobile tagging is the process 
of reading a 2D barcode using a mobile device camera. Allowing the encryption of URLs in the barcodes, 
the mobile tagging can add a digital and/or online layer to any physical object, providing so several 
levels of mixed realities related to that object. The uses of these levels of mixed realities have applications 
in several areas going from medicine and engineering to arts. This paper/presentation will use an artwork 
as example to illustrate the functionality of the mobile tagging for creating mixed reality. 2 Mixed 
Reality According to the Virtuality Continuum[1] concept, the mixed reality is anywhere between the Virtual 
Environment and the Real Environment, comprising stages of reality, augmented reality, augmented virtuality 
and virtuality, as can be seen in the figure 1. Figure 1: Virtuality Continuum (source: Wikipedia) Examples 
of Virtual Reality are the immersive caves, where the interactor dives into the virtual environment. 
Examples of mixed realities applications are: MINI Cabrio [2] (advertisement)  SPOILER [3] (game) 
 BMW [4] (engine maintenance)  Arcane Technologies [5] (educational and military applications)  Several 
kinds of devices and technologies can be used as tools for mixed realities, such as glasses, gloves, 
monitors, * e-mail: martha@martha.com.br / website: www.martha.com.br computers, cameras and mobile devices 
(PDAa and cell phones). Due the pervasive nature of the mobile devices, their potentiality for increasing 
the dissemination of mixed realities is enormous and can be leveraged by mobile tagging as described 
next. 3 Mobile Tagging There are many types of 2D-barcode (tag) and it is possible to encrypt many kinds 
of data into them. However, regarding mobile tagging, the most common encrypted information are URLs. 
The process of mobile tagging can see in the figure 2. Figure 2: Mobile Tagging process (source: http://mobile­tagging.blogspot.com/2007/09/what-is-mobile-tagging.html) 
The most used patterns of 2D-barcodes for Mobile Tagging are QRcode and Datamatrix. Most of new models 
of mobile devices come already with the mobile tags (QRcode and Datamatrix) reader. Older versions of 
devices can install a reader, such as i-nigma (www.i-nigma.com). 3 Conclusion Since the mobile tags 
are simple tags that can be placed in virtually any physical object or person, added to the fact that 
the cell phones with camera have become a very inexpensive and pervasive device, the mobile tagging process 
can be said as one of the easiest and simplest way of creating mixed realities. The use of mobile tagging 
can range from expanding the information on packages, bus stop routes, museum objects, to art. A very 
interesting use of mobile tagging as mixed reality is the Semapedia.org. Another example is the SENSITIVE 
ROSE artwork, which builds an interactive compass rose formed by QRcodes that navigates into people s 
desires (www.sensitiverose.com). Mobile Tags work like physical links to the web, allowing so that virtually 
anything can be part of a expanded mixed reality environment. [1] Milgram, P. &#38; Kishino, F. (1994). 
A Taxonomy of Mixed Reality Visual Displays. IEICE Transactions on Information Systems. Vol. E77-D, No. 
12. Dec.1994. [2] http://www.youtube.com/watch?v=HtYjHNeR_iM [3] http://www.youtube.com/watch?v=dsb76pva4s4 
[4] http://www.youtube.com/watch?v=P9KPJlA5yds [5] http://www.arcane-technologies.com/?page_id=19 Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599322</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<display_no>21</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Rendering of vector objects on curved surface using pivot triangle primitives]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599322</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599322</url>
		<abstract>
			<par><![CDATA[<p>Resolution-independent rendering is important for many applications such as text rendering and rendering vector objects. This theme has attracted interest in recent years owing to the growing popularity of Flash and SVG-based applications [Loop and Blinn 2005]. We had previously presented a fast rendering method using a stencil buffer for deformable vector objects [Kokojima et al.]. One of the advantages of this method is that retriangulation is unnecessary when vector objects deform interactively. However, [Kokojima et al.] only deal with rendering vector objects on a flat surface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621391</person_id>
				<author_profile_id><![CDATA[81442607057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Norihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corp. Research & Development Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621392</person_id>
				<author_profile_id><![CDATA[81319495117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kokojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corp. Research & Development Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621393</person_id>
				<author_profile_id><![CDATA[81100323314]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yasunobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamauchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corp. Research & Development Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1179997</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kokojima, Y., Sugita, K., Saito, T., and Takemoto, T. Resolution independent rendering of deformable vector objects using graphics hardware. ACM SIGGRAPH 2006 Article No. 118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073303</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Loop, C., and Blinn, J. 2005. Resolution independent curve rendering using programmable graphics hardware. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Papers</i>, ACM, New York, NY, USA, 1000--1009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: The outline of our method. 1 Introduction Resolution-independent rendering is important for 
many applica­tions such as text rendering and rendering vector objects. This theme has attracted interest 
in recent years owing to the growing popularity of Flash and SVG-based applications [Loop and Blinn 2005].We 
had previously presented a fast rendering method using a stencil buffer for deformable vector objects 
[Kokojima et al. ]. One of the advantages of this method is that retriangulation is unnec­essary when 
vector objects deform interactively. However, [Koko­jima et al. ] only deal with rendering vector objects 
on a .at surface. We present a new method of vector rendering on a deformable curved surface, using programmable 
graphics hardware. Our method, which is an extension of [Kokojima et al. ], ef.ciently ren­ders vector 
objects on a curved surface when a curved surface and vector objects deform interactively. [Kokojima 
et al. ] draw pivot triangles to a stencil buffer using a bitwise inversion operator; pivot triangles 
are generated from an arbitrary point, line segments, and B´ezier curves of given vector objects. Thus, 
when rendering a vec­tor image on a curved surface, [Kokojima et al. ] need to subdivide pivot triangles 
into dense triangles. In contrast, our method de.nes pivot triangles into a texture coordinate space, 
and decides rendered (discarded) pixels on a texture coordinate space. Thus, our method does not involve 
subdividing pivot triangles when rendering vector objects on a curved surface. 2 Algorithm Figure1 shows 
the outline of our method. Our method requires a curved surface, which is expressed by triangles, and 
vector objects. A curved surface and vector objects are de.ned into a model space and a texture coordinate 
space respectively. Vertices on a curved surface have a u,v parameter corresponding to a position on 
the curved surface; a u,v parameter is usually de.ned from 0 to 1.0. Our method consists of the following 
steps(see Figure2): 1. Curved surface rasterization: A color and u,v parameter of each pixel are de.ned 
by a given curved surface. The u,v parameter is a position of each pixel on a texture coordinate space. 
* e-mail:norihiro.nakamura@toshiba.co.jp e-mail:yoshiyuki.kokojima@toshiba.co.jp e-mail:yasunobu.yamauchi@toshiba.co.jp 
 Figure 2: The process .ow of our method. 2. Pivot triangle de.nition: Pivot triangles are generated 
from line­edges and curve-edges of a given vector object. In particular, line­edged triangles are de.ned 
by the arbitrarily selected point and the .lled dots of the individual closed path, and curve-edged triangles 
are de.ned by a start point, an end point and a control point of each B´ezier curve. 3. Output pixel 
determination: We assume that u,v parameter from step1 is a position of each pixel on a texture coordinate 
space. In the .rst step, our method prepares a counter for each pixel, and judges whether each pixel 
is inside or outside the individual line-edged triangles on a texture coordinate space. If a pixel is 
inside the line­edged triangle, the counter is incremented by one. Subsequently, our method judges if 
a pixel is inside the convex region by evaluat­ing an implicit equation de.ned by curve-edged triangles. 
If a pixel is inside the convex region, the counter is incremented. Then, if the counter of a pixel is 
an odd number, the pixel, corresponding to the counter, is written to a frame buffer.  3 Results We 
applied our method to rendering of Japanese TrueType fonts on a curved surface. Our method rendered dynamically 
deforming 53 characters at about 35 fps. References KOKOJIMA, Y., SUGITA, K., SAITO, T., AND TAKEMOTO, 
T. Resolution independent rendering of deformable vector objects using graphics hardware. ACM SIGGRAPH 
2006 Article No.118. LOOP, C., AND BLINN, J. 2005. Resolution independent curve rendering using programmable 
graphics hardware. In SIG-GRAPH 05: ACM SIGGRAPH 2005 Papers, ACM, New York, NY, USA, 1000 1009. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599323</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<display_no>22</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Three-dimensional auto-stereoscopic animated image with a long viewing distance using high-precision image correction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599323</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599323</url>
		<abstract>
			<par><![CDATA[<p>Three-dimensional (3-D) displays have got a lot of attention because they have a much higher sense of realism and are more intuitive than 2-D displays. In particular, autostereoscopic displays are suitable for everyday use because they can be observed from an arbitrary viewpoint without supplementary glasses or tracking devices. Integral Videography (IV) is one of the methods for autostereoscopic animated images that extends Integral Photography (IP) to animation. IP/IV uses a combination of a lens array and a number of calculated elemental images with different perspectives. In particular, IV with a depth of several meters can be applied in many areas. However, most IV reports have an image depth of only several centimeters. Only a small deviation of a lens from its designed position would result in several degrees of deviation of the light ray from the back of the lens. We have developed the static autostereoscopic image by projecting the light sources from an object onto a photographic film through the lens-array [Liao et al. 2005]. In this study, we obtained animated IV of 1 m image depth with less distortion using our method to correct IV images. The method is technically unique.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621394</person_id>
				<author_profile_id><![CDATA[81442620114]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takehito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teraguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621395</person_id>
				<author_profile_id><![CDATA[81384618229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiromasa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621396</person_id>
				<author_profile_id><![CDATA[81100447521]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masamune]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621397</person_id>
				<author_profile_id><![CDATA[81100169268]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dohi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621398</person_id>
				<author_profile_id><![CDATA[81100507773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hongen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Liao, H., Iwahara, M., Katayama, Y., Hata, N., and Dohi, T. 2005. Three-dimensional display with a long viewing distance by use of integral photography. <i>Opt. Lett. 30</i>, 6 (Mar.), 613--615.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Three-dimensional Auto-stereoscopic Animated Image with a Long Viewing Distance using High-precision 
Image Correction Takehito Teraguchi, Hiromasa Yamashita:, Ken Masamune, Takeyoshi Dohi , Hongen Liao 
The University of Tokyo 1 Introduction Three-dimensional (3-D) displays have got a lot of attention 
be­cause they have a much higher sense of realism and are more in­tuitive than 2-D displays. In particular, 
autostereoscopic displays are suitable for everyday use because they can be observed from an arbitrary 
viewpoint without supplementary glasses or tracking devices. Integral Videography (IV) is one of the 
methods for au­tostereoscopic animated images that extends Integral Photography (IP) to animation. IP/IV 
uses a combination of a lens array and a number of calculated elemental images with different perspectives. 
In particular, IV with a depth of several meters can be applied in many areas. However, most IV reports 
have an image depth of only several centimeters. Only a small deviation of a lens from its de­signed 
position would result in several degrees of deviation of the light ray from the back of the lens. We 
have developed the static au­tostereoscopic image by projecting the light sources from an object onto 
a photographic .lm through the lens-array [Liao et al. 2005]. In this study, we obtained animated IV 
of 1 m image depth with less distortion using our method to correct IV images. The method is technically 
unique. 2 Our Approach The con.guration and procedure of image correction are shown in Figure 2. We 
capture test IV and measure its distortion. Then, IV elemental images are shifted with a sub pixel unit 
using bi-linear interpolation according to the distortion to reproduce the IV cor­rectly. Our method 
of correction is shown in Figure 3. Each lens is precisely arranged as designed (Fig. 3a). The balloon 
shows an enlarged view of deviation in rays of an image behind the right lens shifted by 1 pixel (Fig. 
3b). If the pixel values are changed accord­ing to the lens position, observers obtain a precise image 
(Fig. 3c). We obtained precise 3-D animated images for a long viewing dis­tance with less distortion 
(Fig. 1). In all viewpoints, the center of the poles corresponds approximately to the white point on 
the dif­fuser plate set 1 m in front of the screen. We developed a correction method for high-precision 
autostereo­scopic animated images taken at long viewing distances. It allows *e-mail: {t teraguchi, hiromasa}@atre.t.u-tokyo.ac.jp 
e-mail: {masa, takdohi}@i.u-tokyo.ac.jp e-mail: liao@bmpe.t.u-tokyo.ac.jp Figure 3: IV image correction 
This work was supported in part by the Grant for Industrial Technology Research (07C46050) of NEDO, Japan, 
the SCOPE (062103006) of the MIC in Japan, and the JSPS in Japan. References LIAO, H., IWAHARA, M., 
KATAYAMA, Y., HATA, N., AND DOHI, T. 2005. Three-dimensional display with a long viewing distance by 
use of integral photography. Opt. Lett. 30, 6 (Mar.), 613 615. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599324</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<display_no>23</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[The UnMousePad]]></title>
		<subtitle><![CDATA[the future of touch sensing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599324</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599324</url>
		<abstract>
			<par><![CDATA[<p>Multi-touch input has been an active area of research for over two decades but has always suffered from the absence of an easily available high quality touch input device. For this reason, exciting user interfaces developed in the lab have appeared on CNN, but not on everyone's desk, computer screens, table-tops, walls and floors. What has been needed - and lacking - is a better mousetrap; an inexpensive, flexible and sensitive <i>touch imaging</i> technology.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621399</person_id>
				<author_profile_id><![CDATA[81319500075]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ilya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621400</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621401</person_id>
				<author_profile_id><![CDATA[81416609292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hendee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621402</person_id>
				<author_profile_id><![CDATA[81416604520]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621403</person_id>
				<author_profile_id><![CDATA[81416601724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nadim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Awad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The UnMousePad -The Future of Touch Sensing Ilya Rosenberg*, Charles Hendee , Ken Perlin , Alex Grau§, 
Nadim Awad¶ New York University Media Research Lab Figure 1: Left: Writing/leaning on an UnMousePad 
and the captured force image. Right: Personal work space of the future. 1 Introduction Multi-touch input 
has been an active area of research for over two decades but has always suffered from the absence of 
an easily avail­able high quality touch input device. For this reason, exciting user interfaces developed 
in the lab have appeared on CNN, but not on everyone s desk, computer screens, table-tops, walls and 
.oors. What has been needed -and lacking -is a better mousetrap; an inexpensive, .exible and sensitive 
touch imaging technology. The UnMousePad is that better mousetrap -a novel form of input sensor that 
enables inexpensive multi-touch, pressure acquisition at both small and large form-factors. It can accurately 
measure en­tire images of pressure with continuous bilinear interpolation, per­mitting both high-frame-rate 
and high quality imaging of spatially variant pressure upon a surface. Though the use of force variable 
resistors as multiple points of con­tact input devices is not new, previous work in this area has fo­cused 
mainly on arrays of discrete and independent sensors. The key difference between the UnMousePad and previous 
technolo­gies is that it is based on Interpolating Force Sensitive Resistance (IFSR), which closely mimics 
the multi-resolution properties of hu­man skin, in which the position of a touch can be detected at .ner 
scale than the discrimination of multiple touches. The development of the UnMousePad and other IFSR based 
sen­sors and an improved understanding of their electrical properties enhances the type and quality of 
information that may be obtained in situations where entire images of pressure need to be acquired in 
real-time or in situations where multiple points of pressure need to be continuously tracked. 2 Demonstration 
For our demonstration, we plan to show how we envision ordinary people will use IFSR based sensors in 
their daily lives in the not too distant future. We plan to set up several demonstration areas with the 
following sensor form factors: 1. The future of portable electronic devices: A 2.5 x 3.5 sen­sor will 
be placed on the back of a portable electronic device *e-mail:ilya@cs.nyu.edu e-mail:perlin@cs.nyu.edu 
e-mail:chendee@gmail.com §e-mail:alexgrau@gmail.com ¶nadim.awad@gmail.com having a small screen. The 
device will be operated by press­ing on the back, keeping .ngers from obscuring and leaving smudges on 
the screen. 2. The future of electronic paper: An 8.5 x 11 sensor will be integrated below the eInk 
display of a portable ebook reader. Users will be able to intuitively .ip through pages and write on 
the electronic paper. 3. The future personal work space: A transparent 12 x 16 sen­sor will be placed 
over a 1/8 thick LCD display which will lay .at, having the appearance of an ink blotter. There will 
also be a large 30 display in front of the user with a web cam mounted on top and looking down at the 
user s hands in order to track their position. Software applications will demonstrate the power of this 
interface and its advantages over keyboards and mice. 4. The future of play: A circular 14 diameter 
hand-drum sen­sor will be mounted on a drum stage and covered with a soft rubber pad. Like a real drum, 
the drum will produce different timbres of sound depending on how it is hit, and users will be able to 
change the pitch of the drum by pressing down with one hand while hitting with the other.  For our demonstrations 
of some of the ways in which these devices can be used, we will present the following software applications: 
1. Various visualization of pressure measured by the devices. 2. Tracking of .ngers and pens; writing 
through a pad of paper. 3. Use as a traditional keyboard and mouse. 4. Six degree of freedom manipulation 
of virtual objects. 5. Painting with an overhead camera used to track hands and display their absolute 
position on the screen. 6. Sculpting of surfaces, spherical planet terrain, and clay. 7. Instruments 
including a piano, a synthesizer with continuous tones, FOF voice synthesis, a Theremin, and a hand-drum. 
 8. Animation of a water surface, faces, and virtual characters.  3 Conclusion IFSR based sensor technology 
is inherently unobtrusive, inexpen­sive, and very durable. It has a very wide range of potential appli­cations 
in many sectors of society, enabling multi-touch pressure imaging at a low cost in a wide variety of 
form factors. In our talk, demonstration and poster, we plan to show the technical aspects that underly 
the IFSR technology, and to inspire people to think about the myriad of novel form factors and exciting 
applications that are made possible by this revolutionary new technology. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599325</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<display_no>24</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[A saccade-contingent display for suppressing color breakup]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599325</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599325</url>
		<abstract>
			<par><![CDATA[<p>This work demonstrates a novel channel for a smart display to interact with its user for enhancing the viewing experience. By using a wearable electro-oculography (EOG) circuit, the saccadic eye movements can be detected so that the user's viewing mode can be determined. Different gamut settings are used in the "fixation mode" versus "saccade mode," such that the fast eye movement induced artifacts can be suppressed. Furthermore, by analyzing the saccade patterns, we can determine the user is in the "image viewing" mode or "text reading" mode. Then different brightness, contrast, saturation settings can be assigned accordingly and automatically to improve the user's comfort level.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621404</person_id>
				<author_profile_id><![CDATA[81408595770]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wei-Chung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Chiao Tung University, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621405</person_id>
				<author_profile_id><![CDATA[81442610393]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jih-Fon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Technology Research Institute, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1278286</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ando, H., Watanabe, J., Amemiya, T., Maeda, T. 2007. Fullscale saccade-based display: Public/Private image presentation based on gaze-contingent visual illusion. <i>SIGGRAPH 2007 Emerging Technologies</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bergquist, J., and Wennstam C. 2006. Field-sequential-colour display with adaptive gamut. <i>SID Symposium Digest 37</i>, 1594--1597.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Saccade-contingent Display for Suppressing Color Breakup Wei-Chung Cheng Jih-Fon Huang National Chiao 
Tung University, Taiwan*Industrial Technology Research Institute, Taiwan (a) (b) (c) Figure 1: (a) Saccadic 
color breakup. (b)Detecting horizontal saccades. (c) Electro-oculography circuit. Abstract This work 
demonstrates a novel channel for a smart display to interact with its user for enhancing the viewing 
experience. By using a wearable electro-oculography (EOG) circuit, the saccadic eye movements can be 
detected so that the user's viewing mode can be determined. Different gamut settings are used in the 
"fixation mode" versus "saccade mode," such that the fast eye movement induced artifacts can be suppressed. 
Furthermore, by analyzing the saccade patterns, we can determine the user is in the "image viewing" mode 
or "text reading" mode. Then different brightness, contrast, saturation settings can be assigned accordingly 
and automatically to improve the user's comfort level. Color Breakup The field sequential display synthesizes 
colors in the time domain. By quickly flashing the red, green, and blue field one after the other, the 
observer is unable to distinguish the time difference between the three channels. If such technology 
can be adapted for liquid crystal displays, not only its resolution and luminance efficiency can be increased 
to 3X, which equates to considerable power savings, but the hardware cost can also be cut down to 80% 
because the costly color filter process on the glass substrate can be eliminated. Unfortunately, the 
most infamous artifact on field sequential displays, the color breakup phenomenon (CBU), occurs when 
the red, green, and blue components of the same object project onto different locations of retina upon 
eye movement. Due to different types of eye movements, color breakup may present in different forms. 
The pursued CBU occurs when the gaze position pursues the moving object smoothly. The saccadic CBU, on 
the other hand, occurs when the gaze position jumps from point A to point B instantly (Figure 1a). Because 
saccadic eye movement has much higher velocity and much wider amplitude, the saccadic CBU is much more 
difficult to cope with than pursued CBU. *e-mail: waynecheng@mail.nctu.edu.tw jfhuang@itri.org.tw 2 Our 
Approach Our approach is an adaptive display system. Since CBU only occurs during eye movement, when 
there is no eye movement, the display image should be intact in order to preserve the image quality. 
Only when eye movement is detected, which may lead to CBU, should the image be manipulated in order to 
suppress the CBU-induced artifacts. Two components are required: an electro­oculography circuit to detect 
the signals that trigger eye movements and a gamut modulator to reduce the image chroma by adjusting 
the RGB LED backlights. 3 Implementation Electro-oculography is a technique for measuring the resting 
potential of the retina. A pair of electrodes is placed to the left and right of the eyes, and an indifferent 
electrode is placed on the wrist (Figure 1b). By measuring change of the potential difference caused 
by rotation of eyeballs, the eye movement events can be detected [Ando 2007]. The electrical potential 
level of electro­oculography is so low that it has to be carefully amplified and filtered to reject the 
ambient noises including the electromagnetic radiation noise from the 60Hz power line, the electrical 
signals generated by muscle activities and other organs (Figure 1c). The electro-oculography paradigm 
provides information of when saccadic CBU is likely to happen, which is orthogonal to how saccadic CBU 
will be reduced. Therefore, most techniques of reducing CBU in literature can be applied in our framework. 
Our method is to reduce chroma by mixing the RGB primaries [Bergquist 2006] to obtain 8 different sizes 
of gamut.  References ANDO, H., WATANABE, J., AMEMIYA, T., MAEDA, T. 2007. Full­ scale saccade-based 
display: Public/Private image presentation based on gaze-contingent visual illusion. SIGGRAPH 2007 Emerging 
Technologies. BERGQUIST, J., AND WENNSTAM C. 2006. Field-sequential-colour display with adaptive gamut. 
SID Symposium Digest 37, 1594­ 1597. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599326</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<display_no>25</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Mobile screen transition animations]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599326</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599326</url>
		<abstract>
			<par><![CDATA[<p>Mobile devices have limitations compared to PCs due to their inferior computing power and small screens, but a successful design of animated transitions can hide processing delays and make the user experience smoother. In this paper, we describe the design of animated transitions and present a user study on how they are perceived.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Portable devices (e.g., laptops, personal digital assistants)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621406</person_id>
				<author_profile_id><![CDATA[81350586422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jussi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huhtala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nokia Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621407</person_id>
				<author_profile_id><![CDATA[81442613980]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ari-Heikki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sarjanoja]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nokia Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621408</person_id>
				<author_profile_id><![CDATA[81100147371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jani]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#228;ntyj&#228;rvi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical Research Centre of Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621409</person_id>
				<author_profile_id><![CDATA[81100322492]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Minna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Isomursu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical Research Centre of Finland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621410</person_id>
				<author_profile_id><![CDATA[81100417488]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jonna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[H&#228;kkil&#228;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nokia Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>526507</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cooper, A. 1995. About Face: <i>The Essentials of User Interface Design</i>. John Wiley&amp;Sons Inc., New York, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1294231</ref_obj_id>
				<ref_obj_pid>1294211</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Harrison, C., Amento, B., Kuznetsov, S. and Bell, R. 2007. Rethinking the Progress Bar. <i>In proceedings of UIST'07</i>. ACM, 115--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37407</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lasseter, J. 1987. Principles of Traditional Animation Applied to 3D Computer Animation. In <i>Computer Graphics (Proceedings of ACM SIGGRAPH 87)</i>, ACM, 35--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[McCrickard, D., Zhao Q. and Stasko, J. 1999. Exploring Animation as a Presentation Technique for Dynamic Information Sources. <i>In GVU Technical Report</i>, GIT-GVU-99-47, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>317459</ref_obj_id>
				<ref_obj_pid>317456</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Myers, B. A. 1985. The importance of percent-done progress indicators for computer-human interfaces. In <i>Proceedings of the 1985 SIGCHI Conference on Human Factors in Computing Systems</i>. CHI '85. ACM, 11--17.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mobile Screen Transition Animations Jussi Huhtala1 Ari-Heikki Sarjanoja1 Jani Mäntyjärvi2 Minna Isomursu2 
Jonna Häkkilä1 1Nokia Research Center 2Technical Research Centre of Finland Yrttipellontie 1, 90230 
Oulu, Finland jussi.huhtala@nokia.com +358408036600 Abstract Mobile devices have limitations compared 
to PCs due to their inferior computing power and small screens, but a successful design of animated transitions 
can hide processing delays and make the user experience smoother. In this paper, we describe the design 
of animated transitions and present a user study on how they are perceived. Introduction The small screen 
size and inferior computing power of mobile devices compared to PCs make them an interesting platform 
for UI design and offer new challenges that require the designer s attention. The role of information 
visualization is emphasized as presentation capabilities are restricted in the faculties of size and 
resolution. Animations can be used to make the application more pleasing by creating a perception of 
natural and smooth movements on the UI [Lasseter 1987], as well as to improve the usability and efficiency 
of the user interface. Objects too large to fit to the screen as such can be presented with animations 
[McCrickard 1999], and the user s attention can also be guided as her eyes are caught by the movement 
[Cooper 1995]. Animated transitions can be used to fill the latency times caused by data transfer or 
processor delay. Changing from one user interface view to another is a common stage for delays to occur. 
A transition animation can be used to patch the moment and the user feels time passing faster than it 
does, as done e.g. with progress bars [Myers 1985]. Moreover, it has been found that the acceleration 
of an animation makes its duration feel shorter. [Harrison 2007]. User study In our experiment we measured 
how different timing of fade out, fade in, and zoom out of the image affect the experienced speed in 
a mobile image browser demo (Figure 1.). The test setting consisted of emulating a mobile phone UI with 
flash demo running on a laptop PC. Participants were comparing two transitions and decided which of them 
felt faster. The user study involved 26 participants, who were asked to compare the perceived speed of 
24 animation pairs and respond which transition in each pair was the faster. The first image disappeared 
with a fade-out effect and the second appeared by fading in.  Half of the animation pairs had at least 
one transition which used a gentle zoom effect while fading out.  The moment when the image was changed 
during the overall  transition was varied. Visiblity fade in fade out Time Without zoom effect With zoom  
effect Figure 1. Results Results show that users tend to rate the transitions fast if the first image 
disappears quickly. The fade-in speed of the next view is secondary. According to the results, it is 
most important to bring at least a glimpse of the new content to the screen as quickly as possible. Our 
findings indicate that the psychological mechanism of transitions is completely different from progress 
bars [Harrison 2007]. Progress bars seem to perform best when the bar accelerates towards the end. The 
effect is opposite in UI transitions. The key element is how quickly the new view is presented to the 
user. Using zooming in transitions strengthened the effect.  References COOPER,A.1995. About Face: The 
Essentials of User Interface Design. John Wiley &#38; Sons Inc, New York, 1995. HARRISON,C., AMENTO,B., 
KUZNETSOV,S.andBELL,R.2007. Rethinking the Progress Bar. In proceedings of UIST 07. ACM, 115-118. LASSETER,J.1987. 
Principles of Traditional Animation Applied to 3D Computer Animation. In Computer Graphics (Proceedings 
of ACM SIGGRAPH 87), ACM, 35-44. MCCRICKARD,D.,ZHAO Q.andSTASKO,J.1999. Exploring Animation as a Presentation 
Technique for Dynamic Information Sources. In GVU Technical Report, GIT-GVU-99-47, 1999. MYERS,B.A. 1985. 
The importance of percent-done progress indicators for computer-human interfaces. In Proceedings of the 
1985 SIGCHI Conference on Human Factors in Computing Systems. CHI '85. ACM, 11-17.  Copyright is held 
by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599327</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<display_no>26</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[rAir flow menus]]></title>
		<subtitle><![CDATA[toward reliable 3D gestural input for radial marking menus]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599327</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599327</url>
		<abstract>
			<par><![CDATA[<p>Despite the many challenges understanding how best to interact with large format displays, they are becoming increasingly popular for data analysis tasks in a variety of domains, including scientific, information, and geo-visualization. (Figure 1a shows a relatively small, 60" display; even larger, wall-size displays are also popular.) In order to make the most effective use of the full display, users typically stand and walk around in these environments. In fact, this physical navigation has been shown to be beneficial in data analysis tasks [1]. Since immobile input devices, such as mice, keyboards, or pen-tablets, do not naturally support interaction "on the move", new interactive techniques are needed to facilitate fluid interaction across a range of distances when working with large-format displays. We believe body-centric 3D, gestural input is particularly promising in this regard. Our work investigates techniques for reliable menu selection based upon these ideas, introducing new 3D input strategies for controlling menus. Our work builds upon previous techniques, such as rapMenu [3], which uses rotational hand movements and finger pinches to control menus from a distance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621411</person_id>
				<author_profile_id><![CDATA[81442616211]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Danny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Minnesota]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621412</person_id>
				<author_profile_id><![CDATA[81100317632]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Keefe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Minnesota]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1240656</ref_obj_id>
				<ref_obj_pid>1240624</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BALL, R. NORTH, C., and BOWMAN, D. A. 2007. Move to improve: promoting physical navigation to increase user performance with large displays. <i>In Proceedings of CHI 2007 (May. 2007)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>354778</ref_obj_id>
				<ref_obj_pid>354401</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[GUIMBRETIERE, F. and WINOGRAD, T. 2000. FlowMenu: combining command, text, and data entry. <i>In Proceedings of UIST 2000 (Nov. 2000)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1544221</ref_obj_id>
				<ref_obj_pid>1544196</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[NI, T., MCMAHAN, R. P., and BOWMAN, D. A. 2006. Tech-note: rapMenu: Remote Menu Selection Using Freehand Gestural Input. <i>In</i> Proceedings of IEEE 3DUI 2008 <i>(Mar. 2008)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 rAir Flow Menus: Toward Reliable 3D Gestural Input For Radial Marking Menus Danny Rado, Daniel F. Keefe 
University of Minnesota, Department of Computer Science and Engineering rado@cs.umn.edu, keefe@cs.umn.edu 
(d)  (c)  (a) (b)  Figure 1: rAir Flow Menus variable-distance interaction styles (a) User decides 
to stand at a small distance from the display. (b) Magnetic hand tracker (c) User s hand is pointed up 
away from display. (d) Interaction technique conceptual model. 1 Introduction Despite the many challenges 
understanding how best to interact with large format displays, they are becoming increasingly popular 
for data analysis tasks in a variety of domains, including scientific, information, and geo-visualization. 
(Figure 1a shows a relatively small, 60 display; even larger, wall-size displays are also popular.) In 
order to make the most effective use of the full display, users typically stand and walk around in these 
environments. In fact, this physical navigation has been shown to be beneficial in data analysis tasks 
[1]. Since immobile input devices, such as mice, keyboards, or pen-tablets, do not naturally support 
interaction on the move , new interactive techniques are needed to facilitate fluid interaction across 
a range of distances when working with large-format displays. We believe body-centric 3D, gestural input 
is particularly promising in this regard. Our work investigates techniques for reliable menu selection 
based upon these ideas, introducing new 3D input strategies for controlling menus. Our work builds upon 
previous techniques, such as rapMenu [3], which uses rotational hand movements and finger pinches to 
control menus from a distance. 2 Approach rAir Flow Menus are modeled on traditional Flow Menus [2], 
a Marking Menu implementation that requires the user to return to the menu center to commit selections. 
This tablet technique helps the user navigate to an arbitrary menu depth quickly without lifting the 
pen for each menu selection. Our contribution is redesigning this approach to be driven by 3D input. 
In rAir Flow Menus, the user activates the menu by pressing down a hand-held button attached to a 6-DOF, 
magnetic tracker. The tracker s orientation and position at the time of the initial press are used to 
specify the ray, R, shooting out of the hand; and the plane, P, centered in front of the hand with normal 
equal to the inverse of R s direction (see Figure 1d). After the initial click, as the user s hand moves, 
the direction of R (but not the origin) is updated continuously based on the tracker s orientation, and 
strokes are drawn over the menu by mapping the intersection of R with P into the coordinate space of 
the menu. Thus, regardless of the user s initial hand position or distance from the display, the menu 
will always be available directly in front of her hand. She may point directly at the display, or she 
may operate the menu more naturally with her hand by her side. By updating only the direction of R with 
each hand movement, the technique ignores accidental translational movement of the hand. To better understand 
the efficacy of our technique, we conducted an informal pilot evaluation of a hierarchical menu selection 
task. Participants were given a two-minute introduction to the technique and an additional two minutes 
to practice. They were then asked to spell a three-letter word by traversing three levels in a simple 
menu hierarchy, where all menus had the same letters [A-G] placed in the same positions; participants 
were asked to spell: BAD , FED , BEG , GAG , and ACE . On average, task completion time was roughly five 
seconds per multi-level selection, and participants made an average of one error over the course of the 
five trials. These pilot data suggest that further refinement of the technique will be valuable to increase 
speed and accuracy; however, the fact that novice users were able to quickly understand a radically different 
style of menu selection, based upon flowing motion of the hand in the air, is promising. 3 Future Work 
We plan to conduct formal user studies to determine how performance is affected by the number of menu 
items, the inner menu circle size, and alternatives to direction-based control. We are currently incorporating 
rAir Flow Menus into a 3D application running on a large, tiled display. References [1] BALL, R. NORTH, 
C., and BOWMAN, D. A. 2007. Move to improve: promoting physical navigation to increase user performance 
with large displays. In Proceedings of CHI 2007 (May. 2007). [2] GUIMBRETIERE, F. and WINOGRAD, T. 2000. 
FlowMenu: combining command, text, and data entry. In Proceedings of UIST 2000 (Nov. 2000). [3] NI, T., 
MCMAHAN, R. P., and BOWMAN, D. A. 2006. Tech-note: rapMenu: Remote Menu Selection Using Freehand Gestural 
Input. In Proceedings of IEEE 3DUI 2008 (Mar. 2008). 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599328</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<display_no>27</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Temperature design display device to use peltier elements and liquid crystal thermograph sheet "Thermo-Pict"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599328</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599328</url>
		<abstract>
			<par><![CDATA[<p>"Thermo-Pict" is a design apparatus produced by applying temperature visualization technology linked to an information display with the use of a thermograph sheet. Thermography is used to visualize the surface temperature of objects through their depiction as colors. This technology has been used primarily in the medical and research fields. Thermography display colors come in a wide range of hues and brightness that enables quick visualization of any object's surface temperature distribution. Use of this technology will be attempted as a tool in the production of design displays. [Fig. 1]</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621413</person_id>
				<author_profile_id><![CDATA[81442604614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kensuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621414</person_id>
				<author_profile_id><![CDATA[81442614706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kyoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Higurashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621415</person_id>
				<author_profile_id><![CDATA[81442606589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tatsuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621416</person_id>
				<author_profile_id><![CDATA[81442594241]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Misako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ota]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621417</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621418</person_id>
				<author_profile_id><![CDATA[81100274861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1180027</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kushiyama, K., Inose, M., Yokomatsu, R., Fujita, K., Kitazawa, T., Tamura, M., and Sasada, S. 2006. Thermoesthesia: about collaboration of an artist and a scientist. In <i>ACM SIGGRAPH 2006 Sketches</i>]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Temperature Design Display device to use Peltier elements and liquid crystal thermograph sheet Thermo-Pict 
Kensuke Takada Kyoko Higurashi Tatsuhiko Suzuki Misako Ota Tetsuaki Baba Kumiko Kushiyama Tokyo Metropolitan 
University 1.Introduction Thermo-Pict is a design apparatus produced by applying temperature visualization 
technology linked to an information display with the use of a thermograph sheet. Thermography is used 
to visualize the surface temperature of objects through their depiction as colors. This technology has 
been used primarily in the medical and research fields. Thermography display colors come in a wide range 
of hues and brightness that enables quick visualization of any object s surface temperature distribution. 
Use of this technology will be attempted as a tool in the production of design displays. [ Fig.1] Fig. 
1: Thermo-Pict One typical product that uses Peltier elements is a visual tactile display known as Thermoesthesia 
, as reported by Kushiyama et al. This is an interactive product in which the positions of the tester 
s fingers are detected by a touch panel, and simultaneous control of the temperature detected by Peltier 
elements and the CG image projected on the display can be performed. One limitation of this product is 
the requirement of a projector, which results in a restriction of the display area size. In the present 
study, visualization of information was performed using a thermograph sheet, which enables the display 
even in a well lit area using compact sized instrumentation. 2. System configuration 2.1 Outline For 
Thermo-Pict , which utilizes Peltier elements, the image on the liquid crystal thermograph sheet continuously 
changes. After being touched by an individual, the image is further modified according to the body temperature 
detected. Also, an individual using this technology is able to sense the temperature change caused by 
Peltier elements. Thus the Thermo-Pict product utilizes displays based on both visual and tactile senses. 
[Fig.2] Fig.2 System outline   2.2 Implementation For development of our product, a liquid crystal 
thermograph sheet (C-Task Company) was used. This device responds to a temperature between 20°C ~ 32°C, 
maintaining a black hue at temperatures below 20°C and a bluish green hue at temperatures above 32°C. 
In all, 7 temperature-sensitive colors can be displayed: black, reddish brown, yellowish green, blue, 
purple, green and bluish green. A color chart produced by the manufacturer is shown in Table 1. For this 
study, the display was made by first placing a total of 36 Peltier elements in a 6 x 6 configuration 
occupying a 30 mm x 30 mm area on an aluminum sheet (to dissipate heat) and then covering the Peltier 
elements with a 250 mm x 250 mm liquid crystal thermograph sheet. The surface of the thermograph sheet 
may be touched and the image on the sheet changes according to the person s body temperature. One lengthwise 
series of Peltier elements are controlled by microcomputer 1. A total of 6 PICs receive data from a transmission 
PIC which contains animation programs. From this arrangement, the direction of the current flowing through 
the Peltier elements can be determined. Images which are temperature-responsive can therefore be shown 
on the display. Displayed in a 6 x 6 configuration. Numbers, letters and pictures are programmed as images. 
 Fig.3 Test demonstration  3. Future outlook We are currently developing this product for applications 
to universal product designs such as wall surfaces, public bulletin boards, and table-top sign systems. 
In this production, we developed technology for a tactile display unit which is able to express a novel 
form of tactile communication for everyday life-use. We envision that the present creation will stimulate 
development in many other fields and provide a venue for new inventive creation. References [1] Kushiyama, 
K., Inose, M., Yokomatsu, R., Fujita, K., Kitazawa, T., Tamura, M., and Sasada, S. 2006. Thermoesthesia: 
about collaboration of an artist and a scientist. In ACM SIGGRAPH 2006 Sketches Copyright is held by 
the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599329</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<display_no>28</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Using eye tracking to analyze stereoscopic filmmaking]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599329</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599329</url>
		<abstract>
			<par><![CDATA[<p>This poster will analyze the feasibility of eye tracking as a tool for helping filmmakers to make decisions in a stereoscopic film production. In a conventional dialogue driven shot it is fairly easy to predict where the audiences would be looking. However, for visually complex shots it is not so obvious. In this case, eye tracking can be used as a tool to observe the gaze pattern of the audience to identify the regions of interest in the frame. This information could be used to budget the resources for the shot. It can also be used to identify elements that distract the audience from the flow of the movie. This technique could be used to help filmmakers to make more informed decisions during the film making process. We analyzed a student produced stereoscopic film using this technique. In our study, a number of subjects were asked to watch the film and their gaze data was recorded.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>3D/stereo scene analysis</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621419</person_id>
				<author_profile_id><![CDATA[81442616703]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Celambarasan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramasamy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clemson University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621420</person_id>
				<author_profile_id><![CDATA[81100479080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[House]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clemson University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621421</person_id>
				<author_profile_id><![CDATA[81100175263]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Duchowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clemson University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621422</person_id>
				<author_profile_id><![CDATA[81546308656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Daugherty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clemson University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Using Eye Tracking to Analyze Stereoscopic Filmmaking Celambarasan Ramasamy + Donald H.House +¦ Andrew 
T. Duchowski ¦ Brian Daugherty ¦ Digital Production Arts+ and School of Computing¦, Clemson University 
 1) Introduction: This poster will analyze the feasibility of eye tracking as a tool for helping filmmakers 
to make decisions in a stereoscopic film production. In a conventional dialogue driven shot it is fairly 
easy to predict where the audiences would be looking. However, for visually complex shots it is not so 
obvious. In this case, eye tracking can be used as a tool to observe the gaze pattern of the audience 
to identify the regions of interest in the frame. This information could be used to budget the resources 
for the shot. It can also be used to identify elements that distract the audience from the flow of the 
movie. This technique could be used to help filmmakers to make more informed decisions during the film 
making process. We analyzed a student produced stereoscopic film using this technique. In our study, 
a number of subjects were asked to watch the film and their gaze data was recorded. 2) Eye Tracking 
Study: The film contained experimental first person stereoscopic shots that mimicked the human visual 
field. By working on top of motion capture data, we were able to create stereoscopic shots that mimicked 
the way we see the objects around us through our eyes. By animating a combination of stereo convergence 
point and the depth of field, we were able to create nice eye shift like transitions as the focus changed 
from one object to the next within the shot. In order to gauge the viewers' reaction to this technique, 
the study subjects were shown four short test clips. One clip showed the actual eye shift like transition 
and the other three clips showed variations in depth of field and stereo convergence point of the eye 
shift transitions that were specifically meant to create conflicting visual cues. An analysis of the 
results showed that the gaze data for the clip in which both the depth of field and the stereo convergence 
point were animated together had the least standard deviation, showing that this case had a tendency 
to focus the audience's gaze more when compared to the other cases. (Please look at the accompanying 
video for the visualization of the gaze data from all the four test cases.) In the eye tracking study, 
the audience's gaze data was visualized in the form of a heat map. The accuracy of the eye tracker was 
our major limitation in gathering valid data. The raw gaze data from the eye tracker was quite jittery, 
and occasionally, it would miss the subject s eyes whenever they slouched or leaned back during the test. 
But, we were able to obtain usable data for the most part. So, in order to get a smooth reading, a weighted 
average of the previous and the next five gaze points were used. These smoothed points were then passed 
through a Gaussian filter and motion blurred to construct the heat map. These steps meant that our eye 
tracking data was not suitable for studying the minute movements of the viewer s gaze, but it was able 
to effectively capture the larger eye movements of the viewers within the frame. 3) Other Observations: 
While analyzing the eye tracking data for the stereoscopic short film, we were able to make a number 
of observations that were very interesting from a filmmaker's point of view. In the very last shot of 
the film, a child peers out from under a trap door. The highly focused gaze on the child when he emerges 
from under the trap door shows the audience's curiosity in finding out who is hiding behind the trap 
door. This stands as a testament to the effectiveness of shots in helping to build the audience's involvement 
in the film. The analysis of gaze data helped to reassure us that the shots in the film were presented 
in an effective manner such that the audiences were able to pickup all the crucial elements of the film. 
In comparing the gaze data between the stereoscopic and the non stereoscopic versions of the film, at 
one point when the subjects are looking into a long deep hallway, there is a highly focused gaze intensity 
looking right at the other end of the doorway in the stereoscopic version. But in the non stereoscopic 
version, the gaze data is more spread out across the frame and is not as highly focused as in the case 
of the stereoscopic version. 4) Conclusion: Eye tracking can be a valuable tool in gauging how the viewers 
react to some new experimental framing technique, especially in a medium like stereoscopic 3D films, 
in which the viewers can be quite sensitive to any kind of change. If a consistent viewing pattern was 
identified from the gaze data, it can be used to make effective stereo transitions during cuts. Studying 
the viewer s gaze behavior, particularly for stereoscopic 3D films, can be used to enhance the viewing 
experience of the audience. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599330</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<display_no>29</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[3D-model-based face replacement in video]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599330</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599330</url>
		<abstract>
			<par><![CDATA[<p>The development in digital technologies and the widespread Web 2.0 concept have made many digital videos accessible. Editing and modifying digital videos have become an interesting and important topic. In this paper, we present a system for face replacement in video. Most digital processing software can perform face replacement only when the poses for the source and target faces are similar, and the manipulation process with those software is often time-consuming and labor-intensive. While previous work [Blanz et al. 2004] focuses on image face replacement, our system performs face replacement in video by constructing 3D models for both target and source faces and swapping them accordingly. 3D face models are created by fitting 3D morphable models [Blanz et al. 1999] and the input is reduced to two pictures for the face to be placed in.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Video analysis</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010230</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Video summarization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621423</person_id>
				<author_profile_id><![CDATA[81442597379]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yi-Ting]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621424</person_id>
				<author_profile_id><![CDATA[81442614374]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Virginia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tzeng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621425</person_id>
				<author_profile_id><![CDATA[81421598864]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621426</person_id>
				<author_profile_id><![CDATA[81442596518]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chuan-Chang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Next Media Animation Limited]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621427</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621428</person_id>
				<author_profile_id><![CDATA[81350582710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yung-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621429</person_id>
				<author_profile_id><![CDATA[81100319756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ouhyoung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blanz V, Scherbaum K, Vetter T, and Seidel H. P, "Exchanging Faces in Images," <i>Proc. EUROGRAPHICS</i>, pp. 669--676, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311556</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLANZ V., VETTER T.: A morphable model for the synthesis of 3D faces. In <i>Computer Graphics Proc. SIGGRAPH'99</i> (Los Angeles, 1999), pp. 187--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D-Model-based Face Replacement in Video Yi-Ting Cheng*, Virginia Tzeng*, Yu Liang*, Chuan-Chang Wang 
, Bing-Yu Chen*, Yung-Yu Chuang*, Ming Ouhyoung* *National Taiwan University Next Media Animation Limited 
 1. Introduction  The development in digital technologies and the widespread Web 2.0 concept have 
made many digital videos accessible. Editing and modifying digital videos have become an interesting 
and important topic. In this paper, we present a system for face replacement in video. Most digital processing 
software can perform face replacement only when the poses for the source and target faces are similar, 
and the manipulation process with those software is often time-consuming and labor-intensive. While previous 
work [Blanz et al. 2004] focuses on image face replacement, our system performs face replacement in video 
by constructing 3D models for both target and source faces and swapping them accordingly. 3D face models 
are created by fitting 3D morphable models [Blanz et al. 1999] and the input is reduced to two pictures 
for the face to be placed in. The system is useful in many ways: for example, one may replace a leading 
character s face in the movie with her/his own. In addition, the face model can be altered in user s 
favor and then replace the target face in video. 2. System Overview and Implementation    Figure 
1: System overview for our face replacement in video .  Our system has four main stages (see Figure 
1). First, we create 3D face models for the source and target faces separately. 3D face models are created 
from a couple of images by adopting 3D morphable model fitting method [Blanz et al. 1999]. The source 
face model takes two pictures as input: one is a frontal view, and the other is a profile view. For the 
target face model, we pick an appropriate frame from target video for constructing the target model. 
 Second, we track and align the target face in the video to get feature points of the face, such as eye 
corners, mouth corners, etc. The feature points along with target face model are used to estimate the 
face pose parameters. By using the pose parameters and the normal vector of face model, we estimate nine 
spherical harmonics parameters as lighting parameters for the environment of the target video. Third, 
we use the estimated pose and lighting parameters to rotate and relight the source 3D model accordingly 
and to render the corresponding frames. In the final stage, we use feature points to determine the replacement 
region on the face. Then, we replace the region of the target video with the one from corresponding source 
frame. Although we have adjusted the lighting of source face, it is still possible that the colors of 
source and target faces are slightly different. Therefore, we use Poisson blending to obtain better compositing 
results. 3. Conclusion and future work  We have created a system for face replacement in video. Given 
the target video and two pictures of the face to replace with, we can replace the target face in video 
by a small amount of user inputs: setting the initial guess for face alignment in the first frame (translate 
and scale a face template to roughly align the face) and setting 11 feature points for both source and 
target faces (it is only required for three images: two source image and one frame from the target video). 
In terms of computation, it takes about 2 minutes to construct the 3D model for target and source faces 
and 6 seconds to replace the face for each frame. Results are shown in our demo video. Currently, our 
system only works on neutral face in video due to the limitation on 3D face model database. In the future, 
we plan to capture or create a richer 3D face dataset of facial expressions, so that we can estimate 
expression parameters and replace the face with an expression similar to that of the target face. Reference 
Blanz V, Scherbaum K, Vetter T, and Seidel H.P, Exchanging Faces in Images, Proc. EUROGRAPHICS, pp. 669-676, 
2004. BLANZ V., VETTER T.: A morphable model for the synthesis of 3D faces. In Computer Graphics Proc.SIGGRAPH 
99 (Los Angeles, 1999), pp. 187 194. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599331</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<display_no>30</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[A comparison of three methods of face recognition for home photos]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599331</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599331</url>
		<abstract>
			<par><![CDATA[<p>This poster presents experimental results of three face recognition methods -- Support Vector Machine (SVM), Local Binary Pattern (LBP)-based, and Sparse Represented-based Classification (SRC). We will show the experimental results based on AR face database and on home photos. The experiments show that the three algorithms can achieve over 85% recognition rate in AR database. However, the recognition rate is extremely reduced in home photos. SVM and SRC-based method encounter challenges of selecting training model while LBP-based method encounters the challenge of merging over scattered clusters. Our goal is to improve the accuracy and efficiency especially in home photos based on the three methods.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Object recognition</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.5.2</cat_node>
				<descriptor>Classifier design and evaluation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010257.10010293.10003660</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning approaches->Classification and regression trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010258.10010259.10010263</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Learning paradigms->Supervised learning->Supervised learning by classification</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010251</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Object recognition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621430</person_id>
				<author_profile_id><![CDATA[81442602940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Che-Hua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yeh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621431</person_id>
				<author_profile_id><![CDATA[81442593539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pei-Ruu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shih]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621432</person_id>
				<author_profile_id><![CDATA[81337491320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yin-Tzu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621433</person_id>
				<author_profile_id><![CDATA[81442612622]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kuan-Ting]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621434</person_id>
				<author_profile_id><![CDATA[81442594029]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Huang-Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621435</person_id>
				<author_profile_id><![CDATA[81100319756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ouhyoung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>340806</ref_obj_id>
				<ref_obj_pid>340534</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[P. J. Phillips. Support vector machines applied to face recognition. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, NIPS'98, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1965855</ref_obj_id>
				<ref_obj_pid>1965841</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zhou, Y., Gu, L. and Zhang, H. Bayesian Tangent Shape Model: Estimating Shape and Pose Parameters via Bayesian Inference. CVPR 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1176245</ref_obj_id>
				<ref_obj_pid>1175897</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ahonen, T., Hadid, A. and Pietikainen, M. Face Description with Local Binary Patterns: Application to Face Recognition. PAMI 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1496037</ref_obj_id>
				<ref_obj_pid>1495801</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wright, J., Yang, A. Y., Ganesh, A., Sastry, S. S. and Yi Ma. Robust Face Recognition via Sparse Representation. PAMI 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Comparison of Three Methods of Face Recognition for Home Photos Che-Hua Yeh, Pei-Ruu Shih, Yin-Tzu 
Lin, Kuan-Ting Liu, Huang-Ming Chang, Ming Ouhyoung National Taiwan University  (a)       (b) 
           (c)   Fig. 1. Three algorithms for face recognition. (a) Support Vector Machine 
classification with Bayesian Tangent Shape Model face alignment (b) LBP histogram-based classification 
by Local Binary Pattern operator (c) Linear combination of training images by Sparse Representation-based 
classification This poster presents experimental results of three face recognition methods Support Vector 
Machine (SVM), Local Binary Pattern (LBP)-based, and Sparse Represented-based Classification (SRC). We 
will show the experimental results based on AR face database and on home photos. The experiments show 
that the three algorithms can achieve over 85% recognition rate in AR database. However, the recognition 
rate is extremely reduced in home photos. SVM and SRC-based method encounter challenges of selecting 
training model while LBP-based method encounters the challenge of merging over scattered clusters. Our 
goal is to improve the accuracy and efficiency especially in home photos based on the three methods. 
Researches on face recognition have been presented in many publications. SVM-based face recognition has 
been introduced by Jonathon s work [1]. In our implementation, 87 feature points extracted from each 
image are SVM training data. Afterwards, face images are classified by SVM with trained model. In the 
future, an automatic face alignment step with Bayesian Tangent Shape Model (BTSM) will be adopted [2]. 
LBP-based face recognition was proposed in [3]. The LBP operator assigns a label to every pixel of an 
image by thresholding the 3x3 neighborhood of each pixel with the center pixel value and considering 
the result as a binary number. Then the histogram of the labels is used as a texture descriptor. For 
face images, each image is partitioned into several regions and LBP histograms are combined as the face 
descriptor. Weighted chi square static is used to classify faces into groups. LBP-based face recognition 
method has been proven to be robust to illumination changes. SRC-based face recognition was proposed 
by Wright et al. in 2008 [4]. The basic idea of SRC indicates that the new face image is a linear combination 
of images in the face database and the coefficients are sparse since non-zero coefficients are only contributed 
by face images of the same individual. Based on the sparse representation computed by L1-minimization, 
the face classification algorithm is proposed. The major advantage of SRC-based algorithm is robust to 
occlusions AR face database and home photos are both tested with the three algorithms. In AR database, 
881 frontal face images from 120 subjects are selected. Each subject has 6 or 7 images. For one subject, 
4 out of 7 images are used as training data (480 images in total) while the others are used as testing 
data (401 images in total). The table listed below shows the results. Runtime of image normalization 
and alignment excludes the training time and testing time. Table 1 Results on AR database Algorithm Recognition 
Rate Training Time Testing Time SVM 87.5% 255.2s 1.1s LBP 94.0% 52.3s 58.5s SRC 89.7% - 2147.4s  
Home photos with 334 face images from 5 subjects are selected to test the three algorithms. Experiments 
are separated into two parts, supervised learning and unsupervised learning. In supervised learning, 
SVM-based algorithm is excluded since our face alignment module is currently inefficient in handling 
non-frontal faces. The results are shown in Table 2. The recognition rate of SRC-based algorithm is reduced 
15.1% to 74.6%, while that of LBP-based algorithm is reduced 23.8% to 70.2%. Table 2 Results on home 
photos (supervised learning) Algorithm Recognition Rate Training Time Testing Time LBP 70.2% 11.1s 16.5s 
 SRC 74.6% - 442.2s  Unsupervised learning means that the algorithm classifies faces into clusters by 
its own decision. Only LBP-based algorithm is available for unsupervised learning in our implementation. 
Table 3 shows the results. The error rate grows while reducing the number of clusters. The same experiment 
is also performed on Google Picasa Web Album, which gives 100% accuracy with 117 clusters. Table 3 LBP-based 
algorithm on home photos (unsupervised learning) Clusters 328 314 262 191 Accuracy 100% 99% 95% 83% 
 The improvement in recognizing home photos can focus on illumination normalization, face alignment, 
and runtime. High speed with some user-aided improvement of recognition rate is preferred in our future 
work. Reference [1] P. J. Phillips. Support vector machines applied to face recognition. In M. S. Kearns, 
S. A. Solla, and D. A. Cohn, editors, NIPS 98, 1998. [2] Zhou, Y., Gu, L. and Zhang, H. Bayesian Tangent 
Shape Model: Estimating Shape and Pose Parameters via Bayesian Inference. CVPR 2003. [3] Ahonen, T., 
Hadid, A. and Pietikainen, M. Face Description with Local Binary Patterns: Application to Face Recognition. 
PAMI 2006. [4] Wright, J., Yang, A.Y., Ganesh, A., Sastry, S.S. and Yi Ma. Robust Face Recognition via 
Sparse Representation. PAMI 2008.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599332</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<display_no>31</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Augmenting a camera with a thermometer]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599332</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599332</url>
		<abstract>
			<par><![CDATA[<p>All of today's digital cameras record the date and time at which a photo has been taken; some cameras also record the geographical position. This work proposes yet another augmentation: to record temperatures at different spots picked by the user in the image. This has many applications for both family life and professional engineering: Was the water in the swimming pool heated? Was last Saturday night's party fever really a fever? On the serious side, an engineer may record the temperature of different chips on a printed circuit board or document heat loss due to bad building insulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.7</cat_node>
				<descriptor>Consumer products</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010481.10003558</concept_id>
				<concept_desc>CCS->Applied computing->Operations research->Consumer products</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010481.10003558</concept_id>
				<concept_desc>CCS->Applied computing->Operations research->Consumer products</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621510</person_id>
				<author_profile_id><![CDATA[81408599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loviscach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Bielefeld (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Augmenting a Camera with a Thermometer J¨orn Loviscach* Fachhochschule Bielefeld (University of Applied 
Sciences)   Figure 1: The user shoots one main image which then serves as a backdrop for aiming the 
thermometer (as indicated by the green crosshairs) and for the display of the measurement data. The cur­rent 
camera image is overlaid for feedback (lower left). 1 Introduction All of today s digital cameras record 
the date and time at which a photo has been taken; some cameras also record the geographical position. 
This work proposes yet another augmentation: to record temperatures at different spots picked by the 
user in the image. This has many applications for both family life and professional engi­neering: Was 
the water in the swimming pool heated? Was last Saturday night s party fever really a fever? On the serious 
side, an engineer may record the temperature of different chips on a printed circuit board or document 
heat loss due to bad building insulation. These tasks could be addressed with a thermographic camera 
at a steep price. In contrast to that, non-contact thermometers based on infrared sensors have become 
very affordable. They are based on thermopile sensors that could easily be added to digital cameras. 
One can employ image tracking on the camera to determine which spot is currently captured by the thermometer. 
The user takes one main image and then points the camera s case and hence both the standard camera and 
the infrared thermometer to all spots whose temperature he or she wants to record. The measurements are 
dis­played in the main image, see Figure 1. A different option would be to store the data in speci.c 
tags inside the image, to be read by an image viewer or by data processing software. 2 Prototype The 
prototype, see Figure 2, is based on the combination of a stan­dard webcam with an inexpensive infrared 
thermometer that has a wireless serial connection to send the current measurement data. The software 
has been developed using two libraries: OpenCV for image processing and FFTW for the Fourier transform. 
After the user has pressed a key to select the current image as the main one, the view-.nder is frozen: 
It displays this image and no longer the current camera input. Crosshairs marks the current target *e-mail: 
joern.loviscach@fh-bielefeld.de Figure 2: A webcam and an infrared thermometer are coupled to augment 
each image taken by temperature measurements. point, see Figure 1. The measurements are indicated by 
thermome­ter icons. To not clutter the display, each value is available numeri­cally when the mouse hovers 
over the measurement spot. The ther­mometer s spot size (.eld of view) is indicated by the circles in 
the crosshairs and the thermometer icons. The spot size of the infrared thermometer used in the prototype 
is 30:1 (distance:diameter), a typical value for these devices, which obviously guarantees enough spatial 
resolution for most applications. The separation of the web­cam s and the thermometer s lenses approximately 
3 cm in the prototype causes a parallax error, which is comparable to the spot size at 1 m distance and 
hence is negligible for larger distances. Most infrared thermometers can mark the measurement spot by 
a laser. In the prototype, the laser s dot is used to align the image of the webcam. This has to be done 
once for a given setup of the webcam and the thermometer. In principle, the laser dot could also be used 
to track the measurement spot through a .xed camera that does not move in parallel with the thermometer. 
However, in pre­liminary experiments it turned out that the laser dot often appears much too bright in 
the camera image (and hence white but not red), appears blurred, or is not visible at all, depending 
on the surface. Hence, the camera target is identi.ed by cross-correlating the main image with the current 
camera image at a size of 128 × 128 pixels in RGB space. This cross-correlation is computed in real time 
with help of the FFT of the images padded with zeros to 256 × 256 pixels. To normalize the cross-correlation, 
summed area tables are used to determine the variances of the main image and the current camera image 
in the overlapping region. This approach does not employ frame-to-frame tracking, as such a method would 
be prone to lose the reference when the camera s target changes rapidly. 3 Conclusion and Outlook This 
work demonstrated how a digital camera can be extended by an infrared thermometer to open up novel applications. 
Future work can address the miniaturization: A bare-bones thermopile together with infrared optics could 
be equipped with a BlueTooth transceiver and added to a mobile phone. Other data such as laser-based 
dis­tance measurements could be incorporated in a similar way. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599333</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<display_no>32</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[Automatic colorization of grayscale images using multiple images on the web]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599333</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599333</url>
		<abstract>
			<par><![CDATA[<p>Colorization is the process of adding color to monochrome images and video. It is used to increase the visual appeal of images such as old black and white photos, classic movies, and scientific visualizations. Since colorizing grayscale images involves assigning three-dimensional (RGB) pixel values to an image whose elements are characterized by one feature (luminance) only, the colorization problem does not have a unique solution. Hence, human interaction is typically required in the colorization process. Although existing colorization methods attempt to minimize the amount of user intervention, they require users to manually sellect a similar image to the target image or input a set of color seeds for different regions of the target image. In this paper, we present an entirely automatic colorization method using multiple images collected from the Web. The method generates various and natural colorized images from an input monochrome image by using the information of the scene structure.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Color</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010241</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Image representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621436</person_id>
				<author_profile_id><![CDATA[81442610726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621437</person_id>
				<author_profile_id><![CDATA[81320495523]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621438</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276382</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hays, J., and Efros, A. A. 2007. Scene completion using millions of photographs. <i>ACM Trans. Graph (SIGGRAPH 2007) 26</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566576</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Welsh, T., Ashikhmin, M., and Mueller, K. 2002. Transferring color to greyscale images. <i>ACM Trans. Graph (SIGGRAPH 2002) 21</i>, 3, 277--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automatic colorization of grayscale images using multiple images on the Web Yuji Morimoto* YuichiTaguchi 
The UniversityofTokyo The UniversityofTokyo  Figure 1: Monochrome image Im Figure 2: Automatically 
selected reference images whose composi­ tion are similar to Im 1 Introduction Colorization is the process 
of adding color to monochrome images and video. It is used to increase the visual appeal of images such 
as old black and white photos, classic movies, and scienti.c vi­sualizations. Since colorizing grayscale 
images involves assigning three-dimensional(RGB)pixelvaluestoanimagewhose elements are characterized 
by one feature (luminance) only, the colorization problem does nothaveaunique solution. Hence, human 
interaction is typically required in the colorization process. Although existing colorization methods 
attempt to minimize the amount of user inter­vention, they require users to manually sellect a similar 
image to the target image or input a set of color seeds for different regions of the target image. In 
this paper, we present an entirely automatic colorization method using multiple images collected from 
theWeb. The method generates various and natural colorized images from an input monochrome image by using 
the information of the scene structure.  2 Our Method Our colorization method is entirely automatic 
and outputs multi­ple natural colorized images by using one million images down­loaded from Flickr (http://www..ickr.com/) 
. To colorize an input grayscale image Im (Figure 1) , the method .rst selects reference color images 
(source images) from the image database. It then gen­erates multiple color images by colorizing Im using 
each source image. We select images that have similar scene structure withIm as the source images, because 
images that havesimilar structure are likely tohavea similar color.To .nd the source images from one 
million images, we used gist scene descriptor, which is a feature vector to describe the global scene 
in lower dimension. The gist scene descriptor aggregates oriented edge responses at multiscales into 
very coarse spatial bins.We useda gist scene descriptorbuilt from 6 oriented edge responses at5scales 
aggregated toa4×4spatial resolution and converted one million images to 480 dimensional vectors [Hays 
and Efros 2007].We calculated the SSD between the gist of Im and every gist of the one million images 
and selected the most similar 100 images. From these images, we used the 20 images that have the most 
similar aspect ratio to Im as the source images (Figure 2). *E-mail: morimoto@nae-lab.org Figure 3: 
Result of automatic col­orization of Im (1) Next, we colorize Im using these source images. For this 
pur­pose, we modi.edWelsh et al. s method [2002], which colorizes a grayscale image by transferring color 
from a source color image. In order to transfer chromaticity values from a source image to Im, each pixel 
in Im must be matched to a pixel in the source image. The comparison is based on the luminance value 
and the standard deviation of the luminance in a pixel neighborhood with size of 5×5pixels. Since our 
method selects a source image that is similar to Im not only in color trendbut also in scene structure, 
we can also use the structure information for colorization. For example, since two images have similar 
structure, a color at the top left of the image is likely to appear in the same part of the other image. 
We embedded this information in the pixel matching algorithm by penalizing the matching score when the 
location between the two pixels is separated. Once the best matching pixel is found, chro­maticity values 
are transferred to the target pixel. To transfer only chromaticity values, images are converted to the 
la( color space. The la( color space consists of an achromatic luminance channel (l)and two chromatic 
channelsa and (. This color space mini­mizes the correlation between the three coordinate axes of the 
color space. By transferring a and (, we can transfer only chromaticity values without altering the luminance. 
As a result, our method produces multiple colorized images (Fig­ures3and4), from which users can choose 
the one that theylike. 3 Conclusions and Future Work We presented a novel colorization method using 
one million im­ages collected from theWeb. Our methodis entirely automatic and produces multiple natural 
colorized images by exploiting the simi­larity of the scene structure. Currently, the limitation of our 
method is that it sometimes produces unnatural colorized images, caused by source images that are structurally 
similarbut semantically dif­ferent. To solve this problem, we are considering combining the obtained 
color images to generate a more robust result. References HAYS, J., AND EFROS, A. A. 2007. Scene completion 
using millions of photographs. ACM Trans. Graph (SIGGRAPH 2007) 26, 3. WELSH,T.,ASHIKHMIN,M., AND MUELLER,K. 
2002.Trans­ferring color to greyscale images. ACM Trans. Graph (SIG-GRAPH 2002) 21, 3, 277 280. Figure 
4: Result of automatic col­orization of Im (2) Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599334</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<display_no>33</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Data-driven diffuse-specular separation of spherical gradient illumination]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599334</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599334</url>
		<abstract>
			<par><![CDATA[<p>Separation of the diffuse and specular components of observed reflectance has been an active area of research in computer graphics and vision, with major applications in reflectance modeling and scene analysis. Traditionally, researchers have investigated diffusespecular separation under point or directional illumination conditions while employing polarization and, in the case of dielectric materials, color space analysis techniques. Recently, Ma et al. [2007] introduced a technique for estimating high quality diffuse and specular normals and albedo maps (see Fig. 1, (a) & (d)) of a specular object using polarized spherical gradient illumination. However, the employed polarization technique imposes view-point restriction, and results in insufficient light levels for performance capture with high speed acquisition. Hence, in this work, we look into an alternate diffuse-specular separation technique for spherical gradients based on a data-driven reflectance model. Traditional separation techniques based on color space analysis focus on removing specular reflections from the observation for scene analysis [Mallick et al. 2005]. In contrast, we focus on obtaining high quality estimates of both the diffuse and the specular reflectance components.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621439</person_id>
				<author_profile_id><![CDATA[81442595190]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tongbo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621440</person_id>
				<author_profile_id><![CDATA[81385599077]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Abhijeet]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ghosh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621441</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1069142</ref_obj_id>
				<ref_obj_pid>1068508</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mallick, S. P., Zickler, T. E., Kriegman, D. J., and Belhumeur, P. N. 2005. Beyond lambert: Reconstructing specular surfaces using color. In <i>Proc. IEEE Conf. Computer Vision and Pattern Recognition</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Data-Driven Diffuse-Specular Separation of Spherical Gradient Illumination Tongbo Chen Abhijeet Ghosh 
Paul Debevec USC Institute for Creative Technologies  (a) true albedo (b) initial separation (c) .nal 
albedo (d) true normals (e) separated normals Figure 1: Diffuse-specular separation of a plastic orange 
captured under spherical gradient illumination. Top-row: specular separation. Bottom-row: diffuse separation. 
(a) &#38; (d): Polarization-based separation. (c) &#38; (e): Data-driven separation. 1 Introduction Separation 
of the diffuse and specular components of observed re­.ectance has been an active area of research in 
computer graph­ics and vision, with major applications in re.ectance modeling and scene analysis. Traditionally, 
researchers have investigated diffuse­specular separation under point or directional illumination condi­tions 
while employing polarization and, in the case of dielectric ma­terials, color space analysis techniques. 
Recently, Ma et al. [2007] introduced a technique for estimating high quality diffuse and spec­ular normals 
and albedo maps (see Fig. 1, (a) &#38; (d)) of a specular object using polarized spherical gradient illumination. 
However, the employed polarization technique imposes view-point restric­tion, and results in insuf.cient 
light levels for performance capture with high speed acquisition. Hence, in this work, we look into an 
al­ternate diffuse-specular separation technique for spherical gradients based on a data-driven re.ectance 
model. Traditional separation techniques based on color space analysis focus on removing specu­lar re.ections 
from the observation for scene analysis [Mallick et al. 2005]. In contrast, we focus on obtaining high 
quality estimates of both the diffuse and the specular re.ectance components. 2 Method We propose a 
diffuse-specular separation technique based on a data­driven model of diffuse and specular re.ectance 
of spherical gra­dient illumination. We build this model from example data with known ground truth diffuse-specular 
separation. We use polariza­tion based separation as ground truth data in this work. Note that we do 
not require observation of the exact same object to build our re.ectance model, only an object with similar 
diffuse and specu­lar relectance characteristics. Our separation algorithm proceeds in two stages: First, 
we employ the example data with known ground truth separation to build orientation-based re.ectance pro.les 
for diffuse and specular re.ectance under the uniform spherical illu­mination condition. Thereafter, 
we employ the diffuse and specu­lar re.ectance pro.les to split the uniform illumination observation 
into diffuse and specular albedos (Fig. 1, (b)). Note that these re­.ectance pro.les capture the increased 
specular re.ection at graz­ing angles due to Fresnel re.ectance. We use the unseparated gra­dients to 
compute the surface normals for this initial separation. The above separation (b) serves as an intial 
guess for the following iterative optimization: We relight the separated diffuse and spec­ular albedo 
into the X, Y and Z gradient illumination conditions, sum them up and then compare to the observed unseparated 
gra­dients. The error in the relit conditions are attributed alternatingly to the specular normal estimate 
and to the specular albedo estimate in subsequent iterations. We repeat the above normal and albedo update 
for a few iterations until convergence. 3 Acquisition and Results Our measurement setup consists of 
an LED sphere with approxi­mately 150 individually controllable lights. Each light is covered with a 
linear polarizer in the pattern of [Ma et al. 2007]. Using this setup, we record an object s response 
to the spherical gradient illu­mination patterns in both cross and parallel polarization conditions in 
order to obtain what we consider to be the ground-truth diffuse­specular separation of albedo and surface 
normals. We then employ our example based data-driven separation on the parallel polarized images in 
order to compare the proposed separation technique (see Fig. 1, (c) &#38; (e)) with the polarization-based 
result. References MA, W.-C., HAWKINS, T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 2007. 
Rapid acquisition of specular and diffuse nor­mal maps from polarized spherical gradient illumination. 
In Rendering Techniques, 183 194. MALLICK, S. P., ZICKLER, T. E., KRIEGMAN, D. J., AND BELHUMEUR, P. 
N. 2005. Beyond lambert: Reconstructing specular surfaces using color. In Proc. IEEE Conf. Computer Vision 
and Pattern Recognition. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599335</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<display_no>34</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Deblurring with rank-structured inverse approximations]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599335</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599335</url>
		<abstract>
			<par><![CDATA[<p>In this presentation, the restoration of images blurred by atmospheric turbulence is examined. The proposal uses a new class of approximations to blurring operators representing Gaussian blur. The Toeplitz matrix representing the blur is transformed into a Cauchy-like (CL) matrix using the FFT. In addition to the CL structure, the transformed matrix has a rank structure. In particular, the off-diagonal blocks have low rank. This class of matrices can be approximated quickly, and the structure can be exploited for fast image restoration.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[deblurring]]></kw>
			<kw><![CDATA[rank-structured matrices]]></kw>
			<kw><![CDATA[structured matrices]]></kw>
			<kw><![CDATA[superfast algorithm]]></kw>
			<kw><![CDATA[toeplitz matrices]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>F.2.1</cat_node>
				<descriptor>Computation of transforms (e.g., fast Fourier transform)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003717</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computation of transforms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621442</person_id>
				<author_profile_id><![CDATA[81435610232]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hudachek-Buswell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia State University and Clayton State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621443</person_id>
				<author_profile_id><![CDATA[81442607857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Catherine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Clayton State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621444</person_id>
				<author_profile_id><![CDATA[81406597384]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stewart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1350552</ref_obj_id>
				<ref_obj_pid>1350540</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chandrasekaran, S., Gu, M., Sun, X., Xia, J., and Zhu, J. 2007. A superfast algorithm for toeplitz systems of linear equations. <i>SIAM Journal on Matrix Analysis and Applications 29</i>, 4, 1247--1266.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hansen, P. C., and Jensen, T. K. 2008. Noise propagation in regularizing iterations for image deblurring. <i>Electronic Transactions on Numerical Analysis 31</i>, 204--220.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2319245</ref_obj_id>
				<ref_obj_pid>2318938</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nagy, J. G., Plemmons, R. J., and Torgersen, T. C. 1996. Iterative image restoration using approximate inverse preconditioning. <i>IEEE Transactions on Image Processing 5</i>, 7 (July), 1151--1162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Deblurring with Rank-structured Inverse Approximations Mary Hudachek-Buswell* Catherine Matos Michael 
Stewart Georgia State University Clayton State University Georgia State University Clayton State University 
 Figure 1: (a) The original satellite image of Saturn s rings. (b) The image blurred by a skewnormal 
PSF to model atmospheric turbulence. (c) The deblurred image from proposed rank-structured inverse approximation 
method. Abstract In this presentation, the restoration of images blurred by atmo­spheric turbulence is 
examined. The proposal uses a new class of approximations to blurring operators representing Gaussian 
blur. The Toeplitz matrix representing the blur is transformed into a Cauchy-like (CL) matrix using the 
FFT. In addition to the CL struc­ture, the transformed matrix has a rank structure. In particular, the 
off-diagonal blocks have low rank. This class of matrices can be approximated quickly, and the structure 
can be exploited for fast image restoration. Keywords: Deblurring, Toeplitz matrices, structured matrices, 
rank-structured matrices, superfast algorithm 1 Introduction A satellite image is degraded by atmospheric 
turbulence and ad­ditive noise. Direct inverse methods to restore the image use a point spread function, 
PSF, to model the blur. The matrix equa­tion Ax = b represents the blurring, where A is the blur matrix, 
x is the restored image, and b is the distorted blurred image. Direct inverse methods either explicitly 
invert the matrix A or some ap­proximation to A to obtain x = A-1b. Such methods can be sensi­tive to 
noise because the matrix A is often severely ill-conditioned. Techniques for addressing this dif.culty 
include regularization, and iterative methods such as conjugate gradient (CG) and generalized minimal 
residual (GMRES). Approximations to the matrix A can be exploited in all of these approaches. This work 
approximates A with a rank-structured matrix [Chandrasekaran et al. 2007]. Such approximations can be 
ef.ciently computed, ef.ciently inverted, and can be applied as a preconditioner to enhance the convergence 
of an iterative method [Nagy et al. 1996]. 2 Methods and Implementation The blur used in this work is 
a skewnormal distribution [Hansen and Jensen 2008]. The FFT is used to transform the blur matrix *e-mail: 
MaryHudachek-Buswell@clayton.edu e-mail:cmatos@clayton.edu e-mail:mastewart@gsu.edu into a CL matrix 
with rank structure. In particular, the transformed n × n matrix A when being partitioned as A11 A12 
A = A21 A22 has the property that each of the off-diagonal blocks, A12 and A21, has low rank and can 
be represented by O(n) parameters. This holds for any such partition and, applied recursively, leads 
to a rep­resentation of the entire matrix in terms of O(n) parameters. A fast algorithm is used to extract 
the rank structure in an exploitable form [2007]. The new structure allows for an O(n) solution to the 
approximate system for deblurring. The structured matrix is used in an approximate inverse method and 
as a basis for a preconditioned iterative method. 3 Comments and Results Inverse .ltering or preconditioned 
iteration techniques typically use circulant approximations to the blur matrix in order to exploit the 
FFT in multiplication by the circulant matrix. In this work, a broader class of approximations is considered 
to yield better inverse approximations and faster convergence. Of twenty-two surveyed, 91% chose the 
deblurred image by rank-structured inverse approxi­mation as opposed to restored images without the proposed 
method. Results of using the approximation in an iterative reconstruction are shown above in Figure 1. 
 References CHANDRASEKARAN, S., GU, M., SUN, X., XIA, J., AND ZHU, J. 2007. A superfast algorithm for 
toeplitz systems of linear equations. SIAM Journal on Matrix Analysis and Applications 29, 4, 1247 1266. 
HANSEN, P. C., AND JENSEN, T. K. 2008. Noise propagation in regularizing iterations for image deblurring. 
Electronic Transac­tions on Numerical Analysis 31, 204 220. NAGY, J. G., PLEMMONS, R. J., AND TORGERSEN, 
T. C. 1996. Iterative image restoration using approximate inverse precondi­tioning. IEEE Transactions 
on Image Processing 5, 7 (July), 1151 1162. Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599336</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<display_no>35</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[Image-based dress up system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599336</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599336</url>
		<abstract>
			<par><![CDATA[<p>In this work, we present an image based virtual dress up system according to user input model and garment image. At 'Registration' step, we asked the user manually setting the skeleton structure and matte out the alphamap from the image. Next step, our method automatically deforms the garment image corresponding to model's body. For the boundary fitting, our method uniformly sampled contour points and solves the optimization function. To enhance the more realistic scene, we reconstruct the 2D mesh to the 3D mesh according to a human's standard body shape. For the lighting effect we estimate the light position by using luminance value with the detected face region. Previous 3D scanner based virtual dress up system has expensive cost and under locational limitation issues, but our system integrates various image processing techniques and introduced an easy-to-use system for the general users. We present that our system produces a visually plausible and well-fitted virtual dress up results in a practical and usable way.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shape</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010249</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Shape inference</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621445</person_id>
				<author_profile_id><![CDATA[81442608257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621446</person_id>
				<author_profile_id><![CDATA[81331508005]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jong-Chul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621447</person_id>
				<author_profile_id><![CDATA[81409592301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[In-Kwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BOOKE, J. 1996. Sus: A quick and dirty usability scale. In P. W. Jordan, B. Thomas, B. A. Weerdmeester A. L. McClelland (eds.) Usability Evaluation in Industry, 189--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DIVIVIER, A. 2008. Virtual try-on topics in realistic, indivisualized dressing in virtual reality. In Proceedings of the Virtual and Augmented Reality Status Conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073323</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[IGARASHI, T., MOSCOVICH, T., AND HUGHES, J. F. 2005. As-rigid-as-possible shape manipulation. ACM Trans. Graph. 24, 3, 1134--1141.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599337</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<display_no>36</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Panoramic imaging system for mobile devices]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599337</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599337</url>
		<abstract>
			<par><![CDATA[<p>We present the design and implementation of a mobile imaging system for high resolution panoramic image creation. The system comprises the following components: automatic camera motion tracking and high resolution image capturing, image registration on spherical manifold, image warping, image labeling, and image blending.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Registration</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Portable devices (e.g., laptops, personal digital assistants)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621448</person_id>
				<author_profile_id><![CDATA[81100313933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yingen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xiong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nokia Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621449</person_id>
				<author_profile_id><![CDATA[81442602137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xianglin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nokia Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621450</person_id>
				<author_profile_id><![CDATA[81100595953]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marius]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tico]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nokia Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621451</person_id>
				<author_profile_id><![CDATA[81440606777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chia-Kai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621452</person_id>
				<author_profile_id><![CDATA[81100567347]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pulli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nokia Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adams, A., Gelfand, N., and Pulli, K. 2008. Viewfinder alignment. <i>Computer Graphics Forum 27</i>, 2, 597--606.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015718</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Agarwala, A., Dontcheva, M., Agrawala, M., Drucker, S., Colburn, A., Curless, B., Salesin, D., and Cohen, M. 2004. Interactive digital photomontage. <i>ACM Trans. Graph 23</i>, 294--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882269</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[P&#233;rez, P., Gangnet, M., and Blake, A. 2003. Poisson image editing. <i>ACM Trans. Graph. 22</i>, 3, 313--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258861</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Szeliski, R., and Shum, H.-Y. 1997. Creating full view panoramic image mosaics and environment maps. In <i>Proceedings of SIGGRAPH 1997</i>, 251--258.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Szeliski, R., Uyttendaele, M., and Steedly, D. 2008. Fast poisson blending using multi-splines.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Zitova, B., and Flusser, J. 2003. Image registration methods: a survey. <i>Image and Vision Computing 21</i>, 977--1000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Panoramic Imaging System for Mobile Devices Yingen Xiong1 XianglinWang1 MariusTico1 Chia-Kai Liang1,2 
Kari Pulli1 1Nokia Research Center 2 NationalTaiwan University Figure1:Panoramic image(7664 × 672)created 
in a camera phone based on 18 high-resolution input images. We present the design and implementation 
of a mobile imaging system for high resolution panoramic image creation. The sys­tem comprises the following 
components: automatic camera mo­tion tracking and high resolution image capturing, image registra­tion 
on spherical manifold, imagewarping, image labeling, and im­age blending. Camera motionis estimatedby 
tracking consecutivelow resolution view.nder frames captured at 30 frames per second. The alignment algorithm 
proposed in our previous work [Adams et al. 2008] cre­ates compact summaries of the frames that allow 
rapid tracking of the camera motion. The high resolution images used tobuild the .nal panorama are automatically 
captured when the camera motion with respect to the previous image exceeds a threshold. Image registration 
of captured high resolution frames is essential for ensuring an accurate representation of the scene. 
The exist­ing approaches to image registration could be classi.ed in two categories: feature based, and 
image based methods [Zitova and Flusser 2003]. Our registration method for panorama application uses 
a coarse-to-.ne strategy and a combination between image and feature based registration methods. Image-based 
registration is adopted at the coarse levels of an imagepyramid where the fea­tures are less reliable. 
Finer resolution levels are registered using a feature-based approach. We employed corner features, and 
both image and feature matching operations are carried out using simi­larity measures that are independent 
to illumination changes. Fea­ture based matching is performed in conjunction with RANSACto achieve robustness 
to outliers, such as movingobjects. An unlimited viewing angle is enabled by mapping the .nal panorama 
onto a spherical manifold. In order to reduce the compu­tational complexity, the proposed registration 
method acts directly in the .nal manifold coordinates. Registration parameter estima­tion in spherical 
coordinates has been proposed in [Szeliski and Shum 1997]. In addition, our approach also carries out 
the cor­responding feature selection and matching in spherical coordinates. Noting that spherical manifold 
warping will change the relative co­ordinates between featuresbutithasa smalleffecton local neigh­borhood 
around each feature, we perform feature matching directly in the spherical manifold domain without any 
intermediate image warping operations. The image warping and spherical mapping are then performedinasinglestepandonly 
onceforeachinputimage, based on the estimated registration parameters. Besides computational complexity, 
another advantage of carrying the registration out directly in spherical coordinates, is that the para­metric 
motion model between images is highly simpli.ed, i.e., sim­ilarity transformation is suf.cient. Several 
tests and comparisons have been carried out in order to validate the proposed image reg­istration approach. 
The results reveal that the proposed approach is robust to moving objects in the scene and signi.cant 
illumination differences between images. The mobile implementation of the al­gorithm is also highly ef.cient 
with about 3 seconds per image pair registration when running ona mobilephone. Toavoid ghostingand blurringduetomoving 
objects, parallax,and registration errors, we perform graph cut to .nd the optimal seams in the overlapping 
areas of the source images. Like the approach of Agarwala et al. [2004], our cost function includes two 
terms: the data penalty for each pixel and the interaction penalty for each pair of the neighboring pixels. 
To ef.ciently perform graph cut on the mobile phone, we simplify the cost function. For the data penalty, 
we set it as a very large number if the pixel is in an invalid area; otherwise, we set it as zero. We 
use the color distance to the neighboringpixels as the interaction penalty. By minimizing the cost function 
with alpha expansion, we can .nd the best seams in the overlapping areas. Image blending is the .nal 
and often very important step in cre­ating high quality panoramic images. In this system, we employ several 
approaches for image blending. We use a fast approach called mask-based blending to produce a quick result 
for user pre­view and use a gradient domain blending approach to produce a high-resolution and high-quality 
result. In the mask-based blend­ing, we .rst create a mask for each source image. The mask has highest 
values at the image center and the lowest values on the borders. Then the masks are warped and with their 
corresponding source images usingGPU acceleration. Finally, the masks provide weights for alpha blending. 
Since camera conditions and environment illuminations may be very different when capturing the source 
images for panorama, the seams foundbygraph cut may stillbe visible. Therefore, we apply Poisson blending[P´erezetal.2003]to 
smooththecolordifferences and hide the seams. In the gradientdomain blending, we createa gradient .eldbycopying 
the gradients from the constituent images and setting the gradients across the seams to be the average 
of the overlapped gradients. The best-.t image to the gradient .eld can be recovered by solving the Poisson 
equation. We solve the Poisson equation in the coarse multi-spline domain and generate the .nal imageby 
seam-adaptive interpolation asin[Szeliskietal. 2008], and effectively reduce the memory and runtime by 
orders of mag­nitudes.OnNokiaN95withaARM11332MHz processorand64 MB RAM, our system can generate a 150. 
panorama from5input images in 25 seconds for mask-based blending and 50 seconds for the gradient domain 
blending, respectively. Figure1is anexample of the panoramic images. References ADAMS,A.,GELFAND,N., 
AND PULLI,K. 2008.View.nder alignment. Computer Graphics Forum 27, 2, 597 606. AGARWALA, A., DONTCHEVA, 
M., AGRAWALA, M., DRUCKER, S., COLBURN, A., CURLESS, B., SALESIN, D., AND COHEN, M. 2004. Interactive 
digital photomontage. ACM Trans. Graph 23, 294 302. ´ Trans. Graph. 22, 3, 313 318. PEREZ, P., GANGNET, 
M., AND BLAKE, A. 2003. Poisson image editing. ACM SZELISKI,R., AND SHUM,H.-Y. 1997. Creatingfullview 
panoramicimage mosaics and environment maps. In Proceedings of SIGGRAPH 1997, 251 258. SZELISKI,R.,UYTTENDAELE,M., 
AND STEEDLY,D. 2008.Fast poisson blending using multi-splines. ZITOVA, B., AND FLUSSER, J. 2003. Image 
registration methods: a survey. Image and Vision Computing 21, 977 1000. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599338</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<display_no>37</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[Proportional constraint for seam carving]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599338</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599338</url>
		<abstract>
			<par><![CDATA[<p>Seam carving is an image processing operator for content-aware image resizing [Avidan and Shamir 2007]. It generates an energy map from gradient intensity of pixels and searches for <i>seams</i>, which are vertical or horizontal continuous paths of pixels that run through local minimum energy areas. Removing or inserting pixels along a seam enables users to shrink or enlarge pictures by a wide range, while still retaining all details of the image.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[feedback]]></kw>
			<kw><![CDATA[image resizing]]></kw>
			<kw><![CDATA[seam-carving]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010241</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Image representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621453</person_id>
				<author_profile_id><![CDATA[81319502900]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Utsugi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621454</person_id>
				<author_profile_id><![CDATA[81416595323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takuma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shibahara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621455</person_id>
				<author_profile_id><![CDATA[81331496069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koike]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621456</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276390</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Avidan, S., and Shamir, A. 2007. Seam carving for contentaware image resizing. In <i>SIGGRAPH '07: ACM SIGGRAPH 2007 papers</i>, ACM Press, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360615</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rubinstein, M., Shamir, A., and Avidan, S. 2008. Improved seam carving for video retargeting. In <i>SIGGRAPH '08: ACM SIGGRAPH 2008 papers</i>, ACM, New York, NY, USA, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Proportional Constraint for Seam Carving Kei Utsugi* Takuma Shibahara Takafumi Koike Hitachi Ltd. Hitachi 
Ltd. Hitachi Ltd. University of Tokyo (1) (2) (3) (4) Figure 1: Distributions of seams and size reduction 
by seam carving. (1) Seams and (2) resized image by original seam carving. (3) Proportional-line (red) 
and seams by proposed method. (4) Image improved by proposed method. Keywords: image resizing, seam-carving, 
feedback 1 Introduction and Motivation Seam carving is an image processing operator for content-aware 
image resizing [Avidan and Shamir 2007]. It generates an energy map from gradient intensity of pixels 
and searches for seams, which are vertical or horizontal continuous paths of pixels that run through 
local minimum energy areas. Removing or inserting pixels along a seam enables users to shrink or enlarge 
pictures by a wide range, while still retaining all details of the image. However, in original seam carving, 
clumps of the seams often over­concentrate in a particular low energy area of the image, causing an irregular 
distortion in the surrounding image. Adverse effects of the distortion are conspicuous if these areas 
contain straight lines of arti.cial objects, making the resulting image look unnatural. Al­though forward 
energy proposed by [Rubinstein et al. 2008] has improved this weak point, especially for vertical lines, 
distortion of straight diagonal lines remains an unsolved problem.  2 Proportional-Line We have developed 
proportional-lines, which are optional line ob­jects arranged on straight lines in an image to reduce 
distortion. They monitor distribution of seams by counting the number of seams passing through them and 
distribution of intersection points, and they dynamically arrange additional energy for new seams to 
avoid seams over-concentrating in certain areas. 2.1 Distortion Evaluation We assume that distortion 
energy Edist, which represents the dis­tortion of line Ln, is composed of Erepel and Elength. Erepel 
is an energy that forces itself between two seams on line Ln and repels them away from each other. When 
a registered seam Sk crosses over a proportional line Ln at point pk, we give penalty en­ergy to circumjacent 
pixels x on Ln within Arepel radius to prevent seams congesting in that area. Erepel(n, k, x)= max(0, 
(Arepel -|x - pk|))2 (1) Elen(x) is an energy that makes new seams pass through vacant areas. Points 
x on a segmented line that are de.ned by a pair of *e-mail: kei.utsugi.nz@hitachi.com, utsugi@nae-lab.org 
e-mail: takuma.shibahara.nj@hitachi.com e-mail: takafumi.koike.jf@hitachi.com §e-mail: naemura@nae-lab.org 
 crossing points pk <x<pk+1 obtain bonus energy commensurate with the length of the segmented line. 
Elength(n, k, x)= -Alen|pk - pk+1| (2) in which Alen is an intensity value proportional to the standard 
de­viation of distortion V [Edist]1/2 in proportional line Ln. Edist(n, x)= LErepel(n, k, x)+ LElength(n, 
k, x) (3) kk 2.2 Offset Parameter from Rate of Passage Therefore, a proportional line requires a new 
seam to correct its distortion. Positive feedback for particular proportional lines may monopolize seams. 
To achieve a balance between energies from proportional lines and energy maps of the original energy, 
dynamic parameter Apass obtains feedback in accordance with the rate of seams across the line Ln and 
the length of line Ln. Total energy is given as Etotal(x)= L(Edist(n, x) - E[Edist] - Apass(n)) (4) n 
 3 Result and Discussion The straight line between the wall and .oor in Figs. 1-4 shows an example of 
how the proposed method improves seam carving. This method contains several independent factors for optimizing 
images. We are working on improving its applicability and automizing fea­ture detection for generalized 
images. (5) (6) Figure 2: (5) Original Result. (6) Improved image (Enlarged). References AVIDAN, S., 
AND SHAMIR, A. 2007. Seam carving for content­aware image resizing. In SIGGRAPH 07: ACM SIGGRAPH 2007 
papers, ACM Press, New York, NY, USA. RUBINSTEIN, M., SHAMIR, A., AND AVIDAN, S. 2008. Improved seam 
carving for video retargeting. In SIGGRAPH 08: ACM SIGGRAPH 2008 papers, ACM, New York, NY, USA, 1 9. 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599339</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<display_no>38</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Ray tracing to get 3D fixations on VOIs from portable eye tracker videos]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599339</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599339</url>
		<abstract>
			<par><![CDATA[<p>Our portable video-based monocular eye tracker contains a headgear with two cameras that capture videos of the observer's right eye and the scene from the observer's perspective (Figure 1a). With this eye tracker, we typically obtain a position -- that represents the observer's <i>point of regard (POR)</i> -- in each frame of the scene video (Figure 1b without bottom left box). These POR positions are in the image coordinate system of the scene camera, which moves with the observer's head. Therefore, these POR positions do not tell us where the person is looking in an exocentric reference frame. Currently, the videos are analyzed manually by examining each frame. In short, we aim to automatically determine how long the observer spends fixating specific objects in the scene and in what order these objects are fixated.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621457</person_id>
				<author_profile_id><![CDATA[81350585603]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Susan]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Munn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology, Rochester, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621458</person_id>
				<author_profile_id><![CDATA[81100138613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeff]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Pelz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology, Rochester, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bouguet, J., 2007. Camera Calibration Toolbox for Matlab&#174;.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>373536</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hartley, R., and Zisserman, A. 2004. <i>Multiple View Geometry</i>, 2nd ed. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1577759</ref_obj_id>
				<ref_obj_pid>1577755</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Munn, S. M., and Pelz, J. B. 2009. FixTag: An algorithm for identifying and tagging fixations to simplify the analysis of data collected by portable eye trackers. <i>Transactions on Applied Perception, Special Issue on APGV</i>, In press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Munn, S. M., and Pelz, J. B. 2009. Simple routines to improve feature tracks. In <i>International Conference on Artificial Intelligence and Pattern Recognition (AIPR-09)</i>, In press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ray tracing to get 3D .xations on VOIs from portable eye tracker videos Susan M. Munn* Jeff B. Pelz 
Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology,Rochester, NY 14623 
USA 1 Introduction Our portable video-based monocular eye tracker contains a head­gear with two cameras 
that capture videos of the observer s righteye and the scene from the observer s perspective (Figure 
1a). Withthis eye tracker, we typically obtain a position that represents the observer s point of regard 
(POR) in each frame of the scene video (Figure1b without bottom leftbox). These PORpositions are intheimage 
coordinate system of the scene camera, which moves withthe observer s head. Therefore, these POR positions 
do not tell uswhere the person is looking in an exocentric reference frame. Cur­rently, the videos are 
analyzed manually by examining each frame.In short, we aim to automatically determine how long the observerspends 
.xating speci.c objects in the scene and in what order these objects are .xated. 2 Algorithm For details 
on how we process the eye video to obtain POR po­sitions in the scene video, see [Munn and Pelz 2009a]. 
Here, weassume these POR positions are known. We track the motion ofour scene camera by tracking corners 
through all frames and us­ing these feature tracks to iteratively compute structure and motion.Our algorithm 
requires two inputs (in addition to the eye and scenevideos): (1) intrinsic parameters of the scene camera; 
(2) positionsand dimensions of volumes-of-interest (VOIs) in the scene. Step 1 -Calculate camera motion. 
For this project, the cameraintrinsic parameters were obtained using the Matlab Camera Cali­bration Toolbox 
[Bouguet 2007]. Functions from this toolbox werealso used within our algorithm to convert between image 
and cam­era coordinates and to compute the camera extrinsic parameters.We used our process described 
in [Munn and Pelz 2009b] to trackfeatures through all frames of the scene video and select keyframesto 
be used for structure and motion recovery. Manual selection ofground truth VOI vertices in one keyframe 
was used to initial­ize the camera position and orientation. This ground truth is thenmatched to the 
next keyframe, whose camera position and orienta­tion is also computed. These two keyframes are then 
used to re­construct features in common between the keyframes. These 3Dfeature positions are then used 
to compute additional camera pro­jection matrices, which are in turn used to compute additional fea­tures 
and the process is repeated until a camera projection matrixis computed for every keyframe. Camera projection 
matrices forintraframes are then computed using the reconstructed features andtheir tracks through the 
intraframes. Step 2 -Ray trace .xations. We use our method discussed in [Munn and Pelz 2009a] to identify 
the start and end of all .xations. For each .xation, the middle frame of the .xation is taken as its 
representative frame. The rotation and translation of the camera co­ordinate system for this frame is 
extracted from its camera projec­tion matrix. VOI vertices prepared using input VOI information are 
converted from world to camera coordinates using these camerarotation and translation values. The 2D 
POR in the scene video is converted to camera coordinates using the normalize function from the Camera 
Calibration Toolbox [Bouguet 2007]. The vectorfrom thecamera center tothis point isnormalized tounit 
magnitude *e-mail: smk8165@cis.rit.edu  Figure 1: (a) Eye tracker headgear by Positive Science R @. 
(b) Ex­ample frame of new output video showing automatically tagged .x­ated VOI in bottom left corner. 
(c) PORs manually selected in ran­dom frames of the scene video and (d) the corresponding computed3D 
PORs. to determine the ray direction. This ray is then traced through everyVOI surface. The distance 
to all surfaces intersected is computedand the VOI intersected .rst (with smallest distance to intersectionpoint) 
is determined to be the .xated VOI. The point of intersectionwiththis VOIisthen converted back to world 
coordinates, using thesame camera rotation and translation values, to obtain the 3D POR. 3 Results The 
algorithm was tested on eight scenes using two observers. Au­tomatically determined .xation tags were 
manually coded for accu­racy. The average accuracy of all eight videos (containing 872 .x­ations) was 
95.9%. Figure 1(c,d) shows a test where .xation pointswere manually selected in random frames of a scene 
video and raytracedto determine 3DPOR.Figure 1cshows animage of thescenewith crosshairs overlaid at the 
manually selected .xation positions.Figure 1d shows the VOIs and computed 3D PORs imported intoMaya (via 
Python). The average error of these 3D PORs in terms of Euclidean distance to the measured locations 
 was 0.54 inches (standard deviation = 0.33 in), about 5% the diagonal extent of theaverage VOI face 
whose VOIs spanned about 61,000 in3. References BOUGUET, J., 2007. Camera Calibration Toolbox for Matlab 
R @. HARTLEY, R., AND ZISSERMAN, A. 2004. Multiple View Geom­etry, 2nd ed. Cambridge University Press. 
MUNN, S. M., AND PELZ, J. B. 2009. FixTag: An algorithm for identifying and tagging .xations to simplify 
the analysis ofdata collected by portable eye trackers. Transactions on Applied Perception, Special Issue 
on APGV, In press. MUNN, S. M., AND PELZ, J. B. 2009. Simple routines to im­prove feature tracks. In 
International Conference on Arti.cial Intelligence and Pattern Recognition (AIPR-09), In press. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599340</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<display_no>39</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[Uniform looking vector plot with streamline fragmentation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599340</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599340</url>
		<abstract>
			<par><![CDATA[<p>Vector plot is a frequently used method for illustrating vector fields used in applications such as scientific visualization. Although the method is easy to implement and the resulting image captures the original vector field well, the streamlines are often positioned too closely or too sparsely to one another due to sources and sinks of the original vector field. This results in unevenness of visual density over the entire region, and some previous researches have treated the problem. Mebarki et al [1] proposed the improved strategy that the maximum vacant region should be given priority for a new streamline, but the results still lack uniformity. Other related works [2][3] suggested that both tapering streamlines and controlling intensity improve the visual uniformity of streamlines. We propose another approach for making streamlines look uniform with dotted and broken lines instead of tapering or intensity control. The results are binary images and consist of fixed width streamlines which preserve uniformity.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621459</person_id>
				<author_profile_id><![CDATA[81100533332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dai Nippon Printing Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mebarki A., Alliez, P. and Devillers, O., Farthest Point Seeding for Efficient Placement of Streamlines, <i>Proceedings of IEEE Visualization 2005</i>, pp. 479--486, 2005]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97901</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Saito, T. and Takahashi, T., Comprehensive Rendering of 3-D Shaped, <i>Proceedings of Siggraph 1990</i>, pp. 197--206, 1990]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237285</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Turk, G. and Banks, D., Image-Guided Streamline Placement, <i>Proceedings of Siggraph 1996</i>, pp. 453--460, 1996]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Uniform Looking Vector Plot with Streamline Fragmentation Naoki Kawai Dai Nippon Printing Co., Ltd. 
 Introduction Vector plot is a frequently used method for illustrating vector fields used in applications 
such as scientific visualization. Although the method is easy to implement and the resulting image captures 
the original vector field well, the streamlines are often positioned too closely or too sparsely to one 
another due to sources and sinks of the original vector field. This results in unevenness of visual density 
over the entire region, and some previous researches have treated the problem. Mebarki et al [1] proposed 
the improved strategy that the maximum vacant region should be given priority for a new streamline, but 
the results still lack uniformity. Other related works [2][3] suggested that both tapering streamlines 
and controlling intensity improve the visual uniformity of streamlines. We propose another approach for 
making streamlines look uniform with dotted and broken lines instead of tapering or intensity control. 
The results are binary images and consist of fixed width streamlines which preserve uniformity. Figure 
1: Result of (a) Mebarki s placement and our method with dotted lines and (c) broken lines. Streamline 
Fragmentation Our method refines any set of streamlines generated by arbitrary placement method, based 
on two fundamental strategies. The first strategy is to obtain the local density of streamlines by referring 
a low­pass filtered image of the original streamlines, and the second one is to fragment each streamline 
based on the obtained density at the position. Figure 2 illustrates the overview of proposed method. 
The method requires streamlines represented in vector form such as a series of control points. First 
the streamlines are rasterized as solid lines with one pixel in width, and then low­pass filtering blurs 
the image. The blurred image indicates the density map of streamlines as Turk and Banks pointed out [3]. 
Our second strategy is to remove part of the streamlines following the policy the denser the more . In 
other word, the density map indicates the possibility to erase line segment at each pixel for uniform 
distribution. Lastly the method rasterizes streamlines again as fragmented lines with our second strategy. 
The method has alternative procedures for fragmenting streamlines similar to random dithering and error­dispersion 
in halftoning. The simpler procedure is comparing the possibility and a generated random number pixel 
by pixel over which the streamlines pass. The pixel should be black only if the random number is greater 
than possibility at the pixel. This process generates many isolated dots that look like unexpected noise. 
The improved procedure avoids isolated dots by preserving minimum length of fragments. Once the procedure 
starts plotting dots, it continues to plot dots until the fragment reaches a pre­defined length. The 
error between defined possibility and actual plot is summed during plotting, and it should be diffused 
along the locus of the streamline. After the segment reaches the pre­defined length, the method stops 
to plot dots until the summed error has been canceled. The method repeats plotting with summing and canceling 
over the locus of the streamline. As a result, the improved procedure generates broken lines while the 
simpler procedure generates dotted lines. Figure 2: Process overview. 3 Results Figure 1 shows images 
by conventional vector plot and proposed method of a vector field that flows into the center of the region, 
and Figure 3 shows the case of a general flow. Our method decreases the unevenness in streamlines density 
in both cases. Though the dotted lines show unexpected isolated dots, broken lines capture flow better. 
The variance of blurred images evaluates uniformity quantitatively, and table 1 proves the effect of 
our method. Figure 3: Result of (a) conventional vector plot, and our method with (b) dotted lines and 
(c) broken lines. Table 1: Variance of 9x9 Gaussian filtered images. References [1]Mebarki A., Alliez, 
P. and Devillers, O., Farthest Point Seeding for Efficient Placement of Streamlines, Proceedings of IEEE 
Visualization 2005, pp.479­486, 2005 [2]Saito, T. and Takahashi, T., Comprehensive Rendering of 3­D Shaped, 
Proceedings of Siggraph 1990, pp.197­206, 1990 [3]Turk, G. and Banks, D., Image­Guided Streamline Placement, 
Proceedings of Siggraph 1996, pp.453­460, 1996 Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599341</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<display_no>40</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Video segmentation with motion smoothness]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599341</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599341</url>
		<abstract>
			<par><![CDATA[<p>In this extended abstract, we propose a novel approach for video segmentation by utilizing motion information. Recently, graph-cutbased segmentation methods became popular in this domain but most of them dealt with color information only. Those methods possibly fail if there are regions similar in color between foreground and background. Unfortunately, it is usually hard to avoid, especially when objects are filmed under a natural environment. For instance, Figure 1(a) shows a result of graph cut with a small smoothness weighting, and hence some background regions are incorrectly labeled. On the contrary, if a larger smoothness weighting is used, some background regions near the foreground will be merged as shown in Figure 1(b). To improve those drawbacks, we propose a method based on both of color and motion information to conduct the segmentation. The method is useful because foreground and background usually have different motion patterns as shown in Figure 1(c).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Video analysis</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Graph algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10010917</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Graph algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010230</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Video summarization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621460</person_id>
				<author_profile_id><![CDATA[81319503643]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chung-Lin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621461</person_id>
				<author_profile_id><![CDATA[81442593951]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yu-Ting]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621462</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621463</person_id>
				<author_profile_id><![CDATA[81100565790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brox, T., Bruhn, A., Papenberg, N., and Weickert, J. 2004. High accuracy optical flow estimation based on a theory for warping. In <i>Proceedings of 2004 European Conference on Computer Vision</i>, 25--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073234</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Li, Y., Sun, J., and Shum, H.-Y. 2005. Video object cut and paste. <i>ACM Transactions on Graphics 24</i>, 3, 595--600. (SIGGRAPH 2005 Conference Proceedings).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Video Segmentation with Motion Smoothness Chung-LinWen * Yu-TingWong * Bing-YuChen YoichiSato * NationalTaiwanUniversity 
TheUniversity ofTokyo (a) (b) (c) Figure 1: In traditional 3D Graph Cuts, due to the color similarity, 
there are cases that no matter what smoothness weighting is, artifacts wouldbeproduced asshownin(a) and(b).Hence,wecombinecolorand 
motioninformation toimprovetheresultgreatlyasshownin(c). 1 Introduction In this extended abstract, we 
propose a novel approach for video segmentationby utilizing motion information.Recently,graph-cut­based 
segmentation methods became popular in this domain but most of them dealt with color information only. 
Those methods possiblyfail if thereareregionssimilarincolorbetweenforeground and background. Unfortunately, 
it is usually hard to avoid, espe­cially when objects are .lmed under a natural environment. For in­stance,Figure1(a) 
shows a result ofgraph cut with a small smooth­nessweighting,andhencesomebackground regionsare incorrectly 
labeled. On the contrary, if a larger smoothness weighting is used, some background regions near the 
foreground will be merged as shown in Figure 1(b). To improve those drawbacks, we propose a method based 
on both of color and motion information to con­duct the segmentation. The method is useful because foreground 
andbackground usuallyhavedifferent motionpatternsasshown in Figure1(c). 2 Video Segmentation In traditional 
videosegmentation,such as[Li etal.2005], theseg­mentation problem is usually formalized into an energy 
minimiza­tionwith thefollowing objectivefunction: E = Ed + aEs + ßEt, where Ed is color similarity, Es 
and Et arepenalty termsforcolor differences among neighboring pixels that have different labels, in the 
same frame and neighboring frames, respectively. In our sys­tem, we extend the energy function by further 
incorporating the motion information Em: E = Ed + aEs + ßEt + .Em. where the motion difference Em is 
calculated according to the inner product of normalized motion vector vp and vq of theneighboringpixels 
p and q: Em = |fp -fq |· g(vp · vq ), (p,q).N where fp and fq aregiven labels, and g(X)= X1+1 is implemented 
inverse relation. All themotionvectorsarepre-calculatedbyarevised versionofthe optical .ow algorithm 
proposed by Brox et al. [2004], whichcan produce much denser optical .ow information. However, since 
the * e-mail: {jonathan, callia}@cmlab.csie.ntu.edu.tw e-mail: robin@ntu.edu.tw e-mail: ysato@iis.u-tokyo.ac.jp 
 (a) (b) Figure 2: (a)Beforere.nement, therearesomenoisesinthebound­ary region.(b)After re.nement, some 
noises are removed. raw optical .ow has some noises, we further conduct color-guided weighting average: 
vq · w(p,q) (p,q).N vp = ., w(p,q) (p,q).N where the weighing w(p,q) is inan inverseproportion togeometric 
distance and color difference. After the re.nement, the quality of theoptical .ow information is improved 
asshown inFigure2. 3 Conclusion and Future Work A new video segmentation method is proposed in this 
paper. We do notonly utilize the traditional spatial color smoothness and tem­poral coherence, but also 
encode the motion smoothness into a 3D temporal-spatial graph. In many cases, our method outperformed 
traditional ones taking only color information into account. In the future work, we will try to automatically 
adjust the weighings by the statistics of color and motion of the input video. Secondly, we will improve 
theuser interface to let userprovideadditional strokes and utilizetheoptical.owinformation topropagatethestrokes. 
 References BROX, T., BRUHN, A., PAPENBERG, N., AND WEICKERT, J. 2004. High accuracy optical .ow estimation 
based on a theory for warping. In Proceedings of 2004 European Conference on ComputerVision,25 36. LI,Y.,SUN,J., 
AND SHUM,H.-Y. 2005. Videoobjectcutand paste. ACM Transactions on Graphics 24, 3, 595 600. (SIG-GRAPH2005ConferenceProceedings). 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599342</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<display_no>41</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[A random cursor matrix to hide graphical password input]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599342</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599342</url>
		<abstract>
			<par><![CDATA[<p>Graphical passwords [Suo et al. 2005] address vital problems of textual passwords: Users pick from a limited vocabulary; machine-generated passwords are hard to memorize. Graphical input, however, faces "shoulder surfing," as bystanders can watch the screen. Current solutions to this problem tend to impose high cognitive loads. We propose an easy-to-handle approach.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.6.5</cat_node>
				<descriptor>Authentication</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002978.10002991.10002992</concept_id>
				<concept_desc>CCS->Security and privacy->Security services->Authentication</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Security</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621464</person_id>
				<author_profile_id><![CDATA[81442614594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alice]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boit]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hochschule Bremen (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621465</person_id>
				<author_profile_id><![CDATA[81442619930]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Geimer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hochschule Bremen (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621466</person_id>
				<author_profile_id><![CDATA[81408599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loviscach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Bielefeld (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1106849</ref_obj_id>
				<ref_obj_pid>1106778</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Suo, X., Zhu, Y., and Owen, G. S. 2005. Graphical passwords: A survey. In <i>Proc. ACSAC '05</i>, 463--472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Random Cursor Matrix to Hide Graphical Password Input Alice Boit Thomas Geimer Hochschule Bremen 
(University of Applied Sciences) Figure 1: The position of the matrix of cursor icons is coupled to 
the computer mouse. Icons moving past a boundary reappear on the opposite side, as indicated by the shapes 
outside of the frame. 1 Introduction Graphical passwords [Suo et al. 2005] address vital problems of 
textual passwords: Users pick from a limited vocabulary; machine­generated passwords are hard to memorize. 
Graphical input, how­ever, faces shoulder sur.ng, as bystanders can watch the screen. Current solutions 
to this problem tend to impose high cognitive loads. We propose an easy-to-handle approach. First, the 
user selects a home base point on an image; this point serves as a login name. Then he or she clicks 
on a secret sequence of further secret points. These clicks are hidden from shoulder surfers because 
there is not only a single cursor on the screen, but a matrix of many different decoy cursors that move 
in parallel. To prohibit edge effects, a cursor that moves past the right border reappears one the left 
side, etc. To offer many memorable click spots, our prototype employs a detailed picture of a coral reef, 
see Figure 1. The cursor matrix is formed by 10 × 10 unique icons that depict various marine animals. 
These icons and the background image are customizable to cater for user-speci.c mnemonic strategies. 
Each pair of cursor icon and secret point represents one step of the login sequence. For increased security, 
the user has to accomplish three rounds of selecting secret points. A shoulder surfer gains al­most no 
information because the positions of the decoy cursors are shuf.ed randomly for every login. To break 
this scheme, an at­tacker has to record the position of each and every cursor for at least two login 
attempts of the same user: The cursor that is valid for a given step of the login sequence reveals itself 
as the only one that is positioned over the same spot in both login attempts.  2 User Tests We conducted 
an Web-based test of the system. 54 participants took part actively; 31 completed the .nal survey. We 
recorded 936 login attempts, of which 76 percent were successful. The median of the time required was 
29 seconds. Note that this number results from *e-mail: joern.loviscach@fh-bielefeld.de J¨orn Loviscach* 
Fachhochschule Bielefeld (University of Applied Sciences) the time needed to .nd the speci.c icon in 
the randomized cursor matrix plus the time for dragging the icons to their destination lo­cation on the 
background image. Over the seven or more days that the test lasted per user, the median login time decreased 
by about a quarter, indicating a learning effect. 15 percent of the successful logins took less than 
15 seconds, which demonstrates the feasibility of quick logins with our method. 58 % percent of the participants 
agreed that the method is a viable idea to replace normal logins. There was a split opinion on whether 
the graphical password is easier to remember than a textual one. The time needed to log in was an issue 
for most participants (52 %), but a majority (65 %) af.rmed to have had fun using the system. 29 subjects 
who took part in a special treasure hunt (log in at least .ve times on the .rst day, .ve times on the 
second day, and .ve times after one week or later) were examined concerning the mem­orability of passwords. 
Only 12 were able to remember their .rst chosen password throughout the entire course of the experiment. 
Most forgot their .rst password on the same day they had chosen it, but nobody ever forgot his or her 
second password. This in­dicates that it is possible to remember a graphical password for a longer time. 
However, it takes some time to get used to the proce­dure. This is a general issue of graphical passwords 
and does not so much concern the speci.c should sur.ng problem we address here. A basic parameter that 
in.uences the success rate is the tolerance with which the system accepts mouse clicks. Based on preliminary 
experiments, we chose a tolerance radius of 20 pixels on the image of 800 × 500 pixels. In the .nal test, 
84 % of all clicks fell into that range. The radius allows a best-case estimate on the security: The 
probability to click three times correctly just by chance amounts to three in one billion. This na¨ive 
argument, however, only applies to blind clicking: The spots chosen by the users cluster around hot spots, 
reducing the uncertainty from 8.3 to 6.0 bits per click.  3 Conclusion and Outlook We have presented 
a graphical authentication method that requires only little mental effort from the user but is theoretically 
safe against one-time shoulder sur.ng, even when done with full record­ing. Since the hundred or more 
cursors overtax the memory of a human observer, shoulder sur.ng with the naked eye is hardly pos­sible 
even if several logins of a single user can be overseen. To better convey the idea of unlimited cursor 
motion, we want to conduct future tests with a trackball instead of a mouse. On top of that, the conspicuousness 
of the cursor icons can be improved so that are more memorable and can be discovered faster in the random 
matrix. Nor surprisingly, users in our tests preferred icons in strong primary colors and remembered 
them better. Future versions of the system should suppress the appearance of hot spots in the password 
choices. One part of the solution to this problem may be to capi­talize on the fact that our system does 
not rely on selecting spots in an image alone, but builds on the associative value between an icon and 
a point in an image. References SUO, X., ZHU, Y., AND OWEN, G. S. 2005. Graphical passwords: A survey. 
In Proc. ACSAC 05, 463 472. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599343</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<display_no>42</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[A selective rendering algorithm based on memory schemas]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599343</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599343</url>
		<abstract>
			<par><![CDATA[<p>In order to economize on rendering computation, selective rendering guides high level of detail to specific regions of a synthetic scene and lower quality to the remaining scene, without compromising the level of information transmitted. Scene regions that have been rendered in low and high quality can be combined to form one complete scene. We propose a novel selective rendering approach which is task and gaze-independent, simulating cognitive creation of spatial hypotheses. Scene objects are rendered in varying quality (polygon count) according to how they are associated with the context (schema) of the scene.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621467</person_id>
				<author_profile_id><![CDATA[81442615876]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexandros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zotos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Crete, Greece]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621468</person_id>
				<author_profile_id><![CDATA[81100567689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Katerina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mania]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Crete, Greece]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621469</person_id>
				<author_profile_id><![CDATA[81408599745]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mourkoussis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brewer, W. F. and Treyens, J. C. (1981). Role of Schemata in Memory for Places, <i>Cognitive Psychology</i> 13, 207--2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1160392</ref_obj_id>
				<ref_obj_pid>1160382</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mania, K.&amp;Robinson, A., Brandt, K. (2005). The Effect of Memory Schemas on Object Recognition in Virtual Environments. <i>Presence Teleoperators&amp;Virtual Environments</i>, 14(5), 606--615, MIT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Selective Rendering Algorithm based on Memory Schemas  Alexandros Zotos Technical University of Crete, 
Greece azotos@ced.tuc.gr Katerina Mania Technical University of Crete, Greece k.mania@ced.tuc.gr Nick 
Mourkoussis University of Sussex, UK n.mourkoussis@sussex.ac.uk  Abstract In order to economize on 
rendering computation, selective rendering guides high level of detail to specific regions of a synthetic 
scene and lower quality to the remaining scene, without compromising the level of information transmitted. 
Scene regions that have been rendered in low and high quality can be combined to form one complete scene. 
We propose a novel selective rendering approach which is task and gaze-independent, simulating cognitive 
creation of spatial hypotheses. Scene objects are rendered in varying quality (polygon count) according 
to how they are associated with the context (schema) of the scene. Methodology Previous selective graphics 
research rendered in high quality the fovea regions based on either eye-gaze or on task focus or utilized 
saliency models. Gaze-dependent rendering encounters problems with updating the multi-resolution display 
after an eye movement without disturbing visual processing. Task focus rendering cannot be used when 
there is no overt task to be conducted and even when there is, we cannot predict exactly where each task-relevant 
saccade will land. Finally, selective rendering based on saliency models has revealed that correlation 
between actual human and artificially presented scan paths was much lower than predicted when carrying 
out real-world tasks. The proposed approach is based on classic findings from memory research (Brewer 
et al. 1981). The basic premise is that an individual s prior experience will influence how one perceives, 
comprehends and remembers new information in a scene. Schemas are knowledge structures or cognitive frameworks 
based on past experience. Schema consistent scene elements are those which are expected to be found in 
a given context, as with books in an academic s office. Experimental studies in synthetic scenes have 
revealed that consistent objects which are expected to be found in a scene can be rendered in lower quality 
without affecting information uptake taking advantage of such expectations, whereas inconsistent items 
which are salient would require a high level of rendering detail in order for them to be perceptually 
acknowledged (Mania et al. 2005). Therefore, by exploiting schema theory, it is possible to reduce computational 
complexity, producing scenes from a cognitive point of view without affecting information uptake and 
resulting in an entirely novel and interdisciplinary approach which is gaze, task and saliency-model 
independent. This poster will discuss the implementation of such a selective rendering system. A demo 
synthetic scene was built which included a kitchen, lounge and office area and comprised of consistent 
and inconsistent objects relevant to each context. The office area included two sub-regions, the desk 
area and the bookcase area. Similarly, the lounge area included a sofa area. The input to the system 
is an X3D document that describes the geometry of the scene. Varying versions of polygon detail were 
produced for all objects in the scene. The system (Figure 1, bottom right) comprises of two main components. 
The initially-activated Selective Renderer Pre-process component is responsible for the modification 
of the X3D input document in order to be enriched with metadata which describe the scene objects in relation 
to the level of their consistency with each scene region (kitchen, lounge-sofa, office-desk-bookcase). 
Metadata information were also defined for each area (Proximity Nodes) representing the scene s regions 
or knowledge schemata. The metadata enrichment process of the X3D document can also be achieved with 
the support of a simple GUI. Varied polygon count versions of the objects of the scene are available 
during the real-time selective renderer process. The second main component of the Selective Renderer 
Module is the Selective Renderer Real Time Process Component. This is responsible for dynamically loading 
a low or high quality version of a scene s object. A high quality version of an object is loaded for 
a specific area s inconsistent objects, whereas, low quality version of consistent objects are displayed 
in that region. For example, when the user approaches a scene s region, the content of that specific 
region will be activated this could be the desk s area (schema) in the office, rather than the office 
s schema. In this way, as above, polygon detail displayed is dependent on each object s association with 
the region in focus. Real-time user interaction triggers changes of each object s metadata information 
based on user focus and object type. Such modification of the metadata field of any X3D descriptions 
provokes changes on the corresponding Switch Node of the object. Switch Nodes contain varied quality 
versions of the object but only one version is rendered at one time since the references of the Switch 
Nodes are able to change. It has to be noted that the Selective Rendering Module accommodates any context 
of a scene, as long as relevant object metadata are described. The system s architecture and validation 
results are going to be discussed during the Poster session. C:\Documents and Settings\alejandro\Desktop\topview.JPG 
 Figure 1: Experimental scene and system architecture BREWER, W.F. and TREYENS, J.C. (1981). Role of 
Schemata in Memory for Places, Cognitive Psychology 13, 207-2. MANIA, K. &#38; ROBINSON, A., BRANDT, 
K. (2005). The Effect of Memory Schemas on Object Recognition in Virtual Environments. Presence Teleoperators 
&#38; Virtual Environments, 14(5), 606-615, MIT. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599344</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<display_no>43</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[BiDi screen]]></title>
		<subtitle><![CDATA[depth and lighting aware interaction and display]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599344</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599344</url>
		<abstract>
			<par><![CDATA[<p>We present a BiDirectional screen capable of both imaging and display, that uses an LCD as a spatial light modulator to support seamless transition from on-screen multi-touch interactions to off-screen hover-based gestures.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621470</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621471</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621472</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621473</person_id>
				<author_profile_id><![CDATA[81435601065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holtzman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brown, C. J., Kato, H., Maeda, K., and Hadwen, B. 2007. A continuous-grain silicon-system lcd with optical input function. <i>IEEE Journal Of Solid State Circuits 42</i>, 12 (Dec.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276463</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Veeraraghavan, A., Raskar, R., Agrawal, R., Mohan, A., and Tumblin, J. 2007. Dappled photography: Mask enhanced cameras for heterodyned light fields and coded aperture refocusing. <i>ACM Transactions on Graphics 26</i>, 3, 69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 BiDi Screen: Depth and Lighting Aware Interaction and Display MatthewHirsch1 DouglasLanman2 RameshRaskar1 
HenryHoltzman1 1 MITMediaLab 2 BrownUniversity Figure 1: Wedemonstrateanovel modi.cationof atypicalLCD 
toallowco-locatedimagecaptureanddisplay.(Left)A usercanseamlessly transitionfromon-screenmultitouch tooff-screenhoverandgesture-based 
interaction.(Middle)Multi-vieworthographic imagery recorded in real-time using a mask displayed by the 
LCD-based spatial light modulator. (Right, Top)Image refocused at the depth of a user s hand. (Right,Bottom)Real-timedepthestimateprovidedasinput 
tohovermodeinteractionsystem. Abstract WepresentaBiDirectional screencapableofboth imaging anddis­play,thatusesanLCD 
asaspatiallight modulator tosupport seam­less transitionfromon-screenmulti-touchinteractions tooff-screen 
hover-basedgestures. 1. Introduction Anovel methodforusing lightsensors todetectmultiplepointsof contact 
with thesurfaceofliquidcrystaldisplays(LCDs)isemerg­ing. SharpCorporation[Brown et al.2007] and othershavedemon­strated 
LCDs with arrays of optical sensors interlaced within the pixel grid which can be used to drive a multi-touch 
user interface on the surface of the screen. We present the BiDirectional(BiDi) screen, a system inspired 
by optical multitouch,which usesanLCD asaspatial light modulator to allow both image capture and display. 
We use spatial heterody­ing[Veeraraghavanetal.2007] tocapture theangle,aswellas in­tensity, of light 
entering a co-located sensor array. This allows us to image objects, such as .ngers, that are located 
beyond the dis­play s surface and measure their distance from the display. In our prototype, imaging 
is performed in real-time, enabling the detec­tionof off-screengestures.Whenused witha light-emitting 
wand, the BiDi screen can determine not only where the wand is aimed, but alsotheincidenceangleof light 
cast onthedisplay surface. 2. Vision Earlier light sensing display designs have focused on promoting 
touchinterfaces.Ourdesignenhancesthe.eldby seamlessly tran­sitioningfromon-screenmultitouchinteractions 
tooff-screenhover andgesture-based interfaces.Thissingledevicealternatesbetween forming the displayed 
image and capturing a modulated light .eld through a liquid crystal spatial light modulator. Because 
the mask is formed on an LCD, we can vary the size and density of the ar­ray dynamically, allowing for 
scene-optimized imaging. The BiDi screen has the ability to create multiple orthographic images with­outblockingthebacklight 
orsacri.cingportionsof thedisplay.  Figure 2: Design of a BiDi screen. Image capture and display is 
achievedby rearranging traditionalLCD optical components. Our prototype BiDi screen can recognize on-screen 
as well as off­screen gestures. We also demonstrate its ability to detect the posi­tion and angle oflight-emitting 
widgets, showing novelinteractions betweendisplayedimagesand external lighting. 3. Design As shown in 
Figure 2, the BiDi screen is formed by repurposing typical LCD components. We create a large-aperture, 
multi-view image capture device by using the spatial light modulator to dis­play a pinhole array or tiled 
Modi.ed Uniform Redundant Array (MURA) mask. Our key insight is that, for simultaneous image capture 
and display using an LCD, the backlight diffuser must be moved away from the liquid crystal. In doing 
so, the modulated light .eld is captured on the diffuser. The backlight in the proto­type isprovidedby 
anarray ofLEDs. References BROWN, C. J., KATO, H., MAEDA, K., AND HADWEN, B. 2007. A continuous-grain 
silicon-systemlcdwith opticalinput function. IEEEJournalOfSolidStateCircuits 42,12 (Dec.). VEERARAGHAVAN, 
A., RASKAR, R., AGRAWAL, R., MOHAN, A., AND TUMBLIN, J. 2007. Dappled photography: Mask enhanced cameras 
for heterodyned light .elds and coded aperture refocusing. ACMTransactions onGraphics26,3,69. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599345</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<display_no>44</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Bloxels]]></title>
		<subtitle><![CDATA[glowing blocks as volumetric pixels]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599345</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599345</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we propose a novel block-shaped tangible interface named Bloxel (see Figure 1). A Bloxel is a translucent cubical block that glows in full color and communicates with the neighboring Bloxels through high-speed flickers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621474</person_id>
				<author_profile_id><![CDATA[81442599439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jinha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621475</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621476</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1085755</ref_obj_id>
				<ref_obj_pid>1085714</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dunn, H. N., Nakano, H., and Gibson, J. 2003. Block jam: A tangible interface for interactive music. In <i>NIME2003</i>, pp. 170--177.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401030</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kimura, S., Oguchi, R., Tanida, H., Kakehi, Y., Takahashi, K., and Naemura, T. 2008. Pvlc projector: Image projection with imperceptible pixel-level metadata. In <i>ACM SIGGRAPH 2008 Posters</i>, B177.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>503438</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sharlin, E., Itoh, Y., Watson, B., Kitamura, Y., Liu, L., and Sutphen, S. 2002. Cognitive cubes: a tangible user interface for cognitive assessment. In <i>ACM SIGCHI 2002</i>, pp. 347--354.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bloxels: Glowing Blocks as Volumetric Pixels Jinha Lee* Yasuaki Kakehi* Takeshi Naemura* The UniversityofTokyo 
Keio University The UniversityofTokyo Figure 1: Bloxels Figure 2: Application for Entertainment Figure 
3: Application for Education 1 Introduction In this paper, we propose a novel block-shaped tangible interface 
named Bloxel (see Figure 1). A Bloxel is a translucent cubical block that glows in full color and communicates 
with the neigh­boring Bloxels through high-speed .ickers. Our signi.cant accomplishmentis that users 
canbuild displays with a variety of shapes by stacking hundreds of Bloxels on a tabletop surface. Each 
Bloxel obtains its color data from the lower Bloxel through infra-red high-speed .ickers,and transfersa 
seriesof color data to the upper Bloxel. In this way, Bloxels serve as volumetric pixels which can display 
meaningful content as a whole. With our module-based approach, we introduce a ground-breaking display 
technology. Moreover,asan augmentedversionof children s block play, Bloxels will have a signi.cant novel 
impact on the .eld of physical computing and tangible interfaces. 2 Technical Innovations of Bloxels 
Sofar,severaltypesof block-shaped tangibledevicehavebeenpro­posed[Sharlin et al. 2002][Dunn et al. 2003]. 
Compared with these works, technical innovations of our system are as follows: First, our optical design 
for simple communication through light is crucial for the intuitive manipulation of physical blocks. 
ABloxel consists of two full color LEDs for display, nine infra-red LEDs for data transmission,aphoto 
detector,abattery andamicro controller. The infra-red LEDs are placed so as to realize the data transmission 
even when the neighboring Bloxels are not completely in contact with each other. Second, our data processing 
method enables a simple system con­.guration. While each Bloxel communicates only with the neigh­boring 
ones, 3D sensors or cameras are not necessary to track the positions of the Bloxels. Finally, to send 
signals to the base of the stacked Bloxels, we have inventeda horizontal tabletop display system. Our 
specialized DLP *e-mail: vlc@hc.ic.i.u-tokyo.ac.jp projector [Kimura et al. 2008] can emit high-speed 
.ickering sig­nals pixel by pixel to the base. This allows users to realize several kinds of applications 
as demonstrated in our video. 3 Applications We believe that Bloxels can be applied for media art works 
and en­tertainment purposes as well as for display technologies and human interfaces. By using Bloxels, 
we have already implemented some applications. One is an application for entertainment (see Figure 2). 
In this appli­cation, users can see hidden animations (e.g. .owers) by stacking Bloxels on the tabletop. 
Another application is for education (see Figure 3). In this application, Bloxel can serve as an interactive 
tutorial for a shape creation process. When stacking up Bloxels followed by signals, users can be guided 
to create speci.c shaped objects. In the future, we plan to develop much more applications by using the 
Bloxels in various situations. References DUNN,H.N.,NAKANO,H., AND GIBSON,J. 2003. Block jam: A tangible 
interface for interactive music. In NIME2003, pp. 170 177. KIMURA, S., OGUCHI, R., TANIDA, H., KAKEHI, 
Y., TAKA-HASHI, K., AND NAEMURA, T. 2008. Pvlc projector: Image projection with imperceptible pixel-level 
metadata. In ACM SIG-GRAPH 2008 Posters, B177. SHARLIN, E., ITOH, Y., WATSON, B., KITAMURA, Y., LIU, 
L., AND SUTPHEN,S. 2002. Cognitive cubes:a tangible user inter­face for cognitive assessment. In ACM 
SIGCHI 2002, pp. 347 354. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599346</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<display_no>45</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[collecTable]]></title>
		<subtitle><![CDATA[a natural interface for music collections]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599346</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599346</url>
		<abstract>
			<par><![CDATA[<p><b>Introduction and Related Work</b> Tabletop and tangible interfaces have become common in recent years. Technology trends in this area can be found in commercial products, such as Apple's iPhone#8482; and Microsoft Surface#8482;, as well as in research ventures, such as Reactable and Perceptive Pixel initiatives. Nevertheless, natural human computer interfaces (HCI) to support this hardware technology are still non-intuitive.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621477</person_id>
				<author_profile_id><![CDATA[81421597426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andr&#233;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maximo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[COPPE / UFRJ Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621478</person_id>
				<author_profile_id><![CDATA[81442593630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Maria]]></first_name>
				<middle_name><![CDATA[Paula]]></middle_name>
				<last_name><![CDATA[Saba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESDI / UERJ Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621479</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1226983</ref_obj_id>
				<ref_obj_pid>1226969</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kaltenbrunner, M., and Bencina, R. 2007. reacTIVision: A Computer-Vision Framework for Table-Based Tangible Interaction. In <i>TEI '07: Proceedings of the 1st International Conference on Tangible and Embedded Interaction</i>, ACM, New York, NY, USA, 69--74.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614512</ref_obj_id>
				<ref_obj_pid>614285</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stolte, C., and Hanrahan, P. 2002. Polaris: A System for Query, Analysis, and Visualization of Multidimensional Relational Databases. <i>IEEE Transactions on Visualization and Computer Graphics 8</i>, 52--65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 collecTable: a Natural Interface for Music Collections Andre Maximo ´Maria Paula Saba Luiz Velho COPPE 
/ UFRJ Brazil ESDI / UERJ Brazil IMPA Brazil  (a) (b) (c) (d) Figure 1: The collecTable running on the 
iTable (a), with several .ducials over it. The digital projections of the .ducials .oating over the interface 
(b), each .ducial storing different music collections. The M-Cube interface for music data showing albums 
(c) and tracks (d). Introduction and Related Work Tabletop and tangible inter­faces have become common 
in recent years. Technology trends in this area can be found in commercial products, such as Apple s 
iPhoneTM and Microsoft SurfaceTM, as well as in research ventures, such as Reactable and Perceptive Pixel 
initiatives. Nevertheless, natural human computer interfaces (HCI) to support this hardware technology 
are still non-intuitive. Our goal in this work is to provide a natural HCI software for music collection 
organization using multi-touch and tangible interaction. We built the interactive iTable, shown in Figure 
1(a), to act as our tabletop and tangible hardware by employing the same technology used by the Reactable 
[Kaltenbrunner and Bencina 2007]. Our soft­ware collecTable is built on top of Apple s iTunesTM, we 
read the music collection from its library described in XML, and we dis­play and play the music albums 
in a similar way, using the cover .ow and the player on the top of the screen, as can be seen in Fig­ure 
1(b). However, we employ multi-touch gesture for interaction with the cover .ow and player widgets instead 
of using the mouse and keyboard, similarly to the iPod Touch, but enhanced with a tan­gible interface 
and physical objects. The main contribution of this project is the Multi-dimensional Cube, dubbed M-Cube 
(or M3): a new visualization tool for n-dimensional databases. Although visual query languages ex­ist 
[Stolte and Hanrahan 2002], they rely on simple regular graphic types for visualizing the data. Notwithstanding, 
we display the data in charts, as shown in Figures 1(c) and 1(d), where the axes can be naturally changed 
by touch and each face of the M-Cube is a combination of any two dimensions. collecTable The collecTable 
is a music organizer software de­veloped for the iTable, a natural interface which identi.es multiple 
.nger touches and physical objects. The multi-touch is used to ma­nipulate the digital objects and send 
commands to the collecTable, while the physical objects, called .ducials, virtually store music collections 
and may be used to change between different user pro­.les. Figure 1(a) illustrates a .nger touch dragging 
a music album from the cover .ow in the center. In addition, multiple .ngers can also be used to interact 
with several .oating objects at the same time. Figure 1(b) shows some .oating objects in our interface: 
the two stacked objects in the lower left corner are inactive .ducials out of the table, while the other 
three are active .ducials still on top of the table. Note that these three .ducials are of different 
types: the one on the left represents a music album; the slightly rotated one in the center is a collection 
of music albums; and the one on the right is a collection of tracks. The music albums or tracks can 
be dragged into or out of the active .ducials. When the .ducial is removed from the table, it becomes 
inactive and only its virtual imprint remains. While an inactive .du­cial can still be dragged or removed 
by touch interaction, its con­tents are not displayed anymore. M-Cube The M-Cube is a n-dimensional chart 
where each face shows a 2D chart. The X or Y axis may be changed by touch ges­ture like rotating the 
cube by hand. When rotating one of the axes, the other is .xed allowing a better browsing and understanding 
of the entire music collection. Each axis is one of the M-Cube di­mensions. For the collecTable, we choose 
.ve dimensions from the track attributes on iTunesTM software: artist, genre, play count, time and year. 
In this way, our M-Cube has 20 different faces counting mirrored charts. Depending on the current dimensions 
on the X and Y axes, the M-Cube visualization can be music albums or tracks. Figure 1(c) shows the year 
× genre chart, where all the tracks of a single album share the same attribute, therefore the M-Cube 
dis­plays the albums. On the other hand, Figure 1(d) shows the artist × time chart, where the tracks 
have different duration times and, hence, the M-Cube display the tracks. The M3 is used in the collecTable 
to build playlists in a straight­forward way. In contrast of the iTunesTM style to build playlists, where 
the attributes are .lled on pop-up windows, the M-Cube allows track selection by touching attribute values 
and gesturing for browsing over different attributes. We believe it provides a more intuitive way for 
manipulating and visualizing n-dimensional databases, such as music collections. References KALTENBRUNNER, 
M., AND BENCINA, R. 2007. reacTIVision: A Computer-Vision Framework for Table-Based Tangible Inter­action. 
In TEI 07: Proceedings of the 1st International Confer­ence on Tangible and Embedded Interaction, ACM, 
New York, NY, USA, 69 74. STOLTE, C., AND HANRAHAN, P. 2002. Polaris: A System for Query, Analysis, and 
Visualization of Multidimensional Rela­tional Databases. IEEE Transactions on Visualization and Com­puter 
Graphics 8, 52 65. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599347</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<display_no>46</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[The design and evaluation of a lightweight multi-view interaction metaphor for 3D visualization in the CAVE]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599347</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599347</url>
		<abstract>
			<par><![CDATA[<p>We explore the design of a multi-view interaction metaphor for 3D visualization in the CAVE. We then present the results of a formative evaluation of a "Wizard of Oz" [Kelley 1984] prototype. Although there has been significant prior work on 2D and 3D desktop applications utilizing multiple views, little prior work exists for multi-view systems in immersive virtual environments such as the CAVE, despite the clear advantages enjoyed by desktop analogues. Immersive 3D environments pose unique challenges for such a system. Since the contents of such views are themselves 3D, it is unclear whether users will be able to easily read views independently of one another, as in a naive implementation they might become intermingled; even in a system that is conscious of this problem, some vantage points may cause depth ambiguity problems which make it difficult to read each view. In addition, interaction techniques for controlling and managing such views must be explored. Thus, formative empirical testing is warranted to determine the viability of such a system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621480</person_id>
				<author_profile_id><![CDATA[81381597937]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bragdon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621481</person_id>
				<author_profile_id><![CDATA[81100589961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Laidlaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bavoil, L., et al. 2005. Vistrails: Enabling interactive multiple-view visualizations. <i>Visualization Conference, IEEE 0</i>, 18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364370</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Keefe, D. F., et al. 2001. Cavepainting: a fully immersive 3d artistic medium and interactive experience. In <i>I3D '01</i>, 85--93.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>357420</ref_obj_id>
				<ref_obj_pid>357417</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kelley, J. F. 1984. An iterative design methodology for user-friendly natural language office information applications. <i>ACM Trans. Inf. Syst. 2</i>, 1, 26--41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1165736</ref_obj_id>
				<ref_obj_pid>1165734</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Plumlee, M. D., et al. 2006. Zooming versus multiple window interfaces: Cognitive costs of visual comparisons. <i>ACM Trans. Comput.-Hum. Interact. 13</i>, 2, 179--209.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835879</ref_obj_id>
				<ref_obj_pid>580130</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Zeleznik, R. C., et al. 2002. Pop through button devices for ve navigation and interaction. In <i>VR '02</i>, 127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Design and Evaluation of a Lightweight Multi-View Interaction Metaphor for 3D Visualization in the 
CAVE Andrew Bragdon* David H. Laidlaw Brown University 1 Introduction and Related Work We explore the 
design of a multi-view interaction metaphor for 3D visualization in the CAVE. We then present the results 
of a for­mative evaluation of a Wizard of Oz [Kelley 1984] prototype. Although there has been signi.cant 
prior work on 2D and 3D desk­top applications utilizing multiple views, little prior work exists for 
multi-view systems in immersive virtual environments such as the CAVE, despite the clear advantages enjoyed 
by desktop analogues. Immersive 3D environments pose unique challenges for such a sys­tem. Since the 
contents of such views are themselves 3D, it is un­clear whether users will be able to easily read views 
independently of one another, as in a naive implementation they might become in­termingled; even in a 
system that is conscious of this problem, some vantage points may cause depth ambiguity problems which 
make it dif.cult to read each view. In addition, interaction techniques for controlling and managing 
such views must be explored. Thus, for­mative empirical testing is warranted to determine the viability 
of such a system. Desktop-based visualizations, such as VisTrails [Bavoil et al. 2005], frequently employ 
multiple views, as do popular commer­cial 3D packages such as AutoDesk Maya. One study [Plumlee et al. 
2005] found that multiple windows should be used for tasks in which visual comparisons must be made between 
parts that have greater complexity than can be held in working memory; this empir­ical evidence motivates 
our approach. A pop-through button system in the CAVE lets users create snapshots that can be used to 
travel quickly to the location and orientation at which the snapshot was created [Zeleznik et al. 2002]. 
Our work is the .rst to address simultaneous setup, control and management of multiple views in immersive 
virtual reality environments such as the CAVE, a setting that poses unique challenges. 2 Design and 
Evaluation On the basis of the challenges outlined above, we established the following design goals for 
the overall user experience: (G1) simul­taneous readability: it is important that users be able to read 
multi­ple side-by-side views without abnormal effort -views should not intersect, and if line-of-sight 
superimposed, users should be able to distinguish them; (G2) lightweight view management: since views 
will frequently be created on the .y as part of an ongoing scien­ti.c work.ow, view management techniques 
should be simple and lightweight so as to minimize the cognitive burden on the user s work.ow; (G3) the 
system should support core scienti.c work.ow scenarios; (G4) scalability: the system should take advantage 
of the multiple walls available in the CAVE and scale well to a signi.­cant number of simultaneous views. 
See Representative Image 1 for detailed design and a user interaction scenario. Our prototype was implemented 
in CavePainting [Keefe et al. 2001] in a four-wall CAVE with four 1024x768 projectors. Two related visualizations 
of the aerodynamics of bat .ight were em­ployed; one shows .uid .ow, pressure and vorticity around a 
bat wing; the other shows the kinematics of muscles, bones and tendons during .ight. Five participants, 
aged 23-36 and familiar with such *e-mail: acb@cs.brown.edu dhl@cs.brown.edu visualizations, were recruited 
from bat biology, visualization, and human-computer interaction research groups at Brown University. 
All participants reported playing video games with some frequency; three participants had experienced 
the CAVE at least once. Par­ticipants wearing 6-DOF head-tracked, stereoscopic glasses, .rst had time 
to familiarize themselves with the CAVE. They then were given a walkthrough of our Wizard of Oz prototype; 
users exe­cuted commands by describing their intended interaction out loud, at which point we then updated 
the prototype to a new frame show­ing the appropriate state. Participants were asked to think aloud during 
the study, and to say which features they liked or didn t like, and what they might like to add or remove. 
They were encouraged to view the scene from different locations and orientations, to get a sense of how 
the views overlapped from various vantage points. After the session, averaging 30-60 minutes, each participant 
com­pleted a post-questionnaire. Overall, participants liked the approach, with four of the .ve partic­ipants 
rating it Easier or Much Easier to use than visualization interfaces they had previously experienced. 
All .ve participants re­ported that multiple views would be useful and that it was not con­fusing to 
have multiple views juxtaposed when the vantage point caused multiple views to be combined in the line 
of sight. Partici­pants furthermore reported that segmenting the separate views was not dif.cult in general, 
and in such corner cases in particular. All .ve participants said that fading background views in such 
cases might be helpful but was not strictly necessary. Three participants liked the view management techniques 
presented, including the au­tomatic behaviors and the marking menu, while two felt additional changes 
might be needed. One participant felt that 6-DOF mice were too hands off and that hand gestures would 
be a more nat­ural way to manipulate views. Another felt the UI elements should be larger and closer 
to the user to make them easier to target. He was also concerned about how views change size depending 
on ori­entation, saying that he would prefer constant view size. This result appears to support the value 
of lightweight view management, but additional design work may be needed to address some of the con­cerns 
mentioned. All .ve participants reported that it would be nice to zoom in to a speci.c view for a fully 
immersive experience on a single view, and to zoom out to see multiple views as well. Two users reported 
they would like to superimpose views in 3D to visualize differences between similar views. References 
BAVOIL, L., ET AL. 2005. Vistrails: Enabling interactive multiple­view visualizations. Visualization 
Conference, IEEE 0, 18. KEEFE,D. F., ET AL. 2001. Cavepainting: a fully immersive 3d artistic medium 
and interactive experience. In I3D 01, 85 93. KELLEY, J. F. 1984. An iterative design methodology for 
user­friendly natural language of.ce information applications. ACM Trans. Inf. Syst. 2, 1, 26 41. PLUMLEE, 
M. D., ET AL. 2006. Zooming versus multiple window interfaces: Cognitive costs of visual comparisons. 
ACM Trans. Comput.-Hum. Interact. 13, 2, 179 209. ZELEZNIK, R. C., ET AL. 2002. Pop through button devices 
for ve navigation and interaction. In VR 02, 127. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599348</article_id>
		<sort_key>470</sort_key>
		<display_label>Article No.</display_label>
		<display_no>47</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Factors formatting perceptional impression in 3-D cyber spaces]]></title>
		<subtitle><![CDATA[a cross-cultural study of Korean and American users]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599348</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599348</url>
		<abstract>
			<par><![CDATA[<p>Online spaces are being transformed into new social spaces with a variety of interpersonal relationships and social activities. Especially, cyber spaces based on three dimensions show various cross-cultural social relationships and activities compared with cyber spaces based on two dimensions. These phenomena have different characteristics, depending on users' cultural backgrounds. Relating to social issues in online spaces, many preliminary studies have been conducted. Especially, impressions have been considered important subjects related with social networks. In spite of that, sufficient cross-cultural research related with impressions in online spaces has not been conducted, especially based on 3-D cyber spaces. Therefore, the main goals of this study were to extract 3-D cyber factors formatting perceptional impressions and compare those factors based on cultural differences. In the preliminary research, we identified six impressions dimensions in 3-D cyber space: F1.Cheerful, F2.Logical, F3.Violent, F4.Selfish, F5.Warm, and F6.Seclusive (Lee, Kim & Park, 2009).</p> <p>In order to achieve our goal, first, we selected two countries considering Hofstede's culture dimensions (e.g. Power Distance, Individualism versus Collectivism, Masculinity versus Femininity, Uncertainty Avoidance) (Hofstede, 2005). Korea and America have very different cultural characteristics in terms of Hofstede's culture dimensions (Hofstede & Bond, 1984). Secondly, we conducted in-depth individual interviews. For these interviews, we recruited interviewees as actual users of 3-D cyber spaces (Second Life); depending on the frequency uses and interpersonal relations contained therein, we selected eight Korean participants and eight American participants. Before conducting interviews, we recorded normal lives of participants within a three-day span, for two hours of each day. Then, we conducted the survey to each participant seeing the video clips of others' virtual lives for the purpose of analyzing others' preserved impressions. In-depth interviews were conducted in 3-D cyber space using actual voices. The interview consisted of two parts of questions: 1) What are the factors relating with your perceived impressions?; and 2) If you help an avatar on the video clip before you saw to make clear his/her impression, how will you help? All interviews were recorded as video and audio clips.</p> <p>After collecting data, we analyzed data based on Grounded theory (Strauss, 1990) recognized qualitative research methods. First of all, we accurately transcribed all voice data to text data, and then separated data to minimal units of meaning considering interviewees' intentions. Finally, we extracted properties and grouped properties during axial coding.</p> <p>As a result, Factors formatting perceptional impression in 3-D cyber space was derived with distinction by Korean and American users. These derived factors were linguistic, visual, behavioral, relational, inner-environmental, and outer-environment. Of these, linguistic factors (106, 43%) and behavioral factors (57, 23%) were the most derived. Further, looking at the visual factors, the number of derived factors was similar among Korean and American users. Alternatively, we looked at the detailed factors derived with distinction by Korean and American users. The factors derived by Korean users included exposure degree of clothes, thickness of clothes, while the factors derived by American users included color of clothes and types of avatar.</p> <p>In conclusion, this study has theoretical and empirical significance. The theoretical significance, through the cultural differences research, is to understand how each intercultural impression provided role elements in Korea cultures and American cultures and to understand how the impression provided difference elements. Therefore, more extensive future research on the dimensions of the intercultural impression formation mechanism was proposed, based on this study. The empirical significance was to offer impression dimensions-related elements in 3D gaming to developers and designers in the development of related systems; furthermore, as the results provide data of how elements affect impression in intercultural perception and how in each dimension, the system will be able to provide a basis about impression formation elements in intercultural context.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human factors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621482</person_id>
				<author_profile_id><![CDATA[81442592879]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mi]]></first_name>
				<middle_name><![CDATA[Sun]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621483</person_id>
				<author_profile_id><![CDATA[81442594134]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mi-Gi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621484</person_id>
				<author_profile_id><![CDATA[81442595971]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joo-Youn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621485</person_id>
				<author_profile_id><![CDATA[81442610898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Su-e]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hofstede, G., 2005, Cultures and Organizations: Software of the Mind, OH, US: McGraw-Hill.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hofstede, G., Bond, M. H., 1984, Hofstede's Culture Dimensions. Journal of Cross-Cultural Psychology, 15(4), 417--433.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lee, S., Kim, H., Park, S., 2009, Empirical study of Impression Dimensions in 3D cyber space: Comparison research between 2D cyber space and 3D cyber space. in: Proceeding of Korea Human Computer Interaction (Seoul, February, 2009), 24--31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Strauss, A., 1990, Basics of qualitative research: Grounded theory procedures and techniques. Thousand Oaks, CA, US: Sage Publications, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Factors Formatting Perceptional Impression in 3­D Cyber Spaces : A Cross­Cultural Study of Korean and 
American Users Abstract Online spaces are being transformed into new social spaces with a variety of 
interpersonal relationships and social activities. Especially, cyber spaces based on three dimensions 
show various cross­cultural social relationships and activities compared with cyber spaces based on two 
dimensions. These phenomena have different characteristics, depending on users cultural backgrounds. 
Relating to social issues in online spaces, many preliminary studies have been conducted. Especially, 
impressions have been considered important subjects related with social networks. In spite of that, sufficient 
cross­cultural research related with impressions in online spaces has not been conducted, especially 
based on 3­D cyber spaces. Therefore, the main goals of this study were to extract 3­D cyber factors 
formatting perceptional impressions and compare those factors based on cultural differences. In the preliminary 
research, we identified six impressions dimensions in 3­D cyber space: F1.Cheerful, F2.Logical, F3.Violent, 
F4.Selfish, F5.Warm, and F6.Seclusive (Lee, Kim &#38; Park, 2009). In order to achieve our goal, first, 
we selected two countries considering Hofstede s culture dimensions (e.g. Power Distance, Individualism 
versus Collectivism, Masculinity versus Femininity, Uncertainty Avoidance) (Hofstede, 2005). Korea and 
America have very different cultural characteristics in terms of Hofstede s culture dimensions (Hofstede 
&#38; Bond, 1984). Secondly, we conducted in­depth individual interviews. For these interviews, we recruited 
interviewees as actual users of 3­D cyber spaces (Second Life); depending on the frequency uses and interpersonal 
relations contained therein, we selected eight Korean participants and eight American participants. Before 
conducting interviews, we recorded normal lives of participants within a three­day span, for two hours 
of each day. Then, we conducted the survey to each participant seeing the video clips of others virtual 
lives for the purpose of analyzing others preserved impressions. In­depth interviews were conducted in 
3­D cyber space using actual voices. The interview consisted of two parts of questions: 1) What are the 
factors relating with your perceived impressions?; and 2) If you help an avatar on the video clip before 
you saw to make clear his/her impression, how will you help? All interviews were recorded as video and 
audio clips. After collecting data, we analyzed data based on Grounded theory (Strauss, 1990) recognized 
qualitative research methods. First of all, we accurately transcribed all voice data to text data, and 
then separated data to minimal units of meaning considering interviewees intentions. Finally, we extracted 
properties and grouped properties during axial coding. As a result, Factors formatting perceptional impression 
in 3­D cyber space was derived with distinction by Korean and American users. These derived factors were 
linguistic, visual, behavioral, relational, inner­environmental, and outer­environment. Of these, linguistic 
factors (106,43%) and behavioral factors (57,23%) were the most derived. Further, looking at the visual 
factors, the number of derived factors was similar among Korean and American users. Alternatively, we 
looked at the detailed factors derived with distinction by Korean and American users. The factors derived 
by Korean users included exposure degree of clothes, thickness of clothes, while the factors derived 
by American users included color of clothes and types of avatar. In conclusion, this study has theoretical 
and empirical significance. The theoretical significance, through the cultural differences research, 
is to understand how each intercultural impression provided role elements in Korea cultures and American 
cultures and to understand how the impression provided difference elements. Therefore, more extensive 
future research on the dimensions of the intercultural impression formation mechanism was proposed, based 
on this study. The empirical significance was to offer impression dimensions­related elements in 3D gaming 
to developers and designers in the development of related systems; furthermore, as the results provide 
data of how elements affect impression in intercultural perception and how in each dimension, the system 
will be able to provide a basis about impression formation elements in intercultural context. Acknowledgements 
This work was supported by the Korea Research Foundation Grant funded by the Korean Government (MOEHRD) 
(KRF­D00032) Reference Hofstede, G., 2005, Cultures and Organizations: Software of the Mind, OH, US: 
McGraw­Hill. Hofstede, G., Bond, M. H., 1984, Hofstede's Culture Dimensions. Journal of Cross­Cultural 
Psychology, 15(4), 417­433. Lee, S., Kim, H., Park, S., 2009, Empirical study of Impression Dimensions 
in 3D cyber space: Comparison research between 2D cyber space and 3D cyber space. in: Proceeding of Korea 
Human Computer Interaction (Seoul, February, 2009), 24­31. Strauss, A., 1990, Basics of qualitative research: 
Grounded theory procedures and techniques. Thousand Oaks, CA, US: Sage Publications, Inc. Copyright is 
held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Korea Research Foundation Grant funded by the Korean Government (MOEHRD)</funding_agency>
			<grant_numbers>
				<grant_number>KRF-D00032</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599349</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<display_no>48</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[LipMouse]]></title>
		<subtitle><![CDATA[novel multimodal human-computer interaction interface]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599349</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599349</url>
		<abstract>
			<par><![CDATA[<p>The main goal of each HCI application is to make working with a computer as natural, intuitive and effective as possible. One of the main areas of applications of new human-computer interfaces is making possible to use computers for people with permanent or temporal motor disabilities in an efficient way. There are two main types of such solutions [Aggarwal and Cai 1999]. The first group utilizes devices mounted directly on the user's body. Applications in the second group are contactless and they use remote sensors only, therefore they are much more comfortable for a user. Amongst contactless solutions, vision-based human-computer interfaces are the most promising ones. They utilize cameras and image processing algorithms to detect signs and gestures made by a user and execute configured actions. The most common vision-based applications employ eye and hand tracking [Shin and Chun 2007].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621486</person_id>
				<author_profile_id><![CDATA[81351596363]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Piotr]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dalka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gdansk University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621487</person_id>
				<author_profile_id><![CDATA[81100277913]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrzej]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Czyzewski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gdansk University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>312615</ref_obj_id>
				<ref_obj_pid>312594</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aggarwal J. K., Cai Q. 1999. Human Motion Analysis: A Review. CVIU(73), No. 3, pp. 428--440, March.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320507</ref_obj_id>
				<ref_obj_pid>2319028</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Leung S., Wang S., Lau W. 2004. Lip image segmentation using fuzzy clustering incorporating an elliptic shape function. <i>IEEE Trans. on Image Processing</i>, vol. 13, no. 1, pp. 51--62, Jan.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1335627</ref_obj_id>
				<ref_obj_pid>1335119</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shin G., Chun J. 2007. Vision-based Multimodal Human Computer Interface based on Parallel Tracking of Eye and Hand Motion. <i>Int. Conf. on Conv. Inf. Techn.</i>, p. 2443--2448.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Viola P., Jones M. 2001. Rapid Object Detection using a Boosted Cascade of Simple Features. <i>IEEE CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LipMouse - Novel Multimodal Human-Computer Interaction Interface Piotr Dalka Andrzej Czyzewski Gdansk 
University of Technology, Gdansk University of Technology, Multimedia Systems Department, Multimedia 
Systems Department, Narutowicza 11/12, 80-233 Gdansk, Poland Narutowicza 11/12, 80-233, Gdansk, Poland 
piotr.dalka@sound.eti.pg.gda.pl ac@pg.gda.pl 1 Introduction The main goal of each HCI application is 
to make working with a computer as natural, intuitive and effective as possible. One of the main areas 
of applications of new human-computer interfaces is making possible to use computers for people with 
permanent or temporal motor disabilities in an efficient way. There are two main types of such solutions 
[Aggarwal and Cai 1999]. The first group utilizes devices mounted directly on the user s body. Applications 
in the second group are contactless and they use remote sensors only, therefore they are much more comfortable 
for a user. Amongst contactless solutions, vision-based human­computer interfaces are the most promising 
ones. They utilize cameras and image processing algorithms to detect signs and gestures made by a user 
and execute configured actions. The most common vision-based applications employ eye and hand tracking 
[Shin and Chun 2007].  2 Interface description LipMouse is a contactless human-computer interaction 
(HCI) interface for controlling computers by mouth movements plus lip and tongue gestures. The target 
users for the tool are people who, for any reason, are not able to use conventional input devices or 
do not want to use them in some situations. Therefore, LipMouse provides a solution enabling severely 
disabled or paralyzed people to use a computer and to communicate with the surrounding world. No user 
adaptation is required in order to work with the LipMouse, efficiently. LipMouse is an application running 
on a standard PC computer. It requires only one hardware component: a display-mounted, standard web camera 
capturing images of the user face. The main task of the LipMouse is to detect and to analyze images of 
user s mouth region in a video stream acquired from a web-camera in the real-time. All movements of mouth 
(or head) are converted to movements of the screen cursor. Various parameters regarding threshold, speed 
and alteration of the cursor movement may be set according to user preferences. LipMouse also detects 
4 mouth gestures: neutral face expression (no gesture), opening the mouth, sticking out the tongue and 
shaping the mouth into an O shape (Fig. 1). Each gesture may be associated with an action, which may 
be freely chosen by a user. Possible actions include: clicking various mouse buttons, moving mouse wheels 
and others. Each session with the LipMouse is preceded by a half-minute lasting calibration process helping 
to tune the LipMouse in order to detect mouth gestures made by the user in the current lighting conditions. 
Fig. 2 presents the scheme of the algorithm used in the LipMouse. First, a user s face is detected in 
every image frame captured by a web camera. LipMouse employs a cascade of boosted classifiers working 
with Haar-like features for this purpose [Viola and Jones 2001]. Then, the mouth region is localized 
and its shift from the reference mouth position is calculated. This shift is directly used to move a 
screen cursor; the greater the shift is, the faster the cursor moves towards a given direction. Simultaneously, 
a small region (blob) placed on user lips is found in the mouth region. This blob is used as a initial 
condition for an iterative method for lip shape extraction [Leung et al. 2004]. Lip shape and lip region 
image features are used by an artificial neural network to classify gestures made by a user.  Figure 
1: Lip gestures recognized by LipMouse Figure 2: Scheme of LipMouse algorithm  References AGGARWAL 
J. K., CAI Q. 1999. Human Motion Analysis: A Review. CVIU(73), No. 3, pp. 428-440, March. LEUNG S., WANG 
S., LAU W. 2004. Lip image segmentation using fuzzy clustering incorporating an elliptic shape function. 
IEEE Trans. on Image Processing, vol.13, no.1, pp. 51-62, Jan. SHIN G., CHUN J. 2007. Vision-based Multimodal 
Human Computer Interface based on Parallel Tracking of Eye and Hand Motion. Int. Conf. on Conv. Inf. 
Techn., p. 2443 2448. VIOLA P., JONES M. 2001. Rapid Object Detection using a Boosted Cascade of Simple 
Features. IEEE CVPR. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599350</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<display_no>49</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[<i>MYGLOBE</i>]]></title>
		<subtitle><![CDATA[cognitive map as communication media]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599350</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599350</url>
		<abstract>
			<par><![CDATA[<p><i>MYGLOBE</i> is an interactive map media which allows us to share our cognitive maps. This map grows up with our own activities and shows our subjective view of the city by emphasizing roads or landmarks frequently used. Users can bring up their own city in the device by actually walking in the city, and also share their own maps with each other and discover unknown places. Present map services such as Google maps and Google Earth, provide mash-up tools which allow us to create our own favorite place on the map easily. We can use hand held GPS devices to make our own travel route and navigate to destination places. <i>MYGLOBE</i> allows us to not only tag their favorite places on the map but also change the shape of the map itself. Instead of an accurate geographic map, <i>MYGLOBE</i> provides maps reflecting the user's individual experiments and the view of the city. It can also be used as a communication tool to share the life history with your friends. <i>MYGLOBE</i> will enhance your city experience.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Spatial databases and GIS</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003236</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Spatial-temporal systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003145.10003147.10010887</concept_id>
				<concept_desc>CCS->Human-centered computing->Visualization->Visualization application domains->Geographic visualization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621488</person_id>
				<author_profile_id><![CDATA[81421601327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fumitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ozaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621491</person_id>
				<author_profile_id><![CDATA[81442616351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imbe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621492</person_id>
				<author_profile_id><![CDATA[81442608900]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kiyasu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621493</person_id>
				<author_profile_id><![CDATA[81442608773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugiura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621494</person_id>
				<author_profile_id><![CDATA[81442611762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizukami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621495</person_id>
				<author_profile_id><![CDATA[81331495283]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Shuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishibashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621496</person_id>
				<author_profile_id><![CDATA[81100344143]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Maki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sugimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621497</person_id>
				<author_profile_id><![CDATA[81100424140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621498</person_id>
				<author_profile_id><![CDATA[81100418633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Cheok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621489</person_id>
				<author_profile_id><![CDATA[81319498464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Naohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okude]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621490</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Masahiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
<i>Automatic Generation of Tourist Maps</i> Floraine Grabler, Maneesh Agrawala, Robert W. Sumner, Mark Pauly SIGGRAPH 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360699</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
<i>Sphere: A Multi-Touch Interactive Spherical Display</i> Hrvoje Benko, Andy Wilson, Ravin Balakrishnan, Billy Chen UIST 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 MYGLOBE : Cognitive Map as Communication Media Fumitaka Ozaki* Takuo Imbe Shin Kiyasu Yuta Sugiura Yusuke 
Mizukami Shuichi Ishibashi Maki Sugimoto Masahiko Inami Adrian D. Cheok Naohito Okude Masahiko Inakage 
KMD,Keio University 0B 1 Introduction MYGLOBE is an interactive map media which allows us to share our 
cognitive maps. This map grows up with our own activities and shows our subjective view of the city by 
emphasizing roads or landmarks frequently used. Users can bring up their own city in the device by actually 
walking in the city, and also share their own maps with each other and discover unknown places. Present 
map services such as Google maps and Google Earth, provide mash-up tools which allow us to create our 
own favorite place on the map easily. We can use hand held GPS devices to make our own travel route and 
navigate to destination places. MYGLOBE allows us to not only tag their favorite places on the map but 
also change the shape of the map itself. Instead of an accurate geographic map, MYGLOBE provides maps 
reflecting the user's individual experiments and the view of the city. It can also be used as a communication 
tool to share the life history with your friends. MYGLOBE will enhance your city experience. 3 Figure 
1: MYGLOBE  0B 2 Related Works "Automatic Generation of Tourist Maps" [Grabler 2008] is a prior work 
about maps emphasizing landmarks and streets. The size accords to their scores evaluated by semantic 
information from websites. In contrast, MYGLOBE emphasizes important areas of user's subjective cognitions 
according to the user's own activity history. "Sphere: A Multi-Touch Interactive Spherical Display" is 
a prior work about an interactive spherical display prototype. MYGLOBE is not only a small sized spherical 
multi-touch display but also enables interaction, such as rolling within the hands and data communicating 
between the devices. 0B 3 Implementations MYGLOBE service consists of software for generating the cognitive 
map from user's location data history and hardware for displaying the map. The GPS in the MYGLOBE device 
attains the exact location data (longitude and latitude) of the user at fixed intervals and sends the 
data to the server. Routes you walk very often are indicated broadly. Routes you have walked only once 
are quite narrow. Landmarks you spent your time are expressed larger than usual. The user's territory 
assumed by his/her activity record will be shaped like islands. There are two versions of MYGLOBE, one 
as a base version for viewing the map and another as a mobile version for *e-mail:fumitaka@kmd.keio.ac.jp 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008 "location-logging". Base version can be used to share all the friends' 
cognitive map together as for displaying many maps together at one time. Attached to the axis, there 
is a rotary-encoder which allows the displayed cognitive map to change as it turns. Mobile version is 
composed of spherical display with GPS, ZigBee, and 3D acceleration sensor inside. The mobile version's 
spherical display can project as a perfect sphere because of the omni-directional mirror placed inside 
the sphere. Users can share their cognitive maps with each other by having each other's MYGLOBE close 
together which communicates by ZigBee. Figure 2: two versions 4 User Experience The user goes out to 
city with mobile MYGLOBE which stores his/her activity record. With MYGLOBE, users can express his/her 
own world and individuality by generating their favorite landmarks or roads. Moreover, users can customize 
the favorites as is s/he brought the city up. The user can also look back on their own city and and have 
a new understanding of it. This causes memories relating to the city to flash back, and become more memorable. 
User can share their favorite places by trading the landmarks with each other, like Pokemons. MYGLOBE 
shows places where you have never went before as blank like ancient maps. This causes pioneer sprit. 
 0B 5 Future works / Conclusion By using MYGLOBE, we can have communication with each other and find 
new faces to cities, and also develop social relationships based on locations. This applies new services 
focusing on personal point of view of the city. For example, this could make tourist maps as keeping 
track of the vacation memory. MYGLOBE remembering activities of the vacation would become the best souvenir 
ever. 0B Acknowledgment This project is granted by CREST, JST. 0B Reference [1]Automatic Generation 
of Tourist Maps Floraine Grabler, Maneesh Agrawala, Robert W. Sumner, Mark Pauly SIGGRAPH 2008 [2]Sphere: 
A Multi-Touch Interactive Spherical Display Hrvoje Benko, Andy Wilson, Ravin Balakrishnan, Billy Chen 
UIST 2008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599351</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<display_no>50</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[PCCD]]></title>
		<subtitle><![CDATA[parallel continuous collision detection]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599351</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599351</url>
		<abstract>
			<par><![CDATA[<p>Collision detection between deformable models is one of fundamental tools of various applications including games. Collision detection can be classified into two categories: discrete and continuous collision detection methods. Discrete collision detection (DCD) has been demonstrated to show the interactive performance by using bounding volume hierarchies (BVHs). However, some colliding primitives may be missed since DCD methods find intersecting primitives only at discrete time steps. This issue can be a very serious problem in physical based simulation, CAD/CAM applications and etc. On the other hand, continuous collision detection (CCD) identifies the first time of contact of colliding primitives during a time interval between two discrete time steps.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621499</person_id>
				<author_profile_id><![CDATA[81442593431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Duksu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621500</person_id>
				<author_profile_id><![CDATA[81442594475]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jae-Pil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621501</person_id>
				<author_profile_id><![CDATA[81100019061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sung-eui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Asanovic, K., R., Catanzaro, B., Gebis, J., Husbands, P., K., Patterson, D., Plishker, W., Shalf, J., Williams, S., and Yelick, K. 2006. The landscape of parallel computing research: A view from Berkeley. Tech. Rep. UCB/EECS-2006-183, EECS Dept., Univ. of California, Berkeley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1342260</ref_obj_id>
				<ref_obj_pid>1342250</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Curtis, S., Tamstorf, R., and Manocha, D. 2008. Fast collision detection for deformable models using representative-triangles. <i>Symp. on Interactive 3D Graphics</i>, 61--69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142006</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sud, A., Govindaraju, N., Gayle, R., Kabul, I., and Manocha, D. 2006. Fast proximity computation among deformable models using discrete voronoi diagrams. <i>Proc. of ACM SIGGRAPH</i>, 1144--1153.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PCCD: Parallel Continuous Collision Detection Duksu Kim Jae-Pil Heo Sung-euiYoon KAIST(Korea Advanced 
Instituteof Science andTechnology) Introduction: Collision detection between deformable models is 25 
one of fundamental tools of various applications including games. Collision detection can be classi.ed 
into two categories: discrete and continuous collision detection methods. Discrete collision de­tection 
(DCD) has been demonstrated to show the interactive per- Frame rate (fps) formance by using bounding 
volume hierarchies (BVHs). However, some colliding primitives may be missed since DCD methods .nd intersecting 
primitives only at discrete time steps. This issue can be a very serious problem in physical based simulation, 
CAD/CAM applications and etc. On the other hand, continuous collision detec­tion (CCD) identi.es the 
.rst time of contact of colliding primitives during a time interval between two discrete time steps. 
Although CCD methods improve the accuracy of collision detec­tion compared with DCD methods, CCD methods 
typically require much longer computation time. Therefore, CCD technology has not been actively used 
in interactive applications such as game. There are many approaches to accelerate the performance of 
CCD by de­signing specialized algorithm on certain types of models (e.g., rigid objects, articulated 
bodies, meshes with .xed topology), utilizing the computation power of GPUs[Sud et al. 2006], and introducing 
ef.cient culling methods[Curtis et al. 2008]. However, these meth­ ods may take hundreds of milliseconds 
and, even, a few seconds on performing CCD for deforming models consisting of hundreds of thousand triangles. 
Recently, the performance of CPUs is improved by increasing the number of cores instead of improving 
the clock frequency of a sin­gle core[Asanovic et al. 2006]. Also, most of commodity hardware already 
has two or more cores. Moreover, it is expected that hun­dredsof coresonachip canbebuiltina near future 
accordingtothe technology trend of the transistor integration capacity. There are, however, only a few 
works on parallel collision detection methods using multiple CPUs. Also, none of them are not designed 
to handle deforming models consisting of hundreds of thousands triangles. Our approach: We present a 
novel parallel continuous collision detection (PCCD) method to achieve the interactive performance of 
CCD between deforming models on commodity multi-core CPU architecture. In order to design a highly scalable 
parallel CCD method, we propose novel task decomposition and dynamic task assignment methods. Our novel 
decomposition is based on Self-CD task unit. We also utilize disjoint property of task units to design 
a highly-scalalble parallel CCD method. Self-CD task unit: A self-CD task unit, SCTPS (n), ofa node 
n in a BVH is a set of collision test pairs generated by performing a collision pair between two child 
nodes, na and nb, of a node n.  Disjoint property of task units: If there is no parent-child relationship 
between two nodes, n and m, a set of accessed nodes during performing SCTPS (n) is disjoint from another 
set of accessed nodes performing SCTPS (m).  With the disjoint property, we can design an ef.cient dynamic 
task assignment method and lazy BVH reconstruction method for dy­namically deforming models. Moreover, 
processing a self-CD task unit canbe done without anylocking mechanisminthe main loopof collision detection. 
We demonstrate the performance of our PCCD algorithm with various benchmarks consisting of hundreds of 
thou­sands triangles. We have tested our method with an Intel Xeon machine with two 2.83 GHz quad-core 
CPUs. Advantages of our approach: Our method has the following ben­e.ts: Number of cores Figure 1: Cloth 
Benchmark: The left image shows a frame of our cloth simulation benchmark consisting of 92 K triangles. 
The right image shows the frame rate, frames per second (fps), of our PCCD method with an ideal frame 
rate assuming a perfect scalability. In this benchmark our method spends 56 ms for continuous collision 
detection including self-collisions on average and 6.4 times perfor­mance improvement by using 8-cores 
over using a single-core. Figure 2: Scalability of the PCCD Method: This .gure shows the performance 
improvement of our PCCD method as a function of the number of cores over using a single-core with different 
benchmarks. We canacheiveupto7.3 times speedupbyusing8 cores. 1. High scalability: Due to a few dependencies 
and synchro­nizations between computational tasks, our method achieves highly scalable performance as 
we allocate more cores to our PCCD method. For example, we are able to achieve up to 7.3 times performance 
improvement by using 8-cores over using a single-core (see Fig. 2). 2. Interactive performance: Due 
to the high scalability of our method combined with bene.ts of using lazy BV reconstruc­tion and a feature-based 
BVH, our PCCD method spends 50ms 140ms on average and, thus, is able to achieve inter­active performance 
for CCD including self-collisions between deforming models consisting of tens or hundreds of thousand 
triangles (see Fig. 1). 3. Generality: Our PCCD method can handle various types of deforming models 
including polygon soups. Also, our task decomposition and dynamic task assignment methods can be applicable 
directly to other BVH-based proximity queries such as minimum separation distance.  References ASANOVIC, 
K., R., CATANZARO, B., GEBIS, J., HUSBANDS, P., K., PATTER-SON, D., PLISHKER, W., SHALF, J., WILLIAMS, 
S., AND YELICK, K. 2006. The landscape of parallel computing research: A view from Berkeley. Tech. Rep. 
UCB/EECS-2006-183, EECS Dept., Univ. of California, Berkeley. CURTIS, S., TAMSTORF, R., AND MANOCHA, 
D. 2008. Fast collision detection for deformable models using representative-triangles. Symp. on Interactive 
3D Graph­ics, 61 69. SUD, A., GOVINDARAJU, N., GAYLE, R., KABUL, I., AND MANOCHA, D. 2006. Fast proximity 
computation amongdeformable models using discrete voronoi dia­grams. Proc. ofACM SIGGRAPH, 1144 1153. 
 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599352</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<display_no>51</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[Pen de Touch]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599352</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599352</url>
		<abstract>
			<par><![CDATA[<p>We propose a pen-shaped handheld haptic display that allows haptic interactions with virtual environments by generating kinesthetic sensations on the user's fingers; the user's movements are not restricted since the device does not have mechanical linkages. Unlike conventional haptic displays that provide vibrations, which are not representative of tactile sensation, our proposed device, named "Pen de Touch" (Figure 1), provides kinesthetic sensations to the muscles in the user's fingers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621502</person_id>
				<author_profile_id><![CDATA[81421597185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621503</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621504</person_id>
				<author_profile_id><![CDATA[81100173571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621505</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401657</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kyung, K. and Lee, J. 2008. wUbi-Pen: windows graphical user interface interacting with haptic feedback stylus, <i>ACM SIGGRAPH 2008 New Tech Demos</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fiorentino, M., Uva, A. and Monno, G. 2005. The Senstylus: a novel rumble-feedback pen device for CAD application in Virtual Reality. In <i>Proceedings of WSCG 2005</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pen de Touch Sho Kamuro*1 Kouta Minamizawa*2 Naoki Kawakami*3 Susumu Tachi*4 The University of Tokyo 
The University of Tokyo The University of Tokyo Keio University       Figure 1: Proposed device 
 Pen de Touch Figure 2: Internal structure Figure 3: Prototype system Figure 4: Use of Pen de Touch in 
a museum  We propose a pen-shaped handheld haptic display that allows haptic interactions with virtual 
environments by generating kinesthetic sensations on the user s fingers; the user s movements are not 
restricted since the device does not have mechanical linkages. Unlike conventional haptic displays that 
provide vibrations, which are not representative of tactile sensation, our proposed device, named Pen 
de Touch (Figure 1), provides kinesthetic sensations to the muscles in the user s fingers. 1. Introduction 
Various handheld haptic displays have been developed, which are more easy to use than wearable haptic 
displays because the user needs to merely grasp the device to enjoy the haptic interactions. Human haptic 
sensations include cutaneous sensations of the skin and kinesthetic sensations produced at the joints 
of fingers and arms. PHANToM [SensAble Technologies Inc.] is an example of a typical handheld haptic 
display, which enables a user to perceive kinesthetic sensations with the help of mechanical linkages, 
which are driven by multiple motors. However, this device requires to be grounded, restricting the user 
s movements within the range of the mechanical linkages. Recently developed portable handheld haptic 
displays such as wUbi-Pen [Kyung and Lee 2008] and Senstylus [Fiorentino et al. 2005] can provide haptic 
sensations without mechanical linkages. Although such ungrounded devices do not impose restrictions on 
the motion performed by users, they can provide only cutaneous sensations or periodic kinesthetic sensation. 
The wUbi-Pen requires the use of physical contacts with the screen surface and does not function if it 
is moved in mid-air; on the other hand, Senstylus can provide only vibrations, which do not satisfactorily 
represent the realistic feeling of touch. The development of an ungrounded haptic display that can provide 
continual kinesthetic sensations has not been reported thus far. 2. Method Our proposed haptic device 
is pen-shaped so that the user can hold the device in the same way as he or she would hold a writing 
pen. In order to downsize the device, we developed our device on the basis of the hypothesis that the 
kinesthetic sensations on fingers alone are sufficient to represent the sensations of touch. An ungrounded 
device can not apply an external force to the user s hand; therefore, the point of support and the point 
of application of force must be located within the hand itself. We fixed the supporting point as a point 
on the base of the index finger and applied forces to the fingertips by changing the length of the pen-shaped 
device. Therefore, we developed a haptic display for haptic augmentation, which the user could use and 
freely move his or her hands in mid-air without any restrictions that could be introduced by the use 
of mechanical linkages. Figure 2 shows the mechanism of working of our proposed device. The device consists 
of a part from where the pen is held (grip part) and a base part. When the device is held in a user s 
hand, the base part is fixed to the base of the user s index finger, which is inserted in a ring attached 
to the base part; the user grasps the grip part by tip of the index finger, the middle finger, and the 
thumb. The motion of the device is measured by using an optical motion capture system. When the tip of 
the device touches a virtual object, the grip part is pulled back toward the base part with the help 
of the motors, thereby generating the kinesthetic sensations on the skin and muscles of the user s fingers. 
Inside of the base part, three motors and strings are fixed, which pull each connecting point in the 
grip part and control the 3-DOF motion of the grip part, as shown in Figure 2. The motion parallel to 
the central axis generates pushing or pecking sensations on the fingers, and the motion perpendicular 
to the central axis generates the sensation of friction or the sensation of touching an object by the 
side of the pen. 3. Application Pen de Touch is a simple device, which can be easily applied to various 
types of conventional virtual reality environments that are not haptically augmented. Figure 3 shows 
a prototype haptic interaction system interacting with a computer graphics character. Our haptic display 
device provides the kinesthetic sensations to the users depending on the contacts between the pen tip 
and the virtual character. The virtual character then moves according to the contacts from the user, 
thereby making the virtual reality system interactive. As a result, the user experiences the feeling 
of communicating with the character, as it were actually existing in the real world. The easy accessibility 
and good representational ability of our proposed haptic device has encouraged us to further develop 
this device for use in practical systems; as the next step in this direction, we plan to construct a 
haptic interaction system for multiple users, which can be used in a public domain such as a museum (Figure 
4). References Kyung, K. and Lee, J. 2008. wUbi-Pen: windows graphical user interface interacting with 
haptic feedback stylus, ACM SIGGRAPH 2008 New Tech Demos. *1 e-mail: sho_kamuro@ipc.i.u-tokyo.ac.jp 
*2 e-mail: kouta_minamizawa@ipc.i.u-tokyo.ac.jp *3 e-mail: naoki_kawakami@ipc.i.u-tokyo.ac.jp *4 e-mail: 
tachi@tachilab.org Fiorentino, M., Uva, A. and Monno, G. 2005. The Senstylus: a novel rumble-feedback 
pen device for CAD application in Virtual Reality. In Proceedings of WSCG 2005. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599353</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<display_no>52</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Actual map based interface for browsing content on mobile devices]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599353</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599353</url>
		<abstract>
			<par><![CDATA[<p>The recent popularity of small mobile devices such as cellular phones, digital cameras, and game device has made it increasingly convenient to carry them with us as we go about our daily lives. However, the increased miniaturization and functionality range of these devices can make it hard to access and utilize their contents. For instance, when using a device with a small display screen, it is often impossible to display an entire area of interest in a single view and the existence of numerous buttons can make it difficult to manipulate the device. In this study, we propose a novel interface that allows intuitive browsing of the content of mobile devices using an actual map.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Screen design (e.g., text, graphics, color)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Portable devices (e.g., laptops, personal digital assistants)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003123</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621506</person_id>
				<author_profile_id><![CDATA[81442599677]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621507</person_id>
				<author_profile_id><![CDATA[81447597329]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621508</person_id>
				<author_profile_id><![CDATA[81331505544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Reiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuruno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621509</person_id>
				<author_profile_id><![CDATA[81319502887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Taketoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ushiama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Actual Map Based Interface for Browsing Content on Mobile Devices Toki Takeda Kazuhiko Yamamoto Reiji 
Tsuruno Taketoshi Ushiama Graduate School of Design Graduate School of Design Faculty of Design Faculty 
of Design Kyushu University ADUDU and CDS Kyushu University Kyushu University Kyushu University 1 INTRODUCTION 
The recent popularity of small mobile devices such as cellular phones, digital cameras, and game device 
has made it increasingly convenient to carry them with us as we go about our daily lives. However, the 
increased miniaturization and functionality range of these devices can make it hard to access and utilize 
their contents. For instance, when using a device with a small display screen, it is often impossible 
to display an entire area of interest in a single view and the existence of numerous buttons can make 
it dif.cult to manipulate the device. In this study, we propose a novel interface that allows intuitive 
browsing of the content of mobile devices us­ing an actual map. 2 OVERVIEW We propose a new mobile system 
consisting of two parts; a mobile device and an actual map. The mobile device is used for pointing to 
locations on the map. In our prototype of this basic system, when user takes a picture at location A, 
the picture is saved to the mobile device along with positional information for location A. Then, if 
the user places the mobile device over loca­tion A on the actual map, the picture is displayed on the 
mobile device. With this system, users can explore and select content by moving the device about the 
map and holding it stationary over locations containing content of interest. 3 IMPLEMENTATION This system 
consists of a map, mobile phone, web camera, and a PC. Using the PC, after using the web camera to detect 
the position of the mobile phone on the map, the detected position is automatically converted to latitude/longitude 
information. Using a mobile phone, EXIF information on the pictures stored in the device is then accessed 
to obtain the latitude/longitude informa­tion of the shooting location. The mobile phone then displays 
the picture corresponding to the obtained lat­itude/longitude information received from the PC via the 
Internet. 4 OPERATION In our system, three modes can be used to browse con­tents using location information. 
1.SONAR Users are unable to see the distribution of photos on the map itself. However, when the SONAR 
mode is used, the photo that is nearest to the location that the mobile device is pointed at will be 
displayed. As a result, when this mode is used, it is not necessary to scan across the entire map to 
locate a particular photo. 2.NAVIGATING We also propose an operation to assist in situations where users 
are searching for the location of photos on the map. When used, NAVIGATING mode acts as a compass-like 
navigator to indicate the direction of the location that contains the photos users wish to browse. It 
shows the direction by displaying the photos on the cycle, and reports the distance from the mobile device 
to the photo by changing the sizes of the photos dis­played. The size of the displayed pictures is propor­tional 
to the distance between the device and the posi­tion where the picture can be found on the map. 3.AREA 
RING Using this mode, a desired area can be selected by surrounding a speci.c region on the map with 
the de­vice. This allows content to be selected and managed by specifying a speci.c map location range. 
 5 CONCLUSION By making use of an actual object to provide reference points, such as a map in this case, 
these three browsing interface modes can minimize a number of operational problems related to accessing 
information with mobile devices. In the future, we intend to miniaturize this system and adapt its use 
to objects other than maps. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599354</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<display_no>53</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[Sketch-based annotations in Google Earth]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599354</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599354</url>
		<abstract>
			<par><![CDATA[<p>Painting in a 3D geobrowser is interesting both for artistic uses such as virtual graffiti and for commercial applications such as architectural sketches. It <i>seems</i> straightforward to turn Google Earth into a 3D painting tool: Store the 3D mouse data that the software returns and construct polylines from them. However, this approach has two vexing drawbacks: First, when the user paints past an edge, the end of the stroke will be placed at an incorrect depth, see Figure 1; second, it is not possible to sketch in mid-air, for instance to indicate a planned height extension of a building.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Spatial databases and GIS</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003236</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Spatial-temporal systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003145.10003147.10010887</concept_id>
				<concept_desc>CCS->Human-centered computing->Visualization->Visualization application domains->Geographic visualization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621511</person_id>
				<author_profile_id><![CDATA[81313482267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schulze]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hochschule Bremen (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621512</person_id>
				<author_profile_id><![CDATA[81442618911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Laurens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nienhaus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hochschule Bremen (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621513</person_id>
				<author_profile_id><![CDATA[81408599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loviscach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Bielefeld (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sketch-Based Annotations in Google Earth Christian Schulze Laurens Nienhaus Hochschule Bremen (University 
of Applied Sciences)  1 Introduction Painting in a 3D geobrowser is interesting both for artistic uses 
such as virtual graf.ti and for commercial applications such as architec­tural sketches. It seems straightforward 
to turn Google Earth into a 3D painting tool: Store the 3D mouse data that the software returns and construct 
polylines from them. However, this approach has two vexing drawbacks: First, when the user paints past 
an edge, the end of the stroke will be placed at an incorrect depth, see Figure 1; sec­ond, it is not 
possible to sketch in mid-air, for instance to indicate a planned height extension of a building. These 
issues could be solved easily if 3D meshes were available. However, these data remain mostly inaccessible, 
partly due to copy­right issues. We present a prototype of a collaborative Web-based solution that constructs 
simpli.ed painting surfaces from 3D mouse points collected through Google Earth. (Note that the stand-alone 
version of the Google Earth browser offers tools to draw paths and polygons onto the scene, but is limited 
to ground-level drawings.)  Figure 1: Even though 3D mouse data are available from Google Earth, the 
results can be surprising when one paints past edges (left: view during painting, right: rotated). Before 
starting the actual painting, the user is asked to brush with the mouse across the interesting region 
of the building. We apply RANSAC to .nd up to three planes in the 3D data thus collected. This allows 
dealing, for instance, with typical corner points. After­ward, the user can select from three painting 
modes: Strokes can only be applied to the convex surface formed by the planes.  Strokes can only be 
applied to the surface formed by a sin­gle, selected plane, within the limits given by the intersections 
with the other planes.  Strokes are applied to a single, selected plane, but now are allowed on its 
entire area to support painting in mid-air.  The user may adjust the color, the opacity and the size 
of the brush. This is implemented by creating appropriate 3D polylines inside Google Earth. For .ne adjustment, 
the detected planes can be nudged in the direction or against the direction of their normal. This is 
helpful to correct planes that are offset due to our RANSAC­based algorithm getting stuck in recesses 
or to prevent z-.ghting artifacts between the 3D paint strokes and the buildings. *e-mail: joern.loviscach@fh-bielefeld.de 
 J¨orn Loviscach* Fachhochschule Bielefeld (University of Applied Sciences)  Figure 2: The user interface 
combines the 3D view (Google Earth plug-in) with drawing tools and view controls (HTML, JavaScript). 
 2 Implementation The front end of our system consists of a Web page containing the Google Earth plug-in 
accompanied by controls based on HTML, see Figure 2. The back end is formed by the Google Earth servers 
and by our own application server running Ruby on Rails to pro­vide the users annotations. Already existing 
annotations, in partic­ular those of other users, are inserted from our server into Google Earth s 3D 
scenes by providing KML .les. A client be it the standalone version or the Web-browser plug-in of Google 
Earth once receives a main KML .le containing all cur­rent annotations from our server. This KML .le 
periodically sends requests for updates to the server using a KML NetworkLink. An­other KML .le containing 
instructions on which drawings to add, update or delete is then served to and processed by the Google 
Earth client. The KML NetworkLinkControl technique for modifying el­ements in a KML document is employed 
here. Each request for updates contains the timestamp of the last suc­cessful update in order to only 
send annotations that are new or that have been changed in the meantime. Other users can access work 
in progress as soon as a user has sent his or her changes to the server. The access is possible with 
only a short delay due to the periodical requests for updates described before. 3 Conclusion and Outlook 
We presented a system that leverages existing 3D content despite its restricted availability. Graf.ti-like 
objects can be built with and integrated into standard Web technology. For architects and engineers distributed 
over a construction site, one can imagine a Web-based solution where everybody carries a mo­bile computer, 
uses this to paint markings and look at other persons inputs, and communicates with the others through 
a chat interface realized with Google Earth s placemark balloons. On the side of entertainment, one can 
imagine a voting system for graf.ti or a game that is based on different roles such as sprayer, janitor, 
and police. The set of drawing tools could be extended. En­tertainment applications may include effects 
such as color dripping and the automatic weathering of graf.ti with time. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599355</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<display_no>54</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Smart pen]]></title>
		<subtitle><![CDATA[new multimodal computer control tool for dyslexia therapy]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599355</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599355</url>
		<abstract>
			<par><![CDATA[<p>Dyslexia (dysgraphia) therapy is often boring for children and, what even worse, its results can be unsatisfactory. Hence, many therapists insist on development new methods which would be more interesting for young patients. The Smart Pen is such a tool. It is designed for supporting the therapy of developmental dyslexia, with particular regard to dysgraphia.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.4.2</cat_node>
				<descriptor>Assistive technologies for persons with disabilities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10010927.10003616</concept_id>
				<concept_desc>CCS->Social and professional topics->User characteristics->People with disabilities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003580.10003587</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing profession->Assistive technologies</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621514</person_id>
				<author_profile_id><![CDATA[81100277913]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrzej]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Czyzewski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gdansk University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621515</person_id>
				<author_profile_id><![CDATA[81100439783]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Piotr]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Odya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gdansk University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621516</person_id>
				<author_profile_id><![CDATA[81442618007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Agnieszka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grabkowska]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gdansk University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621517</person_id>
				<author_profile_id><![CDATA[81442616086]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grabkowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Gdansk University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621518</person_id>
				<author_profile_id><![CDATA[81100454798]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Bozena]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kostek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Physiology and Pathology of Hearing, Warsaw]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Smart Pen - new multimodal computer control tool for dyslexia therapy Andrzej Czyzewski1, Piotr Odya1, 
Agnieszka Grabkowska1, Michal Grabkowski1, Bozena Kostek1,2 1Gdansk University of Technology, Multimedia 
Systems Department, Narutowicza 11/12, 80-233 Gdansk, Poland 2Institute of Physiology and Pathology of 
Hearing, Warsaw 1ac@pg.gda.pl, 2bokostek@multimed.org 1 Introduction Dyslexia (dysgraphia) therapy is 
often boring for children and, what even worse, its results can be unsatisfactory. Hence, many therapists 
insist on development new methods which would be more interesting for young patients. The Smart Pen is 
such a tool. It is designed for supporting the therapy of developmental dyslexia, with particular regard 
to dysgraphia. 2 Interface description The emphasis was put on dysgraphia problems. The following assumptions 
were made at the planning stage: The interface should make use of a LCD tablet. In this way, it will 
be possible to measure the pressure put on the surface. It is necessary to obtain the correlation between 
a hand and an eye.  The pen, being the main part of the interface, should be able to monitor whether 
the pupil holds it properly, or not.  The software including a set of activities supporting a therapy 
of dyslexia, along with a database of pupils and therapy results.  2.1 Hardware part The main advantage 
of the Smart Pen is equipping the tablet pen with 3 pressure sensors. These sensors are mounted on a 
special grip which helps to encourage the recommended tripod finger position when holding a pen. The 
grip is made with plastic material that has properties comparable to a hard rubber. The grip is similar 
to the ordinary pen (or pencil) grips used for dyslexic children who often have poor pen grip. Signals 
from the sensors are translated to the PC format using additional converter (also developed in the framework 
of the project) equipped with processor with built-in A/D converters. As a result, the pen is connected 
to the PC via USB interface. The prototype of Smart Pen is presented in Fig 1. 2.2 Software part The 
main aim of the developed application was to allow a pupil to work under teacher or therapist supervision. 
The application allows continuous monitoring of different parameters related to writing. All these parameters 
are stored in a database and can be easily reviewed, e.g. to observe a therapy progress for each pupil. 
This is very useful especially from the therapist point of view. Furthermore, the Smart Pen software 
allows preparing new exercise scenarios and in this way closely matching pupils problems. 5 different 
activity types are possible to choose: drawing, squiggles, coloring, mazes, joining the dots. All of 
these activities are similar to suggested by dysgraphia therapists. For the assessment of whether the 
exercise was carried out by the child correctly, a set of parameters was established, e.g. pen grip and 
grips strength measurements, pen pressure on the tablet, time taken to complete an activity, duration 
of task completes, number of times the hand is lifted from the surface, and the amount of a user s drawing 
outside the perimeter. Monitoring these parameters gives an opportunity to indicate improper performing 
the activity. Furthermore, parameters from each session are stored in the database and can be easily 
recalled to assess the therapy progress. Fig. 1: Prototype of Smart Pen (coloring exercise)  3 Conclusions 
Tests showed that teachers and therapists are very keen on using the new tool for dysgraphia therapy. 
Children taking part in tests also expressed their interest in working with such a system. Furthermore, 
using the Smart Pen it is possible to distinguish children without motoric disruptions and those who 
may be affected by dyslexia/dysgraphia. It can be done by analyzing different parameters, such as the 
pen grip and the pressure put on the tablet. Children with graphomotorical problems more frequently obtained 
worse parameters in activities that required precise pen movements. The Smart Pen interface can be considered 
as a prototype of a new kind of a system that may be used to assist the therapy of children with graphomotorical 
disturbances. New features can be easily added and in this way the possibilities of the Smart Pen will 
be broaden. It is possible, for example. to implement some activities related to learning of mathematics. 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599356</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<display_no>55</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[TypeTile]]></title>
		<subtitle><![CDATA[a keyboard system that decorates characters depending on the way of typing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599356</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599356</url>
		<abstract>
			<par><![CDATA[<p>Generally, people use a keyboards as an interface for text input. However, unlike handwritten characters, typed characters are identical no matter who types them. To create variations in the appearance of typed characters, we usually decorate characters by changing fonts. For example, we change the font size, font color and boldness of the characters and the spacing between characters. However, due to these decorations, people must handle a few input processes such as checking the select menu. To improve current conditions, some research suggests methods of applying the user's unconscious actions while typing to decorate characters, using a laptop with a built-in acceleration sensor or body-worn electronic equipment [Iwasaki et al. 2009][Wang et al. 2004][TypeTrace 2006].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621519</person_id>
				<author_profile_id><![CDATA[81442593922]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621520</person_id>
				<author_profile_id><![CDATA[81335492333]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kensei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621521</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621522</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1520668</ref_obj_id>
				<ref_obj_pid>1520340</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Iwasaki, K., et al. 2009. Expressive typing: a new way to sense typing pressure and its applications, In <i>ACM CHI2009</i>, 4369--4374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>986016</ref_obj_id>
				<ref_obj_pid>985921</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wang, H., et al. 2004. Communicating emotions in online chat using physiological sensors and animated text, In <i>ACM CHI2004</i>, 1171--1174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dividual. 2006. TypeTrace, http://typetrace.jp/(As of May 1st, 2009)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401632</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kakehi, Y., et al. 2008. ForceTile: tabletop tangible interface with vision-based force distribution sensing, In <i>ACM SIGGRAPH New Tech Demos</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: TypeTile Figure 2: The force distribution Figure 3: Samples 1 Introduction Generally, people 
use a keyboards as an interface for text input. However, unlike handwritten characters, typed characters 
are iden­tical no matter who types them. To create variations in the ap­pearance of typed characters, 
we usually decorate characters by changing fonts. For example, we change the font size, font color and 
boldness of the characters and the spacing between characters. However, due to these decorations, people 
must handle a few in­put processes such as checking the select menu. To improve cur­rent conditions, 
some research suggests methods of applying the user s unconscious actions while typing to decorate characters, 
us­ing a laptop with a built-in acceleration sensor or body-worn elec­tronic equipment [Iwasaki et al. 
2009][Wang et al. 2004][TypeTrace 2006]. It is possible to collect a variety of information from the 
way one types. Among that information, we focus on the force distribution on the surface of the keys. 
In this paper, we propose a novel typing interface named TypeTile and its applications. This tile can 
dec­orate characters using the force distribution obtained from typing by using ForceTile [Kakehi et 
al. 2008]. Without any additional effort beyond typing, users can modify the typed characters thick­ness, 
darkness or size instantaneously. 2 A Proposal on TypeTile TypeTile can decorate characters using the 
force distribution ob­tained when one types. TypeTile can measure more information than general keyboard 
systems. Moreover, users can type and dec­orate characters without any body-worn electronic equipment. 
To meet these functions, we adopted the ForceTile system for sens­ing with TypeTile (Figure 1). This 
system can detect the force dis­tribution of each tile on the tabletop by using an infrared camera at­tached 
inside the table. In the TypeTile system, we have made tile­shaped interfaces with transparent bodies 
and markers and printed key areas on each surface according to the QWERTY layout. Thus, our system can 
detect the user s key inputs by measuring the dis­tribution of 3D force vectors applied on each key area. 
By typing words as inputs using our keyboard, decorative characters are seen as outputs on the monitor 
in real time. TypeTile can also recognize how people type from the measurement data. More concretely, 
it measures data used for the force distribution, such as the position *E-mail: tabletop@nae-lab.org 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008 (p), direction (f) and magnitude (|f|) of the forces applied on keys. 
The system then uses the measured values as parameters for deco­ration. Those parameters are as follows: 
f, |f|, t (duration time of pushing a key), ft (integral value of |f|), and d (distance between p and 
the central position of the key). Additionally, we can calculate average values of the magnitudes and 
directions of the force vectors applied on each key. For example, Figure 2 illustrates the average of 
vectors projected onto TypeTile surfaces. 3 Implementation and future works We have created some ways 
to decorate characters and show four examples as follows (see Figure 3), as well as the original undeco­rated 
characters by way of comparison (see Figure 3-0). Mapping |f|, t, and ft to characters size (Figure 3-1) 
 Mapping |f|, t, and ft to characters brightness (Figure 3-2) Mapping |f|, t, and ft to characters boldness 
(Figure 3-3) Mapping f to characters distortion (Figure 3-4)  In the future, we plan to optimize parameters 
for character decora­tion by testing TypeTile on users. Moreover, we will develop more applications for 
TypeTile that enable more .uent communication and apply our tile for media art or CSCW(Computer Supported 
Cooperative Work). References IWASAKI, K., et al. 2009. Expressive typing: a new way to sense typing 
pressure and its applications, In ACM CHI2009, 4369­4374. WANG, H., et al. 2004. Communicating emotions 
in online chat us­ing physiological sensors and animated text, In ACM CHI2004, 1171-1174. DIVIDUAL. 2006. 
TypeTrace, http://typetrace.jp/(As of May 1st, 2009) KAKEHI, Y., et al. 2008. ForceTile: tabletop tangible 
interface with vision-based force distribution sensing, In ACM SIGGRAPH New Tech Demos. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599357</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<display_no>56</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Utilizing photos as program themes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599357</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599357</url>
		<abstract>
			<par><![CDATA[<p>Due to the development in digital technologies, people now can easily retain their valuable memory by taking pictures through digital cameras. The cheap digital storage also encourages people to take lots of photos as they want. However, due to the tremendous amount of digital photos, it is not easy for people to browse all of them. Therefore, some techniques are proposed to help people to enjoy the photos, although it may be difficult for some people to arrange a time slot to watch them intentionally. Hence, in this extended abstract, we propose a system, which can utilize the large number of photos as the program themes (background), so that people will not notice the synthesized background while they are working, but the program themes may still be able to remind their good memory when they taking a short rest.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Screen design (e.g., text, graphics, color)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003123</concept_id>
				<concept_desc>CCS->Human-centered computing->Interaction design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621523</person_id>
				<author_profile_id><![CDATA[81416600159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kai-Yin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621524</person_id>
				<author_profile_id><![CDATA[81442598678]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ko-Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621525</person_id>
				<author_profile_id><![CDATA[81416594330]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sheng-Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621526</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>957094</ref_obj_id>
				<ref_obj_pid>957013</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ma, Y.-F., and Zhang, H.-J. 2003. Contrast-based image attention analysis by using fuzzy growing. In <i>ACM Multimedia 2003 Conference Proceedings</i>, 374--381.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141965</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rother, C., Bordeaux, L., Hamadi, Y., and Blake, A. 2006. Autocollage. <i>ACM Transactions on Graphics 25</i>, 3, 847--852. (SIGGRAPH 2006 Conference Proceedings).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Utilizing Photos As Program Themes Kai-YinCheng * Ko-YuanChou * Sheng-JieLuo * Bing-YuChen NationalTaiwanUniversity 
 Figure 1: Left: inactive status. Middle: mouse hover one ROI under active status. Right: click one 
selected ROI to view the whole picture. 1 Introduction Due to the development in digital technologies, 
people now can easilyretaintheirvaluablememoryby takingpicturesthroughdig­ital cameras. The cheap digital 
storage also encourages people to take lots of photos as they want. However, due to the tremendous amount 
of digital photos, it is not easy for people to browse all of them. Therefore, some techniques are proposed 
to help people to enjoy the photos, although it may be dif.cult for some people to arrange a time slot 
to watch them intentionally. Hence, in this extended abstract,weproposeasystem,which canutilize the large 
numberofphotosas theprogram themes(background),so thatpeo­plewillnotnotice thesynthesizedbackground while 
they arework­ing,but theprogram themesmay stillbeable toremind theirgood memory when they taking a short 
rest. 2 Design Principles In order to achieve this goal, some design principles are needed to be taken 
into account: 1) The synthesized background should not be salient and annoying that distracts people 
s attention while they are working. 2) The synthesized background should not interrupt users work actively. 
3) The synthesized background should obey the minimal aesthetics at least. 3 System Overview and Implementation 
 Figure 2: The overview of the system architecture. As shown in Figure 2, our system .rst analyzes the 
photos to ex­tracttheROIs[MaandZhang2003]. EachfoundROIis treated * e-mail:{keynes,koyuan,forestking}@cmlab.csie.ntu.edu.tw 
e-mail:robin@ntu.edu.tw asabasicelementandhas thechance tobecollaged[Rotheretal. 2006] with other ROIs 
to form a synthesized background image. The system then detects the program layout to determine the size 
of the background image to be generated. After synthesizing the ROIsas thebackgroundimage, thesystemdetectswhatareas 
in the program layout are occupied by active elements, such as icons or panels. Inorder tofollow thedesignprinciple(1), 
thesynthesized background should not cause any interference, the background ar­eas with active elements 
should have less texture and less lumi­nance. Tofollowthedesignprinciple(3), thesystemwillchange the 
synthesized background image s tone according to the color of the program s theme. The synthesized background 
image is static bydefault inorder tofollow thedesignprinciple(2)that thesynthe­sizedbackground shouldnot 
interrupt users work actively.Hence, if a user notices the background intentionally and wants to browse 
the image, he or she needs to activate the synthesized background imageby clicking afunctionalbuttononthe 
toolbar. Figure 1 shows the simulation result. The most left picture is one resultof thesynthesizedprogrambackground.Afteractivating 
the background image, the user can use the mouse cursor to hover the ROI images as shown in the middle 
picture. If the user selects one ROI image, the picture viewer will be popped to let him or her browse 
theoriginalpicturewhich contains theselectedROI image. 4 Conclusion and Future Work Usersareembracedbytheirvaluablememorywhile 
they arework­ing through the synthesized background images generated by our proposed system, which transforms 
the tremendous troublesome photos into program background themes. In the future, we hope to improve the 
usability of the interaction model and consider more aesthetics issues. References MA,Y.-F., AND ZHANG,H.-J.2003.Contrast-basedimageatten­tion 
analysisby usingfuzzygrowing. In ACM Multimedia 2003 Conference Proceedings,374 381. ROTHER, C., BORDEAUX, 
L., HAMADI, Y., AND BLAKE, A. 2006. Autocollage. ACM Transactions on Graphics 25,3, 847 852.(SIGGRAPH2006ConferenceProceedings). 
 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599358</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<display_no>57</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[Virtual stroboscope for robot motion design]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599358</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599358</url>
		<abstract>
			<par><![CDATA[<p>Recently, hobby robots such as pet robots and humanoid robots for entertainment are spreading and becoming more and more familiar each day. Compared to robots such as HONDA's ASIMO, a hobby robot is much cheaper, less rigid and has far less precision in measuring and controlling the angle of its own joints. For this reason, although we can assign joint angle to a key frame, assigned posture cannot be taken like computer graphics. Therefore, to ensure the robot moves as desired, we need to actually look at the robot operating while adjusting key frames respectively. By this, the margin of error in the joint angle and distortion in mechanism can be avoided. For CG animation, the animator observes the animation in real-time and when a problem is encountered, the problem in the key frame is corrected by slowing the animation down or by examining each frame. However, with robots, sudden stopping of ambulatory action makes the robot fall down. Moreover, the error in joint angle and distortion in mechanism are different between operation and geostationary state because of dynamic influences.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[humanoid robot]]></kw>
			<kw><![CDATA[motion design]]></kw>
			<kw><![CDATA[stop motion]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010553.10010554</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems->Robotics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010213.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Control methods->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010199.10010204</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Planning and scheduling->Robotic planning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621527</person_id>
				<author_profile_id><![CDATA[81333489343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tatsuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621528</person_id>
				<author_profile_id><![CDATA[81100270860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtual stroboscope for robot motion design Tatsuya Ishikawa* Shoichi Hasegawa The University of Electro-Communications 
 Keywords: humanoid robot, motion design, stop motion 1 Introduction Recently, hobby robots such as pet 
robots and humanoid robots for entertainment are spreading and becoming more and more familiar each day. 
Compared to robots such as HONDA s ASIMO, a hobby robot is much cheaper, less rigid and has far less 
precision in mea­suring and controlling the angle of its own joints. For this reason, although we can 
assign joint angle to a key frame, assigned posture cannot be taken like computer graphics. Therefore, 
to ensure the robot moves as desired, we need to actually look at the robot oper­ating while adjusting 
key frames respectively. By this, the margin of error in the joint angle and distortion in mechanism 
can be avoided. For CG animation, the animator observes the animation in real-time and when a problem 
is encountered, the problem in the key frame is corrected by slowing the animation down or by examining 
each frame. However, with robots, sudden stopping of ambulatory ac­tion makes the robot fall down. Moreover, 
the error in joint angle and distortion in mechanism are different between operation and geostationary 
state because of dynamic in.uences. 2 Innovation There is 2 main points in this proposition. First, 
by operating the robot in a periodic movement and synchronize capturing it with a camera, we can create 
a virtual stroboscope. By this, the user can stop the movement of the robot and inspects a particular 
key frame or forward a frame to check the movements of the robot. Secondly, to propose a motion editing 
interface that is easy to use similar to the ones in CG animation ones. The user looks on the Virtual 
Stro­boscope image and just by dragging on the legs or hands of robot, key frame editing can be accomplished. 
Then the edited key frame is then immediately re.ected onto the robot in the next motion cy­cle and the 
user can observe the image of the edited posture of the robot. The proposed system con.guration is shown 
in .gure 2. The system consists of a motion controller part (robot controller and treadmill) and Stroboscope 
part to present the user with the image synchronized to the cycle of the robots motion. The motion controller 
part makes the periodic motion of the robot in front of the camera. The wanted movements can be catego­rized 
into two kinds of motion, periodic motion such as walking and non-periodic motion such as reaching motion. 
By repeating a non-periodic motion, a motion can be converted into a periodic motion. To hold the robots 
in front of the camera, the movement of the robot has to be canceled. To cancel the translation motion 
of the robot, the system employs a treadmill. Because the cancellation mechanism for rotation must be 
bulky, we modify the motion of the robot slightly to cancel the rotation motion. The camera mea­sures 
the position and posture of the robot and treadmill so that the average potion and orientation of the 
robot is constant. *e-mail:ishikawa,hase@hi.mce.uec.ac.jp e-mail:hase@hi.mce.uec.ac.jp  Figure 2: Actual 
System  3 Vision Our proposed system makes motion design easy thus population of people who enjoy robots 
as hobby is expected to increase signi.­cantly. Moreover, when entertainment robot becomes an ordinary 
existence in every home, people will want their special one of a kind robot. At this point, many people 
want to easily design orig­inal motion of their robot by themselves. In general, there are two kinds 
of method to instruct a robot -agent like method where robots react to user s voice and, our proposed 
method where users spec­ify detailed motion of robot. For easiness and comfortableness, the agent like 
method is better. However, to design original, special, unique and detailed motion of one s robot, our 
proposed motion is de.nitely preferable. Our proposal will enable various robots to be more familiar 
in our daily lives and excite curiosity of people thus encourages creation of new unique forms of entertainment 
robot. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 
2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599359</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<display_no>58</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[3D reconstruction of planetary nebulae using hybrid models]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599359</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599359</url>
		<abstract>
			<par><![CDATA[<p>Distant astrophysical objects like planetary nebulae can normally only be observed from a single point of view, which makes deducing plausible 3D models a hard task that usually involves a lot of manual work [Nadeau et al. 2001]. However, additional physical assumptions can be used in order to estimate the missing depth information. In previous work [Wenger et al. 2009], a certain axial symmetry was assumed which is present in many planetary nebulae, so that tomographic methods could be used for the reconstruction. However, this assumption obviously fails for many of the most complex and interesting objects in question, and it only leads to unambiguous results as long as no absorption occurs within the nebula.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.2</cat_node>
				<descriptor>Astronomy</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010435</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Astronomy</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621529</person_id>
				<author_profile_id><![CDATA[81442603140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wenger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621530</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621531</person_id>
				<author_profile_id><![CDATA[81442598918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morisset]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad Nacional Aut&#243;noma de M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621532</person_id>
				<author_profile_id><![CDATA[81442616163]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steffen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad Nacional Aut&#243;noma de M&#233;xico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baranoski, G. V. G., and Rokne, J. G. 2006. Rendering plasma phenomena: Applications and challenges. In <i>Eurographics 2006 State of the Art Reports</i>, 63--87.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311602</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Matsuoka, S., and Tanaka, H. 1999. Teddy: a sketching interface for 3D freeform design. In <i>ACM SIGGRAPH 99</i>, 409--416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nadeau, D. R., Genetti, J. D., Napear, S., Pailthorpe, B., Emmart, C., Wesselak, E., and Davidson, D. 2001. Visualizing stars and emission nebulas. <i>Computer Graphics Forum 20</i>, 1, 27--33.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wenger, S., Aja Fern&#225;ndez, J., Morisset, C., and Magnor, M. 2009. Algebraic 3D reconstruction of planetary nebulae. In <i>WSCG '09</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Reconstruction of Planetary Nebulae using Hybrid Models Stephan Wenger*, Marcus Magnor Christophe 
Morisset , Wolfgang Steffen§ Computer Graphics Lab Instituto de Astronom´ia TU Braunschweig Universidad 
Nacional Aut´onoma de M´exico (a) (b) (c) (d) Figure 1: An observational image of the Saturn nebula 
(a), obtained by the Hubble Space Telescope, is used to reconstruct a three­dimensional model with volumetric 
and surface effects: (b) orthographic rendering from the original viewpoint, (c) perspective rendering 
from the same viewpoint, (d) perspective rendering from a different position. 1 Introduction Distant 
astrophysical objects like planetary nebulae can normally only be observed from a single point of view, 
which makes deduc­ing plausible 3D models a hard task that usually involves a lot of manual work [Nadeau 
et al. 2001]. However, additional physical assumptions can be used in order to estimate the missing depth 
in­formation. In previous work [Wenger et al. 2009], a certain axial symmetry was assumed which is present 
in many planetary nebu­lae, so that tomographic methods could be used for the reconstruc­tion. However, 
this assumption obviously fails for many of the most complex and interesting objects in question, and 
it only leads to un­ambiguous results as long as no absorption occurs within the neb­ula. Our new approach 
models the underlying physics of planetary neb­ulae more closely by making use of the fact that they 
often consist of nested gaseous clouds or shells which contain the remains of sub­sequent eruptions of 
a dying star. Within such a cloud, the volume is likely to homogeneously emit light due to recombination 
pro­cesses in the ionized gas, while the surfaces of each cloud may con­tain dust or debris that can 
either absorb radiation that was emitted elsewhere or be excited to emit additional radiation by themselves. 
While off-line renderings of such phenomena have been feasible in the past [Baranoski and Rokne 2006], 
our work deals with the prob­lem of creating suitable models using a minimum amount of user interaction, 
and of visualizing the results interactively. 2 Our Approach As a .rst step, the geometry of the different 
shells has to be speci­.ed. Since the identi.cation of a shell in a planetary nebula usually involves 
a lot of physical reasoning, this task cannot be entirely au­tomatized. However, software can assist 
the user in various ways in this initial step, depending on the amount of control the user *e-mail: wenger@cg.cs.tu-bs.de 
e-mail: magnor@cg.cs.tu-bs.de e-mail: morisset@astroscu.unam.mx §e-mail: wsteffen@astrosen.unam.mx wants 
to exercise. Either an entire mesh can be created using suit­able modeling software, or only the outlines 
of distinct objects are traced by the user, while our program automatically generates a smooth closed 
mesh satisfying the given border conditions, similar to some 3D drawing tools, e.g. [Igarashi et al. 
1999]. As soon as the geometry of the model is determined, the volume emission density of each mesh can 
be estimated so that the average squared difference between the volume-rendered model and the ob­served 
image is minimized. Then, the surface emission and absorp­tion layers have to be determined. Since this 
problem is inherently ambiguous, it is further constrained by demanding that each surface layer is self-consistent, 
that is, it has a unique texture that does not change throughout the layer. We achieve this by taking 
a sample of each surface layer in a region where no other layers are present and by extrapolating this 
sample into the rest of the layer using a texture synthesis method. The residual intensity can .nally 
be distributed among the layers without disturbing the visual continuity. The resulting models are rendered 
at interactive frame rates using a depth peeling algorithm for depth-sorting of surface layers and volume 
rendering techniques for the volumetric and surface effects (Figure 1). References BARANOSKI, G. V. 
G., AND ROKNE, J. G. 2006. Rendering plasma phenomena: Applications and challenges. In Eurograph­ics 
2006 State of the Art Reports, 63 87. IGARASHI, T., MATSUOKA, S., AND TANAKA, H. 1999. Teddy: a sketching 
interface for 3D freeform design. In ACM SIGGRAPH 99, 409 416. NADEAU, D. R., GENETTI, J. D., NAPEAR, 
S., PAILTHORPE, B., EMMART, C., WESSELAK, E., AND DAVIDSON, D. 2001. Vi­sualizing stars and emission 
nebulas. Computer Graphics Forum 20, 1, 27 33. WENGER, S., AJA FERN ´ ANDEZ, J., MORISSET, C., AND MAG-NOR, 
M. 2009. Algebraic 3D reconstruction of planetary nebu­lae. In WSCG 09. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599360</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<display_no>59</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[A natural smile synthesis from an artificial smile]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599360</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599360</url>
		<abstract>
			<par><![CDATA[<p>Everyone is interested in being more attractive. Leyvand et al have been proposed the method which can enhances of facial attractiveness into a photograph [Leyvand et al. 2008]. However, their method can't synthesize more attractive facial expression than that of an input face photograph. Meanwhile, amateur subjects' facial expressions often become unnatural when they act in front of the camera in experimental environments. At such a situation, a natural expression can be synthesized without performance skills.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621533</person_id>
				<author_profile_id><![CDATA[81442613302]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujishiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621534</person_id>
				<author_profile_id><![CDATA[81421595499]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takanori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621535</person_id>
				<author_profile_id><![CDATA[81538188156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shinya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621536</person_id>
				<author_profile_id><![CDATA[81442613614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621537</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARED, N., Reisfeld, D. 1994. Image Warping Using few Anchor Points and Radial Functions. <i>The Eurographics Association 1994</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360637</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[LEYVAND, T., et al. 2008. Data-Driven Enhancement of Facial Attractiveness. <i>ACM SIGGRAPH 2008</i>, 38:1--38:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187134</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[MORISHIMA, S., et al. 2005. Future Cast System. <i>SIGGRAPH 2005</i>, Sketch and Applications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Natural Smile Synthesis from an Artificial Smile Hiroki FUJISHIRO Takanori SUZUKI Shinya NAKANO Akinobu 
MEJIMA Shigeo MORISHIMA Waseda University 1 Introduction Everyone is interested in being more attractive. 
Leyvand et al have been proposed the method which can enhances of facial attractiveness into a photograph 
[Leyvand et al. 2008]. However, their method can t synthesize more attractive facial expression than 
that of an input face photograph. Meanwhile, amateur subjects facial expressions often become unnatural 
when they act in front of the camera in experimental environments. At such a situation, a natural expression 
can be synthesized without performance skills. In this paper, we present a method how to synthesize a 
natural and impressive smile automatically from an artificial and emotionless smile. At first, we analyze 
and quantify the difference between artificial smiles and natural smiles from video database. Especially 
we focus on the landmark points movement on front face changing from neutral to smile expression. We 
set landmark points originally around facial muscles that are tracked automatically by Optical Flow algorithm 
in the video sequence. Then, the quantified movements of each feature vector are categorized into an 
artificial smile and a natural smile by the Support Vector Machine (SVM). Second, a transformation vector 
from an averaged artificial smile to an averaged natural smile is defined. Finally, we can synthesize 
more natural and impressive smile by applying this transformation to an input smile photograph. 2 Smile 
classification An artificial smile and a natural smile of 35 Japanese female were recorded by HD CAM 
recorder (30frames/second). We captured subjects artificial smiles expressed by their own performance 
which is normally produced just in front of the camera in the studio, and natural smiles when they laughed 
unconsciously while watching a comedy video alone. Smiles classification is performed as follows. Firstly 
our system automatically set 87 Facial Feature Points (87FFP) (eyes, brows, nose, mouth, facial outline) 
in an initial frame around (neutral face) of captured video based on FSC system [Morishima et al 2005]. 
By using the face feature points, 220 Landmark Points (220LP) are decided around muscles to measure temporal 
movements shown in Fig. 1(a). Secondly, 220LP are automatically tracked by Optical Flow algorithm. If 
subject blinked in a video scene, we would modify landmark points of eyes, because the tracking performance 
becomes low. Thirdly, the size and the posture of the face and the movement of the whole head are normalized 
by the affine transform. After normalizations, the feature vector is defined as differences of coordinates 
of 220LP that shift from neutral face to smile. Finally, feature vector are classified into artificial 
smiles and natural smiles with the SVM. We used Leave-1-out method for open test. The classification 
accuracy is 67.1%. Supposing the feature of the upper part of the face is effective particularly for 
classification, we remove landmark points except for the points of the upper part into 220LP (Fig.1 (b)) 
and re-perform the same experiment. The accuracy is 80.0%. Based on those results, it is able to be shown 
that this feature vector is significant to distinguish artificial smiles and natural smiles. (a) (b) 
Figure 1: Landmark Points Definitions fj.hiro-85@fuji.waseda.jp 3 Smile synthesis Based on the result 
of smile classification, we found that feature vector is available to synthesize from artificial smiles 
to natural smiles. The photograph becomes more natural if it was transformed to the direction of natural 
smiles. Therefore, we extract the difference from Artificial Smiles Vector (ASV) average to Natural Smiles 
Vector (NSV) average and defined this difference vector (DV) as the direction vector to natural smiles. 
Figure 2(a), (b), (c) show ASV average, NSV average, and DV respectively. We synthesize a natural smile 
from input smile photograph as follows procedure. Firstly, given a smile photograph, 87FFP are set automatically. 
Secondly, we calculate Radial Basis Functions (RBSs)f. using 87FFP and 87 vertices of a generic face 
model corresponding to 87FFP. All vertices of the generic face model are mapped by RBFf. to fit the generic 
face model to input face [Ared et al 1994]. Thirdly, 220LP are set on the photograph, then calculate 
RBF f. with the correspondence between the feature vector extracted from the input smile photograph and 
the vector that DV is added to the feature vector. Finally, we deform the face model fitted to the input 
face by RBF f.. The result of our method is shown in Fig. 3. Users can also control the magnitude of 
the DV, control and direct weight of DV point by point. Fig.3 shows an example when we increase weight 
to the points around cheek. 4 Conclusion In this paper, we proposed a method of synthesizing a natural 
smile from an artificial smile photograph. We analyze difference between artificial smiles and natural 
smiles, and extract a vector from artificial to natural. Based on our method, we can synthesize the natural 
smile by being deformed based on those analyses. We plan to evaluate this system subjectively by questionnaire 
whether it looks natural, and to extend our method to animations.  Reference ARED, N., Reisfeld, D. 
1994. Image Warping Using few Anchor Points and Radial Functions. The Eurographics Association 1994. 
LEYVAND, T., et al. 2008. Data-Driven Enhancement of Facial Attractiveness. ACM SIGGRAPH 2008, 38:1-38:9. 
MORISHIMA, S., et al. 2005. Future Cast System. SIGGRAPH 2005, Sketch and Applications.  Figure 2: (left) 
artificial average, (center) natural average, (right) difference vector from artificial to natural (from 
Green to Red points) Figure 3: (left) Input facial image, (right) The naturalness enhancement result 
by our method Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599361</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<display_no>60</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>60</seq_no>
		<title><![CDATA[Accurate skin deformation model of forearm using MRI]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599361</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599361</url>
		<abstract>
			<par><![CDATA[<p>This paper presents a new methodology for constructing a skin deformation model using MRI and generating accurate skin deformations based on the model. Many methods to generate skin deformations have been proposed and they are classified into three main types. The first type is anatomically based modeling. Anatomically accurate deformations can be reconstructed but computation time is long and controlling generated motion is difficult. In addition, modeling whole body is very difficult. The second is skeleton-subspace deformation (SSD). SSD is easy to implement and fast to compute so it is the most common technique today. However, accurate skin deformations can't be easily realized with SSD. The last type consists of data-driven approaches including example-based methods. In order to construct our model from MRI images, we employ an example-based method. Using examples obtained from medical images, skin deformations can be modeled related to skeleton motions. Retargeting generated motions to other characters is generally difficult with this kind of methods. Kurihara and Miyata realize accurate skin deformations from CT images [Kurihara et al. 2004], but it doesn't mention the possibility of retargeting. With our model, however, generated deformations can be retargeted. Once the model is constructed, accurate skin deformations are easily generated applying our model to a skin mesh. In our experiment, we construct a skin deformation model which reconstructs pronosupination, rotational movement of forearm, and we use range scan data as a skin mesh to apply our model and generate accurate skin deformations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621538</person_id>
				<author_profile_id><![CDATA[81442600865]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621539</person_id>
				<author_profile_id><![CDATA[81447596181]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shinsuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621540</person_id>
				<author_profile_id><![CDATA[81442600573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shota]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621541</person_id>
				<author_profile_id><![CDATA[81335499894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Akane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621542</person_id>
				<author_profile_id><![CDATA[81100474115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shiraishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621543</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kass, M., Witkin, A., and Terzopoulos, D. 1988. "Snakes: Active Contour Models", International Journal of Computer Vision, 1, 321--331]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028571</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kurihara, T., and Miyata, N. 2004. "Modeling Deformable Human Hands from Medical Images", ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 355--363]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Accurate Skin Deformation Model of Forearm Using MRI Kentaro Yamanaka. Shinsuke Nakamura Shota Kobayashi 
Waseda University 1 Introduction This paper presents a new methodology for constructing a skin deformation 
model using MRI and generating accurate skin de­formations based on the model. Many methods to generate 
skin deformations have been proposed and they are classified into three main types. The first type is 
anatomically based modeling. Anatomically accurate deformations can be reconstructed but computation 
time is long and controlling generated motion is difficult. In addition, modeling whole body is very 
difficult. The second is skeleton-subspace deformation (SSD). SSD is easy to implement and fast to compute 
so it is the most common tech­nique today. However, accurate skin deformations can t be easily realized 
with SSD. The last type consists of data-driven ap­proaches including example-based methods. In order 
to construct our model from MRI images, we employ an example-based method. Using examples obtained from 
medical images, skin deformations can be modeled related to skeleton motions. Retar­geting generated 
motions to other characters is generally difficult with this kind of methods. Kurihara and Miyata realize 
accurate skin deformations from CT images [Kurihara et al. 2004], but it doesn t mention the possibility 
of retargeting. With our model, however, generated deformations can be retargeted. Once the model is 
constructed, accurate skin deformations are easily gener­ated applying our model to a skin mesh. In our 
experiment, we construct a skin deformation model which reconstructs pronosu­pination, rotational movement 
of forearm, and we use range scan data as a skin mesh to apply our model and generate accurate skin deformations. 
 2 Image Acquisition One healthy adult male subject underwent the MRI scanning. MRI scans containing 
thin axial slices were obtained. The slice thick­ness was 5 [mm] and the pixel size was 1.0*1.0 [mm]. 
We meas­ured a right arm and forearm with three postures, pronated, supi­nated, and intermediate position 
(Figure 1). The subject was at­tached markers while the scanning. Markers were also attached while range 
scanning was performing. 3 Modeling We model skin deformations on those of contours of forearm ex­tracted 
from MRI images. In our experiment, the contours are extracted by means of snake [Kass et al. 1988]. 
Several feature points are set on MRI images. Using these feature points, a local coordinate system and 
rotation angle . are defined. The local coordinate system normalizes postures as a common criterion and 
rotation of upper arm is also normalized with this coordinate sys­tem. Because . is rotation angle by 
which the radius spins around the head of the ulna, it is an indicator of relative motion of radius and 
ulna. The radius is the forearm bone of the hand and it runs along the thumb side of the forearm. By 
contrast, the ulna is the forearm bone of the elbow and it runs along the little finger side. After normalization, 
each set of contours of an example pos­ture is related to . . Deformations are linearly interpolated 
with . for pronation and supination phase respectively. For this in- .email: kentaro.05-27@suou.waseda.jp 
.email: shigeo@waseda.jp Akane Yano Masashi Shiraishi Shigeo Morishima. Sony Corporation Figure 1: Generated 
Pronosupination (left: supinated position, meddle: intermediate position, right: pronated position) terpolation, 
registration within several examples is estimated from movement of markers attached while MRI scanning. 
We define this interpolated deformation of contours as the skin deformation model. Accurate skin deformations 
are realized applying this model to forearms of characters. 4 Generating Skin Deformations Fitting our 
model to a skin mesh is based on the markers attached while MRI and range scanning. Then, accurate skin 
deformations are generated by means of RBF: N (1) v'=.{Wk f(xk - v )} k=1 where N is a number of vertices 
on our model, v is vertex posi­tion on a skin mesh, v' is deformed vertex position and xk is the position 
of vertex k on our model at an intermediate position. We use f= x2 +s 2 as the basis function. s is the 
minimum distance between the vertex k and another vertex on our model. In order to calculate a weight 
Wk, we solve Equation (1) where the position of vertex i on our model at an intermediate position ( xi 
) and at an arbitrary position ( x' i ) are used instead of v and v' respectively. The result is shown 
in Figure 1. 5 Conclusion In this paper, a method for constructing a skin deformation model from MRI 
images and generating accurate skin deformations with the model was explained. We show the capability 
of our model by generating highly realistic motions using range scan data not only of the subject who 
underwent MRI scanning but of two more male as shown in a video. However, generating accurate pronosupina­tion 
of females or children may be difficult with this model be­cause we use MRI images obtained from one 
male subject. In order to construct a universal skin deformation model, it is neces­sary to increase 
data. Modeling accuracy should be improved to realize more accurate skin deformations. References KASS, 
M., WITKIN, A., AND TERZOPOULOS, D. 1988. Snakes: Active Contour Models , International Journal of Computer 
Vision, 1,321-331 KURIHARA, T., AND MIYATA, N. 2004. Modeling Deformable Human Hands from Medical Images 
, ACM SIG­GRAPH/Eurographics Symposium on Computer Animation, 355-363 Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599362</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<display_no>61</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>61</seq_no>
		<title><![CDATA[Aging model of human face by averaging geometry and filtering texture in database]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599362</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599362</url>
		<abstract>
			<par><![CDATA[<p>The presence of CG character is essential in anime and movie contents recently. Especially, personality and aging factor are also important in character modeling. However, the modeling CG character is based on hand-made process yet, so it costs huge amount of money and labor to give one character several variations. About a facial animation, muscle based process or blend shape based process is very popular in contents production, however, in case of considering aging mechanism on face skin and bone, the different model of each age has to be constructed for every character.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621544</person_id>
				<author_profile_id><![CDATA[81442598650]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Satoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kasai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621545</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1522772</ref_obj_id>
				<ref_obj_pid>1522756</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Maejima, A., Wemler, S., Machida, T., Takebayashi, M., and Morishima, S. 2008. Instant casting movie theater: the future cast system. In The IEICE Transaction on Information and System, Vol. E91-D, No. 4, 1135--1148.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Aging model of human face by averaging geometry and filtering texture in database Satoko KASAI* Shigeo 
MORISHIMA. Waseda University 1. Introduction The presence of CG character is essential in anime and 
movie contents recently. Especially, personality and aging factor are also important in character modeling.However, 
the modeling CG character is based on hand-made process yet, so it costs huge amount of moneyand labor 
to give one character several variations. About a facial animation, muscle based process or blend shape 
based process is very popular in contents production, however, in case of considering aging mechanism 
on faceskin and bone, the different model of each age has to beconstructed for every character. In this 
paper, we propose a new modeling method of facial bone geometry and facial skin texture in each age based 
on the analysis of real human face database. Typical aging feature of facial bone and skin is modeledby 
averaging and filtering huge amount of face models. And it is possible to make one character younger 
or older by mixing original texture with typical age model.  2. Age Modeling We have 600 face database 
with 3D range scan dataand frontal texture with age and gender information.This 3D range scan data is 
fitted and normalized with ageneric face model by 89 automatically detected landmarks and their RBF based 
interpolation [1]. Typicalage feature is modeled by averaging the face geometry and filtering skin texture. 
The averaging geometry process is performed for each 3D vertex of normalized face model without considering 
gender. In case of texture, the averaging pixel values is not good for imaging because the detail blurred 
and aging feature like wrinkles or spots disappeared. So we introduce a new filtering method of each 
pixel by sorting and ranking luminance of images in database, then choosing an optimum order for each 
age and replace with RGB values of the order. Specially, in case the order is just the intermediate number, 
it corresponds to Medianfilter. This process is performed for each gender. As a result, in spite of same 
face geometry modeled for each age, gender feature is appeared evidently in synthesizedface texture, 
typically in a part of eye, eyebrow and corner of mouth (Fig. 1). Fig 1: Filtering texture in database 
for age modeling *skasai@kurenai..waseda.jp .shigeo@waseda.jp Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  3. Aging Control 
When one snap shot of face and his/her ageinformation are given, his/her face of future and past can 
be estimated by our method as follows. After analysis offrontal face image, 89 feature points are estimatedautomatically 
and a generic face model is fitted onto this face by RBF based interpolation. When target age is given, 
averaged face geometry for given age is constructed and the same filtering method described in previous 
section is performed after mixing original texture with textures selected by the target age from database 
by controlling mixing weight. By optimizing mixing weight, you can control the influence of age factor 
and personal factor. Fig.2 shows aging result of anoriginally 5 years old boy with mixing rate of 50%. 
The estimated future faces in 21-25 years old, 31-35 years oldand 41-45 years old are shown in (a), (b) 
and (c) respectively. The personality and aging feature is wellexpressed in the synthesized images. 
 Fig 2: An example of aging result from snap shot  4. Result and Conclusion A robust face aging modeling 
method is proposed. Thefuture and past face can be estimated with current snapshot and age information. 
In our method, not only facegeometry but also face texture are well synthesizedwithout any blurring and 
losing personality. In future, we have to model the personal geometry change in aging and perform an 
evaluation of this system by comparingestimated face with a real personal photo album from baby to now. 
 Acknowledgement This research is supported by Japan Science and Technology Agency,CREST project. Reference 
[1] MAEJIMA, A.,WEMLER, S., MACHIDA, T., TAKEBAYASHI, M., AND MORISHIMA,S. 2008. Instant casting movie 
theater:the future cast system. In The IEICE Transaction on Information and System, Vol.E91-D, No.4, 
1135 1148.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599363</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<display_no>62</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>62</seq_no>
		<title><![CDATA[City generator]]></title>
		<subtitle><![CDATA[GIS driven genetic evolution in urban simulation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599363</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599363</url>
		<abstract>
			<par><![CDATA[<p>This poster presents a range of urban simulation techniques and multidisciplinary research across Architecture design, urban design, interactive design and game design, as well as visual effects. Through a series of design experiments using <i>City Generator</i>, the 3D urban simulation tool, I explored an integrated set of design methods, such as genetic computation, and real time simulation with game engine. The complexity of 3D urban form is procedurally controlled by 2D GIS (Geography Information System) data input and digital elevation model (DEM), which allows designers to efficiently shape urban growth in a preferred direction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Visual</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.2.8</cat_node>
				<descriptor>Spatial databases and GIS</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003236</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Spatial-temporal systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003145.10003147.10010887</concept_id>
				<concept_desc>CCS->Human-centered computing->Visualization->Visualization application domains->Geographic visualization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010365</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Visual analytics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621546</person_id>
				<author_profile_id><![CDATA[81442609898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Savannah College of Art and Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 City Generator: GIS driven genetic evolution in urban simulation Ming Tang Savannah College of Art and 
Design mtang@scad.edu Abstract This poster presents a range of urban simulation techniques and multidisciplinary 
research across Architecture design, urban design, interactive design and game design, as well as visual 
effects. Through a series of design experiments using City Generator, the 3D urban simulation tool, I 
explored an integrated set of design methods, such as genetic computation, and real time simulation with 
game engine. The complexity of 3D urban form is procedurally controlled by 2D GIS (Geography Information 
System) data input and digital elevation model (DEM), which allows designers to efficiently shape urban 
growth in a preferred direction. 1 Introduction In the past five years, GIS program has been taught at 
SCAD to introduce students the use of computers in the assessment and representation of environmental 
landscape. Data collection, assessment and synthesis are incorporated as components of the urban design 
courses. Usually, 2D diagrams and GIS maps are generated at the end of spatial data capture, management, 
manipulation, and analysis process. These 2D images are often used during the early design stage, rather 
than being used as instruments for designers to study, experience, and revise their design in various 
design phases within a 3D environment. Taking advantage of the powerful Genetic Evolution (GE) method, 
this poster describes an alternative and practical approach to use 2D GIS maps for instant 3D urban modeling, 
with urban design students as target audiences. Therefore, traditional 2D images, diagrams and maps can 
be directly associated with the 3D form of a city. It allows designers to explore their design by using 
both 2D and 3D instruments in all design phases. Students can generate instant 3D urban models through 
very simple operations in the computer by creating 2D urban diagrams or manipulating GIS maps. The computation 
tool, City Generator, automatically constructs a 3D city based on the logic of 2D blue print. Besides 
its application in urban design, this method can also be applied in the fields of visual effects, game 
design or city planning for its effectiveness of generating large city models. 2 Our Approach 2.1 Phase 
I: building generation In the first phase, City Generator is used to breed selected architectural models 
across several generations. This process is not related with any urban context and there are no GIS inputs 
involved. The goal is to produce various building models from selected architectural types. 2.2 Phase 
II: automatic placement in the urban context In the second phase, City Generator uses 2D maps to generate 
3D proxy city model. These 2D maps contain GIS data, which reflect zoning, population, transportation, 
and other spatial information. They can be manipulated directly in an image-editing program such as Photoshop 
to represent an urban designer s master plan. City Generator can read the 2D information embedded in 
GIS maps based on the RGBA value of each pixel and execute corresponding modeling operation. In another 
word, during the automatic construction, the resultant 3D city model can inherit all the geographical, 
social, demographic and design information from the 2D GIS maps. 2.3 Phase III: substitute proxy by 
procedural buildingmodels In the final phase, each voxel in the proxy model of Phase II is substituted 
by a detailed procedural building model generated in Phase I. The substitution rule is defined by the 
designer according to design criteria. Copyright is held by the author / owner(s). SIGGRAPH 2009, New 
Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599364</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<display_no>63</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>63</seq_no>
		<title><![CDATA[Defining and computing multi-dimensional skeletons]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599364</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599364</url>
		<abstract>
			<par><![CDATA[<p>Describing shapes is an important task in graphics and vision. A simple, concise descriptor that captures the essential shape properties of an object would greatly facilitate computer-based understanding of the object and applications such as matching and segmentation. For this reason, medial axes (MA) has become a popular shape descriptor since its introduction by Blum [Blum 1967]. The MA of an <i>N</i>-D object is an (<i>N</i> -- 1)-D geometry centered within the object. For example, the MA of a 2D object consists of medial curves at elongated parts, whereas the MA of a 3D object consists of medial surfaces describing the protrusions on the object.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621547</person_id>
				<author_profile_id><![CDATA[81335493704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621548</person_id>
				<author_profile_id><![CDATA[81100098726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blum, H. 1967. A transformation for extracting new descriptors of shape. <i>Models for the Perception of Speech and Visual Form</i>, 362--380.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281975</ref_obj_id>
				<ref_obj_pid>1281957</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dey, T. K., and Sun, J. 2006. Defining and computing curve-skeletons with medial geodesic function. In <i>SGP '06: Proceedings of the fourth Eurographics symposium on Geometry processing</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 143--152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 De.ning and computing multi-dimensional skeletons Lu Liu* Tao Ju Washington University in St. Louis 
 1 Introduction Describing shapes is an important task in graphics and vision. A simple, concise descriptor 
that captures the essential shape prop­erties of an object would greatly facilitate computer-based under­standing 
of the object and applications such as matching and seg­mentation. For this reason, medial axes (MA) 
has become a popular shape descriptor since its introduction by Blum [Blum 1967]. The MA of an N-D object 
is an (N - 1)-D geometry centered within the object. For example, the MA of a 2D object consists of medial 
curves at elongated parts, whereas the MA of a 3D object consists of medial surfaces describing the protrusions 
on the object. Often times, the shape of an object may be better described using medial structures at 
even lower dimensions than the MA. For exam­ple, consider the 3D hand model in Figure 1 (a). The palm 
portion has a plate-like shape and can be naturally described by the MA, which consists of medial surfaces. 
The .ngers, on the other hand, have tube-like shapes and would be more appropriately represented by medial 
curves. In the latter case, the MA may not be the desir­able shape descriptor. The goal of our work is 
to construct a medial structure, or a skele­ton, that describes a shape using medial geometry at multiple, 
ap­propriate dimensions. For example, the desirable skeleton of the hand model in Figure 1 (a) would 
consist of medial curves at the .ngers and a medial surface at the palm. So far, only limited ef­fort 
[Dey and Sun 2006] has been made to de.ne medial geometry at lower dimensions than that of MA, and the 
question of what is the right dimensionality of medial geometry to use has hardly been addressed. In 
this paper, we propose for the .rst time a uni­.ed approach of de.ning medial geometry at different dimensions 
and evaluating their suitability for shape description. Based on the de.nition, we describe a discrete 
distance transform that approxi­mates the medial geometry at various dimensions, which leads to a simple 
and robust algorithm for constructing a multi-dimensional skeleton.  2 Method De.ning medial geometry 
Our de.nition of medial geometry extends the grass.re .ow de.nition of MA to lower dimensions. Given 
an object in an N-D space, its k-D medial geometry is re­cursively de.ned as the quench points of a geodesic 
grass.re that burns inward from the boundary of the (k + 1)-D medial geometry, where the N-D medial geometry 
is the object itself. The lower­dimensional medial geometry shares a number of key properties of the 
MA, including being thin, centered within the object, and ho­motopy equivalent to the object. The grass.re 
de.nition naturally offers a way to evaluate whether the dimension of a medial geometry is suitable for 
describing the shape. Intuitively, the arrival time of the geodesic grass.re at a point p on the k-D 
medial geometry re.ects the maximum isotropic elongation of the object in k directions at p. The suitability 
of using the k-D medial geometry at p can be observed by how much larger this elongation is over the 
isotropic elongation in k + 1 directions. *e-mail: ll10@cse.wustl.edu e-mail: taoju@cse.wustl.edu Figure 
1: A hand model (a), the discrete medial surfaces (b) and curves (c) where faces (edges) with higher 
suitability measures are rendered with larger sizes (lengths) and redder color, and the multi­dimensional 
skeleton (d). Computing skeletons A classical way of discretely approximat­ing the MA, following the 
grass.re analogy, is performing dis­tance transform on grid points. To compute the medial geometry at 
various dimensions de.ned by geodesic grass.re, we use a cell­complex grid that explicitly represents 
the grid elements at different dimensions (e.g., points, edges, faces, etc.), and perform a geodesic 
distance transform that operates on grid elements at speci.c dimen­sions. The discretely approximated 
medial geometry maintains the properties of the continuous counterpart, such as being thin, cen­tered 
and homotopy-preserving. As an example, Figure 1 (b,c) show respectively the discrete me­dial surfaces 
and curves of the hand model rendered by the suit­ability measure. Note that the measures capture the 
overall shape properties well (e.g., .ngers receive low measures on the medial surfaces). Given user-speci.ed 
criteria for the suitability measures at each dimension, we compute, in a single thinning pass of the 
object, a composite skeleton consisting of medial geometry at mul­tiple dimensions that satisfy the criteria. 
The resulting skeleton is homotopy equivalent to the object and robust to irregularity on ob­ject boundary 
(e.g., Figure 1 (d)). References BLUM, H. 1967. A transformation for extracting new descriptors of shape. 
Models for the Perception of Speech and Visual Form, 362 380. DEY, T. K., AND SUN, J. 2006. De.ning and 
computing curve­skeletons with medial geodesic function. In SGP 06: Proceed­ings of the fourth Eurographics 
symposium on Geometry pro­cessing, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 
143 152. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 
7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599365</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<display_no>64</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>64</seq_no>
		<title><![CDATA[Dynamic simplicial meshes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599365</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599365</url>
		<abstract>
			<par><![CDATA[<p>Dynamic simplicial meshes will enable real time unconstrained deformation of triangular and tetrahedral meshes regardless of topological limitations, by means of combining a quality measure with complex topological operations and smoothing techniques, which automatically improve and maintain the quality of the mesh, aiming at mesh and geometry processing applications.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[dynamic meshes]]></kw>
			<kw><![CDATA[mesh optimization]]></kw>
			<kw><![CDATA[quality measure]]></kw>
			<kw><![CDATA[topology processing]]></kw>
			<kw><![CDATA[unconstrained deformation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Hierarchy and geometric transformations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010244</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Hierarchical representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621549</person_id>
				<author_profile_id><![CDATA[81442608514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[Pena]]></middle_name>
				<last_name><![CDATA[Serna]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer-IGD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621550</person_id>
				<author_profile_id><![CDATA[81100152008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stork]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer-IGD TU-Darmstadt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dynamic Simplicial Meshes Sebastian Pena Serna* Andre Stork Fraunhofer-IGD Fraunhofer-IGD TU-Darmstadt 
 Figure 1: Behavior of the neighboring-based quality measure (mesh surface) for a tetahedral sphere 
with 20 tetrahedra (black wireframe) under different conditions: (a) compressed sphere, (b) perfect sphere 
and (c) stretched sphere. Abstract Dynamic simplicial meshes will enable real time unconstrained de­formation 
of triangular and tetrahedral meshes regardless of topo­logical limitations, by means of combining a 
quality measure with complex topological operations and smoothing techniques, which automatically improve 
and maintain the quality of the mesh, aim­ing at mesh and geometry processing applications. Keywords: 
dynamic meshes, mesh optimization, quality measure, unconstrained deformation, topology processing. 1 
Introduction Dynamic simplicial meshes (DSM) is a research on progress mo­tivated by the limitations, 
which several mesh and geometry pro­cessing applications face, when large deformations are applied to 
the mesh, because of the decrement of the mesh quality, which leads to poor quanti.cations of simplex 
properties and to instabili­ties in linear systems. Many algorithms developed by the computer graphics 
community aiming at animating and transforming meshes such as shape deformation, smoothing and morphing, 
avoid chang­ing the mesh topology, because of the higher complexity; however topology processing could 
help to potentiate and unlock these algo­rithms. 2 Technical Approach DSM pro.ts from a novel neighboringbased 
quality measure, which computes the quality of 1-ring vertices, capturing a better under­standing of 
the local mesh con.guration. This quality measure eval­uates the condition of the 1-ring vertex neighborhood 
by means of computing for every involved simplex the deviation between the di­rection of the gradient 
vector and the direction of the mean vector. These deviations are averaged, providing a value in the 
range [0, 1], *e-mail: sebastian.pena.serna@igd.fraunhofer.de andre.stork@igd.fraunhofer.de which quanti.es 
the quality of the 1-ring vertex. In addition of cap­turing a better understanding of the local mesh 
con.guration; the 1-ring quality measure also provides clues on the ill-conditioning of the 1-ring neighborhood, 
which is tackled by improving its inter­polating condition and therefore the gradient of the vertex regard­ing 
the 1-ring neighborhood, leading to a low condition number of the linear system and therefore accuracy 
in the solution. In order to improve the interpolating condition of the 1-ring vertex, regard­ing the 
neighboring vertices, DMS applies an optimization proce­dure, which combines complex topological operations 
and smooth­ing techniques, aiming at generating optimal local con.gurations. The optimal con.gurations 
are found by averaging the height di­rection of the affected simplices, regarding the 1-ring vertex. 
The resulting direction suggests an in.nite line, where an optimal con­.guration can be found for the 
ill-conditioned area, such that the 1­ring-vertex can improve its gradient. The possible con.gurations 
are found by either: i) modifying the neighborhood (by displacing the 1-ring vertex over the line), ii) 
creating a new neighborhood (by adding a new vertex over the line) or iii) eliminating a neighbor­hood 
(by collapsing the 1-ring vertex to a vertex in the neighbor­hood). The evaluation of the possible con.gurations 
is based on an objective function, which maximizes the 1-ring quality for the set of prede.ned con.gurations. 
 3 Conclusion This methodology aims at automatically improving and maintain­ing the quality of the mesh 
during large deformations, providing a real time algorithm, which can contribute to overcome the topo­logical 
limitations, faced by several mesh and geometry processing applications, where the success and accuracy 
rely on the linear sys­tems and therefore on the quality of the mesh. This research is partially supported 
by the European projects 3D-COFORM (FP7­ICT-2007.4.3-231809) and FOKUS K3D (FP7-ICT-2007-214993). Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599366</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<display_no>65</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>65</seq_no>
		<title><![CDATA[Aerial scene synthesis from images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599366</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599366</url>
		<abstract>
			<par><![CDATA[<p>Automated synthetic terrain and architecture generation is now becoming feasible with calibrated camera remote sensing. This poster implements computer vision techniques that have recently become popular to extract "structure from motion" (SfM) of a calibrated camera with respect to a target. This process will build off of Microsoft's popular "PhotoSynth" technique and apply it to geographic scenes imaged from an airborne platform. Additionally, it will be augmented with new features to increase the fidelity of the 3D structure for realistic scene modeling. This includes the generation of both sparse and dense point clouds useful for synthetic macro/micro-scene reconstruction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621551</person_id>
				<author_profile_id><![CDATA[81442617093]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nilosek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621552</person_id>
				<author_profile_id><![CDATA[81440619274]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Karl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Walli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>861369</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hartley, R., and Zisserman, A. 2003. <i>Multiple view geometry in computer vision</i>. Cambridge Univ Pr.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lourakis, M., and Argyros, A. 2004. The design and implementation of a generic sparse bundle adjustment software package based on the levenberg-marquardt algorithm. <i>ICS/FORTH Technical Report TR 340</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851523</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lowe, D. 1999. Object recognition from local scale-invariant features. In <i>International Conference on Computer Vision</i>, vol. 2, Corfu, Greece, 1150--1157.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wolf, P., and Dewitt, B. 1983. <i>Elements of photogrammetry</i>. McGraw-Hill Singapore.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Aerial Scene Synthesis from Images David Nilosek* Karl Walli, Lt Col, USAF Rochester Institute of Technology 
AFIT/CI Carlson Center for Imaging Science Rochester Institute of Technology Carlson Center for Imaging 
Science Figure 1: Scenes are reconstruction on both a macro and micro scale. The macro scale reconstruction 
is used as the basis for the micro scale reconstruction. Scene reconstructions are geographically accurate 
due to the calibrated aerial cameras 1 Overview Automated synthetic terrain and architecture generation 
is now becoming feasible with calibrated camera remote sensing. This poster implements computer vision 
techniques that have recently become popular to extract structure from motion (SfM) of a calibrated camera 
with respect to a target. This process will build off of Microsoft s popular PhotoSynth technique and 
apply it to geographic scenes imaged from an airborne plat­form. Additionally, it will be augmented with 
new features to increase the .delity of the 3D structure for realistic scene modeling. This includes 
the generation of both sparse and dense point clouds useful for synthetic macro/micro-scene reconstruction. 
Although, computer vision has been an active area of re­search for decades, it has recently experienced 
a renaissance due to a few signi.cant breakthroughs. This poster will review the developments in mathematical 
formalism, robust automated point extraction, and ef.cient sparse matrix algorithm implementation that 
have fomented the capability to retrieve 3D structure from multiple aerial images of the same target 
and apply it to geographi­cal scene modeling. Scenes are reconstructed on both a macro and a micro scale. 
The macro scene reconstruction implements the scale invariant feature transform to establish initial 
correspondence, then extracts a scene coordinate estimate using photogrammetric techniques [Wolf, P, 
and Dewitt, B. 1983.]. The estimates along with calibrated camera information are fed through a sparse 
bundle adjustment to extract re.ned scene coordinates [Lourakis,M, and Argyros, A. 2004.]. The micro 
scale reconstruction uses a denser correspondence done on speci.c targets using the epipolar geometry 
derived in the macro method. The seeds of computer vision were actually planted by pho­ *e-mail: drn2369@cis.rit.edu 
e-mail:kcw9016@cis.rit.edu togrammetrists over 40 years ago, through the development of space resectioning 
and bundle adjustment techniques. But it is only the parallel breakthroughs, in the previously mentioned 
areas that have .nally allowed the dream of rudimentary computer vision to be ful.lled in an ef.cient 
and robust fashion. Both areas will bene.t from the application of these advancements to geographical 
synthetic scene modeling. The proposed poster will explore this application in a process the authors 
refer to as Aerial Scene Synthesis (AeroSynth). References HARTLEY, R., AND ZISSERMAN, A. 2003. Multiple 
view geome­try in computer vision. Cambridge Univ Pr. LOURAKIS, M., AND ARGYROS, A. 2004. The design 
and imple­mentation of a generic sparse bundle adjustment software pack­age based on the levenberg-marquardt 
algorithm. ICS/FORTH Technical Report TR 340. LOWE, D. 1999. Object recognition from local scale-invariant 
fea­tures. In International Conference on Computer Vision, vol. 2, Corfu, Greece, 1150 1157. WOLF, P., 
AND DEWITT, B. 1983. Elements of photogrammetry. McGraw-Hill Singapore. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599367</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<display_no>66</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>66</seq_no>
		<title><![CDATA[Inca reconstruction using shape grammar]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599367</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599367</url>
		<abstract>
			<par><![CDATA[<p>M&#252;ller et al. introduced CGA shape, a shape grammar for procedural modeling of architecture [M&#252;ller et al. 2006b], which they applied to Mayan archaeological site in Xkipch&#233; [M&#252;ller et al. 2006a]. Inspired by this application of shape grammars to archaeology, we built a simple reconstruction application that uses a shape grammar to build 3D Inca sites from 2D plans. The application can help users to create a quick 3D overview of an Inca site that they are studying.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621553</person_id>
				<author_profile_id><![CDATA[81442610396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jingyuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621554</person_id>
				<author_profile_id><![CDATA[81100013633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621555</person_id>
				<author_profile_id><![CDATA[81442603072]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cowan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2384323</ref_obj_id>
				<ref_obj_pid>2384301</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, P., Vereenooghe, T., Wonka, P., Paap, I., and Gool, L. V. 2006. Procedural 3d reconstruction of Puuc buildings in Xkipch&#233;. In <i>Eurographics Symposium on Virtual Reality, Archaeology and Cultural Heritage (VAST)</i>, EG, 139--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141931</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, P., Wonka, P., Haegler, S., Ulmer, A., and Gool, L. V. 2006. Procedural modeling of buildings. <i>ACM Trans. Graph. 25</i>, 3, 614--623.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Inca Reconstruction Using Shape Grammar JingyuanHuang * StephenMann Bill Cowan UniversityofWaterloo 
 (a) Userinterface (b) Arandomlygenerated model (c) Anotherrandomlygenerated model Figure 1: 1(a): manual 
modeling; 1(b) 1(c): random modeling 1 Introduction M¨uller et al. introducedCGA shape, a shapegrammarforprocedu­ral 
modeling of architecture[M¨uller et al.2006b], whichthey ap­pliedtoMayan archaeologicalsiteinXkipch´e[M¨uller 
et al.2006a]. Inspired by this application of shape grammars to archaeology, we builta simple reconstruction 
application that uses a shapegrammar tobuild3DInca sitesfrom2Dplans. The application canhelp users to 
create aquick3D overview of anInca site that they are studying. 2 Our System Unlike CGA shape, which 
uses a complex production process to achievedetailed reconstructionsfor a variety of architectural styles, 
our application uses only grammar rules necessary to reconstruct groups of Inca buildings. There are 
several key elements in Inca architecture: houses, walls, and terraces. Each element is handled differently. 
We constructed a comprehensive set of rulesforhouses to let users have full control of their appearance. 
Several rules are shownbelow,using notations similartotheonesusedbyM¨ulleret al. For Inca walls we only 
provide users with controls for transla­tions and rotationsbecause theirlocations among agroup ofhouses 
and terraces are more important than other details. 1 House .T(x, 0, z)Facades T(0, height, 0)Roof 2 
Facades .FrontFacade |BackFacade |SideFacade 3 FrontFacade : num front door=0 .pillar 4 FrontFacade .Facade 
: 0.9 5 SideFacade .TriangularWall 6 SideFacade .Facade 7 Facade .Repeat( x ){wall} 8 Facade .Subdiv( 
x , 1r, total win width, 1r) {wall, Repeat( x ){window, door}, wall} ... ... We extracted a small set 
of rulesfor terracesfrom several siteplans we studied, of which some rules are shown below. The rules 
allow user to explore the shapegrammar s ability togenerate newdesigns * e-mail:j23huang@cgl.uwaterloo.ca 
e-mail:smann@cgl.uwaterloo.ca e-mail:wmcowan@cgl.uwaterloo.ca for modeling architecture. Using these 
rules, our application is ca­pable ofgenerating new virtuallayouts ofInca sites. 1 Plan .T(x, 0, z)LeftTerraces 
T(x+ter width, stair height, z) Stairs T(x+ter width+stair width, 0, z)RightTerraces : 0.5 2 LeftTerraces.Repeat( 
z ){T(0, terrace height, 0)terrace} 3 RightTerraces .Repeat( z )]{TerraceWithHouse} 4 TerraceWithHouse.T(0, 
terrace height, 0)Repeat( z ){terrace, House} Site plans describe complex layouts of Inca towns and 
cities. In­stead of processing a site plan automatically, the application inter­prets the plan with the 
assistance of the user, who chooses the de­sired elementsto model(houses,Inca walls, orterraces) by select­ing 
a modes in the application. In each mode, the user selects the lines that represent a house, a wall, 
or terraces. The software then processes the inputs to create structures based on the shape gram­mar 
rules. After the models are created, the user canfurther modify their appearance by adjusting the corresponding 
parameters. 3 Conclusions We used the system togenerate3DInca site modelsfrom2Dplans. We also created 
novel layouts similar to existing site plans. From our experience with this project we concluded that 
even for very simple shape grammars the effort required to produce the .rst re­sults is greater than 
using traditional modeling tools. With a com­plete system, however, users inexperienced with shape grammars 
and Inca architecture can construct complex layouts with ease. Its capability of creating new sensible 
designs without users domain knowledgeis abigadvantage over traditional modeling techniques. References 
M¨ ULLER, P., VEREENOOGHE, T., WONKA, P., PAAP, I., AND GOOL, L. V. 2006. Procedural3d reconstruction 
ofPuucbuild­ingsinXkipch´e. In Eurographics Symposium on Virtual Reality, Archaeology and Cultural Heritage 
(VAST),EG,139 146. M¨ ULLER, P., WONKA, P., HAEGLER, S., ULMER, A., AND GOOL, L. V. 2006. Procedural 
modeling of buildings. ACM Trans. Graph. 25,3,614 623. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599368</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<display_no>67</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>67</seq_no>
		<title><![CDATA[Lace curtain: modeling and rendering of woven structures using BRDF/BTDF]]></title>
		<subtitle><![CDATA[production of a catalog of curtain animations]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599368</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599368</url>
		<abstract>
			<par><![CDATA[<p>The need for rendering woven fabrics arises frequently in computer graphics[N Adabala, N Magnenat-Thalmann, G Fei 2003]. Woven fabrics have a specific appearance, luster, and transparency. We have proposed a BRDF/BTDF model using the Henyey-Greenstein function and an algorithm for the real-time rendering of woven fabrics based on the texture look-up table[Uno et al. 2008]. However, in order to make the model more accurate, the microlevel BRDF/BTDF is necessary. The objective of this study is to express a more detailed texture of the cloth by creating a 3D model that especially takes into consideration the "twisted structure" of the yarn in the fine woven structure of the cloth and by making the rendering algorithm more precise.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Visual</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010365</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Visual analytics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621556</person_id>
				<author_profile_id><![CDATA[81421599981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizushima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621557</person_id>
				<author_profile_id><![CDATA[81442612264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shuhei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nomura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621558</person_id>
				<author_profile_id><![CDATA[81442615864]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Genki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Umeizumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621559</person_id>
				<author_profile_id><![CDATA[81100522455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Noriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621560</person_id>
				<author_profile_id><![CDATA[81319500817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yoshiyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Fashion Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882430</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[N Adabala, N Magnenat-Thalmann, G Fei. 2003. Visualization of woven cloth. <i>EGRW '03: Proceedings of the 14th Eurographics workshop on Rendering</i>, 178--185.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401007</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Uno, H., Mizushima, Y., Nagata, N., and Sakaguchi, Y. 2008. Lace curtain: measurement of BTDF and rendering of woven cloth:-production of a catalog of curtain animations. In <i>International Conference on Computer Graphics and Interactive Techniques</i>, ACM SIGGRAPH New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lace Curtain: Modeling and Rendering of Woven Structures using BRDF/BTDF Production of a catalog of 
curtain animations Yoshiki Mizushima Shuhei Nomura Genki Umeizumi Kwansei Gakuin University Kwansei Gakuin 
University Kwansei Gakuin University of the rendering based on the BTDF model is presented in Figure4. 
 Noriko Nagata* Kwansei Gakuin University  1 Introduction The need for rendering woven fabrics arises 
frequently in computer graphics[N Adabala, N Magnenat-Thalmann, G Fei 2003]. Woven fabrics have a speci.c 
appearance, luster, and transparency. We have proposed a BRDF/BTDF model using the Henyey-Greenstein 
function and an algorithm for the real-time rendering of woven fab­rics based on the texture look-up 
table[Uno et al. 2008]. How­ever, in order to make the model more accurate, the microlevel BRDF/BTDF 
is necessary. The objective of this study is to ex­press a more detailed texture of the cloth by creating 
a 3D model that especially takes into consideration the twisted structure of the yarn in the .ne woven 
structure of the cloth and by making the rendering algorithm more precise. 2 Measurement of Woven Cloth 
We measured the BTF for culculating the BRDF/BTDF of the wo­ven fabrics by using a BRDF instrument (OGM-3, 
DFL), which consists of a .xed digital camera with a microlens, a movable light source (metal halide), 
and a movable sample plate. 1092 images per cloth were measured by repositioning the lamp. As shown in 
Figure1, we made an observation that yarn has a twisted structure. .30. , .45. .60. , .55. .150. , .45. 
Figure 1: The Twisted Structure of Yarn in the Woven Cloth  3 Recreation of the Woven Structure of 
Cloth Here, we propose a 3D model of the woven structure of the cloth that takes into consideration the 
twisting of the yarn. 3.1 Parameters of the Woven Structure We extracted the intersections using thinning 
processing from the acquired BTF data . Furthermore, from the vertical and horizontal direction histograms, 
we determined the .uctuations of the vertical and horizontal threads and calculated the thickness of 
and the space between each thread. 3.2 Parameters of the Twisted Structure We assumed a basic model 
for yarn in which the cross-section is el­liptical and its structure has a twist direction (right twist, 
left twist) and a twist length. We separated the vertical threads and horizontal threads from the binary 
image and decided the height and width of the ellipse based on the .uctuations of thickness in the vertical 
and horizontal threads. Next, we used an autocorrelation function to calculate the length of the twist 
and determined the direction of the twist of the yarn from the vector direction of the highlighted areas. 
From the height and width of the ellipse and the length of the twist, we hypothesized that the threads 
intersect without being distorted and calculated the slope of the yarn.  3.3 Generation of a 3D Model 
based on Woven Struc­tures and Twisted Structures The recreated woven structure of the cloth is shown 
in Figure3(b). Comparing this to the image of the actual cloth (Figure3(a)), we *e-mail:nagata@kwansei.ac.jp 
Yoshiyuki Sakaguchi Digital Fashion Ltd. model of the twist used in this paper is appropriate. (a) Photo 
  4 cise rendering by using Maya Figure 4: Results of the Rendering  5 Conclusions We proposed recreating 
a 3D model that especially takes into con­sideration the twisted structure of yarn in the .ne woven structure 
of the cloth and making the rendering algorithm more precise. Our goal is to generate a catalog of curtain 
animations that can express various types of woven fabrics under arbitrary light conditions. References 
N ADABALA, N MAGNENAT-THALMANN, G FEI. 2003. Visu­alization of woven cloth. EGRW 03: Proceedings of the 
14th Eurographics workshop on Rendering, 178 185. UNO, H., MIZUSHIMA, Y., NAGATA, N., AND SAKAGUCHI, 
Y. 2008. Lace curtain: measurement of BTDF and rendering of woven cloth:-production of a catalog of curtain 
animations. In International Conference on Computer Graphics and Interactive Techniques, ACM SIGGRAPH 
New York, NY, USA. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599369</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<display_no>68</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>68</seq_no>
		<title><![CDATA[Numerical simulation of fluid flow on complex geometries using the Lattice-Boltzmann method and CUDA-enabled GPUs]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599369</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599369</url>
		<abstract>
			<par><![CDATA[<p>Within computational fluid dynamics (CFD) the Navier-Stokes (NS) equations are traditionally used to describe the physical properties of the fluid. An alternative approach to classical discretizations for the numerical solution of the Navier-Stokes equations, such as Finite-Difference and Finite-Volume schemes, is provided by the Lattice-Boltzmann equations [Benzi et al. 1992], [Chen and G. 1998]. The Lattice-Boltzmann method (LBM) uses a Cartesian grid for propagating and relaxing a discrete velocity distribution function on a lattice at discrete time steps. Usually a very large number of cells is necessary to obtain an accurate prediction of the macroscopic scales for pressure and velocity. However, due to the simple formulation of the underlying algorithm this method is well suited for parallelization and hardware acceleration using general purpose graphical processing units (GPGPU). LBM is used in engineering software for example to compute the aerodynamic drag of a car to improve its efficiency. Therefore LBM has a big practical importance. Improving the performance of a CFD simulation gives the engineers more time and better feedback during the engineering process leading to more efficient engineering processes and more efficient engineering products. Moreover a strong acceleration in simulation performance makes a new quality of physical simulation technology available for desktop computer software like entertainment and content creation software.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>G.1.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003727.10003729</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Differential equations->Partial differential equations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621561</person_id>
				<author_profile_id><![CDATA[81442617561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[E.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Riegel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Munich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621562</person_id>
				<author_profile_id><![CDATA[81442614189]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Indinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Munich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621563</person_id>
				<author_profile_id><![CDATA[81100312786]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[N.]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Adams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Munich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Benzi, R., Succi, S., and Vergassola, M. 1992. The lattice-boltzmann equation: theory and applications. <i>Phys. Rep. 222</i>, 145--197.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chen, S., and G. D. D. 1998. Lattice boltzmann method for fluid flows. <i>Annu. Rev. Fluid Mech. 30</i>, 329--364.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Li, W., Wei, X., and Kaufman, A. 2003. Implementing lattice boltzmann computation on graphics hardware. <i>The Visual Computer 19</i>, 444--456.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1356147</ref_obj_id>
				<ref_obj_pid>1356131</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zhao, Y. 2008. Lattice boltzmann based pde solver on the gpu. <i>Visual Computing 24</i>, 323--333.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Numerical Simulation of Fluid Flow on complex Geometries using the Lattice-Boltzmann Method and CUDA-enabled 
GPUs E. Riegel* T. Indinger N. A. Adams Technical University of Munich Institute of Aerodynamics 1 
Overview Within computational .uid dynamics (CFD) the Navier-Stokes (NS) equations are traditionally 
used to describe thephysical prop­erties of the .uid. An alternative approach to classical discretiza­tions 
for the numerical solution of the Navier-Stokes equations, such as Finite-Difference and Finite-Volume 
schemes, is provided by the Lattice-Boltzmann equations [Benzi et al. 1992], [Chen and G. 1998]. The 
Lattice-Boltzmann method (LBM) uses a Cartesian grid for propagating and relaxing a discrete velocity 
distribution function on a lattice at discrete time steps. Usually a very large number of cells is necessary 
to obtain an accurate prediction of the macroscopic scales for pressure and velocity. However, due to 
the simple formulation of the underlying algorithm this method is well suited for parallelization and 
hardware acceleration using general purpose graphical processing units (GPGPU). LBM is used in en­gineering 
software for example to compute the aerodynamic drag of a car to improve its ef.ciency. Therefore LBM 
has a big prac­tical importance. Improving the performance of a CFD simulation gives the engineers more 
time and better feedback during the en­gineering process leading to more ef.cient engineering processes 
and more ef.cient engineering products. Moreover a strong accel­eration in simulation performance makes 
a new quality of physical simulation technologyavailable for desktop computer software like entertainment 
and content creation software. Similarly to previous work ([Li et al. 2003], [Zhao 2008]) in a stu­dent 
s research project a LBM algorithm has been implemented for computations on GPUs rather than the conventional 
way us­ing CPUs. In contrast to the existing work this one implemented a D3Q15 LBMkernel using the nVidia 
CUDAtechnology focus­ing on accurate and physically valid simulation results. Moreover domain decomposition 
capabilities allow usage of multiple GPU boards for performance and data space extension. Grid re.nement 
techniques, being subjectof ongoingwork, areexpectedto improve simulation sef.ciency. Also an additional 
D3Q15 CPUkernel has been written to allow proper LBM performance comparison be­tween CPU and GPU architectures. 
As a tool for better debugging, interaction and presentation capabilities the software has been en­hanced 
by an online interactive OpenGL 3D-visualisation showing .ow s current state. To obtain an ef.cient CUDAimplementation 
all simulation data is stored inside the GPU memory of up to 4GB providing data space for a sub domain 
consisting of 400x400x400 cells. Shared GPU memoryisusedasadata cache reducingthe amount non-coalesced 
memory accesses to global GPU memory. Domain decomposition is solved by exporting simulation data stored 
on the boundary area of a sub domain, processing it with data from other sub domains and reimporting 
it to its former location. When using a sub domain of 400x400x400 cells this results in 52MB of data 
to be exchanged between the GPU boards per time step. *e-mail: eugen.riegel@mytum.de e-mail: thomas.indinger@tum.de 
 Figure 1: Flow around a motorcycle At current stage the software is capable to perform a physically 
validated simulation on a domain consisting of 914x457x457 cells distributed among three Tesla C1060 
GPU boards .lled with ex­tremely complex static geometry allowing a Reynolds number of 4800. This corresponds 
for example to the .ow around a cyclist riding at 20 km/h or 13 mp/h. The overall computational perfor­mance 
of the three GPU boards is around 191·106 cells per second. On an AMD Phenom X4 2.6GHz the CPU implementation 
achieves 8.42·106 cellsper second resultinginaremarkable speedup around 22.6x for the three GPU boards. 
A single Tesla C1060 board achieves 78 · 106 cells per second when running without domain decomposition. 
This results in a GPU memory bandwidth of 10GB per second (one cell consumes 64bytesofdata).Howevertheusablememory 
bandwidthofaTesla C1060 board is over 100GB per second. Of course, it is probably not possible to utilize 
all of the available bandwidth with a more complex algorithm like LBM. Nevertheless it seems that some 
ma­jor improvements on the GPUkernel performance are still possible, which are investigated by current 
work together with grid re.ne­ment techniques. References BENZI,R.,SUCCI,S., AND VERGASSOLA,M. 1992. 
The lattice­boltzmann equation: theory and applications. Phys. Rep. 222, 145 197. CHEN, S., AND G. D. 
D. 1998. Lattice boltzmann method for .uid .ows. Annu. Rev. Fluid Mech. 30, 329 364. LI, W., WEI, X., 
AND KAUFMAN, A. 2003. Implementing lat­tice boltzmann computation on graphics hardware. The Visual Computer 
19, 444 456. ZHAO, Y. 2008. Lattice boltzmann based pde solver on the gpu. Visual Computing 24, 323 333. 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599370</article_id>
		<sort_key>690</sort_key>
		<display_label>Article No.</display_label>
		<display_no>69</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>69</seq_no>
		<title><![CDATA[Representing 3D mesh with attributed root trees]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599370</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599370</url>
		<abstract>
			<par><![CDATA[<p>In content-based 3D mesh retrieval, graph-based structure is one of the most important shape descriptors. The early work on this issue can be found in [Hilaga et al. 2001], in which a 3D mesh representation, Multiresolutional Reeb Graphs (MRGs), was proposed. Since then, many Reeb-Graph based descriptors have been designed to simplify and represent 3D meshes. However, most of those descriptors are only suitable for graph-based topology matching, which is time-consuming and unreliable. To address this issue, we propose a novel approach to representing 3D mesh by using a tree-like structure, Attributed Root Trees (ARTs). The advantages of our method are three-fold: (1) Tree matching is easier and efficient than graph matching; (2) The representation well reserves topological information of a 3D mesh, thus topology matching can be easily completed; (3) It is possible to perform multi-resolution matching on ARTs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>F.2.2</cat_node>
				<descriptor>Pattern matching</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.2.2</cat_node>
				<descriptor>Trees</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10003634</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Trees</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10010031.10010032</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Data structures design and analysis->Pattern matching</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621564</person_id>
				<author_profile_id><![CDATA[81323497648]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yu-Bin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nanjing University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621565</person_id>
				<author_profile_id><![CDATA[81375608281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jin-Jie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nanjing University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383282</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hilaga, M., Shinagawa, Y., Kohmura, T., and Kunii, T. L. 2001. Topology matching for fully automatic similarity estimation of 3d shapes. In <i>Proceedings of ACM SIGGRAPH 2001</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 (a)(b)(c)(d)(e)(f) Figure 1: Examples of the proposed tree-based mesh representation. The tree nodes 
are shown as red balls, representing the centroids of the connected components generated in the construction 
process of the Attributed Root Trees (ARTs). If a connected component is fully contained in another component, 
a link will be drawn to connect their centroids. The red balls with larger size represent the centroids 
of the initial connected components, i.e., the root nodes of trees. In content-based 3D mesh retrieval, 
graph-based structure is one of the most important shape descriptors. The early work on this issue can 
be found in [Hilaga et al. 2001], in which a 3D mesh representation , Multiresolutional Reeb Graphs (MRGs), 
was pro­posed. Since then, many Reeb-Graph based descriptors have been designed to simplify and represent 
3D meshes. However, most of those descriptors are only suitable for graph-based topology match­ing, which 
is time-consuming and unreliable. To address this issue, we propose a novel approach to representing 
3D mesh by using a tree-like structure, Attributed Root Trees (ARTs). The advantages of our method are 
three-fold: (1) Tree matching is easier and ef­.cient than graph matching; (2) The representation well 
reserves topological information of a 3D mesh, thus topology matching can be easily completed; (3) It 
is possible to perform multi-resolution matching on ARTs. The construction of the ARTs of a 3D mesh is 
described as follows. Step I. De.ne an appropriate function µ : M . R to compute a value for each vertex 
on the surface of the 3D mesh M. Find the maximum value max and minimum value min, and then max-min calculate 
the interval value as: . = L . Here the parameter L is used for controlling granularity in the ART construction 
process. Afterwards, we are able to generate L thresholds by accumulating min with different number of 
.s: min + ., min +2 ×.,..., and min + L ×.. Step II. For each threshold .i = min + i ·. (i =1,...,L), 
ver­tices of which the value is not larger than .i are chosen to build a connected component, and thus 
construct a Reeb graph Ri. The positions of tree nodes are located at the centroids of those vertices. 
Note that when i<j, the vertices selected by using .i are also selected by using .j . Step III. Let i 
= L. In this case, all vertices of the 3D mesh M are chosen to generate a Reeb graph RL because all of 
their values are smaller than .L. Therefore, if all vertices of M are connected, RL will be an ART with 
only one root node. Otherwise, we will achieve a forest of ARTs. Step IV. A node na in ART Ri is set 
as a child of the node nb in ART Ri+1, if the connected component generated by using na is fully contained 
in the one generated by using nb . Step V. Repeat Step IV until i =1. Figure 2: An example of ARTs construction. 
(a) Using the height value as the output of function µ, and set L =4. (b) Regions below the threshold 
min + . are selected and distributed into two connected components, represented by the their centroids, 
n1 and n2. (c)-(e) are Reeb graphs constructed for thresholds min +2 × .,min +3 ×. and min +4 ×. respectively. 
(f) Because the region represented by node n1 is fully contained in the region represented by n3, n1 
is set as a child node of n3. Similarly, n3 is set as a child node of n5, and n5 is set as a child node 
of n7, etc. Consequently, the generated ARTs, the root nodes represent the whole 3D mesh while the leaf 
nodes represent regions that are much smaller but with high saliency. Therefore, an ARTs-based struc­ture 
stores both global and local geometry properties in its root nodes and leaf nodes so that multi-resolution 
matching is able to be conducted with the same tree representation. Moreover, the ARTs representation 
is also bene.cial to mesh segmentation be­cause branches of the ARTs are useful hints for segmenting 
the whole mesh into different regions. Acknowledgments. This work is supported by the National Natural 
Science Foundation of P. R. China (Grants 60875011, 60723003, 60505008), and the Natural Science Foundation 
of Jiangsu Province, P. R. China (Grant BK2007520). References HILAGA, M., SHINAGAWA, Y., KOHMURA, T., 
AND KUNII, T. L. 2001. Topology matching for fully automatic similarity estima­tion of 3d shapes. In 
Proceedings of ACM SIGGRAPH 2001. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Natural Science Foundation of P. R. China</funding_agency>
			<grant_numbers>
				<grant_number>608750116072300360505008</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>Natural Science Foundation of Jiangsu Province, P. R. China</funding_agency>
			<grant_numbers>
				<grant_number>BK2007520</grant_number>
			</grant_numbers>
		</article_sponsors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599371</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<display_no>70</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>70</seq_no>
		<title><![CDATA[User studies on the feasibility of oblique contouring]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599371</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599371</url>
		<abstract>
			<par><![CDATA[<p>MRI and CT scanners have long been used to produce three-dimensional samplings of anatomy elements for use in medical visualization and analysis. Physicians often need to construct surfaces representing the anatomical shape in order to conduct treatment, such as radiating a tumor. Traditionally, this is done by a time-consuming process in which an experienced physician marks a series of parallel contours that outline the object of interest.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621566</person_id>
				<author_profile_id><![CDATA[81331504821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ross]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sowell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621567</person_id>
				<author_profile_id><![CDATA[81335493704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621568</person_id>
				<author_profile_id><![CDATA[81100098726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ju]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621569</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University in St. Louis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621570</person_id>
				<author_profile_id><![CDATA[81430672887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abraham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621571</person_id>
				<author_profile_id><![CDATA[81440618466]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Garima]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gokhroo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621572</person_id>
				<author_profile_id><![CDATA[81335494246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Low]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Liu, L., Bajaj, C., Deasy, J., Low, D., and Ju, T. 2008. Surface reconstruction from non-parallel curve networks. <i>Eurographics (Computer Graphics Forum) 27</i>, 2, 155--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 User Studies on the Feasibility of Oblique Contouring Ross Sowell, Lu Liu, Tao Ju, Cindy Grimm Christopher 
Abraham, Garima Gokhroo, Daniel Low Washington University in St. Louis * Washington University School 
of Medicine 1 Introduction MRI and CT scanners have long been used to produce three­dimensional samplings 
of anatomy elements for use in medical vi­sualization and analysis. Physicians often need to construct 
sur­faces representing the anatomical shape in order to conduct treat­ment, such as radiating a tumor. 
Traditionally, this is done by a time-consuming process in which an experienced physician marks a series 
of parallel contours that outline the object of interest. The recent work of [Liu et al. 2008] provides 
an algorithm for re­constructing a surface from contours drawn on non-parallel planes that could greatly 
reduce the manual component of this process (see Figure 1). However, current medical imaging systems 
do not pro­vide tools for sketching contours on oblique planes. In this paper, we present and evaluate 
an interface (see accompanying video) that allows the user to generate a surface from just a few contours 
drawn Figure 1: Surface models of the brain stem, reconstructed from (a) 27 parallel contours (b) 6 
oblique contours. Red dots represent portions of the benchmark that are not well represented by the surface. 
The use of oblique contours yielded a more accurate reconstruction with the use of fewer contours. 2 
Experiment To examine the effectiveness of our interface we are conducting two user studies. In each 
case, the participant is given a ten minute in­troduction to the interface, and then given .ve minutes 
to practice using the interface before the experiment begins. The .rst study examines the ability of 
the user to accurately and consistently draw contours on predetermined oblique planes as compared to 
the paral­lel case. The second study asks the user to .rst identify the central axis of a structure, 
and then generates the contouring planes based on this choice. 2.1 Contouring on Predetermined Planes 
In this study we had .ve physicians contour the same ten patient datasets twice, once using a predetermined 
set of parallel planes and *e-mail: {rsowell, ll10, taoju, cmg}@cse.wustl.edu e-mail: {cabraham, ggokhroo, 
dlow}@radonc.wustl.edu once using a predetermined set of oblique planes. Of the ten cases, .ve were prostate 
cases and .ve were brain stem cases. Twenty­seven planes were used for the parallel trials and four planes 
were used for the oblique trials. The parallel planes were chosen so as to match those currently used 
in clinical practice, and the oblique planes were chosen so as to accurately capture the shape of the 
structure with as few planes as possible. The physicians all used identical workstations and contouring 
tools. The order of the ten datasets was randomized, and the order of the parallel and oblique trials 
were alternated. Preliminary results show that for contours of similar length and passing through areas 
with similar gradient magnitudes, users are just as fast and consistent in the oblique cases as they 
are in the parallel cases. Each set of contours will also be used to generate a surface mesh. This mesh 
will be compared against a benchmark generated from retrospectively sampled clinical contours. We hy­pothesize 
that the surface meshes reconstructed from a combination of four oblique contours and three parallel 
contours will be just as accurate, if not more accurate, than those reconstructed from all twenty-seven 
parallel contours. This serves as a proof of concept that oblique contouring has the potential to be 
faster and more ac­curate than parallel contouring. 2.2 Identifying a Central Axis In the .rst user 
study, we .xed the position of the oblique planes so that each user contoured on exactly the same planes. 
In the second user study, we will have the user identify the central axis of the structure, and then 
generate the oblique planes based on this initial axis. This will allow us to compare the entire process 
from start to .nish in both the oblique and parallel cases. We will present .ve more physicians with 
ten different patient datasets. Of the ten cases, .ve are the same prostate cases as used in the previous 
study and .ve are cervical spine cases. The cervical spine is an interesting case for this study because 
it is not aligned to the imaging axis. For each oblique trial, the physician will be asked to navigate 
the image plane from its initial position in the center of the dataset to a target view of the long axis 
of the structure of interest. The physician will be asked to mark a dot at the front and end of the structure 
in this view, and the seven planes for the oblique cases will be generated based on this axis. The process 
for the parallel trials will be just the same as in the previous study. This allows us to measure several 
things. First, we can measure how ef.cient and accurate the users were at moving the image plane to the 
target view of the structure, and which of the inter­face tools were most effective for this task. We 
can also build sur­faces from each of the trials and measure their accuracy compared to the benchmark 
and the total time to contour for both parallel and oblique cases. Finally, we can compare the oblique 
trials across dif­ferent users, to see how much the choice of the initial axis affected the resulting 
surface model.  References LIU, L., BAJAJ, C., DEASY, J., LOW, D., AND JU, T. 2008. Surface reconstruction 
from non-parallel curve networks. Euro­graphics (Computer Graphics Forum) 27, 2, 155 164. Copyright is 
held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599372</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<display_no>71</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>71</seq_no>
		<title><![CDATA[A visual beat detection system for grid-based interactive percussion and synchronization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599372</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599372</url>
		<abstract>
			<par><![CDATA[<p>This document introduces a conceptually novel, simple and ordinarily robust computer-vision-based method of extracting musical beats from regular physical gestures of a performer, implemented in VisiBeat: a grid-based percussion system on the Max/MSP/Jitter platform for collaborative interactive music.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[MSP]]></kw>
			<kw><![CDATA[beat]]></kw>
			<kw><![CDATA[collaboration]]></kw>
			<kw><![CDATA[computer vision]]></kw>
			<kw><![CDATA[cv.jit]]></kw>
			<kw><![CDATA[grid]]></kw>
			<kw><![CDATA[horn]]></kw>
			<kw><![CDATA[interaction]]></kw>
			<kw><![CDATA[jitter]]></kw>
			<kw><![CDATA[max]]></kw>
			<kw><![CDATA[music]]></kw>
			<kw><![CDATA[optical flow]]></kw>
			<kw><![CDATA[physical gestures]]></kw>
			<kw><![CDATA[schunk]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621573</person_id>
				<author_profile_id><![CDATA[81414619250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Trishul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mallikarjuna]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Singer E., "Cyclops", Web Resource: http://www.ericsinger.com/cyclopsmax.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Winkler T., "Motion-Sensing Music: Artistic and Technical Challenges in Two Works for Dance", ICMC 1998]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1085221</ref_obj_id>
				<ref_obj_pid>1085171</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Winkler T., "Fusing Movement, Sound, and Video in Falling Up, an Interactive Dance/Theatre Production", NIME 2002]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253779</ref_obj_id>
				<ref_obj_pid>253607</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Singer E., Castiglia C., Liao S., Perlin K., "Real-time Responsive Synthetic Dancers and Musicians"; SIGGRAPH 1996]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Trishul Mallikarjuna Center for Music Technology Georgia Institute of Technology, Atlanta, GA 30332 
USA A Visual Beat Detection System for Grid-Based Interactive Percussion and Synchronization trishul.mallikarjuna@gatech.edu 
Abstract This document introduces a conceptually novel, simple and ordinarily robust computer-vision-based 
method of extracting musical beats from regular physical gestures of a performer, implemented in VisiBeat: 
a grid-based percussion system on the Max/MSP/Jitter platform for collaborative interactive music. Keywords 
 Music, beat, Horn, Schunk, optical flow, Max, MSP, Jitter, cv.jit, computer vision, grid, physical gestures, 
collaboration, interaction 1. Background By providing newer interfaces, technology is getting more non­musicians 
involved in music-making, while enabling musicians to have newer and more direct approaches towards the 
same. In this direction, interfaces based on routine physical actions of humans can be considered to 
be some of the more tangible ones, as humans can relate to actions like nodding their head, waving their 
hands or tapping their feet more easily than the skillful playing of an instrument. Many researchers 
have worked on mapping physical gestures to musical events, such as Eric Singer (commercially released 
Cyclops Max object [1]), Todd Winkler [2] &#38; David Rokeby ( Very Nervous System [2]). Many of these 
systems have been asynchronously used in contexts like dance [3] and interactive music [1], and the inverse 
process of generating visual gestures to musical input has also been successfully pursued [4]. However, 
there doesn t seem to have been an attempt to extract beats from regular, periodic gestures of performers 
and generating musical output in synchrony with their movements. The project VisiBeat discussed here 
aims at the same. 2. Processing The system developed is implemented for prototyping on Cycling-74 s 
Max/MSP/Jitter software platform by on an Apple iMac. Video processing is done using Jitter and cv.jit[2] 
objects, including a custom-built Jitter external in C. It takes in live video input through the camera, 
divides it into a grid (3x4), and processes each pane of the grid to extract beat. Steps involved in 
video processing for beat detection are as follows: 1.1. Input RGB is converted to Luminescence matrix 
1.2. Result is subjected to Horn-Schunk algorithm to detect optical flow, which detects motion for each 
pixel in 4 directions left, right, up and down (different colors in top figure). 1.3. Then, the result 
for all pixels of a pane is averaged to give out a single number for each direction separately 1.4. Results 
for opposite directions along vertical and horizontal axes are added with opposite signs to obtain a 
waveform of oscillation for each axis (bottom figure) 1.5. The troughs of this waveform are taken as 
the instantaneous event onset locations corresponding to the downbeat of nodding/tapping 1.6. Inter-onset-intervals 
(IOIs) are determined 1.7. If IOIs are stable within a factor (0.25) for a good number of consecutive 
runs (3), their average value is taken as the new beat period 1.8. Result is passed on to the percussive 
music synthesis system down the line and used appropriately In the prototype implementation, the beat 
periods are fed into a metronome object that triggers percussive sounds through the internal DLS software 
MIDI synthesizer on the iMac. Volume of all sounds are made proportional to the activity (oscillation 
amplitude) in the corresponding pane, and exponentially damped to give echoing effect. Additionally, 
a Jitter external was also developed to obtain activity level and x/y co-ordinates for peak activity, 
which enabled users to use a few panes as virtual x/y tactile pads for controlling melodic instrument 
voices. 3. Performance and Interface The system is robust in that the scene illumination and contrast 
doesn t need to be adjusted too precisely -and the system would work for various ambiences with little 
retuning required. It doesn t depend on a green screen and doesn t use chroma-keying. Oscillation amplitudes 
are also appropriately normalized and scaled, to ensure reasonable output volume for varied activity 
levels resulting, for example, from different distances of the performer from the camera and different 
lighting conditions. Further, the system was found to respond well within the beat period limits set 
in the system: 125 and 1500 ms. The performers would know in which pane(s) activity is being detected 
through a video interface with real-time feedback of their actions (see representative image); the panes 
without activity would go dark exponentially. 4. Further work The system is being optimized for speed 
with GPGPU programming to promote higher resolutions &#38; frame rates for higher accuracy. Further, 
two-dimensional pattern detection is being developed for analyzing the x/y oscillation waveforms so that 
more complex patterns (like figure of 8) may be detected and used for both downbeat and upbeat ; the 
current system detects only downbeats from linear oscillatory motion. 5. Acknowledgements Thanks to 
Dr. Jason Freeman from GT Center for Music Technology for advising the academic course on interactive 
music under which the project was taken up. 6. References [1] Singer E., Cyclops , Web Resource: http://www.ericsinger.com/cyclopsmax.html 
[2] Winkler T., Motion-Sensing Music: Artistic and Technical Challenges in Two Works for Dance , ICMC 
1998 [3] Winkler T., Fusing Movement, Sound, and Video in Falling Up, an Interactive Dance/Theatre Production 
, NIME 2002 [4] Singer E., Castiglia C., Liao S., Perlin K., "Real-time Responsive Synthetic Dancers 
and Musicians"; SIGGRAPH 1996 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599373</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<display_no>72</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>72</seq_no>
		<title><![CDATA[The blues machine]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599373</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599373</url>
		<abstract>
			<par><![CDATA[<p>Hardware based musical instruments are, in general, from the performer point of view, merely copies of real physical instruments. They do not provide facilities for being played, especially for musically untrained people.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621574</person_id>
				<author_profile_id><![CDATA[81466640832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cicconet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621575</person_id>
				<author_profile_id><![CDATA[81442602956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[I.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paterman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621576</person_id>
				<author_profile_id><![CDATA[81388598371]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carvalho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621577</person_id>
				<author_profile_id><![CDATA[81442611891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[L.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Blues Machine M. Cicconet I. Paterman P. Carvalho L. Velho IMPA IMPA IMPA IMPA  I II 1 Playing 
Computers Hardware based musical instruments are, in general, from the per­former point of view, merely 
copies of real physical instruments. They do not provide facilities for being played, especially for 
mu­sically untrained people. On the other hand, using computer software to simulate traditional musical 
instruments is a dif.cult endeavor too. Unfortunately, with­out special hardware components, the task 
of producing reasonable sounds becomes cumbersome, since mouse and keyboard were not designed to be interfaces 
for musical performance. So, although computers are largely used in today s music produc­tion, we still 
can not play them, in the sense we play the guitar, for example. 2 Playing Blues The 12-bar blues is 
one of the simplest musical styles: there are many songs built upon a scale of just six notes and harmonized 
by just three chords. That s why normaly the guitar student starts improvising on the blues scale, listening 
to a 12-bar blues base. However there is not a short way between learning the blues scale guitar patterns 
(there are .ve of them) and performing non­randomic improvisation. We can enumerate at least three obstacles: 
the .rst is that all of the .ve patterns differ from each other; sec­ondly, many guitar notes are out 
of the scale; thirdly, shifting the patterns when the main chord changes causes global reference con­fusion. 
 3 A New Musical Instrument Blues Machine (I) is a multitouch tangible guitar-like interface for 12-bar 
blues improvisation which aims to reduce the above men­tioned dif.culties. It lies in between software-and 
hardware-based music performance devices. Exploiting multitouch technology, it circumvents mouse and 
keyboard limitations as musical interfaces. Mapping a guitar­like scale of notes on the screen and having 
real strings as perfor­mance guidelines (II), it resembles the real instrument. With a multitouch interface 
the user can play more than just one note at a time, allowing more complex solos. With a tangible in­terface 
the performance becomes natural, decreasing the distance between the machine and the real musical instrument. 
At the same III IV time, Blues Machine simpli.es the task of learning the scale pat­terns themselves. 
Notes in its interface s scale patterns are in one­to-one relation with those of the guitar patterns, 
but the matrix of notes in the Blues Machine is simpler: except for the blue one, notes are verticaly 
aligned (III). Moreover, it s more dif.cult to play a wrong note, since there is no note out of the scale. 
 4 Engine and Interface Blues Machine was developed with Macintosh s Quartz Composer. Quartz Composer 
facilitates dealing with images, but was not es­pecially designed for sound synthesis, so a set of plug-ins 
had to be implemented in Objective-C. The composition receives .nger tracking messages from reacTIVision, 
a computer vision framework developed by the Music Technology Group of the Universitat Pom­peu Fabra 
as part of the Reactable project 1. Sound syhthesis is done via MIDI, by Apple s DLS Synth Audio Unit. 
The composi­tion and plug-ins are available at the project website 2. Blues Machine s interface (IV) 
has two main areas: backing track control and performance. The .rst, located at the top, receives .ve 
kinds of accompaniment instructions: global state of the machine (setup or performance), chord, tempo, 
backing track type and a con­trol to terminate the song at the next turn. Improvisation is done on the 
performance area, whose design ressembles a guitar .nger­board. All commands are guided by guitar strings, 
so that the user interacts with the interface in a (right or left hand) guitar-like move­ment. Even bends 
are possible (III): thanks to the string s physical limitation, the user perceives how far he can go 
with the pitch shift. 5 Habitat We see three main scenarios where the proposed application can be used. 
The .rst and more obvious one is as an entertainment device. We believe people with no special musical 
habilities can perform self pleasing melodies with little practice. Blues Machine can also be used for 
teaching/learning purposes; its characteristics makes more effective the learning curve of a new musical 
scale. The last and more ambitious scenario is its use, after necessary adjustments, in live performance 
by expert musicians. And of course this applies not only to blues band musicians, since the idea of Blues 
Machine could be adapted to any other musical style. 1http://mtg.upf.edu/reactable/ 2www.visgraf.impa.br/bluesmachine 
 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599374</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<display_no>73</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>73</seq_no>
		<title><![CDATA[Pandeiro funk]]></title>
		<subtitle><![CDATA[experiments on rhythm-based interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599374</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599374</url>
		<abstract>
			<par><![CDATA[<p>In this work, we address to the problem of making the machine listen and react to the musician in an improvisation situation with the purpose of generating high-quality music.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Interaction styles (e.g., commands, menus, forms, direct manipulation)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010475</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Sound and music computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003371.10003386.10003390</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Specialized information retrieval->Multimedia and multimodal retrieval->Music retrieval</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621578</person_id>
				<author_profile_id><![CDATA[81447598503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sergio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krakowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621579</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621580</person_id>
				<author_profile_id><![CDATA[81100218356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Francois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pachet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony CSL - Paris]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pandeiro Funk: Experiments on Rhythm-Based Interaction Sergio Krakowski Luiz Velho Francois Pachet IMPA 
IMPA Sony CSL -Paris 1 The Problem In this work, we address to the problem of making the machine listen 
and react to the musician in an improvisation situation with the purpose of generating high-quality music. 
Nowadays, on the shelf technology directed to live performance is still very limited with respect to 
the potential modern computers of­fer in terms of processing power. The great majority of equipments 
designed for musicians allows them to loop live recorded samples or .lter theirs sounds. On the other 
hand, DJ s have access to CD players and Drum Machines (e.g. MPC-2000) but the functionali­ties offered 
by these tools are not in the hand of musicians, in a live context and many times are shown to be in.exible. 
Playing over a .xed-tempo pre-recorded loop in an improvisation context is one of the most unpleasant 
situation a professional mu­sician can go through. For this reason we developed methods that make the 
communication between musician and machine more nat­ural during the improvisation experience. With respect 
to Rowe s classi.cation, we could de.ne our research as a Performance-Driven, Instrument-Driven approach. 
We can .nd many examples of performance-driven systems in com­puter music literature such as Francois 
Pachet s Continuator. Usu­ally, these systems provide one or more modes of interaction. Sim­ply speaking, 
we could see these modes as games the musician play with the computer. In general, a limitation to these 
systems is the fact that the musi­cian don t have the choice to change the mode of interaction she/he 
is using during the interaction experience without interrupting the music .ow. 2 In Music During a performance, 
players have to communicate to each other for many reasons. In a Jazz context, e.g., a musical phrase 
can be used to point out the end of a piece or the restart of the tune s theme. A more complex example 
of musical command are the phrases used by the saxophone player Steve Coleman to give orders to his band. 
This incredible musician developed with his group a vocabu­lary of some phrase-orders that, when played, 
change the behaviour his band interacts with him. Another example are the Bata drum s music. This drum 
ensemble is composed by a hierarchy of three drums, the okonkolo (smaller drum), itotele (medium drum) 
and the yia (biggest leading drum). The yia drum leads the other two drums by play one of the possible 
calls , rhythmic phrases that are interpreted by the other players as a sign to switch to another rhythmic 
pattern. 3 Our main contribution We now propose the main innovation of our approach: The use of rhythmic 
phrases as commands to control the computer. By rhyth­mic phrases we mean the rhythmic content of a musical 
phrase which can be extracted directly from the audio signal captured by a microphone. The advantages 
of this approach are many: 1. Is based in real life experience. 2. The musician can concentrate only 
in the music and not on pedals or other interfaces. 3. Let the musician control the machine without 
stopping the music .ow. 4. Low computational cost and fast results. 5. As it is audio-based, can be 
applied to many sorts of instru­ments. 6. A rhythmic phrase carries several informations that can be 
used as parameters during the interaction. In this way, the commands carry more information and the interaction 
be­comes richer and more natural.  4 The research What we present here is part of a PhD research of 
the .rst author of the abstract. In the thesis currently being written we give a mathe­matical formalization 
to the concept of modes of interaction using the synchronized automata theory which was found to be the 
best suited considering our instrument-driven approach. In the thesis, among other things, we rigorously 
de.ne what are the rhythmic phrases, and propose they can be seen as actions in the synchronized automata 
context. We also propose a robust method for detection of rhythmic phrases. 5 Case Study and Conclusion 
As a case study, we implemented this method in a system adapted to work with a percussion instrument 
called Pandeiro. This Brazil­ian tambourine is the instrument played professionally by the .rst author 
of the abstract. The low level analysis and the detection of rhythmic phrases were all implemented in 
C language as externals in the Pure Data frame­work. This analysis can be easily adapted to other instruments. 
The system has some modes of interaction and to each one, a dif­ferent rhythmic phrase associated to 
it, used to enter or leave that mode. We strongly encourage the reader to watch the Supplemen­tary Video 
to see an explanation about how the system works and a demonstration of the system in action. Our hypotheses 
of a multi-modal system with a natural switching­mode strategy has shown to be very ef.cient during the 
experiment­ing phase and has been used in real life during a series of perfor­mances that took place 
in Rio de Janeiro in the month January 2009. These performances were part of a musical project done by 
the .rst author of this abstract in which he mixed two different Brazilian genres, the traditional Choro 
(where we .nd the Pandeiro) and the .rst Brazilian electronic style called Funk Carioca . In these concerts, 
the system had an important artistic function of blending the traditional Pandeiro playing to resources 
that, up to now, were exclusive to DJ s and VJ s. This can be seen in the Supplementary Video. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599375</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<display_no>74</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>74</seq_no>
		<title><![CDATA[A data-driven visual simulation of fire phenomena]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599375</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599375</url>
		<abstract>
			<par><![CDATA[<p>In order to simulate and visualize natural phenomena, especially fluid behavior such as smoke and fire, many novel studies have recently been conducted. Usually these methods use CFD (computational fluid dynamics), which calculate Navier-Stokes equations in real-time to generate realistic fluid motion and interactions, as well as high-performance GPU technologies. We proposed a new approach to the visual simulation of fluid flow by combining the use of pre-calculated CFD data with the real-time processing of such data. As the domain-specialized CFD solver predicts detailed fluid dynamics to an accuracy of a guaranteed error range, we could provide nearly actual behaviors of a fire-driven fluid flow. Moreover, this CFD data includes physical quantities such as temperature distribution, which can provide useful information to the training evaluation process. However, the data-driven method requires appropriate data processing techniques to create and manage large data sets. In this study, we developed a firefighter training simulator to demonstrate our proposed methods and explore related research issues.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Visual</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Physics</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010441</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Physics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010365</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Visual analytics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621581</person_id>
				<author_profile_id><![CDATA[81350591056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Moohyun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cha]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Institute of Machinery and Materials]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621582</person_id>
				<author_profile_id><![CDATA[81311485768]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jaikyung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Institute of Machinery and Materials]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621583</person_id>
				<author_profile_id><![CDATA[81442602983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Byungil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Institute of Machinery and Materials]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621584</person_id>
				<author_profile_id><![CDATA[81442610130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hyokwang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621585</person_id>
				<author_profile_id><![CDATA[81319492558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Soonhung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Advanced Institute of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[McGrattan K., Klein B., Hostikka S. and Floyd J. 2008. Fire Dynamics Simulator User's Guide. NIST Special Publication 1019--5, NIST.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Data-driven visual simulation of fire phenomena Moohyun Cha*, Jaikyung Lee*, Byungil Choi*, Hyokwang 
Lee**, Soonhung Han** * Korea Institute of Machinery and Materials, ** Korea Advanced Institute of Science 
and Technology 1. Introduction  In order to simulate and visualize natural phenomena, especially fluid 
behavior such as smoke and fire, many novel studies have recently been conducted. Usually these methods 
use CFD (computational fluid dynamics), which calculate Navier-Stokes equations in real-time to generate 
realistic fluid motion and interactions, as well as high-performance GPU technologies. We proposed a 
new approach to the visual simulation of fluid flow by combining the use of pre-calculated CFD data with 
the real-time processing of such data. As the domain-specialized CFD solver predicts detailed fluid dynamics 
to an accuracy of a guaranteed error range, we could provide nearly actual behaviors of a fire-driven 
fluid flow. Moreover, this CFD data includes physical quantities such as temperature distribution, which 
can provide useful information to the training evaluation process. However, the data-driven method requires 
appropriate data processing techniques to create and manage large data sets. In this study, we developed 
a firefighter training simulator to demonstrate our proposed methods and explore related research issues. 
2. The Data Driven Method  Among the training and emergency response systems, ADMS (Advanced Disaster 
Management Simulator, ETC corp.) is one of the most well-known commercialized solution which provides 
fire driven accident scenarios and virtual training environments. This system visualizes fire phenomena 
through simple particle systems, which are not based on fluid dynamics model, like most other commercialized 
training software because of the expensive calculation cost for the large virtual space such as road-tunnel. 
However, our data-driven method can provide the training system with the pre-calculated visual simulation 
of fire based on specific fire scenarios occurred in such a large virtual environment. As this approach 
requires off-line simulation data sources, we first designed numerical fire models such as grids, boundary 
conditions, combustion parameters, obstructions and other external forces. These can have very fine grid 
resolution and unlimited volume size. Next, we calculated Navier-Stokes equations by utilizing the FDS 
(Fire Dynamics Simulator, developed by NIST) [McGrattan et al. 2008], which puts an emphasis on smoke 
and heat transport from fires. The resulting data is non-uniform grid data that consists of many kinds 
of physical quantities, such as soot density, volume fraction of oxygen and carbon monoxide, gas temperature, 
heat release rate and so on. Next, we re-sampled the result in a uniform grid suitable for volume rendering, 
and performed an axis alignment between the solver and the rendering system. In addition, as the size 
of the data set can be extremely large, we used traditional octree-based space partitioning and multi-resolution 
techniques. In the on-line step, stream files are partially mapped onto the main memory and each nodes 
of tree are traversed with view-dependent LOD selections according to multiple distance parameters and 
data-dependent culling. At this point, the traversal process can be used not only for rendering but also 
for accumulating physical quantities around the user. This enables us Figure 1: Real-time rendering 
of firefighter training simulator based on our data-driven method, using a road tunnel traffic accident 
scenario. to evaluate the safety achievement based on the trainee s trace. Finally, volume data are interpolated 
in time space to generate continuous input values for the transfer function used for splat based volume 
rendering. 3. Results and Future Works  * e-mail: {mhcha, jkleece, cbisey}@kimm.re.kr ** e-mail: {adpc92, 
shhan}@kaist.ac.kr The FDS result for a road-tunnel fire scenario that takes 30 minutes consists of 
1800 ASCII files of around 72 GBytes in total. Data for a single time-step consists of 288000 voxels(800x24x15). 
We converted these into 16.4 GBytes binary streams, which represent 5 hierarchical octree levels. Memory 
management was run on a different process from main one to maximize the efficiency of recent multi-core 
CPUs. By means of parameterized multi-resolution options, we could guarantee regular performance under 
various hardware conditions. The experiment maintained a minimum performance of 30 fps under an NVidia 
QuadroFX5600. In addition, we could visualize a great deal of invisible but useful data of fire phenomena 
by applying opacity and color mapping based on the important value ranges related to user safety. From 
the perspective of human interaction, the data-driven system can represent macro-scale fluid flow mainly 
caused by heat transfer interacting with ventilation or obstruction conditions. Thus, users can experience 
pre-calculated fluid flow from different viewing perspectives in real-time, but cannot interact with 
fire or smoke itself by means of user inputs such as extinguishing a fire. We are currently researching 
this issue by utilizing a hybrid method, which synthesizes global motion calculated off-line using CFD 
solver and local details which are calculated in real-time using simplified fire dynamics model. So that 
we could achieve some interactivity while preserving global accuracy of fire phenomena. References McGrattan 
K., Klein B.,Hostikka S. and Floyd J.. 2008. Fire Dynamics Simulator User s Guide. NIST Special Publication 
1019-5, NIST. Acknowledgements This work is supported by the Korean National Emergency Management Agency 
(Korean NEMA). 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599376</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<display_no>75</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>75</seq_no>
		<title><![CDATA[Desired deformation of continuum surfaces in 3DCG animation by time varying stable forms]]></title>
		<subtitle><![CDATA[application to make animations of flowers, wings, cloths etc.]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599376</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599376</url>
		<abstract>
			<par><![CDATA[<p>In this paper, the authors discuss how to make 3DCG animations of flowers, wings, and cloths etc. which are modeled by surfaces. These 3DCG animations are obtained based on the numerical simulation for the Newtonian dynamics equations of surfaces. These equations are obtained from the potential functions (the energy functions) of the surfaces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621586</person_id>
				<author_profile_id><![CDATA[81421598603]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ippei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takauchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621587</person_id>
				<author_profile_id><![CDATA[81442612907]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621588</person_id>
				<author_profile_id><![CDATA[81442600457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiromu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621589</person_id>
				<author_profile_id><![CDATA[81421593574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asakura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621590</person_id>
				<author_profile_id><![CDATA[81430601538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Motofumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hattori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1400888</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Motofumi Hattori, Ippei Takauchi, Ryo Asakura: "A Feedback Control System for Desired Deformation of Cloths by Time Varying Stable Forms", Submission Id: "latebreaking_0148" Poster Number A101 Presented in the SIGGRAPH 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Desired Deformation of Continuum Surfaces in 3DCG Animation by Time Varying Stable Forms - Application 
to Make Animations of Flowers, Wings, Cloths etc. - Ippei TAKAUCHI, Masatoshi OCHIAI, Hiromu SAITO, Ryo 
ASAKURA, Motofumi HATTORI, Kanagawa Institute of Technology, Dept. of Information Media  1 The purpose 
of this research In this paper, the authors discuss how to make 3DCG animations of flowers, wings, and 
cloths etc. which are modeled by surfaces. These 3DCG animations are obtained based on the numerical 
simulation for the Newtonian dynamics equations of surfaces. These equations are obtained from the potential 
functions (the energy functions) of the surfaces. However, it is difficult to obtain suitable 3DCG animations 
only by the above numerical simulation. Most of the 3DCG animations based on the above numerical simulation 
are not natural. In order to obtain the natural 3DCG animation, one must simulate again and again by 
changing the parameters of the surface such as the mass density, the elasticity, and the damping. It 
is difficult to adjust these parameters intuitively for obtaining the natural 3DCG animation. In this 
paper, the natural 3DCG animation will be obtained by operating the stable form in the potential function 
(the energy function) of the surface. The authors propose a method how to adjust the Newtonian dynamics 
equation of the surface which produces the natural 3DCG animation.  2 Modeling of surfaces as the continuum 
Flowers, wings, and cloths etc. are modeled by surfaces. They are modeled by 2 dimensional surfaces in 
a 3 dimensional space. Thus it is modeled by a twice order differential function from some domain in 
2 dimensional Euclidean space to 3 dimensional Euclidean space. In this paper, the dimensions of Euclidean 
spaces are generalized. We consider the general surface (manifold), i.e. the function from some domain 
in dimensional Euclidean space to dimensional Euclidean space. The Newtonian dynamics equation of the 
general surface will be obtained based on the variational principles for continuum mechanics.    
   3 The stable form in the potential function ( the energy function ) of the surface The most stable 
form of a cloth is the form of its paper pattern. As the cloth is modeled by , its paper pattern is 
also modeled by . For general dimensional surface , we call such paper pattern as its stable form . 
Since the most stable form of a surface is the stable form , the potential function which generates the 
inner force of the surface is defined by the distance inner between the surface and with respect to 
their bend, stretch, and shear. When the form of the surface equals to the one of the stable form , the 
surface has the most stable form and inner.When inner, as inner becomes larger and larger, the surface 
  becomes more and more unstable. The Newtonian dynamics equation which describes the deformation process 
of the surface becomes   (1) The 3DCG animation of a newly-emerged butterfly was created by the numerical 
analysis simulation for this equation (1). The stable form is set as a fully expanded wing. The final 
state of this animation is shown in Figure 1(a). By simulating the dynamical equation (1), the solution 
expresses the flexibility of the wing s deformation process.  4 To control the deformation of the surface 
by using the time varying stable form as a control input As the equation (1) shows, the stable form s 
guides simulation to the appropriate form of the surface . In this method, since the CG designer can 
set only one guiding stable form  , he can make limited animation of the surface. In order to make more 
various animations, we must use plural guiding stable forms. By setting plural guiding stable forms, 
we can simulate the various kinds of appropriate animation of the surface intuitively.  In the real 
world, the surface s stable form dose not change as the time goes on, as is fixed in the equation (1). 
But, in the virtual world, one can change the stable from as the time goes on. By this time varying stable 
form, the dynamics equation (1) becomes   (2) The 3DCG animation of flowering (the petal of morning-glory) 
was created by the numerical analysis simulation of the equation (2) for the time varying stable form. 
For time varying stable form, was set as the shape of bud, was set as the shape in start bloom, and was 
set as the shape in full bloom. The final state of this animation is shown in Figure 1(b). The stable 
form at time   prevents the over blooming of the final state . By the effect of the stable form at 
time  , the final bloom is maintained appropriately. By the time varying stable form which consists 
of plural guiding stable forms, we can simulate the various kinds of appropriate animation of the surface 
intuitively. This algorithms was developed as extended functions of the general 3DCG tools. We can make 
suitable time varying stable forms easily and various kinds of appropriate animation of the surface. 
     References Motofumi Hattori , Ippei Takauchi , Ryo Asakura : "A Feedback Control System for 
Desired Deformation of Cloths by Time Varying Stable Forms", Submission Id: "latebreaking_0148" Poster 
Number A101 Presented in the SIGGRAPH 2008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599377</article_id>
		<sort_key>760</sort_key>
		<display_label>Article No.</display_label>
		<display_no>76</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>76</seq_no>
		<title><![CDATA[The framework of sound rendering for particle-based physics]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599377</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599377</url>
		<abstract>
			<par><![CDATA[<p>There has been growing interest in the sound generating technique based on physics from the motions of 3d graphics objects. In recent work several methods have been proposed to physically simulate these audio events natably using modal synthesis [<i>K. van den Doel et al.</i> 2001] or finite element method [<i>O'Brien et al.</i> 2002]. However, in these mesh-based method, it needs complicated operation to preprocess, for example, to generate the computational mesh for 3d object, and to create the system's matrices, etc...</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.5</cat_node>
				<descriptor>Signal analysis, synthesis, and processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010750.10010762.10010767</concept_id>
				<concept_desc>CCS->Hardware->Robustness->Hardware reliability->Signal integrity and noise analysis</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621591</person_id>
				<author_profile_id><![CDATA[81447597329]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383322</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[
<i>
&#60;b&#62;K. van den Doel, P. G. Kry and D. K. Pai. 2001&#60;/b&#62;, FoleyAutomatic: Physically-based Sound Effects for Interactive Simulation and Animation, in Computer Graphics (ACM SIGGRAPH 2001 Conference Proceedings), August 2001</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383321</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[
<i>
&#60;b&#62;James F. O'Brien, Perry R. Cook, and Georg Essl. 2001&#60;/b&#62;, Synthesizing Sounds From Physically Based Motion. Proceedings of ACM SIGGRAPH 2001. pp. 529--536, August 2001</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[
<i>
&#60;b&#62;KOSHIZUKA, S., and OKA, Y. 1996&#60;/b&#62;. Moving-Particles Semiimplicit Method for Fragmentation of Incompressible fluid, Nucl. Sci. Eng, Vol. 4, 29--46</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[
<i>
&#60;b&#62;Hauser, K., Shen, C., O'Brien, J. F. 2003&#60;/b&#62;, "Interactive Deformations Using Modal Analysis with Constraints." To appear in Graphics Interface 2003</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1101554</ref_obj_id>
				<ref_obj_pid>1101530</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[
<i>
&#60;b&#62;Kees van den Doel. 2005&#60;/b&#62;, Physically-based Models for Liquid Sounds, ACM Transactions on Applied Perception, Vol. 2, No. 4, pp. 534--546, October 2005</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Framework of Sound Rendering for Particle-Based Physics Kazuhiko Yamamoto Graduate School of Design 
/ ADCDU and CDS, Kyushu University, Japan 1 Introduction There has been growing interest in the sound 
generating tech­nique based on physics from the motions of 3d graphics ob­jects. In recent work several 
methods have been proposed to physically simulate these audio events natably using modal syn­thesis [K. 
van den Doel et al. 2001] or .nite element method [O'Brien et al. 2002]. However, in these mesh-based 
method, it needs complicated operation to preprocess, for example, to gener­ate the computational mesh 
for 3d object, and to create the system s matrices, etc... In this study, I present a novel idea for 
generating physically based sound using particle based simulation for rigid body, elastic body, .uid 
and their interaction dynamics. The particle based simula­tion treats solids or .uids as the collection 
of many particles. By using this type of method, we can treat various kinds of materials similarly, that 
have different properties respectively, and it makes much easier to discretize, to preprocess, and to 
implement than past mesh-based method, because all we need to initialize is to ar­range particles uniformly, 
and as the result of that, they have uni­form data structure. But it has not been contributed the idea 
to generate sounds from particle-based simulation method yet. So, I construct a method to simulate physical 
sounds from particle­based method, especially for my original orthotropic elastic model of MPS(Moving 
Particles Semi-Implicit) Method based solid, and it makes more easy to synthsis physical sounds in Graphics. 
 2 Method Overview 2.1 Sound Synthesis for Solid Motion 2.1.1 Elastic Body For deformable body, my method 
is based on my original or­thotropic elastic model of MPS(Moving Particles Semi-Implicit) Method. In 
this model, the equation for motion of particles is rep­resented as . NNN ..2 .= E.N.) + ... (1) .+ .(divE. 
-.E · .t2 .t where ., ., div represents Laplacian, gradient, and divergence re­spectively. .Nindicate 
the displacement, ., . denote the dencity, and viscosity coef.cient, respectively. E is orthotropic elastic 
tensor represented by Young modulus, Poisson rate, Shear Elasticity. This equation is discretized by 
replacing each differential operator using the corresponding model of the MPS method, which represent 
the interaction of the neighboring particles [Koshizuka et al. 1999]. I de.ne the sound pressure P radiated 
from a particle that locate at the surface of geometry as 2 re. P = .aircairpre (1+ )Nv · NN(2) 2 where 
., N is the curvature and the normal vector at the point of the surface particle respectively, re is 
the threshold distance to consider the interaction between two particles, .air, and cair are dencity 
and sound velocity of air respectively. Nv denote the velocity of the sur­face particle calculated by 
Eq.(1). This sound pressure is radiated to air after processing low-pass .ltering to prevent the ailiasing 
error. 2.1.2 Rigid Body Rigid body is treated as quasi-elastic body, and its vibratory motion is de.ned 
as Eq.(1). Here, I construct a way to perform the modal analysis for this particle based solid. It compute 
the static modal analysis data by eigenvalue decomposition of linear system s ma­trix like as .nite element 
technique. However, to create the system s matrix is much easier than such a solid discretized by .nite 
element meshes because particle based solid is initialized by arranging par­ticles uniformly. In general, 
linear system is represented as where K, C, and M are repectively known as the system s stiffness, damping, 
and mass matrices, d,and f respectively as the vector of generalized displacements and forces. In MPS 
method, as the re­sult of particlize, virtual springs can be de.ned. In my model, the coef.cients for 
this spring within the two particles i and j is 6m 0 Kij = Eij w(|rij|) (4).N0r02 ij 0 where rij , m, 
and N0 are respectively the distance of the two par­ticles, mass of one particle, weighted dencity of 
neighboring parti­cles, w() denote the weighting function of MPS method. By using this equation, we can 
create stiffness matrix, and perform modal decomposition. Once the modal analysis data is precomputed, 
we can continue to use it while simulation. Each obtained mode are oscilated by forces that are given 
to surface particles using same technique as [O'Brien et al. 2003], and radiated as sound pres­sure from 
surface particles just like the case of elastic body.  2.2 Sound Synthesis for Fluid Motion It is well 
known that .uid by itself hardly makes any sound at all. It is only when air is trapped by .uid in the 
form of bubbles that sounds are heard [K. van den Doel 2005]. The impulse response of radially oscillating 
bubble is given by p = Ae-dtsin(6p/rt) (5) with r the bubble radius in meters. d is the damping parameter. 
For particle simulation of .uid, I apply Smoothed Particles Hydrody­namics Method. To generating bubbles 
from .uid motion, I create air sph particles above the .uid s free surface, and when air parti­cles is 
traped by .uid particles, bubble is formed. When bubble is generated, sound pressure of the bubble can 
be calculated by eq.(5), and radieted from the position of the center of the bubble. 2.3 Sound Synthesis 
for Solid-Fluid Interaction In this method, to synthesis the sound from solid-.uid interaction, we don 
t need any special things. All we should is to simulate solid-.uid coupling by using arbitrary way, and 
by calculated forces given solid to .uid, and .uid to solid, sound pressure is computed automatically 
by above-mentioned method.  3 Conclusion In this study, I presented a novel idea to generating physically 
based sound from particle based simulation. It makes very easy to pre­process and to implement compared 
with the past .nite element mesh-based method because all we need to is to aranging particles uniformly. 
In the future, I ll implement this method on GPU using NVIDIA CUDA, and develop a interactive application. 
 References K. van den Doel, P. G. Kry and D. K. Pai. 2001, FoleyAutomatic: Physically-based Sound Effects 
for Interactive Simulation and Ani­mation, in Computer Graphics (ACM SIGGRAPH 2001 Conference Proceedings), 
August 2001. James F. O Brien, Perry R. Cook, and Georg Essl. 2001, Synthe­sizing Sounds From Physically 
Based Motion. Proceedings of ACM SIGGRAPH 2001. pp. 529-536, August 2001. KOSHIZUKA, S., and OKA, Y. 
1996. Moving-Particles Semi­implicit Method for Fragmentation of Incompressible .uid, Nucl. Sci. Eng, 
Vol.4, 29-46. Hauser, K., Shen, C., O Brien, J. F. 2003, Interactive Deforma­tions Using Modal Analysis 
with Constraints. To appear in Graph­ics Interface 2003. Kees van den Doel. 2005, Physically-based Models 
for Liquid Sounds, ACM Transactions on Applied Perception, Vol. 2, No. 4, d2 pp. 534-546, October 2005. 
Kx + C dx + Mx = f (3) dt dt2 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599378</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<display_no>77</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>77</seq_no>
		<title><![CDATA[Physics for animation artists]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599378</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599378</url>
		<abstract>
			<par><![CDATA[<p>"Physics for Animation Artists" is a joint project by the Department of Physics and the Animation/Illustration Program at San Jose State University to develop a physics curriculum specifically for art majors planning to enter the animation industry.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[art education]]></kw>
			<kw><![CDATA[physics education]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003527</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003527</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1621592</person_id>
				<author_profile_id><![CDATA[81100379394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alejandro]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Garcia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[San Jose State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621593</person_id>
				<author_profile_id><![CDATA[81442610686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alice]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Carter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[San Jose State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621594</person_id>
				<author_profile_id><![CDATA[81442619934]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Courtney]]></middle_name>
				<last_name><![CDATA[Granner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[San Jose State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621595</person_id>
				<author_profile_id><![CDATA[81442595582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[San Jose State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Physics for Animation Artists Alejandro L. Garcia* Alice A. Carter Dept. Physics, San Jose State University 
J. Courtney Granner David Chai§ School of Art &#38; Design, San Jose State University Abstract Physics 
for Animation Artists is a joint project by the Department of Physics and the Animation/Illustration 
Program at San Jose State University to develop a physics curriculum speci.cally for art ma­jors planning 
to enter the animation industry. CR Categories: K.3.0 [COMPUTERS AND EDUCATION]: General Keywords: art 
education, physics education 1 Motivation Though animators may bend, stretch, even break the laws of 
physics for comic or dramatic effect, they are keen observers of the physi­cal world. Animation artists 
carefully study the motion of objects, the appearance of light and shadow, and the properties of materi­als. 
They .ll their sketchbooks with these observations, in the same fashion as their predecessors, going 
back to Leonardo da Vinci. Unfortunately, artists sometimes develop the same false notions as pre-Renaissance 
scientists, such as believing that heavy objects fall faster than lighter ones. This occurs because their 
training within the standard art curriculum does not include physics. Physics in animation has further 
gained importance since com­puter programs now contain sophisticated simulation algorithms for rigid 
body dynamics, .uid mechanics, optical ray-tracing, and other physical modeling. These CGI tools are 
developed by computer scientists, yet often must be used by artists without the bene.t of physics training. 
Furthermore artists and designers are increasingly working alongside computer graphics engineers and 
the disparity in their educational backgrounds makes it dif.cult for the two com­munities to interact 
because they lack a common background and speak different languages. 2 Physics of Animation Project 
With the support of the National Science Foundation, the De­partment of Physics and the Animation/Illustration 
program in the School of Art &#38; Design at San Jose State University have teamed up to develop curriculum 
materials to teach physics to art majors planning to work in the animation industry. Initially these 
materials were embedded as special topic lectures within existing art courses. Speci.cally, in the Spring 
2008, Fall 2008, and Spring 2009 semesters a series of weekly, one-hour physics lectures were presented 
in Art 114 (upper-division Animation) on topics relevant to the students animation exercises (i.e., pencil 
tests). Speci.cally, the students completed seven animation exercises: a bouncing ball; a falling brick; 
a falling .our sack; a .oating leaf; a falling and bouncing water balloon; a human jumping; and a human 
walking. Two of the lectures (Balance and Effects Animation) *e-mail: algarcia@algarcia.org e-mail:alicecarter@stanfordalumni.org 
e-mail:grannerc@earthlink.net §e-mail:davechai@earthlink.net were not linked to a speci.c exercise but 
were of general interest and applicable to more advanced animation work. The titles of the physics lectures 
(and some of the relevant topics discussed) were: 1. Physics of the Ball Drop (Slowing In/Out, Acceleration, 
Squash &#38; Stretch) 2. Physics of the Brick Drop (Path of Action, Anticipation, Impact &#38; Bouncing) 
 3. Physics of the Sack Drop (Follow-through, Secondary Action, Inertia, Soft Matter) 4. Physics of 
the Leaf Drop (Air resistance, Lift, Addition of Forces, Ground Effect) 5. Physics of the Water Balloon 
Drop (Liquids, Pressure, Vibration and Harmonic Motion) 6. Physics of Balance (Statics, Center of Gravity) 
 7. Physics of Jumping (Action/Reaction Forces, Work/Energy principles) 8. Physics of Walking (Gait 
analysis, Pendulums) 9. Physics of Effects Animation  In addition, a single 90 minute physics lecture 
was given in all sec­tions of Art 28 (lower-division Introduction to Animation/ Illustra­tion); that 
lecture was a highly condensed version of the .rst four lectures from Art 114. Finally, during summer 
2008 a series of half­day master classes were offered at San Jose State University and De Anza community 
college. These Master Classes included high school teachers and their students and were featured on National 
Public Radio s All Things Considered. Student surveys collected in all of these classes indicate that 
the students found the physics lectures to be very helpful and instructors report that the animation 
work of these students has improved. We have also launched a website (www.AnimationPhysics.com) for the 
distribution of information and materials. We are converting the Art 114 lectures and master classes 
into a series of tutorials and several are already available on the website. Starting Fall 2009, Physics 
of Animation will be offered as a stand­alone, cross-listed Art/Physics course. The course will be a 
combi­nation of lecture, small group discussion (e.g., critiques of anima­tion clips), in-class demonstrations, 
and mini-experiments; it has been certi.ed for upper-division General Education credit in sci­ence at 
San Jose State.  Acknowledgements Supported by the National Science Foundation s Course, Curricu­lum, 
and Laboratory Improvement program. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599379</article_id>
		<sort_key>780</sort_key>
		<display_label>Article No.</display_label>
		<display_no>78</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>78</seq_no>
		<title><![CDATA[Real-time droplet modeling using color-space environment matting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599379</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599379</url>
		<abstract>
			<par><![CDATA[<p>Amongst all atmospheric phenomena, rain is probably the most commonly used effect to create realistic immersive virtual environments, and to set the mood in movie storytelling. Although not immediately obvious, the beauty of rain emanates from the interplay of the involved light-matter interaction, generating effects of refraction and reflection, coupled with scattering effects. At the core, rain consists of water droplets under the influence of gravity. Current state of the art methods of generating rain are either computationally burdening, or not realistic enough. The key idea we introduce in this paper is to consider these droplets as transparent objects in the environment matting (EM) framework. This enables careful preprocessing to discover the light transport phenomena. We end up with a free-viewpoint real-time technique of simulating realistic droplets and rain in novel environments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621596</person_id>
				<author_profile_id><![CDATA[81324488727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Biswarup]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choudhury]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Indian Institute of Technology Bombay]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621597</person_id>
				<author_profile_id><![CDATA[81442600939]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pisith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Indian Institute of Technology Bombay]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621598</person_id>
				<author_profile_id><![CDATA[81100035301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sharat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chandran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Indian Institute of Technology Bombay]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1357013</ref_obj_id>
				<ref_obj_pid>1342250</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Choudhury, B., Singla, D., and Chandran, S. 2008. Fast color-space decomposition based environment matting. In <i>I3D 2008</i>, 1--1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141985</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Garg, K., and Nayar, S. 2006. Photorealistic Rendering of Rain Streaks. <i>ACM Transactions on Graphics 25</i>, 3, 996--1002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1180044</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lifeng, W., Zhouchen, L., Tian, F., Xu, Y., Xuan, Y., and Bing, K. S. 2006. Real-time rendering of realistic rain. In <i>SIGGRAPH 2006 Sketches</i>, 156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rousseau, P., Jolivet, V., and Ghazanfarpour, D. 2006. Realistic real-time rain rendering. <i>Computers&amp;Graphics 30</i>, 4, 507--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1185828</ref_obj_id>
				<ref_obj_pid>1185657</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Tatarchuk, N. 2006. Artist-directable real-time rain rendering in city environments. In <i>SIGGRAPH 2006 Courses</i>, 23--64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-time Droplet Modeling using Color-Space Environment Matting Biswarup Choudhury* Pisith Hao Sharat 
Chandran Vision, Graphics and Imaging Laboratory Department of Computer Science &#38; Engineering Indian 
Institute of Technology Bombay  (a) (b) (c) (d) Figure 1: Our goal is to push the state of the art in 
realism by discovering re.ection, refraction, and scattering effects due to water droplets. (a) During 
preprocessing, we position a droplet at every position in a notional volumetric space, and use environment 
matting to discover the light transfer matrix. (b) At run time, the derived matte is used to render each 
droplet with global illumination effects based on the novel environment. (c) Rain consists of droplets 
moving rapidly under the in.uence of gravity. Due to persistence of vision, they appear as streaks. 
(d) A streak is also computed using the preprocessed matte, which facilitates real-time rendering due 
to the lightweight nature of the matte.  Abstract: Amongst all atmospheric phenomena, rain is probably 
the most commonly used effect to create realistic immersive virtual environments, and to set the mood 
in movie storytelling. Although not immediately obvious, the beauty of rain emanates from the in­terplay 
of the involved light-matter interaction, generating effects of refraction and re.ection, coupled with 
scattering effects. At the core, rain consists of water droplets under the in.uence of gravity. Current 
state of the art methods of generating rain are either com­putationally burdening, or not realistic enough. 
The key idea we introduce in this paper is to consider these droplets as transparent objects in the environment 
matting (EM) framework. This enables careful preprocessing to discover the light transport phenomena. 
We end up with a free-viewpoint real-time technique of simulating realistic droplets and rain in novel 
environments. Introduction: State-of-the-art techniques for generating rain use particle systems. Broadly 
speaking, rendering is achieved in one of two ways: using a database of rain textures [Garg and Nayar 
2006; Tatarchuk 2006], or using geometry-based techniques on physically modeled droplets [Rousseau et 
al. 2006; Lifeng et al. 2006]. Ren­dering rain with a set of textures produces limited realism. On the 
other hand, geometry-based techniques affect real-time rendering of scenes with large number of optically-active 
particles. Contributions: We present a real-time framework for creating optically-active particles such 
as water droplets and rain. Our goal is to model refraction, re.ection and other complex illumination 
effects associated with a raindrop. The shape of the droplet is mod­eled as per the laws of physics [Rousseau 
et al. 2006]. However, instead of using geometry-based rendering techniques, we prepro­cess the droplet 
using EM techniques. For rain, our algorithm takes as input, parameters such as the raindrop size, quantity 
and velocity. Method: Our method is composed of two parts. During pre­processing, a conceptual volumetric 
representation of the world is used. A camera is pivoted at its center. After positioning the droplet 
at every possible discretized position in space (8000 for our experi­ments), we look at the world in 
all directions, and take a snapshot of the world. Using a small number of patterns for the walls of our 
environment, and the color-space EM technique [Choudhury et al. 2008], we are able to ef.ciently compute 
the effective (8000) *e-mail:{biswarup, pisith, sharat}@cse.iitb.ac.in mattes, which are then stored. 
Any EM method can be used in prin­ciple the advantage of using color-space EM is the small number of 
patterns (7-13), a small memory footprint (24 KB/matte), and fast computation (pre-processing takes 2 
seconds/matte.) At run­time, our system randomly generates particles (with user-speci.ed parameters) 
in the scene. Next, we capture a cube map of the scene from the current camera position and texture map 
the six faces of our environment. This cube map is our novel lighting environment. This, along with our 
matte, is used to relight every droplet. Rain: The human eye s retina takes about 1 th of a second to 
form 24 an image. Since the raindrop is rapidly falling during that expo­sure time, it gets composed 
into a continuous sequence, commonly known as a streak. Thus, a streak is the result of one rapidly mov­ing 
droplet. A careful analysis shows that a weighted mean of each column of our relit droplet image can 
generate the effective color values of the streak. Decreasing transparency values are used to generate 
a column of pixels (for vertically falling rain, Figure 1(d)) Translation, panning, tilting, and zooming 
of the camera is easily handled by picking appropriate mattes. Results: Figure 1(b) shows rendered images 
of a droplet, exhibiting refractive and re.ective properties, under novel environments. A high-speed 
camera can capture similar images even of rain, i.e., as falling ellipsoidal droplets. This expressive 
rendering of rain is often used in motion pictures for a scene played in slow motion. We render the droplets 
and streaks entirely in software, and are able to achieve real-time frame rates (30 fps) for 10,000 particles 
in the scene. All computations and timing calculations have been done with C++ on a Dual-Core AMD processor 
with 2GB RAM. References CHOUDHURY, B., SINGLA, D., AND CHANDRAN, S. 2008. Fast color-space decom­position 
based environment matting. In I3D 2008, 1 1. GARG, K., AND NAYAR, S. 2006. Photorealistic Rendering of 
Rain Streaks. ACM Transactions on Graphics 25, 3, 996 1002. LIFENG, W., ZHOUCHEN, L., TIAN, F., XU, Y., 
XUAN, Y., AND BING, K. S. 2006. Real-time rendering of realistic rain. In SIGGRAPH 2006 Sketches, 156. 
ROUSSEAU, P., JOLIVET, V., AND GHAZANFARPOUR, D. 2006. Realistic real-time rain rendering. Computers 
&#38; Graphics 30, 4, 507 518. TATARCHUK, N. 2006. Artist-directable real-time rain rendering in city 
environments. In SIGGRAPH 2006 Courses, 23 64. Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599380</article_id>
		<sort_key>790</sort_key>
		<display_label>Article No.</display_label>
		<display_no>79</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>79</seq_no>
		<title><![CDATA[Autonomous lighting agents in global illumination]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599380</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599380</url>
		<abstract>
			<par><![CDATA[<p>In computer graphics, physically-based global illumination algorithms such as photon-mapping [2001] have a linear progression between complexity and quality. To a given quality, rendering time scales linearly with computer performances. With Moore's law call in question and increasing demand in quality, those algorithms need more and more optimisations.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[artificial intelligence]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[photon mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Radiosity</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010376</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Reflectance modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621599</person_id>
				<author_profile_id><![CDATA[81442615185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adrien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herubel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institut Gaspard Monge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621600</person_id>
				<author_profile_id><![CDATA[81442615071]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Venceslas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institut Gaspard Monge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621601</person_id>
				<author_profile_id><![CDATA[81442618722]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Farchad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bidgolirad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Duran Duboi]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jensen, H., and Christensen, N. 1995. Efficiently Rendering Shadows using the Photon Map. <i>Compugraphics'95</i>, 285--291.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 2001. <i>Realistic Image Synthesis Using Photon Mapping</i>. A K Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>532434</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Maes, P. 1990. <i>Designing autonomous agents: theory and practice from biology to engineering and back</i>. MIT press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Ward, G., Rubinstein, F., and Clear, R. 1988. A ray tracing solution for diffuse interreflection. <i>Proceedings of the 15th annual conference on Computer graphics and interactive techniques</i>, 85--92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Autonomous lighting agents in global illumination * AdrienHerubel VenceslasBiri FarchadBidgolirad InstitutGaspardMonge 
InstitutGaspardMonge DuranDuboi Figure 1: Traditional photon map, Rendered picture and Autonomous Lighting 
Agents networks Keywords: global illumination, arti.cial intelligence, photon mapping 1 Introduction 
In computer graphics, physically-based global illumination algo­rithms such as photon-mapping [2001] 
have a linear progression betweencomplexityandquality.Toagivenquality,renderingtime scales linearlywithcomputerperformances.WithMoore 
slawcall inquestionandincreasingdemandinquality, thosealgorithmsneed more and more optimisations. Common 
optimisations such as irradiance caching [1988] or shadowphotons[1995] arethemselvesvery linearinperformance 
gain. They usually consist of adding knowledge about the scene and us ing it to interpolate previously 
computed values. Data is gathered in large volumes although due to heavy redundancy we observea lowdensityof 
usefuldata. We propose a new optimisation method for the photon-mapping global illumination algorithm. 
We use structures borrowed from Arti.cial Intelligence such as autonomous agents to reduce com­puting 
timeof thephotoncastingphaseand renderingphaseof the algorithm. Our structure calledAutonomousLightingAgents 
starts byusing anagent-based scenediscovery algorithm that is laterused to make decisions during rendering, 
inducing less photons being casted and shorter rendering times. 2 Autonomous lighting agents Discovering 
the scene using autonomous agents We real­izedthat optimisation techniquesinphoton-mapping useadditional 
data about the scene. The information carried about the model in these optimisations is either too redundant 
or too local. In shadow photons, aphoton-map using1 millionphotons can easilygenerate a 100K shadow photons 
in which only a small fraction is useful. In irradiance caching, data only consists of local distances 
and ge­ometry gradients. Thus, to optimize the algorithm we developed a less uniform, and more data-dense 
global structure. Our structure borrows the properties of the photon-map but its goal is to carry local 
data about the scene instead of carrying light .ux. Ef.ciently discovering the scene implies a non-uniform 
data density. Data * e-mail: herubel@igm.univ-mlv.fr e-mail:biri@univ-mlv.fr e-mail:fbidgolirad@quintaindustries.com 
 density should match rendering needs. Therefore, we decided to implement our method using arti.cial 
intelligence techniques. We use theautonomousagent softwareparadigm[1990]todiscover the scene. Determining 
areas of interest should be without human in­tervention in order to be usable. Our autonomous lighting 
agents ef.ciently gather large amounts of useful data using various sen­sors and store it using decision-making 
algorithms. Agents exist within multiplegraph representations. Eachgraphdepicts relations between agents 
such as neighbourhood or light paths. The result­ing structure isanetwork of agentsstoringdatanon-uniformlywith 
increaseddensity inzonesof interest. The oracle model The agent network is a complex structure to use 
during the rendering pass. To reduce the complexity we con­tinue touseanagentbased model.Thereforetheraytracerusesthis 
model like an oracle when a decision has to be taken. Our model gives answers to questions following 
the Should I? pattern. The raytracer questions the oracle, then this question is transferred to the most 
quali.ed agent. It may transfer the question through its various networks and/or answer itself, before 
giving the answer to the raytracer. 3 Results and discussion Our .rst results show a 33% decrease of 
memory occupation and slightly shorter rendering times for equal image quality. We plan to achieve signi.cant 
gain of speed particularly in indirect diffuse illumination. References JENSEN, H., AND CHRISTENSEN, 
N. 1995. Ef.cientlyRendering Shadowsusing thePhotonMap. Compugraphics 95,285 291. JENSEN, H. W. 2001. 
Realistic Image Synthesis Using Photon Mapping. AKPeters. MAES, P. 1990. Designing autonomous agents: 
theory and prac­tice from biology to engineering and back. MITpress. WARD, G., RUBINSTEIN, F., AND CLEAR, 
R. 1988. A ray trac­ing solution for diffuse interre.ection. Proceedings of the 15th annual conference 
on Computer graphics and interactive tech­niques,85 92. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599381</article_id>
		<sort_key>800</sort_key>
		<display_label>Article No.</display_label>
		<display_no>80</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>80</seq_no>
		<title><![CDATA[Cosine lobe based relighting from gradient illumination photographs]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599381</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599381</url>
		<abstract>
			<par><![CDATA[<p>Image-based relighting is a powerful technique for synthesizing images of a scene under novel illumination conditions, based on a set of input photographs. While successful relighting methods exist, they either require many photographs [Debevec et al. 2000], or operate on a limited class of materials or illumination conditions [Ma et al. 2007][Ramamoorthi 2006].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621602</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Hawkins, T., Tchou, C., Duiker, H.-P., Sarokin, W., and Sagar, M. 2000. Acquiring the reflectance field of a human face. In <i>Siggraph 2000, Computer Graphics Proceedings</i>, ACM Press / ACM SIGGRAPH / Addison Wesley Longman, K. Akeley, Ed., 145--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Rendering Techniques</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ramamoorthi, R. 2006. Modeling Illumination Variation with Spherical Harmonics. In <i>Face Processing: Advanced Modeling Methods</i>. 385--424.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cosine Lobe Based Relighting from Gradient Illumination Photographs Graham Fyffe* University of Southern 
California Institute for Creative Technologies  Figure 1: Relighting from four photographs: Cosine lobes 
are .t to the re.ectance function at each pixel and then relit with a point light. Left: uniform and 
gradient illumination input photographs; Right: relit images under some example point light illumination 
conditions. 1 Introduction in the re.ectance function, which in our case is the exponent n of a cosine 
lobe re.ectance function. Image-based relighting is a powerful technique for synthesizing im- We explore 
two different cosine lobe re.ectance functions, which ages of a scene under novel illumination conditions, 
based on a set integrate easily over the uniform and gradient illumination condi­ of input photographs. 
While successful relighting methods exist, tions, so that an analytic solution for the .t is obtained. 
Cosine they either require many photographs [Debevec et al. 2000], or op­ 1 p lobes of the form k max(0,a 
· l)n with k = I(n + 1) work erate on a limited class of materials or illumination conditions [Ma 2 
 well for diffuse and specular materials, but fail for materials with et al. 2007][Ramamoorthi 2006]. 
11 broader scattering such as fur. An alternative form k( )n a · l + 221 p We present an image-based 
method for relighting a scene by an-with k = I(n + 1) works well for broad scattering and still pro­ 
4 alytically .tting a cosine lobe to the re.ectance function at each pixel, based on gradient illumination 
photographs. An acceptable .t can be obtained for many materials using a single, colored co­sine lobe, 
which is obtained from just two color photographs: one under uniform white illumination and the other 
under colored gra­dient illumination. For materials exhibiting wavelength-dependent scattering distributions, 
a better .t can be obtained using an inde­pendent cosine lobe for each of the red, green, and blue channels, 
which is obtained from three monochromatic gradient illumination conditions instead of a single colored 
gradient condition, requiring a total of four photographs. 2 Method We photograph the scene inside a 
geodesic sphere of colored LED lights, which can be programmed to produce gradient illumination as well 
as uniform white illumination, similar to [Ma et al. 2007]. We compute the mean spherical angle of re.ected 
light a from the ratio images of the gradient illumination photographs to the uni­form illumination photograph, 
and we compute the total amount of re.ected light I from the uniform illumination photograph, as in [Ma 
et al. 2007]. If a single colored gradient is used instead of sep­arate x, y and z gradients, we make 
the assumption that the ratio images are wavelength-independent. Unlike [Ma et al. 2007], we do not estimate 
normals or albedo from these quantities, but instead take them as measured properties of the re.ectance 
function itself. Further, we make use of the additional information contained within the denominator 
in the normalization step of computing the mean spherical angle. This allows us to solve for an additional 
parameter *e-mail: fyffe@ict.usc.edu duces visually plausible results for diffuse and specular materials. 
 3 Results The method works well whenever the re.ectance function is well approximated by a single smooth 
lobe. Results for highly specular materials are noisy, due to the hardware limitation of using LED lights 
to approximate continuous illumination. Ground truth com­parisons con.rm that effects due to non-smooth 
re.ectance func­tions are not reproduced, such as hard shadows, but even with these limitations the method 
produces visually plausible results with no disturbing artifacts over a wide range of materials and illumination. 
Furthermore, the results are more consistent with ground truth than those obtained from using the input 
photographs as a linear basis. References DEBEVEC, P., HAWKINS, T., TCHOU, C., DUIKER, H.-P., SAROKIN, 
W., AND SAGAR, M. 2000. Acquiring the re­.ectance .eld of a human face. In Siggraph 2000, Computer Graphics 
Proceedings, ACM Press / ACM SIGGRAPH / Addi­son Wesley Longman, K. Akeley, Ed., 145 156. MA, W.-C., 
HAWKINS, T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 2007. Rapid acquisition of specular 
and diffuse normal maps from polarized spherical gradient illumina­tion. In Rendering Techniques, 183 
194. RAMAMOORTHI, R. 2006. Modeling Illumination Variation with Spherical Harmonics. In Face Processing: 
Advanced Modeling Methods. 385 424. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599382</article_id>
		<sort_key>810</sort_key>
		<display_label>Article No.</display_label>
		<display_no>81</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>81</seq_no>
		<title><![CDATA[Curvature-dependent local illumination approximation for translucent materials]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599382</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599382</url>
		<abstract>
			<par><![CDATA[<p>Simulating sub-surface scattering is one of the most effective ways to realistically synthesize translucent materials such as marble, milk and human skin. In previous work, the method developed by Jensen et al. [2002] improved significantly on the speed of the simulation, yet still cannot produce real-time rendering. Thus, we have developed a simple local illumination model which mimics the presence of a subsurface scattering effect. Furthermore, this approach is easy implementable on the GPU and doesn't require any complicated pre-processing as is often the case in this area of research [Mertens et al. 2003].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621603</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621604</person_id>
				<author_profile_id><![CDATA[81442616932]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hariu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621605</person_id>
				<author_profile_id><![CDATA[81314492254]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shuhei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wemler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621606</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., and Buhler, J. 2002. A rapid hierarchical rendering technique for translucent materials. 576--581.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882423</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mertens, T., Kautz, J., Bekaert, P., Seidelz, H.-P., and Van Reeth, F. 2003. Interactive rendering of translucent deformable objects. In <i>EGRW '03: Proceedings of the 14th Eurographics workshop on Rendering</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 130--140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Curvature-Dependent Local Illumination Approximation for Translucent Materials Hiroyuki Kubo* Mai Hariu 
Shuhei Wemler Shigeo Morishima Waseda University Waseda University Waseda University Waseda University 
Japan Society for the Promotion of Science (a) (b) (c) Figure 1: Synthesized images of a marble object: 
(a) Lambertian, (b) Our method, (c) Our method with texture and highlight 1 Introduction Simulating sub-surface 
scattering is one of the most effective ways to realistically synthesize translucent materials such as 
marble, milk and human skin. In previous work, the method developed by Jensen et al. [2002] improved 
signi.cantly on the speed of the simulation, yet still cannot produce real-time rendering. Thus, we have 
devel­oped a simple local illumination model which mimics the presence of a subsurface scattering effect. 
Furthermore, this approach is easy implementable on the GPU and doesn t require any complicated pre-processing 
as is often the case in this area of research [Mertens et al. 2003]. 2 Basic Idea The effects of subsurface 
scattering tend to be more noticeable on small, intricate objects than on simpler, .atter ones, which 
indicates that surface complexity largely determines these effects. This ob­servation led us to investigate 
the relationship between the effects of subsurface scattering and surface complexity more deeply. For 
the purposes of our research, we decided to use curvature to repre­sent surface complexity, combined 
with a simple local illumination model. Figure 2: a BRDF table showing curavature for marble *e-mail: 
hkubo@suou.waseda.jp 3 BRDF Approximation of Subsurface Scat­tering Effects It is necessary for subsurface 
scattering pro.les to be acquired and interpreted for our algorithm prior to rendering. We rendered sev­eral 
spheres of varying radii to reveal the relationship between cur­vature . and re.ectance function fr. 
The effects of subsurface scattering were simulated on each of the spheres using the dipole model. The 
spheres were illuminated by a directional light from the left side of the sphere, as shown in Figure-2. 
The calculated color of each pixel on the equatorial line represents a relation of e (the an­gle between 
the light direction and the normal vector) and fr of the sphere s particular curvature. These images 
(Figure-2-left) show how curved surfaces appear under given lighting conditions. In this research, we 
treated this curvature-dependent re.ectance function fr(e, .) as a BRDF 2D table (Figure-2-right). 4 
Results Figure1 shows several synthesized images of a marble object. We implemented our algorithm as 
a hardware-accelerated real-time renderer. The render is implemented as a pixel shader in HLSL. The vertex 
shader is used only to compute the model-view-projection transformation and passes on other vertex properties 
(vertex nor­mal etc). The pixel shader evaluates our BRDF, which requires 11 slots including one tex2d 
looking up BRDF table. In comparison, the simple Lambertian requires 6 slots. 5 Conclusion We have developed 
a method for approximating the effects of sub­surface scattering using a curvature-dependent re.ectance 
function. Since the function is a local illumination model, we are able to syn­thesize realistic translucent 
materials in real-time. Furthermore, the only pre-processing task required is to acquire the surface 
curva­ture. Acknowledgement This research is supported by Japan Science and Technology Agency, CREST 
project. References JENSEN, H. W., AND BUHLER, J. 2002. A rapid hierarchical rendering technique for 
translucent materials. 576 581. MERTENS, T., KAUTZ, J., BEKAERT, P., SEIDELZ, H.-P., AND VAN REETH, F. 
2003. Interactive rendering of translucent de­formable objects. In EGRW 03: Proceedings of the 14th Eu­rographics 
workshop on Rendering, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, 130 140. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599383</article_id>
		<sort_key>820</sort_key>
		<display_label>Article No.</display_label>
		<display_no>82</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>82</seq_no>
		<title><![CDATA[Direct illumination from dynamic area lights]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599383</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599383</url>
		<abstract>
			<par><![CDATA[<p>Area light sources are common in the real world, and thus important in realistic images. However, interactive rendering with area light sources is challenging, as each surface in a scene can receive light from every point in the area light. This problem is similar in nature to the rendering of single-bounce indirect illumination, and can be addressed with similar techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621607</person_id>
				<author_profile_id><![CDATA[81414601310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nichols]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Iowa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621608</person_id>
				<author_profile_id><![CDATA[81100265704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wyman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Iowa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1111428</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dachsbacher, C., and Stamminger, M. 2006. Splatting indirect illumination. In <i>Proceedings of the Symposium on Interactive 3D Graphics and Games</i>, 93--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507162</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nichols, G., and Wyman, C. 2009. Multiresolution splatting for indirect illumination. In <i>Proceedings of the ACM Symposium on Interactive 3D Graphics and Games</i>, 83--90.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Direct Illumination from Dynamic Area Lights Greg Nichols* Chris Wyman University of Iowa University 
of Iowa Figure 1: An indoor garden illuminated solely by a large area light source displaying a video. 
Different frames from the video produce illumination of varying intensity and colors. The above .gure 
displays three examples. 1 Introduction Area light sources are common in the real world, and thus important 
in realistic images. However, interactive rendering with area light sources is challenging, as each surface 
in a scene can receive light from every point in the area light. This problem is similar in nature to 
the rendering of single-bounce indirect illumination, and can be addressed with similar techniques. One 
previous method of rendering indirect illumination [Dachs­bacher and Stamminger 2006] computes a re.ective 
shadow map (RSM), chooses VPLs from the RSM, and splats each VPL s con­tribution onto the scene. Prior 
work [Nichols and Wyman 2009] describes a multiresolution variant of re.ective shadow maps. Each VPL 
s splat is adaptively re.ned into a set of multiresolution sub­splats at the appropriate eye-space resolution, 
greatly reducing the .ll-rate requirement of re.ective shadow maps. These techniques extend naturally 
to allow rendering of direct il­lumination from area light sources: we simply use a different set of 
VPLs. Our method requires no precomputation, enabling lights with textures and even video sources, as 
depicted in Figure 1. We also describe a light-clustering method that allows hierarchical se­lection 
of patches in both eye-space and light-space; this ap­proaches the idea of hierarchical radiosity, but 
works in image­space instead of object-space. 2 Our Approach Like multiresolution splatting [2009], 
we adaptively subdivide the scene into eye-space patches, each of which is rendered with illu­mination 
from a set of VPLs. Instead of choosing these VPLs by sampling the RSM, however, we create them by sampling 
the sur­face of the area light source. To reduce artifacts arising from VPL undersampling while still 
maintaining interactive performance, we place VPLs .nely around luminance discontinuities in the area 
light s texture or video source, and more coarsely in areas without them. A single geometry shader *e-mail: 
gbnichol@cs.uiowa.edu e-mail: cwyman@cs.uiowa.edu Figure 2: Beginning with an initial dense VPL sampling 
(left), we cluster VPLs in areas without luminance discontinuities (right). pass processes initial, densely-sampled 
VPLs and discards those without nearby discontinuities, weighting the remaining VPLs ap­propriately. 
Discontinuity detection relies on a min-max mipmap similar to that used during eye-space re.nement. Figure 
2 illus­trates an example of this process. During rendering, each eye-space fragment gathers illumination 
from the appropriate set of VPLs. Currently, each VPL contributes light to every eye-space fragment; 
we are exploring a variation in which the best set of VPLs is selected on a per-fragment basis, cho­sen 
to reduce fragment error below a user-speci.ed threshold. Our method renders the scene in Figure 1 at 
25-30fps, with full­motion video on the light source. Like other RSM approaches, our method currently 
ignores visibility. In future work, we plan to ad­dress this, enabling direct shadows and improving our 
approxima­tion. Acknowledgements: All video stills are from Big Buck Bunny , copyright c &#38;#169; Blender 
Foundation (www.blender.org). References DACHSBACHER, C., AND STAMMINGER, M. 2006. Splatting in­direct 
illumination. In Proceedings of the Symposium on Inter­active 3D Graphics and Games, 93 100. NICHOLS, 
G., AND WYMAN, C. 2009. Multiresolution splatting for indirect illumination. In Proceedings of the ACM 
Symposium on Interactive 3D Graphics and Games, 83 90. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599384</article_id>
		<sort_key>830</sort_key>
		<display_label>Article No.</display_label>
		<display_no>83</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>83</seq_no>
		<title><![CDATA[Gaussian projection]]></title>
		<subtitle><![CDATA[a novel PBR algorithm for real-time rendering]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599384</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599384</url>
		<abstract>
			<par><![CDATA[<p>Gaussian Projection is an algorithm based on the Gaussian Pyramid, that can render point-sampled-geometry - obtained from stereo data in the form of range images - without polygonization, and at full native resolution. Gaussian Projection makes use of the GPU to perform efficient and fast multi-resolution rendering of point-based data, with automatic hole-filling (scattered point interpolation), and without any preprocessing other than Image Pyramid generation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621609</person_id>
				<author_profile_id><![CDATA[81442611911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sajid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farooq]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Glasgow]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621610</person_id>
				<author_profile_id><![CDATA[81100318527]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Paul]]></middle_name>
				<last_name><![CDATA[Siebert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Glasgow]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Farooq, S., and Siebert, J. P. 2009. Pyramidal multi-view pbr a point-based algorithm for multi-view multi-resolution rendering of large data sets from range images. In <i>VISIGRAPP2009 - The International Joint Conference on Computer Vision, Imaging and ComputerGraphics Theory and Applications</i>, N/A.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383300</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zwicker, M., Pfister, H., van Baar, J., and Gross, M. 2001. Surface splatting. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, ACM, New York, NY, USA, 371--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Gaussian Projection: A novel PBR algorithm for real-time rendering * Sajid Farooq J. Paul Siebert 
University of Glasgow University of Glasgow 1 Introduction Gaussian Projection is an algorithm based 
on the Gaus­sianPyramid,that canrenderpoint-sampled-geometry -ob­tainedfromstereodataintheformof rangeimages 
-without polygonization, and at full native resolution. Gaussian Pro­jection makes use of the GPU to 
perform e.cient and fast multi-resolution rendering of point-based data, with auto­matichole-.lling(scatteredpointinterpolation), 
and with­out anypreprocessing other thanImagePyramidgeneration. 2 Problem A native point-based render 
of a 3D model will generally result in holes due to a lack of connectivity information as­sociated with 
the points. Gaussian Projection is based on two insights: First, that a point-based image rendered onto 
a smaller view-port will have fewer holes, since the grid-size (pixel-size)remains constant. Second:AnImagepyramid 
of the range-image will produce a set of 3d models of varying scales/frequencies whereholesinthehigherfrequency 
model can be .lled by the low-frequency model. These properties can be exploited to .ll the holes in 
a PBR render by pyra­midizing the range image, and rendering the various models in screen-space at various 
viewport sizes, and letting each high-resimage seethrough totheimagebelowit. Thiswork is related to previous 
work by the same authors[Farooq and Siebert 2009]. Where the previous work used a Laplacian Pyramid to 
perform multi-view splining, it did not handle scattered-data interpolation. The proposed method uses 
a Gaussian Pyramid, performs in real-time, and focuses on scattered-data interpolation rather than multi-view 
display. 3 Proposed Algorithm Our rendering algorithm relies on a range-image, a texture image, and 
a mask image as input. We begin by creating multiple copies of the texture images, each one a blurred 
version of the previous. In essence, this constructs a Gaus­sian Image Pyramid. We then proceed to render 
each of the range-texture-mask as separate textures (via Render­To-Texture or RTT as supported by modern 
GPUs) with an alpha of 1, however, with the viewport size decreasing by an octave. Hence, the blurrier 
the picture, the smaller an area of the screen it occupies. This e.ectively creates a Gaussian ImagePyramidinscreen-space. 
Sinceeachlevel of thePyra­mid is, by default, separated by the spatial frequency, the resulting rendered 
images are also conveniently separated into frequencies, with the base image containing all frequen­cies, 
i.e, the most detail. This is also the image that will contain the most holes. Since this is the highest-resolution 
image, further high-frequency information is not present in order to .ll the gaps. The most logical solution 
is to use lower-frequency detail to .ll in the holes, rather than trying to generate high-frequency detail. 
The next low-frequency image(texture)in the screen-spacepyramidis expandedand placed beneath the high-frequency 
image. The holes in the *e-mail:sajid@dcs.gla.ac.uk psiebert@dcs.gla.ac.uk high-frequency image will 
allow the low-frequency image to show-through. In order to provide anti-aliasing, points are rendered 
using3x3gaussian weightedkernels. TheGaussian weights used areprecomputed anddiscretized, and represent 
all possible sub-pixel shifts to a .nite resolution. Thus, in multiple passes, using the same geometry 
at each step,theproposed methodproduces ahole-free, multi­resolution image from point-sampled geometry. 
The images produced require an additional step of sillhouette mask­ing , where the textures in screen-space 
at lower-resolutions will be masked out if they exceed the silhouette of highest­resolution image. This 
prevents the di.usion from bleeding out of the edges of the image. A fast algorithm for this has been 
proposed but not implemented yet. 4 Results Gaussian Projection has been implemented in C++/OpenGL, 
taking advantage of the GPU. The source image had 3000x4500 samples, and the GPU used wasRadeonSapphireX1900(512MB).The 
results sofar, are real-time, and an order of a magnitude faster than polygonal rendering on the same 
system. Conclusive tests of exact frame-rate di.erences, however, are yet to be carried out, and will 
depend on the .nal implementation of the algorithm. References Farooq, S., and Siebert, J. P. 2009. 
Pyramidal multi­view pbr a point-based algorithm for multi-view multi­resolution rendering of large data 
sets from range images. In VISIGRAPP2009 -The International Joint Confer­ence on Computer Vision, Imaging 
and ComputerGraph­ics Theory and Applications, N/A. Zwicker, M., Pfister, H., van Baar, J., and Gross, 
M. 2001. Surface splatting. In SIGGRAPH 01: Proceed­ings of the 28th annual conference on Computer graphics 
and interactive techniques, ACM, New York, NY, USA, 371 378. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599385</article_id>
		<sort_key>840</sort_key>
		<display_label>Article No.</display_label>
		<display_no>84</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>84</seq_no>
		<title><![CDATA[Beyond triangles]]></title>
		<subtitle><![CDATA[gigavoxels effects in video games]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599385</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599385</url>
		<abstract>
			<par><![CDATA[<p>Voxel representations are commonly used for scientific data visualization, but also for many special effects involving complex or fuzzy data (e.g., clouds, smoke, foam). Since voxel rendering permits better and easier filtering than triangle-based representations it is also an efficient high-quality choice for complex meshes (with several triangles per pixel) and detailed geometric data (<i>e.g.</i>, boats in <i>Pirates of the Caribbean</i>).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621611</person_id>
				<author_profile_id><![CDATA[81421599021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crassin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA / Grenoble Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621612</person_id>
				<author_profile_id><![CDATA[81100506206]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fabrice]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neyret]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA / Grenoble Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621613</person_id>
				<author_profile_id><![CDATA[81100198784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sylvain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lefebvre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621614</person_id>
				<author_profile_id><![CDATA[81100048438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sainz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621615</person_id>
				<author_profile_id><![CDATA[81310501633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Elmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Saarland Univ. / MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1507152</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crassin, C., Neyret, F., Lefebvre, S., and Eisemann, E. 2009. Gigavoxels: Ray-guided streaming for efficient and detailed voxel rendering. In <i>ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Beyond Triangles : Gigavoxels Effects In Video Games Cyril Crassin Fabrice Neyret Sylvain Lefebvre 
Miguel Sainz Elmar Eisemann INRIA / Grenoble Univ. INRIA NVIDIA Corporation Saarland Univ. / MPI Informatik 
 Figure 1: Our technique renders complex scenes consisting of billions of voxels in real time and supports 
soft shadows, instancing, high-quality .ltering, and depth-of-.eld effects. Overview Voxel representations 
are commonly used for scienti.c data visu­alization, but also for many special effects involving complex 
or fuzzy data (e.g., clouds, smoke, foam). Since voxel rendering per­mits better and easier .ltering 
than triangle-based representations it is also an ef.cient high-quality choice for complex meshes (with 
several triangles per pixel) and detailed geometric data (e.g., boats in Pirates of the Caribbean). We 
have shown in [Crassin et al. 2009] that highly detailed voxel data can be rendered in high quality at 
real-time rates. The work foreshadows the use of very large volumetric data sets in the con­text of video-games. 
Our system, based on ray-casting of a gen­eralized sparse octree structure on GPU, achieves high rendering 
performance for billions of voxels. To further underline the usefulness in the context of video games, 
this sketch introduces new features of our system, namely free ob­ject instantiation and the mixing with 
existing triangle scenes. We also demonstrate how to render complex visual effects like depth­of-.elds 
or approximated soft shadows in very ef.cient ways, ex­ploiting intrinsic properties of our multi-resolution 
scheme. GigaVoxels rendering In GigaVoxels, voxel data is ef.ciently managed by structuring small voxel 
volumes, so-called bricks, in form of an octree. In such a way, hardware-based tri-linear interpolation 
is combined with a fast traversal, even for semi-transparent volumes. One key fea­ture of our method 
is hybrid update/ray-casting process. During the traversal of the hierarchical volume, rays record information 
about the traversed octree nodes. Consequently, a ray s return value is not only a color value. E.g., 
if visited nodes were not present at the correct resolution a subdivision requests is returned. Because 
the marching is terminated when the volume becomes opaque, intra­volume occlusion is taken into account 
and only visible data is loaded at the needed resolution and involving minimum CPU inter­vention. This 
out-of-core scheme drives a volume cache that man­ages the octree structure as well as the brick storage 
on the GPU in a uni.ed manner. Volume scene instancing We extended GigaVoxels to allow scene-graph like 
instancing in order to create and render large scenes composed of millions of octree-based volumetric 
objects. To allow many semi-transparent objects, our method relies on a BVH acceleration structure. To 
maintain compatibility with our out-of-core update scheme, this structure is integrated in our ray-tracing 
process during the render­ing. A new screen space structure allows us to ef.ciently disam­biguate overlapping 
instances of semi-transparent volumes. Soft shadows and depth-of-.eld effects In [Crassin et al. 2009], 
we demonstrated that multisampling techniques can be ef.ciently approximated by preintegrated 3D mipmapping. 
Quadrilinear .ltering during ray casting was enabled by maintaining a sparse octree with mipmapped bricks. 
Soft shadows are usually approximated by integrating visibility over the surface of the light source. 
This relates to the amount of light that reaches a given point on an object. For a volumetric source, 
the rays towards the source form a bundle that we represent with a single cone. The blocking contribution 
of the object inside this cone is estimated by accumulating opacity. This is done in the same way as 
for eye rays, involving the mipmapping mechanism to approximate the cone integration. The resulting opacity 
value rep­resents the amount of occlusion. This approach provides cheap, but plausible soft shadows while 
avoiding the classical multisampling. Similarly, the mipmapping mechanism can be used to approximate 
a depth-of-.eld effect. The aperture of real cameras causes only points on a focal plane to project to 
a single point on the image, whereas all others produce a circle of confusion. During the eye-ray traversal, 
it is possible to compute this projection size and use it to determine a corresponding mipmap level for 
each sample lookup. The technique results in a cheap depth-of-.eld effect. As a side effect, the use 
of blurred data even accelerates the rendering, which is a powerful property. In conclusion, GigaVoxels 
can be used to render many effects ef­.ciently. Our ray-based update mechanism ensures to upload only 
the data that is actually needed for the image production which lim­its bandwidth consumption as well 
as pressure on the voxel cache. References CRASSIN, C., NEYRET, F., LEFEBVRE, S., AND EISEMANN, E. 2009. 
Gigavoxels : Ray-guided streaming for ef.cient and de­tailed voxel rendering. In ACM SIGGRAPH Symposium 
on In­teractive 3D Graphics and Games (I3D). Copyright is held by the author / owner(s). SIGGRAPH 2009, 
New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599386</article_id>
		<sort_key>850</sort_key>
		<display_label>Article No.</display_label>
		<display_no>85</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>85</seq_no>
		<title><![CDATA[Interactive lighting manipulation application on GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599386</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599386</url>
		<abstract>
			<par><![CDATA[<p>We present a technique for relighting an image such that different areas of the image are illuminated with different combinations of lighting directions. The key idea is to capture illumination data using a lighting apparatus system such as Hawkins et al. [2004], calculate radial basis function interpolation of light constraints specified by users and render the calculated illumination result in realtime using GPU. The application can simulate the result of unnatural lighting conditions, for example, the image of a whole face lit from per pixel view dependence reflection angles or from gazing angles (see Fig. 1, a). The application can also render a high-resolution result at 1920 x 1080 in three to four minutes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Graphics processors</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621616</person_id>
				<author_profile_id><![CDATA[81442617187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Borom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tunwattanapong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621617</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383575</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hawkins, T., Wenger, A., Tchou, C., Gardner, A., G&#246;ransson, F., and Debevec, P. 2004. Animatable facial reflectance fields. In <i>Rendering Techniques 2004: 15th Eurographics Workshop on Rendering</i>, 309--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Lighting Manipulation Application on GPU Borom Tunwattanapong Paul Debevec University of 
Southern California, Institute for Creative Technologies  (a) novel lighting (b) interface of application 
(c) combining result Figure 1: (a): The face lit from per pixel view dependence re.ection angles. (b): 
Interface of the application implemented on Qt. (c): Specifying illumination from light direction map 
and combining the result from several sets of map. Figure 2: Illumination data from 480 light directions. 
 1 Overview We present a technique for relighting an image such that different areas of the image are 
illuminated with different combinations of lighting directions. The key idea is to capture illumination 
data us­ing a lighting apparatus system such as Hawkins et al. [2004], cal­culate radial basis function 
interpolation of light constraints speci­.ed by users and render the calculated illumination result in 
real­time using GPU. The application can simulate the result of unnat­ural lighting conditions, for example, 
the image of a whole face lit from per pixel view dependence re.ection angles or from gaz­ing angles 
(see Fig. 1, a). The application can also render a high­resolution result at 1920 × 1080 in three to 
four minutes. 2 Capturing illumination data We use the same apparatus lighting system as in Hawkins 
et al. [2004] to capture the illumination data. The lighting system is synchronized with a stationary 
high speed camera which can shoot hundreds of frames per second. By rapidly casting the lights to the 
actor in 32 directions in longitude and 15 directions in latitude, we capture 480 illumination data, 
each image corresponds to one light direction (see Fig. 2). 3 Radial basis function interpolation of 
light constraints Users can manipulate lights by applying light constraints to a se­lected area. Each 
constraint speci.es a direction, color and inten­sity of the light illuminating the area. The application 
uses GPU to calculate a radial basis function interpolation of all the lighting constraints. Our interpolation 
algorithm employs Gaussian radial basis function. The exponential calculation is enhanced using GPU which 
is known for parallel processing performance. The result of the interpolation can also be visualized 
in real-time as a false-color image of the directions by mapping X-Y-Z components to R-G-B channels respectively 
(see Fig. 1, c). Users can also specify mul­tiple sets of lighting constraints and combine them to create 
more complicated lighting con.gurations. 4 Calculating the .nal result The interpolated lighting constraints 
specify look-ups into the illu­mination data, with sub-pixel accuracy using bilinear interpolation. For 
each set of lighting constraints, one pixel of the .nal result cor­responds to one direction of light. 
By having more than one set of lighting constraints, one pixel can be lit from several directions, colors 
and intensities of lights. Users can also enable or disable sets of lighting constraints to see the effect 
of an individual set. The application can render a 480 × 270 result in real-time using GPU calculation. 
It also has a multi-thread system that will calculate the result of 960 ×540 in the background which 
takes about one to two second to display the result (see Fig. 1, c). This multi-thread sys­tem improves 
the interaction performance by receiving inputs from users and calculating the result at the same time. 
 References HAWKINS, T., WENGER, A., TCHOU, C., GARDNER, A., G ¨ ORANSSON, F., AND DEBEVEC, P. 2004. 
Animatable facial re.ectance .elds. In Rendering Techniques 2004: 15th Eurographics Workshop on Render­ing, 
309 320. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 
7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599387</article_id>
		<sort_key>860</sort_key>
		<display_label>Article No.</display_label>
		<display_no>86</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>86</seq_no>
		<title><![CDATA[Layered solid texture synthesis from a single 2D exemplar]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599387</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599387</url>
		<abstract>
			<par><![CDATA[<p>In our previous work of lapped solid textures [Takayama et al. 2008], layered (or 'type 1-b') texture exemplars were used to create solid textured models such as strata and cakes. However, no methods have been proposed so far to synthesize this kind of texture automatically. This poster proposes an extension of Kopf et al.'s method [2007] to synthesize such layered solid textures from single 2D exemplars.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621618</person_id>
				<author_profile_id><![CDATA[81335498203]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takayama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621619</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276380</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kopf, J., Fu, C.-W., Cohen-Or, D., Deussen, O., Lischinski, D., and Wong, T.-T. 2007. Solid texture synthesis from 2d exemplars. <i>ACM Trans. Graph. 26</i>, 3, 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360652</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Takayama, K., Okabe, M., Ijiri, T., and Igarashi, T. 2008. Lapped solid textures: filling a model with anisotropic textures. <i>ACM Trans. Graph. 27</i>, 3, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Kenshi Takayama* The University of Tokyo 1 Introduction In our previous work of lapped solid textures 
[Takayama et al. 2008], layered (or type 1-b ) texture exemplars were used to cre­ate solid textured 
models such as strata and cakes. However, no methods have been proposed so far to synthesize this kind 
of tex­ture automatically. This poster proposes an extension of Kopf et al. s method [2007] to synthesize 
such layered solid textures from single 2D exemplars. Layered Solid Texture Synthesis from a Single 
2D Exemplar Takeo Igarashi The University of Tokyo, JST/ERATO  Figure 3: The cause of the sweeping artifact. 
 output is a 3D solid texture whose cross sections parallel to the depth direction (assumed to be the 
z-direction) always look similar to the 2D exemplar. Figure 1: The problem of layered solid texture synthesis. 
it starts with proper depth values (Fig. 2(right)). We perform the two-pass synthesis process (search 
and optimization) only in the x­and y-directions, because we have only a single 2D exemplar cor­responding 
to those directions.  ered textures. Figure 2: The additional depth map channel of the 2D exemplar (left) 
and the altered random initialization of the 3D synthesis vol­ume at the coarsest level (right). However, 
the above scheme can easily lead to a se­vere sweeping artifact as shown in the inset. This is because 
it accepts a simple sweep of the 2D ex­emplar in a direction orthogonal to the z-direction and oblique 
to both the x-and y-directions, as an optimized synthesis result. In other words, for each voxel, the 
two neighborhoods of the current synthesis volume cor­responding to the x-and y-directions are very likely 
to best match with exactly the same neighborhood in the 2D exemplar (Fig. 3). *E-mail: kenshi@ui.is.s.u-tokyo.ac.jp 
 Figure 5: Results of our layered solid texture synthesis algorithm. References KOPF, J., FU, C.-W., 
COHEN-OR, D., DEUSSEN, O., LISCHIN-SKI, D., AND WONG, T.-T. 2007. Solid texture synthesis from 2d exemplars. 
ACM Trans. Graph. 26, 3, 2. TAKAYAMA, K., OKABE, M., IJIRI, T., AND IGARASHI, T. 2008. Lapped solid textures: 
.lling a model with anisotropic textures. ACM Trans. Graph. 27, 3, 1 9. Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599388</article_id>
		<sort_key>870</sort_key>
		<display_label>Article No.</display_label>
		<display_no>87</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>87</seq_no>
		<title><![CDATA[Oriental stylization with strokes and shades]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599388</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599388</url>
		<abstract>
			<par><![CDATA[<p>In oriental paintings, artists have developed a unique style that exploits the effects of the dispersion of the ink, composed of soot and glue, onto absorbent paper. These artists produce these effects purposely by manipulating the ink concentration, stroke speed and brush angle. We describe these artistic styles of oriental painting and show how we render a single image in the oriental painting style. Our work is to take a single image as input and produce an oriental brushwork-like image automatically as a result. This nonphotorealistic rendering do not have richness of physical painting system such as 3-D brushes but reproduce artistic style of paintings to an image with a speed. In recent years, oriental brushwork rendering have been applied for 3-D objects e.g. <i>Non-photorealistic rendering in Chinese painting of animals by Yeh and Ouhyoung 2002</i> and stereo images e.g. <i>Humanistic oriental art created using automated computer pocessing and non-photorealistic rendering</i> by <i>Cheok et al. 2007</i> that require geometric details and depth information, but our method proposes a system that reproduce the styles and effects of oriental paintings from single 2-D image by using a set of image processing techniques. Figure 1 shows an input image and the reproduced oriental paintings. We focus on reproducing the stroke drawing and artistic shades that are essential in conventional oriental paintings.</p> <p>Some artistic styles of painting exploit the variety of lines and deep shades at once, giving a charming feel to the oriental brushwork. One general technique is "gu-ru law"Figure 2(a), in which artists draw tufted thin lines for boundaries and geometric details, and brush the interior with abstract but contrasting shades of ink. Another artistic style shades the whole objects instead of drawing its boundary with lines Figure 2(b), and paints near boundary regions with highly contrasting intensities.</p> <p>Our overall framework consists of two modules: stroke drawing and artistic shading Figure 3. These modules, essential to oriental painting, are generated using a set of techniques. <i>Abstraction:</i> Given an input image, we simplify it for geometric abstraction and color quantization using bilateral filtering. The abstracted image is then used as the input for our stroke drawing and artistic shading modules. <i>Stroke drawing:</i> We extract and draw representative lines for boundaries and geometric details. Line extraction includes selecting feature points, discarding small features, and clustering and linking similar points. Next, the spline curves are fitted to the linked points after the linked points are once again subdivided into several clusters according to their size. Line fitting produces smooth curves of appropriate length and curvature, similar to human-drawn strokes. Then, the curves' thickness is defined as proportional to the curvatures. <i>Artistic shading:</i> While oriental ink interacts with absorbent paper, ink disperses on the paper as water flows, the concentration of ink leaves bright-and-dark shades, and the residual ink disperses along the direction of the paper's fibers. We reproduce these effects by following steps. First, filtering over time is used to reproduce the ink dispersion effects when water spreads and stops, by applying blurring and sharpening filters consecutively. Next, we reproduce the bright and dark shades that arise from irregular ink concentration by stretching lightness contrast nonlinearly based on intensity values from blurred images. Finally, we reproduce the local patterns due to dispersion along the paper's fibers by using texture masks that are similar to textures of absorbent paper fibers. <i>Composition:</i> We compose the final results by combining stroke drawing and artistic shading, and by adding textures of absorbent papers for realistic effects.</p> <p>We produce the final oriental painting rendering as a composite of the results of stroke drawing and artistic shading from a single 2-D image. Figure 3 shows that strokes drawing; (a) is a map of edge by canny's edge filter, (b) is for clustering, linking sampled features and discarding some groups which have few features, (c) presents a thickness in each group, artistic shading; (d) Filtering overtime from input image, (e) contrast stretching, and (f) applying the mask of paper-fiber.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621620</person_id>
				<author_profile_id><![CDATA[81547278156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Seungju]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chung-ang University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621621</person_id>
				<author_profile_id><![CDATA[81442597537]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kyoungju]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chung-ang University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Oriental Stylization with Strokes and Shades Seungju Bang* Kyoungju Park Chung-ang University Chung-ang 
University  Figure 1: Oriental stylization. (a)input image, (b) Result image (color), (c) Result image 
(gray) Abstract In oriental paintings, artists have developed a unique style that ex­ploits the effects 
of the dispersion of the ink, composed of soot and glue, onto absorbent paper. These artists produce 
these effects purposely by manipulating the ink concentration, stroke speed and brush angle. We describe 
these artistic styles of oriental painting and show how we render a single image in the oriental painting 
style. Our work is to take a single image as input and produce an oriental brushwork-like image automatically 
as a result. This non­photorealistic rendering do not have richness of physical painting system such 
as 3-D brushes but reproduce artistic style of paintings to an image with a speed. In recent years, oriental 
brushwork ren­dering have been applied for 3-D objects e.g. Non-photorealistic rendering in Chinese painting 
of animals by Yeh and Ouhyoung 2002 and stereo images e.g. Humanistic oriental art created us­ing automated 
computer pocessing and non-photorealistic render­ing by Cheok et al. 2007 that require geometric details 
and depth information, but our method proposes a system that reproduce the styles and effects of oriental 
paintings from single 2-D image by us­ing a set of image processing techniques. Figure 1 shows an input 
image and the reproduced oriental paintings. We focus on repro­ducing the stroke drawing and artistic 
shades that are essential in conventional oriental paintings. Figure 2: Real oriental painting, Some 
artistic styles of painting exploit the variety of lines and deep shades at once, giving a charming feel 
to the oriental brushwork. One general technique is gu-ru law Figure 2(a), in which artists draw tufted 
thin lines for boundaries and geometric details, and brush the interior with abstract but contrasting 
shades of ink. An­other artistic style shades the whole objects instead of drawing its boundary with 
lines Figure 2(b), and paints near boundary regions with highly contrasting intensities. Our overall 
framework consists of two modules: stroke drawing and artistic shading Figure 3. These modules, essential 
to orien­tal painting, are generated using a set of techniques. Abstraction: Given an input image, we 
simplify it for geometric abstraction and color quantization using bilateral .ltering. The abstracted 
image is *e-mail: formlife1225@naver.com e-mail:kjpark@cau.ac.kr Figure 3: Procedure of generating brushwork-like 
image: .rst col­umn is for strokes steps, second column is for shades steps; (a) canny s edge map, (b) 
clustering, linking and discarding (c)Making a thickness,(d) Filtering overtime, (e) contrast stretching, 
(f) apply­ing the mask of .ber then used as the input for our stroke drawing and artistic shading modules. 
Stroke drawing: We extract and draw representative lines for boundaries and geometric details. Line extraction 
includes se­lecting feature points, discarding small features, and clustering and linking similar points. 
Next, the spline curves are .tted to the linked points after the linked points are once again subdivided 
into sev­eral clusters according to their size. Line .tting produces smooth curves of appropriate length 
and curvature, similar to human-drawn strokes. Then, the curves thickness is de.ned as proportional to 
the curvatures. Artistic shading: While oriental ink interacts with absorbent paper, ink disperses on 
the paper as water .ows, the con­centration of ink leaves bright-and-dark shades, and the residual ink 
disperses along the direction of the paper s .bers. We reproduce these effects by following steps. First, 
.ltering over time is used to reproduce the ink dispersion effects when water spreads and stops, by applying 
blurring and sharpening .lters consecutively. Next, we reproduce the bright and dark shades that arise 
from irregular ink concentration by stretching lightness contrast nonlinearly based on intensity values 
from blurred images. Finally, we reproduce the local patterns due to dispersion along the paper s .bers 
by using texture masks that are similar to textures of absorbent paper .bers. Composition: We compose 
the .nal results by combining stroke drawing and artistic shading, and by adding textures of absorbent 
papers for realistic effects. We produce the .nal oriental painting rendering as a composite of the results 
of stroke drawing and artistic shading from a single 2-D image. Figure 3 shows that strokes drawing; 
(a) is a map of edge by canny s edge .lter, (b) is for clustering, linking sampled features and discarding 
some groups which have few features , (c) presents a thickness in each group, artistic shading; (d) Filtering 
overtime from input image, (e) contrast stretching, and (f) applying the mask of paper-.ber. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599389</article_id>
		<sort_key>880</sort_key>
		<display_label>Article No.</display_label>
		<display_no>88</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>88</seq_no>
		<title><![CDATA[Painterly caricature maker]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599389</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599389</url>
		<abstract>
			<par><![CDATA[<p>This study describes a fully automated system for generating caricature images by using a painterly rendering method. This system transforms photos into caricature images automatically. A few similar approaches have been proposed by other researchers including [Gooch et al. 2004] and [Liang et al. 2002]. These methods, however, did not produce satisfying results, as the caricatures produced did not resemble handiwork. By simulating the brush strokes used by painters [Park et al. 2006], we reproduced the brush painting technique and incorporated it into our caricature system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Fine arts</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621622</person_id>
				<author_profile_id><![CDATA[81442611228]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoon-Seok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETRI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621623</person_id>
				<author_profile_id><![CDATA[81409592301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[In-Kwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621624</person_id>
				<author_profile_id><![CDATA[81319494904]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bon-Ki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ETRI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378090</ref_obj_id>
				<ref_obj_pid>378040</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cootes, T. F., Edwards, D. J., and Taylor, S. J. 2001. Active appearance models. <i>IEEE Trans. Pattern Anal. Mach. Intell. 23</i>, 6 (Jun), 681--685.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>966133</ref_obj_id>
				<ref_obj_pid>966131</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gooch, B., Erinhard, E., and Gooch, A. 2004. Human facial illustrations: creation and psychophysical evaluation. <i>ACM Transactions on Graphics 23</i>, 1 (January), 27--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826637</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Liang, L., Chen, H., Xu, Y.-Q., and Shum, H.-Y. 2002. Examplebased caricature generation with exaggeration. In <i>Proceedings of the 10th Pacific Conference on Computer Graphics and Applications</i>, 386.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179973</ref_obj_id>
				<ref_obj_pid>1179849</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Park, J. W., Koo, B. K., Barry, R., and Hong, S. D. and Yoon, K. H., 2006. Painterly rendering with designed imperfection. ACM SIGGRAPH 2006 Sketches, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Painterly Caricature Maker Yoon-Seok Choi* In-Kwon Lee Bon-KiKoo ETRI Yonsei University ETRI Figure 
1: Generating a caricature using painterly rendering. 1 Introduction This study describes a fully automated 
system for generating car­icature images by using a painterly rendering method. This sys­tem transforms 
photos into caricature images automatically.Afew similar approacheshavebeen proposedbyother researchers 
includ­ing [Gooch et al. 2004] and [Liang et al. 2002]. These methods, however, did not produce satisfying 
results, as the caricatures pro­duced did not resemble handiwork. By simulating the brush strokes used 
by painters [Park et al. 2006], we reproduced the brush paint­ing technique and incorporated it into 
our caricature system. We trained the AAM [Cootes et al. 2001], which contains a sta­tistical model of 
the shape and texture of theface, using theface data pertainingto150 peopletoextract feature pointsoffaces 
that might enable more accurate automatic caricaturing. The most im­portant component of our system are 
the painterly rendering and the fastface -feature -extraction method; our system produces real-time caricature 
images. 2 Properties of our system Our system is different from previously developed one because of 
two reasons. Firstly, we used a face recognition method and a greedy search engine includingaperturbation 
method such as kick move to obtainthe initialface featurepointswhichareusedinthe AAM; this increases 
the accuracyof the extraction. Secondly, we adopteda painterly rendering method so that images obtained 
from our system resemble handiwork. The system accomplishes this through the following operations. First, 
we extract the feature points including eyes, nose, lips, and face contour using AAM, which is known 
to be relatively stable to be used for matching and trackingfaces. However, AAM is af­fected by change 
in illuminations, poses, and expressions; further, searching the database is expensive. Moreover, unless 
the initial solution is well set up, the facial feature points might not be de­termined. The brute-force 
method is the fundamental method that determinesthe initial solutionoftheface contourtraverseanditis 
carried out several timesby setting theaveragefacial contour as the constant interval on an whole image 
[Cootes et al. 2001] and the location, producing the best matching, is chosen as the initial solu­tion. 
However, the iterative process has to be performed over the whole image and it is expensive due to optimization. 
In order to *e-mail:ys-choi@etri.re.kr solve the problem, we usedface recognition and heuristic search­ing 
method to increase the search speed and the accuracyof feature points extraction, respectively. The initial 
solutionis determined usingaface recognition method such as Haar, and the AAMextracts theface feature 
points on the basis of the solution. If the result of AAM process has not reached a terminating condition, 
the initial solution is perturbed to obtain the another initial solution for AAM. This process is iterated 
until a good overall match is obtained. Subsequently, by comparing the extracted feature points and the 
average feature points of theface images that are used to train the AAM, weexaggerate theface images 
to obtain the super-portrait. Then, we transformed the exaggerated image into a single-color image by 
using the illustration proposed by Gooch [Gooch et al. 2004]; Goochs methods is more appropriate for 
painterly render­ing than anyother color conversion method. Finally, the caricature is generated by using 
the painterly rendering method. The whole process is shown in .gure 1. 3 Conclusions We developed a 
fully automated system for generating caricatures in real-time by using a painterly rendering method. 
Using our sys­tem, users can create their own artistic caricatures more conve­niently in various environments. 
1 References COOTES, T. F., EDWARDS, D. J., AND TAYLOR, S. J. 2001. Active appearance models. IEEETrans.Pattern 
Anal. Mach. Intell.23,6(Jun), 681 685. GOOCH,B.,ERINHARD,E., AND GOOCH,A. 2004. Humanfacial illus­trations:creation 
and psychophysical evaluation. ACMTransactions on Graphics 23,1(January), 27 44. LIANG,L.,CHEN,H.,XU,Y.-Q., 
AND SHUM,H.-Y. 2002. Example­based caricature generation with exaggeration. In Proceedings of the 10thPaci.c 
Conference on ComputerGraphics and Applications, 386. PARK, J. W., KOO, B. K., BARRY, R., AND HONG, S. 
D.AND YOON, K. H., 2006. Painterly rendering with designed imperfection. ACM SIGGRAPH 2006 Sketches, 
July. 1 This work was supported by the IT R&#38;D program of MKE/MCST/IITA. [2009-S-001-01,Development 
of Full 3D Recon­structionTechnology for Broadcasting Communication Fusion] Copyright is held by the 
author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599390</article_id>
		<sort_key>890</sort_key>
		<display_label>Article No.</display_label>
		<display_no>89</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>89</seq_no>
		<title><![CDATA[RACBVHs]]></title>
		<subtitle><![CDATA[random-accessible compressed bounding volume hierarchies]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599390</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599390</url>
		<abstract>
			<par><![CDATA[<p>Bounding volume hierarchies (BVHs) are widely used to accelerate the performance of various geometric and graphics applications. These applications include ray tracing, collision detection, visibility queries, dynamic simulation, and motion planning. These applications typically precompute BVHs of input models and traverse the BVHs at runtime in order to perform intersection or culling tests.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621625</person_id>
				<author_profile_id><![CDATA[81442603931]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tae-Joon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621626</person_id>
				<author_profile_id><![CDATA[81442602822]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bochang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621627</person_id>
				<author_profile_id><![CDATA[81442593431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Duksu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621628</person_id>
				<author_profile_id><![CDATA[81100019061]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sung-Eui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[KAIST (Korea Advanced Institute of Science and Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1313121</ref_obj_id>
				<ref_obj_pid>1313046</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yoon, S.-E., and Lindstrom, P. 2007. Random-accessible compressed triangle meshes. <i>IEEE Trans. on Visualization and Computer Graphics (Proc. Visualization) 13</i>, 6, 1536--1543]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 RACBVHs: Random-Accessible Compressed Bounding Volume Hierarchies Tae-Joon Kim Bochang Moon Duksu Kim 
Sung-EuiYoon KAIST(Korea Advanced Instituteof Science andTechnology) 1 Introduction Bounding volume 
hierarchies (BVHs) are widely used to acceler­ate the performance of various geometric and graphics applications. 
These applications include ray tracing, collision detection, visibility queries, dynamic simulation, 
and motion planning. These applica­tions typically precompute BVHs of input models and traverse the BVHs 
at runtime in order to perform intersection or culling tests. A major problem with using BVHs is that 
BVHs require large amounts of the memory space. Therefore, BVHs of large models consisting of hundreds 
of millions of triangles can take tens of giga­bytes of space. Moreover, the typical data access pattern 
on BVHs cannot be determined at the preprocessing time and is random at run­time. Therefore, accessing 
BVHs at runtime can have low I/O ef.­ciency and cache utilization. An aggravating trend is that the growth 
rate of the data access speed is signi.cantlyslower than that of the processing speed. There­fore, the 
problem of high storage requirements and low I/O ef.­ciency/cache utilization of BVHs will become more 
pronounced in the future. Several approaches have been developed to address this problem. One class of 
methods uses compact in-core BV representations by using quantizedBVinformationorbyexploitingthe connectivityin­formation 
of an original mesh and coupling the mesh and the BVH. Another class of methods stores BV nodes in a 
cache-coherent man­ner to improve cache utilization and, thus, improve the performance of traversing 
BVHs. However, due to the wideninggap between data access speeds and processing speeds, prior work may 
not provide enough reduction in storage requirements nor achieve high I/O ef.­ciency during the BVH traversal. 
2 Our Approach In order to ef.ciently access BVHs and improve the performance of various applications 
using BVHs, we propose a novel BVH com­pression and decompression method supporting random access. We 
compress BVs of a BVH by sequentially reading BVs in the BV lay­out of the BVH.We choose our compression 
method to preserve the original layout of the BVH in order to achieve the high cache uti­lization which 
the original layouts may maintain. We decompose the original layout of the BVH into a set of clusters. 
We assign consec­utive BVs in the BV layout to each cluster and set each cluster to have the same number 
of BVs to quickly identify a cluster contain­ing a BV node requested by an application at runtime. We 
compress each cluster separately from other clusters so that the clusters can be decompressed in any 
order. We de.ne an atomic BVH access API supporting transparent and random access on the compressed BVHs. 
The runtime framework fetches and decompresses the cluster into an in-core representation. Based on our 
in-core representation, we can very ef.ciently support random access to applications. Our run­time BVH 
access framework is guaranteed to return the correct BV information of the requested data when applications 
access the com­pressed data via our BVH access API. We employthe RACM representation [Yoon and Lindstrom 
2007] to further reduce the storage requirement of meshes, which are used together with BVHs for various 
applications. We achive up to a 12 : 1 compression ratio in our benchmark models. We implement two different 
applications, ray tracing and collision detection, to verify the bene.ts of our proposed method. In 
 (a) St. Matthew scene (b) Lucyand CAD turbine models Figure 1: The left image shows the result of ray 
tracing using our random-accessible compressed bounding volume hierarchies (RACBVHs) of St. Matthew model 
consisting of 128M triangles. The right image showsa frame duringa rigid-body simulation using collision 
detection between two models including the Lucy model consisting of 28 M triangles.ByusingRACBVHs,wecan 
reducethestorage requirementby afactorof10:1 and, more importantly, improve the performance of ray tracing 
and collision detection by up to a factor of four over using uncompressed data. Our representation enables 
ray tracing and collision detection with large models using commodity hardware. Figure 2: Hubo and Power 
Plant Models: The Hubo robot model (16 K triangles) is placed in the upper left corner of the power plant 
model (13 M triangles). The entire powerplant modelisshownonthe right.Weimprovethe performancebyafactorof2:1 
for collision detection using our RACBVH representation. these applications, we improve the performance 
by up to a factor of four over using uncompressed data. 3 Advantages of Our Approach 1. Wide applicability: 
The provided BVH access API allows var­ious applications to transparently access the compressed BVHs. 
Moreover, our BVH access API supports random access and does not restrict the access pattern of BVH-based 
applications. 2. Low storage requirement: Our RACBVH representation has up to a 12:1 compression ratio 
compared to an uncompressed BV representation. 3. Improved performance: We can achieve up to a 4:1 perfor­mance 
improvement on our tested applications over using un­compressed data.  References YOON, S.-E., AND 
LINDSTROM, P. 2007. Random-accessible compressed triangle meshes. IEEE Trans. on Visualization and Computer 
Graphics (Proc. Visualization) 13, 6, 1536 1543. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599391</article_id>
		<sort_key>900</sort_key>
		<display_label>Article No.</display_label>
		<display_no>90</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>90</seq_no>
		<title><![CDATA[Reflection model of metallic paints for reflectance acquisition]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599391</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599391</url>
		<abstract>
			<par><![CDATA[<p>Nowadays metallic paints are used in many situations. Although a lot of industrial products are painted because of the significant appearance, the reflection of the metallic paint is very complex and it is difficult to generate a photo-realistic image of a particular metallic paint. Recently, Rump et al. [Rump et al. 2008] proposed a method to acquire the reflectance and to generate photo-realistic images of metallic paints. They used BTF to capture and represent flakes in metallic paints, however, it is hard to capture and to store. In this paper, we propose a simple model to express the metallic paints including the sparkling effect of the flakes in metallic paints.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621629</person_id>
				<author_profile_id><![CDATA[81100395213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621630</person_id>
				<author_profile_id><![CDATA[81100515686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rump, M., M&#252;ller, G., Sarlette, R., Koch, D., and Klein, R. 2008. Photo-realistic rendering of metallic car paint from image-based measurements. <i>Computer Graphics Forum 27</i>, 2, 527--536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Re.ection Model of Metallic Paints for Re.ectance Acquisition Masashi Baba Hiroshima City University 
 1 Introduction Nowadays metallic paints are used in many situations. Although a lot of industrial products 
are painted because of the signi.cant ap­pearance,the re.ection of the metallic paint is very complex 
and it is dif.cult to generate a photo-realistic image of a particular metallic paint. Recently, Rump 
et al.[Rump et al. 2008] proposed a method to acquire the re.ectance and to generate photo-realistic 
images of metallic paints. They used BTF to capture and represent .akes in metallic paints, however, 
it is hard to capture and to store. In this paper, we propose a simple model to express the metallic 
paints including the sparkling effect of the .akes in metallic paints. 2 Re.ection Model We assume the 
re.ection of the metallic paints as shown in .gure 1. There are four kinds of re.ection in metallic paints; 
specular and diffuse re.ection of the color layer, specular re.ection of the .akes, and mirror re.ection 
at the surface of the metallic paints. We modeled all of them separately. The re.ection of .akes in metallic 
paints is modeled as a specular re.ection. In addition, the normal directions of .akes are modeled differently 
from the normal direc­tion of the object s surface. The re.ection of the .akes is modeled as follows. 
If (a)=Kf cost (a) (1) where Kf is a re.ectance of .ake, t is a power coef.cient, a is the angle between 
the normal vector and the half way vector of light direction and viewing direction. The sparkling effect 
of metallic paints can be represented by the slant of the .ake from the surface direction. So, we model 
the nor­mal distribution of the .akes by using Gaussian function. 1 (a'- µ)2 p(a')= v exp- (2) 2ps 
2s2 (c) (d) Figure 1: Re.ection model of metallic paints. (a) is a specular re.ection and (b) is a 
diffuse re.ection of the color layer in metallic paints. (c) shows the re.ection of .akes in metallic 
paints. (d) is a mirror re.ection at the surface of the metallic paints. Naoki Asada Hiroshima City University 
 where µ is an average and s2 is a variance of the normal distribu­tion of the .akes, a' is the difference 
in angle from a of the surface. Finally, the re.ection model of the metallic paints is expressed as follows. 
N Io(a,f)=Ks cosm(a)+Kd cosn(f)+ 1 Kf cost (a+ak' ) (3) N . k=1 where f is an angle between the normal 
and the light direction, N is the sampling number per pixel. 3 Experimental Results We apply an acquiring 
method to a real object which is painted with metallic paints. Figure 2 shows a real image of the metallic 
painted object, and .gure 3 shows a generated image by using acquired parameters of our re.ection model. 
The generated image is very similar to the real image, and can represent the sparkling effect of the 
.akes in metallic paints.  Figure 3: Generated image of the real object.  4Conclusion We proposed 
a new re.ection model of metallic paints which can represent the effect of the .akes in metallic paints. 
The model can be used for the re.ectance acquisition of real metallic paints. In the experiments, we 
estimated re.ectance properties of real metallic paints, and made realistic images of metallic painted 
objects using our model. References M¨KLEIN, R. 2008. Photo-realistic rendering of metallic car paint 
from image-based measurements. Computer Graphics Forum 27, 2, 527 536. RUMP, M., ULLER, G., SARLETTE, 
R., KOCH, D., AND Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599392</article_id>
		<sort_key>910</sort_key>
		<display_label>Article No.</display_label>
		<display_no>91</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>91</seq_no>
		<title><![CDATA[Single-pass rendering of composable volumetric lens effects]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599392</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599392</url>
		<abstract>
			<par><![CDATA[<p>We describe an efficient single-pass rendering approach for composable 3D volumetric lenses. Composing rendering effects by intersecting multiple 3D lenses is a logical and intuitive extension of the Magic Lens metaphor and volumetric lenses. However, 3D lens composition was once considered intractable and recent multi-lens approaches require a number of passes that can grow exponentially with lens count. They generally involve substantial per-frame data structure generation or advanced techniques such as depth peeling. In contrast, we summarize a simple and effective technique that renders intersecting lenses of various shapes and effects in a single pass, does not require the maintenance of costly data structures, and can easily be incorporated into existing real-time rendering systems. It also supports more flexibility in the way complex lens effects are combined by using shade tree concepts to build composite shader programs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621631</person_id>
				<author_profile_id><![CDATA[81435608486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jan-Phillip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tiesel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Louisiana at Lafayette]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621632</person_id>
				<author_profile_id><![CDATA[81100223773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Borst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Louisiana at Lafayette]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Best, C. M., and Borst, C. W. 2008. New rendering approach for composable volumetric lenses. In <i>IEEE Virtual Reality 2008</i>, 189--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tiesel, J.-P. 2009. <i>Composable Visual and Temporal Lens Effects in a Scene Graph-based Visualization System</i>. Master's thesis, University of Louisiana at Lafayette.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Single-Pass Rendering of Composable Volumetric Lens Effects Jan-Phillip Tiesel * Christoph W. Borst 
* University of Louisiana at Lafayette 1 Introduction We describe an ef.cient single-pass rendering 
approach for com­posable 3D volumetric lenses. Composing rendering effects by in­tersecting multiple 
3D lenses is a logical and intuitive extension of the Magic Lens metaphor and volumetric lenses. However, 
3D lens composition was once considered intractable and recent multi-lens approaches require a number 
of passes that can grow exponentially with lens count. They generally involve substantial per-frame data 
structure generation or advanced techniques such as depth peeling. In contrast, we summarize a simple 
and effective technique that renders intersecting lenses of various shapes and effects in a sin­gle pass, 
does not require the maintenance of costly data structures, and can easily be incorporated into existing 
real-time rendering sys­tems. It also supports more .exibility in the way complex lens ef­fects are combined 
by using shade tree concepts to build composite shader programs. 2 Implementation Current GPUs allow 
per-fragment evaluation of complex shader ef­fects. This enables us to apply different effects to individual 
frag­ments based on their location with respect to 3D lenses in the ren­dered scene. To implement different 
shading effects for geometry inside of lens volumes and their intersections, the computation of the .nal 
pixel color in the fragment shader has to be delayed until the membership to a certain region has been 
determined. For each lens, we calculate Mlens, which transforms from lens co­ordinates to eye coordinates, 
and pass its inverse to the shader pro­gram. A vertex shader then uses M-1 and the current modelview 
lens matrix Mmodelview to transform the position pmodel of an incom­ing vertex from the model coordinate 
system of the rendered geom­etry to a Cartesian lens coordinate system. -1 plens = MMmodelview pmodel 
lens Hardware interpolation then provides a per-fragment coordinate that allows us to perform an ef.cient 
in-out test to determine if the fragment falls inside the boundaries of a particular lens. While it is 
suf.cient for many applications to employ a range test to create simple lens shapes (e.g., boxes and 
ellipsoids), more sophisticated tests can be performed. Figure 1 includes an extrusion lens shape generated 
using 2D texture lookup for a threshold test. When in-out tests for all active lenses are evaluated in 
the fragment shader, results are combined into a bitmask that expresses the mem­bership of the fragment 
to individual lens regions (e.g., for a scene with three lenses, a fragment that lies inside all of them 
would yield a bitmask of 111, while a fragment that lies inside the second and outside of the other lenses 
would yield 010). The number of possi­ble regions is 2n, for a scene with n lenses. Depending on computed 
bitmask, the fragment shader branches to the respective set of instructions to implement the (composite) 
shading effect. These instructions modify global shader attributes (e.g., surface material properties, 
specular BRDF term, or current effect blending mode) to successively add individual effects before the 
attributes are combined into the .nal fragment color. *e-mail: {jpt4246|cborst}@cacs.louisiana.edu  
Figure 1: Example scene showing three volumetric lenses using cubic, spherical, and texture-de.ned shapes. 
Lens blending modes and order can be changed interactively. Dragon model is property of Stanford University 
Computer Graphics Laboratory. A composite shader factory constructs GLSL code for a single GPU program 
using an ordered list of lenses and their respective effect signatures with blending options. Renaming 
of internal variables and uniform parameters is done automatically for every effect in­stance used by 
the factory class. We use shade tree concepts to combine chains of per-lens shader effects into a composite 
sequen­tial representation. Details on how individual lens effects are de­.ned and composed in our system 
can be found in [Tiesel 2009]. 3 Results and Outlook The scene shown in Figure 1 contains 100,000 primitives 
and ren­ders at 192 fps (1280x1024 pixels, 16x antialiasing and anisotropic .ltering) on our test machine 
(NVIDIA 9800 GX2, Intel Core2 2.4GHz). Doubling the number of lenses in this scene decreases frame rate 
to 115 fps, a promising result compared to other meth­ods. Moving and intersecting lens volumes does 
not affect perfor­mance, which is a substantial improvement compared to our previ­ous technique [Best 
and Borst 2008]. We imagine a hybrid solution that combines both the presented technique and our previous 
ap­proach to support vertex-level effects exclusive to a single region (e.g., displacement mapping), 
which cannot be rendered using the approach summarized here. We show in [Tiesel 2009] how multiple GPU 
programs are used for correct rendering of object-level effects not supported at the fragment level (e.g., 
x-ray vision). References BEST, C. M., AND BORST, C. W. 2008. New rendering approach for composable 
volumetric lenses. In IEEE Virtual Reality 2008, 189 192. TIESEL, J.-P. 2009. Composable Visual and Temporal 
Lens Effects in a Scene Graph-based Visualization System. Master s thesis, University of Louisiana at 
Lafayette. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 
3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599393</article_id>
		<sort_key>920</sort_key>
		<display_label>Article No.</display_label>
		<display_no>92</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>92</seq_no>
		<title><![CDATA[Variance minimization light probe sampling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599393</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599393</url>
		<abstract>
			<par><![CDATA[<p>We present a technique for sampling the light probe image using variance minimization. The technique modifies median cut algorithm for light probe sampling [Debevec 2005] so that the variance of each region is minimized. The algorithm is fast, efficient, and easy to implement.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Photometry</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621633</person_id>
				<author_profile_id><![CDATA[81442619459]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kuntee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Viriyothai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621634</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187029</ref_obj_id>
				<ref_obj_pid>1186954</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 2005. A Median Cut Algorithm for Light Probe Sampling <i>(SIGGRAPH Poster 2005)</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383909</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CLINE, D., EGBERT, P. K., TALBOT, J. F., AND CARDON, D. L. 2006. Two Stage Importance Sampling for Direct Lighting <i>(EGSR 2006)</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Variance Minimization light probe sampling Kuntee Viriyothai Paul Debevec ABSTRACT We present a technique 
for sampling the light probe image using variance minimization. The technique modifies median cut algorithm 
for light probe sampling [Debevec 2005] so that the variance of each region is minimized. The algorithm 
is fast, efficient, and easy to implement. Introduction The median cut algorithm for light probe sampling 
does not behave optimally, when a region contains two main unequal intensity light sources. The median 
cut algorithm tends to cut through the brighter light source, so the sampled lights are not placed at 
the right places. To solve this, a variance in a region is taken into account. This was done before with 
BRDF in a two stage importance sampling technique [Cline et al. 2006]. Therefore, this work attempts 
to minimize the variance of sub-region so that the lights are grouped as much as possible. Figure 1: 
The Grace Cathedral light probe is divided into 64 regions using the variance minimization light probe 
sampling algorithm. Algorithm The algorithm divides the entire light probe image into 2n regions as follows: 
1. Add the entire light probe image to the region list as a single region. 2. For each region in the 
list, subdivide such that the maximum of two sub-region s variances is minimized. 3. If the number of 
iterations is less than n, return to step 2. 4. Place a light source at the centroid of each region, 
and set the light source color to the sum of the pixel values within the region.  The variance of each 
region r can be calculated from the following equation: 2 Variance = .Lpdp p.r Lp is the light energy 
weighting factor of p dp is the distance from p to the centriod of r Copyright is held by the author 
/ owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
University of Southern California Implementation The calculation of variance of each region can be accelerated 
by using 5 summed area tables. Results Fig. 1 shows the 64 lights sampled from the Grace Cathedral lighting 
environment. Fig.2 shows the case that median cut algorithm does not behave optimally but variance minimization 
does. Fig.3 shows the diffuse scene rendered with different numbers of sampled light from the Grace Cathedral 
light probe. Using 256 sampled lights produces a close result to the Monte-Carlo solution. (a) Source 
(b) Median (c) Variance Figure 2: (a) the environment with two main different intensity light sources. 
(b) Cut made by median cut approach (c) Result from variance minimization approach (a) Median 64 lights 
(b) Variance 64 lights (c) Variance 256 lights (d) Monte Carlo rendering  Figure 3: (a) (c) Renderings 
of Grace Cathedral lighting environment using different approaches and numbers of lights. (d) A Monte-Carlo 
solution considered as a ground truth image Reference DEBEVEC, P. 2005. A Median Cut Algorithm for Light 
Probe Sampling (SIGGRAPH Poster 2005) CLINE, D., EGBERT, P. K., TALBOT, J. F., AND CARDON, D. L. 2006. 
Two Stage Importance Sampling for Direct Lighting (EGSR 2006)  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599394</article_id>
		<sort_key>930</sort_key>
		<display_label>Article No.</display_label>
		<display_no>93</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>93</seq_no>
		<title><![CDATA[Virtual video camera]]></title>
		<subtitle><![CDATA[image-based viewpoint navigation through space and time]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599394</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599394</url>
		<abstract>
			<par><![CDATA[<p>We present an image-based rendering system to viewpoint-navigate through space and time of complex real-world, dynamic scenes. Our approach accepts unsynchronized, uncalibrated multi-video footage as input. Inexpensive, consumer-grade camcorders suffice to acquire arbitrary scenes, e.g., in the outdoors, without elaborate recording setup procedures. Instead of scene depth estimation, layer segmentation, or 3D reconstruction, our approach is based on dense image correspondences, treating view interpolation uniformly in space and time: spatial viewpoint navigation, slow motion, and freeze-and-rotate effects can all be created in the same fashion. Acquisition simplification, generalization to difficult scenes, and space-time symmetric interpolation amount to a widely applicable Virtual Video Camera system.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[freeviewpoint video]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
			<kw><![CDATA[video-based rendering]]></kw>
			<kw><![CDATA[viewpoint navigation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621635</person_id>
				<author_profile_id><![CDATA[81365598480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lipski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621636</person_id>
				<author_profile_id><![CDATA[81331498092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Linz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621637</person_id>
				<author_profile_id><![CDATA[81385596095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621638</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E., and Williams, L. 1993. View interpolation for image synthesis. In <i>Proc. of ACM SIGGRAPH'93</i>, ACM Press/ACM SIGGRAPH, New York, 279--288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Goesele, M., Snavely, N., Curless, B., Hoppe, H., and Seitz, S. 2007. Multi-view stereo for community photo collections. <i>ICCV'07</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>253292</ref_obj_id>
				<ref_obj_pid>253284</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mark, W., McMillan, L., and Bishop, G. 1997. Post-Rendering 3D Warping. In <i>Proc. of Symposium on Interactive 3D Graphics</i>, 7--16.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1291367</ref_obj_id>
				<ref_obj_pid>1291233</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Shrstha, P., Barbieri, M., and Weda, H. 2007. Synchronization of multi-camera video recordings based on audio. In <i>Proc. of MULTIMEDIA'07</i>, ACM Press, New York, 545--548.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141964</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Snavely, N., Seitz, S. M., and Szeliski, R. 2006. Photo tourism: Exploring photo collections in 3d. <i>ACM Trans. on Graphics 25</i>, 3, 835--846.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Stich, T., Linz, C., Albuquerque, G., and Magnor, M. 2008. View and Time Interpolation in Image Space. <i>Computer Graphics Forum (Proc. of PG'08) 27</i>, 7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtual Video Camera: Image-Based Viewpoint Navigation Through Space and Time Christian Lipski, Christian 
Linz, Kai Berger, Marcus Magnor* Computer Graphics Lab, TU Braunschweig  Figure 1: Viewpoint navigation 
space: time and camera directions span our navigation space. Each cube represents one video frame. The 
navigation space is partitioned into tetrahedra with video frames as vertices. Tetrahedral edges denote 
correspondence .elds between video frames. Our virtual video camera view is interpolated by warping and 
compositing the four video frames of the enclosing tetrahedron (blue) in real-time. Abstract We present 
an image-based rendering system to viewpoint-navigate through space and time of complex real-world, dynamic 
scenes. Our approach accepts unsynchronized, uncalibrated multi-video footage as input. Inexpensive, 
consumer-grade camcorders suf.ce to acquire arbitrary scenes, e.g., in the outdoors, without elabo­rate 
recording setup procedures. Instead of scene depth estima­tion, layer segmentation, or 3D reconstruction, 
our approach is based on dense image correspondences, treating view interpola­tion uniformly in space 
and time: spatial viewpoint navigation, slow motion, and freeze-and-rotate effects can all be created 
in the same fashion. Acquisition simpli.cation, generalization to dif.cult scenes, and space-time symmetric 
interpolation amount to a widely applicable Virtual Video Camera system. Keywords: image-based rendering, 
video-based rendering, free­viewpoint video, viewpoint navigation 1 Motivation The objective common to 
all free-viewpoint navigation systems is to render photo-realistic vistas of real-world, dynamic scenes 
from arbitrary perspective, given a number of simultaneously recorded video streams. Most systems exploit 
epipolar geometry based on either dense depth/disparity maps or complete geometry models, relying on 
calibrated and synchronized multi-video data. The cost, time and effort involved in recording synchronized 
multi-video data constitutes a major obstacle towards economically viable applica­tions for free-viewpoint 
navigation. Further on, most existing sys­tems are designed to interpolate virtual camera positions only 
along spatial dimensions. Temporal view interpolation requires additional scene motion information. Lacking 
this, viewpoint navigation sys­tems can provide scene views only for discrete moments in time, and the 
scene cannot be viewed in slow-motion. With our system, we address these limitations and propose a purely 
image-based approach to free-viewpoint navigation through space as well as time that accepts unsynchronized, 
uncalibrated multi­video footage as input. * {lipski,linz,berger,magnor}@cg.cs.tu-bs.de  2 Approach 
Our approach is motivated by the pioneering work on view inter­polation by Chen and Williams [1993]. 
The Virtual Video Camera systems picks up on their idea to interpolate different image acqui­sition attributes 
in a higher-dimensional space and suitably extends it to be applicable to view interpolation in the spatial 
as well as temporal domain. Putting the temporal dimension on a par with the spatial dimension allows 
for a uniform framework to continuously interpolate virtual video camera positions across space and time. 
On the technical side, in our system dense image correspondences take the place of depth/disparity or 
3D geometry, extending appli­cability to scenes whose object surfaces are highly variable in ap­pearance, 
or hard to reconstruct for other reasons. Our processing pipeline makes use of known techniques and suitably 
adapts them to solve the problem at hand: The multi-video data is .rst color corrected [Snavely et al. 
2006], and extrinsic camera parameters [Goesele et al. 2007] and inter-camera time offsets are estimated 
[Shrstha et al. 2007]. The video frames are then embedded into navigation space, adjacency of video frames 
is determined via con­strained tetrahedralization, and dense morphing correspondences are estimated between 
adjacent video frames [Stich et al. 2008]. After these of.ine processing steps, the navigation space 
can be interactively explored (viewing directions and time) by real-time rendering [Mark et al. 1997]. 
 References CHEN, S. E., AND WILLIAMS, L. 1993. View interpolation for image synthesis. In Proc. of ACM 
SIGGRAPH 93, ACM Press/ACM SIGGRAPH, New York, 279 288. GOESELE, M., SNAVELY, N., CURLESS, B., HOPPE, 
H., AND SEITZ, S. 2007. Multi-view stereo for community photo collections. ICCV 07, 1 8. MARK, W., MCMILLAN, 
L., AND BISHOP, G. 1997. Post-Rendering 3D Warping. In Proc. of Symposium on Interactive 3D Graphics, 
7 16. SHRSTHA, P., BARBIERI, M., AND WEDA, H. 2007. Synchronization of multi­ camera video recordings 
based on audio. In Proc. of MULTIMEDIA 07, ACM Press, New York, 545 548. SNAVELY, N., SEITZ, S. M., AND 
SZELISKI, R. 2006. Photo tourism: Exploring photo collections in 3d. ACM Trans. on Graphics 25, 3, 835 
846. STICH, T., LINZ, C., ALBUQUERQUE, G., AND MAGNOR, M. 2008. View and Time Interpolation in Image 
Space. Computer Graphics Forum (Proc. of PG 08) 27, 7. Copyright is held by the author / owner(s). SIGGRAPH 
2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599395</article_id>
		<sort_key>940</sort_key>
		<display_label>Article No.</display_label>
		<display_no>94</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>94</seq_no>
		<title><![CDATA[The journey of the Cystic Fibrosis gene]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599395</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599395</url>
		<abstract>
			<par><![CDATA[<p>A 3D medical animation that visually describes the role of the Cystic Fibrosis Transmembrane conductance Regulator, which is directed by the code dictated by the Cystic Fibrosis gene in chromosome number seven.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621639</person_id>
				<author_profile_id><![CDATA[81442606368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Phoebe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Colorado Denver]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621640</person_id>
				<author_profile_id><![CDATA[81442595897]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Journey of the Cystic Fibrosis Gene Phoebe Coleman, University of Colorado Denver Bill Elder, Stanford 
University  I. Statement Summary A 3D medical animation that visually describes the role of the Cystic 
Fibrosis Transmembrane conductance Regulator, which is directed by the code dictated by the Cystic Fibrosis 
gene in chromosome number seven.  II. Short Overview Today Cystic Fibrosis is a serious genetic disorder 
with a current life expectancy of 37 years, and with time and research, scientists working with the Cystic 
Fibrosis Foundation hope to control this fatal disease. The medical animation brings to life a hybrid 
between photorealistic and 2D medical illustration images. Science can be used as a tool that influences 
the outcomes of art, and similarly, art can also be used to stimulate science innovation and creativity. 
Medical animations can incorporate the sciences, the arts, and education all in one medium. Through The 
Journey of the CF Gene production, Phoebe Coleman is creating an educational animation that raises awareness 
and knowledge about the disease. Animations are part of the growing image in the information era in science 
technology and art. The production is a joint effort of collaboration between an animation student at 
the University of Colorado Denver, a premedical student at Stanford University, and with copyright permissions 
granted by the Cystic Fibrosis Foundation, Colorado Chapter. Bill Elder was diagnosed with CF at the 
age of 8. The animation is dedicated to him and to the other people afflicted with CF. III. Description 
The animation explains the genetic causes of the Cystic Fibrosis gene. The gene is located in chromosome 
number seven (Figure 1), and only those people with a double recessive trait of the gene will be afflicted 
by the disease. CF is caused by a mutation in the Cystic Fibrosis Transmembrane conductance Regulator, 
which controls what particles are emitted in and out of the cell.  (Figure 1) Chromosomes, 2009, Digital 
Animation, 1920x1080 pixels After science transcended its way into art, art influenced its way into science 
education. Frank Netter (1906-1991) was a medical illustrator who created a new method for medical education. 
Over 4,000 of Netter s medical illustrations of human anatomy and physiology were published in the 1989 
Atlas of Human Anatomy, which soon became the number one source or anatomical reference. Very soon thereafter, 
Netter was recognized as a top contributor to medical education. Through his paintings and drawings, 
Netter was able to visually educate students and doctors about the human body and medical practices such 
as artificial heart transplants. From the integration of the art and the sciences, art has been used 
as a medium to bring innovation and creativity to science education. The idea of creating visually appealing 
animations translated into scientific findings into a readily understandable language demonstrates creativity. 
This type of work requires an animator to pay careful attention to the relative size of objects, the 
translucency of materials, the consistency and movement of cells and other internal organs. Structures 
such as the skeletal and nervous systems, and the ultrastructural components of cells must be rendered 
in precise and accurate detail. These details might be considered constraints on one s artistic expression, 
but these visual aspects from a particular perspective while using color and movement provide a distinct 
artistic challenge. The animation has been rendered out using the Mental Ray global illumination system, 
as well as other added lighting systems. The images created aim to bring to life typical 2D medical illustrations 
found in an academic textbook. Although the images have a 2D textbook feel, the animation transforms 
them into a 3D space making them come to life. All animation related aspects of the production are done 
solely by Phoebe Coleman. The 3D animation production is intended to be viewed by doctors, patients 
with Cystic Fibrosis, and those who are unaware of the process that the disease takes. The animation 
is meant to be used as an educational tool and to raise awareness of the Cystic Fibrosis genetic disorder. 
As this is a work in progress, there will be other sequences to be added, such as the CF proteins communicating 
with each other. The completed short will be linked to the Cystic Fibrosis Foundation s website: www.cff.org. 
Current production stages of the animation can be followed at: http://www.colemancfmedanimationproduction.blogspot.com. 
Further information regarding the production, please visit: www.BioMations.com.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599396</article_id>
		<sort_key>950</sort_key>
		<display_label>Article No.</display_label>
		<display_no>95</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>95</seq_no>
		<title><![CDATA[Spatio-temporal sensing and visualizing of CO<sub>2</sub>]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599396</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599396</url>
		<abstract>
			<par><![CDATA[<p>Global warming is one of many urgent problems we face and a positive individual attitude to the environment is necessary. The goal of our project is to raise awareness to the carbon dioxide (CO<sub>2</sub>) surrounding people which is a main factor causing global warming.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Visual</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010365</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Visual analytics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621641</person_id>
				<author_profile_id><![CDATA[81442616042]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nariya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621642</person_id>
				<author_profile_id><![CDATA[81413594858]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Young]]></first_name>
				<middle_name><![CDATA[Ah]]></middle_name>
				<last_name><![CDATA[Seong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621643</person_id>
				<author_profile_id><![CDATA[81100411762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621644</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1322273</ref_obj_id>
				<ref_obj_pid>1322263</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eisenman, S. B., Miluzzo, E., Lane, N. D., Peterson, R. A., Ahn, G-S., and Campbell, A. T. 2007. The BikeNet mobile sensing system for cyclist experience mapping, In <i>Proceedings of the 5th international conference on Embedded networked sensor systems</i>, 87--101.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1777239</ref_obj_id>
				<ref_obj_pid>1777235</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rekimoto, J., Miyaki, T., and Ishizawa, T. 2007. LifeTag: WiFi-based Continuous Location Logging for Life Pattern Anysis, In <i>3rd International Symposium on Location- and Context-Awareness (LOCA2007)</i>, 35--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spatio-Temporal Sensing and Visualizing of CO2 Takashi Nariya. Young Ah Seong Tomoko Hashida Takeshi 
Naemura The UniversityofTokyo Figure 1: System diagram of data .ow Figure 2: CO2 distribution is re.ected 
in colors Figure 3: CO2 distribution measuredby3users 1 Introduction Global warming is one of many urgent 
problems we face and a positive individual attitude to the environment is necessary. The goalof our projectisto 
raiseawarenesstothe carbon dioxide(CO2) surrounding people whichisa mainfactor causing globalwarming. 
So far, several types of environmental monitoring systems have been proposed [Eisenman et al.2007]. Compared 
with these works, our system is different in that it is designed for easy to visualize data and that 
we focus speci.cally on CO2 concentration around people indoorsand outdoors.Inthis paper,we proposeanovelCO2 
sensing device and a visualization software. Additionally, through preliminary experiments, we indicate 
the possibility of our system to monitor CO2 and to stir a feeling of intrigue within people for their 
surrounding environment. 2 System Con.guration Our system consistsofaCO2 sensingdeviceandbrowsing software. 
It has the following features. First, the system is portable (see Figure1). Our system includes a CO2 
sensor, an Arduino, and a laptop computer. A CO2 sensor, controlled by an Arduino, is packaged in a plastic 
box with holes. Sensitivity of the sensor is the level of parts per million(ppm). So it can sense dynamic 
changes of CO2 concentration in the air. When sensing CO2 concentration, we insert these devices in an 
open bag. In addition, to detect the location(coordinates)of users, we use PlaceEngine. PlaceEngine is 
a core technology that enables a de­vice equipped withWi-Fi to determine its current location [Reki­moto 
et al.2006] It can detect the location according to the database evenifusersarein enclosed spacessuchasinbuildingsorsubways 
as long as it catchesWi-Fi signals. By carrying this portable sys­tem, people can simultaneously record 
the CO2 concentration with their time and location information. Furthermore, we implemented a data visualization 
software. The software represents CO2 distribution on a display based on time and location. It plots 
data in the shape of circles on a map. And the colors of the circles change corresponding to the CO2 
concen­trations (see Figure2). The status color bar is represented at the bottom of the display. It shows 
the CO2 distribution with colors ranging from blue to red (blue corresponds to the minimum values, andred 
correspondstothe maximumvaluesineachexperiment).In this software, users can recognize the change of CO2 
concentration around them with time-locations. User can view the CO2 distribu­tion at speci.c point in 
time by scrolling the black box in the time line at the top of the display. The box on the time line 
represents the point of that time. .e-mail:life@nae-lab.org 3 PreliminaryExperiments and Conclusion 
We examined our system through following two different types of experiments. First, we compared the concentration 
of CO2 in several different situations including enclosed spaces in the Tokyo area using the portable 
system. According to the colors shown in Figure 2, the concentration of CO2 investigated in enclosed 
spaces such as in trains or subways was relatively high. With our investigation, the CO2 concentration 
outdoors was low. In this way, users can recog­nize CO2 distribution on the display intuitively and instantaneously. 
Second, three users measured CO2 levels in various situations out­doors in the Shinjuku area in Tokyo 
simultaneously. (see Fig­ure3). The Shinjuku area has various environmental situations: tall buildings, 
heavily-traf.cked roadways, crowds of people, and large parks and green spaces. As a result, low CO2 
concentration was measured especially near areas with manytrees such as in parks. In contrast, near the 
roadway or in urban districts, high CO2 concen­trations were recorded. This result shows that the CO2 
distribution in Shinjuku differs in each area according to the surrounding envi­ronment. Some users pointed 
out that it is interesting to be able to intuitively visualize the CO2 with our system, since they were 
un­aware of it before. This illustrates how our system can be used as a tool to provide an intuitive 
awareness of the CO2 distribution in their surrounding environment. In the future, we plan to improve 
our system so that the distribution of CO2 can be measured by multiple users and re.ected back to the 
usersin real time, enabling more applications.We envision our systemtoprovideanewmeansfor peopleto re.ectonthe 
necessity of reducing CO2. References EISENMAN, S. B., MILUZZO, E., LANE, N. D., PETERSON, R. A., AHN, 
G-S., AND CAMPBELL, A. T. 2007. The BikeNet mobile sensing system for cyclist experience mapping, In 
Pro­ceedings of the 5th international conference on Embedded net­worked sensor systems, 87-101. REKIMOTO, 
J., MIYAKI, T., AND ISHIZAWA, T. 2007. LifeTag:WiFi-based Continuous Location Logging for LifePat­tern 
Anysis, In 3rdInternational Symposium on Location-and Context-Awareness (LOCA2007), 35-49. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599397</article_id>
		<sort_key>960</sort_key>
		<display_label>Article No.</display_label>
		<display_no>96</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>96</seq_no>
		<title><![CDATA[Visualization laboratory for Earth Sciences]]></title>
		<subtitle><![CDATA[a multidisciplinary visual learning environment]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599397</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599397</url>
		<abstract>
			<par><![CDATA[<p>Modern technology allows to improve teaching methologies by allowing interactivity and to involve more senses in the learning process at an affordable price. In particular, visual learning has proved to be a very important way to understand otherwise elusive scientific principles. Our Visualization Laboratory for Earth Sciences is aiming to be a visual learning environment for Earth Science graduate students and at the same time to be a training platform for Computer Science undergraduates specializing in Graphics Application Programming. The importance of visual learning, specifically for Earth Sciences, has been examined previously [McGrath and Brown 2005], [Reynolds 2005].</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[education]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
			<kw><![CDATA[visual skills]]></kw>
			<kw><![CDATA[visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>K.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010489</concept_id>
				<concept_desc>CCS->Applied computing->Education</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621645</person_id>
				<author_profile_id><![CDATA[81442617340]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alejandro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aguilar-Sierra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1092247</ref_obj_id>
				<ref_obj_pid>1092228</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[McGrath, M. B., and Brown, J. R. 2005. Visual learning for science and engineering. <i>IEEE Computer Graphics and Applications 25</i>, 56--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Reynolds, S. J. 2005. The hidden earth: Visualization of geologic features and their subsurface geometry. <i>AAAS Press</i>, 141--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualization Laboratory for Earth Sciences: a multidisciplinary visual learning environment Alejandro 
Aguilar-Sierra* Center of Atmospheric Sciences National Autonomous University of Mexico  Figure 1: Sea 
Surface Temperature anomaly of August 1997, an interactive georeferenced scenary to study El Ni no event 
and a visualization of eddy-.eld circulation in the Gulf of Mexico in Winter and Summer. Keywords: visualization, 
virtual reality, education, visual skills I hear and I forget. I see and I remember. I do and I understand. 
 Confucius. 1 Introduction Modern technology allows to improve teaching methologies by al­lowing interactivity 
and to involve more senses in the learning pro­cess at an affordable price. In particular, visual learning 
has proved to be a very important way to understand otherwise elusive scien­ti.c principles. Our Visualization 
Laboratory for Earth Sciences is aiming to be a visual learning environment for Earth Science grad­uate 
students and at the same time to be a training platform for Computer Science undergraduates specializing 
in Graphics Appli­cation Programming. The importance of visual learning, speci.­cally for Earth Sciences, 
has been examined previously [McGrath and Brown 2005], [Reynolds 2005]. 2 Work in progress Based on 
.ve years of experience building Earth Science interac­tive applications for virtual reality rooms and 
teaching Scienti.c Visualization and Computer Graphics to Computer Science under­graduates, we are developing 
an innovative multidisciplinary learn­ing environment for Earth Science. This environment will consist 
in a Software Development Laboratory (SDL), a Virtual Reality Room (VRR) and a Digital Media Library 
hosted in a remote server (DML). The students visual skill improvement will be evaluated pe­riodically 
with special tests using Augmented Reality techniques. * e-mail: algsierra@gmail.com In the SDL, computer 
science professionals, with the help of their undergraduate students, will build course materials, consisting 
in 3D interactive applications to be used in the VRR. These applica­tions will include visualization 
of massive geophysical data (Sea Surface Temperature, leftmost picture, .gure 1), georeferenced physical 
simulations of natural phenomena in which users will be able to interact with (El Ni no, second picture, 
.gure 1), etc. The scripts and content of these course materials will be prepared with Earth Science 
teachers and professors. The two pilot courses we are currently working on are Physics of the Earth s 
Interior and Physics of the atmosphere . The course materials will be stored, properly classi.ed, in 
the DML linked to the laboratory by a fast local network. Through a web user inter­face, a teacher or 
student will be allowed to prepare a session, se­lecting the appropriate course materials, which will 
be ready to be used in the VRR at the time indicated by the teacher. Computer Science students will be 
trained on Agile Software De­velopment methodologies, team work and Computer Graphics, and will grow 
their creativity by building course materials. Earth Sci­ence students will improve their visual skills 
and understand other­wise elusive scienti.c concepts. We will use affordable new generation hardware 
like new GPUs, High De.nition displays and stereo rendering devices. The course materials will be built 
mainly over Open Source libraries. 3 Conclusion This is a work in progress but so far we have been able 
to build an affordable, multisensory learning environment to study Earth Sci­ences, suitable to be adopted 
as a model by other developing coun­tries universities. References MCGRATH, M. B., AND BROWN, J. R. 
2005. Visual learning for science and engineering. IEEE Computer Graphics and Appli­cations 25, 56 63. 
REYNOLDS, S. J. 2005. The hidden earth: Visualization of geo­logic features and their subsurface geometry. 
AAAS Press, 141 146. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599398</article_id>
		<sort_key>970</sort_key>
		<display_label>Article No.</display_label>
		<display_no>97</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>97</seq_no>
		<title><![CDATA[Augmented reality under water]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599398</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599398</url>
		<abstract>
			<par><![CDATA[<p>Fascinated by a stunning variety of corals and fishes or mysterious wrecks more and more people are attracted by snorkeling and diving adventures. Virtual Reality scenarios like the virtual oceanarium [Froehlich 2000] try to satisfy this interest by allowing for discovery of underwater worlds in a riskfree and comfortable way, but a realistic feeling of diving is never achieved by virtual submarine worlds.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621646</person_id>
				<author_profile_id><![CDATA[81442604464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blum]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer FIT, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621647</person_id>
				<author_profile_id><![CDATA[81100578067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Broll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer FIT, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621648</person_id>
				<author_profile_id><![CDATA[81405596027]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#252;ller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Koblenz-Landau, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1544229</ref_obj_id>
				<ref_obj_pid>1544196</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Broll, W., Herling, J., and Blum, L. 2008. Interactive Bits: Prototyping of Mixed Reality Applications and Interaction Techniques through Visual Programming. In <i>IEEE Symposium on 3D User Interfaces 2008</i>, 109--115.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>341868</ref_obj_id>
				<ref_obj_pid>341852</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Froehlich, T. 2000. The virtual oceanarium. <i>Commun. ACM</i> 43, 7 (Jul. 2000), 94--101.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Augmented Reality under water Lisa Blum* Fraunhofer FIT, Germany Wolfgang Broll Fraunhofer FIT, Germany 
Stefan Müller University of Koblenz-Landau, Germany   1 Introduction   Fascinated by a stunning variety 
of corals and fishes or mysterious wrecks more and more people are attracted by snorkeling and diving 
adventures. Virtual Reality scenarios like the virtual oceanarium [Froehlich 2000] try to satisfy this 
interest by allowing for discovery of underwater worlds in a riskfree and comfortable way, but a realistic 
feeling of diving is never achieved by virtual submarine worlds. In this paper we present a system allowing 
users to sense virtual corals and fishes even physically under water. We developed a mobile prototype 
using Augmented Reality techniques to enhance a regular swimming pool with virtual objects. For demonstration 
purposes an underwater Augmented Reality game was created turning a swimming pool into a maritime environment 
(cf. fig.1).  2 Our approach   The realization of the overall system required waterproof and rugged 
hardware as well as appropriate software to be used in a swimming pool environment. The central component 
of our mobile hardware prototype is an optical see-through display mounted in front of a diving mask. 
Rendering of the virtual objects is performed on a waterproof sealed ultra mobile PC connected to the 
display via a flexible tube. A hybrid tracking approach combining inertial and magnetic-field-based with 
optical (marker-based) tracking is used to determine position and orientation of the user. The inertial 
and magnetic-field-based tracking bridges the gap if the marker tracking fails which can easily happen 
due to occlusion of the marker during swimming movements. To avoid reflections of the marker we developed 
special non-reflective, water-resistant markers that even work in direct sunlight. The Augmented Reality 
game turning the swimming pool into an attractive underwater landscape is based on our AR/VR framework 
Morgan. The user acts as a maritime archaeologist who is supposed to find and open a sunken treasure 
chest. To open it a numerical code hidden in magical shells has to be solved. Opening of the magical 
shells is realized by an interaction technique solely based on approaching the virtual objects. Swimming 
far away the shells stay closed but by getting closer they open up allowing the user to see their numbered 
pearl. By this intuitive technique the user is able to interact without controlling input devices that 
are probably difficult to handle under water as arms and legs are needed for swimming movements. We realized 
the mentioned interaction technique as well as different animation effects for the underwater world with 
an earlier approach based on interaction prototyping [Broll et al. 2008].  3 Results   We evaluated 
the system in an initial user study with 8 participants (6 male, 2 female). The majority found the system 
easy to use and rated the handling very positive. Most of the participants felt in no way handicapped 
by the system and were able to fully concentrate on the perception of the virtual corals and fishes. 
The developed tracking system worked up to a water depth of almost 4 metres. The main drawback of the 
system was the small field of view of the optical see-through display. Color representation and contrast 
as well as the occlusion of the real world with virtual objects, which sometimes is a problem of optical 
see-through displays, yielded good results. Additionally all users declared that the haptic influences 
of the water had intensified their experiences and lead to a new form of perception.  4 Conclusion 
  The developed system shows how Augmented Reality techniques can be used to extend a usual swimming 
pool with virtual objects. Based on the positive results of the initial user study we consider the system 
to be a good basis for future employment of Augmented Reality under water. Prospective directions of 
our group include the development of more sophisticated applications and the enhancement of technical 
aspects of the hardware prototype. As well, we plan to extend our knowledge to support professional divers 
in maritime environments in different tasks.  5 References   BROLL, W., HERLING, J., AND BLUM, L. 
2008. Interactive Bits: Prototyping of Mixed Reality Applications and Interaction Techniques through 
Visual Programming. In IEEE Symposium on 3D User Interfaces 2008, 109 115. FROEHLICH, T. 2000. The virtual 
oceanarium. Commun. ACM 43, 7 (Jul. 2000), 94-101. Figure 1: User discovering the virtual underwater 
world  *e-mail: lisa.blum@fit.fraunhofer.de e-mail: wolfgang.broll@fit.fraunhofer.de e-mail: stefanm@uni-koblenz.de 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599399</article_id>
		<sort_key>980</sort_key>
		<display_label>Article No.</display_label>
		<display_no>98</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>98</seq_no>
		<title><![CDATA[Bare hand interaction in tabletop augmented reality]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599399</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599399</url>
		<abstract>
			<par><![CDATA[<p>Augmented Reality (AR) techniques have been applied to many application areas; however, there is still research that needs to be conducted on the best way to interact with AR content. Since hands are our main means of interaction with objects in real life, it would be natural for AR interfaces to allow free hand interaction with virtual objects. We present a system that tracks the 2D position of the user's hands on a tabletop surface, allowing the user to move, rotate and resize the virtual objects over this surface. Our implementation is based on a computer vision tracking system that processes the video stream of a single usb camera.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621649</person_id>
				<author_profile_id><![CDATA[81442611493]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruno]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fernandes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Catalonia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621650</person_id>
				<author_profile_id><![CDATA[81442603982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joaquin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fern&#225;ndez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Catalonia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bradski, G. and Kaehler, A. 2008. <i>Learning OpenCV: Computer vision with the OpenCV library</i>. O'Reilly Media.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1069138</ref_obj_id>
				<ref_obj_pid>1068508</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fiala, M. 2005. ARTag, a fiducial marker system using digital techniques. <i>Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</i>, 590--596 vol. 592.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Freund, Y. and Schapire, R. 1996. Experiments with a New Boosting Algorithm. <i>International Conference on Machine Learning</i>, 148--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1779582</ref_obj_id>
				<ref_obj_pid>1779576</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Song, P., Winkler, S., Gilani, S. and Zhou, Z. 2007. Vision-Based Projected Tabletop Interface for Finger Interactions. In <i>Human-Computer Interaction</i>, 49--58.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Bare Hand Interaction in Tabletop Augmented Reality Bruno Fernandes*, Joaquin Fernández Technical University 
of Catalonia  Figure 1: (a) Hand image samples under different lighting conditions, used for classifier 
training. (b) User holds a virtual object from below in order to move it. (c) User applies both hands 
in order to rotate and resize the virtual object. 1 Introduction Augmented Reality (AR) techniques have 
been applied to many application areas; however, there is still research that needs to be conducted on 
the best way to interact with AR content. Since hands are our main means of interaction with objects 
in real life, it would be natural for AR interfaces to allow free hand interaction with virtual objects. 
We present a system that tracks the 2D posi­tion of the user s hands on a tabletop surface, allowing 
the user to move, rotate and resize the virtual objects over this surface. Our implementation is based 
on a computer vision tracking system that processes the video stream of a single usb camera. Vision-based 
hand tracking is an important problem in the field of human-computer interaction. Song et al. [Song et 
al. 2007] list many constrains of some vision-based techniques used to track hands: methods based on 
color segmentation need users to wear colored gloves for efficient detection; methods based on back­ground 
image subtraction have difficulties when applied to im­ages with a complex background; contour based 
methods work only on restricted backgrounds; infrared segmentation based me­thods require expensive infrared 
cameras; correlation-based me­thods require an explicit setup stage before the tracking starts; and the 
blob-model based methods imposes restrictions on the maxi­mum speed of hand movements. 2 Our Approach 
The approach we ve chosen is to use statistical models (classifi­ers). These models can be obtained by 
analyzing a set of training images and then be used to detect the hands. Statistical model­based training 
takes multiple image samples of the desired object (hands in our case) and multiple images that do not 
contain hands (so called negative images ). Different features are extracted from the training samples 
and distinctive features that can classify the hands are selected. The Haar-like features [Bradski and 
Kaeh­ler 2008] were used combined with the AdaBoost [Freund and Schapire 1996] algorithm to extract the 
features characteristics of the hands. To create each classifier we used 3000 hand images from 12 persons 
under different lighting conditions (Figure 1a) and 7000 negative images. The training process took 4 
days in a computer with a Core 2 Duo 1.8 GHz processor and 1 GB RAM. We have implemented our tabletop 
system using ARTag [Fiala 2005] to track an array of markers over the table. In order to create the feature 
classifiers and detect the user s hands at run time we used the OpenCV [Bradski and Kaehler 2008] library. 
As seen in Figure 1b, the palm up posture is employed to change the object position over the table. In 
order to resize and rotate the object, the user should employ both hands in a different posture, as shown 
in Figure 1c. The object will be affected by the movement of the hands, when the distance between the 
hands increase the object size will increase accordingly and when this distance decreases the object 
size will decrease. To rotate the object the user will move its hands as if each hand were holding one 
side of the object. We tested our system in a laptop with a 2.2 GHz Core 2 Duo processor and got 15 fps 
when performing the hand recognition at an image with 320 x 240 pixels resolution. Compared to previous 
approaches, our system is less sensitive to changes in illumination, it can recognize different hand 
poses and needs no calibration prior to usage, the user just needs to print the array of markers on an 
A3 (or bigger) sheet of paper and place it over any planar surface to be able to use the system. Another 
ad­vantage of our approach is that other skin color regions can appear in the view without interfering 
in the tracking, provided they are not a hand in one of the recognizable postures. References BRADSKI, 
G. AND KAEHLER, A. 2008. Learning OpenCV : Com­puter vision with the OpenCV library. O'Reilly Media. 
FIALA, M. 2005. ARTag, a fiducial marker system using digital techniques. Computer Vision and Pattern 
Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, 590-596 vol. 592. FREUND, Y. AND SCHAPIRE, 
R. 1996. Experiments with a New Boosting Algorithm. International Conference on Machine Learning, 148-156. 
 SONG, P., WINKLER, S., GILANI, S. AND ZHOU, Z. 2007. Vision- Based Projected Tabletop Interface for 
Finger Interactions. In Human Computer Interaction, 49-58. * brunofer@gmail.com, jfernandez@ege.upc.edu 
 Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599400</article_id>
		<sort_key>990</sort_key>
		<display_label>Article No.</display_label>
		<display_no>99</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>99</seq_no>
		<title><![CDATA[Flexible foot interface for versatile training field]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599400</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599400</url>
		<abstract>
			<par><![CDATA[<p>We propose the wellness entertainment system Versatile Training Field (VTF). In this system, we use the flexible foot interface as the input device. The system enables the user to move and jump freely in VR space by exaggerated movement corresponding to walking or jumping on the mini trampoline of the flexible foot interface. Improvements in exercise motivation and support for continuous exercise are achieved in our system, since it is possible to enjoy strolling through a virtual space, which is usually difficult to experience, by exercising on the mini trampoline without injury to the user's joints.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621651</person_id>
				<author_profile_id><![CDATA[81100501102]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba CREST, JST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621652</person_id>
				<author_profile_id><![CDATA[81331504069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazuhito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shiratori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba CREST, JST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621653</person_id>
				<author_profile_id><![CDATA[81413604705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujieda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621654</person_id>
				<author_profile_id><![CDATA[81100230368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jun'ichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba CREST, JST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Flexible Foot Interface for Versatile Training Field Hiroshi Mori Kazuhito Shiratori Tomoyuki Fujieda 
Jun ichi Hoshino University of Tsukuba CREST, JST (a)Above the interface (b)Under the Interface and 
two PSD sensor (c)VTF(Versatile Training Field) Figure 1. Overview of the flexible foot interface. ABSTRACT 
We propose the wellness entertainment system Versatile Training Field (VTF). In this system, we use the 
flexible foot interface as the input device. The system enables the user to move and jump freely in VR 
space by exaggerated movement corresponding to walking or jumping on the mini trampoline of the flexible 
foot interface. Improvements in exercise motivation and support for continuous exercise are achieved 
in our system, since it is possible to enjoy strolling through a virtual space, which is usually difficult 
to experience, by exercising on the mini trampoline without injury to the user s joints. 1. AUGMENTING 
TRAMPOBICS Recently, lifestyle-related diseases have become recognized as problems. Changes in eating 
habits, stress in day-to-day living, smoking, and excessive drinking all contribute to the development 
of lifestyle-related diseases. Regular exercise is needed to prevent these diseases. Particularly beneficial 
are effective and continuous exercises appropriate for our physical strength. Nowadays, exercise machines 
for home use, such as running machines or aerobics bicycles, are prevalent. Trampobics is an aerobics 
training method using a mini trampoline. It substantially decreases damage to the joints compared with 
floor-based aerobics. Moreover, it has benefits such as constipation prophylaxis, diarrhea prevention, 
and alleviation of tension in the shoulders. It enables us to exercise effectively in a short time by 
controlling the heart rate. Because of this, it is expected to be a lifelong exercise. 2. SYSTEM OVERVIEW 
VTF is composed of the flexible foot interface(Figure 1(a)) and an image-generation system, which consists 
of a PC, two short-focus projectors, and two large-scale screens(Figure 1(c)). The flexible foot interface 
consists of a mini trampoline and two position-sensitive detector (PSD) sensors (Sharp products, GP2Y0A21YK0F) 
(Figure 1(b)), which are installed beneath the center of the trampoline and measures the distance between 
the sensor head and the trampoline bed. Then, the application removes high-frequency noise from the received 
data with a low­pass filter. The PC application detects the type and extent of the user s movement by 
processing the time series of the amount of changes in the trampoline bed. When the application detects 
the user s motion and calculates the corresponding viewpoint in VR space, the short-focus projector projects 
the view onto a large­scale screen in front of the user. This projected image, synchronized with the 
exercise, gives the user the sense that he/she is exercising within the virtual scenery. default stand 
walk jump (low) jump (high)  Figure 2. Relationship of exercise and an input signal. 3. USER EXPERIENCE 
We propose a method to distinguish the type of exercise and status of the user according to the amount 
of change in the trampoline bed(Figure 2). In particular, our system provides a user with an image of 
the virtual viewpoint, synchronized with the exercises and varied according to whether the user is standing 
(at rest), walking, balancing, or jumping on the trampoline. Our system prepares versatile exercise content 
in VR selected according to the interests of the user and the desired exercise time. Examples of such 
content are strolling around streets, jumping higher than the roof of a city building, jumping sky-high 
in a natural environment such as a mountainous area, playing a sport, having a snowball fight, playing 
a treasure-hunting game, and so on. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, 
Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599401</article_id>
		<sort_key>1000</sort_key>
		<display_label>Article No.</display_label>
		<display_no>100</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>100</seq_no>
		<title><![CDATA[Haptic ring]]></title>
		<subtitle><![CDATA[touching virtual creatures in mixed reality environments]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599401</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599401</url>
		<abstract>
			<par><![CDATA[<p>We propose a new method for Symmetrical Haptic Interaction System with Virtual Creatures (VCs) in Mixed Reality. It's achieved by small and light haptic interface and Reactive VCs with touch sensations. People can touch VCs directly by fingers and watch their reaction. And VCs also can touch us directly.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[haptic interface]]></kw>
			<kw><![CDATA[mixed reality]]></kw>
			<kw><![CDATA[virtual creature]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Haptic I/O</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011752</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Haptic devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621655</person_id>
				<author_profile_id><![CDATA[81543929256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621656</person_id>
				<author_profile_id><![CDATA[81316489667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitake]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621657</person_id>
				<author_profile_id><![CDATA[81100270860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Electro-Communications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621658</person_id>
				<author_profile_id><![CDATA[81100567066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187309</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aoki, T., Mitake, H., Asano, K., Toyama, T., Ichikawa, H., Kuriyama, T., Ayukawa, R., Kawase, T., Matsumura, I., Matsushita, T., Iio, Y., Hasegawa, S., and Sato, M. 2005. Kobito -virtual brownies-. In <i>SIGGRAPH '05: ACM SIGGRAPH 2005 Emerging technologies</i>, ACM, New York, NY, USA, 11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276422</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Raskar, R., Nii, H., de Decker, B., Hashimoto, Y., Summet, J., Moore, D., Zhao, Y., Westhues, J., Dietz, P., Inami, M., Nayar, S., Barnwell, J., Noland, M., Bekaert, P., Branzoi, V., and Bruns, E. 2007. Prakash: Lighting aware motion capture using photosensing markers and multiplexed illuminators. <i>ACM Transactions on Graphics 26</i>, 3 (Aug.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Haptic Ring: Touching Virtual Creatures in Mixed Reality Environments Takafumi Aoki* Hironori Mitake 
* Shoichi Hasegawa Makoto Sato * P &#38; I lab. Tokyo Institute of Technology * University of Electro-Communications 
 Keywords: Mixed Reality, Haptic Interface, Virtual Creature 1 Introduction We propose a new method for 
Symmetrical Haptic Interaction Sys­tem with Virtual Creatures (VCs) in Mixed Reality. It s achieved by 
small and light haptic interface and Reactive VCs with touch sen­sations. People can touch VCs directly 
by .ngers and watch their reaction. And VCs also can touch us directly. Various interactions with VCs 
are required in .elds of entertain­ment, such as video games. We have proposed an entertainment system, 
which enables people to interact with VCs in the real world like Kobito -Virtual Brownies- [Aoki et al. 
2005]. This time we propose to interact VCs directly, not only through some tools but also with our .ngers. 
It also can be experienced by a lot of people at once. Our system enables us to touch virtual creatures 
directly in Mixed Reality (MR) environment, and enhance the sense of presence and variety of expressions 
of VCs. At the same time, our system does not disturb manipulations of real objects by our .ngers. Therefore, 
you can also manipulate the real part of the Mixed Reality environ­ment intuitively. This brings us a 
totally new form of entertainment. Figure 1: A small lightweight haptic interface  2 Innovation Three 
technical innovations exist in this research: a small lightweight haptic interface, high-speed position 
sensor for MR en­vironment and VCs with models of optical and touch sensation. Our haptic device presents 
force with several thin wires to the cu­taneous sensation on the tip of the .nger. The device generates 
spatiotemporal modulation of the force, which makes possible to present a strong sense of contact (sense 
of touching on something) by small force. The device put only thin strings on the ball of the .nger and 
put actuators on the nail side of the .nger like rings. Thereby, the device can overlay the sense of 
force without barring ordinary manipulation of real objects or other users. In order to measure the three-dimensional 
position of the device in high-speed and low time-lag, this system employs the high-speed position measuring 
technique[Raskar et al. 2007]. This enables presentation of the counterforce to the .ngers just in the 
moment of touching and decreases a gap of visual presentation and force *e-mail:{aoki,mitake,msato}@hi.pi.titech.ac.jp 
e-mail:hase@hi.mce.uec.ac.jp presentation, and prevents passes of .ngers through virtual objects. As 
a result, the system can present force correctly. Moreover, the problem of the occlusion of MR can be 
solved by using the position obtained in high-speed by this technique. We installed models of optical 
/ touch sensation into our VCs. It enables VCs to react for user s actions to VCs in various and appro­priate 
ways. For example, they express attention by looking back when they are hit from backward. They express 
happiness when they are stroked gently, and they step away after a strong hit. It also enables cooperation 
between people and virtual creatures, such as handing over some items. Unifying these techniques, we 
achieve a symmetrical haptic inter­action with VCs, which exists at hand. If we touch VCs, we can see 
 Figure 2: Reacitve Virtual Creature with Optical / Touch Sensation 3 Vision The goal of this project 
is to realize new entertainment using Mixed Reality system in homes. Many consumer games using physics 
simulators have appeared in recent years. They have high af.nity with this proposal. For example, You 
can build the stage of a game using the real object on a desk, and do haptic interaction with game characters 
(VCs). Such completely new entertainment will be real­ized by this system. We believe that this system 
can be used in the .eld of entertainment. If this entertainment system is introduced into many homes, 
the world of enjoying an interaction with VCs with a families, friends, and a sweetheart will come true. 
In the future, the relationship be­tween people and Virtual Creatures will approach good relationship 
like a pet which brings comfort just by being a side all the time and enables physical contact. References 
AOKI, T., MITAKE, H., ASANO, K., TOYAMA, T., ICHIKAWA, H., KURIYAMA, T., AYUKAWA, R., KAWASE, T., MATSUMURA, 
I., MATSUSHITA, T., IIO, Y., HASEGAWA, S., AND SATO, M. 2005. Kobito -virtual brownies-. In SIGGRAPH 
05: ACM SIGGRAPH 2005 Emerging technologies, ACM, New York, NY, USA, 11. RASKAR, R., NII, H., DE DECKER, 
B., HASHIMOTO, Y., SUMMET, J., MOORE, D., ZHAO, Y., WESTHUES, J., DIETZ, P., INAMI, M., NAYAR, S., BARNWELL, 
J., NOLAND, M., BEKAERT, P., BRANZOI, V., AND BRUNS, E. 2007. Prakash: Lighting aware motion capture 
using photosensing markers and multiplexed illu­minators. ACM Transactions on Graphics 26, 3 (Aug.). 
Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. 
ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599402</article_id>
		<sort_key>1010</sort_key>
		<display_label>Article No.</display_label>
		<display_no>101</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>101</seq_no>
		<title><![CDATA[Non-photorealistic 3D video-avatar]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599402</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599402</url>
		<abstract>
			<par><![CDATA[<p>The digital games industry is always looking for innovation in user experience. A new trend in this field is the use of Augmented Reality (AR) techniques for intuitive, novel game interfaces [Bernardes Jr et al. 2008]. Among the several technologies related to AR, video avatars are one of the most attractive for games, because they allow the insertion of the game player's image in the game. Furthermore, the availability of commodity stereo cameras makes it feasible to employ 3D video avatars in consumer games. However, when a non-photorealistic rendering style is required for design reasons (often the case for games), the use of a conventional video avatar, even with 3D information, results in visual inconsistencies. This work presents a new approach to enable nonphotorealistic rendering of a real-time 3D video avatar. The project builds upon a 3D video avatar system designed for teleconferencing over Internet 2 for educational purposes, extending it with this new rendering method. The expected result is a reduction in the visual and cognitive mismatch between player image and synthetic environment, allowing for a more immersive experience.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[game design]]></kw>
			<kw><![CDATA[non-photorealistic augmented reality]]></kw>
			<kw><![CDATA[video-avatar]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.7.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P1621659</person_id>
				<author_profile_id><![CDATA[81414620439]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[Makoto]]></middle_name>
				<last_name><![CDATA[Tokunaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interlab Escola Polit&#233;cnica da USP]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621660</person_id>
				<author_profile_id><![CDATA[81414599707]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ricardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interlab Escola Polit&#233;cnica da USP]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621661</person_id>
				<author_profile_id><![CDATA[81319502432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Romero]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interlab Escola Polit&#233;cnica da USP]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bernardes Jr, J. L., Tori, R., Nakamura, R., Calife, D., and Tomoyose, A. 2008. <i>Augmented Reality Games. In: Extending Experiences: Structure, analysis and design of computer game player experience</i>. Lapland University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1179136</ref_obj_id>
				<ref_obj_pid>1179133</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fischer, J., Bartz, D., and Strasser, W. 2006. The augmented painting. In <i>SIGGRAPH '06: ACM SIGGRAPH 2006 Emerging technologies</i>, ACM, New York, NY, USA, 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tokunaga, Makoto, D., Sanches, R. R., S., Trias, Padovani, L., Bernardes, Jr., J. A. L., Nakamura, R., and Tori, R. 2009. Video-based microfacet-billboard avatar for educational immersive teleconference systems. In <i>SVR '09: Proceedings of XI Symposium on Virtual and Augmented Reality</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non-Photorealistic 3D Video-Avatar Daniel Makoto Tokunaga* Ricardo Nakamura Romero Tori Interlab Escola 
Polit´ ecnica da USP  (b) using our approach (c) different angle (d) different scene and a different 
avatar render style Figure 1: Non-photorealistic video-avatar rendering results CR Categories: H.5.1 
[information Interfaces and Pre­sentation]: Multimedia Information Systems Arti.cial, aug­mented, and 
virtual realities ; K.7.m [The Computing Profession]: Miscellaneous Ethics Keywords: video-avatar, non-photorealistic 
augmented reality, game design 1 Introduction The digital games industry is always looking for innovation 
in user experience. A new trend in this .eld is the use of Aug­mented Reality (AR) techniques for intuitive, 
novel game interfaces [Bernardes Jr et al. 2008]. Among the several technologies related to AR, video 
avatars are one of the most attractive for games, be­cause they allow the insertion of the game player 
s image in the game. Furthermore, the availability of commodity stereo cameras makes it feasible to employ 
3D video avatars in consumer games. However, when a non-photorealistic rendering style is required for 
design reasons (often the case for games), the use of a conven­tional video avatar, even with 3D information, 
results in visual in­consistencies. This work presents a new approach to enable non­photorealistic rendering 
of a real-time 3D video avatar. The project builds upon a 3D video avatar system designed for teleconferenc­ing 
over Internet 2 for educational purposes, extending it with this new rendering method. The expected result 
is a reduction in the visual and cognitive mismatch between player image and synthetic environment, allowing 
for a more immersive experience. 2 Proof of Concept As a proof of concept (POC) we have created a system 
in which the user avatar is rendered using a depth map combined with video­based microfacet billboarding 
[Tokunaga et al. 2009]. This avatar and the scene are rendered using a cartoon-like style like in Fisher 
and Strasser work [Fischer et al. 2006]. To create the cartoon-like style used in the scene, techniques 
like cell shading, multiple tex­turing and normal mapping are applied. And for this initial POC, pre-captured 
video and depth map streams are used, to provide a controlled and repeatable data set for our tests. 
Figure 1 shows some .rst results. Comparing the scenes rendered with (.gure 1b) and without (.gure 1a) 
our approach, it is possible *e-mail: daniel.tokunaga@poli.usp.br to see that the rendered avatar in 
1b matches more closely the whole virtual environment design concept. In .gure 1a the avatar is too re­alistic 
compared to the environment around, creating an undesirable mismatch.  3 Conclusion We have presented 
the current state of a system for the insertion of a non-photorealistic 3D video avatar in virtual environments, 
which can be applied in the development of AR games, as well as educa­tional applications. Currently, 
we have developed a POC in which a pre-captured video stream is rendered as a 3D video avatar in a cartoon-like 
style inside a virtual environment. This initial results lead us to believe that our approach helps to 
get a seamless integra­tion between the avatar and the virtual environment and also helps to get a better 
.t of the avatar with the whole cartoon-like environ­ment design, thus proving the main idea of the project. 
The next step in this project includes improving the stylization al­gorithm, performing tests with real-time 
video streams, performing social presence and user immersion study. The authors gratefully acknowledge 
the valuable .nancial support from FAPESP (TIDIA Project) and CAPES, hardware assist from 3DV System, 
as well as the contributions of designer students Bruno S. Viana, Carina M. B. da Costa, Pedro G. C amara 
and Samuel H. S. da Cruz.  References BERNARDES JR, J. L., TORI, R., NAKAMURA, R., CALIFE, D., AND TOMOYOSE, 
A. 2008. Augmented Reality Games. In: Ex­tending Experiences: Structure, analysis and design of computer 
game player experience. Lapland University Press. FISCHER, J., BARTZ, D., AND STRASSER, W. 2006. The 
aug­mented painting. In SIGGRAPH 06: ACM SIGGRAPH 2006 Emerging technologies, ACM, New York, NY, USA, 
2. TOKUNAGA, MAKOTO, D., SANCHES, R. R., S., TRIAS, PADOVANI, L., BERNARDES, JR., J. A. L., NAKAMURA, 
R., AND TORI, R. 2009. Video-based microfacet-billboard avatar for educational immersive teleconference 
systems. In SVR 09: Proceedings of XI Symposium on Virtual and Augmented Reality. Copyright is held 
by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599403</article_id>
		<sort_key>1020</sort_key>
		<display_label>Article No.</display_label>
		<display_no>102</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>102</seq_no>
		<title><![CDATA[Robot rockstars]]></title>
		<subtitle><![CDATA[a mixed-reality game]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599403</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599403</url>
		<abstract>
			<par><![CDATA[<p>The mixed-reality game Robot RockStars combines a massively-interactive videogame controlled by multiple WiiGuitar players with a herd of Pleo and AIBO wireless robots as back-up singers, which can both react and affect gameplay. Attendees can affect gameplay by interacting with the robots or with mobile wireless devices. The main innovation lies in this project with the novelty of creating open-source, mixed-reality, plug-n-play tools that we build and the challenge for applications, such as the mixed-reality game Robot RockStars, to be constructed with them, allowing technology to go beyond what is currently possible for massively interactive collaboration between people and agents.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621662</person_id>
				<author_profile_id><![CDATA[81339531734]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sheila]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tejada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CUNY Brooklyn College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Robot RockStars: A Mixed-Reality Game Sheila Tejada Computer and Information Science Dept, CUNY Brooklyn 
College tejada@sci.brooklyn.cuny.edu  1 Introduction Figure 1: Mixed-reality application with interactive 
game, Wiigui­tar and Pleo robot The mixed-reality game Robot RockStars combines a massively­interactive 
videogame controlled by multiple WiiGuitar players with a herd of Pleo and AIBO wireless robots as back-up 
singers, which can both react and affect gameplay. Attendees can affect gameplay by interacting with 
the robots or with mobile wireless devices. The main innovation lies in this project with the novelty 
of creating open-source, mixed-reality, plug-n-play tools that we build and the challenge for applications, 
such as the mixed-reality game Robot RockStars, to be constructed with them, allowing technology to go 
beyond what is currently possible for massively interactive collaboration between people and agents. 
As an exemplar, Robot RockStars is a massively multi-player game allowing mobile users who roam the real 
world to interact with the virtual world pictured in the immersive environment the combi­nation allows 
different levels of access to the same shared (mixed) reality. Users will then have the option of different 
modes of access which permit different operations and will have opportunities to collaborate in heterogeneous 
teams. Our prior work on mixed-reality applications has given us a good basis from which to start developing 
tools. We identi.ed aspects of Virtual Synergy, AiBee and KITS which can be employed in further mixed-reality 
applications. This list of components will include the virtual environment engine at the heart of Virtual 
Synergy (which also offers the option of human users and software agents appearing as avatars), the web-interface 
component from AiBee along with the software that allows communication with and remote control of the 
robot. To this we will then add additional components which allow us to extend the scope of the mixed-reality 
components, such as an additional robot commmunication module that will permit high- Figure 2: View of 
(clockwise from upper left) virtual synergy, visi­tors to the AiBee exhibit, the HoneyComb city from 
AiBee, and KITS in action. More information about these project can be found here: http://www.sci.brooklyn.cuny.edu/ 
tejada/projects bandwidth data from a robot to be uploaded into the virtual world, and interfaces that 
allow data to be received by the Virtual Syn­ergy engine not only from a web-interface, but also direct 
from cell-phones (so users can text information that affects an applica­tion), and from next-generation 
game controllers like the Wii re­mote. This above list is based on technological feasibility. With an 
eye to the long term aims of the project, we will design a uniform API for the components, and develop 
them using rapid prototyping to obtain frequent feedback on the creative impact of the components as 
they evolve. 2 Robot RockStars Prototype In December 2008 students from the CUNY Brooklyn Col­lege Computer 
and Information Science Department demonstrated projects that they developed in the undergraduate Game 
Program­ming course and the graduate Arti.cial Intelligence course. Each project constructed a 3D game 
for a different type of Wii remote, such as the Wiimote, Wii Guitar and the WiiFit, and the graduate 
student project also includes the Pleo robot as part of the game that everyone can interact with. Copyright 
is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599404</article_id>
		<sort_key>1030</sort_key>
		<display_label>Article No.</display_label>
		<display_no>103</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>103</seq_no>
		<title><![CDATA[SteganoScan]]></title>
		<subtitle><![CDATA[persistence of vision display with pixel-level visible light communication projector]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599404</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599404</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we present a novel persistence of vision display named SteganoScan. SteganoScan is a stick shaped display device with a single line of LEDs that emits light in full color and shows 2D images without any screen.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621663</person_id>
				<author_profile_id><![CDATA[81442611373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kishi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621664</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P1621665</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401030</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kimura, S., Oguchi, R., Tanida, H., Kakehi, Y., Takahashi, K., and Naemura, T. 2008 "PVLC projector: image projection with imperceptible pixel-level metadata," In <i>ACM SIGGRAPH 2008, Posters, B177</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400935</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, G., Ahn, Y. 2008 "An LED Using Active Reflectors and Free-Space Optical Transmission," In <i>ACM SIGGRAPH 2008, Posters, F218</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278286</ref_obj_id>
				<ref_obj_pid>1278280</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ando, H., Watanabe, J., Amemiya, T., Maeda, T. 2007 "Full-scale saccade-based display: Public/Private image presentation based on gaze-contingent visual illusion," In <i>ACM SIGGRAPH 2007 Emerging Technologies</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: system Figure 2: afterimages Figure 3: afterimages and projected images 1 Introduction In 
this paper, we present a novel persistence of vision display named SteganoScan. SteganoScan is a stick 
shaped display device with a single line of LEDs that emits light in full color and shows 2D images without 
any screen. This system is composed of two units: our PVLC (Pixel-level Vi­sual Light Communication) 
projector [Kimura et al. 2008] and a receiver made up of an LED array and photo sensors. The PVLC projector 
can embed information pixel by pixel in the projected im­age. When users put the SteganoScan in the projection 
light, it cap­tures and displays the information simultaneously. Moreover, by waving the SteganoScan 
quickly, viewers can perceive 2D images caused by the effects of retinal afterimages. This system requires 
no synchronization of the speed of the LED array with the speed of its emitting pattern, and allows users 
to wave arbitrarily. Furthermore, this system can present different 2D images according to its loca­tion. 
Therefore, SteganoScan can work as a new type of augmented reality visual display. 2 SteganoScan In 
SteganoScan, we offer core technical innovations as following. One is the interface design and the method 
for data transmission. SteganoScan consists of full color LEDs, photo sensors, and micro controllers. 
A photo sensor is attached to each LED of the array, and the color of each LED is independently controlled 
with high-speed .ickers. The idea of combining optical devices as the transmitter and photo sensors has 
been already proposed [Lee et al. 2008]. However, this project did not achieve high-density data transmis­sion. 
To embed the high-density information through high-speed .ickers in open space, we use a specialized 
DLP projector we have developed. This projector can embed human-imperceptive meta­data into the projected 
image and send distinct high frequency sig­nals pixel by pixel. When users wave SteganoScan in the projection 
light, it receives the color data and shows 2D images. Therefore this system requires no 3D sensors or 
cameras. Secondly, SteganoScan can present different 2D images according to its location rather than 
to the user s motions. While several per­sistence of vision displays have been proposed [Ando et al. 
2007], most of them can show 2D images only when the speed of the LED *e-mail: vlc@hc.ic.i.u-tokyo.ac.jp 
array synchronizes with the speed of its emitting pattern, restrict­ing the user to certain de.nite actions 
when moving the LED array. On the other hand, SteganoScan requires no synchronization and allows users 
to wave arbitrarily, because it can capture and display information at the same time. Thirdly, when we 
project images on a screen and wave SteganoScan in front of it, SteganoScan can show 2D images related 
to the pro­jected images. Therefore this system can serve the interaction be­tween afterimages and projected 
images. Finally, because our system transmits information in parallel, we can simultaneously decode them 
using several SteganoScans. This serves to greatly enhance a variety of applications. 3 Applications 
We have developed a prototype system and implemented two types of applications. One is an application 
utilizing only afterimages. In this applica­tion (Figure 2), the user can show hidden 2D images (e.g. 
Siggraph logo) by waving SteganoScan. Another is an Augmented Reality application. In this application 
(Figure 3), the color data (e.g. but­ter.ies) are superimposed on the projected image (e.g. garden). 
 4 Conclusion We introduced a novel persistence of vision display, and imple­mented several applications 
with it. Future work will be focused on improving the shape of display and on expanding a variety of 
applications. References KIMURA, S., OGUCHI, R., TANIDA, H., KAKEHI, Y., TAKA-HASHI, K., AND NAEMURA, 
T. 2008 PVLC projector : im­age projection with imperceptible pixel-level metadata, In ACM SIGGRAPH 2008, 
Posters, B177. LEE, G., AHN, Y. 2008 An LED Using Active Re.ectors and Free-Space Optical Transmission, 
In ACM SIGGRAPH 2008, Posters, F218. ANDO, H., WATANABE, J., AMEMIYA, T., MAEDA, T. 2007 Full-scale saccade-based 
display: Public/Private image presentation based on gaze-contingent visual illusion, In ACM SIGGRAPH 
2007 Emerging Technologies. Copyright is held by the author / owner(s). SIGGRAPH 2009, New Orleans, Louisiana, 
August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1599405</article_id>
		<sort_key>1040</sort_key>
		<display_label>Article No.</display_label>
		<display_no>104</display_no>
		<article_publication_date>08-03-2009</article_publication_date>
		<seq_no>104</seq_no>
		<title><![CDATA[Stereoscopic display technique for Web3D images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/1599301.1599405</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1599405</url>
		<abstract>
			<par><![CDATA[<p>A simple and unique stereoscopic display technique for Web3D images is presented. Various Web3D systems are being widely used, and, in these systems, 3D objects being displayed in a WWW browser can be interactively rotated in arbitrary directions by manipulating a pointing device such as a mouse. Although complete information on the 3D shape of objects is included in the Web3D data, a stereoscopic view of the objects is not possible because Web3D player programs currently available do not support this function. Stereoscopic viewing of Web3D images is obviously technologically possible if both the player program and the Web3D data on the web are remodeled, but it requires huge cost. Therefore, I developed a new method in which neither the existing player program nor the Web3D data needs to be modified.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>3D/stereo scene analysis</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P1621666</person_id>
				<author_profile_id><![CDATA[81421598106]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stereoscopic Display Technique for Web3D Images Kazuhisa Yanaka Kanagawa Institute of Technology 1 
Introduction A simple and unique stereoscopic display technique for Web3D images is presented. Various 
Web3D systems are being widely used, and, in these systems, 3D objects being displayed in a WWW browser 
can be interactively rotated in arbitrary directions by manipulating a pointing device such as a mouse. 
Although complete information on the 3D shape of objects is included in the Web3D data, a stereoscopic 
view of the objects is not possible because Web3D player programs currently available do not support 
this function. Stereoscopic viewing of Web3D images is obviously technologically possible if both the 
player program and the Web3D data on the web are remodeled, but it requires huge cost. Therefore, I developed 
a new method in which neither the existing player program nor the Web3D data needs to be modified. 2 
Principle of 3D Among the various factors related to stereoscopic viewing, one of the major ones is binocular 
parallax, which is the slight difference between images of the left and right eyes. Most stereoscopic 
display systems including systems that use LCD shutter glass, polarized light glass, a parallax barrier, 
or a lenticular lens have binocular parallax. If Web3D systems were to become stereoscopic, they would 
be richer media. To achieve stereoscopic viewing, we need to obtain the images from the right and left 
eyes. Fortunately, an object can be turned around by dragging on the image displayed on the screen with 
the mouse. Although the mouse is usually operated by a person, an event that is equivalent to the mouse 
operation can technically be achieved with an application program. This is true in many operating systems, 
including Windows XP and Vista. Moreover, pixel data of the image displayed on the screen can be acquired 
from another application program. These functions can be applied in developing a 3D display system for 
Web3D images without touching the Web3D player. Figure 1 Principle of our method. e-mail: yanaka@ic.kanagawa-it.ac.jp 
Figure 1 shows the principle of our method. Suppose that a car is displayed by a Web3D player that was 
built into a WWW browser. If the car is dragged right, it faces to the right. If it is dragged left, 
it faces to the left. In my program, events equivalent to mouse operation are automatically generated 
repeatedly by using a timer interrupt. As a result, the displayed car alternately vibrates to the right 
and left at a certain interval time. The frequency of the vibration is 1 Hz, for instance. The program 
also acquires the pixel data from the video memory and saves the data in either left or right eye memory 
whenever the direction of the car changes. In addition, whenever either the left or right eye memory 
is updated, the program synthesizes a new image for the stereo display by using the latest data. The 
pixel arrangement depends on the 3D display used. The synthesized image is then displayed on the PC screen, 
and users can view stereoscopic images.  3 Experiment In my experiment, ViewpointTM was used because 
it is a typical Web3D technology. Viewpoint media player, which works on Microsoft Windows XP, was used 
as the Web3D player. The personal computer used in the experiment was a Sharp PC-AL3DH because it is 
a laptop PC that has a stereoscopic LCD using the parallax barrier method, the principle of which is 
shown in Figure 2. Excellent stereoscopic images were displayed, as shown in Figure 3. Parallax Switch 
barrier LCD TFT LCD   Figure 3. Images displayed on the screen.  4 Conclusion A new stereoscopic display 
method for Web3D images was implemented in a commercially available laptop computer with parallax barrier 
LCD, enabling excellent binocular vision. A big advantage of this method is that an existing Viewpoint 
media player and its data can be used without any modifications. Therefore, Web3D pages already open 
to the public on the WWW can be made stereoscopic immediately. Copyright is held by the author / owner(s). 
SIGGRAPH 2009, New Orleans, Louisiana, August 3 7, 2009. ISBN 978-1-60558-726-4/09/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Copyright is held by the author/owner(s).</copyright_holder_name>
				<copyright_holder_year>2009</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
